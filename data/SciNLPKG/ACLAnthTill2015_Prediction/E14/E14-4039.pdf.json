{"title": [{"text": "Multi-Domain Sentiment Relevance Classification with Automatic Representation Learning", "labels": [], "entities": [{"text": "Multi-Domain Sentiment Relevance Classification", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7490549236536026}]}], "abstractContent": [{"text": "Sentiment relevance (SR) aims at identifying content that does not contribute to sentiment analysis.", "labels": [], "entities": [{"text": "Sentiment relevance (SR)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9043091535568237}, {"text": "sentiment analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9061653316020966}]}, {"text": "Previously, automatic SR classification has been studied in a limited scope, using a single domain and feature augmentation techniques that require large hand-crafted databases.", "labels": [], "entities": [{"text": "automatic SR classification", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7565295894940695}]}, {"text": "In this paper, we present experiments on SR classification with automatically learned feature representations on multiple domains.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.992672473192215}]}, {"text": "We show that a combination of transfer learning and in-task supervision using features learned unsupervisedly by the stacked denoising autoencoder significantly outperforms a bag-of-words baseline for in-domain and cross-domain classification.", "labels": [], "entities": [{"text": "cross-domain classification", "start_pos": 215, "end_pos": 242, "type": "TASK", "confidence": 0.6844266206026077}]}], "introductionContent": [{"text": "Many approaches to sentiment analysis rely on term-based clues to detect the polarity of sentences or documents, using the bag-of-words (BoW) model (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9538691341876984}]}, {"text": "One drawback of this approach is that the polarity of a clue is often treated as fixed, which can be problematic when content is not intended to contribute to the polarity of the entity but contains a term with a known lexical non-neutral polarity.", "labels": [], "entities": []}, {"text": "For example, movie reviews often have plot summaries which contain subjective descriptions, e.g., \"April loves her new home and friends.\", containing \"loves\", commonly a subjective positive term.", "labels": [], "entities": []}, {"text": "Other domains contain different types of nonrelevant content: Music reviews may contain track listings, product reviews on retail platforms contain complaints that do not concern the product, e.g., about shipping and handling.", "labels": [], "entities": []}, {"text": "Filtering such nonrelevant content can help to improve sentiment analysis ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.964509904384613}]}, {"text": "Sentiment relevance () formalizes this distinction: Content that contributes to the overall sentiment of a document is said to be sentiment relevant (SR), other content is sentiment nonrelevant (SNR).", "labels": [], "entities": []}, {"text": "The main bottleneck in automatic SR classification is the lack of annotated data.", "labels": [], "entities": [{"text": "automatic SR classification", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6824027895927429}]}, {"text": "On the sentence level, it has been attempted for the movie review domain () on a manually annotated dataset that covers around 3,500 sentences.", "labels": [], "entities": []}, {"text": "The sentiment analysis data by contains SR annotations for five product review domains, four of which have fewer than 1,000 annotated examples.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9057343900203705}]}, {"text": "As the amount of labeled data is low, we adopt transfer learning), which has been used before for SR classification.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.9822626709938049}]}, {"text": "In this setup, we train a classifier on a different task, using subjectivity-labeled data -for which a large number of annotated examples is available -and apply it for SR classification.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.9730254113674164}]}, {"text": "To enable knowledge transfer between the tasks, feature space augmentation has been proposed.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7595426738262177}]}, {"text": "For this purpose, we employ automatic representation learning, using the stacked denoising autoencoder) which has been applied successfully to other domain adaptation problems such as crossdomain sentiment analysis.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7224988043308258}, {"text": "crossdomain sentiment analysis", "start_pos": 184, "end_pos": 214, "type": "TASK", "confidence": 0.8600476980209351}]}, {"text": "In this paper, we present experiments on both multi-domain and cross-domain SR classification.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8653118908405304}]}, {"text": "We show that compared to the in-domain baseline, TL with SDA features increases F 1 by 6.8% on average.", "labels": [], "entities": [{"text": "F 1", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9934976100921631}]}, {"text": "We find that domain adaptation using TL with the SDA compensates for strong domain shifts, reducing the average classification transfer loss by 12.7%.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7949293255805969}]}], "datasetContent": [{"text": "The task in this paper is multi-and cross-domain SR classification.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.9051007330417633}]}, {"text": "Two aspects motivate our work: First, we need to address the sparse data situation.", "labels": [], "entities": []}, {"text": "Second, we are interested in how crossdomain effects influence SR classification.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9875698387622833}]}, {"text": "We classify SR in three different setups: in-domain (ID), in which we take the training and test data from the same domain; domain adaptation (DA), where training and test data are from different domains; and transfer learning (TL), where we use a much larger amount of data from a different but related task.", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7819518446922302}, {"text": "transfer learning (TL)", "start_pos": 209, "end_pos": 231, "type": "TASK", "confidence": 0.7931910514831543}]}, {"text": "To improve the generalization capabilities of the models, we use representations learned by the SDA.", "labels": [], "entities": []}, {"text": "We will next describe our classification setup in more detail.", "labels": [], "entities": []}, {"text": "Data We use the following datasets for our experiments.", "labels": [], "entities": []}, {"text": "shows statistics on the datasets.", "labels": [], "entities": []}, {"text": "In-Domain Classification (ID) shows macro-averaged F 1 for different SR models.", "labels": [], "entities": [{"text": "In-Domain Classification (ID)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6776773273944855}, {"text": "F 1", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9377411007881165}]}, {"text": "We first turn to fully supervised SR classification with bag-of-words (BoW) features using ID training (line 2).", "labels": [], "entities": [{"text": "SR classification", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.9807486236095428}]}, {"text": "While the results for CINEMA are high, on par with the reported results in related work, they are low for the PRODUCTS data.", "labels": [], "entities": [{"text": "CINEMA", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.7638257145881653}, {"text": "PRODUCTS data", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.7949657738208771}]}, {"text": "This is not surprising as the SVM is trained with fewer than 600 examples on each domain.", "labels": [], "entities": []}, {"text": "Also, no unknown category exists in the latter dataset.", "labels": [], "entities": []}, {"text": "While ambiguous examples on CINEMA are annotated as unknown, they receive an SR label on PRODUCTS.", "labels": [], "entities": [{"text": "CINEMA", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9273298382759094}, {"text": "SR label", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9816851317882538}, {"text": "PRODUCTS", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8887682557106018}]}, {"text": "Thus, many examples are ambiguous and thus difficult to classify.", "labels": [], "entities": []}, {"text": "SDA features worsen results significantly \u2020 (lines 3-4) on all domains except CINEMA and DVDS due to data sparsity.", "labels": [], "entities": [{"text": "DVDS", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.7285290956497192}]}, {"text": "They are the two most homogeneous domains where plot descriptions makeup a large part of the SNR content.", "labels": [], "entities": [{"text": "SNR", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9319682717323303}]}, {"text": "On many domains, there is no single prototypical type of SNR which could be learned from a small amount of training data.", "labels": [], "entities": [{"text": "SNR", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.97869473695755}]}, {"text": "We find that 1-SDA (line 6) already performs significantly \u2020 better than the ID baseline on all domains except CINEMA which has a much larger amount of ID training data available than the other domains (approx. 1700 sentences vs. fewer than 600).", "labels": [], "entities": [{"text": "CINEMA", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.9346139430999756}]}, {"text": "Using stacking, 2-SDA (line 7) improves the results on three domains significantly, \u2020 and performs on par with the ID classifier on CIN-EMA.", "labels": [], "entities": [{"text": "\u2020", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9961007833480835}]}, {"text": "We found that stack depths of k > 2 do not significantly \u2020 increase performance.", "labels": [], "entities": []}, {"text": "Finally, we try a combination of ID and TL (ID+TL), training on both P&L and the respective ID training fold of CINEMA/PRODUCTS.", "labels": [], "entities": [{"text": "TL", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9523757100105286}, {"text": "CINEMA/PRODUCTS", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.685496966044108}]}, {"text": "The results for this experiment are shown in lines 8-10 in.", "labels": [], "entities": []}, {"text": "Comparing BoW models, we beat both ID and TL across all domains (lines 2 and 5).", "labels": [], "entities": []}, {"text": "With SDA features, we are able to beat ID for CINEMA.", "labels": [], "entities": [{"text": "CINEMA", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.8793786764144897}]}, {"text": "The results on the other domains are comparable to plain TL.", "labels": [], "entities": []}, {"text": "This is a promising result, showing that with SDA features, ID+TL performs as well as or better than plain TL.", "labels": [], "entities": []}, {"text": "This property could be exploited for domains where labeled data is not available.", "labels": [], "entities": []}, {"text": "We will show below that SDA features become important when we apply ID+TL to domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7281540334224701}]}, {"text": "We also conducted experiments using only the 5,000 most frequent features but found that the SDA does not generalize well from this input representation, particularly on EL and MUSIC.", "labels": [], "entities": []}, {"text": "This confirms that in SR, rare features make an important contribution (such as named entities in the movie domain).", "labels": [], "entities": [{"text": "SR", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9737333059310913}]}, {"text": "setups with BoW and 2-SDA features.", "labels": [], "entities": [{"text": "BoW", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.530398428440094}]}, {"text": "We measure the transfer losses we suffer from training on one domain and evaluating on another ().", "labels": [], "entities": []}, {"text": "The overall picture is the same as above: ID+TL 2-SDA models perform best.", "labels": [], "entities": []}, {"text": "In the baseline BoW ID setup, domain shifts have a strong influence on the results.", "labels": [], "entities": [{"text": "BoW ID", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8253112435340881}]}, {"text": "The combination of out-of-domain and out-of-task data in ID+TL keeps losses uniformly low.", "labels": [], "entities": []}, {"text": "2-SDA features lower almost all losses further.", "labels": [], "entities": []}, {"text": "On average, 2-SDA ID+TL reduces transfer loss by 12.7 points compared to the baseline (  BOOKS is the most challenging domain in all setups.", "labels": [], "entities": [{"text": "BOOKS", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9918617010116577}]}, {"text": "It is particularly heterogeneous, containing both fiction and non-fiction reviews which feature different SNR aspects.", "labels": [], "entities": [{"text": "SNR", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9585253000259399}]}, {"text": "Both results illustrate that domain effects depend on how diverse SNR content is within the domain.", "labels": [], "entities": [{"text": "SNR content", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8998004198074341}]}, {"text": "Overall, the results show that ID+TL leads to a successful compensation of cross-domain effects.", "labels": [], "entities": [{"text": "TL", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.6531254053115845}]}, {"text": "SDA features improve the results significantly \u2020 for ID+TL.", "labels": [], "entities": []}, {"text": "In particular, we find that the SDA successfully compensates for the strong domain shift between CINEMA and PRODUCTS.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Macro-averaged F 1 (%) evaluating on each test domain on both folds. \u2205 = row mean. Bold:  best result in each column and results in that column not significantly different from it.", "labels": [], "entities": [{"text": "F 1", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8805935680866241}, {"text": "row mean", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9489046335220337}]}]}