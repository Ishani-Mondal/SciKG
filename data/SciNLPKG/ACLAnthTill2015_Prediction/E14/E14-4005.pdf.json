{"title": [{"text": "Measuring the Similarity between Automatically Generated Topics", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison of the topics' word probability distributions.", "labels": [], "entities": [{"text": "measuring similarity between automatically generated topics", "start_pos": 38, "end_pos": 97, "type": "TASK", "confidence": 0.7326856752236685}]}, {"text": "This paper presents alternative approaches, including ones based on distributional semantics and knowledge-based measures, evaluated by comparison with human judgements.", "labels": [], "entities": []}, {"text": "The best performing methods provide reliable estimates of topic similarity comparable with human performance and should be used in preference to the word probability distribution measures used previously.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models () have proved to be useful for interpreting and organising the contents of large document collections.", "labels": [], "entities": [{"text": "interpreting and organising the contents of large document collections", "start_pos": 45, "end_pos": 115, "type": "TASK", "confidence": 0.7353292968538072}]}, {"text": "It seems intuitively plausible that some automatically generated topics will be similar while others are dis-similar.", "labels": [], "entities": []}, {"text": "For example, a topic about basketball (team game james season player nba play knicks coach league) is more similar to a topic about football (world cup team soccer africa player south game match goal) than one about the global finance (fed financial banks federal reserve bank bernanke rule crisis credit).", "labels": [], "entities": []}, {"text": "Methods for automatically determining the similarity between topics have several potential applications, such as analysis of corpora to determine topics being discussed ( or within topic browsers to decide which topics should be shown together.", "labels": [], "entities": []}, {"text": "Latent Dirichlet Allocation (LDA) () is a popular type of topic model but cannot capture such correlations unless the semantic similarity between topics is measured.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.8664879083633423}]}, {"text": "Other topic models, such as the Correlated Topic Model (CTM) (), overcome this limitation and identify correlations between topics.", "labels": [], "entities": []}, {"text": "Approaches to identifying similar topics fora range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated.", "labels": [], "entities": []}, {"text": "Word distributions have been compared using a variety of measures such as KL-divergence (, cosine measure () and the average Log Odds Ratio (.", "labels": [], "entities": [{"text": "Log Odds Ratio", "start_pos": 125, "end_pos": 139, "type": "METRIC", "confidence": 0.6455376545588175}]}, {"text": "also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard's Coefficient, Kendall's \u03c4 coefficient, Discount Cumulative.", "labels": [], "entities": []}, {"text": "This paper compares a wider range of approaches to measuring topic similarity than previous work.", "labels": [], "entities": []}, {"text": "In addition these measures are evaluated directly by comparing them against human judgements.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We created a data set consisting of pairs of topics generated by two topic models (LDA and CTM) over two document collections using different numbers of topics.", "labels": [], "entities": []}, {"text": "The first consists of 47,229 news articles from New York Times (NYT) in the GigaWord corpus and the second contains 50,000 articles from ukWAC ().", "labels": [], "entities": [{"text": "New York Times (NYT)", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.6633315682411194}, {"text": "GigaWord corpus", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9137807786464691}, {"text": "ukWAC", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.9815471172332764}]}, {"text": "Each article is tokenised then stop words and words appearing fewer than five times in the corpora removed.", "labels": [], "entities": []}, {"text": "This results in a total of 57,651 unique tokens for the NYT corpus and 72,672 for ukWAC.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.96717169880867}, {"text": "ukWAC", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.985363781452179}]}, {"text": "LDA Topics are learned by training LDA models over the two corpora using gensim 3 . The number of topics is set to T = 50, 100, 200 and hyperparameters, \u03b1 and \u03b2, are set to 1 T . Randomly selecting pairs of topics will result to a data set in which the majority of pairs would not be similar.", "labels": [], "entities": []}, {"text": "We overcome that problem by assuming that the JSD between likely relevant pairs will below while it will be higher for less relevant pairs of topics.", "labels": [], "entities": [{"text": "JSD", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.8052075505256653}]}, {"text": "We selected 800 pairs of topics.", "labels": [], "entities": []}, {"text": "600 pairs represent topics with similar word distributions (in the top 6 most relevant topics ranked by JSD).", "labels": [], "entities": [{"text": "JSD", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.6862807869911194}]}, {"text": "The remaining 200 pairs were selected randomly.", "labels": [], "entities": []}, {"text": "CTM is trained using the EM algorithm . The number of topics to learn is set to T = 50, 100, 200 and the rest of the settings are set to their default values.", "labels": [], "entities": [{"text": "T", "start_pos": 80, "end_pos": 81, "type": "METRIC", "confidence": 0.9933236241340637}]}, {"text": "The topic graph generated by CTM was used to create all the possible pairs between topics that are connected.", "labels": [], "entities": []}, {"text": "This results in a total of 70, 468 and 695 pairs in NYT, and a total of 80, 246 and 258 pairs in ukWAC for the 50, 100 and 200 topics respectively.", "labels": [], "entities": [{"text": "NYT", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8720159530639648}, {"text": "ukWAC", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9777767658233643}]}, {"text": "Incoherent topics are removed using an approach based on distributional semantics.", "labels": [], "entities": []}, {"text": "Each topic is represented using the top 10 words with the highest marginal probability.", "labels": [], "entities": []}, {"text": "Human Judgements of Topic Similarity were obtained using an online crowdsourcing platform, Crowdflower.", "labels": [], "entities": [{"text": "Human Judgements of Topic Similarity", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7293405592441559}]}, {"text": "Annotators were provided with pairs of topics and were asked to judge how similar the topics are by providing a rating on a scale of 0 (completely unrelated) to 5 (identical).", "labels": [], "entities": []}, {"text": "The average response for each pair was calculated in order to create the final similarity judgement for use as a gold-standard.", "labels": [], "entities": [{"text": "similarity judgement", "start_pos": 79, "end_pos": 99, "type": "METRIC", "confidence": 0.9354145526885986}]}, {"text": "The average Inter-Annotator agreement (IAA) across all pairs for all of the collections is in the range of 0.53-0.68.", "labels": [], "entities": [{"text": "Inter-Annotator agreement (IAA)", "start_pos": 12, "end_pos": 43, "type": "METRIC", "confidence": 0.9461855530738831}]}, {"text": "The data set together with gold-standard annotations is freely available 5 . shows the correlation (Spearman) between the topic similarity metrics described in Section 2 and average human judgements for the LDA and CTM topic pairs.", "labels": [], "entities": [{"text": "Spearman)", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9882601797580719}]}, {"text": "It also shows the performance of a Word Overlap baseline which measures the number of terms that two topics have in common normalised by the total number of topic terms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for various approaches to topic similarity. All correlations are significant p < 0.001.  Underlined scores denote best performance of a single feature. Bold denotes best overall performance.", "labels": [], "entities": [{"text": "topic similarity", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6996806114912033}]}]}