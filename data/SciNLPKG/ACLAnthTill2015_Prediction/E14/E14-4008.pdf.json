{"title": [{"text": "Chasing Hypernyms in Vector Spaces with Entropy", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we introduce SLQS, anew entropy-based measure for the unsupervised identification of hypernymy and its directionality in Distributional Semantic Models (DSMs).", "labels": [], "entities": []}, {"text": "SLQS is assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various semantic relations.", "labels": [], "entities": []}, {"text": "In both tasks, SLQS outperforms other state-of-the-art measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.7093717207511266}]}, {"text": "DSMs rely on the Distributional Hypothesis and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine.", "labels": [], "entities": []}, {"text": "DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.9738195240497589}]}, {"text": "(. One major shortcoming of current DSMs is that they are notable to discriminate among different types of semantic relations linking distributionally similar lexemes.", "labels": [], "entities": []}, {"text": "For instance, the nearest neighbors of dog in vector spaces typically include hypernyms like animal, cohyponyms like cat, meronyms like tail, together with other words semantically related to dog.", "labels": [], "entities": []}, {"text": "DSMs tell us how similar these words are to dog, but they do not give us a principled way to single out the items linked by a specific relation (e.g., hypernyms).", "labels": [], "entities": []}, {"text": "Another related issue is to what extent distributional similarity, as currently measured by DSMs, is appropriate to model the semantic properties of a relation like hypernymy, which is crucial for Natural Language Processing.", "labels": [], "entities": []}, {"text": "Similarity is by definition asymmetric notion (a is similar to b if and only if b is similar to a) and it can therefore naturally model symmetric semantic relations, such as synonymy and cohyponymy.", "labels": [], "entities": []}, {"text": "It is not clear, however, how this notion can also model hypernymy, which is asymmetric.", "labels": [], "entities": []}, {"text": "In fact, it is not enough to say that animal is distributionally similar to dog.", "labels": [], "entities": []}, {"text": "We must also account for the fact that animal is semantically broader than dog: every dog is an animal, but not every animal is a dog.", "labels": [], "entities": []}, {"text": "In this paper, we introduce SLQS, anew entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality.", "labels": [], "entities": []}, {"text": "We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other semantic relations (detection task).", "labels": [], "entities": []}, {"text": "Given the centrality of hypernymy, the relevance of the themes we address hardly needs any further motivation.", "labels": [], "entities": []}, {"text": "Improving the ability of DSMs to identify hypernyms is in fact extremely important in tasks such as Recognizing Textual Entailment (RTE) and ontology learning, as well as to enhance the cognitive plausibility of DSMs as general models of the semantic lexicon.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 100, "end_pos": 136, "type": "TASK", "confidence": 0.7394790301720301}]}], "datasetContent": [{"text": "For the experiments, we used a standard window-based DSM recording co-occurrences with the nearest 2 content words to the left and right of each target word.", "labels": [], "entities": []}, {"text": "Co-occurrences were extracted from a combination of the freely available ukWaC and WaCkypedia corpora (with 1.915 billion and 820 million words, respectively) and weighted with LMI.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.9855908155441284}, {"text": "WaCkypedia corpora", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.8575819134712219}, {"text": "LMI", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.9147841334342957}]}, {"text": "To assess SLQS we relied on a subset of BLESS (), a freelyavailable dataset that includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities (e.g. BIRD, FRUIT, etc.).", "labels": [], "entities": [{"text": "SLQS", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9470957517623901}, {"text": "BLESS", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9963735938072205}, {"text": "BIRD", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.8886159062385559}, {"text": "FRUIT", "start_pos": 213, "end_pos": 218, "type": "METRIC", "confidence": 0.5961779356002808}]}, {"text": "For each target concept, BLESS contains several relata, connected to it through one relation, such as cohyponymy (COORD), hypernymy (HYPER), meronymy (MERO) or no-relation (RANDOM-N).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.983562171459198}, {"text": "MERO", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.6012513041496277}, {"text": "RANDOM-N", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.8533889651298523}]}, {"text": "Since BLESS contains different numbers of pairs for every relation, we randomly extracted a subset of 1,277 pairs for each relation, where 1,277 is the maximum number of HYPER-related pairs for which vectors existed in our DSM.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9731252193450928}]}], "tableCaptions": [{"text": " Table 1. Accuracy for Task 1.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987913966178894}]}, {"text": " Table 2. AP values for T", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.998177170753479}, {"text": "T", "start_pos": 24, "end_pos": 25, "type": "TASK", "confidence": 0.6130558252334595}]}]}