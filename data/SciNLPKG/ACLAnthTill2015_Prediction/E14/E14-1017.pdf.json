{"title": [{"text": "Structured and Unstructured Cache Models for SMT Domain Adaptation", "labels": [], "entities": [{"text": "SMT Domain Adaptation", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.947880208492279}]}], "abstractContent": [{"text": "We present a French to English translation system for Wikipedia biography articles.", "labels": [], "entities": [{"text": "French to English translation", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6350495144724846}]}, {"text": "We use training data from out-of-domain corpora and adapt the system for biographies.", "labels": [], "entities": []}, {"text": "We propose two forms of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7362486869096756}]}, {"text": "The first biases the system towards words likely in biogra-phies and encourages repetition of words across the document.", "labels": [], "entities": []}, {"text": "Since biographies in Wikipedia follow a regular structure, our second model exploits this structure as a sequence of topic segments, where each segment discusses a narrower subtopic of the biography domain.", "labels": [], "entities": []}, {"text": "In this structured model, the system is encouraged to use words likely in the current segment's topic rather than in biographies as a whole.", "labels": [], "entities": []}, {"text": "We implement both systems using cache-based translation techniques.", "labels": [], "entities": []}, {"text": "We show that a system trained on Europarl and news can be adapted for biographies with 0.5 BLEU score improvement using our models.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9822616577148438}, {"text": "BLEU score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9809490442276001}]}, {"text": "Further the structure-aware model out-performs the system which treats the entire document as a single segment.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper explores domain adaptation of statistical machine translation (SMT) systems to contexts where the target documents have predictable regularity in topic and document structure.", "labels": [], "entities": [{"text": "domain adaptation of statistical machine translation (SMT)", "start_pos": 20, "end_pos": 78, "type": "TASK", "confidence": 0.7166799141301049}]}, {"text": "Regularities can take the form of high rates of word repetition across documents, similarities in sentence syntax, similar subtopics and discourse organization.", "labels": [], "entities": []}, {"text": "Domain adaptation for such documents can exploit these similarities.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7014846354722977}]}, {"text": "In this paper we focus on topic (lexical) regularities in a domain.", "labels": [], "entities": []}, {"text": "We present a system that translates Wikipedia biographies from French to English by adapting a system trained on Europarl and news commentaries.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.9840015172958374}]}, {"text": "This task is interesting for the following two reasons.", "labels": [], "entities": []}, {"text": "Many techniques for SMT domain adaption have focused on rather diverse domains such as using systems trained on Europarl or news to translate medical articles), blogs () and transcribed lectures).", "labels": [], "entities": [{"text": "SMT domain adaption", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.9452938040097555}, {"text": "Europarl", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.9938580393791199}]}, {"text": "The main challenge for such systems is translating out-of-vocabulary words).", "labels": [], "entities": [{"text": "translating out-of-vocabulary words", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.8873837987581888}]}, {"text": "In contrast, words in biographies are closer to a training corpus of news commentaries and parlimentary proceedings and allow us to examine how well domain adaptation techniques can disambiguate lexical choices.", "labels": [], "entities": []}, {"text": "Such an analysis is harder to do on very divergent domains.", "labels": [], "entities": []}, {"text": "In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics such as 'childhood', 'schooling', 'career' and 'later life', and a likely chronological order to these topics.", "labels": [], "entities": []}, {"text": "These regularities become more predictable in documents from sources such as Wikipedia.", "labels": [], "entities": []}, {"text": "This setting allows us to explore the utility of models which make translation decisions depending on the discourse structure.", "labels": [], "entities": []}, {"text": "Translation methods for structured documents have only recently been explored in.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9476039409637451}]}, {"text": "However, their system was developed for parlimentary proceedings and translations were adapted using separate language models based upon the identity of the speaker, text type (questions, debate, etc.) and the year when the proceedings took place.", "labels": [], "entities": []}, {"text": "Biographies constitute a more realistic discourse context to develop structured models.", "labels": [], "entities": []}, {"text": "This paper introduces anew corpus consisting of paired French-English translations of biography articles from Wikipedia.", "labels": [], "entities": []}, {"text": "We translate this corpus by developing cache-based domain adaptation methods, a technique recently proposed by.", "labels": [], "entities": []}, {"text": "In such methods, cache(s) can be filled with relevant items for translation and translation hypotheses that match a greater number of cache items are scored higher.", "labels": [], "entities": [{"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9789820909500122}]}, {"text": "These cache scores are used as additional features during decoding.", "labels": [], "entities": []}, {"text": "We use two types of cache-one which encourages the use of words more indicative of the biography domain and another which encourages word repetition in the same document.", "labels": [], "entities": [{"text": "word repetition", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.6961857974529266}]}, {"text": "We also show how cache models allow for straightforward implementation of structured translation by refreshing the cache in response to topic segment boundaries.", "labels": [], "entities": []}, {"text": "We fill caches with words relevant to the topic of the current segment which is being translated.", "labels": [], "entities": []}, {"text": "The cache contents are obtained from an unsupervised topic model which induces clusters of words that are likely to appear in the same topic segment.", "labels": [], "entities": []}, {"text": "Evaluation results show that cache-based models give upto 0.5 BLEU score improvements over an out-of-domain system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9730965495109558}]}, {"text": "In addition, models that take topical structure into account score 0.3 BLEU points higher than those which ignore discourse structure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.99950110912323}]}], "datasetContent": [{"text": "We use the Moses phrase-based translation system () to implement our models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of Wikipedia biographies data", "labels": [], "entities": []}, {"text": " Table 2: Top 50 words from 2 topics of the T = 50 topic model", "labels": [], "entities": []}, {"text": " Table 3: Best weights for cache features and  BLEU scores (averaged for tuning documents).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9982492923736572}]}, {"text": " Table 4: BLEU scores on the test set. 'doc' in- dicates BLEU scores averaged over documents,  'sent' indicates sentence-level BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987358450889587}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9371866583824158}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9309992790222168}]}, {"text": " Table 5: Average BLEU score gains from a struc- tured cache (compared to domain caches) split by  different properties of documents in the test set", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.960556298494339}]}, {"text": " Table 6: Translation performance on automati- cally segmented test corpus", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9840627908706665}]}]}