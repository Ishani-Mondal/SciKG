{"title": [{"text": "Is Machine Translation Getting Better over Time?", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7860906422138214}]}], "abstractContent": [{"text": "Recent human evaluation of machine translation has focused on relative preference judgments of translation quality, making it difficult to track longitudinal improvements overtime.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7407766580581665}]}, {"text": "We carryout a large-scale crowd-sourcing experiment to estimate the degree to which state-of-the-art performance in machine translation has increased over the past five years.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7400761842727661}]}, {"text": "To facilitate longitudinal evaluation, we move away from relative preference judgments and instead ask human judges to provide direct estimates of the quality of individual translations in isolation from alternate outputs.", "labels": [], "entities": []}, {"text": "For seven European language pairs, our evaluation estimates an average 10-point improvement to state-of-the-art machine translation between 2007 and 2012, with Czech-to-English translation standing out as the language pair achieving most substantial gains.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.754618227481842}, {"text": "Czech-to-English translation", "start_pos": 160, "end_pos": 188, "type": "TASK", "confidence": 0.6750945299863815}]}, {"text": "Our method of human evaluation offers an economically feasible and robust means of performing ongoing longitudinal evaluation of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7770771980285645}]}], "introductionContent": [{"text": "Human evaluation provides the foundation for empirical machine translation (MT), whether human judges are employed directly to evaluate system output, or via the use of automatic metricsvalidated through correlation with human judgments.", "labels": [], "entities": [{"text": "empirical machine translation (MT)", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.8263692259788513}]}, {"text": "Achieving consistent human evaluation is not easy, however.", "labels": [], "entities": []}, {"text": "Annual evaluation campaigns conduct large-scale human assessment but report ever-decreasing levels of judge consistency -when given the same pair of translations to repeat-assess, even expert human judges will worryingly often contradict both the preference judgment of other judges and their own earlier preference (.", "labels": [], "entities": [{"text": "consistency", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.7662357687950134}]}, {"text": "For this reason, human evaluation has been targeted within the community as an area in need of attention, with increased efforts to develop more reliable methodologies.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.6954928040504456}]}, {"text": "One standard platform for human evaluation is WMT shared tasks, where assessments have (since 2007) taken the form of ranking five alternate system outputs from best to worst).", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7569286227226257}]}, {"text": "This method has been shown to produce more consistent judgments compared to fluency and adequacy judgments on a five-point scale.", "labels": [], "entities": []}, {"text": "However, relative preference judgments have been criticized for being a simplification of the real differences between translations, not sufficiently taking into account the large number of different types of errors of varying severity that occur in translations (.", "labels": [], "entities": []}, {"text": "Relative preference judgments do not take into account the degree to which one translation is better than another -there is noway of knowing if a winning system produces far better translations than all other systems, or if that system would have ranked lower if the severity of its inferior translation outputs were taken into account.", "labels": [], "entities": []}, {"text": "Rather than directly aiming to increase human judge consistency, some methods instead increase the number of reference translations available to automatic metrics.", "labels": [], "entities": [{"text": "consistency", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.8795726895332336}]}, {"text": "HTER () employs humans to post-edit each system output, creating individual human-targeted reference translations which are then used as the basis for computing the translation error rate.", "labels": [], "entities": [{"text": "HTER", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7905993461608887}]}, {"text": "HyTER, on the other hand, is a tool that facilitates creation of very large numbers of reference translations.", "labels": [], "entities": []}, {"text": "Although both approaches increase fairness compared to automatic metrics that use a single generic reference translation, even human post-editors will inevitably vary in the way they post-edit translations, and the process of creating even a single new reference trans-lation for each system output is often too resourceintensive to be used in practice.", "labels": [], "entities": []}, {"text": "With each method of human evaluation, a tradeoff exists between annotation time and the number of judgments collected.", "labels": [], "entities": []}, {"text": "At one end of the spectrum, the WMT human evaluation collects large numbers of quick judgments (approximately 3.5 minutes per screen, or 20 seconds per label).", "labels": [], "entities": [{"text": "WMT human evaluation", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7780518531799316}]}, {"text": "1 In contrast, HMEANT (Lo and Wu, 2011) uses a more time-consuming fine-grained semantic-role labeling analysis at a rate of approximately 10 sentences per hour (.", "labels": [], "entities": [{"text": "semantic-role labeling analysis", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.7470326125621796}]}, {"text": "But even with this detailed evaluation methodology, human judges are inconsistent (.", "labels": [], "entities": []}, {"text": "Although the trend appears to be toward more fine-grained human evaluation of MT output, it remains to be shown that this approach leads to more reliable system rankings -with a main reason to doubt this being that far fewer judgments will inevitably be possible.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9747394919395447}]}, {"text": "We take a counterapproach and aim to maintain the speed by which assessments are collected in shared task evaluations, but modify the evaluation set-up in two main ways: (1) we structure the judgments as monolingual tasks, reducing the cognitive load involved in assessing translation quality; and (2) we apply judge-intrinsic quality control and score standardization, to minimize noise introduced when crowd-sourcing is used to leverage numbers of assessments and to allow for the fact that human judges will vary in the way they assess translations.", "labels": [], "entities": []}, {"text": "Assessors are regarded as reliable as long as they demonstrate consistent judgments across a range of different quality translations.", "labels": [], "entities": []}, {"text": "We elicit direct estimates of quality from judges, as a quantitative estimate of the magnitude of each attribute of interest).", "labels": [], "entities": []}, {"text": "Since we no longer look for relative preference judgments, we revert back to the original fluency and adequacy criteria last used in WMT 2007 shared task evaluation.", "labels": [], "entities": [{"text": "WMT 2007 shared task evaluation", "start_pos": 133, "end_pos": 164, "type": "TASK", "confidence": 0.5696692585945129}]}, {"text": "Instead of fivepoint fluency/adequacy scales, however, we use a (100-point) continuous rating scale, as this facilitates more sophisticated statistical analyses of score distributions for judges, including workerintrinsic quality control for crowd-sourcing.", "labels": [], "entities": []}, {"text": "The latter does not depend on agreement with experts, and is made possible by the reduction in information-loss when a continuous scale is used.", "labels": [], "entities": []}, {"text": "In addition, translations are assessed in isolation from alternate system outputs, so that judgments collected are no longer relative to a set of five translations.", "labels": [], "entities": []}, {"text": "This has the added advantage of eliminating the criticism made of WMT evaluations that systems sometimes gain advantage from luckof-the-draw comparison with low quality output, and vice-versa).", "labels": [], "entities": [{"text": "WMT evaluations", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8576349020004272}, {"text": "luckof-the-draw", "start_pos": 125, "end_pos": 140, "type": "METRIC", "confidence": 0.9565361738204956}]}, {"text": "Based on our proposed evaluation methodology, human judges are able to work quickly, on average spending 18 and 13 seconds per single segment adequacy and fluency judgment, respectively.", "labels": [], "entities": []}, {"text": "Additionally, when sufficiently large volumes of such judgments are collected, mean scores reveal significant differences between systems.", "labels": [], "entities": []}, {"text": "Furthermore, since human evaluation takes the form of direct estimates instead of relative preference judgments, our evaluation introduces the possibility of large-scale longitudinal human evaluation.", "labels": [], "entities": []}, {"text": "We demonstrate the value of longitudinal evaluation by investigating the improvement made to stateof-the-art MT over a five year time period (between 2007 and 2012) using the best participating WMT shared task system output.", "labels": [], "entities": [{"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9779833555221558}, {"text": "WMT shared task", "start_pos": 194, "end_pos": 209, "type": "TASK", "confidence": 0.7261478702227274}]}, {"text": "Since it is likely that the test data used for shared tasks has varied in difficulty over this time period, we additionally propose a simple mechanism for scaling system scores relative to task difficulty.", "labels": [], "entities": []}, {"text": "Using the proposed methodology for measuring longitudinal change in MT, we conclude that, for the seven European language pairs we evaluate, MT has made an average 10% improvement over the past 5 years.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9864835143089294}, {"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9603005051612854}]}, {"text": "Our method uses non-expert monolingual judges via a crowd-sourcing portal, with fast turnaround and at relatively modest cost.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are several reasons why the assessment of MT quality is difficult.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9754898548126221}]}, {"text": "Ideally, each judge should be a native speaker of the target language, while at the same time being highly competent in the source language.", "labels": [], "entities": []}, {"text": "Genuinely bilingual people are rare, however.", "labels": [], "entities": []}, {"text": "As a result, judges are often people with demonstrated skills in the target language, and a working knowledge -often self-assessedof the source language.", "labels": [], "entities": []}, {"text": "Adding to the complexity is the discipline that is required: the task is cognitively difficult and time-consuming when done properly.", "labels": [], "entities": []}, {"text": "The judge is, in essence, being asked to decide if the supplied translations are what they would have generated if they were asked to do the same translation.", "labels": [], "entities": []}, {"text": "The assessment task itself is typically structured as follows: the source segment (a sentence or a phrase), plus five alternative translations and a \"reference\" translation are displayed.", "labels": [], "entities": []}, {"text": "The judge is then asked to assign a rank order to the five translations, from best to worst.", "labels": [], "entities": []}, {"text": "A set of pairwise preferences are then inferred, and used to generate system rankings, without any explicit formation of stand-alone system \"scores\".", "labels": [], "entities": []}, {"text": "This structure introduces the risk that judges will only compare translations against the reference translation.", "labels": [], "entities": []}, {"text": "Certainly, judges will vary in the degree they rely on the reference translation, which will in turn impact on inter-judge inconsistency.", "labels": [], "entities": []}, {"text": "For instance, even when expert judges do assessments, it is possible that they use the reference translation as a substitute for reading the source input, or do not read the source input at all.", "labels": [], "entities": []}, {"text": "And if crowd-sourcing is used, can we really expect high proportions of workers to put the additional effort into reading and understanding the source input when a reference translation (probably in their native language) is displayed?", "labels": [], "entities": []}, {"text": "In response to this potential variability in how annotators go about the assessment task, we trial assessments of adequacy in which the source input is not displayed to human judges.", "labels": [], "entities": []}, {"text": "We structure assessments as a monolingual task and pose them in such away that the focus is on comparing the meaning of reference translations and system outputs.", "labels": [], "entities": []}, {"text": "We therefore ask human judges to assess the degree to which the system output conveys the same meaning as the reference translation.", "labels": [], "entities": []}, {"text": "In this way, we focus the human judge indirectly on the question we wish to answer when assessing MT: does the translation convey the meaning of the source?", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9755280613899231}]}, {"text": "The fundamental assumption of this approach is that the reference translation accurately captures the meaning of the source; once that assumption is made, it is clear that the source is not required during the evaluation.", "labels": [], "entities": []}, {"text": "Benefits of this change are that the task is both easier to describe to novice judges, and easier to answer, and that it requires only monolingual speakers, opening up the evaluation to a vastly larger pool of genuinely qualified workers.", "labels": [], "entities": []}, {"text": "With this set-up in place for adequacy, we also re-introduce a fluency assessment.", "labels": [], "entities": []}, {"text": "Fluency ratings can be carried out without the presence of a reference translation, reducing any remnant bias towards reference translations in the evaluation setup.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9527311325073242}]}, {"text": "That is, we propose a judgment regime in which each task is presented as a two-item fluency and adequacy judgment, evaluated separately, and with adequacy restructured into a monolingual \"similarity of meaning\" task.", "labels": [], "entities": []}, {"text": "When fluency and adequacy were originally used for human evaluation, each rating used a 5-point adjective scale.", "labels": [], "entities": []}, {"text": "However, adjectival scale labels are problematic and ratings have been shown to be highly dependent on the exact wording of descriptors (.", "labels": [], "entities": []}, {"text": "Alexandrov (2010) provides a summary of the extensive problems associated with the use of adjectival scale labels, including bias resulting from positively-and negatively-worded items not being true opposites of one another, and items intended to have neutral intensity in fact proving to have specific conceptual meanings.", "labels": [], "entities": []}, {"text": "It is often the case, however, that the question could be restructured so that the rating scale no longer requires adjectival labels, by posing the question as a statement such as The text is fluent English and asking the human assessor to specify how strongly they agree or disagree with that statement.", "labels": [], "entities": []}, {"text": "The scale and labels can then beheld constant across experimental set-ups for all attributes evaluated -meaning that if the scale is still biased in someway it will be equally so across all set-ups.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Total quality control filtered workers and  assessments (F = fluency; A = adequacy).", "labels": [], "entities": [{"text": "F", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.9916638135910034}, {"text": "A = adequacy", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.8134841124216715}]}, {"text": " Table 3: Average human evaluation results for all language pairs; mean and standardized z scores are  computed in each case for n translations. In this table, and in Tables 4 and 5, all reported fluency and  adequacy values are in points relative to the 100-point assessment scale.", "labels": [], "entities": []}, {"text": " Table 4: Human evaluation of WMT 2007 and 2012 best systems for to-English language pairs. Mean  scores are computed in each case for n translations.", "labels": [], "entities": [{"text": "WMT 2007", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.6797591745853424}, {"text": "Mean", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9930704236030579}]}, {"text": " Table 5: Human evaluation of WMT 2007 and 2012 best systems for out of English language pairs.", "labels": [], "entities": [{"text": "WMT 2007", "start_pos": 30, "end_pos": 38, "type": "TASK", "confidence": 0.623480424284935}]}]}