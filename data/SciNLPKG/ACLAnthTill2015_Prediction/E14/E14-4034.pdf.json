{"title": [{"text": "Simple and Effective Approach for Consistent Training of Hierarchical Phrase-based Translation Models", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.5677217642466227}]}], "abstractContent": [{"text": "In this paper, we present a simple approach for consistent training of hierarchical phrase-based translation models.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.5686809718608856}]}, {"text": "In order to consistently train a translation model, we perform hierarchical phrase-based decoding on training data to find derivations between the source and target sentences.", "labels": [], "entities": []}, {"text": "This is done by synchronous parsing the given sentence pairs.", "labels": [], "entities": []}, {"text": "After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts.", "labels": [], "entities": []}, {"text": "We show the effectiveness of our procedure on the IWSLT German\u2192English and English\u2192French translation tasks.", "labels": [], "entities": [{"text": "IWSLT German\u2192English and English\u2192French translation tasks", "start_pos": 50, "end_pos": 107, "type": "TASK", "confidence": 0.6606045067310333}]}, {"text": "Our results show improvements of up to 1.6 points BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9996339082717896}]}], "introductionContent": [{"text": "In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ ( or fast align.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.6497590641180674}, {"text": "word alignment", "start_pos": 153, "end_pos": 167, "type": "TASK", "confidence": 0.7010523974895477}]}, {"text": "Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (.", "labels": [], "entities": []}, {"text": "However, this extraction method causes several problems.", "labels": [], "entities": []}, {"text": "First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not.", "labels": [], "entities": []}, {"text": "Further, during the extraction process, models employed in decoding are not considered.", "labels": [], "entities": []}, {"text": "For phrase-based translation, a successful approach addressing these issues is presented in.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8480083346366882}]}, {"text": "By applying a phrasebased decoder on the source sentences of the training data and constraining the translations to the corresponding target sentences, k-best segmentations are produced.", "labels": [], "entities": []}, {"text": "Then, the phrases used for these segmentations are extracted and counted.", "labels": [], "entities": []}, {"text": "Based on the counts, the translation model probabilities are recomputed.", "labels": [], "entities": []}, {"text": "To avoid over-fitting, leave-one-out is applied.", "labels": [], "entities": []}, {"text": "However, for hierarchical phrase-based translation an equivalent approach is still missing.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.5892055829366049}]}, {"text": "In this paper, we present a simple and effective approach for consistent reestimation of the translation model probabilities in a hierarchical phrase-based translation setup.", "labels": [], "entities": []}, {"text": "Using a heuristically extracted translation model as starting point, the training data are parsed bilingually.", "labels": [], "entities": []}, {"text": "From the resulting hypergraphs, we extract k-best derivations and the rules applied in each derivation.", "labels": [], "entities": []}, {"text": "This is done with a top-down k-best parsing algorithm.", "labels": [], "entities": []}, {"text": "Finally, the translation model probabilities are recomputed based on the counts of the extracted rules.", "labels": [], "entities": []}, {"text": "In our procedure, we employ leave-one-out to avoid over-fitting.", "labels": [], "entities": []}, {"text": "Further, we consider all models which are used in translation to ensure a consistent training.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9665085673332214}]}, {"text": "Experimental results are presented on the German\u2192English and English\u2192French IWSLT shared machine translation task (.", "labels": [], "entities": [{"text": "IWSLT shared machine translation task", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.7330984950065613}]}, {"text": "We are able to gain improvements of up to 1.6% BLEU absolute and 1.4% TER over a competitive baseline.", "labels": [], "entities": [{"text": "BLEU absolute", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.8514361679553986}, {"text": "TER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9996330738067627}]}, {"text": "On all tasks and test sets, the improvements are statistically significant with at least 99% confidence.", "labels": [], "entities": []}, {"text": "The paper is structured as follow.", "labels": [], "entities": []}, {"text": "First, we revise the state of the art hierarchical phrase-based extraction and translation process.", "labels": [], "entities": [{"text": "phrase-based extraction", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7108244746923447}, {"text": "translation", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.7291211485862732}]}, {"text": "In Section 3, we propose our training procedure.", "labels": [], "entities": []}, {"text": "Finally, experimental results are given in Section 4 and we conclude with Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results for the IWSLT 2013 German\u2192English task. The development set used for MERT is  marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the  baseline are printed in boldface.", "labels": [], "entities": [{"text": "IWSLT 2013 German\u2192English task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6312153736750284}, {"text": "MERT", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.7394453287124634}]}, {"text": " Table 3: Results for the IWSLT 2013 English\u2192French task. The development set used for MERT is  marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the  baseline are printed in boldface.", "labels": [], "entities": [{"text": "IWSLT 2013 English\u2192French task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6641205648581187}, {"text": "MERT", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.7081594467163086}]}]}