{"title": [{"text": "Automatic Creation of Arabic Named Entity Annotated Corpus Using Wikipedia", "labels": [], "entities": [{"text": "Automatic Creation of Arabic Named Entity Annotated Corpus", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.6048419587314129}]}], "abstractContent": [{"text": "In this paper we propose anew methodology to exploit Wikipedia features and structure to automatically develop an Arabic NE annotated corpus.", "labels": [], "entities": []}, {"text": "Each Wikipedia link is transformed into an NE type of the target article in order to produce the NE annotation.", "labels": [], "entities": []}, {"text": "Other Wikipedia features-namely redirects , anchor texts, and inter-language links-are used to tag additional NEs, which appear without links in Wikipedia texts.", "labels": [], "entities": []}, {"text": "Furthermore, we have developed a filtering algorithm to eliminate ambiguity when tagging candidate NEs.", "labels": [], "entities": [{"text": "tagging candidate NEs", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.808825155099233}]}, {"text": "Herein we also introduce a mechanism based on the high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation.", "labels": [], "entities": [{"text": "tagging NEs in Arabic text", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.8530160546302795}]}, {"text": "The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains.", "labels": [], "entities": [{"text": "NE tagger", "start_pos": 71, "end_pos": 80, "type": "TASK", "confidence": 0.8286746144294739}]}, {"text": "Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora.", "labels": [], "entities": [{"text": "NE tagger", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.9029927849769592}, {"text": "WDC", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9124612808227539}]}], "introductionContent": [{"text": "Supervised learning techniques are well known for their effectiveness to develop Named Entity Recognition (NER) taggers ().", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) taggers", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.7611131795815059}]}, {"text": "The main disadvantage of supervised learning is that it requires a large annotated corpus.", "labels": [], "entities": []}, {"text": "Although a substantial amount of annotated data is available for some languages, for other languages, including Arabic, more work is needed to enrich their linguistic resources.", "labels": [], "entities": []}, {"text": "In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort.", "labels": [], "entities": []}, {"text": "Therefore, current research focuses on approaches that require minimal human intervention to facilitate the process of moving the NE classifiers to new domains and to expand NE classes.", "labels": [], "entities": []}, {"text": "Semi-supervised and unsupervised learning approaches, along with the automatic creation of tagged corpora, are alternatives that avoid manually annotated data ().", "labels": [], "entities": []}, {"text": "The high coverage and rich informational structure of online encyclopedias can be exploited for the automatic creation of datasets.", "labels": [], "entities": []}, {"text": "For example, many researchers have investigated the use of Wikipedia's structure to classify Wikipedia articles and to transform links into NE annotations according to the link target type.", "labels": [], "entities": []}, {"text": "In this paper we present our approach to automatically derive a large NE annotated corpora from Arabic Wikipedia.", "labels": [], "entities": []}, {"text": "The key to our method lies in the exploitation of Wikipedia's concepts, specifically anchor texts 1 and redirects, to handle the rich morphology in Arabic, and thereby eliminate the need to perform any deep morphological analysis.", "labels": [], "entities": []}, {"text": "In addition, a capitalisation probability measure has been introduced and incorporated into the approach in order to replace the capitalisation feature that does not exist in the Arabic script.", "labels": [], "entities": []}, {"text": "This capitalisation measure has been utilised in order to filter ambiguous Arabic NE phrases during annotation process.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows: Section 2 illustrates structural information about Wikipedia.", "labels": [], "entities": []}, {"text": "Section 3 includes background information on NER, including recent work.", "labels": [], "entities": [{"text": "NER", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9234216809272766}]}, {"text": "Section 4 summarises the proposed methodology.", "labels": [], "entities": []}, {"text": "Sections 5, 6, and 7 describe the proposed algorithm in detail.", "labels": [], "entities": []}, {"text": "The experimental setup and the evaluation results are reported and discussed in Section 8.", "labels": [], "entities": []}, {"text": "Finally, the conclusion features comments regarding our future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to develop a Wikipedia document classifier, we used a set of 4,000 manually classified Wikipedia articles that are available free online . The set was manually classified using the ACE (2008) taxonomy and anew class (Product).", "labels": [], "entities": [{"text": "Wikipedia document classifier", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6329977711041769}, {"text": "ACE (2008) taxonomy", "start_pos": 190, "end_pos": 209, "type": "DATASET", "confidence": 0.9161462783813477}]}, {"text": "Therefore, there were eight coarse-grained categories in total: Facility, Geo-Political, Location, Organisation, Person, Vehicle, Weapon, and Product.", "labels": [], "entities": []}, {"text": "As our work adheres to the CoNLL definition, we mapped these classified Wikipedia articles into CoNLL NE types -namely person, location, organisation, miscellaneous, or other -based on the CoNLL 2003 annotation guidelines).", "labels": [], "entities": [{"text": "CoNLL definition", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9161938428878784}, {"text": "CoNLL 2003 annotation guidelines", "start_pos": 189, "end_pos": 221, "type": "DATASET", "confidence": 0.9494640082120895}]}, {"text": "To evaluate the quality of the methodology, we used WDC as training data to build an NER model.", "labels": [], "entities": [{"text": "WDC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.861815333366394}, {"text": "NER", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9289786219596863}]}, {"text": "Then we tested the resulting classifier on datasets from different domains.", "labels": [], "entities": []}, {"text": "For the evaluation purposes, we used three datasets: ANERcorp, NEWS, and TWEETS.", "labels": [], "entities": [{"text": "ANERcorp", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.904995322227478}, {"text": "NEWS", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.531694769859314}, {"text": "TWEETS", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.8821049332618713}]}, {"text": "ANERcorp is a news-wire domain dataset built and tagged especially for the NER task by.", "labels": [], "entities": [{"text": "ANERcorp", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7201051115989685}, {"text": "NER task", "start_pos": 75, "end_pos": 83, "type": "TASK", "confidence": 0.8535545766353607}]}, {"text": "It contains around 150k tokens and is available for free.", "labels": [], "entities": []}, {"text": "We tested our methodology on the ANERcorp test corpus because it is widely used in the literature for comparing with existing systems.", "labels": [], "entities": [{"text": "ANERcorp test corpus", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.9249658981959025}]}, {"text": "The NEWS dataset is also a news-wire domain dataset collected by", "labels": [], "entities": [{"text": "NEWS dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9806066155433655}, {"text": "news-wire domain dataset collected", "start_pos": 27, "end_pos": 61, "type": "DATASET", "confidence": 0.7744271159172058}]}], "tableCaptions": []}