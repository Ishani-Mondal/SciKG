{"title": [{"text": "Generalizing a Strongly Lexicalized Parser using Unlabeled Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Statistical parsers trained on labeled data suffer from sparsity, both grammatical and lexical.", "labels": [], "entities": []}, {"text": "For parsers based on strongly lexicalized grammar formalisms (such as CCG, which has complex lexical categories but simple combinatory rules), the problem of sparsity can be isolated to the lexicon.", "labels": [], "entities": []}, {"text": "In this paper, we show that semi-supervised Viterbi-EM can be used to extend the lexicon of a generative CCG parser.", "labels": [], "entities": []}, {"text": "By learning complex lexical entries for low-frequency and unseen words from unlabeled data, we obtain improvements over our supervised model for both in-domain (WSJ) and out-of-domain (ques-tions and Wikipedia) data.", "labels": [], "entities": []}, {"text": "Our learnt lexicons when used with a discriminative parser such as C&C also significantly improve its performance on unseen words.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important open problem in natural language parsing is to generalize supervised parsers, which are trained on hand-labeled data, using unlabeled data.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.6383178432782491}]}, {"text": "The problem arises because further handlabeled data in the amounts necessary to significantly improve supervised parsers are very unlikely to be made available.", "labels": [], "entities": []}, {"text": "Generalization is also necessary in order to achieve good performance on parsing in textual domains other than the domain of the available labeled data.", "labels": [], "entities": []}, {"text": "For example, parsers trained on Wall Street Journal (WSJ) data suffer a fall inaccuracy on other domains.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) data", "start_pos": 32, "end_pos": 62, "type": "DATASET", "confidence": 0.9162697877202716}]}, {"text": "In this paper, we use self-training to generalize the lexicon of a Combinatory Categorial Grammar (CCG) parser.", "labels": [], "entities": []}, {"text": "CCG is a strongly lexicalized formalism, in which every word is associated with a syntactic category (similar to an elementary syntactic structure) indicating its subcategorization potential.", "labels": [], "entities": []}, {"text": "Lexical entries are fine-grained and expressive, and contain a large amount of language-specific grammatical information.", "labels": [], "entities": []}, {"text": "For parsers based on strongly lexicalized formalisms, the problem of grammar generalization can be cast largely as a problem of lexical extension.", "labels": [], "entities": [{"text": "grammar generalization", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7499589920043945}]}, {"text": "The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data.", "labels": [], "entities": []}, {"text": "Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data.", "labels": [], "entities": []}, {"text": "Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words).", "labels": [], "entities": []}, {"text": "The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia).", "labels": [], "entities": []}, {"text": "Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser.", "labels": [], "entities": [{"text": "opendomain parsing", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.7005960643291473}]}, {"text": "We show that a simple self-training method, Viterbi-EM () when used to enhance the lexicon of a strongly-lexicalized parser can bean effective strategy for self-training and domain-adaptation.", "labels": [], "entities": []}, {"text": "Our learnt lexicons improve on the lexical category accuracy of two supervised CCG parsers ( and the parser, C&C) on within-domain (WSJ) and out-of-domain test sets (a question corpus and a Wikipedia corpus).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.983819842338562}, {"text": "Wikipedia corpus", "start_pos": 190, "end_pos": 206, "type": "DATASET", "confidence": 0.8902120590209961}]}, {"text": "In most prior work, when EM was initialized based on labeled data, its performance did not improve over the supervised model.", "labels": [], "entities": []}, {"text": "We found that in order for performance to improve, unlabeled data should be used only for parameters which are not well covered by the labeled data, while those that are well covered should remain fixed.", "labels": [], "entities": []}, {"text": "In an additional contribution, we compare two strategies for treating unseen words (a smoothingbased, and a part-of-speech back-off method) and find that a smoothing-based strategy for treating unseen words is more effective for semisupervised learning than part-of-speech back-off.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran our semi-supervised method using our parser with a smoothed lexicon (from \u00a74.1.1) as the initial model, on unlabeled data of different sizes/domains.", "labels": [], "entities": []}, {"text": "For comparison, we also ran experiments using a POS-backed off parser (the original LexCat model) as the initial model.", "labels": [], "entities": [{"text": "LexCat", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.9651416540145874}]}, {"text": "Viterbi-EM converged at 4-5 iterations.", "labels": [], "entities": []}, {"text": "We then parsed various test sets using the semi-supervised lexicons thus obtained.", "labels": [], "entities": []}, {"text": "In all experiments, the labeled data was scaled to match the size of the unlabeled data.", "labels": [], "entities": []}, {"text": "Thus, the scaling factor of labeled data was 10 for unlabeled data of 10M words, 20 for 20M words, etc.", "labels": [], "entities": []}, {"text": "We focused our evaluations on unseen and lowfrequency verbs, since verbs are the most important open-class lexical entries and the most ambiguous to learn from unlabeled data (approx. 600 categories, versus 150 for nouns).", "labels": [], "entities": []}, {"text": "We report lexical category accuracy in parses produced using our semi-supervised lexicon, since it is a direct measure of the effect of the lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9816650748252869}]}, {"text": "We discuss four . We use all these sections in order to get a reasonable token count of unseen verbs, which was not possible with Sec.", "labels": [], "entities": []}, {"text": "shows the performance of the smoothed supervised model (SUP) and the semi-supervised model (SEMISUP) on this testset.", "labels": [], "entities": [{"text": "SEMISUP", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.8192323446273804}]}, {"text": "There is a significant improvement in performance on unseen verbs, showing that the semi-supervised model learns good entries for unseen verbs over and above the smoothed entry in the supervised lexicon.", "labels": [], "entities": []}, {"text": "This results in an improvement in the overall lexical category accuracy of the parser on all words, and all verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.98504239320755}]}, {"text": "We also performed semi-supervised training using a supervised model that treated unseen words with a POS-backoff strategy SUP bkof f . We used the same settings of cut-off and the same scaling of labeled counts as before.", "labels": [], "entities": []}, {"text": "The supervised backed-off model performs somewhat better than the supervised smoothed model.", "labels": [], "entities": []}, {"text": "However, it did not improve as much as the smoothed one from unlabeled data.", "labels": [], "entities": []}, {"text": "Additionally, the overall accuracy of SEMISUP bkof f fell below the supervised level, in contrast to the smoothed model, where overall numbers improved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9996515512466431}, {"text": "SEMISUP bkof f", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.6166468461354574}]}, {"text": "This could indicate that the accuracy of a POS tagger on unseen words, especially verbs, maybe an important bottleneck in semi-supervised learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9989069700241089}]}, {"text": "Low-frequency verbs We also obtain improvements on verbs that are seen but with a low frequency in the labeled data.", "labels": [], "entities": []}, {"text": "We divided category accuracy, but a dependency evaluation is more relevant when comparing performance with parsers in other formalisms and does not have much utility here.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9780739545822144}]}, {"text": "verbs occurring in TEST-4SEC into different bins according to their occurrence frequency in the labeled data (bins of frequency 1-5, 6-10 and 11-20).", "labels": [], "entities": [{"text": "TEST-4SEC", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.8196980357170105}]}, {"text": "Semi-supervised training improves over the supervised baseline for all bins of low-frequency verbs.", "labels": [], "entities": []}, {"text": "Note that our cut-off frequency for using unlabeled data is 5, but there are improvements in the 6-10 and 11-20 bins as well, suggesting that learning better categories for rare words (below the cut-off) impacts the accuracy of words above the cut-off as well, by affecting the rest of the parse positively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9968348145484924}]}], "tableCaptions": [{"text": " Table 1: Lexical category accuracy on TEST-4SEC", "labels": [], "entities": [{"text": "Lexical category", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7777607142925262}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8387570381164551}, {"text": "TEST-4SEC", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.6258013248443604}]}, {"text": " Table 2: Seen but rare verbs, TEST-4SEC", "labels": [], "entities": [{"text": "TEST-4SEC", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.8642109632492065}]}, {"text": " Table 3: Lexical category accuracy in TEST-HOV.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9760316014289856}]}, {"text": " Table 4:  Out-of-domain:  Questions and  Wikipedia, *p<0.05, McNemar test", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.7328742742538452}]}]}