{"title": [{"text": "Discriminating Rhetorical Analogies in Social Media", "labels": [], "entities": [{"text": "Discriminating Rhetorical Analogies in Social Media", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7323223650455475}]}], "abstractContent": [{"text": "Analogies are considered to be one of the core concepts of human cognition and communication , and are very efficient at encoding complex information in a natural fashion.", "labels": [], "entities": []}, {"text": "However , computational approaches towards large-scale analysis of the semantics of analogies are hampered by the lack of suitable corpora with real-life example of analogies.", "labels": [], "entities": []}, {"text": "In this paper we therefore propose a workflow for discriminating and extracting natural-language analogy statements from the Web, focusing on analogies between locations mined from travel reports , blogs, and the Social Web.", "labels": [], "entities": [{"text": "extracting natural-language analogy statements", "start_pos": 69, "end_pos": 115, "type": "TASK", "confidence": 0.7894037663936615}]}, {"text": "For realizing this goal, we employ feature-rich supervised learning models which we extensively evaluate.", "labels": [], "entities": []}, {"text": "We also showcase a crowd-supported workflow for building a suitable Gold dataset used for this purpose.", "labels": [], "entities": [{"text": "Gold dataset", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.8667061030864716}]}, {"text": "The resulting system is able to successfully learn to identify analogies to a high degree of accuracy (F-Score 0.9) by using a high-dimensional subsequence feature space.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9974871873855591}, {"text": "F-Score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.9828255772590637}]}], "introductionContent": [{"text": "Analogies are one of the core concepts of human cognition, and it has been suggested that analogical inference is the \"thing that makes us smart\").", "labels": [], "entities": []}, {"text": "An analogy can be seen as a pattern of speech leading to a cognitive process that transfers some high-level meaning from one particular subject (often called the analogue or the source) to another subject, usually called the target.", "labels": [], "entities": [{"text": "a pattern of speech leading to a cognitive process that transfers some high-level meaning from one particular subject (often called the analogue or the source) to another subject, usually called the target", "start_pos": 26, "end_pos": 231, "type": "Description", "confidence": 0.8332694223948888}]}, {"text": "When using analogies, one emphasizes that the \"essence\" of source and target is similar, i.e. their most discriminating and prototypical processes and properties are perceived in a similar way.", "labels": [], "entities": []}, {"text": "The nature of analogies has been discussed and studied since the ancient Greeks, however computational approaches are still rather limited and in their infancy.", "labels": [], "entities": []}, {"text": "One reason for this is that text corpora containing analogies are crucial to study the syntactic and semantic patterns of analogies in order to make progress on automated understanding techniques.", "labels": [], "entities": []}, {"text": "For example, to learn about their distribution and the attribute-value pairs that are compared.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, no such corpus is freely available.", "labels": [], "entities": []}, {"text": "We will therefore in this paper present a method for creating such a corpus in an efficient fashion, and make our corpus available for further research efforts.", "labels": [], "entities": []}, {"text": "As an example, consider this brief statement: \"West Shinjuku (a Tokyo district) is like Lower Manhattan\" It allows readers who know New York, but not Tokyo, to infer some of the more significant properties of the unknown district (e.g., it is an important business district, hosts the headquarters of many companies, features many skyscrapers, etc.).", "labels": [], "entities": []}, {"text": "However, automatically understanding analogies is surprisingly hard due to the extensive domain knowledge required in order to perform analogical reasoning.", "labels": [], "entities": [{"text": "automatically understanding analogies", "start_pos": 9, "end_pos": 46, "type": "TASK", "confidence": 0.6378308236598969}, {"text": "analogical reasoning", "start_pos": 135, "end_pos": 155, "type": "TASK", "confidence": 0.7439016997814178}]}, {"text": "For example, an analogy repository containing such domain knowledge has to provide information on which attributes of source and target are generally considered comparable.", "labels": [], "entities": []}, {"text": "In contrast to Linked Open Data or typical ontologies, such analogical knowledge is consensual, i.e. there is no undisputable truth to analogical information, but a statement can be considered \"good\" analogical knowledge if its' semantics are perceived similarly by enough people.", "labels": [], "entities": []}, {"text": "For example, while many properties of West Shinjuku and Lower Manhattan are dissimilar, nonetheless most people will immediately recognize dominant similarities.", "labels": [], "entities": []}, {"text": "In order to build an analogy repositories, a large number of actual analogy statements reflecting the diversity of people's opinions are required for analysis.", "labels": [], "entities": []}, {"text": "In this paper, we make a start on this task by proposing a workflow for reliably extracting such statements by using feature-rich supervised learning models, and demonstrate its effectiveness for analogies between different places.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are as follows: \uf0b7 First, we build a suitable Gold corpus for training and testing supervised learning models, focusing on analogies between places.", "labels": [], "entities": []}, {"text": "This corpus will be based upon content mined from search engines and social media.", "labels": [], "entities": []}, {"text": "\uf0b7 We show the effectiveness, but also the challenges of crowd-sourcing as a technique for screening and refining potential Gold corpus documents.", "labels": [], "entities": [{"text": "Gold corpus documents", "start_pos": 123, "end_pos": 144, "type": "DATASET", "confidence": 0.6910920639832815}]}, {"text": "This process results in multi-sentence text snippets containing an analogy extracted from these documents.", "labels": [], "entities": []}, {"text": "\uf0b7 We design and evaluate supervised learning models with rich feature sets to recognize analogy statements automatically, allowing us to substitute crowd-sourcing with automated techniques for further expanding the corpus.", "labels": [], "entities": []}, {"text": "\uf0b7 We extensively evaluate our models, and discuss their strengths and shortcomings.", "labels": [], "entities": []}], "datasetContent": [{"text": "As the goal of this paper is to supply the tools for creating a large corpus of analogies from the Web, we require a reliable mechanism for automatically classifying if a text snippet contains an analogy or not.", "labels": [], "entities": []}, {"text": "Such classification requires a Gold dataset which we construct in this section and which we make available to the community for download . As we expect the number of analogies in a completely random collection of web documents to be extremely low, we first start by collecting a set of web documents that are likely to contain an analogy by applying some easy-to-implement but rather coarse techniques as follows: In order to obtain a varied set of text snippets (i.e. short excerpts from larger Web documents), we first used a Web search engine (Google Search API) with simple Hearst-like patterns for crawling potentially relevant websites.", "labels": [], "entities": []}, {"text": "These patterns were selected manually based on analysis of sample Web data by three experts.", "labels": [], "entities": []}, {"text": "In contrast to other approaches relying on extraction patters, e.g. or (), our patterns are semi-open, e.g. \"# * similar to * as\", where # is replaced by one of 19 major cities we used for corpus extraction.", "labels": [], "entities": [{"text": "corpus extraction", "start_pos": 189, "end_pos": 206, "type": "TASK", "confidence": 0.7407967150211334}]}, {"text": "* is a wildcard, therefore only one entity of the analogy is fixed by the pattern.", "labels": [], "entities": []}, {"text": "Each pattern is created by combining one base part (in this case, \"# * similar to *\") with an extension part (\"as\").", "labels": [], "entities": []}, {"text": "We used 17 different base parts, and 14 different extensions, resulting in 238 different extraction patterns before inserting the city names.", "labels": [], "entities": []}, {"text": "Using Web search, we initially obtained 109,121 search results and used them to crawl 22,360 documents, for which we extracted the text snippets surrounding the occurrence of the pattern (2 preceding and 2 succeeding sentences).", "labels": [], "entities": []}, {"text": "The intention of our open Hearst-like patterns is to obtain a wide variety of text snippets which are not limited to simple analogy cases, so most snippets obtained will actually not be analogies at all.", "labels": [], "entities": []}, {"text": "Therefore, additional filtering is required to find those which do actually contain an analogy between places.", "labels": [], "entities": []}, {"text": "Unlike e.g. where patterns of the form , with X and Y two given entities, are used, we chose a more general approach and filtered out all snippets not containing at least two different locations (and hence no place analogy, locations provided by Stanford CoreNLP NER tagger), which left 14,141 snippets.", "labels": [], "entities": []}, {"text": "Since we lacked the means to manually classify all of these snippets as a Gold set, we randomly selected a subset of 8000 snippets, and performed a crowd-sourcing based filtering to detect potential analogies, as described in the following.", "labels": [], "entities": []}, {"text": "Our complete Gold dataset of 3,918 text snippets shows a ratio of positive to negative examples of roughly 1:8.", "labels": [], "entities": [{"text": "Gold dataset of 3,918 text snippets", "start_pos": 13, "end_pos": 48, "type": "DATASET", "confidence": 0.9439395666122437}]}, {"text": "For training and evaluation, we perform four stratified random selections on the Gold set to obtain 4 training sets with 2/3 of the overall size (2,611), and respective test sets with 1/3 size.", "labels": [], "entities": [{"text": "Gold set", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.8924999237060547}]}, {"text": "In each set, the original ratio between positive example (analogies) and negative examples (not analogies) is retained.", "labels": [], "entities": []}, {"text": "We prefer this approach over n-fold cross-validation as some of our models are expensive to train.", "labels": [], "entities": []}, {"text": "All snippets in the Gold set consist of 5 sentences, with 105 words per snippet on average.", "labels": [], "entities": [{"text": "Gold set", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9742856025695801}]}, {"text": "This average does not significantly vary between positively and negatively classified snippets (94 vs. 106).", "labels": [], "entities": []}, {"text": "The overall vocabulary contains 31,878 unique words, with 6,960 words in the positive and 30,234 in the negative subset.", "labels": [], "entities": []}, {"text": "5,316 of these words are shared between both sets (76% of those in the Gold set).", "labels": [], "entities": [{"text": "Gold set", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9109332263469696}]}, {"text": "This observation implies that the language in our snippets is highly varied and far from saturated (for the significantly smaller positive set, 12.84 new words per snippet are added to the vocabulary on average, while for the larger negative subset, this value only drops to 8.95).", "labels": [], "entities": []}, {"text": "This situation looks similar for locations, which play a central role in this classification task: the overall number of different locations encountered in all snippets is 2,631, with 0.86 new locations per snippet in the positive set and 0.73 in the negative set.", "labels": [], "entities": []}, {"text": "On average, there are 3.18 locations mentioned in a given snippet, again with no significant differences in the positive and negative subset (3.67 vs. 3.10).", "labels": [], "entities": []}, {"text": "Please refer to for exhaustive statistics.", "labels": [], "entities": []}, {"text": "In the following we evaluate the effectiveness of our analogy classifiers and models.", "labels": [], "entities": []}, {"text": "We primarily rely on the informedness measure for quantifying performance.", "labels": [], "entities": []}, {"text": "In contrast to using only precision, recall, or F-Measure, it respects all error types, false positives (FP) and false negatives (FN), but also true positives (TP) and true negatives (TN), making it a fair and unbiased measure for classification.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9991942048072815}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.998788058757782}, {"text": "F-Measure", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9906766414642334}, {"text": "error types, false positives (FP) and false negatives (FN)", "start_pos": 75, "end_pos": 133, "type": "METRIC", "confidence": 0.7313408872910908}]}, {"text": "Furthermore, it compensates biased class distributions in datasets, e.g. as in our dataset the ratio of positive to negative snippets is 1:8, even an \"always no\" classifier has a correctness of 85%, but will have an informedness of 0.", "labels": [], "entities": []}, {"text": "Informedness is given by: In, we provide the average informedness, the percentage of correctly classified snippets, Fmeasure, precision, recall, and inverse recall (true negative rate) for all experiments.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9986234903335571}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9989416003227234}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9989839196205139}, {"text": "inverse recall (true negative rate)", "start_pos": 149, "end_pos": 184, "type": "METRIC", "confidence": 0.7686523965426854}]}, {"text": "A discussion of these results follows in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Characteristics of Gold Data", "labels": [], "entities": [{"text": "Gold Data", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.807859480381012}]}, {"text": " Table 2: Classifier Result Comparison with respect to the Gold classification", "labels": [], "entities": [{"text": "Classifier Result", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6948387920856476}]}]}