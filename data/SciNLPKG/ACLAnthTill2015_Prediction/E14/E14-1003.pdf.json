{"title": [{"text": "Minimum Translation Modeling with Recurrent Neural Networks", "labels": [], "entities": [{"text": "Minimum Translation Modeling", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9765776594479879}]}], "abstractContent": [{"text": "We introduce recurrent neural network-based Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts.", "labels": [], "entities": [{"text": "recurrent neural network-based Minimum Translation Unit (MTU)", "start_pos": 13, "end_pos": 74, "type": "TASK", "confidence": 0.6510649787055122}]}, {"text": "Traditional back-off n-gram models suffer under the sparse nature of MTUs which makes estimation of high-order sequence models challenging.", "labels": [], "entities": []}, {"text": "We tackle the sparsity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and target words.", "labels": [], "entities": [{"text": "MTUs", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9704779982566833}]}, {"text": "Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 41, "end_pos": 85, "type": "TASK", "confidence": 0.5768024772405624}, {"text": "WMT 2012 French-English data", "start_pos": 104, "end_pos": 132, "type": "DATASET", "confidence": 0.9481270611286163}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9959934949874878}, {"text": "BLEU", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.985847532749176}]}], "introductionContent": [{"text": "Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.5957234054803848}]}, {"text": "Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based) as well as phrase-based () models by explicitly modeling relationships between phrases.", "labels": [], "entities": [{"text": "Minimum Translation Units (MTUs)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.6403024047613144}]}, {"text": "MTU models have been traditionally estimated using standard back-off n-gram techniques), similar to wordbased language models ( \u00a72).", "labels": [], "entities": [{"text": "MTU", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9043173789978027}]}, {"text": "However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (; bilingual units are much sparser than words and are therefore even harder to estimate.", "labels": [], "entities": [{"text": "estimation", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9642168879508972}]}, {"text": "Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (.", "labels": [], "entities": []}, {"text": "Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (, as well as translation modeling).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7223383486270905}, {"text": "translation modeling", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.9743815064430237}]}, {"text": "These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units.", "labels": [], "entities": []}, {"text": "Similar words are grouped in the same sub-space rather than being treated as separate entities.", "labels": [], "entities": []}, {"text": "Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier.", "labels": [], "entities": []}, {"text": "However, feed-forward networks do not directly address the limited context issue either, since predictions are based on a fixed-size context, similar to back-off n-gram models.", "labels": [], "entities": []}, {"text": "We therefore focus in this paper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies.", "labels": [], "entities": []}, {"text": "Recurrent architectures have recently advanced the state of the art in language modeling () outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7237497568130493}]}, {"text": "Recent work has also shown successful applications to machine translation  sues regarding data sparsity and limited context sizes by leveraging continuous representations and the unbounded history of the recurrent architecture.", "labels": [], "entities": [{"text": "machine translation  sues", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.8743305603663126}]}, {"text": "Our first approach frames the problem as a sequence modeling task over minimal units ( \u00a73).", "labels": [], "entities": [{"text": "sequence modeling task", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7392534911632538}]}, {"text": "The second model improves over the first by modeling an MTU as a bag-of-words, thereby allowing us to learn representations over sub-structures of minimal units that are shared across MTUs ( \u00a74).", "labels": [], "entities": []}, {"text": "Our models significantly outperform the traditional back-off n-gram based approach and we show that they act complementary to a very strong recurrent neural network-based language model based solely on target words ( \u00a75).", "labels": [], "entities": []}, {"text": "ples obtained by an algorithm similar to phraseextraction (.", "labels": [], "entities": []}, {"text": "Modeling minimal units has two advantages over considering larger phrase pairs that are effectively composed of MTUs: First, minimal units result in a unique partitioning of a sentence pair.", "labels": [], "entities": []}, {"text": "This has the advantage that we avoid modeling spurious derivations, that is, multiple derivations generating the same sentence pair.", "labels": [], "entities": []}, {"text": "Second, minimal units result in smaller models with a smoother distribution than models based on composed units (.", "labels": [], "entities": []}, {"text": "Sentence pairs can be generated in multiple orders, such as left-to-right or right-to-left, either in source or target order.", "labels": [], "entities": []}, {"text": "For example, the source left-to-right order of the sentence pair in is simply M1, M2, M3, M4, M5, while the target left-to-right order is M3, M4, M5, M1, M2.", "labels": [], "entities": []}, {"text": "We deal with inserted or deleted words similar to: The source side null token of an inserted target phrase is placed next to the last source word aligned to the closest preceding nonnull aligned target phrase; a similar rule is applied to null tokens on the target side.", "labels": [], "entities": []}, {"text": "For example, in we place M4 straight after M3 because \"the\", the aligned target phrase, is after \"held\", the previous non-null aligned target phrase.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness of both the atomic MTU RNN model ( \u00a73) and the simplified bag-ofwords MTU RNN model ( \u00a74) in an n-best rescoring setting, comparing against a trigram back-off MTU model as well as the phrasal decoder 1-best output which we denote as the baseline.", "labels": [], "entities": []}, {"text": "We experiment with an in-house phrase-based system similar to, scoring translations by a set of common features including maximum likelihood estimates of source given target mappings p M LE (e|f ) and vice versa p M LE (f |e), as well as lexical weighting estimates p LW (e|f ) and p LW (f |e), word and phrase-penalties, a linear distortion feature and a lexicalized reordering feature.", "labels": [], "entities": []}, {"text": "The baseline includes a standard modified Kneser-Ney wordbased language model trained on the target-side of the parallel corpora described below.", "labels": [], "entities": []}, {"text": "Log-linear weights are estimated with minimum error rate training (MERT;.", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 38, "end_pos": 65, "type": "METRIC", "confidence": 0.804585337638855}, {"text": "MERT", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.876339316368103}]}, {"text": "The 1-best output by the phrase-based decoder is the baseline accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.767359733581543}]}, {"text": "As a second baseline we experiment with a trigram back-off MTU model trained on all extracted MTUs, denoted as n-gram MTU.", "labels": [], "entities": []}, {"text": "The trigram MTU model is estimated with the same modified Kneser-Ney framework as the target side language model.", "labels": [], "entities": [{"text": "MTU", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8427934646606445}]}, {"text": "All MTU models are trained in target left-to-right MTU order which performed well in initial experiments.", "labels": [], "entities": [{"text": "MTU", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9331971406936646}]}, {"text": "We test our approach on two different data sets.", "labels": [], "entities": []}, {"text": "First, we train a German to English system based on the data of the WMT 2006 shared task ().", "labels": [], "entities": [{"text": "WMT 2006 shared task", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.8900889158248901}]}, {"text": "The parallel corpus includes about 35M words of parliamentary proceedings for training, a development set and two test sets with 2000 sentences each.", "labels": [], "entities": []}, {"text": "Second, we experiment with a French to English system based on 102M words of training data from the WMT 2012 campaign.", "labels": [], "entities": [{"text": "WMT 2012 campaign", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.8171460628509521}]}, {"text": "The majority of the training data set is parliamentary proceedings except for about 5m words which are newswire; all MTU models are trained on the newswire subset since we found similar accuracy to using all data in initial experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.997345507144928}]}, {"text": "We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set containing between 2034 to 3003 sentences.", "labels": [], "entities": [{"text": "newswire domain test sets", "start_pos": 20, "end_pos": 45, "type": "DATASET", "confidence": 0.8180725425481796}]}, {"text": "Log-linear weights are estimated on the 2009 data set com-prising 2525 sentences.", "labels": [], "entities": [{"text": "2009 data set com-prising 2525 sentences", "start_pos": 40, "end_pos": 80, "type": "DATASET", "confidence": 0.8993973533312479}]}, {"text": "We evaluate all systems in a single reference BLEU setting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9971451163291931}]}, {"text": "We rescore the 1000-best output of the baseline phrase-based decoder by either the trigram back-off MTU model or the RNN models.", "labels": [], "entities": []}, {"text": "The baseline accuracy is obtained by choosing the 1-best decoder output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9484332203865051}]}, {"text": "We reestimate the log-linear weights for rescoring by running a further iteration of MERT with the additional feature values; we initialize the rescoring feature weight to zero and try 20 random restarts.", "labels": [], "entities": []}, {"text": "At test time we use the new set of log-linear weights to rescore the test set n-best list.", "labels": [], "entities": []}, {"text": "We trained the recurrent neural network models on between 88% and 93% of each data set and used the remainder as validation data.", "labels": [], "entities": []}, {"text": "The vocabulary of the atomic MTU RNN model is comprised of all MTU types which were observed more than once in the training data.", "labels": [], "entities": [{"text": "MTU RNN", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.8010761439800262}]}, {"text": "Similarly, we modeled all non-singleton words for the bag-of-words MTU RNN model.", "labels": [], "entities": [{"text": "MTU RNN model", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.6776524384816488}]}, {"text": "We obtain classes for words or MTUs using aversion of Brown-Clustering with an additional regularization term to optimize the runtime of the language model (.", "labels": [], "entities": []}, {"text": "Direct connections use features over unigrams, bigrams and trigrams of words or MTUs, depending on the model.", "labels": [], "entities": []}, {"text": "Features are hashed to a table with at most 500 million values following.", "labels": [], "entities": []}, {"text": "We use the standard settings for the model with the default learning rate \u03b1 = 0.1 that decays exponentially if the validation set entropy does not decrease.", "labels": [], "entities": []}, {"text": "Back propagation through time computes error gradients over the past twenty time steps.", "labels": [], "entities": []}, {"text": "Training is stopped after 20 epochs or when the validation entropy does not decrease over two epochs.", "labels": [], "entities": [{"text": "validation entropy", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8601637184619904}]}, {"text": "Throughout, we use a hidden layer size of 100 which provided a good trade-off between time and accuracy in initial experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9974609613418579}]}], "tableCaptions": [{"text": " Table 1: Token and type counts for both source  and target words as well as MTUs based on the  WMT 2006 German to English data set (cf.  \u00a75).", "labels": [], "entities": [{"text": "WMT 2006 German to English data set", "start_pos": 96, "end_pos": 131, "type": "DATASET", "confidence": 0.9478066052709307}]}, {"text": " Table 3: French to English BLEU results for the decoder 1-best output (Baseline) compared to various  MTU models (cf. Table 2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9945907592773438}, {"text": "Baseline)", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.8483453094959259}]}]}