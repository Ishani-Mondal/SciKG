{"title": [{"text": "Information Structure Prediction for Visual-world Referring Expressions", "labels": [], "entities": [{"text": "Information Structure Prediction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.640694260597229}, {"text": "Visual-world Referring Expressions", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.6694809794425964}]}], "abstractContent": [{"text": "We investigate the order of mention for objects in relational descriptions in visual scenes.", "labels": [], "entities": []}, {"text": "Existing work in the visual domain focuses on content selection for text generation and relies primarily on templates to generate surface realizations from underlying content choices.", "labels": [], "entities": [{"text": "text generation", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7324595004320145}]}, {"text": "In contrast, we seek to clarify the influence of visual perception on the linguistic form (as opposed to the content) of descriptions, mod-eling the variation in and constraints on the surface orderings in a description.", "labels": [], "entities": []}, {"text": "We find previously-unknown effects of the visual characteristics of objects; specifically, when a relational description involves a visually salient object, that object is more likely to be mentioned first.", "labels": [], "entities": []}, {"text": "We conduct a detailed analysis of these patterns using logistic regression, and also train and evaluate a classifier.", "labels": [], "entities": []}, {"text": "Our methods yield significant improvement in classification accuracy over a naive baseline.", "labels": [], "entities": [{"text": "classification", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.9573932886123657}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9660612344741821}]}], "introductionContent": [{"text": "Visual-world referring expression generation (REG) is the task of instructing a listener how to find an object (the target) in a visual scene.", "labels": [], "entities": [{"text": "Visual-world referring expression generation (REG) is the task of instructing a listener how to find an object (the target) in a visual scene", "start_pos": 0, "end_pos": 141, "type": "Description", "confidence": 0.8031278473359568}]}, {"text": "In complicated scenes, people often produce relational descriptions, in which the target object is described relative to another (a landmark).", "labels": [], "entities": []}, {"text": "While existing REG systems can generate relational descriptions, they tend to focus on content selection (that is, choosing an appropriate set of landmarks for each object).", "labels": [], "entities": []}, {"text": "Surface realization (turning the selected content into a string of words) is handled by simple heuristics, such as sets of templates.", "labels": [], "entities": [{"text": "Surface realization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7297231107950211}]}, {"text": "Complex descriptions, however, have a non-trivial information structure-objects are not mentioned in an arbitrary order.", "labels": [], "entities": []}, {"text": "Numerous studies in non-visual domains show that English speakers favor constructions that place familiar (given) information before unfamiliar (new).", "labels": [], "entities": []}, {"text": "We show that this pattern also holds for visualworld referring expressions (REs), and moreover, that objects with sufficient visual prominence are treated as given.", "labels": [], "entities": [{"text": "visualworld referring expressions (REs)", "start_pos": 41, "end_pos": 80, "type": "TASK", "confidence": 0.7310656855503718}]}, {"text": "Thus, we argue that the concept of salience used in surface realization should incorporate metrics from visual perception.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7787838876247406}]}, {"text": "In this study, we create a model of information ordering in complex relational descriptions.", "labels": [], "entities": []}, {"text": "Using a discriminative classifier, we learn to predict the information structuring strategies used in our corpus.", "labels": [], "entities": []}, {"text": "We compare these strategies to the typical given/new pattern of English discourse.", "labels": [], "entities": []}, {"text": "Experiments on a corpus of descriptions of cartoon people in the childrens' book \"Where's Wally\", corpus described in (, show that our approach significantly outperforms a naive baseline, improving especially on prediction of non-canonical orderings.", "labels": [], "entities": []}, {"text": "This study has three main contributions.", "labels": [], "entities": []}, {"text": "First, it demonstrates that humans use sophisticated information ordering strategies for REG, and therefore that the template strategies used in previous work do not adequately model human production.", "labels": [], "entities": [{"text": "REG", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9747816324234009}]}, {"text": "Second, it makes a practical proposal for an improved model which is capable of predicting these orderings; while this model is not a full-scale surface realizer, we view it as an important intermediate step towards one.", "labels": [], "entities": []}, {"text": "Finally, it makes a theoretical contribution: By linking the information structures observed in the data to the existing re-search on salience and information structure, we show that visually prominent objects are treated as part of common ground despite the lack of previous mention.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a collection of referring expressions elicited on Mechanical Turk, previously described in (.", "labels": [], "entities": []}, {"text": "The dataset contains descriptions of targets in 11 images from the childrens' book Where's Wally 4; in each image, 16 people were designated as targets.", "labels": [], "entities": []}, {"text": "Each participant saw each scene only once.", "labels": [], "entities": []}, {"text": "An example scene is shown in.", "labels": [], "entities": []}, {"text": "The participant was instructed to type a description of the person in the red box so that another person viewing the same scene (but without the box) would be able to find them; to make sure this was clear, as part of the study instructions, they completed a few visual searches based on text descriptions.", "labels": [], "entities": []}, {"text": "The image in the figure also contains a black box (not part of the initial stimulus), which the annotator has added to designate the landmark object \"burning hut\").", "labels": [], "entities": []}, {"text": "The dataset contains 1672 descriptions, contributed by 152 different participants (152 participants \u00d7 11 scenes).", "labels": [], "entities": []}, {"text": "The REs are annotated for visual and linguistic content.", "labels": [], "entities": []}, {"text": "The annotation scheme indicates which substrings of the RE describe the target object, another mentioned objector an image region.", "labels": [], "entities": []}, {"text": "References to parts or attributes of objects are not treated as separate objects; \"a man holding torch and sword\" in is a single object.", "labels": [], "entities": []}, {"text": "The mentioned objects are linked to bounding boxes (or for very large objects, bounding polygons) in the image.", "labels": [], "entities": []}, {"text": "For each mention of a non-target object, the annotation indicates whether it is part of a relational description of a specific anchor, and if so which; if it is not, it receives an ESTABLISH tag.", "labels": [], "entities": []}, {"text": "These annotations are used to determine the ordering strategies used in this study.", "labels": [], "entities": []}, {"text": "In some cases, the linkage between objects is implicit: ...there are 4 men smoking...", "labels": [], "entities": []}, {"text": "the man you are looking for is the one [=of the 4 men] leaning against a crate In the above RE, 4 men is first introduced in an ESTABLISH construction.", "labels": [], "entities": [{"text": "RE", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.7161217927932739}]}, {"text": "The word \"one\" refers implicitly to part of this set of men, so the annotator marks a relational link from \"4 men\" to \"one\".", "labels": [], "entities": []}, {"text": "In our analysis in this study, we treat the entity \"crates\" as anchored to the target (\"one\") on the basis of this implicit link (so that this is an instance of the PRECEDE-ESTABLISH pattern), but we do not treat the hidden link itself as a mention or try to predict its nonexistent \"position\" in the string.", "labels": [], "entities": []}, {"text": "We holdout three images (vikings, airport, blackandwhite) as a development set.", "labels": [], "entities": []}, {"text": "In test, we exclude these 3 documents and use the other 8 for evaluation.", "labels": [], "entities": []}, {"text": "In both development and test, we conduct experiments by crossvalidation, testing on one document at a time and training on the other ten.", "labels": [], "entities": []}, {"text": "We report two trivial baseline strategies, all landmarks following (the best baseline for overall accuracy) and all landmarks preceding (the best baseline for predicting the direction, but not as good overall because the PRECEDE predictions are split between ESTABLISH and not ESTABLISH).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9898345470428467}]}, {"text": "Our preliminary analysis shows that regions have a strong tendency to precede their anchors, so we also report results fora baseline using this pattern (regions preceding, everything else following).", "labels": [], "entities": []}, {"text": "We believe this baseline pattern is the one which would be learned as a template by previous systems like Di, since this system can learn relationships between broad types of entities (target, landmark and region) but does not use visual features of the actual entities in the scene to make any finer distinctions.", "labels": [], "entities": []}, {"text": "We also provide two \"inter-subject\" oracle scores intended to estimate the performance ceiling imposed by human variability.", "labels": [], "entities": []}, {"text": "This oracle assigns each anchor/landmark pair the direction and ESTABLISH status assigned by the majority of speakers who mentioned that pair.", "labels": [], "entities": [{"text": "direction", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9581367373466492}, {"text": "ESTABLISH status", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.9836804270744324}]}, {"text": "The \"multiple mentions\" estimate of agreement is the one mentioned in Section 5; it was based only on pairs mentioned by multiple speakers.", "labels": [], "entities": []}, {"text": "The \"all\" estimate is based on all objects; it is higher because, for pairs mentioned by only one speaker, it is by definition perfect.", "labels": [], "entities": []}, {"text": "Our system's use of the number of descendants feature is not captured by this oracle-these features capture information about a particular speaker's content plan beyond their decision to mention a particular pair-but we suspect that the oracle's performance will nonetheless be hard for any practical system to beat.", "labels": [], "entities": []}, {"text": "We report gross accuracy (correctly predicting both DIR and ESTABLISH) for relational pairs (Table 5), and also decompose by direction and ESTABLISH status ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993317723274231}]}, {"text": "The baseline correctly predicts 43% of pairs, implying that this pattern (regions precede, landmarks follow) covers a bit under half the data.", "labels": [], "entities": []}, {"text": "The classifier improves this to 52%.", "labels": [], "entities": []}, {"text": "When predicting the direction alone, the best baseline (PRECEDE) scores 42%; the classifier scores 57%.", "labels": [], "entities": [{"text": "PRECEDE)", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9572487473487854}]}, {"text": "All system scores are significantly better than the baseline (sign test on pairs, p < 0.01).", "labels": [], "entities": []}, {"text": "In predictions of ESTABLISH tags, our result is a 60% f-score, which is indistinguishable from the lower bound ysis on the training examples.", "labels": [], "entities": [{"text": "f-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9921342134475708}]}, {"text": "Data size does appear to matter; training on 8 documents at a time and testing on 3 yields poorer results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Regression coefficients, standard deviations and Z-scores from one-vs-all logistic regressions  with direction/ESTABLISH status as output variable. Only effects significant at p < .05 level are shown;  other effects are displayed as -.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9911386370658875}, {"text": "ESTABLISH status", "start_pos": 121, "end_pos": 137, "type": "METRIC", "confidence": 0.915688693523407}]}, {"text": " Table 3: Feature templates and number of instan- tiations in our discriminative system.", "labels": [], "entities": []}, {"text": " Table 4: Direction scores (p/r/f per direction and total pair directions correctly predicted) in 2382 pairs  in test set. Overall accuracy differences between system and baselines are significant (p < .01).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9989676475524902}]}, {"text": " Table 5: Gross accuracy (%) for 2382 test pairs.", "labels": [], "entities": [{"text": "Gross", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9336844682693481}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9080820083618164}]}, {"text": " Table 6: ESTABLISH scores (p/r/f for EST=TRUE)  in 2382 pairs in test set.", "labels": [], "entities": [{"text": "ESTABLISH", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8775596022605896}, {"text": "TRUE", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8231876492500305}]}]}