{"title": [{"text": "Fast and Accurate Unlexicalized Parsing via Structural Annotations", "labels": [], "entities": [{"text": "Accurate", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.991172730922699}]}], "abstractContent": [{"text": "We suggest anew annotation scheme for unlexicalized PCFGs that is inspired by formal language theory and only depends on the structure of the parse trees.", "labels": [], "entities": []}, {"text": "We evaluate this scheme on the T\u00fcBa-D/Z treebank w.r.t. several metrics and show that it improves both parsing accuracy and parsing speed considerably.", "labels": [], "entities": [{"text": "T\u00fcBa-D/Z treebank w.r.t.", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.9125869989395141}, {"text": "parsing", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.9792999625205994}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9474506974220276}, {"text": "parsing", "start_pos": 124, "end_pos": 131, "type": "TASK", "confidence": 0.9689224362373352}, {"text": "speed", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.6562525033950806}]}, {"text": "We also show that our strategy can be fruitfully combined with known ones like parent annotation to achieve accuracies of over 90% labeled F 1 and leaf-ancestor score.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9961989521980286}, {"text": "labeled F 1", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.8031403223673502}]}, {"text": "Despite increasing the size of the grammar, our annotation allows for parsing more than twice as fast as the PCFG baseline.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9642935991287231}, {"text": "PCFG baseline", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9356749355792999}]}], "introductionContent": [{"text": "As shown by, unlexicalized PCFGs can achieve high parsing accuracies when training trees are annotated with additional information.", "labels": [], "entities": []}, {"text": "An annotation basically amounts to splitting each nonterminal into several subcategories, which can even be derived automatically (.", "labels": [], "entities": []}, {"text": "Currently used annotation strategies, e.g. parent annotation or selectively splitting special nonterminals (e.g. marking relative clauses) as in), are mostly linguistically motivated (with the exception of the above mentioned automatic approach).", "labels": [], "entities": []}, {"text": "In this paper we study new heuristics motivated by formal language theory for improving the parsing accuracy of unlexicalized PCFGs by means of refining the nonterminals of the grammar: One heuristic splits a nonterminal X into a family of nonterminals (X d ) d\u2208D based on the notion of the dimension (also Horton-Strahler number) of a tree).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8624452948570251}]}, {"text": "The dimension of a rooted tree t is defined as the height of the highest perfect binary tree we can obtain from t by pruning subtrees and contracting edges.", "labels": [], "entities": []}, {"text": "A result of shows that the dimension characterizes the minimal amount of memory that is required to traverse a tree.", "labels": [], "entities": []}, {"text": "So, intuitively, parse trees of high dimension should indicate an unnaturally complex sentence structure requiring the reader to remember too many incomplete dependent clauses in the course of reading the sentence.", "labels": [], "entities": []}, {"text": "Section 2 corroborates experimentally that, indeed, parse trees of natural language have small dimension.", "labels": [], "entities": []}, {"text": "Since dimension is a meaningful measure of complexity and parse trees have low dimension, we conjectured that annotating nonterminals with the dimension of the subtree rooted at them could improve parsing accuracy (see for an illustration).", "labels": [], "entities": [{"text": "parsing", "start_pos": 197, "end_pos": 204, "type": "TASK", "confidence": 0.9673502445220947}, {"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.8910423517227173}]}, {"text": "Section 5 shows that this is indeed the case: The combination of the dimension annotation and the well known parent annotation technique leads to absolute improvements of more than 5% F 1 , 7-8% leaf-ancestor score, and a relative reduction of the number of crossing brackets of over 25% compared to a plain PCFG baseline.", "labels": [], "entities": [{"text": "F 1", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9939380884170532}]}, {"text": "At the same time, quite surprisingly, parsing speed more than doubles.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9791457653045654}, {"text": "speed", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.8716992139816284}]}, {"text": "It could be argued that any other graph theoretical measure for the complexity of a tree could lead to similar results.", "labels": [], "entities": []}, {"text": "For this reason we have also considered annotating nonterminals with the height of the subtree rooted at them (the height is the most basic measure related to trees).", "labels": [], "entities": []}, {"text": "Our experiments show that height annotation is also beneficial but further refinement via parent annotation yields less improvements than for the dimension annotation.", "labels": [], "entities": []}, {"text": "In the following two sections, we present more details on the use of tree dimension in NLP, continue with describing our experiments (Section 4) together with their results (Section 5), and finally conclude with some ideas for further improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use release 8 of the T\u00fcBa-D/Z treebank () as dataset.", "labels": [], "entities": [{"text": "T\u00fcBa-D/Z treebank", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.8804148882627487}]}, {"text": "To combine easy prototyping and data exploration with efficient parsing and standard evaluation methods we used python nltk) together with the Stanford parser ().", "labels": [], "entities": [{"text": "data exploration", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7274558991193771}]}, {"text": "For evaluation we used the builtin evalb, leaf-ancestor, and crossing brackets metrics provided by the Stanford parser.", "labels": [], "entities": []}, {"text": "Isis important to note that all our experiments use gold tags from the treebank 3 which had the pleasant side effect that no parse failures were encountered.", "labels": [], "entities": []}, {"text": "All experiments were carried out on a machine with an Intel i7 2.7 GHz CPU and 8 GB RAM and took about one week to run . Our scripts and raw data can be obtained freely from https://github.com/ mschlund/nlp-newton.", "labels": [], "entities": []}, {"text": "To thoroughly assess the performance of our annotation schemes we not only report the usual constituency measures (labeled precision/recall/F 1 and crossing brackets) proposed originally by (Abney et al., 1991) but also calculate leafancestor scores (LA) proposed by) since it has been argued that LA-scores describe the informal notion of a \"good\" parse better than the usual constituency measures.", "labels": [], "entities": [{"text": "precision/recall/F 1", "start_pos": 123, "end_pos": 143, "type": "METRIC", "confidence": 0.8347643415133158}, {"text": "leafancestor scores (LA)", "start_pos": 230, "end_pos": 254, "type": "METRIC", "confidence": 0.8739846229553223}]}, {"text": "This is especially relevant for comparing parsing accuracy over different treebanks (Rehbein and Van Genabith, 2007a; Rehbein and van Genabith, 2007b).", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9761156439781189}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.8761033415794373}]}], "tableCaptions": [{"text": " Table 1: Average and maximum dimension for  several treebanks of natural languages. Sources:  English -10% sample from the Penn treebank  shipped with python nltk (Loper and Bird, 2002),  German(2) -release 8 of the T\u00fcBa-D/Z treebank  (Telljohann et al., 2003), the remaining treebanks  are taken from the SPMRL shared task dataset  (Seddah et al., 2013).", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.9834316968917847}, {"text": "T\u00fcBa-D/Z treebank", "start_pos": 217, "end_pos": 234, "type": "DATASET", "confidence": 0.7050512731075287}, {"text": "SPMRL shared task dataset", "start_pos": 307, "end_pos": 332, "type": "DATASET", "confidence": 0.6274346485733986}]}, {"text": " Table 2: Average grammar sizes, parsing speed, and parsing accuracies according to various metrics (for  the 70k samples only, i.e. on 7000 test trees). All numbers are averaged over 10 independent random  samples. |G| denotes the number of rules in the grammar, parsing speed is measured in sentences per  second. LA scores are reported as sentence-level (s) and corpus-level (c) averages, respectively. All  accuracies reported in % (except # CB -the average number of crossing brackets per sentence).", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9650894999504089}, {"text": "LA", "start_pos": 316, "end_pos": 318, "type": "METRIC", "confidence": 0.9594153761863708}]}]}