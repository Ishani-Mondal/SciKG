{"title": [{"text": "IMPROVING CHINESE TOKENIZATION WITH LINGUISTIC FILTERS ON STATISTICAL LEXICAL ACQUISITION", "labels": [], "entities": [{"text": "IMPROVING CHINESE TOKENIZATION WITH LINGUISTIC FILTERS", "start_pos": 0, "end_pos": 54, "type": "METRIC", "confidence": 0.5841744095087051}]}], "abstractContent": [{"text": "The first step in Chinese NLP is to tokenize or segment character sequences into words, since the text contains no word delimiters.", "labels": [], "entities": [{"text": "tokenize or segment character sequences into words", "start_pos": 36, "end_pos": 86, "type": "TASK", "confidence": 0.7775924035481044}]}, {"text": "Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date have been based on dictionary lookup (e.g., Chang &Chen 1993; Chiang et al. 1992; Linet al.", "labels": [], "entities": []}, {"text": "1993; Wu & Tseng 1993; Sproat et al.", "labels": [], "entities": [{"text": "Sproat et al.", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.8593077659606934}]}, {"text": "We present empirical evidence for four points concerning tokenization of Chinese text: (I) More rigorous \"blind\" evaluation methodology is needed to avoid inflated accuracy measurements; we introduce the nk-blind method.", "labels": [], "entities": [{"text": "tokenization of Chinese text", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.9113687723875046}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.98167484998703}]}, {"text": "(2) The extent of the unknown-word problem is far more serious than generally thought, when tokenizing unrestricted texts in realistic domains.", "labels": [], "entities": [{"text": "tokenizing unrestricted texts", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.8987215956052145}]}, {"text": "(3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%.", "labels": [], "entities": [{"text": "Statistical lexical acquisition", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8813960353533427}, {"text": "tokenization", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.9697311520576477}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8540125489234924}, {"text": "error rates", "start_pos": 127, "end_pos": 138, "type": "METRIC", "confidence": 0.9546819627285004}]}, {"text": "(4) When augmenting the lexicon, linguistic constraints can provide simple inexpensive filters yielding significantly better precision, reducing error rates as much as 49.4%.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9960338473320007}, {"text": "error rates", "start_pos": 145, "end_pos": 156, "type": "METRIC", "confidence": 0.9643511176109314}]}], "introductionContent": [], "datasetContent": [{"text": "Tokenizing independently derived test data.", "labels": [], "entities": [{"text": "Tokenizing independently derived test", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8120591640472412}]}, {"text": "The unknown word problem is now widely recognized, but we believe its severity is still greatly underestimated.", "labels": [], "entities": [{"text": "severity", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.990574300289154}]}, {"text": "As an \"acid test\", we tokenized a corpus that was derived completely independently of the dictionary that our tokenizer is based on.", "labels": [], "entities": []}, {"text": "We used a statistical dictionary-based tokenizer designed to be representative of current tokenizing approaches, which chooses the segmentation that maximizes the product of the individual words' probabilities.", "labels": [], "entities": []}, {"text": "The baseline dictionary used by the tokenizer is the BDC dictionary (BDC 1992), containing 89,346 unique orthographic forms.", "labels": [], "entities": [{"text": "BDC dictionary (BDC 1992)", "start_pos": 53, "end_pos": 78, "type": "DATASET", "confidence": 0.9510062336921692}]}, {"text": "The text, drawn from the HKUST English-Chinese Parallel Bilingual Corpus (Wu 1994), consists of transcripts from the parliamentary proceedings of the Hong Kong Legislative Council.", "labels": [], "entities": [{"text": "HKUST English-Chinese Parallel Bilingual Corpus (Wu 1994)", "start_pos": 25, "end_pos": 82, "type": "DATASET", "confidence": 0.9500081208017137}]}, {"text": "Thus, the text can be expected to contain many references to subjects outside the domains under consideration by our dictionary's lexicographers in Taiwan.", "labels": [], "entities": []}, {"text": "Regional usage differences are also to be expected.", "labels": [], "entities": []}, {"text": "The results (see show accuracy rates far below the 90-99% range which is typically reported.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9995884299278259}]}, {"text": "Visual inspection of tokenized output showed that an overwhelming majority of the errors arose from missing dictionary entries.", "labels": [], "entities": []}, {"text": "Tokenization performance on realistic unrestricted text is still seriously compromised.", "labels": [], "entities": []}, {"text": "Tokenization with statistical lexicon augmentation.", "labels": [], "entities": []}, {"text": "To alleviate the unknown word problem, we next experimented with augmenting the tokenizer's dictionary using CXtract, a statistical tool that finds morpheme sequences likely to be Chinese words.", "labels": [], "entities": []}, {"text": "In the earlier work we found CXtract to be a good generator of previously unknown lexical entries, so overall token recall was expected to improve.", "labels": [], "entities": [{"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9877250790596008}]}, {"text": "However, it was not clear whether the gain would outweigh errors introduced by the illegitimate lexical entries that CXtract also produces.", "labels": [], "entities": []}, {"text": "The training corpus consisted of approximately 2 million Chinese characters drawn from the Chinese half of our bilingual corpus.", "labels": [], "entities": []}, {"text": "The unsupervised training procedure is described in detail in.", "labels": [], "entities": []}, {"text": "The training suggested 6,650 candidate lexical entries.", "labels": [], "entities": []}, {"text": "Of these, 2,040 were already present 1RCI in the dictionary, leaving 4,610 previously unknown new entries.", "labels": [], "entities": [{"text": "1RCI", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9370924830436707}]}, {"text": "The same tokenization experiment was then run, using the augmented dictionary instead.", "labels": [], "entities": []}, {"text": "The results shown in bear out our hypothesis that augmenting the lexicon with CXtract's statistically generated lexical entries would improve the overall precision, reducing error rates as much as 32.0% fork = 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9990050196647644}, {"text": "error rates", "start_pos": 174, "end_pos": 185, "type": "METRIC", "confidence": 0.9690325558185577}, {"text": "fork", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9671195149421692}]}, {"text": "Morphosyntactic filters for lexicon candidates.", "labels": [], "entities": []}, {"text": "CXtract produces excellent recall but we wished to improve precision further.", "labels": [], "entities": [{"text": "CXtract", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9282630085945129}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9991835951805115}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9995489716529846}]}, {"text": "Ideally, the false candidates should be rejected by some automatic means, without eliminating valid lexical entries.", "labels": [], "entities": []}, {"text": "To this end, we investigated a set of 34 simple filters based on linguistic principles.", "labels": [], "entities": []}, {"text": "Space precludes a full listing; selected filters are discussed below.", "labels": [], "entities": []}, {"text": "Our filters can be extremely inexpensive because CXtract's statistical criteria are already tuned for high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9902509450912476}]}, {"text": "The filtering process first segments the candidate using the original dictionary, to identify the component words.", "labels": [], "entities": []}, {"text": "It then applies morphological and syntactic constraints to eliminate (a) sequences that should remain multiple segments and (b) illformed sequences.", "labels": [], "entities": []}, {"text": "The morphologically-based filters reject a hypothesized lexical entry if it matches any filtering pattern.", "labels": [], "entities": []}, {"text": "The particular characters in these filters are usually classified either as morphological affixes, or as individual words.", "labels": [], "entities": []}, {"text": "We reject any sequence with the affix on the wrong end (the special case of the genitive fl\",j (de) is considered below).", "labels": [], "entities": []}, {"text": "Because morphemes such as the plural marker ~ (m6n) or the instance marker -3k (d) are suffixes, we can eliminate candidate sequences that begin with them.", "labels": [], "entities": []}, {"text": "Similarly, we can reject sequences that end with the ordinal prefix (di) or the preverbial durative ~ (z/d).", "labels": [], "entities": []}, {"text": "Filtering characters cannot be used if they are polysemous or homonymous and can participate in legitimate words in other uses.", "labels": [], "entities": []}, {"text": "For example, the durative ~i~ (zhe) is not a good filter because the same character (with varying pronunciations) can be used to mean \"apply\", \"trick\", or \"touch\", among others.", "labels": [], "entities": []}, {"text": "Any candidate lexical entry is filtered if it contains the genitive/associative ~ (de).", "labels": [], "entities": []}, {"text": "This includes, for example, both illformed boundary-crossing patterns like ~j~ Closed-class syntactic constraints.", "labels": [], "entities": []}, {"text": "The closed-class filters operate on two distinct principles.", "labels": [], "entities": []}, {"text": "Sequences ending with strongly prenominal or preverbial words are rejected, as are sequences beginning with postnominals and postverbials.", "labels": [], "entities": []}, {"text": "A majority of the filtering patterns match correct syntactic units, including prepositional, conjunctive, modal, adverbial, and verb phrases.", "labels": [], "entities": []}, {"text": "The rationale for rejecting such sequences is that these closed-class words do not satisfy the criteria for being bound into compounds, and just co-occur with some sequences by chance because of their high frequency.", "labels": [], "entities": []}, {"text": "The same tokenization experiment was run using the filtered augmented dictionary.", "labels": [], "entities": []}, {"text": "The filters left 5,506 candidate lexical entries out of the original 6,650, of which 3,467 were previously unknown.", "labels": [], "entities": []}, {"text": "shows significantly improved precision in every measurement except fora very slight drop with k = 8, with an error rate reduction of 49.4% at k : 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.999602735042572}, {"text": "error rate reduction", "start_pos": 109, "end_pos": 129, "type": "METRIC", "confidence": 0.9837900400161743}]}, {"text": "Thus any loss in token recall due to the filters is outweighed by the gain in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9652288556098938}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9984773993492126}]}, {"text": "This maybe taken as indirect evidence that the loss in recall is not large.", "labels": [], "entities": [{"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9989866614341736}]}], "tableCaptions": []}