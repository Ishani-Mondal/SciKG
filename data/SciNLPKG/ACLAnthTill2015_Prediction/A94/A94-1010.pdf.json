{"title": [{"text": "Improving Language Models by Clustering Training Sentences", "labels": [], "entities": [{"text": "Improving Language Models", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9163244167963663}]}], "abstractContent": [{"text": "Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7930491864681244}]}, {"text": "I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcor-pora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster.", "labels": [], "entities": []}, {"text": "This kind of clustering offers away to represent important contextual effects and can therefore significantly improve the performance of a model.", "labels": [], "entities": []}, {"text": "It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclus-tered model.", "labels": [], "entities": []}, {"text": "As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain.", "labels": [], "entities": [{"text": "clustering", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.955099880695343}, {"text": "ATIS domain", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.8161438703536987}]}, {"text": "These results are consistent with other findings for such models, suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model.", "labels": [], "entities": []}], "introductionContent": [{"text": "In speech recognition and understanding systems, many kinds of language model maybe used to choose between the word and sentence hypotheses for which there is evidence in the acoustic data.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7132740765810013}]}, {"text": "Some words, word sequences, syntactic constructions and semantic structures are more likely to occur than others, and the presence of more likely objects in a sentence hypothesis is evidence for the correctness of that hypothesis.", "labels": [], "entities": []}, {"text": "Evidence from different knowledge sources can be combined in an attempt to optimize the selection of correct hypotheses; see e.g. Alshawi and ;;.", "labels": [], "entities": []}, {"text": "Many of the knowledge sources used for this purpose score a sentence hypothesis by calculating a simple, typically linear, combination of scores associated with objects, such as N-grams and grammar rules, that characterize the hypothesis or its preferred linguistic analysis.", "labels": [], "entities": []}, {"text": "When these scores are viewed as log probabilities, taking a linear sum corresponds to making an independence assumption that is known to beat best only approximately true, and that may give rise to inaccuracies that reduce the effectiveness of the knowledge source.", "labels": [], "entities": []}, {"text": "The most obvious way to make a knowledge source more accurate is to increase the amount of structure or context that it takes account of.", "labels": [], "entities": []}, {"text": "For example, a bigram model maybe replaced by a trigram one, and the fact that dependencies exist among the likelihoods of occurrence of grammar rules at different locations in a parse tree can be modeled by associating probabilities with states in a parsing table rather than simply with the rules themselves.", "labels": [], "entities": []}, {"text": "However, such remedies have their drawbacks.", "labels": [], "entities": []}, {"text": "Firstly, even when the context is extended, some important influences may still not be modeled.", "labels": [], "entities": []}, {"text": "For example, dependencies between words exist at separations greater than those allowed for by trigrams (for which long-distance N-grams area partial remedy), and associating scores with parsing table states may not model all the important correlations between grammar rules.", "labels": [], "entities": []}, {"text": "Secondly, extending the model may greatly increase the amount of training data required if sparseness problems are to be kept under control, and additional data maybe unavailable or expensive to collect.", "labels": [], "entities": []}, {"text": "Thirdly, one cannot always know in advance of doing the work whether extending a model in a particular direction will, in practice, improve results.", "labels": [], "entities": []}, {"text": "If it turns out not to, considerable ingenuity and effort may have been wasted.", "labels": [], "entities": []}, {"text": "In this paper, I argue fora general method for extending the context-sensitivity of any knowledge source that calculates sentence hypothesis scores as linear combinations of scores for objects.", "labels": [], "entities": []}, {"text": "The method, which is related to that of Iyer,, involves clustering the sentences in the training corpus into a number of subcorpora, each predicting a different probability distribution for linguistic objects.", "labels": [], "entities": []}, {"text": "An utterance hypothesis encountered at run time is then treated as if it had been selected from the subpopulation of sentences represented by one of these subcorpora.", "labels": [], "entities": []}, {"text": "This technique addresses as follows the three drawbacks just alluded to.", "labels": [], "entities": []}, {"text": "Firstly, it is able to capture the most important sentence-internal contextual effects regardless of the complexity of the probabilistic dependencies between the objects involved.", "labels": [], "entities": []}, {"text": "Secondly, it makes only modest additional demands on training data.", "labels": [], "entities": []}, {"text": "Thirdly, it can be applied in a standard way across knowledge sources for very different kinds of object, and if it does improve on the unclustered model this constitutes proof that additional, as yet unexploited relationships exist between linguistic objects of the type the model is based on, and that therefore it is worth looking fora more specific, more powerful way to model them.", "labels": [], "entities": []}, {"text": "The use of corpus clustering often does not boost the power of the knowledge source as much as a specific hand-coded extension.", "labels": [], "entities": [{"text": "corpus clustering", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7199008464813232}]}, {"text": "For example, a clustered bigram model will probably not be as powerful as a trigram model.", "labels": [], "entities": []}, {"text": "However, clustering can have two important uses.", "labels": [], "entities": []}, {"text": "One is that it can provide some improvement to a model even in the absence of the additional (human or computational) resources required by a hand-coded extension.", "labels": [], "entities": []}, {"text": "The other use is that the existence or otherwise of an improvement brought about by clustering can be a good indicator of whether additional performance can in fact be gained by extending the model by hand without further data collection, with the possibly considerable additional effort that extension would entail.", "labels": [], "entities": []}, {"text": "And, of course, there is no reason why clustering should not, where it gives an advantage, also be used in conjunction with extension by hand to produce yet further improvements.", "labels": [], "entities": []}, {"text": "As evidence for these claims, I present experimental results showing how, fora particular task and training corpus, clustering produces a sizeable improvement in unigram-and bigram-based models, but not in trigram-based ones; this is consistent with experience in the speech understanding community that while moving from bigrams to trigrams usually produces a definite payoff, a move from trigrams to 4-grams yields less clear benefits for the domain in question.", "labels": [], "entities": [{"text": "speech understanding community", "start_pos": 268, "end_pos": 298, "type": "TASK", "confidence": 0.7775693635145823}]}, {"text": "I also show that, for the same task and corpus, clustering produces improvements when sentences are assessed not according to the words they contain but according to the syntax rules used in their best parse.", "labels": [], "entities": []}, {"text": "This work thus goes beyond that of Iyer et al by focusing on the methodological importance of corpus clustering, rather than just its usefulness in improving overall systemperformance, and by exploring in detail the way its effectiveness varies along the dimensions of language model type, language model complexity, and number of clusters used.", "labels": [], "entities": [{"text": "corpus clustering", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7293543517589569}]}, {"text": "It also differs from Iyer et al's work by clustering at the utterance rather than the paragraph level, and by using a training corpus of thousands, rather than millions, of sentences; in many speech applications, available training data is likely to be quite limited, and may not always be chunked into paragraphs.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were carried out to assess the effectiveness of clustering, and therefore the existence of unexploited contextual dependencies, for instances of two general types of language model.", "labels": [], "entities": []}, {"text": "In the first experiment, sentence hypotheses were evaluated on the N-grams of words and word classes they contained.", "labels": [], "entities": []}, {"text": "In the second experiment, evaluation was on the basis of grammar rules used rather than word occurrences.", "labels": [], "entities": []}, {"text": "In the first experiment, reference versions of a set of 5,873 domain-relevant (classes A and D) ATIS-2 sentences were allocated to K clusters for K = 2, 3, 5, 6, 10 and 20 for the unigram, bigram and trigram conditions and, for unigrams and bigrams only, K = 40 and 100 as well.", "labels": [], "entities": []}, {"text": "Each run was repeated for ten different random orders for presentation of the training data.", "labels": [], "entities": []}, {"text": "The unclustered (K = 1) version of each language model was also evaluated.", "labels": [], "entities": []}, {"text": "Some words, and some sequences of words such as \"San Francisco\", were replaced by class names to improve performance.", "labels": [], "entities": []}, {"text": "The improvement (if any) due to clustering was measured by using the various language models to make selections from N-best sentence hypothesis lists; this choice of test was made for convenience rather than out of any commitment to the N-best paradigm, and the techniques described here could equally well be used with other forms of speechlanguage interface.", "labels": [], "entities": []}, {"text": "Specifically, each clustering was tested against 1,354 hypothesis lists output by aversion of the DECIPHER (TM) speech recognizer) that itself used a (rather simpler) bigram model.", "labels": [], "entities": [{"text": "DECIPHER (TM) speech recognizer", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6284712553024292}]}, {"text": "Where more then ten hypothesis were output fora sentence, only the top ten were considered.", "labels": [], "entities": []}, {"text": "These 1,354 lists were the subset of two 1,000 sentence sets (the February and November 1992 ATIS evaluation sets) for which the reference sentence itself occurred in the top ten hypotheses.", "labels": [], "entities": [{"text": "ATIS evaluation sets", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.8426273862520853}]}, {"text": "The clustered language model was used to select the most likely hypothesis from the list without paying any attention either to the score that DECIPHER assigned to each hypothesis on the basis of acoustic information or its own bigram model, or to the ordering of the list.", "labels": [], "entities": []}, {"text": "Ina real system, the DECIPHER scores would of course betaken into account, but they were ignored herein order to maximize the discriminatory power of the test in the presence of only a few thousand test utterances.", "labels": [], "entities": [{"text": "DECIPHER scores", "start_pos": 21, "end_pos": 36, "type": "METRIC", "confidence": 0.9529742002487183}]}, {"text": "To avoid penalizing longer hypotheses, the probabilities assigned to hypotheses were normalized by sentence length.", "labels": [], "entities": []}, {"text": "The probability assigned by a cluster to an N-gram was taken to be the simple maximum likelihood (relative frequency) value where this was non-zero.", "labels": [], "entities": []}, {"text": "When an N-gram in the test data had not been observed at all in the training sentences assigned to a given cluster, a \"failure\", representing a vanishingly small probability, was assigned.", "labels": [], "entities": []}, {"text": "A number of backoff schemes of various degrees of sophistication, including that of, were tried, but none produced any improvement in performance, and several actuMly worsened it.", "labels": [], "entities": []}, {"text": "The average percentages of sentences correctly identified by clusterings for each condition were as given in.", "labels": [], "entities": []}, {"text": "The maximum possible score was 100%; the baseline score, that expected from a random choice of a sentence from each list, was 11.4%.", "labels": [], "entities": []}, {"text": "The unigram and bigram scores show a steady and, in fact, statistically significant 1 increase with the number of clusters.", "labels": [], "entities": []}, {"text": "Using twenty clusters for bigrams (score 43.9%) in fact gi~ces more than half the advantage over unclustered bigrams that is given  by moving from unclustered bigrams to unclustered trigrams.", "labels": [], "entities": []}, {"text": "However, clustering trigrams produces no improvement in score; in fact, it gives a small but statistically significant deterioration, presumably due to the increase in the number of parameters that need to be calculated.", "labels": [], "entities": [{"text": "score", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9863033890724182}]}, {"text": "The random choice of a presentation order for the data meant that different clusterings were arrived at on each run fora given condition ((N, K) for Ngrams and K clusters).", "labels": [], "entities": []}, {"text": "There was some limited evidence that some clusterings for the same condition were significantly better than others, rather than just happening to perform better on the particular test data used.", "labels": [], "entities": []}, {"text": "More trials would be needed to establish whether presentation order does in general make a genuine difference to the quality of a clustering.", "labels": [], "entities": []}, {"text": "If there is one, however, it would appear to be fairly small compared to the improvements available (in the unigram and bigram cases) from increasing the numbers of clusters.", "labels": [], "entities": []}, {"text": "In the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine) trained on the ATIS domain.", "labels": [], "entities": [{"text": "ATIS domain", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.9458523690700531}]}, {"text": "Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower).", "labels": [], "entities": [{"text": "ATIS adaptation", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.9023884534835815}]}, {"text": "When a sentence was analysed successfully, several semantic analyses were, in general, created, and a selection was made from among these on the basis of trained preference functions (.", "labels": [], "entities": []}, {"text": "For the purpose of the experiment, clustering and hypothesis selection were performed on the basis not of the words in a sentence but of the grammar rules used to construct its most preferred analysis.", "labels": [], "entities": [{"text": "hypothesis selection", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6823386400938034}]}, {"text": "The simplest condition, hereafter referred to as \"lrule\", was analogous to the unigram case for wordbased evaluation.", "labels": [], "entities": []}, {"text": "A sentence was modeled simply as a bag of rules, and no attempt (other than the clustering itself) was made to account for dependencies between rules.", "labels": [], "entities": []}, {"text": "Another condition, henceforth \"2-rule\" because of its analogy to bigrams, was also tried.", "labels": [], "entities": []}, {"text": "Here, each rule occurrence was represented not in isolation but in the context of the rule immediately above it in the parse tree.", "labels": [], "entities": []}, {"text": "Other choices of context might have worked as well or better; our purpose here is simply to illustrate and assess ways in which explicit context modeling can be combined with clustering.", "labels": [], "entities": []}, {"text": "The training corpus consisted of the 4,279 sentences in the 5,873-sentence set that were analysable and consisted of fifteen words or less.", "labels": [], "entities": []}, {"text": "The test corpus consisted of 1,106 hypothesis lists, selected in the same way (on the basis of length and analysability of their reference sentences) from the 1,354 used in the first experiment.", "labels": [], "entities": []}, {"text": "The \"baseline\" score for this test corpus, expected from a random choice of (analysable) hypothesis, was 23.2%.", "labels": [], "entities": []}, {"text": "This was rather higher than the 11.4% for word-based selection because the hypothesis lists used were in general shorter, unanalysable hypotheses having been excluded.", "labels": [], "entities": [{"text": "word-based selection", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7534778416156769}]}, {"text": "The average percentages of correct hypotheses (actual word strings, not just the rules used to represent them) selected by the 1-rule and 2-rule conditions were as given in.", "labels": [], "entities": []}, {"text": "These results show that clustering gives a significant advantage for both the 1-rule and the 2-rule types of model, and that the more clusters are created, the larger the advantage is, at least up to K = 20 clusters.", "labels": [], "entities": []}, {"text": "As with the N-gram experiment, there is weak evidence that some clusterings are genuinely better than others for the same condition.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average percentage scores for cluster-based  N-gram models", "labels": [], "entities": [{"text": "Average percentage scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9304580291112264}]}]}