{"title": [], "abstractContent": [{"text": "Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment.", "labels": [], "entities": [{"text": "Labeling of sentence boundaries", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8669267892837524}, {"text": "part-of-speech tagging", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.698363646864891}, {"text": "sentence alignment", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.7467793226242065}]}, {"text": "End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules.", "labels": [], "entities": []}, {"text": "As an alternative, we have developed an efficient , trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network.", "labels": [], "entities": []}, {"text": "This work demonstrates the feasibility of using prior probabilities of part-of-speech assignments, as opposed to words or definite part-of-speech assignments, as contextual information.", "labels": [], "entities": []}, {"text": "After training for less than one minute, the method correctly labels over 98.5% of sentence boundaries in a corpus of over 27,000 sentence-boundary marks.", "labels": [], "entities": []}, {"text": "We show the method to be efficient and easily adaptable to different text genres, including single-case texts.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We tested the boundary labeler on a large body of text containing 27,294 potential sentence-ending punctuation marks taken from the Wall Street Journal portion of the ACL/DCI collection.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the ACL/DCI collection", "start_pos": 132, "end_pos": 185, "type": "DATASET", "confidence": 0.9676199316978454}]}, {"text": "No preprocessing was performed on the test text, aside from removing unnecessary headers and correcting existing errors.", "labels": [], "entities": []}, {"text": "(The sen-81 tence boundaries in the WSJ text had been previously labeled using a method similar to that used in PARTS and is described in more detail in; we found and corrected several hundred errors.)", "labels": [], "entities": [{"text": "WSJ text", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9625634849071503}]}, {"text": "We trained the weights in the neural network with a back-propagation algorithm on a training set of 573 items from the same corpus.", "labels": [], "entities": []}, {"text": "To increase generalization of training, a separate cross-validation set (containing 258 items also from the same corpus) was also fed through the network, but the weights were not trained on this set.", "labels": [], "entities": []}, {"text": "When the cumulative error of the items in the cross-validation set reached a minimum, training was stopped.", "labels": [], "entities": []}, {"text": "Training was done in batch mode with a learning rate of 0.08.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9743010997772217}]}, {"text": "The entire training procedure required less than one minute on a Hewlett Packard 9000/750 Workstation.", "labels": [], "entities": [{"text": "Hewlett Packard 9000/750 Workstation", "start_pos": 65, "end_pos": 101, "type": "DATASET", "confidence": 0.9170391857624054}]}, {"text": "This should be contrasted with Riley's algorithm which required 25 million words of training data in order to compile probabilities.", "labels": [], "entities": []}, {"text": "If we use Riley's statistics presented in Section 1, we can determine a lower bound fora sentence boundary disambiguation algorithm: an algorithm that always labels a period as a sentence boundary would be correct 90% of the time; therefore, any method must perform better than 90%.", "labels": [], "entities": [{"text": "sentence boundary disambiguation", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.6382933159669241}]}, {"text": "In our experiments, performance was very strong: with both sensitivity thresholds set to 0.5, the network method was successful in disambiguating 98.5% of the punctuation marks, mislabeling only 409 of 27,294.", "labels": [], "entities": []}, {"text": "These errors fall into two major categories: (i)\"false positive\": the method erroneously labeled a punctuation mark as a sentence boundary, and (ii) \"false negative\": the method did not label a sentence boundary as such.", "labels": [], "entities": []}, {"text": "224 (54.8%) false positives 185 (45.2%) false negatives 409 total errors out of 27,294 items: Results of testing on 27,294 mixed-case items; to --tl --0.5, 6-context, 2 hidden units.", "labels": [], "entities": []}, {"text": "The 409 errors from this testing run can be decomposed into the following groups: 37.6% false positive at an abbreviation within a title or name, usually because the word following the period exists in the lexicon with other parts-of-speech (Mr. Gray, Col. North, Mr. Major, Dr. Carpenter, Mr. Sharp).", "labels": [], "entities": [{"text": "false positive", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.8458459675312042}, {"text": "Sharp", "start_pos": 294, "end_pos": 299, "type": "METRIC", "confidence": 0.5345862507820129}]}, {"text": "Also included in this group are items such as U.S. Supreme Court or U.S. Army, which are sometimes mislabeled because U.S. occurs very frequently at the end of a sentence as well.", "labels": [], "entities": [{"text": "U.S. Supreme Court", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.7621910572052002}]}, {"text": "22.5% false negative due to an abbreviation at the end of a sentence, most frequently Inc., Co., Corp., or U.S., which all occur within sentences as well.", "labels": [], "entities": []}, {"text": "11.0% false positive or negative due to a sequence of characters including a punctuation mark and quotation marks, as this sequence can occur both within and at the end of sentences.", "labels": [], "entities": []}, {"text": "9.2% false negative resulting from an abbreviation followed by quotation marks; related to the previous two types.", "labels": [], "entities": []}, {"text": "9.8% false positive or false negative resulting from presence of ellipsis (...), which can occur at the end of or within a sentence.", "labels": [], "entities": []}, {"text": "9.9% miscellaneous errors, including extraneous characters (dashes, asterisks, etc.), ungrammatical sentences, misspellings, and parenthetical sentences.", "labels": [], "entities": []}, {"text": "The results presented above (409 errors) are obtained when both to and tl are set at 0.5.", "labels": [], "entities": []}, {"text": "Adjusting the sensitivity thresholds decreases the number of punctuation marks which are mislabeled by the method.", "labels": [], "entities": []}, {"text": "For example, when the upper threshold is set at 0.8 and the lower threshold at 0.2, the network places 164 items between the two.", "labels": [], "entities": []}, {"text": "Thus when the algorithm does not have enough evidence to classify the items, some mislabeling can be avoided, s We also experimented with different context sizes and numbers of hidden units, obtaining the results shown in.", "labels": [], "entities": []}, {"text": "All results were found using the same training set of 573 items, cross-validation set of 258 items, and mixed-case test set of 27,294 items.", "labels": [], "entities": []}, {"text": "The \"Training Error\" is one-half the sum of all the errors for all 573 items in the training set, where the \"error\" is the difference between the desired output and the actual output of the neural net.", "labels": [], "entities": [{"text": "Training Error\"", "start_pos": 5, "end_pos": 20, "type": "METRIC", "confidence": 0.8117364247639974}]}, {"text": "The \"Cross Error\" is the equivalent value for the cross-validation set.", "labels": [], "entities": [{"text": "Cross Error\"", "start_pos": 5, "end_pos": 17, "type": "METRIC", "confidence": 0.6740950147310892}]}, {"text": "These two error figures give an indication of how well the network learned the training data before stopping.", "labels": [], "entities": []}, {"text": "We observed that a net with fewer hidden units results in a drastic decrease in the number of false positives and a corresponding increase in the number of false negatives.", "labels": [], "entities": []}, {"text": "Conversely, increasing the number of hidden units results in a decrease of false negatives (to zero) and an increase in false positives.", "labels": [], "entities": []}, {"text": "A network with 2 hidden units produces the best overall error rate, with false negatives and false positives nearly equal.", "labels": [], "entities": [{"text": "error rate", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9623240232467651}]}, {"text": "From these data we concluded that a context of six surrounding tokens and a hidden layer with two 5We will report on results of varying the thresholds in future work.", "labels": [], "entities": []}, {"text": "units worked best for our test set.", "labels": [], "entities": []}, {"text": "After converting the training, cross-validation and test texts to a lower-case-only format and retraining, the network was able to successfully disambiguate 96.2% of the boundaries in a lower-case-only test text.", "labels": [], "entities": []}, {"text": "Repeating the procedure with an upper-caseonly format produced a 97.4% success rate.", "labels": [], "entities": []}, {"text": "Unlike most existing methods which rely heavily on capitalization information, the network method is reasonably successful at disambiguating single-case texts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of comparing context sizes (2 hidden units).", "labels": [], "entities": []}, {"text": " Table 3: Results of comparing hidden layer sizes (6-context). Training was done on 573 items, using a cross  validation set of 258 items.", "labels": [], "entities": []}]}