{"title": [{"text": "CONTEXTUAL WORD SIMILARITY AND ESTIMATION FROM SPARSE DATA", "labels": [], "entities": [{"text": "CONTEXTUAL WORD SIMILARITY", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.4511163334051768}, {"text": "ESTIMATION FROM SPARSE DATA", "start_pos": 31, "end_pos": 58, "type": "METRIC", "confidence": 0.6569585800170898}]}], "abstractContent": [{"text": "In recent years there is much interest in word cooccurrence relations, such as n-grams, verb-object combinations, or cooccurrence within a limited context.", "labels": [], "entities": []}, {"text": "This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data.", "labels": [], "entities": []}, {"text": "We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric.", "labels": [], "entities": []}, {"text": "Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical data on word cooccurrence relations play a major role in many corpus based approaches for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6614447236061096}]}, {"text": "Different types of cooccurrence relations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context.", "labels": [], "entities": []}, {"text": "Statistical data about these various cooccurrence relations is employed fora variety of applications, such as speech recognition, language generation (, lexicography, machine translation (, information retrieval and various disambiguation tasks (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7983656823635101}, {"text": "language generation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.772720605134964}, {"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.8295243084430695}, {"text": "information retrieval", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.8085413575172424}]}, {"text": "A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus.", "labels": [], "entities": []}, {"text": "Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even fora very large training corpus.", "labels": [], "entities": []}, {"text": "Since applications often have to compare alternative hypothesized cooccurrences, it is important to distinguish between those unobserved cooccurrences that are likely to occur in anew piece of text and those that are not.", "labels": [], "entities": []}, {"text": "These distinctions ought to be made using the data that do occur in the corpus.", "labels": [], "entities": []}, {"text": "Thus, beyond its own practical importance, the sparse data problem provides an informative touchstone for theories on generalization and analogy in linguistic data.", "labels": [], "entities": []}, {"text": "The literature suggests two major approaches for solving the sparse data problem: smoothing and class based methods.", "labels": [], "entities": []}, {"text": "Smoothing methods estimate the probability of unobserved cooccurrences using frequency information.", "labels": [], "entities": []}, {"text": "show, that for unobserved bigrams, the estimates of several smoothing methods closely agree with the probability that is expected using the frequencies of the two words and assuming that their occurrence is independent,).", "labels": [], "entities": []}, {"text": "Furthermore, using held out data they show that this is the probability that should be estimated by a smoothing method that takes into account the frequencies of the individual words.", "labels": [], "entities": []}, {"text": "Relying on this result, we will use frequency based es~imalion (using word frequencies) as representative for smoothing estimates of unobserved cooccurrences, for comparison purposes.", "labels": [], "entities": []}, {"text": "As will be shown later, the problem with smoothing estimates is that they ignore the expected degree of association between the specific words of the cooccurrence.", "labels": [], "entities": []}, {"text": "For example, we would not like to estimate the same probability for two cooccurrences like 'eat bread' and 'eat cars', despite the fact that both 'bread' and 'cars' may have the same frequency.", "labels": [], "entities": []}, {"text": "Class based models () distinguish between unobserved cooccurrences using classes of \"similar\" words.", "labels": [], "entities": []}, {"text": "The probability of a specific cooccurrence is determined using generalized parameters about the probability of class cooccur-] 64 rence.", "labels": [], "entities": []}, {"text": "This approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture \"typical\" properties of classes of words.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7244945168495178}]}, {"text": "However, it is not clear at all that unrestricted language is indeed structured the way it is assumed by class based models.", "labels": [], "entities": []}, {"text": "In particular, it is not clear that word cooccurrence patterns can be structured and generalized to class cooccurrence parameters without losing too much information.", "labels": [], "entities": []}, {"text": "This paper suggests an alternative approach which assumes that class based generalizations should be avoided, and therefore eliminates the intermediate level of word classes.", "labels": [], "entities": []}, {"text": "Like some of the class based models, we use a similarity metric to measure the similarity between cooccurrence patterns of words.", "labels": [], "entities": []}, {"text": "But then, rather than using this metric to construct a set of word classes, we use it to identify the most specific analogies that can he drawn for each specific estimation.", "labels": [], "entities": []}, {"text": "Thus, to estimate the probability of an unobserved cooccurfence of words, we use data about other cooccurfences that were observed in the corpus, and contain words that are similar to the given ones.", "labels": [], "entities": []}, {"text": "For example, to estimate the probability of the unobserved cooccurrence 'negative results', we use cooccurrences such as 'positive results' and 'negative numbers', that do occur in our corpus.", "labels": [], "entities": []}, {"text": "The analogies we make are based on the assumption that similar word cooccurrences have similar values of mutual information.", "labels": [], "entities": []}, {"text": "Accordingly, our similarity metric was developed to capture similarities between vectors of mutual information values.", "labels": [], "entities": []}, {"text": "In addition, we use an efficient search heuristic to identify the most similar words fora given word, thus making the method computationally affordable.", "labels": [], "entities": []}, {"text": "illustrates a portion of the similarity network induced by the similarity metric (only some of the edges, with relatively high values, are shown).", "labels": [], "entities": []}, {"text": "This network maybe found useful for other purposes, independently of the estimation method.", "labels": [], "entities": [{"text": "estimation", "start_pos": 73, "end_pos": 83, "type": "TASK", "confidence": 0.9459183216094971}]}, {"text": "The estimation method was implemented using the relation of cooccurrence of two words within a limited distance in a sentence.", "labels": [], "entities": [{"text": "estimation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9557214379310608}]}, {"text": "The proposed method, however, is general and is applicable for anY type of lexical cooccurrence.", "labels": [], "entities": []}, {"text": "The method was evaluated in two experiments.", "labels": [], "entities": []}, {"text": "In the first one we achieved a complete scenario of the use of the estimation method, by implementing a variant of the d, for sense selection in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.6752173602581024}]}, {"text": "The estimation method was then successfully used to increase the coverage of the disambiguation method by 15%, with an increase of the overall precision compared to a naive, frequency based, method.", "labels": [], "entities": [{"text": "estimation", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9361931681632996}, {"text": "coverage", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9789507985115051}, {"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9986810088157654}]}, {"text": "In the second experiment we evaluated the estimation method on a data recovery task.", "labels": [], "entities": [{"text": "estimation", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.8858641386032104}, {"text": "data recovery task", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.79892897605896}]}, {"text": "The task simulates atypical scenario in disambiguation, and also relates to theoretical questions about redundancy and idiosyncrasy in cooccurrence data.", "labels": [], "entities": []}, {"text": "In this evaluation, which involved 300 examples, the performance of the estimation method was by 27% better than frequency based estimation.", "labels": [], "entities": [{"text": "estimation", "start_pos": 72, "end_pos": 82, "type": "TASK", "confidence": 0.9370523691177368}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The similarity based estimate as an average on similar pairs: [(chapter, describes) = 6.41", "labels": [], "entities": []}, {"text": " Table 2: The similarity based estimate for a pair of unassociated words: I(chapter, knows) = 0", "labels": [], "entities": []}, {"text": " Table 4: Results of TWS, Augmented TWS and  Word Frequency methods", "labels": [], "entities": []}]}