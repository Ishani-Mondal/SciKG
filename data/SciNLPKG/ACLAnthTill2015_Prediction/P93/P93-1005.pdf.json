{"title": [{"text": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing*", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity.", "labels": [], "entities": []}, {"text": "HBG incorporates lexical, syntactic , semantic, and structural information from the parse tree into the disambiguation process in a novel way.", "labels": [], "entities": []}, {"text": "We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.", "labels": [], "entities": []}, {"text": "This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse.", "labels": [], "entities": []}, {"text": "In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.", "labels": [], "entities": [{"text": "parsing", "start_pos": 175, "end_pos": 182, "type": "TASK", "confidence": 0.9595668315887451}, {"text": "accuracy rate", "start_pos": 183, "end_pos": 196, "type": "METRIC", "confidence": 0.9377149045467377}, {"text": "error", "start_pos": 233, "end_pos": 238, "type": "METRIC", "confidence": 0.9785134792327881}]}], "introductionContent": [{"text": "Almost any natural language sentence is ambiguous in structure, reference, or nuance of meaning.", "labels": [], "entities": []}, {"text": "Humans overcome these apparent ambiguities by examining the contez~ of the sentence.", "labels": [], "entities": []}, {"text": "But what exactly is context?", "labels": [], "entities": []}, {"text": "Frequently, the correct interpretation is apparent from the words or constituents immediately surrounding the phrase in question.", "labels": [], "entities": []}, {"text": "This observation begs the following question: How much information about the context of a sentence or phrase is necessary and sufficient to determine its meaning?", "labels": [], "entities": []}, {"text": "This question is at the crux of the debate among computational linguists about the application and implementation of statistical methods in natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.6543374160925547}]}, {"text": "Previous work on disambiguation and probabilistic parsing has offered partial answers to this question.", "labels": [], "entities": []}, {"text": "Hidden Markov models of words and *Thanks to Philip Resnik and Stanley Chen for their valued input.", "labels": [], "entities": []}, {"text": "their tags, introduced in (5) and (5) and popularized in the natural language community by Church (5), demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity.", "labels": [], "entities": []}, {"text": "Hindle and Rooth (5) use a statistical measure of lexical associations to resolve structural ambiguities.", "labels": [], "entities": []}, {"text": "Brent (5) acquires likely verb subcategorization patterns using the frequencies of verbobject-preposition triples.", "labels": [], "entities": []}, {"text": "propose a model of context that combines the n-gram model with information from dominating constituents.", "labels": [], "entities": []}, {"text": "All of these aspects of context are necessary for disambiguation, yet none is sufficient.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9728460311889648}]}, {"text": "We propose a probabilistic model of context for disambiguation in parsing, HBG, which incorporates the intuitions of these previous works into one unified framework.", "labels": [], "entities": [{"text": "HBG", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8029561638832092}]}, {"text": "Let p(T, w~) be the joint probability of generating the word string w~ and the parse tree T.", "labels": [], "entities": []}, {"text": "Given w~, our parser chooses as its parse tree that tree T* for which T\" =arg maxp(T, w~) where ~(w~) is the set of all parses produced by the grammar for the sentence w~.", "labels": [], "entities": []}, {"text": "Many aspects of the input sentence that might be relevant to the decision-making process participate in the probabilistic model, providing a very rich if not the richest model of context ever attempted in a probabilistic parsing model.", "labels": [], "entities": []}, {"text": "In this paper, we will motivate and define the HBG model, describe the task domain, give an overview of the grammar, describe the proposed HBG model, and present the results of experiments comparing HBG with an existing state-ofthe-art model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}