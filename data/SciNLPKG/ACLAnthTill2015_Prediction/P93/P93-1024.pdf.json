{"title": [], "abstractContent": [{"text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts.", "labels": [], "entities": []}, {"text": "Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering.", "labels": [], "entities": []}, {"text": "Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership.", "labels": [], "entities": []}, {"text": "In many cases, the clusters can bethought of as encoding coarse sense distinctions.", "labels": [], "entities": []}, {"text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the an-nealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data.", "labels": [], "entities": []}, {"text": "Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.", "labels": [], "entities": []}, {"text": "INTRODUCTION Methods for automatically classifying words according to their contexts of use have both scientific and practical interest.", "labels": [], "entities": []}, {"text": "The scientific questions arise in connection to distributional views of linguistic (particularly lexical) structure and also in relation to the question of lexical acquisition both from psychological and computational learning perspectives.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7408864200115204}]}, {"text": "From the practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models, particularly models for deciding among alternative analyses proposed by a grammar.", "labels": [], "entities": [{"text": "word classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7886606454849243}]}, {"text": "It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations.", "labels": [], "entities": []}, {"text": "The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities.", "labels": [], "entities": []}, {"text": "Hindle (1990) proposed dealing with the", "labels": [], "entities": []}], "introductionContent": [{"text": "Methods for automatically classifying words according to their contexts of use have both scientific and practical interest.", "labels": [], "entities": []}, {"text": "The scientific questions arise in connection to distributional views of linguistic (particularly lexical) structure and also in relation to the question of lexical acquisition both from psychological and computational learning perspectives.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7408864200115204}]}, {"text": "From the practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models, particularly models for deciding among alternative analyses proposed by a grammar.", "labels": [], "entities": [{"text": "word classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7886615991592407}]}, {"text": "It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations.", "labels": [], "entities": []}, {"text": "The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities.", "labels": [], "entities": []}, {"text": "Hindle proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of \"similar\" events that have been seen.", "labels": [], "entities": []}, {"text": "For instance, one may estimate the likelihood of a particular direct object fora verb from the likelihoods of that direct object for similar verbs.", "labels": [], "entities": []}, {"text": "This requires a reasonable definition of verb similarity and a similarity estimation method.", "labels": [], "entities": []}, {"text": "In Hindle's proposal, words are similar if we have strong statistical evidence that they tend to participate in the same events.", "labels": [], "entities": []}, {"text": "His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association.", "labels": [], "entities": []}, {"text": "Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves.", "labels": [], "entities": []}, {"text": "While it maybe worth basing such a model on preexisting sense classes, in the work described here we look at how to derive the classes directly from distributional data.", "labels": [], "entities": []}, {"text": "More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw ) for each word w.", "labels": [], "entities": []}, {"text": "Most other class-based modeling techniques for natural language rely instead on \"hard\" Boolean classes (.", "labels": [], "entities": []}, {"text": "Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above.", "labels": [], "entities": [{"text": "Class construction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7883542478084564}]}, {"text": "Our approach avoids both problems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}