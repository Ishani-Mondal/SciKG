{"title": [{"text": "USING BRACKETED PARSES TO EVALUATE A GRAMMAR CHECKING APPLICATION", "labels": [], "entities": [{"text": "BRACKETED", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.7055807709693909}, {"text": "PARSES", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.6149154305458069}, {"text": "EVALUATE A GRAMMAR CHECKING", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.7025031894445419}, {"text": "APPLICATION", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.5762991309165955}]}], "abstractContent": [{"text": "We describe a method for evaluating a grammar checking application with hand-bracketed parses.", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7483786046504974}]}, {"text": "A randomly-selected set of sentences was submitted to a grammar checker in both bracketed and unbracketed formats.", "labels": [], "entities": []}, {"text": "A comparison of the resulting error reports illuminates the relationship between the underlying performance of the parser-grammar system and the error critiques presented to the user.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent development of broad-coverage natural language processing systems has stimulated work on the evaluation of the syntactic component of such systems, for purposes of basic evaluation and improvement of system performance.", "labels": [], "entities": []}, {"text": "Methods utilizing hand-bracketed corpora (such as the University of Pennsylvania Treebank) as a basis for evaluation metrics have been discussed in,, and.", "labels": [], "entities": [{"text": "Pennsylvania Treebank", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.7064334452152252}]}, {"text": "Three metrics discussed in those works were the Crossing Parenthesis Score (a count of the number of phrases in the machine produced parse which cross with one or more phrases in the hand parse), Recall (the percentage of phrases in the hand parse that are also in the machine parse), and Precision (the percentage of phrases in the machine parse that are in the hand parse).", "labels": [], "entities": [{"text": "Recall", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9834123849868774}, {"text": "Precision", "start_pos": 289, "end_pos": 298, "type": "METRIC", "confidence": 0.9782716631889343}]}, {"text": "We have developed a methodology for using hand-bracketed parses to examine both the internal and external performance of a grammar checker.", "labels": [], "entities": []}, {"text": "The internal performance refers to the behavior of the underlying system--i.e, the tokenizer, parser, lexicon, and grammar.", "labels": [], "entities": []}, {"text": "The external performance refers to the error critiques generated by the system.", "labels": [], "entities": []}, {"text": "1 Our evaluation methodology relies on three separate error reports generated from a corpus of randomly selected sentences: 1) a report based on unbracketed sentences, 2) a report based on optimally bracketed sentences with our current system, and 3) a report based on the optimal bracketings with the system modified to insure the same coverage as the unbracketed corpus.", "labels": [], "entities": []}, {"text": "The bracketed report from the unmodified system tells us something about the coverage of our underlying system in its current state.", "labels": [], "entities": []}, {"text": "The bracketed report from the modified system tells us something about the external accuracy of the error reports presented to the user.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9819214344024658}]}, {"text": "Our underlying system uses a bottom-up, funambiguity parser.", "labels": [], "entities": []}, {"text": "Our error detection method relies on including grammar rules for parsing errorful sentences, with error critiques being generated from the occurrence of an error rule in the parse.", "labels": [], "entities": [{"text": "error detection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.705690547823906}]}, {"text": "Error critiques are based on just one of all the possible parse trees that the system can find fora given sentence.", "labels": [], "entities": []}, {"text": "Our major concern about the underlying system is whether the system has a correct parse for the sentence in question.", "labels": [], "entities": []}, {"text": "We are also concerned about the accuracy of the selected parse, but our current methodology does not directly address that issue, because correct error reports do not depend on having precisely the correct parse.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9987965822219849}]}, {"text": "Consequently, our evaluation of the underlying grammatical coverage is based on a simple metric, namely the parser success rate for satisfying sentence bracketings (i.e. correct parses).", "labels": [], "entities": [{"text": "sentence bracketings", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.7321544885635376}]}, {"text": "Either the parser can produce the optimal parse or it can't.", "labels": [], "entities": []}, {"text": "We have a more complex approach to evaluating the performance of the system's ability to detect errors.", "labels": [], "entities": []}, {"text": "Here, we need to look at both the 1.", "labels": [], "entities": []}, {"text": "We use the term critique to represent an instance of an error detected.", "labels": [], "entities": []}, {"text": "Each sentence may have zero or more critiques reported for it. overgeneration and undergeneration of individual error critiques.", "labels": [], "entities": []}, {"text": "What is the rate of spurious critiques, or critiques incorrectly reported, and what is the rate of missed critiques, or critiques not reported.", "labels": [], "entities": []}, {"text": "Therefore we define two additional metrics, which illuminate the spurious and missed critique rates, respectively: Precision: the percentage of correct critiques from the unbracketed corpus.", "labels": [], "entities": [{"text": "Precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9966816306114197}]}, {"text": "Recall: the percentage of critiques generated from an ideal bracketed corpus that are also present among those in the unbracketed corpus.", "labels": [], "entities": []}, {"text": "Precision tells us what percentage of reported critiques are reliable, and Recall tells iJs what percentage of correct critiques have been reported (modulo the coverage).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7803357839584351}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Overview Of The Results", "labels": [], "entities": [{"text": "Overview", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8623211979866028}]}]}