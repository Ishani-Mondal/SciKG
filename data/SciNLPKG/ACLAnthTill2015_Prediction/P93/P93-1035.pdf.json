{"title": [{"text": "Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach", "labels": [], "entities": [{"text": "Automatic Grammar Induction and Parsing Free Text", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7249101400375366}]}], "abstractContent": [{"text": "In this paper we describe anew technique for parsing free text: a transformational grammar I is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.", "labels": [], "entities": [{"text": "parsing free text", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8964146375656128}, {"text": "parsing text into binary-branching syntactic trees", "start_pos": 148, "end_pos": 198, "type": "TASK", "confidence": 0.7930513819058737}]}, {"text": "The algorithm works by beginning in a very naive state of knowledge about phrase structure.", "labels": [], "entities": []}, {"text": "By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error.", "labels": [], "entities": []}, {"text": "After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.", "labels": [], "entities": [{"text": "automatic grammar induction", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.6419083972771963}]}], "introductionContent": [{"text": "There has been a great deal of interest of late in the automatic induction of natural language grammar.", "labels": [], "entities": [{"text": "automatic induction of natural language grammar", "start_pos": 55, "end_pos": 102, "type": "TASK", "confidence": 0.775827944278717}]}, {"text": "Given the difficulty inherent in manually building a robust parser, along with the availability of large amounts of training material, automatic grammar induction seems like a path worth pursuing.", "labels": [], "entities": [{"text": "automatic grammar induction", "start_pos": 135, "end_pos": 162, "type": "TASK", "confidence": 0.6537377933661143}]}, {"text": "A number of systems have been built that can be trained automatically to bracket text into syntactic constituents.", "labels": [], "entities": []}, {"text": "In (MM90) mutual information statistics are extracted from a corpus of text and this information is then used to parse new text.", "labels": [], "entities": [{"text": "MM90", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9014671444892883}]}, {"text": "(Sam86) defines a function to score the quality of parse trees, and then uses simulated annealing to heuristically explore the entire space of possible parses fora given sentence.", "labels": [], "entities": []}, {"text": "In (BM92a), distributional analysis techniques are applied to a large corpus to learn a context-free grammar.", "labels": [], "entities": [{"text": "BM92a", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9560828804969788}]}, {"text": "1 Not in the traditional sense of the term.", "labels": [], "entities": []}, {"text": "based on the inside-outside algorithm, which can be used to train stochastic context-free grammars.", "labels": [], "entities": []}, {"text": "The inside-outside algorithm is an extension of the finite-state based Hidden Markov Model (by (Bak79)), which has been applied successfully in many areas, including speech recognition and part of speech tagging.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.8580990135669708}, {"text": "speech tagging", "start_pos": 197, "end_pos": 211, "type": "TASK", "confidence": 0.745934784412384}]}, {"text": "A number of recent papers have explored the potential of using the insideoutside algorithm to automatically learn a grammar (LY90, SJM90, PS92, BW92, CC92, SRO93).", "labels": [], "entities": [{"text": "BW92", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.619473397731781}, {"text": "SRO93", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.9066040515899658}]}, {"text": "Below, we describe anew technique for grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.8148052096366882}]}, {"text": "The algorithm works by beginning in a very naive state of knowledge about phrase structure.", "labels": [], "entities": []}, {"text": "By repeatedly comparing the results of parsing in the current state to the proper phrase structure for each sentence in the training corpus, the system learns a set of ordered transformations which can be applied to reduce parsing error.", "labels": [], "entities": []}, {"text": "We believe this technique has advantages over other methods of phrase structure induction.", "labels": [], "entities": [{"text": "phrase structure induction", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.8624962766965231}]}, {"text": "Some of the advantages include: the system is very simple, it requires only a very small set of transformations, a high degree of accuracy is achieved, and only a very small training corpus is necessary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9992567896842957}]}, {"text": "The trained transformational parser is completely symbolic and can bracket text in linear time with respect to sentence length.", "labels": [], "entities": []}, {"text": "In addition, since some tokens in a sentence are not even considered in parsing, the method could prove to be considerably more robust than a CFG-based approach when faced with noise or unfamiliar input.", "labels": [], "entities": []}, {"text": "After describing the algorithm, we present results and compare these results to other recent results in automatic phrase structure induction.", "labels": [], "entities": [{"text": "phrase structure induction", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.7787564893563589}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparing two learning methods on the  ATIS corpus.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9546305537223816}]}, {"text": " Table 4: WSJ Sentences of Length 2 to 20.", "labels": [], "entities": [{"text": "WSJ Sentences", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6956392824649811}]}]}