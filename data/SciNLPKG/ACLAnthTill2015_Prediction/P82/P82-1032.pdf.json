{"title": [{"text": "A Model of Early Syntactic Development", "labels": [], "entities": [{"text": "Syntactic Development", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8210639655590057}]}], "abstractContent": [{"text": "AMBER is a model of first language acquisition that improves its performance through a process of error recovery.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7178599834442139}, {"text": "first language acquisition", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.6659627159436544}]}, {"text": "The model is implemented as an adaptive production system that introduces new condition-action rules on the basis of experience.", "labels": [], "entities": []}, {"text": "AMBER starts with the ability to say only one word at a time, but adds rules for ordering goals and producing grammatical morphemes, based on comparisons between predicted and observed sentences.", "labels": [], "entities": []}, {"text": "The morpheme rules maybe overly general and lead to errors of commission; such errors evoke a discrimination process, producing more conservative rules with additional conditions.", "labels": [], "entities": []}, {"text": "The system's performance improves gradually, since rules must be relearned many times before they are used.", "labels": [], "entities": []}, {"text": "AMBER'S learning mechanisms account for some of the major developments observed in children's early speech.", "labels": [], "entities": [{"text": "AMBER'S", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7004938125610352}]}], "introductionContent": [{"text": "In this paper, I present a model that attempts to explain the regularities in children's early syntactic development.", "labels": [], "entities": []}, {"text": "The model is called AMBER, an acronym for Acquisition Model Based on Error Recovery.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.7705514430999756}]}, {"text": "As its name implies, AMBER learns language by comparing its own utterances to those of adults and attempting to correct any errors.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 21, "end_pos": 26, "type": "TASK", "confidence": 0.9198145270347595}]}, {"text": "The model is implemented as an adaptive production system -a formalism well-suited to modeling the incremental nature of human learning.", "labels": [], "entities": []}, {"text": "AMEER focuses on issues such as the omission of content words, the occurrence of telegraphic speech, and the order in which function words are mastered.", "labels": [], "entities": [{"text": "AMEER", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5153210163116455}]}, {"text": "Before considering AMBER in detail, I will first review some major features of child language, and discuss some earlier models of these phenomena.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.9712283611297607}]}, {"text": "Children do not learn language in an all.or.none fashion.", "labels": [], "entities": []}, {"text": "They begin their linguistic careers uttering one word at a time, and slowly evolve through a number of stages, each containing more adult-like speech than the one before.", "labels": [], "entities": []}, {"text": "Around the age of one year, the child begins to produce words in isolation, and continues this strategy for some months.", "labels": [], "entities": []}, {"text": "At approximately 18 months, the child begins to combine words into meaningful sequences.", "labels": [], "entities": []}, {"text": "In order-based languages such as English, the child usually follows the adult order.", "labels": [], "entities": []}, {"text": "Initially only pairs of words are produced, but these are followed by three-word and later by four-word utterances.", "labels": [], "entities": []}, {"text": "The simple sentences occurring in this stage consist almost entirely of content words, while grammatical morphemes such as tense endings and prepositions are largely absent.", "labels": [], "entities": []}, {"text": "During the period from about 24 to 40 months, the child masters the grammatical morphemes which were absent during the previous stage.", "labels": [], "entities": []}, {"text": "These \"function words\" are learned gradually; the time between the initial production of a morpheme and its mastery maybe as long as 16 months. has examined the order in which 14 English morphemes are acquired, finding the order of acquisition to be remarkably consistent across children.", "labels": [], "entities": []}, {"text": "In addition, those morphemes with simpler meanings and involved in fewer transformations are learned earlier than more complex ones.", "labels": [], "entities": []}, {"text": "These findings place some strong constraints on the learning mechanisms one postulates for morpheme acquisition.", "labels": [], "entities": [{"text": "morpheme acquisition", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7844360172748566}]}, {"text": "Now that we have reviewed some of the major aspects of child language, let us consider the earlier attempts at modeling these phenomena.", "labels": [], "entities": []}, {"text": "Computer programs that learn language can be usefully divided into two groups: those which take advantage of semantic feedback, and those which do not.", "labels": [], "entities": []}, {"text": "In general, the early work concerned itself with learning grammars in the absence of information about the meaning of sentences.", "labels": [], "entities": []}, {"text": "Examples of this approach can be found in, and.", "labels": [], "entities": []}, {"text": "Since children almost certainly have semantic information available to them, I will not focus on their research here.", "labels": [], "entities": []}, {"text": "However, much of the early work is interesting in its own right, and some excellent systems along these lines have recently been produced by and.", "labels": [], "entities": []}, {"text": "In the late 1960's, some researchers began to incorporate semantic information into their language learning systems.", "labels": [], "entities": []}, {"text": "The majority of the resulting programs showed little concern with the observed phenomena, including Siklossy's ZBIE (1972),, Hedrick's production system model (1976),, and Sembugamoorthy's PLAS.", "labels": [], "entities": [{"text": "Siklossy's ZBIE (1972)", "start_pos": 100, "end_pos": 122, "type": "DATASET", "confidence": 0.8239955206712087}]}, {"text": "These systems failed as models of human language acquisition in two major areas.", "labels": [], "entities": [{"text": "human language acquisition", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.6405890782674154}]}, {"text": "First, they learned language in an all-or.none manner, and much too rapidly to provide useful models of child language.", "labels": [], "entities": []}, {"text": "Second, these systems employed conservative learning strategies in the hope of avoiding errors.", "labels": [], "entities": []}, {"text": "In contrast, children themselves make many errors in their early constructions, but eventually recover from them.", "labels": [], "entities": []}, {"text": "However, a few researchers have attempted to construct plausible models of the child's learning process.", "labels": [], "entities": []}, {"text": "For example, has described an \"hypothesis testing\" model that learned successively more complex phrase structure grammars for parsing simple sentences.", "labels": [], "entities": [{"text": "parsing simple sentences", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.8629027803738912}]}, {"text": "As new syntactic classes became available, the program rejected its current grammar in favor of a more accurate one.", "labels": [], "entities": []}, {"text": "Thus, the model moved from a stage in which individual words were viewed as \"things\" to the more sophisticated view that \"subjects\" precede \"actions\".", "labels": [], "entities": []}, {"text": "One drawback of the model was that it could not learn new categories on its own initiative; instead, the author was forced to introduce them manually.", "labels": [], "entities": []}, {"text": "Reeker (1976) has described PST, another theory of early syntactic development.", "labels": [], "entities": [{"text": "PST", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9558205604553223}, {"text": "early syntactic development", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6197042961915334}]}, {"text": "This model assumed that children have limited short term memories, so that they store onty portions of an adult sample sentence.", "labels": [], "entities": []}, {"text": "The model compared this reduced sentence to an internally generated utterance, and differences between the two were noted.", "labels": [], "entities": []}, {"text": "Six types of differences were recognized (missing prefixes, missing suffixes, missing infixes, substitutions, extra words, and transpositions), and each led to an associated alteration of the grammar.", "labels": [], "entities": []}, {"text": "PST accounted for children's omission of content words and the gradual increase in utterance length.", "labels": [], "entities": [{"text": "PST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9181761741638184}]}, {"text": "The limited memory hypothesis also explained the telegraphic nature of early speech, though Reeker did not address the issue of function word acquisition.", "labels": [], "entities": [{"text": "function word acquisition", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.6460776329040527}]}, {"text": "Overgeneralizations did occur in PST, but the model could revise its grammar upon their discovery, so as to avoid similar errors in the future.", "labels": [], "entities": [{"text": "PST", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8508346080780029}]}, {"text": "PST also helped account for the incremental nature of language acquisition, since differences were addressed one at a time and the grammar changed only slowly. has described CHILD, another program that attempted to explain some of the basic phenomena of first language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7025162428617477}, {"text": "first language acquisition", "start_pos": 254, "end_pos": 280, "type": "TASK", "confidence": 0.6671396493911743}]}, {"text": "This system began by learning the meanings of words in terms of a conceptual dependency representation.", "labels": [], "entities": []}, {"text": "Word meanings were initially overly specific, but were generalized as more examples were encountered.", "labels": [], "entities": []}, {"text": "As more words were learned and their definitions became less restrictive, the length of CHILD'S utterances increased.", "labels": [], "entities": [{"text": "length", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9941190481185913}, {"text": "CHILD'S utterances", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.6510430723428726}]}, {"text": "CHILD differed from other models of language learning by incorporating, a nonlinguistic component.", "labels": [], "entities": []}, {"text": "This enabled the system to correctly respond to adult sentences such as Put the ba/I in the box, and led to the appearance that the system understood language before it could produce it.", "labels": [], "entities": []}, {"text": "Of course, this strategy sometimes led to errors in comprehension.", "labels": [], "entities": []}, {"text": "Coupled with the disapproval of a tutor, such errors were one of the major spurs to the learning of word orders.", "labels": [], "entities": [{"text": "learning of word orders", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7053802907466888}]}, {"text": "Syntactic knowledge was stored with the meanings of words, so that the acquisition of syntax necessarily occurred after the acquisition of individual words.", "labels": [], "entities": []}, {"text": "Although tl~ese systems fare much better as psychological models than other language learning programs, they have some important limitations.", "labels": [], "entities": []}, {"text": "We have seen that Kelley's system required syntactic classes to be introduced by hand, making his explanation less than satisfactory.", "labels": [], "entities": []}, {"text": "Selfridge's CHILD was much more robust than Kelley's program, and was unique in modeling children's use of nonlinguistic cues for understanding.", "labels": [], "entities": [{"text": "Selfridge's CHILD", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9125870664914449}]}, {"text": "However, CHILD'S explanation for the omission of content words -that those words are not yet known -was implausible, since children often omit words that they have used in previous utterances.", "labels": [], "entities": []}, {"text": "Reeker's PST explained this phenomenon through a limited memory hypothesis, which is consistent with our knowledge of children's memory skills.", "labels": [], "entities": [{"text": "PST", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.7304466366767883}]}, {"text": "Still, PST included no model of the process through which memory improved; in order to simulate the acquisition of longer constructions, Reeker would have had to increase the system's memory size by hand.", "labels": [], "entities": [{"text": "PST", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.7604818344116211}]}, {"text": "Both CHILD and PST learned relatively slowly, and made mistakes of the general type observed with children.", "labels": [], "entities": [{"text": "CHILD", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.7933372855186462}, {"text": "PST", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9223965406417847}]}, {"text": "Both systems addressed the issue of error recovery, starting off as abominable language users, but getting progressively better with time.", "labels": [], "entities": [{"text": "error recovery", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.6614821553230286}]}, {"text": "This is a promising approach that I' attempt to develop it in its extreme form in the following pages.", "labels": [], "entities": []}, {"text": "2. An Overview of AMBER Although Reeker's PST and Selfridge's CHILD address the transition from one-word to multi-word utterances, we have seen that problems exist with both accounts.", "labels": [], "entities": [{"text": "Reeker's PST", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.6924523909886678}, {"text": "Selfridge's CHILD", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.846184770266215}]}, {"text": "Neither of these programs focus on the acquisition of function words, their explanations of content word omissions leave something to be desired, and though they learn more slowly than other systems, they still learn more rapidly than children.", "labels": [], "entities": []}, {"text": "In response to these limitations, the goals of the current research are: \u2022 Account for the omission of content\" words, and the eventual recovery from such omissions.", "labels": [], "entities": [{"text": "Account", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9981301426887512}]}, {"text": "\u2022 Account for the omission of function words, and the order in which these morphemes are mastered.", "labels": [], "entities": [{"text": "Account", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9988431930541992}]}, {"text": "\u2022 Account for the gradual nature of both these linguistic developments.", "labels": [], "entities": [{"text": "Account", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9977745413780212}]}, {"text": "In this section I provide an overview of AMBER, a model that provides one set of answers to these questions.", "labels": [], "entities": [{"text": "AMBER", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.3403855264186859}]}, {"text": "Since more is known about children's utterances than their ability to understand the utterances of others, AMBER models the learning of generation strategies, rather than strategies for understanding language.", "labels": [], "entities": []}, {"text": "Selfridge's and Reeker's models differ from other language learning systems in their concern with the problem of recovering from errors.", "labels": [], "entities": [{"text": "Selfridge", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9444242119789124}]}, {"text": "The current research extends this idea even further, since all of AMBER'S learning strategies operate through a process of error recovery.", "labels": [], "entities": []}, {"text": "1 The model is presented with three pieces of information: a legal sentence, an event to be described, and a main goal or topic of the sentence.", "labels": [], "entities": []}, {"text": "An event is represented as a semantic network, using relations like agent, action, object, size, color, and type.", "labels": [], "entities": []}, {"text": "The specification of one of the nodes as the main topic allows the system to restate the network as a tree structure, and it is from this tree that AMBER generates a sentence.", "labels": [], "entities": []}, {"text": "If this sentence is identical to the sample sentence, no learning is required.", "labels": [], "entities": []}, {"text": "If a disagreement between the two sentences is found, AMBER modifies its set of rules in an attempt to avoid similar errors in the future, and the system moves onto the next example.", "labels": [], "entities": []}, {"text": "AMBER'S performance system is stated as a set of conditionaction rules or productions that operate upon the goal tree to produce utterances.", "labels": [], "entities": [{"text": "AMBER'S", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7632270455360413}]}, {"text": "Although the model starts with the potential for producing (unordered) telegraphic sentences, it can initially generate only one word at a time.", "labels": [], "entities": []}, {"text": "To see why this occurs, we must consider the three productions that makeup AMBER'S initial performance system.", "labels": [], "entities": []}, {"text": "The first rule (the start rul~) is responsible for establishing subgoals; it maybe paraphrased as:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}