{"title": [{"text": "A Unified Local and Global Model for Discourse Coherence", "labels": [], "entities": [{"text": "Discourse Coherence", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8001128733158112}]}], "abstractContent": [{"text": "We present a model for discourse coherence which combines the local entity-based approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004).", "labels": [], "entities": []}, {"text": "Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful.", "labels": [], "entities": []}, {"text": "As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively.", "labels": [], "entities": []}, {"text": "Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on.", "labels": [], "entities": []}], "introductionContent": [{"text": "Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation () and evaluation of human-produced text in educational applications).", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6685408353805542}, {"text": "text generation", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7837000489234924}]}, {"text": "Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization ( ).", "labels": [], "entities": []}, {"text": "Models of coherence tend to fall into two classes.", "labels": [], "entities": []}, {"text": "Local models) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words.", "labels": [], "entities": []}, {"text": "Models of this type are good at finding sentences that belong near one another in the document.", "labels": [], "entities": []}, {"text": "However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries).", "labels": [], "entities": [{"text": "finding the beginning or end of the document", "start_pos": 27, "end_pos": 71, "type": "TASK", "confidence": 0.7199928537011147}]}, {"text": "Some local models also have trouble deciding which of a pair of related sentences ought to come first.", "labels": [], "entities": []}, {"text": "In contrast, the global HMM model of tries to track the predictable changes in topic between sentences.", "labels": [], "entities": []}, {"text": "This gives it a pronounced advantage in ordering sentences, since it can learn to represent beginnings, ends and boundaries as separate states.", "labels": [], "entities": []}, {"text": "However, it has no local features; the particular words in each sentence are generated based only on the current state of the document.", "labels": [], "entities": []}, {"text": "Since information can pass from sentence to sentence only in this restricted manner, the model sometimes fails to place sentences next to the correct neighbors.", "labels": [], "entities": []}, {"text": "We attempt hereto unify the two approaches by constructing a model with both sentence-to-sentence dependencies providing local cues, and a hidden topic variable for global structure.", "labels": [], "entities": []}, {"text": "Our local features are based on the entity grid model of ( ).", "labels": [], "entities": []}, {"text": "This model has previously been most successful in a conditional setting; to integrate it into our model, we first relax its independence assumptions to improve its performance when used generatively.", "labels": [], "entities": []}, {"text": "Our global model is an HMM like that of, but with emission probabilities drawn from the entity grid.", "labels": [], "entities": []}, {"text": "We present results for two tasks, the ordering task, on which global models usually do well, and the discrimination task, on which local models tend to outperform them.", "labels": [], "entities": []}, {"text": "Our model improves on purely global or local approaches on both tasks.", "labels": [], "entities": []}, {"text": "Previous work by has also attempted to integrate local and global features using a mixture model, with promising results.", "labels": [], "entities": []}, {"text": "However, mixture models lack explanatory power; since each of the individual component models is known to be flawed, it is difficult to say that the combination is theoretically more sound than the parts, even if it usually works better.", "labels": [], "entities": []}, {"text": "Moreover, since the model we describe uses a strict subset of the features used in the component models of), we suspect that adding it to the mixture would lead to still further improved results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments use the popular AIRPLANE corpus, a collection of documents describing airplane crashes taken from the database of the National Transportation Safety Board, used in ().", "labels": [], "entities": [{"text": "AIRPLANE corpus", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.7896998226642609}]}, {"text": "We use the standard division of the corpus into 100 training and 100 test documents; for development purposes we did 10-fold cross-validation on the training data.", "labels": [], "entities": []}, {"text": "The AIRPLANE documents have some advantages for coherence research: they are short (11.5 sentences on average) and quite formulaic, which makes it easy to find lexical and structural patterns.", "labels": [], "entities": [{"text": "AIRPLANE documents", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.916616290807724}]}, {"text": "On the other hand, they do have some oddities.", "labels": [], "entities": []}, {"text": "46 of the training documents begin with a standard preamble: \"This is preliminary information, subject to change, and may contain errors.", "labels": [], "entities": []}, {"text": "Any errors in this report will be corrected when the final report has been completed,\" which essentially gives coherence models the first two sentences for free.", "labels": [], "entities": []}, {"text": "Others, however, begin abruptly with no introductory material whatsoever, and sometimes without even providing references for their definite noun phrases; one document begins: \"At V1, the DC-10-30's number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.\"", "labels": [], "entities": [{"text": "V1", "start_pos": 180, "end_pos": 182, "type": "DATASET", "confidence": 0.5837165713310242}]}, {"text": "Even humans might have trouble identifying this sentence as the beginning of a document.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for 10-fold cross-validation on AIR- PLANE training data.", "labels": [], "entities": [{"text": "AIR- PLANE training data", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.5992588639259339}]}]}