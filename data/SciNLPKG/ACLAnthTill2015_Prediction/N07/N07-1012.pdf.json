{"title": [], "abstractContent": [{"text": "We explore the problem of retrieving semi-structured documents from a real-world collection using a structured query.", "labels": [], "entities": []}, {"text": "We formally develop Structured Relevance Models (SRM), a retrieval model that is based on the idea that plausible values fora given field could be inferred from the context provided by the other fields in the record.", "labels": [], "entities": [{"text": "Structured Relevance Models (SRM)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.6790695240100225}]}, {"text": "We then carryout a set of experiments using a snapshot of the National Science Digital Library (NSDL) repository, and queries that only mention fields missing from the test data.", "labels": [], "entities": [{"text": "National Science Digital Library (NSDL) repository", "start_pos": 62, "end_pos": 112, "type": "DATASET", "confidence": 0.9650879874825478}]}, {"text": "For such queries, typical field matching would retrieve no documents at all.", "labels": [], "entities": []}, {"text": "In contrast, the SRM approach achieves a mean average precision of over twenty percent.", "labels": [], "entities": [{"text": "SRM", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9847546815872192}, {"text": "mean average", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.8604340851306915}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.5110238194465637}]}], "introductionContent": [{"text": "This study investigates information retrieval on semi-structured information, where documents consist of several textual fields that can be queried independently.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7884861528873444}]}, {"text": "If documents contained subject and author fields, for example, we would expect to see queries looking for documents about theory of relativity by the author Einstein.", "labels": [], "entities": []}, {"text": "This setting suggests exploring the issue of inexact match-is special theory of relativity relevant?-that has been explored elsewhere.", "labels": [], "entities": []}, {"text": "Our interest is in an extreme case of that problem, where the content of afield is not corrupted or incorrect, but is actually absent.", "labels": [], "entities": []}, {"text": "We wish to find relevant information in response to a query such as the one above even if a relevant document is completely missing the subject and author fields.", "labels": [], "entities": []}, {"text": "Our research is motivated by the challenges we encountered in working with the National Science Digital Library (NSDL) collection.", "labels": [], "entities": [{"text": "National Science Digital Library (NSDL) collection", "start_pos": 79, "end_pos": 129, "type": "DATASET", "confidence": 0.9643355906009674}]}, {"text": "Each item in the collection is a scientific resource, such as a research paper, an educational video, or perhaps an entire website.", "labels": [], "entities": []}, {"text": "In addition to its main content, each resource is annotated with metadata, which provides information such as the author or creator of the resource, its subject area, format (text/image/video) and intended audience -in allover 90 distinct fields (though some are very related).", "labels": [], "entities": []}, {"text": "Making use of such extensive metadata in a digital library paves the way for constructing highly-focused models of the user's information need.", "labels": [], "entities": []}, {"text": "These models have the potential to dramatically improve the user experience in targeted applications, such as the NSDL portals.", "labels": [], "entities": []}, {"text": "To illustrate this point, suppose that we are running an educational portal targeted at elementary school teachers, and some user requests teaching aids for an introductory class on gravity.", "labels": [], "entities": []}, {"text": "An intelligent search system would be able to translate the request into a structured query that might look something like: subject='gravity' AND audience='grades 1-4' AND format='image,video' AND rights='free-foracademic-use'.", "labels": [], "entities": []}, {"text": "Such a query can be efficiently answered by a relational database system.", "labels": [], "entities": []}, {"text": "Unfortunately, using a relational engine to query a semi-structured collection similar to NSDL will run into a number of obstacles.", "labels": [], "entities": []}, {"text": "The simplest problem is that natural language fields are filled inconsistently: e.g., the audience field contains values such as K-4, K-6, second grade, and learner, all of which are clearly semantically related.", "labels": [], "entities": []}, {"text": "A larger problem, and the one we focus on in this study, is that of missing fields.", "labels": [], "entities": []}, {"text": "For example 24% of the items in the NSDL collection have no subject field, 30% are missing the author information, and over 96% mention no target audience (reading level).", "labels": [], "entities": [{"text": "NSDL collection", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.950906366109848}]}, {"text": "This means that a relational query for elementary school material will consider at most 4% of all potentially relevant resources in the NSDL collection.", "labels": [], "entities": [{"text": "NSDL collection", "start_pos": 136, "end_pos": 151, "type": "DATASET", "confidence": 0.9678142964839935}]}, {"text": "The goal of our work is to introduce a retrieval model that will be capable of answering complex structured queries over a semi-structured collection with corrupt and missing field values.", "labels": [], "entities": [{"text": "answering complex structured queries", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.8246608674526215}]}, {"text": "This study focuses on the latter problem, an extreme version of the former.", "labels": [], "entities": []}, {"text": "Our approach is to use a generative model to compute how plausible a word would appear in a record's empty field given the context provided by the other fields in the record.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We survey previous attempts at handling semistructured data in section 2.", "labels": [], "entities": []}, {"text": "Section 3 will provide the details of our approach, starting with a high-level view, then providing a mathematical framework, and concluding with implementation details.", "labels": [], "entities": []}, {"text": "Section 4 will present an extensive evaluation of our model on the large set of queries over the NSDL collection.", "labels": [], "entities": [{"text": "NSDL collection", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9637621641159058}]}, {"text": "We will summarize our results and suggest directions for future research in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the performance of our model on a January 2005 snapshot of the National Science Digital Library repository.", "labels": [], "entities": [{"text": "National Science Digital Library repository", "start_pos": 73, "end_pos": 116, "type": "DATASET", "confidence": 0.9609846472740173}]}, {"text": "The snapshot contains a total of 656,992 records, spanning 92 distinct (though sometimes related) fields.", "labels": [], "entities": []}, {"text": "6 Only 7 of these fields are present in every record, and half the fields are present in less than 1% of the records.", "labels": [], "entities": []}, {"text": "An average record contains only 17 of the 92 fields.", "labels": [], "entities": []}, {"text": "Our experiments focus on a subset of 5 fields (title, description, subject, content and audience).", "labels": [], "entities": []}, {"text": "These fields were selected for two reasons: (i) they occur frequently enough to allow a meaningful evaluation and (ii) they seem plausible to be included in a potential query.", "labels": [], "entities": []}, {"text": "Of these fields, title represents the title of the resource, description is a very brief abstract, content is a more detailed description (but not the full content) of the resource, subject is a library-like classification of the topic covered by the resource, and audience reflects the target reading level (e.g. elementary school or post-graduate).", "labels": [], "entities": []}, {"text": "Summary statistics for these fields are provided in.", "labels": [], "entities": []}, {"text": "The dataset was randomly split into three subsets: the training set, which comprised 50% of the records and was used for estimating the relevance models as described in section 3.5; the held-out set, which comprised 25% of the data and was used to tune the smoothing parameters \u00b5 i and the bandwidth parameters \u03b1 i ; and the evaluation set, which contained 25% of the records and was used to evaluate the performance of the tuned model . Our experiments are based on a set of 127 automatically generated queries.", "labels": [], "entities": []}, {"text": "We randomly split the queries into two groups, 64 for training and 63 for evaluation.", "labels": [], "entities": []}, {"text": "The queries were constructed by combining two randomly picked subject words with two audience words, and then discarding any combination that had less than 10 exact matches in any of the three subsets of our collection.", "labels": [], "entities": []}, {"text": "This procedure yields queries such as Q 91 ={subject:'artificial intelligence' AND audience='researchers'}, or Q 101 ={subject:'philosophy' AND audience='high school'}.", "labels": [], "entities": []}, {"text": "We evaluate our model by its ability to find \"relevant\" records in the face of missing values.", "labels": [], "entities": []}, {"text": "We de-fine a record w to be relevant to the user's query q if every keyword in q is found in the corresponding field of w.", "labels": [], "entities": []}, {"text": "For example, in order to be relevant to Q 101 a record must contain the word 'philosophy' in the subject field and words 'high' and 'school' in the audience field.", "labels": [], "entities": [{"text": "Q 101", "start_pos": 40, "end_pos": 45, "type": "TASK", "confidence": 0.6822544038295746}]}, {"text": "If either of the keywords is missing, the record is considered non-relevant.", "labels": [], "entities": []}, {"text": "When the testing records are fully observable, achieving perfect retrieval accuracy is trivial: we simply return all records that match all query keywords in the subject and audience fields.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9630470871925354}]}, {"text": "As we stated earlier, our main interest concerns the scenario when parts of the testing data are missing.", "labels": [], "entities": []}, {"text": "We are going to simulate this scenario in a rather extreme manner by completely removing the subject and audience fields from all testing records.", "labels": [], "entities": []}, {"text": "This means that a straightforward approach -matching query fields against record fields -will yield no relevant results.", "labels": [], "entities": []}, {"text": "Our approach will rank testing records by comparing their title, description and content fields against the query-based relevance models, as discussed in section 3.5.", "labels": [], "entities": []}, {"text": "We will use the standard rank-based evaluation metrics: precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9995917677879333}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9985930323600769}]}, {"text": "Let NR be the total number of records relevant to a given query, suppose that the first K records in our ranking contain N K relevant ones.", "labels": [], "entities": []}, {"text": "Precision at rank K is defined as N K K and recall is defined as N K NR . Average precision is defined as the mean precision overall ranks where relevant items occur.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9758619666099548}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.999388575553894}, {"text": "Average", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9845377206802368}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.4999611973762512}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.8413792252540588}]}, {"text": "R-precision is defined as precision at rank K=N R .  The upper half of shows precision at fixed recall levels; the lower half shows precision at different ranks.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9989795088768005}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9987794756889343}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9575556516647339}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9984018206596375}]}, {"text": "The %change column shows relative difference between our model and the baseline bLM.", "labels": [], "entities": []}, {"text": "The improved column shows the number of queries where SRM exceeded bLM vs. the number of queries where performance was different.", "labels": [], "entities": [{"text": "SRM", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.49114659428596497}, {"text": "bLM", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9336733818054199}]}, {"text": "For example, 33/49 means that SRM out-performed bLM on 33 queries out of 63, underperformed on 49\u221233=16 queries, and had exactly the same performance on 63\u221249=14 queries.", "labels": [], "entities": [{"text": "SRM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.867958128452301}]}, {"text": "Bold figures indicate statistically significant differences (according to the sign test with p < 0.05).", "labels": [], "entities": []}, {"text": "The results show that SRM outperforms three baselines in the high-precision region, beating bLM's mean average precision by 29%.", "labels": [], "entities": [{"text": "SRM", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8944368958473206}, {"text": "bLM's mean average precision", "start_pos": 92, "end_pos": 120, "type": "METRIC", "confidence": 0.7524741649627685}]}, {"text": "Useroriented metrics, such as R-precision and precision at 10 documents, are improved by 39.4% and 44.3% respectively.", "labels": [], "entities": [{"text": "R-precision", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9889606833457947}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9992806315422058}]}, {"text": "The absolute performance figures are also very encouraging.", "labels": [], "entities": []}, {"text": "Precision of 28% at rank 10 means that on average almost 3 out of the top 10 records in the ranked list are relevant, despite the requested fields not being available to the model.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9690878987312317}]}, {"text": "We note that SRM continues to outperform bLM until very high recall and until the 100-document cutoff.", "labels": [], "entities": [{"text": "SRM", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6818895936012268}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9995941519737244}]}, {"text": "After that, SRM degrades rapidly with respect to bLM.", "labels": [], "entities": [{"text": "SRM", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7130765914916992}]}, {"text": "We feel the drop in effectiveness is of marginal interest because precision is already well below 10% and few users will be continuing to that depth in the list.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9995760321617126}]}, {"text": "It is encouraging to see that SRM outperforms both cLM, the cheating baseline that takes advantage of the field values that are supposed to be \"missing\", and bMatch, suggesting that best-match retrieval provides a superior strategy for selecting a set of appropriate training records.", "labels": [], "entities": [{"text": "SRM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9483699202537537}]}], "tableCaptions": [{"text": " Table 1: Summary statistics for the five NSDL fields  used in our retrieval experiments.", "labels": [], "entities": []}, {"text": " Table 2: Performance of the 63 test queries retrieving 1000 documents on the evaluation data. Bold figures  show statistically significant differences. Across all 63 queries, there are 1253 relevant documents.", "labels": [], "entities": []}]}