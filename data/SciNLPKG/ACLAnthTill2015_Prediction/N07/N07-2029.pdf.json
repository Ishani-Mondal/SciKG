{"title": [{"text": "Hybrid Document Indexing with Spectral Embedding", "labels": [], "entities": [{"text": "Hybrid Document Indexing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6447386840979258}]}], "abstractContent": [{"text": "Document representation has a large impact on the performance of document retrieval and clustering algorithms.", "labels": [], "entities": [{"text": "Document representation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9076216220855713}, {"text": "document retrieval", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7336372435092926}]}, {"text": "We propose a hybrid document indexing scheme that combines the traditional bag-of-words representation with spectral embedding.", "labels": [], "entities": []}, {"text": "This method accounts for the specifics of the document collection and also uses semantic similarity information based on a large scale statistical analysis.", "labels": [], "entities": []}, {"text": "Clustering experiments showed improvements over the traditional tf-idf representation and over the spectral methods based solely on the document collection.", "labels": [], "entities": []}], "introductionContent": [{"text": "Capturing semantic relations between words in a document representation is a difficult problem.", "labels": [], "entities": [{"text": "Capturing semantic relations between words", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.9027069449424744}]}, {"text": "Different approaches tried to overcome the term independence assumption of the bag-of-words representation () for example by using distributional term clusters) and expanding the document vectors with synonyms, see ( ).", "labels": [], "entities": []}, {"text": "Since content words can be combined into semantic classes there has been a considerable interest in low-dimensional term and document representations.", "labels": [], "entities": []}, {"text": "Latent Semantic Analysis (LSA)) is one of the best known dimensionality reduction algorithms.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA))", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7213177581628164}, {"text": "dimensionality reduction", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.736064076423645}]}, {"text": "In the LSA space documents are indexed with latent semantic concepts.", "labels": [], "entities": [{"text": "LSA space documents", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.8721126119295756}]}, {"text": "LSA showed large performance improvements over the traditional tf-idf representation on small document collections) but often does not perform well on large heterogeneous collections.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7239347100257874}]}, {"text": "LSA maps all words to low dimensional vectors.", "labels": [], "entities": []}, {"text": "However, the notion of semantic relatedness is defined differently for subsets of the vocabulary.", "labels": [], "entities": []}, {"text": "In addition, the numerical information, abbreviations and the documents' style maybe very good indicators of their topic.", "labels": [], "entities": []}, {"text": "However, this information is no longer available after the dimensionality reduction.", "labels": [], "entities": []}, {"text": "We use a hybrid approach to document indexing to address these issues.", "labels": [], "entities": [{"text": "document indexing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6705612391233444}]}, {"text": "We keep the notion of latent semantic concepts and also try to preserve the specifics of the document collection.", "labels": [], "entities": []}, {"text": "We use a lowdimensional representation only for nouns and represent the rest of the document's content as tf-idf vectors.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses our approach.", "labels": [], "entities": []}, {"text": "Section 3 reports the experimental results.", "labels": [], "entities": []}, {"text": "We conclude in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed document clustering experiments to validate our approach.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7384887039661407}]}, {"text": "Subset m-n #topics min #d max #d av.", "labels": [], "entities": []}, {"text": "#d: TDT2 topic subsets containing between m and n documents: the number of topics per subset, the minimum, the maximum and the average number of documents per topic in each subset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: TDT2 topic subsets containing between m  and n documents: the number of topics per subset,  the minimum, the maximum and the average number  of documents per topic in each subset.", "labels": [], "entities": []}, {"text": " Table 2. The vocabulary  size we used with the tf-idf indexing was 114,127.  For computational reasons we used the set of words  that occurred in at least 20 documents with our spec- tral methods. We used 17,633 words for index-ing with LSA and GLSA local and 17,572 words for  GLSA. We also indexed documents using only the  15,325 nouns: tf-idf N and GLSA N . The hybrid rep- resentation was computed using the tf-idf indexing  without nouns and the GLSA N nouns vectors.", "labels": [], "entities": []}, {"text": " Table 3: Clustering accuracy (first row) and F1 score (second row) for each indexing scheme. The measures  are averaged over 30 random initiations of the clustering algorithm, the standard deviation is shown in  brackets. For the last experiment, 5-10 s , we used one document per cluster as the initial assignment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.897412121295929}, {"text": "F1 score", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9935677349567413}]}]}