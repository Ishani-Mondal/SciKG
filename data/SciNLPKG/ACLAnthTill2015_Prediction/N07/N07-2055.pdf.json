{"title": [{"text": "A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we describe automatic information nuggetization and its application to text comparison.", "labels": [], "entities": [{"text": "text comparison", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7912220358848572}]}, {"text": "More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material.", "labels": [], "entities": []}, {"text": "A semi-automatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many natural language processing (NLP) tasks, we are faced with the problem of determining the appropriate granularity level for information units.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.7854888354028974}]}, {"text": "Most commonly, we use sentences to model individual pieces of information.", "labels": [], "entities": []}, {"text": "However, more NLP applications require us to define text units smaller than sentences, essentially decomposing sentences into a collection of phrases.", "labels": [], "entities": []}, {"text": "Each phrase carries an independent piece of information that can be used as a standalone unit.", "labels": [], "entities": []}, {"text": "These finer-grained information units are usually referred to as nuggets.", "labels": [], "entities": []}, {"text": "When performing within-sentence comparison for redundancy and/or relevancy judgments, without a precise and consistent breakdown of nuggets we can only rely on rudimentary n-gram segmentations of sentences to form nuggets and perform subsequent n-gram-wise text comparison.", "labels": [], "entities": []}, {"text": "This is not satisfactory fora variety of reasons.", "labels": [], "entities": []}, {"text": "For example, one n-gram window may contain several separate pieces of information, while another of the same length may not contain even one complete piece of information.", "labels": [], "entities": []}, {"text": "Previous work shows that humans can create nuggets in a relatively straightforward fashion.", "labels": [], "entities": []}, {"text": "In the PYRAMID scheme for manual evaluation of summaries (), machine-generated summaries were compared with human-written ones at the nugget level.", "labels": [], "entities": [{"text": "summaries", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.7899371981620789}]}, {"text": "However, automatic creation of the nuggets is not trivial.", "labels": [], "entities": []}, {"text": "explore the enumeration and combination of all words in a sentence to create the set of all possible nuggets.", "labels": [], "entities": []}, {"text": "Their automation process still requires nuggets to be manually created a priori for reference summaries before any summary comparison takes place.", "labels": [], "entities": []}, {"text": "This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons.", "labels": [], "entities": []}, {"text": "Without the human-created nuggets, text comparison falls back to its dependency on n-grams.", "labels": [], "entities": [{"text": "text comparison", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8265632390975952}]}, {"text": "Similarly, in question-answering (QA) evaluations, gold-standard answers use manually created nuggets and compare them against system-produced answers broken down into n-gram pieces, as shown in POURPRE () and NUGGETEER.", "labels": [], "entities": []}, {"text": "A serious problem in manual nugget creation is the inconsistency inhuman decisions (.", "labels": [], "entities": [{"text": "manual nugget creation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.6336270372072855}]}, {"text": "The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators.", "labels": [], "entities": []}, {"text": "And if the annotation is performed over an extended period of time, the consistency is even lower.", "labels": [], "entities": [{"text": "consistency", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9996170997619629}]}, {"text": "In recent exercises of the PYRAMID evaluation, inconsistent nuggets are flagged by a tracking program and returned back to the annotators, and resolved manually.", "labels": [], "entities": [{"text": "PYRAMID evaluation", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.5741244107484818}]}, {"text": "Given these issues, we address two questions in this paper: First, how do we define nuggets so that they are consistent in definition?", "labels": [], "entities": []}, {"text": "Secondly, how do we utilize automatically extracted nuggets for various evaluation purposes?", "labels": [], "entities": []}], "datasetContent": [{"text": "In recent QA and summarization evaluation exercises, manually created nuggets play a determinate role in judging system qualities.", "labels": [], "entities": [{"text": "QA", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9731948971748352}, {"text": "summarization evaluation", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.814028263092041}]}, {"text": "Although the two task evaluations are similar, the text comparison task in summarization evaluation is more complex because systems are required to produce long responses and thus it is hard to yield high agreement if manual annotations are performed.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.9663800895214081}]}, {"text": "The following experiments are conducted in the realm of summarization evaluation.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9801020324230194}]}], "tableCaptions": []}