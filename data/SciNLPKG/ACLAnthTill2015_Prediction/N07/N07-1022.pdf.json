{"title": [{"text": "Generation by Inverting a Semantic Parser That Uses Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.560728391011556}]}], "abstractContent": [{"text": "This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7757723579804102}, {"text": "tactical natural language generation", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.6315488591790199}]}, {"text": "We present results on using phrase-based SMT for learning to map meaning representations to natural language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.7731480002403259}]}, {"text": "Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9654963612556458}]}, {"text": "Finally, we show that hybridizing these two approaches results in still more accurate generation systems.", "labels": [], "entities": []}, {"text": "Automatic and human evaluation of generated sentences are presented across two domains and four languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper explores the use of statistical machine translation (SMT) methods in natural language generation (NLG), specifically the task of mapping statements in a formal meaning representation language (MRL) into a natural language (NL), i.e. tactical generation.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.8055948118368784}, {"text": "natural language generation (NLG)", "start_pos": 80, "end_pos": 113, "type": "TASK", "confidence": 0.8255295554796854}, {"text": "mapping statements in a formal meaning representation language (MRL)", "start_pos": 140, "end_pos": 208, "type": "TASK", "confidence": 0.6575888286937367}, {"text": "tactical generation", "start_pos": 244, "end_pos": 263, "type": "TASK", "confidence": 0.8600147068500519}]}, {"text": "Given a corpus of NL sentences each paired with a formal meaning representation (MR), it is easy to use SMT to construct a tactical generator, i.e. a statistical model that translates MRL to NL.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9552432298660278}]}, {"text": "However, there has been little, if any, research on exploiting recent SMT methods for NLG.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9928992986679077}]}, {"text": "In this paper we present results on using a recent phrase-based SMT system, PHARAOH (, for NLG.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.8773574233055115}, {"text": "PHARAOH", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9271265864372253}]}, {"text": "Although moderately effec-tive, the inability of PHARAOH to exploit the formal structure and grammar of the MRL limits its accuracy.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.6259645819664001}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9967623949050903}]}, {"text": "Unlike natural languages, MRLs typically have a simple, formal syntax to support effective automated processing and inference.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9688996076583862}]}, {"text": "This MRL structure can also be used to improve language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7361382842063904}]}, {"text": "Tactical generation can also be seen as the inverse of semantic parsing, the task of mapping NL sentences to MRs.", "labels": [], "entities": [{"text": "Tactical generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8728694915771484}, {"text": "semantic parsing", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7365710884332657}]}, {"text": "In this paper, we show how to \"invert\" a recent SMT-based semantic parser, WASP), in order to produce a more effective generation system.", "labels": [], "entities": [{"text": "SMT-based semantic parser", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.9207903146743774}]}, {"text": "WASP exploits the formal syntax of the MRL by learning a translator (based on a statistical synchronous contextfree grammar) that maps an NL sentence to a linearized parse-tree of its MR rather than to a flat MR string.", "labels": [], "entities": [{"text": "WASP", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7218356728553772}]}, {"text": "In addition to exploiting the formal MRL grammar, our approach also allows the same learned grammar to be used for both parsing and generation, an elegant property that has been widely advocated.", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.7074788014094034}]}, {"text": "We present experimental results in two domains previously used to test WASP's semantic parsing ability: mapping NL queries to a formal database query language, and mapping NL soccer coaching instructions to a formal robot command language.", "labels": [], "entities": [{"text": "WASP's semantic parsing", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7112104222178459}]}, {"text": "WASP \u22121 is shown to produce a more accurate NL generator than PHARAOH.", "labels": [], "entities": [{"text": "accurate NL generator", "start_pos": 35, "end_pos": 56, "type": "METRIC", "confidence": 0.7048957844575247}, {"text": "PHARAOH", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.7803179025650024}]}, {"text": "We also show how the idea of generating from linearized parse-trees rather than flat MRs, used effectively in WASP \u22121 , can also be exploited in PHARAOH.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.7174164056777954}]}, {"text": "A version of PHARAOH that exploits this approach is experimentally shown to produce more accurate generators that are more competitive with WASP \u22121 's.", "labels": [], "entities": []}, {"text": "Finally, we also show how (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half.", "labels": [], "entities": []}, {"text": "aspects of PHARAOH's phrase-based model can be used to improve WASP \u22121 , resulting in a hybrid system whose overall performance is the best.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8156523108482361}, {"text": "WASP \u22121", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.7183759907881418}]}], "datasetContent": [{"text": "We evaluated all four SMT-based NLG systems introduced in this paper: PHARAOH, WASP \u22121 , and the hybrid systems, PHARAOH++ and WASP \u22121 ++.", "labels": [], "entities": [{"text": "SMT-based NLG", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.8528389632701874}]}, {"text": "We used the ROBOCUP and GEOQUERY corpora in our experiments.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.7591837644577026}, {"text": "GEOQUERY corpora", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.8259448409080505}]}, {"text": "The ROBOCUP corpus consists of 300 pieces of coach advice taken from the log files of the 2003 ROBOCUP Coach Competition.", "labels": [], "entities": [{"text": "ROBOCUP corpus consists of 300 pieces of coach advice taken from the log files of the 2003 ROBOCUP Coach Competition", "start_pos": 4, "end_pos": 120, "type": "DATASET", "confidence": 0.7268005967140198}]}, {"text": "The advice was written in CLANG and manually translated to English ().", "labels": [], "entities": [{"text": "CLANG", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8350206017494202}]}, {"text": "The average MR length is 29.47 tokens, or 12.82 nodes for linearized parse-trees.", "labels": [], "entities": [{"text": "MR length", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9790110886096954}]}, {"text": "The average sentence length is 22.52.", "labels": [], "entities": []}, {"text": "The GEOQUERY corpus consists of 880 English questions gathered from various sources.", "labels": [], "entities": [{"text": "GEOQUERY corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9474232494831085}]}, {"text": "The questions were manually translated to the functional GEOQUERY language ().", "labels": [], "entities": [{"text": "GEOQUERY language", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.8922501504421234}]}, {"text": "The average MR length is 17.55 tokens, or 5.55 nodes for linearized parse-trees.", "labels": [], "entities": [{"text": "MR length", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9680792987346649}]}, {"text": "The average sentence length is 7.57.", "labels": [], "entities": []}, {"text": "Reference: If our player 2, 3, 7 or 5 has the ball and the ball is close to our goal line ...", "labels": [], "entities": []}, {"text": "PHARAOH++: If player 3 has the ball is in 2 5 the ball is in the area near our goal line ...", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9675583243370056}]}, {"text": "WASP \u22121 ++: If players 2, 3, 7 and 5 has the ball and the ball is near our goal line ...: Results of automatic evaluation; bold type indicates the best performing system (or systems) fora given domain-metric pair (p < 0.05)  We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score () and the NIST score).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 339, "end_pos": 349, "type": "METRIC", "confidence": 0.9530037045478821}, {"text": "NIST score", "start_pos": 361, "end_pos": 371, "type": "DATASET", "confidence": 0.9110142886638641}]}, {"text": "Both MT metrics measure the precision of a translation in terms of the proportion of n-grams that it shares with the reference translations, with the NIST score focusing more on n-grams that are less frequent and more informative.", "labels": [], "entities": [{"text": "MT", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9718034267425537}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.997983455657959}, {"text": "NIST", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.8186742663383484}]}, {"text": "Both metrics have recently been used to evaluate generators).", "labels": [], "entities": []}, {"text": "All systems were able to generate sentences for more than 97% of the input.", "labels": [], "entities": []}, {"text": "shows some sample output of the systems.", "labels": [], "entities": []}, {"text": "Our BLEU scores are not as high as those reported in Langkilde-Geary (2002) and, which are around 0.7-0.9.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9993083477020264}]}, {"text": "However, their work involves the regeneration of automatically parsed text, and the MRs that they use, which are essentially dependency parses, contain extensive lexical information of the target NL.", "labels": [], "entities": []}, {"text": "Automatic evaluation is only an imperfect substitute for human assessment.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6645980179309845}]}, {"text": "While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems), it is best to support these figures with human evaluation, which we did on a small scale.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9974234104156494}, {"text": "NIST", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8289427757263184}]}, {"text": "We recruited 4 native speakers of English with no previous experience with the ROBOCUP and GEOQUERY domains.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.42928722500801086}, {"text": "GEOQUERY", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.770412802696228}]}, {"text": "Each subject was given the same 20 sentences for each domain, randomly chosen from the test sets.", "labels": [], "entities": []}, {"text": "For each sentence, the subjects were asked to judge the output of PHARAOH++ and WASP \u22121 ++ in terms of fluency and adequacy.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9732977747917175}, {"text": "WASP \u22121 ++", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9560171067714691}]}, {"text": "They were presented with the following definition, adapted from  For each generated sentence, we computed the average of the 4 human judges' scores.", "labels": [], "entities": []}, {"text": "No score normalization was performed.", "labels": [], "entities": []}, {"text": "Then we compared the two systems using a paired t-test.", "labels": [], "entities": []}, {"text": "shows that WASP \u22121 ++ produced better generators than PHARAOH++ in the ROBOCUP domain, consistent with the results of automatic evaluation.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.8769367933273315}]}, {"text": "Lastly, we describe our experiments on the multilingual GEOQUERY data set.", "labels": [], "entities": [{"text": "GEOQUERY data set", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9579447905222574}]}, {"text": "The 250-example data set is a subset of the larger GEOQUERY corpus.", "labels": [], "entities": [{"text": "250-example data set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7942944963773092}, {"text": "GEOQUERY corpus", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9788787066936493}]}, {"text": "All English questions in this data set were manually translated into Spanish, Japanese and Turkish, while the corresponding MRs remain unchanged.", "labels": [], "entities": [{"text": "MRs", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.934890627861023}]}, {"text": "shows the results, which are similar to previous results on the larger GEOQUERY corpus.", "labels": [], "entities": [{"text": "GEOQUERY corpus", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9748168587684631}]}, {"text": "WASP \u22121 ++ outperformed PHARAOH++ for some language-metric pairs, but otherwise performed comparably.", "labels": [], "entities": [{"text": "WASP \u22121", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7253988981246948}]}], "tableCaptions": [{"text": " Table 1: Results of automatic evaluation; bold type  indicates the best performing system (or systems)  for a given domain-metric pair (p < 0.05)", "labels": [], "entities": []}, {"text": " Table 2: Results of human evaluation", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.6748582720756531}]}]}