{"title": [{"text": "Tree Revision Learning for Dependency Parsing", "labels": [], "entities": [{"text": "Tree Revision", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6388547122478485}, {"text": "Parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.6227027177810669}]}], "abstractContent": [{"text": "We present a revision learning model for improving the accuracy of a dependency parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9982499480247498}]}, {"text": "The revision stage corrects the output of the base parser by means of revision rules learned from the mistakes of the base parser itself.", "labels": [], "entities": []}, {"text": "Revision learning is performed with a discriminative classi-fier.", "labels": [], "entities": [{"text": "Revision learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9443097710609436}]}, {"text": "The revision stage has linear complexity and preserves the efficiency of the base parser.", "labels": [], "entities": []}, {"text": "We present empirical evaluations on the treebanks of two languages, which show effectiveness in relative error reduction and state of the art accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9968096613883972}]}], "introductionContent": [{"text": "A dependency parse tree encodes useful semantic information for several language processing tasks.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.7151056081056595}]}, {"text": "Dependency parsing is a simpler task than constituent parsing, since dependency trees do not have extra non-terminal nodes and there is no need fora grammar to generate them.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.858075886964798}, {"text": "constituent parsing", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7437437176704407}]}, {"text": "Approaches to dependency parsing either generate such trees by considering all possible spanning trees (), or build a singletree on the flyby means of shift-reduce parsing actions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7485451400279999}]}, {"text": "In particular, and have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications.", "labels": [], "entities": [{"text": "deterministic dependency parsers", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.6820339858531952}, {"text": "information retrieval", "start_pos": 167, "end_pos": 188, "type": "TASK", "confidence": 0.75883549451828}]}, {"text": "We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8603234887123108}]}, {"text": "Similarly to re-ranking, the second stage attempts to improve the output of abase parser.", "labels": [], "entities": []}, {"text": "Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, either the f irst-best or the one constructed by a deterministic shift-reduce parser, as in transformation-based learning.", "labels": [], "entities": []}, {"text": "Parse trees are revised by applying rules which replace incorrect with correct dependencies.", "labels": [], "entities": [{"text": "Parse trees", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7868602275848389}]}, {"text": "These rules are learned by comparing correct parse trees with incorrect trees produced by the base parser on a training corpus.", "labels": [], "entities": []}, {"text": "We use the same training corpus on which the base parser was trained, but this need not be the case.", "labels": [], "entities": []}, {"text": "Hence, we define anew learning task whose output space is a set of revision rules and whose input is a set of features extracted at each node in the parse trees produced by the parser on the training corpus.", "labels": [], "entities": []}, {"text": "A statistical classifier is trained to solve this task.", "labels": [], "entities": []}, {"text": "The approach is more suitable for dependency parsing since trees do not have non-terminal nodes, therefore revisions do not require adding/removing nodes.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8467767238616943}]}, {"text": "However, the method applies to any parser since it only analyzes output trees.", "labels": [], "entities": []}, {"text": "An intuitive motivation for this method is the observation that a dependency parser correctly identifies most of the dependencies in a tree, and only local corrections might be necessary to produce a correct tree.", "labels": [], "entities": []}, {"text": "Performing several parses in order to generate multiple trees would often just repeat the same steps.", "labels": [], "entities": []}, {"text": "This could be avoided by focusing on the points where attachments are incorrect.", "labels": [], "entities": []}, {"text": "In the experiments reported below, on average, the revision stage performs 4.28 corrections per sentence, or one every 6.25 tokens.", "labels": [], "entities": [{"text": "corrections", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9511212706565857}]}, {"text": "In our implementation we adopt a shift-reduce parser which minimizes computational costs.", "labels": [], "entities": []}, {"text": "The resulting two-stage parser has complexity O(n), linear in the length of the sentence.", "labels": [], "entities": [{"text": "O", "start_pos": 46, "end_pos": 47, "type": "METRIC", "confidence": 0.5274551510810852}]}, {"text": "We evaluated our model on the treebanks of English and Swedish.", "labels": [], "entities": []}, {"text": "The experimental results show a relative error reduction of, respectively, 16% and 11% with respect to the base parser, achieving state of accuracy on Swedish.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9994064569473267}, {"text": "Swedish", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.9418677687644958}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3. 20 most frequent revision rules in wsj22.", "labels": [], "entities": []}, {"text": " Table 4. Results on the Wall Street Journal Penn Tree- bank.", "labels": [], "entities": [{"text": "Wall Street Journal Penn Tree- bank", "start_pos": 25, "end_pos": 60, "type": "DATASET", "confidence": 0.9485722269330706}]}, {"text": " Table 5. Results on the Swedish Treebank.", "labels": [], "entities": [{"text": "Swedish Treebank", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8681282103061676}]}]}