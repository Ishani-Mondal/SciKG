{"title": [{"text": "Combining Outputs from Multiple Machine Translation Systems", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7329435348510742}]}], "abstractContent": [{"text": "Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8654275178909302}]}, {"text": "These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.960212230682373}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8987405300140381}]}, {"text": "The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems.", "labels": [], "entities": []}, {"text": "This paper describes three different approaches to MT system combination.", "labels": [], "entities": [{"text": "MT system combination", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.9322298566500345}]}, {"text": "These combination methods operate on sentence, phrase and word level exploiting information from \u00a4-best lists, system scores and target-to-source phrase alignments.", "labels": [], "entities": []}, {"text": "The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods.", "labels": [], "entities": [{"text": "NIST MT05", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.8752920925617218}, {"text": "GALE 2006 dry-run", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.8426279028256735}]}], "introductionContent": [{"text": "In recent years, machine translation systems based on new paradigms have emerged.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7975295782089233}]}, {"text": "These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.6108659952878952}]}, {"text": "For example, hierarchical and syntax-based () systems have recently improved in both accuracy and scalability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9990720748901367}]}, {"text": "Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8028159737586975}]}, {"text": "System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years.", "labels": [], "entities": [{"text": "speech recognition evaluation", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.8345898588498434}]}, {"text": "Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9953658580780029}]}, {"text": "One of the most successful approaches is consensus network decoding () which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position.", "labels": [], "entities": [{"text": "consensus network decoding", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7836204171180725}]}, {"text": "This requires aligning the system outputs to form a consensus network and -during decoding -simply finding the highest scoring path through this network.", "labels": [], "entities": []}, {"text": "The alignment of speech recognition outputs is fairly straightforward due to the strict constraint in word order.", "labels": [], "entities": [{"text": "alignment of speech recognition outputs", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.7186908662319184}]}, {"text": "However, machine translation outputs do not have this constraint as the word order maybe different between the source and target languages.", "labels": [], "entities": [{"text": "machine translation outputs", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.7944277922312418}]}, {"text": "MT systems employ various re-ordering (distortion) models to take this into account.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.932084858417511}]}, {"text": "Three MT system combination methods are presented in this paper.", "labels": [], "entities": [{"text": "MT system combination", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.8843271931012472}]}, {"text": "They operate on the sentence, phrase and word level.", "labels": [], "entities": []}, {"text": "The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists.", "labels": [], "entities": []}, {"text": "This method does not generate new hypotheses -unlike the phrase and word-level methods.", "labels": [], "entities": []}, {"text": "The phrase-level combination is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table.", "labels": [], "entities": [{"text": "extracting sentence-specific phrase translation", "start_pos": 41, "end_pos": 88, "type": "TASK", "confidence": 0.6418041214346886}]}, {"text": "This approach is similar to the multi-engine MT framework proposed in which is not capable of re-ordering.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.7475235462188721}]}, {"text": "The word-level combination is based on consensus network decoding.", "labels": [], "entities": []}, {"text": "Translation edit rate (TER)) is used to align the hypotheses and minimum Bayes risk decoding under TER () is used to select the alignment hypothesis.", "labels": [], "entities": [{"text": "Translation edit rate (TER))", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.8189684351285299}, {"text": "TER", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9857800602912903}]}, {"text": "All combination methods use weights which maybe tuned using Powell's method on \u00a4 -best lists.", "labels": [], "entities": []}, {"text": "Both sentence and phrase-level combination methods can generate \u00a4 -best lists which may also be used as new system outputs in the word-level combination.", "labels": [], "entities": []}, {"text": "Experiments on combining six machine translation system outputs were performed.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7305482923984528}]}, {"text": "Three systems were phrasal, two hierarchical and one syntaxbased.", "labels": [], "entities": []}, {"text": "The systems were evaluated on NIST MT05 and the newsgroup portion of the GALE 2006 dryrun sets.", "labels": [], "entities": [{"text": "NIST MT05", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.8141781687736511}, {"text": "GALE 2006 dryrun sets", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.9498236328363419}]}, {"text": "The outputs were evaluated on both TER and BLEU.", "labels": [], "entities": [{"text": "TER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9873384237289429}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9931268692016602}]}, {"text": "As the target evaluation metric in the GALE program was human-mediated TER (HTER)), it was found important to improve both of these automatic metrics.", "labels": [], "entities": [{"text": "GALE", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.7259779572486877}, {"text": "TER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.8927308320999146}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the evaluation metrics and a generic discriminative optimization technique used in tuning of the various system combination weights.", "labels": [], "entities": []}, {"text": "Sentence, phrase and word-level system combination methods are presented in Sections 3, 4 and 5.", "labels": [], "entities": [{"text": "word-level system combination", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.5873808761437734}]}, {"text": "Experimental results on Arabic and Chinese to English newswire and newsgroup test data are presented in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official metric of the 2006 DARPA GALE evaluation was human-mediated translation edit rate (HTER).", "labels": [], "entities": [{"text": "DARPA GALE", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.43816693127155304}, {"text": "translation edit rate (HTER)", "start_pos": 73, "end_pos": 101, "type": "METRIC", "confidence": 0.8141083916028341}]}, {"text": "HTER is computed as the minimum translation edit rate (TER) between a system output and a targeted reference which preserves the meaning and fluency of the sentence ().", "labels": [], "entities": [{"text": "HTER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.602425754070282}, {"text": "translation edit rate (TER)", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.7770521293083826}]}, {"text": "The targeted reference is generated by human posteditors who make edits to a reference translation so as to minimize the TER between the reference and the MT output without changing the meaning of the reference.", "labels": [], "entities": [{"text": "TER", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9966970682144165}]}, {"text": "Computing the HTER is very time consuming due to the human post-editing.", "labels": [], "entities": []}, {"text": "It is desirable to have an automatic evaluation metric that correlates well with the HTER to allow fast evaluation of the MT systems during development.", "labels": [], "entities": [{"text": "HTER", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.7926806807518005}, {"text": "MT", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.9768214821815491}]}, {"text": "Correlations of different evaluation metrics have been studied) but according to various internal HTER experiments it is not clear whether TER or BLEU correlates better.", "labels": [], "entities": [{"text": "TER", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9971669316291809}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9839349389076233}]}, {"text": "Therefore it is probably safest to try and not degrade either.", "labels": [], "entities": []}, {"text": "The TER of a translation is computed as is the total number of words in the reference translation . In the case of multiple references, the edits are counted against all references, \u00a4 is the average number of words in the reference translations and the final TER is computed using the minimum number of edits.", "labels": [], "entities": [{"text": "TER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9971001744270325}, {"text": "TER", "start_pos": 259, "end_pos": 262, "type": "METRIC", "confidence": 0.9934189915657043}]}, {"text": "The NIST BLEU-4 is a variant of BLEU () and is computed as maybe used to find weights that optimize the appropriate evaluation metric in the re-scored \u00a4 -best list.", "labels": [], "entities": [{"text": "NIST", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8942570090293884}, {"text": "BLEU-4", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9652939438819885}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9980251789093018}]}, {"text": "The optimization starts at a random initial point in the s -dimensional parameter space, first searching through an initial set of basis vectors.", "labels": [], "entities": []}, {"text": "As searching repeatedly through the set of basis vectors is inefficient, the direction of the vectors is gradually moved toward a larger positive change in the evaluation metric.", "labels": [], "entities": []}, {"text": "To improve the chances of finding a global optimum, the algorithm is repeated with varying initial values.", "labels": [], "entities": []}, {"text": "The modified Powell's method has been previously used in optimizing the weights of a standard feature-based MT decoder in where a more efficient algorithm for log-linear models was proposed.", "labels": [], "entities": [{"text": "MT decoder", "start_pos": 108, "end_pos": 118, "type": "TASK", "confidence": 0.8164532780647278}]}, {"text": "However, this is specific to log-linear models and cannot be easily extended for more complicated functions.", "labels": [], "entities": []}, {"text": "Six systems trained on all data available for GALE 2006 evaluation were used in the experiments to demonstrate the performance of all three system combination methods on Arabic and Chinese to English MT tasks.", "labels": [], "entities": [{"text": "GALE 2006", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.6965503394603729}, {"text": "MT tasks", "start_pos": 200, "end_pos": 208, "type": "TASK", "confidence": 0.8582316339015961}]}, {"text": "Three systems were phrase-based (A, C and E), two hierarchical (B and D) and one syntax-based (F).", "labels": [], "entities": []}, {"text": "The phrase-based systems used different sets of features and re-ordering approaches.", "labels": [], "entities": []}, {"text": "The hierarchical systems used different rule sets.", "labels": [], "entities": []}, {"text": "All systems were tuned on NIST MT02 evaluation sets with four references.", "labels": [], "entities": [{"text": "NIST MT02 evaluation sets", "start_pos": 26, "end_pos": 51, "type": "DATASET", "confidence": 0.9262846410274506}]}, {"text": "Systems A and B were tuned to minimize TER, the other systems were tuned to maximize BLEU.", "labels": [], "entities": [{"text": "TER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9995504021644592}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9927161335945129}]}, {"text": "As discussed in Section 2, the system combination tuning metric was chosen so that gains were observed in both TER and BLEU on development test sets.", "labels": [], "entities": [{"text": "TER", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9957910776138306}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9864889979362488}]}, {"text": "NIST MT05 comprising only newswire data (1056 Arabic and 1082 Chinese sentences) with four reference translations and the newsgroup portion of the GALE 2006 dry-run (203 Arabic and 126 Chinese sentences) with one reference translation were used as the test sets.", "labels": [], "entities": [{"text": "NIST MT05", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8683499991893768}, {"text": "GALE 2006 dry-run", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.9235391418139139}]}, {"text": "It was found that minimizing TER on Arabic also resulted in higher BLEU scores compared to the best single system.", "labels": [], "entities": [{"text": "TER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9952869415283203}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9994320273399353}]}, {"text": "However,  minimizing TER on Chinese resulted in significantly lower BLEU.", "labels": [], "entities": [{"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9988211989402771}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9993379712104797}]}, {"text": "So, TER was used in tuning the combination weights on Arabic and BLEU on Chinese.", "labels": [], "entities": [{"text": "TER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9993583559989929}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9985206723213196}]}, {"text": "The sentence and phrase-level combination weights were tuned on NIST MT03 evaluation sets.", "labels": [], "entities": [{"text": "NIST MT03 evaluation sets", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.9127167612314224}]}, {"text": "On the tuning sets, both methods yield about 0.5%-1.0% gain in TER and BLEU.", "labels": [], "entities": [{"text": "TER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9991581439971924}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9966405630111694}]}, {"text": "The mixed-case TER and BLEU scores on both test sets are shown in for Chinese (phrcomb represents phrase and sentcomb sentence-level combination).", "labels": [], "entities": [{"text": "TER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9907930493354797}, {"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9926797151565552}]}, {"text": "The phrase-level combination seems to outperform the sentence-level combination in terms of both metrics on Arabic although gains over the best single system are modest, if any.", "labels": [], "entities": []}, {"text": "On Chinese, the sentence-level combination yields higher BLEU scores than the phrase-level combination.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9995163679122925}]}, {"text": "The combination BLEU scores on the newsgroup data are not higher than the best system, though.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.996371865272522}, {"text": "newsgroup data", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.901357889175415}]}, {"text": "The word-level combination was evaluated in three settings.", "labels": [], "entities": []}, {"text": "First, simple confusion network decoding with six systems without system weights was performed (no weights 6 in the tables).", "labels": [], "entities": []}, {"text": "Second, system weights were trained for combining six systems (TER/BLEU 6 in the tables).", "labels": [], "entities": [{"text": "TER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9988864064216614}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.890838086605072}]}, {"text": "Finally, all six system outputs as well as the sentence and phrase-level combination outputs were combined with system weights (TER/BLEU 8 in the tables).", "labels": [], "entities": [{"text": "TER/BLEU 8", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.8649974465370178}]}, {"text": "The 6-way combination weights were tuned on merged NIST MT03 and MT04 evaluation sets and the 8-way combination weights were tuned only on NIST MT04 since the sentence and phraselevel combination methods were already tuned on NIST MT03.", "labels": [], "entities": [{"text": "NIST MT03 and MT04 evaluation sets", "start_pos": 51, "end_pos": 85, "type": "DATASET", "confidence": 0.8080568859974543}, {"text": "NIST MT04", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.8709145486354828}, {"text": "NIST MT03", "start_pos": 226, "end_pos": 235, "type": "DATASET", "confidence": 0.8788060247898102}]}, {"text": "The word-level combination yields about 2.0%-3.0% gain in TER and 2.0%-4.0% gain in BLEU on the tuning sets.", "labels": [], "entities": [{"text": "TER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9969929456710815}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9987687468528748}]}, {"text": "The test set results show that the simple confusion network decoding without system weights yields very good scores, mostly better than either sentence or phrase-level combination.", "labels": [], "entities": []}, {"text": "The system weights seem to yield even higher BLEU scores but not always lower TER scores on both languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9990642666816711}, {"text": "TER scores", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9808519184589386}]}, {"text": "Despite slightly hurting the TER score on Arabic, the TER 8 combination result was considered the best due to the highest BLEU and significantly lower TER compared to any single system.", "labels": [], "entities": [{"text": "TER score", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9830089211463928}, {"text": "Arabic", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8692793250083923}, {"text": "TER 8 combination", "start_pos": 54, "end_pos": 71, "type": "METRIC", "confidence": 0.9433159430821737}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9994930028915405}, {"text": "TER", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9985182881355286}]}, {"text": "Similarly, the BLEU 8 was considered the best combination result on Chinese.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9939409494400024}, {"text": "Chinese", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.966366171836853}]}, {"text": "Internal HTER experiments showed that BLEU 8 yielded lower scores after post-editing even though no weights 6 had lower automatic TER score.", "labels": [], "entities": [{"text": "BLEU 8", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9671052396297455}, {"text": "automatic", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9469454288482666}, {"text": "TER score", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.8890060782432556}]}], "tableCaptions": [{"text": " Table 1: Mixed-case TER and BLEU scores on  Arabic NIST MT05 (newswire) and the newsgroups  portion of the GALE 2006 dry-run data.", "labels": [], "entities": [{"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9907806515693665}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9734202027320862}, {"text": "Arabic NIST MT05 (newswire)", "start_pos": 45, "end_pos": 72, "type": "DATASET", "confidence": 0.8650618692239126}, {"text": "GALE 2006 dry-run data", "start_pos": 108, "end_pos": 130, "type": "DATASET", "confidence": 0.9035142958164215}]}, {"text": " Table 2: Mixed-case TER and BLEU scores on Chi- nese NIST MT05 (newswire) and the newsgroups  portion of the GALE 2006 dry-run data.", "labels": [], "entities": [{"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9916621446609497}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9799792766571045}, {"text": "Chi- nese NIST MT05 (newswire)", "start_pos": 44, "end_pos": 74, "type": "DATASET", "confidence": 0.7919748425483704}, {"text": "GALE 2006 dry-run data", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.9071339666843414}]}]}