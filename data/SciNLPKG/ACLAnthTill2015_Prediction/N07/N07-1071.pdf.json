{"title": [{"text": "ISP: Learning Inferential Selectional Preferences", "labels": [], "entities": [{"text": "Learning Inferential Selectional Preferences", "start_pos": 5, "end_pos": 49, "type": "TASK", "confidence": 0.6101235300302505}]}], "abstractContent": [{"text": "Semantic inference is a key component for advanced natural language understanding.", "labels": [], "entities": [{"text": "Semantic inference", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8218776881694794}, {"text": "natural language understanding", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.678788810968399}]}, {"text": "However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual en-tailment and question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.9167174398899078}]}, {"text": "This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences.", "labels": [], "entities": []}, {"text": "We evaluate ISP and present empirical evidence of its effectiveness.", "labels": [], "entities": [{"text": "ISP", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9368610382080078}]}], "introductionContent": [{"text": "Semantic inference is a key component for advanced natural language understanding.", "labels": [], "entities": [{"text": "Semantic inference", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8218777775764465}, {"text": "natural language understanding", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.678788810968399}]}, {"text": "Several important applications are already relying heavily on inference, including question answering (), information extraction (), and textual entailment ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8765967488288879}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.8293029367923737}, {"text": "textual entailment", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.6808819621801376}]}, {"text": "In response, several researchers have created resources for enabling semantic inference.", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8217593431472778}]}, {"text": "Among manual resources used for this task are WordNet and.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9676914811134338}]}, {"text": "Although important and useful, these resources primarily contain prescriptive inference rules such as \"X divorces Y \u21d2 X married Y\".", "labels": [], "entities": []}, {"text": "In practical NLP applications, however, plausible inference rules such as \"X married Y\" \u21d2 \"X dated Y\" are very useful.", "labels": [], "entities": []}, {"text": "This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections () and paraphrase collections (.", "labels": [], "entities": []}, {"text": "Using these resources in applications has been hindered by the large amount of incorrect inferences they generate, either because of altogether incorrect rules or because of blind application of plausible rules without considering the context of the relations or the senses of the words.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence: Using this rule, we can infer that \"federal prosecutors announced the arrest of Terry Nichols\".", "labels": [], "entities": []}, {"text": "However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization.", "labels": [], "entities": [{"text": "Fraud", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9590319395065308}]}, {"text": "the plausible inference rule (1) would incorrectly infer that \"CCM telemarketers announced the arrest of accounts\".", "labels": [], "entities": []}, {"text": "This example depicts a major obstacle to the effective use of automatically learned inference rules.", "labels": [], "entities": []}, {"text": "What is missing is knowledge about the admissible argument values for which an inference rule holds, which we call Inferential Selectional Preferences.", "labels": [], "entities": []}, {"text": "For example, inference rule (1) should only be applied if X is a Person and Y is a Law Enforcement Agent or a Law Enforcement Agency.", "labels": [], "entities": []}, {"text": "This knowledge does not guarantee that the inference rule will hold, but, as we show in this paper, goes along way toward filtering out erroneous applications of rules.", "labels": [], "entities": []}, {"text": "In this paper, we propose ISP, a collection of methods for learning inferential selectional preferences and filtering out incorrect inferences.", "labels": [], "entities": []}, {"text": "The presented algorithms apply to any collection of inference rules between binary semantic relations, such as example (1).", "labels": [], "entities": []}, {"text": "ISP derives inferential selectional preferences by aggregating statistics of inference rule instantiations over a large corpus of text.", "labels": [], "entities": []}, {"text": "Within ISP, we explore different probabilistic models of selectional preference to accept or reject specific inferences.", "labels": [], "entities": []}, {"text": "We present empirical evidence to support the following main contribution: Claim: Inferential selectional preferences can be automatically learned and used for effectively filtering out incorrect inferences.", "labels": [], "entities": [{"text": "Claim", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9568437337875366}]}], "datasetContent": [{"text": "This section describes the methodology for testing our claim that inferential selectional preferences can be learned to filter incorrect inferences.", "labels": [], "entities": []}, {"text": "Given a collection of inference rules of the form pi \u21d2 p j , our task is to determine whether a particular instance \u2329x, p j , y\u232a holds given that \u2329x, pi , y\u232a holds 4 . In the next sections, we describe our collection of inference rules, the semantic classes used for forming selectional preferences, and evaluation criteria for measuring the filtering quality.", "labels": [], "entities": []}, {"text": "The goal of the filtering task is to minimize false positives (incorrectly accepted inferences) and false negatives (incorrectly rejected inferences).", "labels": [], "entities": []}, {"text": "A standard methodology for evaluating such tasks is to compare system filtering results with a gold standard using a confusion matrix.", "labels": [], "entities": [{"text": "system filtering", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7081220000982285}]}, {"text": "A confusion matrix captures the filtering performance on both correct and incorrect inferences: where A represents the number of correct instances correctly identified by the system, D represents the number of incorrect instances correctly identified by the system, B represents the number of false positives and C represents the number of false negatives.", "labels": [], "entities": []}, {"text": "To compare systems, three key measures are used to summarize confusion matrices: \u2022 Sensitivity, defined as , captures the probability of a filter being correct.", "labels": [], "entities": []}, {"text": "In this section, we provide empirical evidence to support the main claim of this paper.", "labels": [], "entities": []}, {"text": "Given a collection of DIRT inference rules of the form pi \u21d2 p j , our experiments, using the methodology of Section 4, evaluate the capability of our ISP models for determining if \u2329x, p j , y\u232a holds given that \u2329x, pi , y\u232a holds.", "labels": [], "entities": []}], "tableCaptions": []}