{"title": [{"text": "Combining Reinforcement Learning with Information-State Update Rules *", "labels": [], "entities": []}], "abstractContent": [{"text": "Reinforcement learning gives away to learn under what circumstances to perform which actions.", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8880315721035004}]}, {"text": "However, this approach lacks a formal framework for specifying hand-crafted restrictions, for specifying the effects of the system actions, or for specifying the user simulation.", "labels": [], "entities": []}, {"text": "The information state approach, in contrast, allows system and user behavior to be specified as update rules, with preconditions and effects.", "labels": [], "entities": []}, {"text": "This approach can be used to specify complex dialogue behavior in a systematic way.", "labels": [], "entities": []}, {"text": "We propose combining these two approaches, thus allowing a formal specification of the dialogue behavior, and allowing hand-crafted preconditions, with remaining ones determined via reinforcement learning so as to minimize dialogue cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Two different approaches have become popular for building spoken dialogue systems.", "labels": [], "entities": []}, {"text": "The first is the symbolic reasoning approach.", "labels": [], "entities": [{"text": "symbolic reasoning", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7677770256996155}]}, {"text": "Speech actions are defined in a formal logic, in terms of the situations in which they can be applied, and what effect they will have on the speaker's and the listener's mental state.", "labels": [], "entities": []}, {"text": "One of these approaches is the information state (IS) approach).", "labels": [], "entities": []}, {"text": "The knowledge of the agent is formalized as the state.", "labels": [], "entities": []}, {"text": "The IS state is updated byway of update rules, which have preconditions and effects.", "labels": [], "entities": []}, {"text": "The preconditions specify what must be true of the state in order * The author wishes to thank Fan Yang and Michael English for helpful conversations.", "labels": [], "entities": []}, {"text": "Funding from the National Science Foundation under grant IIS-0326496 is gratefully acknowledged.", "labels": [], "entities": []}, {"text": "The effects specify how the state changes as a result of applying the rule.", "labels": [], "entities": []}, {"text": "At a minimum, two sets of update rules are used: one set, understanding rules, specify the effect of an utterance on the agent's state and a second set, action rules, specify which speech action can be performed next.", "labels": [], "entities": []}, {"text": "For example, a precondition for asking a question is that the agent does not know the answer to the question.", "labels": [], "entities": []}, {"text": "An effect of an answer to a question is that the hearer now knows the answer.", "labels": [], "entities": []}, {"text": "One problem with this approach is that although necessary preconditions for speech actions are easy to code, there are typically many speech actions that can be applied at any point in a dialogue.", "labels": [], "entities": []}, {"text": "Determining which one is the optimal one is a daunting task for the dialogue designer.", "labels": [], "entities": []}, {"text": "The second approach for building spoken dialogue systems is to use reinforcement learning (RL) to automatically determine what action to perform in each different dialogue state so as to minimize some cost function (e.g.;).", "labels": [], "entities": []}, {"text": "The problem with this approach, however, is that it lacks the framework of IS to specify the manner in which the internal state is updated.", "labels": [], "entities": []}, {"text": "Furthermore, sometimes no preconditions are even specified for the actions, even though they are obvious to the dialogue designer.", "labels": [], "entities": []}, {"text": "Thus RL needs to search over a much larger search space, even over dialogue strategies that do not make any sense.", "labels": [], "entities": [{"text": "RL", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9663146138191223}]}, {"text": "This not only substantially slows down the learning procedure, but also increases the chance of being caught in a locally optimal solution, rather than the global optimal.", "labels": [], "entities": []}, {"text": "Furthermore, this large search space will limit the complexity of the domains to which RL can be applied.", "labels": [], "entities": []}, {"text": "In this paper, we propose combining IS and RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.6489976644515991}]}, {"text": "IS update rules are formulated for both the system and the simulated user, thus allowing RL to use a rich formalism for specifying complex dialogue processing.", "labels": [], "entities": [{"text": "IS update", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.7179575264453888}, {"text": "specifying complex dialogue processing", "start_pos": 120, "end_pos": 158, "type": "TASK", "confidence": 0.6501098498702049}]}, {"text": "The preconditions on the action rules of the system, however, only need to specify the neces-sary preconditions that are obvious to the dialogue designer.", "labels": [], "entities": []}, {"text": "Thus, preconditions on the system's actions might not uniquely identify a single action that should be performed in a given state.", "labels": [], "entities": []}, {"text": "Instead, RL is used to determine which of the applicable actions minimizes a dialogue cost function.", "labels": [], "entities": [{"text": "RL", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.6439172625541687}]}, {"text": "In the rest of the paper, we first present an example domain.", "labels": [], "entities": []}, {"text": "Section 3 gives an overview of applying RL to dialogue strategy and Section 4 gives an overview of IS.", "labels": [], "entities": [{"text": "RL", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9590698480606079}, {"text": "dialogue strategy", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7767573595046997}, {"text": "IS", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9481841325759888}]}, {"text": "Section 5 demonstrates that IS can be used for simulating a dialogue between the system and a user.", "labels": [], "entities": []}, {"text": "Section 6 demonstrates how IS can be used with RL.", "labels": [], "entities": []}, {"text": "Section 7 gives results on using hand-crafted preconditions specified in the IS update rules to simplify learning dialogue strategies with RL.", "labels": [], "entities": []}, {"text": "Section 8 gives concluding comments.", "labels": [], "entities": []}], "datasetContent": [{"text": "To show the usefulness of starting RL with some of the preconditions hand-crafted, we applied RL using four different sets of action schemes.", "labels": [], "entities": []}, {"text": "The first set, 'none', includes no preconditions on any of the system's actions.", "labels": [], "entities": []}, {"text": "The second through fourth sets correspond to the precondition distinctions in, of 'speech act', 'application' and 'partial strategy'.", "labels": [], "entities": []}, {"text": "For each set of action schemas, we trained 30 dialogue policies using an epoch size of 100.", "labels": [], "entities": []}, {"text": "Each dialogue was run with the \u01eb-greedy method, with \u01eb set at 0.15.", "labels": [], "entities": [{"text": "\u01eb set", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9814808070659637}]}, {"text": "After certain epochs, we ran the learned policy 2500 times strictly according to the policy.", "labels": [], "entities": []}, {"text": "We found that policies did not always converge.", "labels": [], "entities": []}, {"text": "Hence, we trained the policies for each set of preconditions for enough epochs so that the average cost no longer improved.", "labels": [], "entities": []}, {"text": "More work is needed to investigate this issue.", "labels": [], "entities": []}, {"text": "The results of the simulations are given in.", "labels": [], "entities": []}, {"text": "The first row reports the average dialogue cost that the 30 learned policies achieved.", "labels": [], "entities": []}, {"text": "We see that all four conditions achieved an average cost less than the baseline strategy of, which was 17.17.", "labels": [], "entities": []}, {"text": "The best result was achieved by the 'application' preconditions.", "labels": [], "entities": []}, {"text": "This is probably because 'partial' included some constraints that were not optimal, while the search strategy was not adequate to deal with the large search space in 'speech acts' and 'none'.", "labels": [], "entities": []}, {"text": "The more important result is in the second row of.", "labels": [], "entities": []}, {"text": "The more constrained precondition sets result in significantly fewer states being explored, ranging from 275 for the 'partial' preconditions, up to 18,206 for no preconditions.", "labels": [], "entities": []}, {"text": "In terms of number of potential policies explored (computed as the product of the number of actions explored in each state), this ranges from 10 58 to 10 7931 . As can be seen, by placing restrictions on the system actions, the space that needs to be explored is substantially reduced.", "labels": [], "entities": []}, {"text": "The restriction in the size of the search space affects how quickly RL takes to find a good solution.", "labels": [], "entities": [{"text": "RL", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9546850919723511}]}, {"text": "shows how the average cost for each set of  As can be seen, by including more preconditions in the action definitions, RL is able to find a good solution more quickly.", "labels": [], "entities": []}, {"text": "For the 'partial' preconditions, after 10 epochs, RL achieves a cost less than 17.0.", "labels": [], "entities": [{"text": "RL", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.8751644492149353}]}, {"text": "For the 'application' setting, this does not happen until 40 epochs.", "labels": [], "entities": []}, {"text": "For 'speech act', it takes 1000 epochs, and for 'none', it takes 3700 epochs.", "labels": [], "entities": []}, {"text": "So, adding hand-crafted preconditions allows RL to converge more quickly.", "labels": [], "entities": [{"text": "RL", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.8212239742279053}]}], "tableCaptions": [{"text": " Table 1. The more constrained precondition sets  result in significantly fewer states being explored,  ranging from 275 for the 'partial' preconditions, up  to 18,206 for no preconditions. In terms of number  of potential policies explored (computed as the prod- uct of the number of actions explored in each state),  this ranges from 10 58 to 10 7931 . As can be seen, by  placing restrictions on the system actions, the space  that needs to be explored is substantially reduced.", "labels": [], "entities": []}, {"text": " Table 1: Comparison of Preconditions", "labels": [], "entities": [{"text": "Preconditions", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.7368127107620239}]}]}