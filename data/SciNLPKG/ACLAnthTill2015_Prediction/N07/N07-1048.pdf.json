{"title": [{"text": "Analysis of Morph-Based Speech Recognition and the Modeling of Out-of-Vocabulary Words Across Languages", "labels": [], "entities": [{"text": "Morph-Based Speech Recognition", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.6014623939990997}]}], "abstractContent": [{"text": "We analyze subword-based language models (LMs) in large-vocabulary continuous speech recognition across four \"morphologically rich\" languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic.", "labels": [], "entities": [{"text": "continuous speech recognition", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.746319313844045}]}, {"text": "By estimating n-gram LMs over sequences of morphs instead of words, better vocabulary coverage and reduced data sparsity is obtained.", "labels": [], "entities": []}, {"text": "Standard word LMs suffer from high out-of-vocabulary (OOV) rates, whereas the morph LMs can recognize previously unseen word forms by concatenating morphs.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) rates", "start_pos": 35, "end_pos": 64, "type": "METRIC", "confidence": 0.7305508136749268}]}, {"text": "We show that the morph LMs generally outperform the word LMs and that they perform fairly well on OOVs without compromising the accuracy obtained for in-vocabulary words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9986467957496643}]}], "introductionContent": [{"text": "As automatic speech recognition systems are being developed for an increasing number of languages, there is growing interest in language modeling approaches that are suitable for so-called \"morphologically rich\" languages.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6992771824200948}]}, {"text": "In these languages, the number of possible word forms is very large because of many productive morphological processes; words are formed through extensive use of, e.g., inflection, derivation and compounding (such as the English words 'rooms', 'roomy', 'bedroom', which all stem from the noun 'room').", "labels": [], "entities": []}, {"text": "For some languages, language modeling based on surface forms of words has proven successful, or at least satisfactory.", "labels": [], "entities": []}, {"text": "The most studied language, English, is not characterized by a multitude of word forms.", "labels": [], "entities": []}, {"text": "Thus, the recognition vocabulary can simply consist of a list of words observed in the training text, and n-gram language models (LMs) are estimated over word sequences.", "labels": [], "entities": []}, {"text": "The applicability of the word-based approach to morphologically richer languages has been questioned.", "labels": [], "entities": []}, {"text": "In highly compounding languages, such as the Germanic languages German, Dutch and Swedish, decomposition of compound words can be carried out to reduce the vocabulary size.", "labels": [], "entities": []}, {"text": "Highly inflecting languages are found, e.g., among the Slavic, Romance, Turkic, and Semitic language families.", "labels": [], "entities": []}, {"text": "LMs incorporating morphological knowledge about these languages can be applied.", "labels": [], "entities": []}, {"text": "A further challenging category comprises languages that are both highly inflecting and compounding, such as the Finno-Ugric languages Finnish and Estonian.", "labels": [], "entities": []}, {"text": "Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models.", "labels": [], "entities": [{"text": "Morphology modeling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8544799983501434}, {"text": "outof-vocabulary (OOV) rate", "start_pos": 39, "end_pos": 66, "type": "METRIC", "confidence": 0.9411506533622742}]}, {"text": "However, obtaining considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1-4 % relative) over standard word-based models accomplished by, e.g.,,,, Whittaker and,, and for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by for German and for Czech.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7096662223339081}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9021309614181519}]}, {"text": "Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (), Finnish, Estonian () and Turkish ().", "labels": [], "entities": []}, {"text": "In this paper, subword language models in the recognition of speech of four languages are ana-lyzed: Finnish, Estonian, Turkish, and the dialect of Arabic spoken in Egypt, Egyptian Colloquial Arabic (ECA).", "labels": [], "entities": []}, {"text": "All these languages are considered \"morphologically rich\", but the benefits of using subword-based LMs differ across languages.", "labels": [], "entities": []}, {"text": "We attempt to discover explanations for these differences.", "labels": [], "entities": []}, {"text": "In particular, the focus is on the analysis of OOVs: A perceived strength of subword models, when contrasted with word models, is that subword models can generalize to previously unseen word forms by recognizing them as sequences of shorter familiar word fragments.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the conducted experiments is to compare n-gram language models based on morphs to standard word n-gram models in automatic speech recognition across languages.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.6781441370646158}]}], "tableCaptions": [{"text": " Table 1. Together these two partitions make up the entire test set vocabulary. For comparison, the results for  the entire sets are shown using gray-shaded bars (also displayed in", "labels": [], "entities": []}, {"text": " Table 1. For comparison, the gray- shaded bars show the corresponding results for the  entire test sets (also displayed in", "labels": [], "entities": []}]}