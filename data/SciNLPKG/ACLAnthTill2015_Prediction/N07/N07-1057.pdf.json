{"title": [{"text": "Multilingual Structural Projection across Interlinear Text", "labels": [], "entities": [{"text": "Multilingual Structural Projection", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7195308605829874}]}], "abstractContent": [{"text": "This paper explores the potential for annotating and enriching data for low-density languages via the alignment and projection of syntactic structure from parsed data for resource-rich languages such as English.", "labels": [], "entities": []}, {"text": "We seek to develop enriched resources fora large number of the world's languages, most of which have no significant digital presence.", "labels": [], "entities": []}, {"text": "We do this by tapping the body of Web-based linguistic data, most of which exists in small, analyzed chunks embedded in scholarly papers, journal articles, Web pages, and other online documents.", "labels": [], "entities": []}, {"text": "By harvesting and enriching these data, we can provide the means for knowledge discovery across the resulting corpus that can lead to building computational resources such as grammars and transfer rules, which, in turn, can be used as bootstraps for building additional tools and resources for the languages represented.", "labels": [], "entities": []}], "introductionContent": [{"text": "Developing natural language applications is generally dependent on the availability of annotated corpora.", "labels": [], "entities": []}, {"text": "Building annotated resources, however, is a significantly time consuming process involving considerable human effort.", "labels": [], "entities": []}, {"text": "Although a number of projects have been undertaken to develop annotated resources for non-English languages, e.g., treebanks, the development of these resources has been no small feat, and to date have been limited to a very small number of the world's languages (e.g., Chinese, German, Arabic, Korean, etc.).", "labels": [], "entities": []}, {"text": "Some notable efforts have been undertaken to develop automated means for creating annotated corpora through the projection of annotations ().", "labels": [], "entities": []}, {"text": "The resulting methods, however, can only be applied to a small number of language pairs due mostly to the need for sizeable parallel corpora.", "labels": [], "entities": []}, {"text": "Unfortunately, most languages do not have parallel corpora of sufficient size, making these methods inapplicable for the vast majority of the world's languages.", "labels": [], "entities": []}, {"text": "We describe a method for bootstrapping resource creation by tapping the wealth of multilingual data on the Web that has been created by linguists.", "labels": [], "entities": [{"text": "bootstrapping resource creation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.8515670299530029}]}, {"text": "Of particular note is the linguistic presentation format of \"interlinear text\", a common format used for presenting language data and analysis relevant to a particular argument or investigation.", "labels": [], "entities": []}, {"text": "Since interlinear examples consist of orthographically or phonetically encoded language data aligned with an English translation, the \"database\" of interlinear examples found on the Web, when taken together, constitute a significant multilingual, parallel corpus covering hundreds to thousands of the world's languages.", "labels": [], "entities": []}, {"text": "We do not propose that a database of interlinear text alone is sufficient to create NLP resources and tools, but rather that it may act as a means for more rapidly developing such tools using less data.", "labels": [], "entities": []}, {"text": "We contend that such a resource allows one to develop computational artifacts, such as grammars and transfer rules, which can be used as \"seed\" knowledge for building larger resources.", "labels": [], "entities": []}, {"text": "In particular, knowing a little about the structure of a language can help in developing annotated corpora and tools, since a little knowledge can go along way in inducing accurate structure and annotations).", "labels": [], "entities": []}, {"text": "Of particular relevance to MT is the issue of struc-tural divergence.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9970122575759888}, {"text": "struc-tural divergence", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8706328272819519}]}, {"text": "Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in ().", "labels": [], "entities": [{"text": "MT", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9775000214576721}, {"text": "direct correspondence assumption (DCA)", "start_pos": 45, "end_pos": 83, "type": "METRIC", "confidence": 0.6474823206663132}]}, {"text": "However, to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data).", "labels": [], "entities": []}, {"text": "A larger sample of typologically diverse language data can help test the assumption for hundreds of languages.", "labels": [], "entities": []}, {"text": "We contend that the knowledge garnered from structural projections applied to interlinear text can bootstrap the development of resources and tools across parallel corpora, where such corpora could be of smaller size and the resulting tools more robust, opening the door to the development of tools and resources fora larger number of the world's languages.", "labels": [], "entities": []}, {"text": "Given the imminent death of half of the world's 6,000 languages, the development of any language specific tools fora larger percentage of the world's languages than is currently possible can aid in both their documentation and preservation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the feasibility of our approach on a small set of IGT examples for seven languages: German (GER), Korean (KKN), Hausa (HUA), Malagasy (MEX), Welsh (WLS), Irish (GLI), and Yaqui (YAQ).", "labels": [], "entities": []}, {"text": "This set of languages was chosen because of its typological diversity: GER and HUA are SVO languages, KKN and YAQ are SOV, GLI and WLS are VSO, and MEX is VOS.", "labels": [], "entities": [{"text": "GER", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.7116511464118958}]}, {"text": "In addition, while German and Korean are well-studied and have readily accessible resources that we could use to test the effectiveness and accuracy of our methods, Yaqui, with about 16,000 speakers, is a highly endangered language and serves as a demonstration of our methods for resource-poor and endangered languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9988507032394409}]}], "tableCaptions": [{"text": " Table 1: The size and average sentence length of the test data", "labels": [], "entities": []}, {"text": " Table 2: The training data for GIZA++", "labels": [], "entities": []}, {"text": " Table 3: The word alignment results when gloss  words are not split into morphemes", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7649047672748566}]}, {"text": " Table 4: The word alignment results when gloss  words are split into morphemes", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7601373195648193}]}, {"text": " Table 5: The word alignment results when (x,x) pairs  are added", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7484709024429321}]}, {"text": " Table 6: The performance of heuristic word aligner", "labels": [], "entities": []}, {"text": " Table 7: The system performance on the seven languages", "labels": [], "entities": []}, {"text": " Table 8: The F-measure of source dependency links with perfect English DS and/or word alignment", "labels": [], "entities": [{"text": "F-measure", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9982154369354248}, {"text": "word alignment", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.7109102457761765}]}, {"text": " Table 9: Extracted CFGs and evidence of word order", "labels": [], "entities": [{"text": "Extracted CFGs", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.5752924084663391}]}]}