{"title": [{"text": "Analysis and System Combination of Phrase-and N -gram-based Statistical Machine Translation Systems", "labels": [], "entities": [{"text": "Phrase-and N -gram-based Statistical Machine Translation", "start_pos": 35, "end_pos": 91, "type": "TASK", "confidence": 0.49075606039592196}]}], "abstractContent": [{"text": "In the framework of the Tc-Star project, we analyze and propose a combination of two Statistical Machine Translation systems: a phrase-based and an N-gram-based one.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.7530725598335266}]}, {"text": "The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system's output.", "labels": [], "entities": []}, {"text": "Additionally, we combine both systems, showing accuracy improvements.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996423721313477}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8444105287392935}, {"text": "word-based translation", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7176110744476318}]}, {"text": "The so-called phrase-based and N -gram-based models are two examples of these approaches ().", "labels": [], "entities": []}, {"text": "In current state-of-the-art SMT systems, the phrase-based or the N -gram-based models are usually the main features in a log-linear framework, reminiscent of the maximum entropy modeling approach.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9915456175804138}]}, {"text": "Two basic issues differentiate the N -gram-based system from the phrase-based one: the training data is sequentially segmented into bilingual units; and the probability of these units is estimated as a bilingual N -gram language model.", "labels": [], "entities": []}, {"text": "In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by relative frequencies.", "labels": [], "entities": []}, {"text": "This paper extends the analysis of both systems performed in () by additionally performing a manual error analysis of both systems, which were the ones used by UPC and RWTH in the last Tc-Star evaluation.", "labels": [], "entities": [{"text": "UPC", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.935197114944458}, {"text": "RWTH", "start_pos": 168, "end_pos": 172, "type": "DATASET", "confidence": 0.7923349738121033}]}, {"text": "Furthermore, we will propose away to combine both systems in order to improve the quality of translations.", "labels": [], "entities": []}, {"text": "Experiments combining several kinds of MT systems have been presented in (), based only on the single best output of each system.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9853028059005737}]}, {"text": "Recently, a more straightforward approach of both systems has been performed in ) which simply selects, for each sentence, one of the provided hypotheses.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we briefly describe the phrase and the N -gram-based baseline systems.", "labels": [], "entities": []}, {"text": "In the next section we present the evaluation framework.", "labels": [], "entities": []}, {"text": "In Section 4 we report a structural comparison performed for both systems and, afterwards, in Section 5, we analyze the errors of both systems.", "labels": [], "entities": []}, {"text": "Finally, in the last two sections we rescore and combine both systems, and the obtained results are discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "The translation models presented so far were the ones used by UPC and RWTH in the second evaluation campaign of the Tc-Star project.", "labels": [], "entities": [{"text": "UPC", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9249228239059448}, {"text": "RWTH", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8785858750343323}]}, {"text": "The goal of this project is to build a speech-to-speech translation system that can deal with real life data.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7134945690631866}]}, {"text": "The corpus consists of the official version of the speeches held in the European Parliament Plenary Sessions (EPPS), as available on the web page of the European Parliament.", "labels": [], "entities": [{"text": "European Parliament Plenary Sessions (EPPS)", "start_pos": 72, "end_pos": 115, "type": "DATASET", "confidence": 0.6628848910331726}]}, {"text": "The following tools have been used for building both systems: Word alignments were computed using GIZA++, language models were estimated using the SRILM toolkit), decoding was carried out by the free available MARIE decoder () and the optimization was performed through an in-house implementation of the simplex method", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.6837371736764908}]}], "tableCaptions": [{"text": " Table 1: Statistics of the EPPS Corpora.", "labels": [], "entities": [{"text": "EPPS Corpora", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9386456310749054}]}, {"text": " Table 2: Impact on efficiency of the beam size in PB  (top) and NB system (bottom).", "labels": [], "entities": []}, {"text": " Table 3: Impact on accuracy of the beam size in PB  (top) and NB system (bottom).", "labels": [], "entities": [{"text": "Impact", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9635807275772095}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9994378685951233}]}, {"text": " Table 4: Sentences (%) produced by each system.", "labels": [], "entities": []}, {"text": " Table 5: Rescoring and system combination results.", "labels": [], "entities": [{"text": "Rescoring", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.6976161003112793}]}]}