{"title": [{"text": "Efficient Phrase-table Representation for Machine Translation with Applications to Online MT and Speech Translation", "labels": [], "entities": [{"text": "Phrase-table Representation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.754481703042984}, {"text": "Machine Translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7205015569925308}, {"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.7743039131164551}, {"text": "Speech Translation", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.6870118081569672}]}], "abstractContent": [{"text": "In phrase-based statistical machine translation , the phrase-table requires a large amount of memory.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.6277013793587685}]}, {"text": "We will present an efficient representation with two key properties: on-demand loading and a prefix tree structure for the source phrases.", "labels": [], "entities": []}, {"text": "We will show that this representation scales well to large data tasks and that we are able to store hundreds of millions of phrase pairs in the phrase-table.", "labels": [], "entities": []}, {"text": "For the large Chinese-English NIST task, the memory requirements of the phrase-table are reduced to less than 20 MB using the new representation with no loss in translation quality and speed.", "labels": [], "entities": []}, {"text": "Additionally , the new representation is not limited to a specific test set, which is important for online or real-time machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7027280330657959}]}, {"text": "One problem in speech translation is the matching of phrases in the input word graph and the phrase-table.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7043656557798386}]}, {"text": "We will describe a novel algorithm that effectively solves this com-binatorial problem exploiting the prefix tree data structure of the phrase-table.", "labels": [], "entities": []}, {"text": "This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "In phrase-based statistical machine translation, a huge number of source and target phrase pairs is memorized in the so-called phrase-table.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.5883718729019165}]}, {"text": "For medium sized tasks and phrase lengths, these phrase-tables already require several GBs of memory or even do not fit at all.", "labels": [], "entities": [{"text": "phrase lengths", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7364165484905243}]}, {"text": "If the source text, which is to be translated, is known in advance, a common trick is to filter the phrase-table and keep a phrase pair only if the source phrase occurs in the text.", "labels": [], "entities": []}, {"text": "This filtering is a time-consuming task, as we have to go over the whole phrase-table.", "labels": [], "entities": []}, {"text": "Furthermore, we have to repeat this filtering step whenever we want to translate anew source text.", "labels": [], "entities": []}, {"text": "To address these problems, we will use an efficient representation of the phrase-table with two key properties: on-demand loading and a prefix tree structure for the source phrases.", "labels": [], "entities": []}, {"text": "The prefix tree structure exploits the redundancy among the source phrases.", "labels": [], "entities": []}, {"text": "Using on-demand loading, we will load only a small fraction of the overall phrase-table into memory.", "labels": [], "entities": []}, {"text": "The majority will remain on disk.", "labels": [], "entities": []}, {"text": "The on-demand loading is employed on a per sentence basis, i.e. we load only the phrase pairs that are required for one sentence into memory.", "labels": [], "entities": []}, {"text": "Therefore, the memory requirements are low, e.g. less than 20 MB for the Chin.-Eng.", "labels": [], "entities": [{"text": "Chin.-Eng.", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.9490369111299515}]}, {"text": "Another advantage of the on-demand loading is that we are able to translate new source sentences without filtering.", "labels": [], "entities": [{"text": "translate new source sentences", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.8246389776468277}]}, {"text": "A potential problem is that this on-demand loading might be too slow.", "labels": [], "entities": []}, {"text": "To overcome this, we use a binary format which is a memory map of the internal representation used during decoding.", "labels": [], "entities": []}, {"text": "Additionally, we load coherent chunks of the tree structure instead of individual phrases, i.e. we have only few disk access operations.", "labels": [], "entities": []}, {"text": "In our experiments, the on-demand loading is not slower than the traditional approach.", "labels": [], "entities": []}, {"text": "As pointed out in (), one problem in speech translation is that we have to match the phrases of our phrase-table against a word graph representing the alternative ASR tran-scriptions.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7097522765398026}]}, {"text": "We will present a phrase matching algorithm that effectively solves this combinatorial problem exploiting the prefix tree data structure of the phrase-table.", "labels": [], "entities": [{"text": "phrase matching", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.796553760766983}]}, {"text": "This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality.", "labels": [], "entities": []}, {"text": "The remaining part is structured as follows: we will first discuss related work in Sec.", "labels": [], "entities": []}, {"text": "2. Then, in Sec.", "labels": [], "entities": []}, {"text": "3, we will describe the phrase-table representation.", "labels": [], "entities": []}, {"text": "Afterwards, we will present applications in speech translation and online MT in Sec.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7933281064033508}, {"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.92393559217453}]}, {"text": "4 and 5, respectively.", "labels": [], "entities": []}, {"text": "Experimental results will be presented in Sec.", "labels": [], "entities": []}, {"text": "6 followed by the conclusions in Sec.", "labels": [], "entities": []}, {"text": "7.) and () presented data structures fora compact representation of the word-aligned bilingual data, such that on-the-fly extraction of long phrases is possible.", "labels": [], "entities": []}, {"text": "The motivation in) is that there are some long source phrases in the test data that also occur in the training data.", "labels": [], "entities": []}, {"text": "However, the more interesting question is if these long phrases really help to improve the translation quality.", "labels": [], "entities": []}, {"text": "We have investigated this and our results are inline with ( showing that the translation quality does not improve if we utilize phrases beyond a certain length.", "labels": [], "entities": []}, {"text": "Furthermore, the suffix array data structure of) requires a fair amount of memory, about 2 GB in their example, whereas our implementation will use only a tiny amount of memory, e.g. less than 20 MB for the large Chinese-English NIST task.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: NIST task: corpus statistics.  Chinese English  Train Sentence pairs  7 M  Running words  199 M  213 M  Vocabulary size  222 K  351 K  Test  2002 Sentences  878  3 512  Running words  25 K  105 K  2005 Sentences  1 082  4 328  Running words  33 K  148 K", "labels": [], "entities": []}, {"text": " Table 2: NIST task: translation quality as a function of the maximum source phrase length.  src  NIST 2002 set (dev)  NIST 2005 set (test)  len WER[%] PER[%] BLEU[%] NIST WER[%] PER[%] BLEU[%] NIST  1  71.9  46.8  27.07  8.37  78.0  49.0  23.11  7.62  2  62.4  41.2  34.36  9.39  68.5  42.2  30.32  8.74  3  62.0  41.1  34.89  9.33  67.7  42.1  30.90  8.74  4  61.7  41.1  35.05  9.27  67.6  41.9  30.99  8.75  5  61.8  41.2  34.95  9.25  67.6  41.9  30.93  8.72  \u221e  61.8  41.2  34.99  9.25  67.5  41.8  30.90  8.73", "labels": [], "entities": [{"text": "NIST 2002 set", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9577889442443848}, {"text": "NIST 2005 set", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.8918239076932272}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.8924754858016968}, {"text": "BLEU", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9795703887939453}, {"text": "NIST", "start_pos": 194, "end_pos": 198, "type": "DATASET", "confidence": 0.9013513326644897}]}, {"text": " Table 3: NIST task: phrase-table statistics.  src  number of distinct  avg. tgt  len src phrases src-tgt pairs candidates  1  221 505  17 456 415  78.8  2  5 000 041  39 436 617  7.9  3  20 649 699  58 503 904  2.8  4  31 383 549  58 436 271  1.9  5  32 679 145  51 255 866  1.6  total 89 933", "labels": [], "entities": []}, {"text": " Table 4: EPPS task: corpus statistics.  Train  Spanish English  Sentence pairs  1.2 M  Running words  31 M  30 M  Vocabulary size  140 K  94 K  Test confusion networks  Full Pruned  Sentences  1 071  Avg. length  23.6  Avg. / max. depth  2.7 / 136 1.3 / 11  Avg. number of paths  10 75  264 K", "labels": [], "entities": []}, {"text": " Table 5: EPPS task: translation quality and time for  different input conditions (CN=confusion network,  time in seconds per sentence).  Input type BLEU[%] Time [sec]  Single best  37.6  2.7  CN pruned  38.5  4.8  full  38.9  9.2", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9651638865470886}, {"text": "Input type BLEU[%] Time [sec]  Single best", "start_pos": 138, "end_pos": 180, "type": "METRIC", "confidence": 0.8009815812110901}]}]}