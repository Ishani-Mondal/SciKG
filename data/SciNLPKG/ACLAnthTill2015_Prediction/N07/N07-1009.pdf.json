{"title": [{"text": "Structured Local Training and Biased Potential Functions for Conditional Random Fields with Application to Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.9447838962078094}]}], "abstractContent": [{"text": "Conditional Random Fields (CRFs) have shown great success for problems involving structured output variables.", "labels": [], "entities": []}, {"text": "However, for many real-world NLP applications, exact maximum-likelihood training is intractable because computing the global normal-ization factor even approximately can be extremely hard.", "labels": [], "entities": []}, {"text": "In addition, optimizing likelihood often does not correlate with maximizing task-specific evaluation measures.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.8663302063941956}]}, {"text": "In this paper, we present a novel training procedure, structured local training, that maximizes likelihood while exploiting the benefits of global inference during training: hidden variables are used to capture interactions between local inference and global inference.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce biased potential functions that empirically drive CRFs towards performance improvements w.r.t. the preferred evaluation measure for the learning task.", "labels": [], "entities": []}, {"text": "We report promising experimental results on two coreference data sets using two task-specific evaluation measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Undirected graphical models such as Conditional Random Fields (CRFs) ( have shown great success for problems involving structured output variables (e.g. ,).", "labels": [], "entities": []}, {"text": "For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard.", "labels": [], "entities": []}, {"text": "Previous approaches for training CRFs have either (1) opted fora training method that no longer maximizes the likelihood, (e.g., ) , or (2) opted fora simplified graph structure to avoid intractable global normalization (e.g. , ).", "labels": [], "entities": []}, {"text": "Solutions of the first type replace the computation of the global normalization factor \ud97b\udf59 y p(y|x) with argmax y p(y|x) during training, since finding an argmax of a probability distribution is often an easier problem than finding the entire probability distribution.", "labels": [], "entities": []}, {"text": "Training via the voted perceptron algorithm) or using a max-margin criterion also correspond to the first option (e.g. ,).", "labels": [], "entities": []}, {"text": "But without the global normalization, the maximumlikelihood criterion motivated by the maximum entropy principle) is no longer a feasible option as an optimization criterion.", "labels": [], "entities": []}, {"text": "The second solution simplifies the graph structure for training, and applies complex global inference only for testing.", "labels": [], "entities": []}, {"text": "In spite of the discrepancy between the training model and the testing model, it has been empirically shown that (1) performing global inference only during testing can improve performance (e.g., ), and (2) full-blown global training can often perform worse due to insufficient training data (e.g.).", "labels": [], "entities": []}, {"text": "Importantly, however, attempts to reduce the discrepancy between the training and test models -by judiciously adding the effect of global inference to the training -have produced substantial performance improvements over locally trained models (e.g.,).", "labels": [], "entities": []}, {"text": "In this paper, we present structured local training, a novel training procedure for maximum-likelihood training of undirected graphical models, such as CRFs.", "labels": [], "entities": []}, {"text": "The procedure maximizes likelihood while exploiting the benefits of global inference during training by capturing the interactions between local inference and global inference via hidden variables.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9595692157745361}]}, {"text": "Furthermore, we introduce biased potential functions that redefine the likelihood for CRFs so that the performance of CRFs trained under the maximum likelihood criterion correlates better empirically with the preferred evaluation measures such as F-score and MUC-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 247, "end_pos": 254, "type": "METRIC", "confidence": 0.9843977093696594}, {"text": "MUC-score", "start_pos": 259, "end_pos": 268, "type": "DATASET", "confidence": 0.5620646476745605}]}, {"text": "We focus on the problem of coreference resolution; however, our approaches are general and can be extended to other NLP applications with structured output.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.979179173707962}]}, {"text": "Our approaches also extend to nonconditional graphical models such as Markov Random Fields.", "labels": [], "entities": []}, {"text": "In experiments on two coreference data sets, structured local training reduces the error rate significantly (3.5%) for one coreference data set and minimally (\u2264 1%) for the other.", "labels": [], "entities": [{"text": "error rate", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9894531667232513}]}, {"text": "Experiments using biased potential functions increase recall uniformly and significantly for both data sets and both taskspecific evaluation measures.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9993917942047119}]}, {"text": "Results for the combination of the two techniques are promising, but mixed: pairwise F1 increases by 0.8-5.5% for both data sets; MUC F1 increases by 3.5% for one data set, but slightly hurts performance for the second data set.", "labels": [], "entities": [{"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.8887354731559753}, {"text": "MUC F1", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.6308200061321259}]}, {"text": "In \u00a72, we describe structured local training, and follow with experimental results in \u00a73.", "labels": [], "entities": []}, {"text": "In \u00a74, we describe biased potential functions and follow with experimental results in \u00a75.", "labels": [], "entities": []}, {"text": "We discuss related work in \u00a76.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data set: We evaluate our approach with two coreference data sets: MUC6 (MUC-6, 1995) and MPQA 7 ().", "labels": [], "entities": [{"text": "MUC6 (MUC-6, 1995)", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.7934302687644958}, {"text": "MPQA 7", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.8914975821971893}]}, {"text": "For the MUC6 data set, we extract noun phrases (mentions) automatically, but for MPQA, we assume mentions for coreference resolution are given as in.", "labels": [], "entities": [{"text": "MUC6 data set", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9618392586708069}, {"text": "MPQA", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9190295934677124}, {"text": "coreference resolution", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.9565387964248657}]}, {"text": "For MUC6, we use the standard training/test data split.", "labels": [], "entities": [{"text": "MUC6", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.5232500433921814}, {"text": "training/test data split", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.6708311319351197}]}, {"text": "For MPQA, we use 150 documents for training, and 50 documents for testing.", "labels": [], "entities": []}, {"text": "for feature vector construction for each pair of mentions, and Finley and Joachims (2005) for constructing a training/testing instance for each document: a training/testing instance consists of all pairs of mentions in a document.", "labels": [], "entities": [{"text": "feature vector construction", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7048420310020447}]}, {"text": "Then, a single pair of mentions is a sub-instance.", "labels": [], "entities": []}, {"text": "We use the Mallet 9 implementation of CRFs, and set a Gaussian prior of 1.0 for all experiments.", "labels": [], "entities": [{"text": "Mallet 9", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.9308459162712097}, {"text": "Gaussian prior", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.9428873956203461}]}, {"text": "At each M-step, we train CRFs starting from the parameters from the previous M-step.", "labels": [], "entities": []}, {"text": "We train CRFs up to 200 iterations, but because we start training CRFs from the previous parameters, the convergence from the second M-step becomes much faster.", "labels": [], "entities": []}, {"text": "We apply up to 5 EM iterations, and choose best performing \u03b8 (t) , 2 \u2264 t \u2264 5 based on the performance on the training data.", "labels": [], "entities": []}, {"text": "10 Hypothesis: For the baseline (BASE) we employ the locally trained model for pairwise decisions without global inference.", "labels": [], "entities": [{"text": "BASE", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9830499291419983}]}, {"text": "Clustering is applied only attest time, in order to make the assignment on the output variables coherent.", "labels": [], "entities": []}, {"text": "We hypothesize that for the baseline, maximizing the likelihood for training will correlate more with the pairwise accuracy of the Available at http://nrrc.mitre.org/NRRC/publications.htm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.967099666595459}, {"text": "NRRC", "start_pos": 166, "end_pos": 170, "type": "DATASET", "confidence": 0.9221364259719849}]}, {"text": "8 In particular, our feature set corresponds to \"All Features\" in, and we discretized numeric values.", "labels": [], "entities": []}, {"text": "Available at http://mallet.cs.umass.edu.", "labels": [], "entities": []}, {"text": "Selecting \u03b8 (t) on a separate tuning data would be better, but the data for MUC6 in particular is very limited.", "labels": [], "entities": [{"text": "MUC6", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8781546354293823}]}, {"text": "Notice that we don't pick \u03b8 1 when reporting the performance of SLT, because it is identical to the baseline.", "labels": [], "entities": [{"text": "SLT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9431517124176025}]}, {"text": "incoherent decisions before clustering than the pairwise accuracy of the coherent decisions after clustering.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9737697839736938}]}, {"text": "We also hypothesize that by performing structured local training (SLT), maximizing the likelihood will correlate more with the pairwise accuracy after clustering.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9664928317070007}]}, {"text": "Data sets and configurations for experiments are identical to those used in \u00a73.", "labels": [], "entities": []}, {"text": "Hypothesis: We hypothesize that using biased potential functions, maximizing the likelihood for training can correlate better with F1-score or MUCscore than the pairwise accuracy.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9995471835136414}, {"text": "MUCscore", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9845694899559021}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.8695767521858215}]}, {"text": "In particular,  we hypothesize that biasing on every coreferent pair will correlate more with F1-score, and biasing on close coreferent pairs will correlate more with MUC-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9993002414703369}, {"text": "MUC-score", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.6314275860786438}]}, {"text": "In general, we expect that biasing on coreferent pairs will boost recall, potentially decreasing precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9994801878929138}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9980840682983398}]}], "tableCaptions": [{"text": " Table 1: Performance of Structured Local Training: SLT re-", "labels": [], "entities": [{"text": "SLT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9538799524307251}]}, {"text": " Table 2: Performance of Biased Potential Functions: pairwise", "labels": [], "entities": []}, {"text": " Table 3: Performance of Biased Potential Functions with", "labels": [], "entities": []}]}