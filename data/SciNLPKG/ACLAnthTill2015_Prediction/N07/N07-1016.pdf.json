{"title": [{"text": "Unsupervised Resolution of Objects and Relations on the Web", "labels": [], "entities": [{"text": "Unsupervised Resolution of Objects and Relations on the Web", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.8628937403361002}]}], "abstractContent": [{"text": "The task of identifying synonymous relations and objects, or Synonym Resolution (SR), is critical for high-quality information extraction.", "labels": [], "entities": [{"text": "Synonym Resolution (SR)", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.89324392080307}, {"text": "information extraction", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7538967132568359}]}, {"text": "The bulk of previous SR work assumed strong domain knowledge or hand-tagged training examples.", "labels": [], "entities": [{"text": "SR", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9820937514305115}]}, {"text": "This paper investigates SR in the context of unsupervised information extraction , where neither is available.", "labels": [], "entities": [{"text": "SR", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9867703914642334}, {"text": "information extraction", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.711174339056015}]}, {"text": "The paper presents a scalable, fully-implemented system for SR that runs in O(KN log N) time in the number of extractions N and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them.", "labels": [], "entities": [{"text": "SR", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9886894822120667}, {"text": "O", "start_pos": 76, "end_pos": 77, "type": "METRIC", "confidence": 0.9868608117103577}, {"text": "RESOLVER", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9364492893218994}]}, {"text": "Given two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and an estimated 68% recall and resolves relations with 90% precision and 35% recall.", "labels": [], "entities": [{"text": "RESOLVER", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.797214686870575}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9977250695228577}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9983099699020386}, {"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9968792200088501}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9960732460021973}]}], "introductionContent": [{"text": "Web Information Extraction (WIE) systems extract assertions that describe a relation and its arguments from Web text (e.g., (is capital of, D.C., United States)).", "labels": [], "entities": [{"text": "Web Information Extraction (WIE)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7231930245955785}]}, {"text": "WIE systems can extract hundreds of millions of assertions containing millions of different strings from the Web (e.g., the TEXTRUNNER system ().", "labels": [], "entities": []}, {"text": "1 WIE systems often extract assertions that describe the same real-world objector relation using different names.", "labels": [], "entities": []}, {"text": "For example, a WIE system might extract (is capital city of, Washington, U.S.), which describes the same relationship as above but contains a different name for the relation and each argument.", "labels": [], "entities": []}, {"text": "Synonyms are prevalent in text, and the Web corpus is no exception.", "labels": [], "entities": [{"text": "Web corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8689917922019958}]}, {"text": "Our data set of two million assertions extracted from a Web crawl contained over a half-dozen different names each for the United States and Washington, D.C., and three for the \"is capital of\" relation.", "labels": [], "entities": []}, {"text": "The top 80 most commonly extracted objects had an average of 2.9 extracted names per entity, and several had as many as 10 names.", "labels": [], "entities": []}, {"text": "The top 100 most commonly extracted relations had an average of 4.9 synonyms per relation.", "labels": [], "entities": []}, {"text": "We refer to the problem of identifying synonymous object and relation names as Synonym Resolution (SR).", "labels": [], "entities": [{"text": "Synonym Resolution (SR)", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.8470115900039673}]}, {"text": "An SR system for WIE takes a set of assertions as input and returns a set of clusters, with each cluster containing coreferential object strings or relation strings.", "labels": [], "entities": []}, {"text": "Previous techniques for SR have focused on one particular aspect of the problem, either objects or relations.", "labels": [], "entities": [{"text": "SR", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9908389449119568}]}, {"text": "In addition, the techniques either depend on a large set of training examples, or are tailored to a specific domain by assuming knowledge of the domain's schema.", "labels": [], "entities": []}, {"text": "Due to the number and diversity of the relations extracted, these tech-niques are not feasible for WIE systems.", "labels": [], "entities": []}, {"text": "Schemata are not available for the Web, and hand-labeling training examples for each relation would require a prohibitive manual effort.", "labels": [], "entities": []}, {"text": "In response, we present RESOLVER, a novel, domain-independent, unsupervised synonym resolution system that applies to both objects and relations.", "labels": [], "entities": [{"text": "RESOLVER", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9539092183113098}, {"text": "synonym resolution", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6971414089202881}]}, {"text": "RESOLVER clusters coreferential names together using a probabilistic model informed by string similarity and the similarity of the assertions containing the names.", "labels": [], "entities": []}, {"text": "Our contributions are: 1.", "labels": [], "entities": []}, {"text": "A scalable clustering algorithm that runs in time O(KN log N ) in the number of extractions N and maximum number of synonyms per word, K, without discarding any potentially matching pair, under exceptionally weak assumptions about the data.", "labels": [], "entities": [{"text": "O", "start_pos": 50, "end_pos": 51, "type": "METRIC", "confidence": 0.9623866081237793}]}, {"text": "2. An unsupervised probabilistic model for predicting whether two objector relation names co-refer.", "labels": [], "entities": []}, {"text": "3. An empirical demonstration that RESOLVER can resolve objects with 78% precision and 68% recall, and relations with 90% precision and 35% recall.", "labels": [], "entities": [{"text": "RESOLVER", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8731968998908997}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.998437225818634}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9982690811157227}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9982326030731201}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9954735636711121}]}, {"text": "The next section discusses previous work.", "labels": [], "entities": []}, {"text": "Section 3 introduces our probabilistic model for SR.", "labels": [], "entities": [{"text": "SR", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9812229871749878}]}, {"text": "Section 4 describes our clustering algorithm.", "labels": [], "entities": []}, {"text": "Section 5 describes extensions to our basic SR system.", "labels": [], "entities": []}, {"text": "Section 6 presents our experiments, and section 7 discusses our conclusions and areas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments demonstrate that the ESP model is significantly better at resolving synonyms than a widely-used distributional similarity metric, the cosine similarity metric (CSM), and that RESOLVER is significantly better at resolving synonyms than either of its components, SSM or ESP.", "labels": [], "entities": [{"text": "cosine similarity metric (CSM)", "start_pos": 150, "end_pos": 180, "type": "METRIC", "confidence": 0.771743247906367}, {"text": "RESOLVER", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.8846781253814697}]}, {"text": "We test these models on a data set of 2.1 million assertions extracted from a Web crawl.", "labels": [], "entities": []}, {"text": "All models ran overall assertions, but compared only those objects or relations that appeared at least 25 times in the data, to give the ESP and CSM models sufficient data for estimating similarity.", "labels": [], "entities": []}, {"text": "However, the models douse strings that appear less than 25 times as features.", "labels": [], "entities": []}, {"text": "In all, the data contains 9,797 distinct object strings and 10,151 distinct relation strings that appear at least 25 times.", "labels": [], "entities": []}, {"text": "We judged the precision of each model by manually labeling all of the clusters that each model outputs.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.999043881893158}]}, {"text": "Judging recall would require inspecting not just the clusters that the system outputs, but the entire data set, to find all of the true clusters.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9815496802330017}]}, {"text": "Because of the size of the data set, we instead estimated recall over a smaller subset of the data.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9984051585197449}]}, {"text": "We took the top 200 most frequent object strings and top 200 most frequent relation strings in the data.", "labels": [], "entities": []}, {"text": "For each one of these high-frequency strings, we manually searched through all strings with frequency over 25 that shared at least one property, as well as all strings that contained one of the keywords in the high-frequency strings or obvious variations of them.", "labels": [], "entities": []}, {"text": "We manually clustered the resulting matches.", "labels": [], "entities": []}, {"text": "The top 200 object strings formed 51 clusters of size greater than one, with an average cluster size of 2.9.", "labels": [], "entities": []}, {"text": "For relations, the top 200 strings and their matches formed 110 clusters with size greater than one, with an average cluster size of 4.9.", "labels": [], "entities": []}, {"text": "We measured the recall of our models by comparing the set of all clusters containing at least one of the high-frequency words against these gold standard clusters.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.999098539352417}]}, {"text": "For our precision and recall measures, we only compare clusters of size two or more, in order to focus on the interesting cases.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.999481737613678}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.998794674873352}]}, {"text": "Using the term hypothesis cluster for clusters created by one of the models, we define the precision of a model to be the number of elements in all hypothesis clusters which are correct divided by the total number of elements in hypothesis clusters.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9956854581832886}]}, {"text": "An element sis marked correct if a plurality of the elements in s's cluster refer to the same entity ass; we break ties arbitrarily, as The data is made available at http://www.cs.washington.edu/homes/ayates/.", "labels": [], "entities": []}, {"text": "they do not affect results.", "labels": [], "entities": []}, {"text": "We define recall as the sum over gold standard clusters of the most number of elements found in a single hypothesis cluster, divided by the total number of elements in gold standard clusters.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9986169338226318}]}, {"text": "For the ESP and SSM models in our experiment, we prevented mutual recursion by clustering relations and objects separately.", "labels": [], "entities": []}, {"text": "Only the full RE-SOLVER system uses mutual recursion.", "labels": [], "entities": [{"text": "RE-SOLVER", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.8303536772727966}]}, {"text": "For the CSM model, we create for each distinct string a row vector, with each column representing a property.", "labels": [], "entities": []}, {"text": "If that property applies to the string, we set the value of that column to the inverse frequency of the property and zero otherwise.", "labels": [], "entities": []}, {"text": "CSM finds the cosine of the angle between the vectors for each pair of strings, and merges the best pairs that score above threshold.", "labels": [], "entities": []}, {"text": "Each model requires a threshold parameter to determine which scores are suitable for merging.", "labels": [], "entities": []}, {"text": "For these experiments we arbitrarily chose a threshold of 3 for the ESP model (that is, the data needs to be 3 times more likely given the merged cluster than the unmerged clusters in order to perform the merge) and chose thresholds for the other models by hand so that the difference between them and ESP would be roughly even between precision and recall, although for relations it was harder to improve the recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 336, "end_pos": 345, "type": "METRIC", "confidence": 0.998661994934082}, {"text": "recall", "start_pos": 350, "end_pos": 356, "type": "METRIC", "confidence": 0.9938230514526367}, {"text": "recall", "start_pos": 410, "end_pos": 416, "type": "METRIC", "confidence": 0.9982830286026001}]}, {"text": "It is an important item for future work to be able to estimate these thresholds and perhaps other parameters of our models from unlabeled data, but the chosen parameters worked well enough for the experiments.", "labels": [], "entities": []}, {"text": "shows the precision and recall of our models.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994931221008301}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9993128776550293}]}, {"text": "The extensions to RESOLVER attempt to address the confusion between similar and identical pairs.", "labels": [], "entities": [{"text": "RESOLVER", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.6167465448379517}]}, {"text": "Experiments with the extensions, using the same datasets and metrics as above, demonstrate that the Function Filter (FF) and the Coordination-Phrase Filter (CPF) boost RESOLVER's performance.", "labels": [], "entities": []}, {"text": "FF requires as input the set of functional and oneto-one relations in the data.", "labels": [], "entities": [{"text": "FF", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.815302312374115}]}, {"text": "contains a samis capital of is capital city of named after was named after headquartered in is headquartered in   pling of the manually-selected functions used in our experiment.", "labels": [], "entities": []}, {"text": "Automatically discovering such functions from extractions has been addressed in AnaMaria Popescu's dissertation, and we did not attempt to duplicate this effort in RE-SOLVER.", "labels": [], "entities": [{"text": "RE-SOLVER", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.6106969714164734}]}, {"text": "contains the results of our experiments.", "labels": [], "entities": []}, {"text": "With coordination-phrase filtering, RESOLVER's F1 is 28% higher than SSM's on objects, and 6% higher than RESOLVER's F1 without filtering.", "labels": [], "entities": [{"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9422712922096252}]}, {"text": "While function filtering is a promising idea, FF provides a smaller benefit than CPF on this dataset, and the merges that it prevents are, with a few exceptions, a subset of the merges prevented by CPF.", "labels": [], "entities": [{"text": "function filtering", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7670375406742096}]}, {"text": "This is in part due to the limited number of functions available in the data.", "labels": [], "entities": []}, {"text": "In addition to outperforming FF on this dataset, CPF has the added advantage that it does not require additional input, like a set of functions.", "labels": [], "entities": [{"text": "FF", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9326140880584717}]}], "tableCaptions": [{"text": " Table 1: Comparison of the cosine similarity metric (CSM), RESOLVER components (SSM and ESP), and the RESOLVER", "labels": [], "entities": [{"text": "cosine similarity metric (CSM)", "start_pos": 28, "end_pos": 58, "type": "METRIC", "confidence": 0.7599755227565765}]}, {"text": " Table 2: A sample of the set of functions used by the Func-", "labels": [], "entities": [{"text": "Func-", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.8854385316371918}]}, {"text": " Table 3: Comparison of object merging results for the", "labels": [], "entities": [{"text": "object merging", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.6829163283109665}]}]}