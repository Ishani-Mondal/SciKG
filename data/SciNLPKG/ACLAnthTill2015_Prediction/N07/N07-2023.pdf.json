{"title": [{"text": "A Geometric Interpretation of Non-Target-Normalized Maximum Cross-channel Correlation for Vocal Activity Detection in Meetings", "labels": [], "entities": [{"text": "Vocal Activity Detection in Meetings", "start_pos": 90, "end_pos": 126, "type": "TASK", "confidence": 0.7587799191474914}]}], "abstractContent": [{"text": "Vocal activity detection is an important technology for both automatic speech recognition and automatic speech understanding.", "labels": [], "entities": [{"text": "Vocal activity detection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6083046197891235}, {"text": "automatic speech recognition", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6129780113697052}, {"text": "automatic speech understanding", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.5992982983589172}]}, {"text": "In meetings, standard vocal activity detection algorithms have been shown to be ineffective, because participants typically vocalize for only a fraction of the recorded time and because, while they are not vocalizing, their channels are frequently dominated by crosstalk from other participants.", "labels": [], "entities": [{"text": "vocal activity detection", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7688394983609518}]}, {"text": "In the present work, we review a particular type of normaliza-tion of maximum cross-channel correlation, a feature recently introduced to address the crosstalk problem.", "labels": [], "entities": []}, {"text": "We derive a plausible geometric interpretation and show how the frame size affects performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vocal activity detection (VAD) is an important technology for any application with an automatic speech recognition (ASR) front end.", "labels": [], "entities": [{"text": "Vocal activity detection (VAD)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7682173401117325}, {"text": "automatic speech recognition (ASR)", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.7819246351718903}]}, {"text": "In meetings, participants typically vocalize for only a fraction of the recorded time.", "labels": [], "entities": []}, {"text": "Their temporally contiguous contributions should be identified prior to ASR in order to leverage speaker adaptation schemes and language model constraints, and to associate recognized output with specific speakers (who said what).", "labels": [], "entities": [{"text": "ASR", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9859419465065002}]}, {"text": "Segmentation into such contributions is informed primarily by VAD on a frame-by-frame basis.", "labels": [], "entities": [{"text": "VAD", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.5950184464454651}]}, {"text": "Individual head-mounted microphone (IHM) recordings of meetings present a particular challenge for VAD, due to crosstalk from other participants.", "labels": [], "entities": [{"text": "VAD", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.975635290145874}]}, {"text": "Most state-of-the-art VAD systems for meetings rely on decoding in a binary speech/non-speech space, assuming independence among participants, but are increasingly relying on features specifically designed to address the crosstalk issue (.", "labels": [], "entities": [{"text": "VAD", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.961753249168396}]}, {"text": "A feature which has attracted attention since its use in VAD post-processing in () is the maximum cross-channel correlation (XC), max \u03c4 \u03c6 jk (\u03c4 ), between channels j and k, where \u03c4 is the lag.", "labels": [], "entities": [{"text": "VAD post-processing", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8901286423206329}, {"text": "cross-channel correlation (XC)", "start_pos": 98, "end_pos": 128, "type": "METRIC", "confidence": 0.8194776177406311}]}, {"text": "When designing features descriptive of the kth channel, XC is frequently normalized by the energy in the target 1 channel k (.", "labels": [], "entities": [{"text": "XC", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.8787792325019836}]}, {"text": "Alternately, XC can be normalized by the energy in the non-target channel j (), a normalization which we refer to here as NT-Norm, extending the Normand S-Norm naming conventions in ().", "labels": [], "entities": []}, {"text": "shows several types of normalizations which have been explored.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments we present aim to compare ILAave and ILAmin, and to show how the size of the integration window, \u2126, affects system performance.", "labels": [], "entities": []}, {"text": "As our VAD decoder operates at a frame size of 100ms, we introduce a reframing step between the ILA component and both AM training and decoding; see Figure 1.", "labels": [], "entities": []}, {"text": "V is assigned to each 100ms frame if 50% or more of the frame duration is assigned V by ILA; otherwise, the 100ms frame is assigned an N label.", "labels": [], "entities": [{"text": "V", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.8878852725028992}]}, {"text": "We measure performance in four locations within the combined VAD+ASR system architecture, also shown in.", "labels": [], "entities": [{"text": "VAD+ASR", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.6149821281433105}]}, {"text": "We compute a VAD frame error just after reframing (\u02dc q F ), just after decoding (q * ), and just after smoothing (\u03c3 (q * )).", "labels": [], "entities": [{"text": "VAD frame error", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.821970264116923}]}, {"text": "This error is the sum of the miss rate (MS), and the false alarm rate excluding intervals of all-participant silence (FAX), computed against unsmoothed wordlevel forced alignment references.", "labels": [], "entities": [{"text": "miss rate (MS)", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.966215169429779}, {"text": "false alarm rate excluding intervals of all-participant silence (FAX)", "start_pos": 53, "end_pos": 122, "type": "METRIC", "confidence": 0.7835770439017903}]}, {"text": "We use this metric for comparative purposes only, across the various measurement points.", "labels": [], "entities": []}, {"text": "We also use first-pass ASR word error rates (WERs), after lattice rescoring, as a final measure of performance impact.", "labels": [], "entities": [{"text": "ASR word error rates (WERs)", "start_pos": 23, "end_pos": 50, "type": "METRIC", "confidence": 0.8562393188476562}]}, {"text": "We evaluate, over a range of ILA frame sizes, the performance of ILAave, with a maximum number of simultaneously vocalizing participants of 3, and for the contrastive ILAmin.", "labels": [], "entities": []}, {"text": "We note that ILAmin is capable of declaring at most one microphone at a time as being worn by a current speaker.", "labels": [], "entities": []}, {"text": "As a result, construction of acoustic models for overlapped vocal activity states, described in (), results in states of at most 2 simultaneously vocalizing participants.", "labels": [], "entities": []}, {"text": "We therefore refer to ILAmin as ILAmin, and additionally consider ILAave(2), in which states with 3 simultaneously vocalizing participants are removed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: VAD errors, measured at three points in our  system, and first-pass WERs for rt05s eval (05),  as well as first-pass WERs for rt05s eval* (05*)  and rt06s eval (06). Results are shown for 3 con- trastive VAD systems (ILAave(3), ILAave(2) and  ILAmin", "labels": [], "entities": [{"text": "VAD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8730380535125732}, {"text": "WERs", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.8497574925422668}, {"text": "ILAmin", "start_pos": 253, "end_pos": 259, "type": "DATASET", "confidence": 0.669761061668396}]}]}