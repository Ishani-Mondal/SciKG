{"title": [{"text": "Chinese Named Entity Recognition with Cascaded Hybrid Model", "labels": [], "entities": [{"text": "Chinese Named Entity Recognition", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7510569989681244}]}], "abstractContent": [{"text": "We propose a high-performance cascaded hybrid model for Chinese NER.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.8169204890727997}]}, {"text": "Firstly, we use Boosting, a standard and theoretically well-founded machine learning method to combine a set of weak classifiers together into abase system.", "labels": [], "entities": []}, {"text": "Secondly, we introduce various types of heuristic human knowledge into Markov Logic Networks (MLNs), an effective combination of first-order logic and probabilistic graphi-cal models to validate Boosting NER hypotheses.", "labels": [], "entities": [{"text": "Boosting NER hypotheses", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.4500308930873871}]}, {"text": "Experimental results show that the cascaded hybrid model significantly outperforms the state-of-the-art Boosting model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER) involves the identification and classification of certain proper nouns in text, such as person names (PERs), locations (LOCs), organizations (ORGs), miscellaneous names (MISCs), temporal, numerical and monetary phrases.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8090236733357111}, {"text": "identification and classification of certain proper nouns in text, such as person names (PERs), locations (LOCs), organizations (ORGs), miscellaneous names (MISCs)", "start_pos": 44, "end_pos": 207, "type": "TASK", "confidence": 0.6377484039826826}]}, {"text": "It is a wellestablished task in the NLP community and is regarded as crucial technology for many NLP applications, such as information extraction, question answering, information retrieval and machine translation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.830938994884491}, {"text": "question answering", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.9134090542793274}, {"text": "information retrieval", "start_pos": 167, "end_pos": 188, "type": "TASK", "confidence": 0.8210121691226959}, {"text": "machine translation", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.8190529346466064}]}, {"text": "Compared to European-language NER, Chinese NER seems to be more difficult ().", "labels": [], "entities": []}, {"text": "Recent approaches to Chinese NER area shift away from manually constructed rules or finite state patterns towards machine learning or statistical methods.", "labels": [], "entities": [{"text": "NER area", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.7739690244197845}]}, {"text": "However, rulebased NER systems lack robustness and portability, and machine learning approaches might be unsatisfactory to learn linguistic information in Chinese NEs.", "labels": [], "entities": []}, {"text": "In fact, Chinese NEs have distinct linguistic characteristics in their composition and human beings usually use prior knowledge to recognize NEs.", "labels": [], "entities": []}, {"text": "For example, about 365 of the highest frequently used surnames cover 99% Chinese surnames.", "labels": [], "entities": []}, {"text": "For the LOC \"\u00ac \u00a2/Beijing City\", \"\u00ac/Beijing\" is the name part and \"\u00a2/City\" is the salient word.", "labels": [], "entities": [{"text": "LOC \"\u00ac \u00a2/Beijing City", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.8138324379920959}]}, {"text": "\u009c/BeijingCityGovernment\", \"\u00ac/Beijing\" is the LOC name part, \"\u00a2/City\" is the LOC salient word and \"?", "labels": [], "entities": [{"text": "BeijingCityGovernment", "start_pos": 2, "end_pos": 23, "type": "DATASET", "confidence": 0.905034601688385}]}, {"text": "\u009c/Government\" is the ORG salient word.", "labels": [], "entities": [{"text": "ORG", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9820681214332581}]}, {"text": "Some ORGs contain one or more PERs, LOCs, MISCs and ORGs.", "labels": [], "entities": []}, {"text": "A more complex example is the nested ORG \"V\u00a1fI \u00a2fI'f\u00a1\u0097:fb/School of Computer Science, Wuhan University, Wuhan City, Hubei Province\" which contains two ORGs \"fI'f/Wuhan University\" and \"\u00a1 \u0097:fb/School of Computer Science\" and two LOCs \"V \u00a1/Hubei Province\" and \"f I\u00a2/Wuhan City\".", "labels": [], "entities": []}, {"text": "The two ORGs contain ORG salient words \"' f/University\" and \"fb/School\", while the two LOCs contain LOC salient words \"\u00a1/Province\" and \"\u00a2/City\" respectively.", "labels": [], "entities": [{"text": "ORGs", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9551069736480713}, {"text": "ORG", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9444701075553894}]}, {"text": "Inspired by the above observation, we propose a cascaded hybrid model for Chinese NER . First, we employ Boosting, which has theoretical justification and has been shown to perform well on other NLP problems, to combine weak classifiers into a strong classifier.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.7285272777080536}]}, {"text": "We then exploit a variety of heuristic human knowledge into MLNs, a powerful combination of logic and probability, to validate Boosting NER hypotheses.", "labels": [], "entities": [{"text": "Boosting NER hypotheses", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.43502310911814374}]}, {"text": "We also use three Markov chain Monte Carlo (MCMC) algorithms for probabilistic inference in MLNs.", "labels": [], "entities": []}, {"text": "Experimental results show that the cascaded hybrid model yields better NER results than the stand-alone Boosting model by a large margin.", "labels": [], "entities": [{"text": "NER", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8818647861480713}]}], "datasetContent": [{"text": "We randomly selected 15,000 and 3,000 sentences from the People's Daily corpus as training and test sets, respectively.", "labels": [], "entities": [{"text": "People's Daily corpus", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.9526847451925278}]}, {"text": "We used the decision stump 2 as the weak classifier in Boosting to construct a character-based Chinese NER baseline system.", "labels": [], "entities": [{"text": "Boosting", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.8835185170173645}, {"text": "Chinese NER baseline", "start_pos": 95, "end_pos": 115, "type": "DATASET", "confidence": 0.7882409493128458}]}, {"text": "The features used were as follows: \u2022 The current character and its POS tag.", "labels": [], "entities": []}, {"text": "\u2022 The characters within a window of 3 characters before and after the current character.", "labels": [], "entities": []}, {"text": "\u2022 The POS tags within a window of 3 characters before and after the current character.", "labels": [], "entities": []}, {"text": "\u2022 A small set of conjunctions of POS tags and characters within a window of 3 characters of the current character.", "labels": [], "entities": []}, {"text": "\u2022 The BIO 3 chunk tags of the previous 3 characters.", "labels": [], "entities": [{"text": "BIO", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.6752265095710754}]}, {"text": "We declared 10 predicates and specified 9 firstorder formulas according to the heuristic human knowledge in Section 4.", "labels": [], "entities": []}, {"text": "For example, we used person(candidate) to predicate whether a candidate is a PER.", "labels": [], "entities": [{"text": "predicate whether a candidate is a PER", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.6060390216963631}]}, {"text": "Formulas are recursively constructed from atomic formulas using logical connectives and quantifiers.", "labels": [], "entities": []}, {"text": "They are constructed using four types of symbols: constants, variables, functions, and predicates.", "labels": [], "entities": []}, {"text": "Constant symbols represent objects in the domain of interest (e.g., \"\u00ac/Beijing\" and \" w/Shanghai\" are LOCs).", "labels": [], "entities": []}, {"text": "Variable symbols range over the objects in the domain.", "labels": [], "entities": []}, {"text": "Function symbols represent mappings from tuples of objects to objects.", "labels": [], "entities": []}, {"text": "Predicate symbols represent relations among objects in the domain or attributes of objects.", "labels": [], "entities": []}, {"text": "For example, the formula endwith(r, p)\u02c6locsalientword(p)=>location(r) means if r ends with a location salient word p, then it is a LOC.", "labels": [], "entities": []}, {"text": "We extracted all the distinct NEs (4,475 PERs, 2,170 LOCs, 2,823 ORGs and 614 MISCs) from the 15,000-sentence training corpus.", "labels": [], "entities": []}, {"text": "An MLN training database, which consists of 10 predicates and 44,810 ground atoms was built.", "labels": [], "entities": []}, {"text": "A ground atom is anatomic formula all of whose arguments are ground terms (terms containing no variables).", "labels": [], "entities": []}, {"text": "For example, the ground atom location( \u00ac\u00a2) conveys that \" \u00ac\u00a2/Beijing City\" is a LOC.", "labels": [], "entities": [{"text": "Beijing City", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.7989448010921478}]}, {"text": "During MLN learning, each formula is converted to Conjunctive Normal Form (CNF), and a weight is learned for each of its clauses.", "labels": [], "entities": [{"text": "MLN learning", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.9206619560718536}]}, {"text": "The weight of a clause is used as the mean of a Gaussian prior for the learned weight.", "labels": [], "entities": []}, {"text": "These weights reflect how often the clauses are actually observed in the training data.", "labels": [], "entities": []}, {"text": "We validated 352 Boosting results to construct the MLN testing database, which contains 1,285 entries and these entries are used as evidence for inference.", "labels": [], "entities": [{"text": "MLN testing database", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.684973398844401}]}, {"text": "Inference is performed by grounding the minimal subset of the network required for answering the query predicates.", "labels": [], "entities": []}, {"text": "We applied 3 MCMC algorithms: Gibbs sampling (GS), MC-SAT and Simulated Tempering (ST) for inference and the comparative NER results are shown in.", "labels": [], "entities": [{"text": "NER", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.8935782313346863}]}, {"text": "The cascaded hybrid model greatly outperforms the Boosting model.", "labels": [], "entities": [{"text": "Boosting", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.8267661333084106}]}, {"text": "We obtained the same results using GS and ST algorithms.", "labels": [], "entities": [{"text": "GS", "start_pos": 35, "end_pos": 37, "type": "DATASET", "confidence": 0.6774663925170898}]}, {"text": "And GS (or ST) yields slightly better results than the MC-SAT algorithm.", "labels": [], "entities": [{"text": "GS", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.8960438966751099}]}], "tableCaptions": []}