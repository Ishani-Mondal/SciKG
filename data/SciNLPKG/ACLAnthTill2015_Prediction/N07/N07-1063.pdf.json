{"title": [{"text": "An Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MT", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.5500316619873047}]}], "abstractContent": [{"text": "We present an efficient, novel two-pass approach to mitigate the computational impact resulting from online intersection of an n-gram language model (LM) and a probabilistic synchronous context-free grammar (PSCFG) for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 219, "end_pos": 250, "type": "TASK", "confidence": 0.7417440215746561}]}, {"text": "In first pass CYK-style decoding , we consider first-best chart item approximations , generating a hypergraph of sentence spanning target language derivations.", "labels": [], "entities": []}, {"text": "In the second stage, we instantiate specific alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the first pass.", "labels": [], "entities": []}, {"text": "Model search errors in our approach are comparable to those made by the state-of-the-art \"Cube Pruning\" approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax-driven () and hierarchical translation models) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process.", "labels": [], "entities": []}, {"text": "These models extend the domain of locality (over phrase-based models) during decoding, representing a significantly larger search space of possible translation derivations.", "labels": [], "entities": []}, {"text": "While PSCFG models are often induced with the goal of producing grammatically correct target translations as an implicit syntaxstructured language model, we acknowledge the value of n-gram language models (LM) in phrasebased approaches.", "labels": [], "entities": []}, {"text": "Integrating n-gram LMs into PSCFGs based decoding can be viewed as online intersection of the PSCFG grammar with the finite state machine represented by the n-gram LM, dramatically increasing the effective number of nonterminals in the decoding grammar, rendering the decoding process essentially infeasible without severe, beam-based lossy pruning.", "labels": [], "entities": []}, {"text": "The alternative, simply decoding without the n-gram LM and rescoring N-best alternative translations, results in substantially more search errors, as shown in ().", "labels": [], "entities": []}, {"text": "Our two-pass approach involves fast, approximate synchronous parsing in a first stage, followed by a second, detailed exploration through the resulting hypergraph of sentence spanning derivations, using the n-gram LM to drive that search.", "labels": [], "entities": []}, {"text": "This achieves search errors comparable to a strong \"Cube Pruning\", single-pass baseline.", "labels": [], "entities": []}, {"text": "The first pass corresponds to a severe parameterization of Cube Pruning considering only the first-best (LM integrated) chart item in each cell while maintaining unexplored alternatives for second-pass consideration.", "labels": [], "entities": []}, {"text": "Our second stage allows the integration of long distance and flexible history n-gram LMs to drive the search process, rather than simply using such models for hypothesis rescoring.", "labels": [], "entities": [{"text": "hypothesis rescoring", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.7731852531433105}]}, {"text": "We begin by discussing the PSCFG model for statistical machine translation, motivating the need for effective n-gram LM integration during decoding.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.7062322696050009}]}, {"text": "We then present our two-pass approach and discuss Cube Pruning as a state-of-the-art baseline.", "labels": [], "entities": []}, {"text": "We present results in the form of search error analysis and translation quality as measured by the BLEU score () on the IWSLT 06 text translation task , comparing Cube Pruning with our two-pass approach.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9626365005970001}, {"text": "IWSLT 06 text translation task", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.8092905521392822}]}], "datasetContent": [{"text": "We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2m target words).", "labels": [], "entities": [{"text": "IWSLT 2006 Chinese to English translation task", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.8225488833018711}, {"text": "Full BTEC corpus", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.8047809998194376}]}, {"text": "The evaluation test set contains 500 sentences with an average length of 10.3 source words.", "labels": [], "entities": []}, {"text": "Grammar rules were induced with the syntaxbased SMT system \"SAMT\" described in), which requires initial phrase alignments that we generated with \"GIZA++\" (, and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. pre-trained on the Penn Treebank.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8818114399909973}, {"text": "Stanford Parser (D. pre-trained on the Penn Treebank", "start_pos": 234, "end_pos": 286, "type": "DATASET", "confidence": 0.7321705288357205}]}, {"text": "All these systems are freely available on the web.", "labels": [], "entities": []}, {"text": "We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (.", "labels": [], "entities": []}, {"text": "The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals.", "labels": [], "entities": [{"text": "Penn Treebank nonterminals", "start_pos": 110, "end_pos": 136, "type": "DATASET", "confidence": 0.9746376474698385}]}, {"text": "Parameters \u03bb used to calculate P (D) are trained using MER training on development data.", "labels": [], "entities": [{"text": "P (D)", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9065552949905396}]}], "tableCaptions": []}