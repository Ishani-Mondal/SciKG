{"title": [{"text": "Question Answering using Integrated Information Retrieval and Information Extraction", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7920715510845184}, {"text": "Integrated Information Retrieval", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6505848666032156}, {"text": "Information Extraction", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7250348627567291}]}], "abstractContent": [{"text": "This paper addresses the task of providing extended responses to questions regarding specialized topics.", "labels": [], "entities": []}, {"text": "This task is an amalgam of information retrieval, topical summarization, and Information Extraction (IE).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7915447950363159}, {"text": "topical summarization", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.6548816561698914}, {"text": "Information Extraction (IE)", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.8129199802875519}]}, {"text": "We present an approach which draws on methods from each of these areas , and compare the effectiveness of this approach with a query-focused summa-rization approach.", "labels": [], "entities": []}, {"text": "The two systems are evaluated in the context of the prosecution queries like those in the DARPA GALE distillation evaluation.", "labels": [], "entities": [{"text": "DARPA GALE distillation evaluation", "start_pos": 90, "end_pos": 124, "type": "DATASET", "confidence": 0.6748712807893753}]}], "introductionContent": [{"text": "As question-answering systems advance from handling factoid questions to more complex requests, they must be able to determine how much information to include while making sure that the information selected is indeed relevant.", "labels": [], "entities": []}, {"text": "Unlike factoid questions, there is no clear criterion that defines the kind of phrase that answers the question; instead, there maybe many phrases that could makeup an answer and it is often unclear in advance, how many.", "labels": [], "entities": []}, {"text": "As system developers, our goal is to yield high recall without sacrificing precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9988086223602295}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9972478747367859}]}, {"text": "In response to questions about particular events of interest that can be enumerated in advance, it is possible to perform a deeper semantic analysis focusing on the entities, relations, and sub-events of interest.", "labels": [], "entities": []}, {"text": "On the other hand, the deeper analysis maybe errorful and will also not always provide complete coverage of the information relevant to the query.", "labels": [], "entities": []}, {"text": "The challenge, therefore, is to blend a shallower, robust approach with the deeper approach in an effective way.", "labels": [], "entities": []}, {"text": "In this paper, we show how this can be achieved through a synergistic combination of information retrieval and information extraction.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7750934362411499}, {"text": "information extraction", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.8204182386398315}]}, {"text": "We interleave information retrieval (IR) and response generation, using IR in high precision mode in the first stage to return a small number of documents that are highly likely to be relevant.", "labels": [], "entities": [{"text": "interleave information retrieval (IR)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.7836528519789377}, {"text": "response generation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7033609449863434}]}, {"text": "Information extraction of entities and events within these documents is then used to pinpoint highly relevant sentences and associated words are selected to revise the query fora second pass of retrieval, improving recall.", "labels": [], "entities": [{"text": "Information extraction of entities and events within these documents", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.8531443244881101}, {"text": "recall", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.9972810745239258}]}, {"text": "As part of this process, we approximate the relevant context by measuring the proximity of the target name in the query and extracted events.", "labels": [], "entities": []}, {"text": "Our approach has been evaluated in the framework of the DARPA GALE 1 program.", "labels": [], "entities": [{"text": "DARPA GALE 1 program", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.766287237405777}]}, {"text": "One of the GALE evaluations involves responding to questions based on a set of question templates, ranging from broad questions like \"Provide information on X\", where X is an organization, to questions focused on particular classes of events.", "labels": [], "entities": [{"text": "GALE evaluations", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7623216509819031}]}, {"text": "For the experiments presented here, we used the GALE program's prosecution class of questions.", "labels": [], "entities": [{"text": "GALE program", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.6748393326997757}]}, {"text": "These are given in the following form: \"Describe the prosecution of X for Y,\" where X is a person and Y is a crime or charge.", "labels": [], "entities": []}, {"text": "Our results show that we are able to achieve higher accu-1 Global Autonomous Language Exploitation racy with a system that exploits the justice events identified by IE than with an approach based on query-focused summarization alone.", "labels": [], "entities": [{"text": "accu-1", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9939810633659363}, {"text": "Global Autonomous Language Exploitation racy", "start_pos": 59, "end_pos": 103, "type": "TASK", "confidence": 0.5925591945648193}]}, {"text": "In the following sections, we first describe the task and then review related work in questionanswering.", "labels": [], "entities": []}, {"text": "Section 3 details our procedure for finding answers as well as performing the information retrieval and information extraction tasks.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.8029743134975433}, {"text": "information extraction", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.7902291417121887}]}, {"text": "Section 4 compares the results of the two approaches.", "labels": [], "entities": []}, {"text": "Finally, we present our conclusion and plans for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of our evaluation are shown in.", "labels": [], "entities": []}, {"text": "We increased the number of test questions over the number used in the official GALE evaluation and we used only previously unseen questions.", "labels": [], "entities": [{"text": "GALE evaluation", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.7277170717716217}]}, {"text": "Documents for the baseline system were selected without use of the event annotations from Proteus.", "labels": [], "entities": []}, {"text": "We paired the 25 questions for judges, so that both the system's answer and the baseline answer were assigned to the same person.", "labels": [], "entities": []}, {"text": "We provided explicit instructions on the handling on implicit references, allowing the judges to use the context of the question and other answer sentences to determine if a sentence was relevant -following the practice of the GALE evaluation.", "labels": [], "entities": [{"text": "GALE evaluation", "start_pos": 227, "end_pos": 242, "type": "TASK", "confidence": 0.4877983182668686}]}, {"text": "Our judges were randomly assigned questions and asked whether the snippets, which in our case were individual sentences, were relevant or not; they could respond Relevant, Not Relevant or Don't Know.", "labels": [], "entities": []}, {"text": "In cases where references were unclear, the judges were asked to choose Don't Know and these were removed from the scoring.", "labels": [], "entities": []}], "tableCaptions": []}