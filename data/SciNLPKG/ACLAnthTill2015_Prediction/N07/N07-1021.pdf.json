{"title": [{"text": "Probabilistic Generation of Weather Forecast Texts", "labels": [], "entities": [{"text": "Probabilistic Generation of Weather Forecast Texts", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.4276428371667862}]}], "abstractContent": [{"text": "This paper reports experiments in which pCRU-a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space-is used to semi-automatically create several versions of a weather forecast text generator.", "labels": [], "entities": []}, {"text": "The generators are evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system, and (iii) a HALOGEN-style statistical generator.", "labels": [], "entities": []}, {"text": "The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators receive higher scores from human judges than forecasts written by experts.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The two automatic metrics used in the evaluations, and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9984886646270752}, {"text": "Pearson correlation", "start_pos": 119, "end_pos": 138, "type": "METRIC", "confidence": 0.9337278008460999}]}, {"text": "Input,,: Forecast texts (for 05-10-2000) generated by each of the pCRU generators, the SUMTIME-Hybrid system and three experts.", "labels": [], "entities": []}, {"text": "The corresponding input to the generators is shown in the first row.", "labels": [], "entities": []}, {"text": "BLEU () is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams (n \u2264 4 has become standard) that it shares with several reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9848971962928772}, {"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9952071309089661}]}, {"text": "BLEU also incorporates a 'brevity penalty' to counteract scores increasing as length decreases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9466352462768555}, {"text": "brevity penalty", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.9631734192371368}, {"text": "length", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9751180410385132}]}, {"text": "BLEU scores range from 0 to 1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9800041317939758}]}, {"text": "The NIST metric) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more weight to less frequent (hence more informative) n-grams.", "labels": [], "entities": [{"text": "NIST metric", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8198385834693909}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9966464638710022}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9961085915565491}, {"text": "NIST", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.915900468826294}]}, {"text": "There is evidence that NIST correlates better with human judgments than BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9979775547981262}]}, {"text": "The results below include human scores from two separate experiments.", "labels": [], "entities": []}, {"text": "The first was an experiment with 9 subjects experienced in reading marine forecasts (), the second is anew experiment with 14 similarly experienced subjects 3 . The main differences were that in Experiment 1, subjects rated on a scale from 0 to 5 and were asked for overall quality scores, whereas in Experiment 2, subjects rated on a 1-7 scale and were asked for language quality scores.", "labels": [], "entities": []}, {"text": "In comparing different pCRU modes, NIST and BLEU scores were computed against the test set part of the corpus which contains texts by five different authors.", "labels": [], "entities": [{"text": "NIST", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.6118109822273254}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9990735054016113}]}, {"text": "In the two human experiments, NIST and BLEU scores were computed against sets of multiple reference texts (2 for each date in Experiment 1, and 3 in Experiment 2) written by forecasters who had not contributed to the corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.995864748954773}]}, {"text": "One-way ANOVAs with post-hoc Tukey HSD tests were used to analyse variance and statistical significance of all results.", "labels": [], "entities": [{"text": "variance", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9216685891151428}]}, {"text": "the systems included in the evaluations reported below, together with the corresponding input and three texts created by humans for the same data.", "labels": [], "entities": []}, {"text": "shows results for the five different pCRU generation modes, for training sets (top) and test sets (bottom), in terms of NIST-5 and BLEU-4 scores averaged over the five runs of the hold-out validation, with average mean deviation figures across the runs shown in brackets.", "labels": [], "entities": [{"text": "NIST-5", "start_pos": 120, "end_pos": 126, "type": "DATASET", "confidence": 0.9079791903495789}, {"text": "BLEU-4", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9972493052482605}]}], "tableCaptions": []}