{"title": [], "abstractContent": [{"text": "We introduce an answer typing strategy specific to quantifiable how questions.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.749532550573349}]}, {"text": "Using the web as a data source, we automatically collect answer units appropriate to a given how-question type.", "labels": [], "entities": []}, {"text": "Experimental results show answer typing with these units outperforms traditional fixed-category answer typing and other strategies based on the occurrences of numerical entities in text.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9133220613002777}, {"text": "answer typing", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.6955537497997284}]}], "introductionContent": [{"text": "Question answering (QA) systems are emerging as a viable means of obtaining specific information in the face of large availability.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9145977020263671}]}, {"text": "Answer typing is an important part of QA because it allows the system to greatly reduce the number of potential answers, using general knowledge of the answer form fora specific question.", "labels": [], "entities": [{"text": "Answer typing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9009514451026917}, {"text": "QA", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9499456286430359}]}, {"text": "For example, for what, where, and who questions like \"What is the capital of Canada?\", answer typing can filter the phrases which might be proposed as candidate answers, perhaps only identifying those textual entities known to be cities.", "labels": [], "entities": []}, {"text": "We focus on answer typing for how-questions, a subset of questions which have received little specific attention in the QA community.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.8749160766601562}]}, {"text": "Rather than seeking an open-ended noun or verb phrase, howquestions often seek a numerical measurement expressed in terms of a certain kind of unit, as in the following example: Example 1: \"How heavy is a grizzly bear?\"", "labels": [], "entities": []}, {"text": "An answer typing system might expect answers to include units like kilograms, pounds, or tons.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.7992494106292725}]}, {"text": "Entities with inappropriate units, such as feet, meters, or honey pots, would be excluded as candidate answers.", "labels": [], "entities": []}, {"text": "We specifically handle the subset of howquestions that we call how-adjective questions; that is, questions of the form \"How adjective...?\" such as Example 1.", "labels": [], "entities": []}, {"text": "In particular, we do not address \"how many\" questions, which usually specify the units directly following many, nor \"how much\" questions, which generally seek a monetary value.", "labels": [], "entities": []}, {"text": "Hand-crafting a comprehensive list of units appropriate to many different adjectives is timeconsuming and likely to miss important units.", "labels": [], "entities": []}, {"text": "For example, an annotator might miss gigabytes fora measure of \"how large.\"", "labels": [], "entities": []}, {"text": "Instead of compiling a list manually, we propose a means of automatically generating lists of appropriate units fora number of realworld questions.", "labels": [], "entities": []}, {"text": "How-adjective questions represent a significant portion of queries sent to search engines; of the 35 million queries in the AOL search query data set (), over 11,000 are of the form \"how adjective...\"", "labels": [], "entities": [{"text": "AOL search query data set", "start_pos": 124, "end_pos": 149, "type": "DATASET", "confidence": 0.8085700988769531}]}, {"text": "-close to one in every three thousand queries.", "labels": [], "entities": []}, {"text": "Of those 11,000 queries, 152 different adjectives are used, ranging from the expected \"how old\" and \"how far\" to the obscure \"how orwellian.\"", "labels": [], "entities": []}, {"text": "This high proportion of queries is especially striking given that search engines provide little support for answering how-adjective questions.", "labels": [], "entities": []}, {"text": "Indeed, most IR systems work by keyword matching.", "labels": [], "entities": [{"text": "IR", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9766632914543152}, {"text": "keyword matching", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7555277943611145}]}, {"text": "Entering Example 1 into a search engine returns documents discussing the grizzly's \"heavy fur,\" \"heavy, shaggy coat\" and \"heavy stout body.\"", "labels": [], "entities": []}, {"text": "When faced with such results, a smart search engine user knows to inject answer units into their query to refine their search, perhaps querying \"grizzly pounds.\"", "labels": [], "entities": []}, {"text": "They may also convert their adjective (heavy) to a related concept (weight), for the query \"grizzly weight.\"", "labels": [], "entities": []}, {"text": "Similarly, our approach discovers unit types by first converting the adjective to a related concept, using information in a structured ontology.", "labels": [], "entities": []}, {"text": "For example, \"big\" can be used to obtain \"size,\" and \"tall\" can derive \"height.\"", "labels": [], "entities": []}, {"text": "We then use an online search engine to automatically find units appropriate to the concept, given the assumption that the concept is explicitly measured in terms of specific units, e.g., height can be measured in feet, weight can be measured in pounds, and size can be measured in gigabytes.", "labels": [], "entities": []}, {"text": "By automatically extracting units, we do not require a set of prior questions with associated answers.", "labels": [], "entities": []}, {"text": "Instead, we use actual questions as a source of realistic adjectives only.", "labels": [], "entities": []}, {"text": "This is important because while large sets of existing questions can be obtained (), there are many fewer questions with available answers.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that how-questionspecific unit lists consistently achieve higher answer identification performance than fixed-type, generalpurpose answer typing (which propose all numerical entities as answer candidates).", "labels": [], "entities": [{"text": "answer identification", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7838627696037292}]}, {"text": "Furthermore, our precomputed, automatically-generated unit lists are shown to consistently achieve better performance than baseline systems which derive unit lists at runtime from documents relevant to the answer query, even when such documents are gathered using perfect knowledge of the answer distribution.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we outline related work.", "labels": [], "entities": []}, {"text": "In Section 3 we provide the framework of our answer-typing model.", "labels": [], "entities": []}, {"text": "Section 4 describes the implementation details of the model.", "labels": [], "entities": []}, {"text": "Section 5 describes our experimental methodology, while Section 6 shows the benefits of using automatic how-question answer-typing.", "labels": [], "entities": []}, {"text": "We conclude with possible directions of future research opened by this novel problem formulation.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents experiments comparing our how-adjective answer typing approach to alternative schemes on an answer identification task.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.7743076384067535}, {"text": "answer identification task", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.8624889651934305}]}, {"text": "We compare our two unit ranking functions ! \u00a2 \u00a1 % and ! \u00a2 \u00a1 % \u00a1 % ( Section 3.3) and test the merits of using the similar unit expansion (Section 4.2).", "labels": [], "entities": []}, {"text": "The clearest way to test a QA system is to evaluate it on a large set of questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9222463369369507}]}, {"text": "Although our answer typing system is not capable of fully answering questions, we will make use of the how-adjective questions from TREC) as a set of test data.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8499541580677032}, {"text": "TREC", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.7128652334213257}]}, {"text": "We take eight of the questions as a development set (used for preliminary investigations of scoring functions -no parameters can beset on the development set specifically) and 86 of the questions as a final, unseen test set.", "labels": [], "entities": []}, {"text": "Seventeen different adjectives occur in the test questions.", "labels": [], "entities": []}, {"text": "We evaluate our system with an approach we call Answer-Identification Precision Recall (AIPR).", "labels": [], "entities": [{"text": "Answer-Identification Precision Recall (AIPR)", "start_pos": 48, "end_pos": 93, "type": "TASK", "confidence": 0.5791447311639786}]}, {"text": "For a particular scoring threshold (Section 3.3), each adjective has a corresponding unit list, which is used to extract answer candidates from documents (Section 4.3).", "labels": [], "entities": []}, {"text": "To ensure the performance of the IR-engine is not an issue in evaluation, we only use documents judged to contain the correct answer by TREC.", "labels": [], "entities": [{"text": "IR-engine", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.8732133507728577}, {"text": "TREC", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.6166558861732483}]}, {"text": "Answer-identification precision corresponds to the number of correct answers among the candidate answers extracted by our system.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.8852663636207581}]}, {"text": "Answeridentification recall is the number of correct answers extracted among the total number of correct answers in the answer documents.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8952833414077759}]}, {"text": "A plot of AIPR allows the designer of a particular QA system to decide on the optimum PR-tradeoff for the answer typing task.", "labels": [], "entities": [{"text": "answer typing task", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8119799693425497}]}, {"text": "If other stages of QA rely on a large number of candidates, a high recall value maybe desired so no potential answers are missed.", "labels": [], "entities": [{"text": "QA", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9784970283508301}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9991771578788757}]}, {"text": "If answer typing is used as a means of boosting already-likely answers, high precision may instead be favoured.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.8379320800304413}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9989128112792969}]}], "tableCaptions": []}