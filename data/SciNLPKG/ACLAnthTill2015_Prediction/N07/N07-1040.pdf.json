{"title": [], "abstractContent": [{"text": "Scientific papers revolve around citations , and for many discourse level tasks one needs to know whose work is being talked about at any point in the discourse.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the scientific attribution task, which links different linguistic expressions to citations.", "labels": [], "entities": []}, {"text": "We discuss the suitability of different evaluation met-rics and evaluate our classification approach to deciding attribution both in-trinsically and in an extrinsic evaluation where information about scientific attribution is shown to improve performance on Argumentative Zoning, a rhetorical classification task.", "labels": [], "entities": [{"text": "rhetorical classification task", "start_pos": 282, "end_pos": 312, "type": "TASK", "confidence": 0.8033031622568766}]}], "introductionContent": [{"text": "In the recent past, there has been a focus on information management from scientific literature.", "labels": [], "entities": [{"text": "information management", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8736908137798309}]}, {"text": "In the genetics domain, for instance, information extraction of genes and gene-protein interactions helps geneticists scan large amounts of information (e.g., as explored in the TREC Genomics track).", "labels": [], "entities": [{"text": "information extraction of genes and gene-protein interactions", "start_pos": 38, "end_pos": 99, "type": "TASK", "confidence": 0.8472400307655334}, {"text": "TREC Genomics track", "start_pos": 178, "end_pos": 197, "type": "DATASET", "confidence": 0.8614331285158793}]}, {"text": "Elsewhere, citation indexes ( provide bibliometric data about the frequency with which particular papers are cited.", "labels": [], "entities": []}, {"text": "The success of citation indexers such as CiteSeer () and Google Scholar relies on the robust detection of formal citations in arbitrary text.", "labels": [], "entities": []}, {"text": "In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper; presents an approach for identifying the area around citations where the text focuses on that citation.", "labels": [], "entities": [{"text": "bibliographic information retrieval", "start_pos": 3, "end_pos": 38, "type": "TASK", "confidence": 0.6715530157089233}]}, {"text": "And automatic citation classification () determines the function that a citation plays in the discourse.", "labels": [], "entities": [{"text": "automatic citation classification", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.660365621248881}]}, {"text": "For such information access and retrieval purposes, the relevance of a citation within a paper is often crucial.", "labels": [], "entities": [{"text": "information access and retrieval", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.6822478696703911}]}, {"text": "One can estimate how important a citation is by simply counting how often it occurs in the paper.", "labels": [], "entities": []}, {"text": "But as argue, this ignores many expressions in text which refer to the cited author's work but which are not as easy to recognise as citations.", "labels": [], "entities": []}, {"text": "They address the resolution of instances of the third person personal pronoun \"they\" in astronomy papers: it can either refer to a citation or to some entities that are part of research within the paper (e.g., planets or galaxies).", "labels": [], "entities": [{"text": "resolution of instances of the third person personal pronoun \"they\" in astronomy papers", "start_pos": 17, "end_pos": 104, "type": "TASK", "confidence": 0.6998053848743438}]}, {"text": "Several applications should profit in principle from detecting connections between referring expressions and citations.", "labels": [], "entities": []}, {"text": "For instance, in citation function classification, the task is to find out if a citation is described as flawed or as useful.", "labels": [], "entities": [{"text": "citation function classification", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.8280529181162516}]}, {"text": "Consider: Most computational models of discourse are based primarily on an analysis of the intentions of the speakers[[ WEAK . The speaker will form intentions based on his goals and then act on these intentions, producing utterances.", "labels": [], "entities": []}, {"text": "The hearer will then reconstruct a model of the speaker's intentions upon hearing the utterance.", "labels": [], "entities": []}, {"text": "This approach has many strong points, but does not provide a very satisfactory account of the adherence to discourse conventions in dialogue.", "labels": [], "entities": []}, {"text": "The three citations above are described as flawed (detectable by \"does not provide a very satisfactory account\"), and thus receive the label Weak.", "labels": [], "entities": []}, {"text": "However, in order to detect this, one must first realise that \"this approach\" refers to the three cited papers.", "labels": [], "entities": []}, {"text": "A contrasting hypothesis could be that the citations are used (thus deserving the label Use; the cue phrase \"based on\" might make us think so (as in the context \"our work is based on\").", "labels": [], "entities": []}, {"text": "This, however, can be ruled out if we know that \"the speaker\" is not referring to some aspect of the current paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider two evaluation metrics.", "labels": [], "entities": []}, {"text": "The first is the scoring system used for the co-reference task in the Message Understanding Conferences MUC-6 and MUC-7.", "labels": [], "entities": [{"text": "Message Understanding Conferences MUC-6", "start_pos": 70, "end_pos": 109, "type": "TASK", "confidence": 0.7160726487636566}, {"text": "MUC-7", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.9345093369483948}]}, {"text": "The second is Krippendorff's \u03b1.", "labels": [], "entities": []}, {"text": "We briefly discuss both below.", "labels": [], "entities": []}, {"text": "We ran a machine learning experiment using 10-fold cross-validation and the memorybased learner IBk 3 (with k=6), using the Weka toolkit).", "labels": [], "entities": []}, {"text": "The performance is shown in.", "labels": [], "entities": []}, {"text": "To position these results we compare them with three baseline lower bounds and the human performance upper bound in.", "labels": [], "entities": []}, {"text": "We use three baselines: As shows, our machine learning approach performs much better than the baselines on all the agreement metrics, and is indeed closer to human performance than to any of the baselines.", "labels": [], "entities": []}, {"text": "The MUC evaluation appears to produce highly inflated results on our task -when there is a small set of co-reference classes and one of these classes contains 70% of data points, it takes only a small number of missing links to correct annotations.", "labels": [], "entities": [{"text": "MUC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6638795733451843}]}, {"text": "This results in unreasonably high values, particularly for the majority class baseline of labelling every data point as Current-Paper.", "labels": [], "entities": [{"text": "Current-Paper", "start_pos": 120, "end_pos": 133, "type": "DATASET", "confidence": 0.9604330062866211}]}, {"text": "We believe that the \u03b1 metrics provide a much more realistic estimate of the difficulty of the task and the relative performances of different approaches.", "labels": [], "entities": []}, {"text": "shows the performance of the machine learner for each of the three types of linguistic expressions considered.", "labels": [], "entities": []}, {"text": "Pronouns are the easiest to resolve, with on average 90% resolved correctly (an agreement with the human gold standard of \u03b1 = .71).", "labels": [], "entities": []}, {"text": "This drops to 85% (\u03b1 = .68) for definite descriptions and demonstratives, and further to 78% (\u03b1 = .63) for re-   maining work nouns (i.e., those not already in a definite noun phrase).", "labels": [], "entities": []}, {"text": "While all the features contributed to the reported results, the most important features (in terms of information gain) for deciding attribution to a citation were the paragraph level citation count 3(b), the distance features 2(a,b,c,d), the rank 3(a) and the Hobbs' prediction 1(d).", "labels": [], "entities": [{"text": "paragraph level citation count 3(b)", "start_pos": 167, "end_pos": 202, "type": "METRIC", "confidence": 0.7547614127397537}]}, {"text": "The most important features for deciding attribution to the current paper were the distance features 2(a,c,e), the rank 3(a) and the Hobbs' prediction 1(d).", "labels": [], "entities": []}, {"text": "To demonstrate the use of automatic scientific attribution classification, we studied its utility for one well known discourse annotation task: Argumentative Zoning ().", "labels": [], "entities": [{"text": "automatic scientific attribution classification", "start_pos": 26, "end_pos": 73, "type": "TASK", "confidence": 0.7100370079278946}, {"text": "Argumentative Zoning", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.6278207898139954}]}, {"text": "Argumentative Zoning (AZ) is the task of applying one of seven discourse level tags to each sentence in a scientific paper.", "labels": [], "entities": [{"text": "Argumentative Zoning (AZ)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8432486057281494}]}, {"text": "These categories model several aspects of scientific papers: from the distinction of segments by who an idea is attributed to, to the judgement of how the au-  Other work provides basis for own work to the rhetorical status of high-level discourse goals (statement of Aim; overview of section structure (Textual)).", "labels": [], "entities": []}, {"text": "Some of these categories (Background, Other and Own) occur in zones that span many sentences.", "labels": [], "entities": []}, {"text": "Other categories typically occur in short zones, often just a single sentence (Textual, Aim, Contrast, Basis).", "labels": [], "entities": []}, {"text": "In all work to date, classification of sentences into one of the AZ categories has been performed on the basis of features extracted from within the sentence, and a few contextual features such as section heading and location in document.", "labels": [], "entities": []}, {"text": "Scientific attribution links previously unresolved noun phrases or pronouns in the sentence to citations.", "labels": [], "entities": []}, {"text": "As this provides the machine learner with more information, AZ results should improve.", "labels": [], "entities": [{"text": "AZ", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9971601963043213}]}], "tableCaptions": [{"text": " Table 1: Agreement with Human Gold Standard", "labels": [], "entities": [{"text": "Human Gold", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8762001693248749}]}, {"text": " Table 2: Evaluation using MUC-6/7 software", "labels": [], "entities": [{"text": "MUC-6/7", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.7822391788164774}]}, {"text": " Table 3: Comparison with Baselines and Human  Performance (Averaged results)", "labels": [], "entities": []}, {"text": " Table 4: Results for different markable types", "labels": [], "entities": []}, {"text": " Table 5: Improvement on AZ from using auto- matic scientific attribution classification.", "labels": [], "entities": [{"text": "AZ", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9046613574028015}, {"text": "auto- matic scientific attribution classification", "start_pos": 39, "end_pos": 88, "type": "TASK", "confidence": 0.631513848900795}]}, {"text": " Table 6: Best AZ results using Stacked classifier:  with and without Attribution Features.", "labels": [], "entities": [{"text": "AZ", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.8303559422492981}]}, {"text": " Table 7: Teufel and Moens (2002)'s best AZ re- sults (Naive Bayes Classifier).", "labels": [], "entities": [{"text": "AZ re-", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.8382056752840678}]}]}