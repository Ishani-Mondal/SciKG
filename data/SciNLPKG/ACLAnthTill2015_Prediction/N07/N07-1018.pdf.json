{"title": [{"text": "Bayesian Inference for PCFGs via Markov chain Monte Carlo", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm.", "labels": [], "entities": [{"text": "Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings", "start_pos": 71, "end_pos": 158, "type": "TASK", "confidence": 0.7378581624764663}]}, {"text": "We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "The standard methods for inferring the parameters of probabilistic models in computational linguistics are based on the principle of maximum-likelihood estimation; for example, the parameters of Probabilistic Context-Free Grammars (PCFGs) are typically estimated from strings of terminals using the InsideOutside (IO) algorithm, an instance of the Expectation Maximization (EM) procedure).", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 348, "end_pos": 377, "type": "TASK", "confidence": 0.7691425561904908}]}, {"text": "However, much recent work in machine learning and statistics has turned away from maximum-likelihood in favor of Bayesian methods, and there is increasing interest in Bayesian methods in computational linguistics as well ().", "labels": [], "entities": []}, {"text": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms for inferring PCFGs and their parses from strings alone.", "labels": [], "entities": []}, {"text": "These can be viewed as Bayesian alternatives to the IO algorithm.", "labels": [], "entities": []}, {"text": "The goal of Bayesian inference is to compute a distribution over plausible parameter values.", "labels": [], "entities": []}, {"text": "This \"posterior\" distribution is obtained by combining the likelihood with a \"prior\" distribution P(\u03b8) over parameter values \u03b8.", "labels": [], "entities": []}, {"text": "In the case of PCFG inference \u03b8 is the vector of rule probabilities, and the prior might assert a preference fora sparse grammar (see below).", "labels": [], "entities": []}, {"text": "The posterior probability of each value of \u03b8 is given by Bayes' rule: P(\u03b8|D) \u221d P(D|\u03b8)P(\u03b8).", "labels": [], "entities": []}, {"text": "(1) In principle Equation 1 defines the posterior probability of any value of \u03b8, but computing this may not be tractable analytically or numerically.", "labels": [], "entities": []}, {"text": "For this reason a variety of methods have been developed to support approximate Bayesian inference.", "labels": [], "entities": []}, {"text": "One of the most popular methods is Markov chain Monte Carlo (MCMC), in which a Markov chain is used to sample from the posterior distribution.", "labels": [], "entities": []}, {"text": "This paper presents two new MCMC algorithms for inferring the posterior distribution over parses and rule probabilities given a corpus of strings.", "labels": [], "entities": []}, {"text": "The first algorithm is a component-wise Gibbs sampler which is very similar in spirit to the EM algorithm, drawing parse trees conditioned on the current parameter values and then sampling the parameters conditioned on the current set of parse trees.", "labels": [], "entities": []}, {"text": "The second algorithm is a component-wise Hastings sampler that \"collapses\" the probabilistic model, integrating over the rule probabilities of the PCFG, with the goal of speeding convergence.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 147, "end_pos": 151, "type": "DATASET", "confidence": 0.9590095281600952}]}, {"text": "Both algo-rithms use an efficient dynamic programming technique to sample parse trees.", "labels": [], "entities": []}, {"text": "Given their usefulness in other disciplines, we believe that Bayesian methods like these are likely to be of general utility in computational linguistics as well.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.724642425775528}]}, {"text": "As a simple illustrative example, we use these methods to infer morphological parses for verbs from Sesotho, a southern Bantu language with agglutinating morphology.", "labels": [], "entities": []}, {"text": "Our results illustrate that Bayesian inference using a prior that favors sparsity can produce linguistically reasonable analyses in situations in which EM does not.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The next section introduces the background for our paper, summarizing the key ideas behind PCFGs, Bayesian inference, and MCMC.", "labels": [], "entities": []}, {"text": "Section 3 introduces our first MCMC algorithm, a Gibbs sampler for PCFGs.", "labels": [], "entities": []}, {"text": "Section 4 describes an algorithm for sampling trees from the distribution over trees defined by a PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.950626015663147}]}, {"text": "Section 5 shows how to integrate out the rule weight parameters \u03b8 in a PCFG, allowing us to sample directly from the posterior distribution over parses fora corpus of strings.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.9166921973228455}]}, {"text": "Finally, Section 6 illustrates these methods in learning Sesotho morphology.", "labels": [], "entities": [{"text": "Sesotho morphology", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7510892152786255}]}], "datasetContent": [], "tableCaptions": []}