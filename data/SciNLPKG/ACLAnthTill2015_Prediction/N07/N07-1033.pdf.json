{"title": [{"text": "Using \"Annotator Rationales\" to Improve Machine Learning for Text Categorization *", "labels": [], "entities": [{"text": "Improve Machine Learning", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.831178347269694}, {"text": "Text Categorization", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7354107201099396}]}], "abstractContent": [{"text": "We propose anew framework for supervised machine learning.", "labels": [], "entities": []}, {"text": "Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with \"ra-tionales.\"", "labels": [], "entities": []}, {"text": "When annotating an example, the human teacher will also highlight evidence supporting this annotation-thereby teaching the machine learner why the example belongs to the category.", "labels": [], "entities": []}, {"text": "We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance significantly on a sample task, namely sentiment classification of movie reviews.", "labels": [], "entities": [{"text": "sentiment classification of movie reviews", "start_pos": 176, "end_pos": 217, "type": "TASK", "confidence": 0.9407199740409851}]}, {"text": "We hypothesize that in some situations, providing rationales is a more fruitful use of an an-notator's time than annotating more examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "Annotation cost is a bottleneck for many natural language processing applications.", "labels": [], "entities": []}, {"text": "While supervised machine learning systems are effective, it is laborintensive and expensive to construct the many training examples needed.", "labels": [], "entities": []}, {"text": "Previous research has explored active or semi-supervised learning as possible ways to lessen this burden.", "labels": [], "entities": []}, {"text": "We propose anew way of breaking this annotation bottleneck.", "labels": [], "entities": []}, {"text": "Annotators currently indicate what the correct answers are on training data.", "labels": [], "entities": []}, {"text": "We propose that they should also indicate why, at least by coarse hints.", "labels": [], "entities": []}, {"text": "We suggest new machine learning approaches that can benefit from this \"why\" information.", "labels": [], "entities": []}, {"text": "For example, an annotator who is categorizing phrases or documents might also be asked to highlight a few substrings that significantly influenced her judgment.", "labels": [], "entities": []}, {"text": "We call such clues \"rationales.\"", "labels": [], "entities": []}, {"text": "They need not correspond to machine learning features.", "labels": [], "entities": []}, {"text": "* This work was supported by the JHU WSE/APL Partnership Fund; National Science Foundation grant No. 0347822 to the second author; and an APL Hafstad Fellowship to the third.", "labels": [], "entities": [{"text": "JHU WSE/APL Partnership Fund", "start_pos": 33, "end_pos": 61, "type": "DATASET", "confidence": 0.8904802799224854}, {"text": "National Science Foundation grant No. 0347822", "start_pos": 63, "end_pos": 108, "type": "DATASET", "confidence": 0.8981668750445048}]}, {"text": "In some circumstances, rationales should not be too expensive or time-consuming to collect.", "labels": [], "entities": []}, {"text": "As long as the annotator is spending the time to study example xi and classify it, it may not require much extra effort for her to mark reasons for her classification.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average number of rationales and inter-annotator agreement for Tasks 2 and 3. A rationale by Ai (\"I think this is a great  movie!\") is considered to have been annotated also by Aj if at least one of Aj's rationales overlaps it (\"I think this is a great  movie!\"). In computing pairwise agreement on rationales, we ignored documents where Ai and Aj disagreed on the class. Notice  that the most thorough annotator AY caught most rationales marked by the others (exhibiting high \"recall\"), and that most rationales  enjoyed some degree of consensus, especially those marked by the least thorough annotator A1 (exhibiting high \"precision\").", "labels": [], "entities": [{"text": "recall", "start_pos": 488, "end_pos": 494, "type": "METRIC", "confidence": 0.9962330460548401}, {"text": "precision", "start_pos": 635, "end_pos": 644, "type": "METRIC", "confidence": 0.992240309715271}]}, {"text": " Table 2: Average annotation rates on each task.", "labels": [], "entities": [{"text": "Average annotation rates", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7888701756795248}]}]}