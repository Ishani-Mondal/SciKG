{"title": [], "abstractContent": [{"text": "This paper describes a novel text comparison environment that facilities text comparison administered through assessing and aggregating information nuggets automatically created and extracted from the texts in question.", "labels": [], "entities": [{"text": "text comparison", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7130584716796875}, {"text": "text comparison", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.7505235970020294}]}, {"text": "Our goal in designing such a tool is to enable and improve automatic nugget creation and present its application for evaluations of various natural language processing tasks.", "labels": [], "entities": [{"text": "nugget creation", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7056946456432343}]}, {"text": "During our demonstration at HLT, new users will able to experience firsthand text analysis can be fun, enjoyable, and interesting using system-created nuggets.", "labels": [], "entities": [{"text": "HLT", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.9228401780128479}, {"text": "text analysis", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.6930772662162781}]}], "introductionContent": [{"text": "In many natural language processing (NLP) tasks, such as question answering (QA), summarization, etc., we are faced with the problem of determining the appropriate granularity level for information units in order to conduct appropriate and effective evaluations.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.8495301306247711}, {"text": "summarization", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9339123368263245}]}, {"text": "Most commonly, we use sentences to model individual pieces of information.", "labels": [], "entities": []}, {"text": "However, more and more NLP applications require us to define text units smaller than sentences, essentially decomposing sentences into a collection of phrases.", "labels": [], "entities": []}, {"text": "Each phrase carries an independent piece of information that can be used as a standalone unit.", "labels": [], "entities": []}, {"text": "These finer-grained information units are usually referred to as nuggets.", "labels": [], "entities": []}, {"text": "Previous work shows that humans can create nuggets in a relatively straightforward fashion.", "labels": [], "entities": []}, {"text": "A serious problem in manual nugget creation is the inconsistency inhuman decisions (.", "labels": [], "entities": [{"text": "manual nugget creation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.6336270372072855}]}, {"text": "The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators.", "labels": [], "entities": []}, {"text": "And if the annotation is performed over an extended period of time, the consistency is even lower.", "labels": [], "entities": [{"text": "consistency", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9996170997619629}]}, {"text": "Given concerns over these issues, we have set out to design an evaluation toolkit to address three tasks in particular: 1) provide a consistent definition of what a nugget is; 2) automate the nugget extraction process systematically; and 3) utilize automatically extracted nuggets for text comparison and aggregation.", "labels": [], "entities": [{"text": "nugget extraction", "start_pos": 192, "end_pos": 209, "type": "TASK", "confidence": 0.7436100840568542}, {"text": "text comparison", "start_pos": 285, "end_pos": 300, "type": "TASK", "confidence": 0.7378547489643097}]}, {"text": "The idea of using semantic equivalent nuggets to compare texts is not new.", "labels": [], "entities": []}, {"text": "QA and summarization evaluations) have been carried out by using a set of manually created nuggets and the comparison procedure itself is either automatic using n-gram overlap counting or manually performed.", "labels": [], "entities": [{"text": "summarization evaluations", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.9069406986236572}]}, {"text": "We envisage the nuggetization process being automated and nugget comparison and aggregation being performed by humans.", "labels": [], "entities": [{"text": "nugget comparison", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7427375316619873}]}, {"text": "It's crucial to still involve humans in the process because recognizing semantic equivalent text units is not a trivial task.", "labels": [], "entities": []}, {"text": "In addition, since nuggets are systemproduced and can be imperfect, annotators are allowed to reject and re-create them.", "labels": [], "entities": []}, {"text": "We provide easy-to-use editing functionalities that allow manual overrides.", "labels": [], "entities": []}, {"text": "Record keeping on edits over erroneous nuggets is conducted in the background so that further improvements can be made for nugget extraction.", "labels": [], "entities": [{"text": "nugget extraction", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8659378290176392}]}], "datasetContent": [], "tableCaptions": []}