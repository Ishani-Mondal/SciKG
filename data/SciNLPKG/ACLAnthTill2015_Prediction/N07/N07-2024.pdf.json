{"title": [{"text": "Detection of Non-native Sentences using Machine-translated Training Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Training statistical models to detect non-native sentences requires a large corpus of non-native writing samples, which is often not readily available.", "labels": [], "entities": []}, {"text": "This paper examines the extent to which machine-translated (MT) sentences can substitute as training data.", "labels": [], "entities": []}, {"text": "For the native vs non-native classification task, non-native training data yields better performance ; for the ranking task, however, models trained with a large, publicly available set of MT data perform as well as those trained with non-native data.", "labels": [], "entities": []}], "introductionContent": [{"text": "For non-native speakers writing in a foreign language, feedback from native speakers is indispensable.", "labels": [], "entities": []}, {"text": "While humans are likely to provide higherquality feedback, a computer system can offer better availability and privacy.", "labels": [], "entities": []}, {"text": "A system that can distinguish non-native (\"ill-formed\") English sentences from native (\"well-formed\") ones would provide valuable assistance in improving their writing.", "labels": [], "entities": []}, {"text": "Classifying a sentence into discrete categories can be difficult: a sentence that seems fluent to one judge might not be good enough to another.", "labels": [], "entities": [{"text": "Classifying a sentence", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8572090665499369}]}, {"text": "An alternative is to rank sentences by their relative fluency.", "labels": [], "entities": []}, {"text": "This would be useful when a non-native speaker is unsure which one of several possible ways of writing a sentence is the best.", "labels": [], "entities": []}, {"text": "We therefore formulate two tasks on this problem.", "labels": [], "entities": []}, {"text": "The classification task gives one sentence to the system, and asks whether it is native or non-native.", "labels": [], "entities": []}, {"text": "The ranking task submits sentences with the same intended meaning, and asks which one is best.", "labels": [], "entities": []}, {"text": "To tackle these tasks, hand-crafting formal rules would be daunting.", "labels": [], "entities": []}, {"text": "Statistical methods, however, require a large corpus of non-native writing samples, which can be difficult to compile.", "labels": [], "entities": []}, {"text": "Since machine-translated (MT) sentences are readily available in abundance, we wish to address the question of whether they can substitute as training data.", "labels": [], "entities": []}, {"text": "The next section provides background on related research.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 describe our experiments, followed by conclusions and future directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Examples of sentences translated from a Chinese source sentence by a native speaker, by a non- native speaker, and by a machine translation system.", "labels": [], "entities": []}, {"text": " Table 2: Data sets used in this paper.", "labels": [], "entities": []}, {"text": " Table 3: Classification accuracy on JLE test. (-)  indicates accuracy on non-native sentences, and (+)  indicates accuracy on native sentences. The overall  accuracy is their average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9171513319015503}, {"text": "JLE test", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.7464300990104675}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9967054724693298}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9961154460906982}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9988275170326233}]}, {"text": " Table 4: Ranking accuracy on JLE test.", "labels": [], "entities": [{"text": "Ranking", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9395394921302795}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9400908350944519}, {"text": "JLE test", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.6862625330686569}]}]}