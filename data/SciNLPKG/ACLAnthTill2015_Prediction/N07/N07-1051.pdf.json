{"title": [], "abstractContent": [{"text": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs.", "labels": [], "entities": []}, {"text": "First, we present a novel coarse-to-fine method in which a grammar's own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank.", "labels": [], "entities": []}, {"text": "In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9817304611206055}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9157965779304504}]}, {"text": "Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs.", "labels": [], "entities": [{"text": "risk minimization", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7946739196777344}]}, {"text": "Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse fora sentence given the learned model.", "labels": [], "entities": [{"text": "Treebank parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5716239809989929}]}, {"text": "Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation ( or automatic state splitting ().", "labels": [], "entities": []}, {"text": "In particular, we demonstrated in) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9986074566841125}]}, {"text": "However, many questions about inference with such split PCFGs remain open.", "labels": [], "entities": []}, {"text": "In this work, we present 1.", "labels": [], "entities": []}, {"text": "an effective method for pruning in split PCFGs 2.", "labels": [], "entities": []}, {"text": "a comparison of objective functions for inference in split PCFGs, 3.", "labels": [], "entities": []}, {"text": "experiments on automatic splitting for languages other than English.", "labels": [], "entities": [{"text": "automatic splitting", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.669078528881073}]}, {"text": "3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs.", "labels": [], "entities": []}, {"text": "Our method considers the splitting history of the final grammar, projecting it onto its increasingly refined prior stages.", "labels": [], "entities": []}, {"text": "For any projection of a grammar, we give anew method for efficiently estimating the projection's parameters from the source PCFG itself (rather than a treebank), using techniques for infinite tree distributions () and iterated fixpoint equations.", "labels": [], "entities": []}, {"text": "We then parse with each refinement, in sequence, much along the lines of , except with much more complex and automatically derived intermediate grammars.", "labels": [], "entities": []}, {"text": "Thresholds are automatically tuned on heldout data, and the final system parses up to 100 times faster than the baseline PCFG parser, with no loss in test set accuracy.", "labels": [], "entities": [{"text": "Thresholds", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9832136631011963}, {"text": "parses", "start_pos": 73, "end_pos": 79, "type": "TASK", "confidence": 0.959837794303894}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9782845377922058}]}, {"text": "4, we consider the well-known issue of inference objectives in split PCFGs.", "labels": [], "entities": []}, {"text": "As in many model families, split PCFGs have a derivation / parse distinction.", "labels": [], "entities": []}, {"text": "The split PCFG directly describes a generative model over derivations, but evaluation is sensitive only to the coarser treebank symbols.", "labels": [], "entities": []}, {"text": "While the most probable parse problem is NP-complete, several approximate methods exist, including n-best reranking by parse likelihood, the labeled bracket algorithm of, and a variational approximation introduced in.", "labels": [], "entities": []}, {"text": "We present experiments which explicitly minimize various evaluation risks over a candidate set using samples from the split PCFG, and relate those conditions to the existing non-sampling algorithms.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.7732763886451721}]}, {"text": "We demonstrate that n-best reranking according to likelihood is superior for exact match, and that the non-reranking methods are superior for maximizing F 1 . A specific contribution is to discuss the role of unary productions, which previous work has glossed over, but which is important in understanding why the various methods work as they do.", "labels": [], "entities": []}, {"text": "5, we learn state-split PCFGs for German and Chinese and examine out-of-domain performance for English.", "labels": [], "entities": []}, {"text": "The learned grammars are compact and parsing is very quick in our multi-stage scheme.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9619046449661255}]}, {"text": "These grammars produce the highest test set parsing figures that we are aware of in each language, except for English for which non-local methods such as feature-based discriminative reranking are available).", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained models for English, Chinese and German using the standard corpora and splits as shown in 3.", "labels": [], "entities": []}, {"text": "We applied our model directly to each of the treebanks, without any language dependent modifications.", "labels": [], "entities": []}, {"text": "Specifically, the same model hyperparameters (merging percentage and smoothing factor) were used in all experiments.", "labels": [], "entities": []}, {"text": "4 shows that automatically inducing latent structure is a technique that generalizes well across language boundaries and results instate of the art performance for Chinese and German.", "labels": [], "entities": []}, {"text": "On English, the parser is outperformed only by the reranking parser of, which has access to a variety of features which cannot be captured by a generative model.", "labels": [], "entities": []}, {"text": "Space does not permit a thorough exposition of our analysis, but as in the case of English (), the learned subcategories exhibit interesting linguistic interpretations.", "labels": [], "entities": []}, {"text": "In German, for example, the model learns subcategories for different cases and genders.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A 10-best list from our best G can be reordered as to  maximize a given objective either using samples or, under some  restricting assumptions, in closed form.", "labels": [], "entities": []}, {"text": " Table 4: Our final test set parsing performance compared to the  best previous work on English, German and Chinese.", "labels": [], "entities": []}]}