{"title": [], "abstractContent": [{"text": "We propose to use a statistical phrase-based machine translation system in a post-editing task: the system takes as input raw machine translation output (from a commercial rule-based MT system), and produces post-edited target-language text.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.62391247600317}]}, {"text": "We report on experiments that were performed on data collected in precisely such a setting: pairs of raw MT output and their manually post-edited versions.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9153435230255127}]}, {"text": "In our evaluation, the output of our automatic post-editing (APE) system is not only better quality than the rule-based MT (both in terms of the BLEU and TER metrics), it is also better than the output of a state-of-the-art phrase-based MT system used in standalone translation mode.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9987230896949768}, {"text": "TER metrics", "start_pos": 154, "end_pos": 165, "type": "METRIC", "confidence": 0.9509758949279785}]}, {"text": "These results indicate that automatic post-editing constitutes a simple and efficient way of combining rule-based and statistical MT technologies.", "labels": [], "entities": [{"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.827035129070282}]}], "introductionContent": [{"text": "The quality of machine translation (MT) is generally considered insufficient for use in the field without a significant amount of human correction.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.8217792391777039}]}, {"text": "In the translation world, the term post-editing is often used to refer to the process of manually correcting MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 109, "end_pos": 118, "type": "TASK", "confidence": 0.8104406297206879}]}, {"text": "While the conventional wisdom is that postediting MT is usually not cost-efficient compared to full human translation, there appear to be situations where it is appropriate and even profitable.", "labels": [], "entities": [{"text": "postediting MT", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.8700197637081146}]}, {"text": "Unfortunately, there are few reports in the literature about such experiences (but see Allen (2004) for examples).", "labels": [], "entities": []}, {"text": "One of the characteristics of the post-editing task, as opposed to the revision of human translation for example, is its partly repetitive nature.", "labels": [], "entities": [{"text": "revision of human translation", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.7916239649057388}]}, {"text": "Most MT systems invariably produce the same output when confronted with the same input; in particular, this means that they tend to make the same mistakes over and over again, which the post-editors must correct repeatedly.", "labels": [], "entities": [{"text": "MT", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9843892455101013}]}, {"text": "Batch corrections are sometimes possible when multiple occurrences of the same mistake appear in the same document, but when it is repeated over several documents, or equivalently, when the output of the same machine translation system is handled by multiple post-editors, then the opportunities for factoring corrections become much more complex.", "labels": [], "entities": [{"text": "Batch corrections", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7310833930969238}, {"text": "factoring corrections", "start_pos": 300, "end_pos": 321, "type": "TASK", "confidence": 0.870072603225708}]}, {"text": "MT users typically try to reduce the post-editing load by customizing their MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8480814099311829}]}, {"text": "However, in Rule-based Machine Translation (RBMT), which still constitutes the bulk of the current commercial offering, customization is usually restricted to the development of \"user dictionaries\".", "labels": [], "entities": [{"text": "Rule-based Machine Translation (RBMT)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.7553046494722366}]}, {"text": "Not only is this time-consuming and expensive, it can only fix a subset of the MT system's problems.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9700887799263}]}, {"text": "The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see,) into the commercial arena seems to hold the promise of a solution to this problem: because the MT system learns directly from existing translations, it can be automatically customized to new domains and tasks.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.8364822864532471}, {"text": "MT", "start_pos": 197, "end_pos": 199, "type": "TASK", "confidence": 0.9669974446296692}]}, {"text": "However, the success of this operation cru-cially depends on the amount of training data available.", "labels": [], "entities": []}, {"text": "Moreover, the current state of the technology is still insufficient for consistently producing human readable translations.", "labels": [], "entities": []}, {"text": "This state of affairs has prompted some to examine the possibility of automating the post-editing process itself, at least as far as \"repetitive errors\" are concerned.", "labels": [], "entities": []}, {"text": "sketch the outline of such an automated post-editing (APE) system, which would automatically learn post-editing rules from a tri-parallel corpus of source, raw MT and post-edited text.", "labels": [], "entities": []}, {"text": "Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.", "labels": [], "entities": []}, {"text": "also argue in favor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation techniques.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 186, "end_pos": 217, "type": "TASK", "confidence": 0.6256133715311686}]}, {"text": "To the best of our knowledge, however, this idea has never been implemented.", "labels": [], "entities": []}, {"text": "In this paper, we explore the idea of using a PBMT system as an automated post-editor.", "labels": [], "entities": []}, {"text": "The underlying intuition is simple: if we collect a parallel corpus of raw machine-translation output, along with its human-post-edited counterpart, we can train the system to translate from the former into the latter.", "labels": [], "entities": []}, {"text": "In section 2, we present the case study that motivates our work and the associated data.", "labels": [], "entities": []}, {"text": "In section 3, we describe the phrase-based post-editing model that we use for improving the output of the automatic translation system.", "labels": [], "entities": []}, {"text": "In section 4, we illustrate this on a dataset of moderate size containing job ads and their translation.", "labels": [], "entities": []}, {"text": "With less than 500k words of training material, the phrase-based MT system already outperforms the rule-based MT baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9133755564689636}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9145956039428711}]}, {"text": "However, a phrase-based post-editing model trained on the output of that baseline outperforms both by a fairly consistent margin.", "labels": [], "entities": []}, {"text": "The resulting BLEU score increases by up to 50% (relative) and the TER is cut by one third.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9733563661575317}, {"text": "TER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.994500994682312}]}], "datasetContent": [{"text": "The corpus described in section 2.2 is available for two language pairs: English-to-French and Frenchto-English.", "labels": [], "entities": []}, {"text": "In each direction, each block is available in three versions (or slices): the original text (or source), the output of the commercial rule-based MT system (or baseline) and the final, post-edited version (or reference).", "labels": [], "entities": []}, {"text": "In each direction (French-to-English and Englishto-French), we held out two subsets of approximately 1000 randomly picked blocks.", "labels": [], "entities": []}, {"text": "The validation set is used for testing the impact of various highlevel choices such as pre-processing, or for obtaining preliminary results based on which we setup new experiments.", "labels": [], "entities": []}, {"text": "The test set is used only once, in order to obtain the final experimental results reported here.", "labels": [], "entities": []}, {"text": "The rest of the data constitutes the training set, which is split in two.", "labels": [], "entities": []}, {"text": "We sampled a subset of 1000 blocks as train-2, which is used for optimiz-English-to-French French-to-English Corpus words: words: blocks source baseline reference blocks source baseline reference: Data and split used in our experiments, (in thousand words).", "labels": [], "entities": []}, {"text": "'baseline' is the output of the commercial rule-based MT system and 'reference' is the final, post-edited text.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9143009781837463}]}, {"text": "ing the log-linear model parameters used for decoding and rescoring.", "labels": [], "entities": []}, {"text": "The rest is the train-1 set, used for estimating IBM translation models, constructing phrasetables and estimating a target language model.", "labels": [], "entities": [{"text": "estimating IBM translation models", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.7444649934768677}]}, {"text": "The composition of the various sets is detailed in.", "labels": [], "entities": []}, {"text": "All data was tokenized and lowercased; all evaluations were performed independent of case.", "labels": [], "entities": []}, {"text": "Note that the validation and test sets were originally made out of 1000 blocks sampled randomly from the data.", "labels": [], "entities": []}, {"text": "These sets turned out to contain blocks identical to blocks from the training sets.", "labels": [], "entities": []}, {"text": "Considering that these would normally have been handled by the translation memory component (see the HRSDC workflow description in Section 2.1), we removed those blocks for which the source part was already found in the training set (in either train-1 or train-2), hence their smaller sizes.", "labels": [], "entities": []}, {"text": "In order to check the sensitivity of experimental results to the choice of the train-2 set, we did a run of preliminary experiments using different subsets of 1000 blocks.", "labels": [], "entities": []}, {"text": "The experimental results were nearly identical and highly consistent, showing that the choice of a particular train-2 subset has no influence on our conclusions.", "labels": [], "entities": []}, {"text": "In the experiments reported below, we therefore use a single identical train-2 set.", "labels": [], "entities": []}, {"text": "We initially performed two sets of experiments on this data.", "labels": [], "entities": []}, {"text": "The first was intended to compare the performance of the Portage PBMT system as an alternative to the commercial rule-based MT system on this type of data.", "labels": [], "entities": [{"text": "Portage PBMT system", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.7422159910202026}, {"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.9308061003684998}]}, {"text": "In these experiments, Englishto-French and French-to-English translation systems were trained on the source and reference (manually post-edited target language) slices of the training set.", "labels": [], "entities": []}, {"text": "In addition to the target language model estimated on the train-1 data, we used an external contribution,  a trigram target language model trained on a fairly large quantity of data from the Canadian Hansard.", "labels": [], "entities": [{"text": "train-1 data", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9357074201107025}, {"text": "Canadian Hansard", "start_pos": 191, "end_pos": 207, "type": "DATASET", "confidence": 0.8230350017547607}]}, {"text": "The goal of the second set of experiments was to assess the potential of the Portage technology in automatic post-editing mode.", "labels": [], "entities": []}, {"text": "Again, we built systems for both language directions, but this time using the existing rule-based MT output as source and the reference as target.", "labels": [], "entities": []}, {"text": "Apart from the use of different source data, the training procedure and system configurations of the translation and post-editing systems were in all points identical.", "labels": [], "entities": []}, {"text": "The results of both experiments are presented in Table 2.", "labels": [], "entities": []}, {"text": "Results are reported both in terms of the TER and BLEU metrics; Baseline refers to the commercial rule-based MT output.", "labels": [], "entities": [{"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.991640567779541}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.987236499786377}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9583675861358643}]}, {"text": "The first observation from these results is that, while the performance of Portage in translation mode is approximately equivalent to that of the baseline system when translating into French, its performance is much better than the baseline when translating into English.", "labels": [], "entities": []}, {"text": "Two factors possibly contribute to this result: first, the fact that the baseline system itself performs better when translating into French; second, and possibly more importantly, the fact that we had access to less training data for English-toFrench translation.", "labels": [], "entities": [{"text": "English-toFrench translation", "start_pos": 235, "end_pos": 263, "type": "TASK", "confidence": 0.621433213353157}]}, {"text": "The second observation is that when Portage is used in automatic post-editing mode, on top of the baseline MT system, it achieves better quality than either of the two translation systems used on its own.", "labels": [], "entities": []}, {"text": "This appears to be true regardless of the translation direction or metric.", "labels": [], "entities": []}, {"text": "This is an extremely interesting result, especially in light of how little data was actually available to train the post-editing system.", "labels": [], "entities": []}, {"text": "One aspect of statistical MT systems is that, contrary to rule-based systems, their performance (usually) increases as more training data is available.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.817952036857605}]}, {"text": "In order to quantify this effect in our setting, we have computed learning curves by training the Portage translation and Portage APE systems on subsets of the training data of increasing sizes.", "labels": [], "entities": [{"text": "Portage translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.6461251527070999}]}, {"text": "We start with as little as 1000 blocks, which corresponds to around 10-15k words.", "labels": [], "entities": []}, {"text": "(next page) compares the learning rates of the two competing approaches (Portage translation vs. Portage APE).", "labels": [], "entities": [{"text": "Portage translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7809315621852875}, {"text": "Portage APE", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.44476786255836487}]}, {"text": "Both approaches display very steady learning rates (note the logarithmic scale for training data size).", "labels": [], "entities": []}, {"text": "These graphs strongly suggest that both systems would continue to improve given more training data.", "labels": [], "entities": []}, {"text": "The most impressive aspect is how little data is necessary to improve upon the baseline, especially when translating into English: as little as 8000 blocks (around 100k words) for direct translation and 2000 blocks (around 25k words) for automatic post-editing.", "labels": [], "entities": []}, {"text": "This suggests that such a post-editing setup might be worth implementing even for specialized domains with very small volumes of data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data and split used in our experiments, (in thousand words). 'baseline' is the output of the com- mercial rule-based MT system and 'reference' is the final, post-edited text.", "labels": [], "entities": []}, {"text": " Table 2: Experimental Results: For TER, lower (er- ror) is better, while for BLEU, higher (score) is bet- ter. Best results are in bold.", "labels": [], "entities": [{"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9965864419937134}, {"text": "er- ror)", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9453107863664627}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9977542757987976}]}, {"text": " Table 3: Portage translation -Portage APE system  combination experimental results.", "labels": [], "entities": [{"text": "Portage translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9418231546878815}, {"text": "Portage APE", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.564777672290802}]}]}