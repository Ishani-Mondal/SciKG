{"title": [{"text": "Cross-Instance Tuning of Unsupervised Document Clustering Algorithms *", "labels": [], "entities": [{"text": "Cross-Instance Tuning of Unsupervised Document Clustering Algorithms", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.7559003148760114}]}], "abstractContent": [{"text": "In unsupervised learning, where no training takes place, one simply hopes that the unsupervised learner will work well on any unlabeled test collection.", "labels": [], "entities": []}, {"text": "However , when the variability in the data is large, such hope maybe unrealistic; a tuning of the unsupervised algorithm may then be necessary in order to perform well on new test collections.", "labels": [], "entities": []}, {"text": "In this paper, we show how to perform such a tuning in the context of unsupervised document clustering, by (i) introducing a degree of freedom, \u03b1, into two leading information-theoretic clustering algorithms, through the use of generalized mutual information quantities; and (ii) selecting the value of \u03b1 based on clusterings of similar, but supervised document collections (cross-instance tuning).", "labels": [], "entities": [{"text": "document clustering", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7302740514278412}]}, {"text": "One option is to perform a tuning that directly minimizes the error on the supervised data sets; another option is to use \"strapping\" (Eisner and Karakos, 2005), which builds a classifier that learns to distinguish good from bad clusterings, and then selects the \u03b1 with the best predicted clustering on the test set.", "labels": [], "entities": []}, {"text": "Experiments from the \"20 Newsgroups\" corpus show that, although both techniques improve the performance of the baseline algorithms , \"strapping\" is clearly a better choice for cross-instance tuning.", "labels": [], "entities": [{"text": "20 Newsgroups\" corpus", "start_pos": 22, "end_pos": 43, "type": "DATASET", "confidence": 0.7530945837497711}, {"text": "cross-instance tuning", "start_pos": 176, "end_pos": 197, "type": "TASK", "confidence": 0.8602053821086884}]}], "introductionContent": [{"text": "The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learning) has been studied in the literature under various guises.", "labels": [], "entities": []}, {"text": "A variety of algorithms (e.g., bootstrapping, co-training), alternating structure optimization (Ando and), etc.) have been developed in order to improve the performance of supervised algorithms, by automatically extracting knowledge from lots of unlabeled examples.", "labels": [], "entities": [{"text": "alternating structure optimization", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.6352799534797668}]}, {"text": "Of special interest is the work of, where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand.", "labels": [], "entities": []}, {"text": "In essence, transform the semi-supervised learning problem to a multi-task learning problem; in multi-task learning, a (usually large) set of supervised tasks is available for training, and the goal is to build models which can simultaneously do well on all of them).", "labels": [], "entities": []}, {"text": "Little work, however, has been devoted to study the situation where lots of labeled examples, of one kind, are used to build a model which is tested on unlabeled data of a \"different\" kind.", "labels": [], "entities": []}, {"text": "This problem, which is the topic of this paper, cannot be cast as a multi-task learning problem (since there are labeled examples of only one kind), neither can be cast as a semi-supervised problem (since there are no training labels for the test task).", "labels": [], "entities": []}, {"text": "Note that we are interested in the case where the hidden test labels may have no semantic relationship with the training labels; in some cases, there may not even be any information about the test labels-what they represent, how many they are, or at what granularity they describe the data.", "labels": [], "entities": []}, {"text": "This situation can arise in the case of unsupervised clustering of documents from a large and diverse corpus: it may not be known in what way the resulting clusters split the corpus (is it in terms of topic?", "labels": [], "entities": []}, {"text": "a combination of the above?), unless one inspects each resulting cluster to determine its \"meaning.\"", "labels": [], "entities": []}, {"text": "At this point, we would like to differentiate between two concepts: a target task refers to a class of problems that have a common, high-level description (e.g., the text document clustering task, the speech recognition task, etc.).", "labels": [], "entities": [{"text": "text document clustering task", "start_pos": 166, "end_pos": 195, "type": "TASK", "confidence": 0.7419832646846771}, {"text": "speech recognition task", "start_pos": 201, "end_pos": 224, "type": "TASK", "confidence": 0.7949958046277364}]}, {"text": "On the other hand, a task instance refers to a particular example from the class.", "labels": [], "entities": []}, {"text": "For instance, if the task is \"document clustering,\" a task instance could be \"clustering of a set of scientific documents into particular fields\"; or, if the task is \"parsing,\" a task instance could be \"parsing of English sentences from the Wall Street Journal corpus\".", "labels": [], "entities": [{"text": "document clustering", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.6941034942865372}, {"text": "parsing", "start_pos": 167, "end_pos": 174, "type": "TASK", "confidence": 0.9638071060180664}, {"text": "Wall Street Journal corpus", "start_pos": 241, "end_pos": 267, "type": "DATASET", "confidence": 0.6560058146715164}]}, {"text": "For the purposes of this paper, we further assume that there are task instances which are unrelated, in the sense that that there are no common labels between them.", "labels": [], "entities": []}, {"text": "For example, if the task is \"clustering from the 20 Newsgroups corpus,\" then \"clustering of the computer-related documents into PC-related and Mac-related\" and \"clustering of the politics-related documents into Middle-Eastrelated and non-Middle-East-related\" are two distinct, unrelated instances.", "labels": [], "entities": [{"text": "20 Newsgroups corpus", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.6198404133319855}]}, {"text": "In more mathematical terms, if task instances T 1 , T 2 take sets of observations X 1 , X 2 as input, and try to predict labels from sets S 1 , S 2 , respectively, then they are called unrelated if X 1 \u2229 X 2 = \u2205 and S 1 \u2229 S 2 = \u2205.", "labels": [], "entities": []}, {"text": "The focus of this paper is to study the problem of cross-instance tuning of unsupervised algorithms: how one can tune an algorithm, which is used to solve a particular task instance, using knowledge from an unrelated task instance.", "labels": [], "entities": [{"text": "cross-instance tuning of unsupervised algorithms", "start_pos": 51, "end_pos": 99, "type": "TASK", "confidence": 0.8431210517883301}]}, {"text": "To the best of our knowledge, this cross-instance learning problem has only been tackled in , whose \"strapping\" procedure learns a meta-classifier for distinguishing good from bad clusterings.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a scalar parameter \u03b1 (a new degree of freedom) into two basic unsupervised clustering algorithms.", "labels": [], "entities": []}, {"text": "We can tune \u03b1 to maximize unsupervised clustering performance on different task instances where the correct clustering is known.", "labels": [], "entities": []}, {"text": "The hope is that tuning the parameter learns something about the task in general, which transfers from the supervised task instances to the unsupervised one.", "labels": [], "entities": []}, {"text": "Alternatively, we can tune a metaclassifier so as to select good values of \u03b1 on the supervised task instances, and then use the same metaclassifier to select a good (possibly different) value of \u03b1 in the unsupervised case.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 gives a background on text categorization, and briefly describes the algorithms that we use in our experiments.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.786771148443222}]}, {"text": "Section 3 describes our parameterization of the clustering algorithms using Jensen-R\u00e9nyi divergence and Csisz\u00e1r's mutual information.", "labels": [], "entities": []}, {"text": "Experimental results from the \"20 Newsgroups\" data set are shown in Section 4, along with two techniques for cross-instance learning: (i) \"strapping,\" which, attest time, picks a parameter based on various \"goodness\" cues that were learned from the labeled data set, and (ii) learning the parameter from a supervised data set which is chosen to statistically match the test set.", "labels": [], "entities": [{"text": "Newsgroups\" data set", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8288239985704422}]}, {"text": "Finally, concluding remarks appear in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We demonstrate the feasibility of cross-instance tuning with experiments on unsupervised document categorization from the 20 Newsgroups corpus; this corpus consists of roughly 20,000 news articles, evenly divided among 20 Usenet groups.", "labels": [], "entities": [{"text": "cross-instance tuning", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8440958857536316}, {"text": "20 Newsgroups corpus", "start_pos": 122, "end_pos": 142, "type": "DATASET", "confidence": 0.6971475183963776}]}, {"text": "Random samples of 500 articles each were chosen by) to create multiple test collections: 250 each from 2 arbitrarily chosen Usenet groups for the Binary test collection, 100 articles each from 5 groups for the Multi5 test collection, and 50 each from 10 groups for the Multi10 test collection.", "labels": [], "entities": [{"text": "Binary test collection", "start_pos": 146, "end_pos": 168, "type": "DATASET", "confidence": 0.7587096591790518}, {"text": "Multi5 test collection", "start_pos": 210, "end_pos": 232, "type": "DATASET", "confidence": 0.9164714217185974}, {"text": "Multi10 test collection", "start_pos": 269, "end_pos": 292, "type": "DATASET", "confidence": 0.9210414489110311}]}, {"text": "Three independent test collections of each kind (Binary, Multi5 and Multi10) were created, fora total of 9 collections.", "labels": [], "entities": []}, {"text": "The sIB method was used to separately cluster each collection, given the correct number of clusters.", "labels": [], "entities": []}, {"text": "A comparison of sIB and IDTs on the same 9 test collections was reported in ().", "labels": [], "entities": []}, {"text": "Matlab code from was used for the sIB experiments, while the parameterized mutual information measures of Section 3 were used for the IDTs.", "labels": [], "entities": [{"text": "IDTs", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.8605602383613586}]}, {"text": "A comparison was also made with the EM-based Gaussian mixtures clustering tool mclust, and with a simple K-means algorithm.", "labels": [], "entities": [{"text": "EM-based Gaussian mixtures clustering", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.4824925437569618}]}, {"text": "Since the two latter techniques gave uniformly worse clusterings than those of sIB and IDTs, we omit them from the following discussion.", "labels": [], "entities": []}, {"text": "To show that our methods work beyond the 9 particular 500-document collections described above, in this paper we instead use five different randomly sampled test collections for each of the Binary, Multi5 and Multi10 cases, making fora total of 15 new test collections in this paper.", "labels": [], "entities": []}, {"text": "For diversity, we ensure that none of the five test collections (in each case) contain any documents used in the three collections of) (for the same case).", "labels": [], "entities": [{"text": "diversity", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9605388641357422}]}, {"text": "We pre-process the documents of each test collection using the procedure 2 mentioned in ().", "labels": [], "entities": []}, {"text": "The 15 test collections are then converted to feature matrices-term-document frequency matrices for sIB, and discounted tf/idf matrices (according to the Okapi formula () for IDTs-with each row of a matrix representing one document in that test collection.", "labels": [], "entities": []}, {"text": "Excluding the subject line, the header of each abstract is removed.", "labels": [], "entities": []}, {"text": "Stop-words such as a, the, is, etc. are removed, and stemming is performed (e.g., common suffixes such as -ing, -er, -ed, etc., are removed).", "labels": [], "entities": []}, {"text": "Also, all numbers are collapsed to one symbol, and non-alphanumeric sequences are converted to whitespace.", "labels": [], "entities": []}, {"text": "Moreover, as suggested in) as an effective method for reducing the dimensionality of the feature space (number of distinct words), all words which occur fewer than t times in the corpus are removed.", "labels": [], "entities": []}, {"text": "For the sIB experiments, we use t = 2 (as was done in), while for the IDT experiments we use t = 3; these choices result in the best performance for each method, respectively, on another dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average classification errors for IDTs and  sIB, using strapping (\"str\" rows) and regularized  least squares (\"rls\" rows) to pick \u03b1 in Jensen-R\u00e9nyi  divergence and Csisz\u00e1r's mutual information. Rows  \"MI\" show the errors resulting from the untuned al- gorithms, which use the regular mutual information  objective (\u03b1 = 1). Results which are better than the  corresponding \"MI\" results are shown in bold.", "labels": [], "entities": []}]}