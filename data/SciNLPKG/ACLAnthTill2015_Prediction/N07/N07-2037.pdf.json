{"title": [{"text": "Joint Morphological-Lexical Language Modeling for Machine Translation", "labels": [], "entities": [{"text": "Morphological-Lexical Language Modeling", "start_pos": 6, "end_pos": 45, "type": "TASK", "confidence": 0.7232720653216044}, {"text": "Machine Translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7078800648450851}]}], "abstractContent": [{"text": "We present a joint morphological-lexical language model (JMLLM) for use in statistical machine translation (SMT) of language pairs where one or both of the languages are morphologically rich.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.783822163939476}]}, {"text": "The proposed JMLLM takes advantage of the rich morphology to reduce the Out-Of-Vocabulary (OOV) rate, while keeping the predictive power of the whole words.", "labels": [], "entities": [{"text": "JMLLM", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8572714924812317}, {"text": "Out-Of-Vocabulary (OOV) rate", "start_pos": 72, "end_pos": 100, "type": "METRIC", "confidence": 0.969080924987793}]}, {"text": "It also allows incorporation of additional available semantic, syntactic and linguistic information about the morphemes and words into the language model.", "labels": [], "entities": []}, {"text": "Preliminary experiments with an English to Dialectal-Arabic SMT system demonstrate improved translation performance over trigram based baseline language model.", "labels": [], "entities": [{"text": "English to Dialectal-Arabic SMT", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.4668953940272331}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) methods have evolved from using the simple word based models ( to phrase based models ().", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8373958766460419}]}, {"text": "More recently, there is a significant effort focusing on integrating richer knowledge, such as syntactic parse trees) within the translation process to overcome the limitations of the phrase based models.", "labels": [], "entities": []}, {"text": "The SMT has been formulated as a noisy channel model in which the target language sentence, e is seen as distorted by the channel into the foreign language f : where P(f | e) is the translation model and P(e) is language model of the target language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9865036010742188}]}, {"text": "The overwhelming proportion of the SMT research has been focusing on improving the translation model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9955719709396362}, {"text": "translation", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.9770196676254272}]}, {"text": "Despite several new studies), language modeling for SMT has not been receiving much attention.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7683297097682953}, {"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9816122651100159}]}, {"text": "Currently, the state-of-the-art SMT systems have been using the standard word n-gram models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9952324032783508}]}, {"text": "Since n-gram models learn from given data, a severe drop in performance maybe observed if the target domain is not adequately covered in the training data.", "labels": [], "entities": []}, {"text": "The coverage problem is aggravated for morphologically rich languages.", "labels": [], "entities": []}, {"text": "Arabic is such a language where affixes are appended to the beginning or end of a stem to generate new words that indicate case, gender, tense etc.", "labels": [], "entities": []}, {"text": "Hence, it is natural that this leads to rapid vocabulary growth, which is accompanied by worse language model probability estimation due to data sparsity and high Out-Of-Vocabulary (OOV) rate.", "labels": [], "entities": [{"text": "Out-Of-Vocabulary (OOV) rate", "start_pos": 163, "end_pos": 191, "type": "METRIC", "confidence": 0.9581731796264649}]}, {"text": "Due to rich morphology, one would suspect that words may not be the best lexical units for Arabic, and perhaps morphological units would be a better choice.", "labels": [], "entities": []}, {"text": "Recently, there have been a number of new methods using the morphological units to represent lexical items ().", "labels": [], "entities": []}, {"text": "Factored Language Models (FLMs) ( share the same idea to some extent but here words are decomposed into a number of features and the resulting representation is used in a generalized back-off scheme to improve the robustness of probability estimates for rarely observed word n-grams.", "labels": [], "entities": []}, {"text": "In this study we propose a tree structure called Morphological-Lexical Parse Tree (MLPT) to combine the information provided by a morphological analyzer with the lexical information within a single Joint Morphological-Lexical Language Model (JMLLM).", "labels": [], "entities": []}, {"text": "The MLPT allows us to include available syntactic and semantic information about the morphological segments 1 (i.e. prefix/stem/suffix), words or group of words.", "labels": [], "entities": [{"text": "MLPT", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6869421601295471}]}, {"text": "The JMLLM can also be used to guide the recognition for selecting high probability morphological sentence segmentations.", "labels": [], "entities": [{"text": "JMLLM", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8639314770698547}, {"text": "selecting high probability morphological sentence segmentations", "start_pos": 56, "end_pos": 119, "type": "TASK", "confidence": 0.6633666902780533}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides a description of the morphological segmentation method.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.8600726127624512}]}, {"text": "A short overview of Maximum Entropy modeling is given in Section 3.", "labels": [], "entities": [{"text": "Maximum Entropy modeling", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6639286677042643}]}, {"text": "The proposed JMLLM is presented in Section 4.", "labels": [], "entities": [{"text": "JMLLM", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.631662130355835}]}, {"text": "Section 5 introduces the SMT system and Section 6 describes the experimental results followed by the conclusions in Section 7.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9943991303443909}]}], "datasetContent": [{"text": "The parallel corpus has 459K utterance pairs with 90K words (50K morphemes).", "labels": [], "entities": []}, {"text": "The Iraqi-Arabic language model training data is slightly larger than the Iraqi-Arabic side of the parallel corpus and it has 2.8M words with 98K unique lexical items.", "labels": [], "entities": [{"text": "Iraqi-Arabic language model training data", "start_pos": 4, "end_pos": 45, "type": "DATASET", "confidence": 0.5760407149791718}]}, {"text": "The morphologically analyzed training data has 2.6M words with 58K unique vocabulary items.", "labels": [], "entities": []}, {"text": "A statistical trigram language model using Modified Knesser-Ney smoothing has been built for the morphologically segmented data.", "labels": [], "entities": []}, {"text": "The test data consists of 2242 utterances (3474 unique words).", "labels": [], "entities": []}, {"text": "The OOV rate for the unsegmented test data is 8.7%, the corresponding number for the morphologically analyzed data is 7.4%.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9922293424606323}]}, {"text": "Hence, morphological segmentation reduces the OOV rate by 1.3% (15% relative), which is not as large reduction as compared to training data (about 40% relative reduction).", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.8729130625724792}, {"text": "OOV rate", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9211948812007904}]}, {"text": "We believe this would limit the potential improvement we could get from JMLLM, since JMLLM is expected to be more effective compared to word ngram models, when the OOV rate is significantly reduced after segmentation.", "labels": [], "entities": [{"text": "JMLLM", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.8923037052154541}, {"text": "OOV rate", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9853032827377319}]}, {"text": "We measure translation performance by the BLEU score () with one reference for each hypothesis.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9632931351661682}, {"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9723430275917053}]}, {"text": "In order to evaluate the performance of the JMLLM, a translation N-best list (N=10) is generated using the baseline Morpheme-trigram language model.", "labels": [], "entities": [{"text": "JMLLM", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.7420660853385925}]}, {"text": "First, on a heldout development data all feature weights including the language model weight are optimized to maximize the BLEU score using the downhill simplex method).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9776175022125244}]}, {"text": "These weights are fixed when the language models are used on the test data.", "labels": [], "entities": []}, {"text": "The translation BLEU (%) scores are given in.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9072751402854919}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8922603726387024}]}, {"text": "The first entry (37.59) is the oracle BLEU score for the N-best list.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9807349741458893}]}, {"text": "The baseline morpheme-trigram achieved 29.63, word-trigram rescoring improved the BLEU score to 29.91.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9822037518024445}]}, {"text": "The JMLLM achieved 30.20 and log-linear interpolation with the morpheme-trigram improved the BLEU score to 30.41.", "labels": [], "entities": [{"text": "JMLLM", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7886514067649841}, {"text": "BLEU score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9845939874649048}]}], "tableCaptions": []}