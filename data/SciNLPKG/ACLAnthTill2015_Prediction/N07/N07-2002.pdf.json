{"title": [{"text": "Automatic acquisition of grammatical types for nouns", "labels": [], "entities": [{"text": "Automatic acquisition of grammatical types", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7647711217403412}]}], "abstractContent": [{"text": "The work 1 we present here is concerned with the acquisition of deep grammatical information for nouns in Spanish.", "labels": [], "entities": [{"text": "acquisition of deep grammatical information for nouns in Spanish", "start_pos": 49, "end_pos": 113, "type": "TASK", "confidence": 0.7780677676200867}]}, {"text": "The aim is to build a learner that can handle noise, but, more interestingly, that is able to overcome the problem of sparse data, especially important in the case of nouns.", "labels": [], "entities": []}, {"text": "We have based our work on two main points.", "labels": [], "entities": []}, {"text": "Firstly, we have used distributional evidences as features.", "labels": [], "entities": []}, {"text": "Secondly, we made the learner deal with all occurrences of a word as a single complex unit.", "labels": [], "entities": []}, {"text": "The obtained results show that grammatical features of nouns is a level of generalization that can be successfully approached with a Decision Tree learner.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our work aims to the acquisition of deep grammatical information for nouns, because having information such as countability and complementation is necessary for different applications, especially for deep analysis grammars, but also for question answering, topic detection and tracking, etc.", "labels": [], "entities": [{"text": "question answering", "start_pos": 237, "end_pos": 255, "type": "TASK", "confidence": 0.900283932685852}, {"text": "topic detection and tracking", "start_pos": 257, "end_pos": 285, "type": "TASK", "confidence": 0.8653384894132614}]}, {"text": "Most successful systems of deep lexical acquisition are based on the idea that distributional features (i.e. the contexts where words occur) are associated to concrete lexical types.", "labels": [], "entities": [{"text": "deep lexical acquisition", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6894871095816294}]}, {"text": "The difficulties are, on the one hand, that some filtering must be applied to get rid of noise, that is, contexts wrongly assessed as cues of a given type and, on the other hand, that fora pretty large number of words, their occurrences in a corpus of any length are very few, making statistical treatment very difficult.", "labels": [], "entities": []}, {"text": "The phenomenon of noise is related to the fact that one particular context can be a cue of different lexical types.", "labels": [], "entities": []}, {"text": "The problem of sparse data is predicted by the Zipfian distribution of words in texts: there is a large number of words likely to occur a very reduced number of times in any corpus.", "labels": [], "entities": []}, {"text": "Both of these typical problems are maximized in the case of nouns.", "labels": [], "entities": []}, {"text": "The aim of the work we present here is to build a learner that can handle noise, but, more interestingly, that is able to overcome the problem of sparse data.", "labels": [], "entities": []}, {"text": "The learner must predict the correct type both when there is a large number of occurrences as well as when there are only few occurrences, by learning on features that maximize generalization capacities of the learner while controlling overfitting phenomena.", "labels": [], "entities": []}, {"text": "We have based our work on two main points.", "labels": [], "entities": []}, {"text": "Firstly, we have used morphosyntactic information as features.", "labels": [], "entities": []}, {"text": "Secondly, we made the learner deal with all occurrences of a word as a complex unit.", "labels": [], "entities": []}, {"text": "In our system, linguistic cues of every occurrence are collected in the signature of the word (more technically a pair lema + part of speech) in a particular corpus.", "labels": [], "entities": []}, {"text": "In the next sections we give further details about the features used, as well as about the use of signatures.", "labels": [], "entities": []}, {"text": "The rest of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents an overview of the state of the art in deep lexical acquisition.", "labels": [], "entities": [{"text": "deep lexical acquisition", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6881600022315979}]}, {"text": "In section 3, we introduce details about our selection of linguistically motivated cues to be used as features for training a Decision Tree (DT).", "labels": [], "entities": []}, {"text": "Section 4 shortly introduces the methodology and data used in the experiments whose results are presented in section 5.", "labels": [], "entities": []}, {"text": "And in section 6 we conclude by comparing with the published results for similar tasks and we sketch future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "The purpose of the evaluation was to validate our system with respect to the two problems mentioned: noise filtering and generalization capacity by measuring type precision and type recall.", "labels": [], "entities": [{"text": "noise filtering", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7040423899888992}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.7949606776237488}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9439286589622498}]}, {"text": "We understand type precision as a measure of the noise filtering success, and recall as a measure of the generalization capacity.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8920673727989197}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9990022778511047}]}, {"text": "In the following tables we present the results of the different experiments.", "labels": [], "entities": []}, {"text": "In, there is a view of the results of the experiment after training and testing with the signatures got in the smaller corpus.", "labels": [], "entities": []}, {"text": "The results are for the assignment of the grammatical feature for the two values, yes and no.", "labels": [], "entities": []}, {"text": "And the column named global refers to the total percentage of correctly classified instances..", "labels": [], "entities": []}, {"text": "DT results of economy signatures for training and test The most difficult task for the learner is to identify nouns with bound prepositions.", "labels": [], "entities": []}, {"text": "Note that there are only 20 nouns with prepositional complements of the 289 test nouns, and that the occurrence of the preposition is not mandatory, and hence the signatures are presented to the learner with very little information.", "labels": [], "entities": []}, {"text": "shows the results for 50 nouns with only one occurrence in the corpus.", "labels": [], "entities": []}, {"text": "The performance does not change significantly, showing that the generalization capacity of the learner can cope with low frequency words, and that noise in larger signatures has been adequately filtered.", "labels": [], "entities": []}, {"text": "shows that there is little variation in the results of training with signatures of the economy corpus and testing with ones of the medicine corpus.", "labels": [], "entities": []}, {"text": "As expected, no variation due to domain is relevant as the information learnt should be valid in all domains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. DT results of economy signatures for  training and test", "labels": [], "entities": []}, {"text": " Table 2. DT results for training with signatures of  the economy corpus and testing 50 unseen nouns  with a single occurrence as test", "labels": [], "entities": [{"text": "DT", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.5652952790260315}]}, {"text": " Table 3. DT results for training with economy sig- natures and testing with medicine signatures", "labels": [], "entities": [{"text": "DT", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.5931390523910522}]}]}