{"title": [{"text": "Virtual Evidence for Training Speech Recognizers using Partially Labeled data", "labels": [], "entities": [{"text": "Training Speech Recognizers", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6177490750948588}]}], "abstractContent": [{"text": "Collecting supervised training data for automatic speech recognition (ASR) systems is both time consuming and expensive.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.8074301332235336}]}, {"text": "In this paper we use the notion of virtual evidence in a graphical-model based system to reduce the amount of supervisory training data required for sequence learning tasks.", "labels": [], "entities": []}, {"text": "We apply this approach to a TIMIT phone recognition system, and show that our VE-based training scheme can, relative to a baseline trained with the full segmentation, yield similar results with only 15.3% of the frames labeled (keeping the number of utterances fixed).", "labels": [], "entities": [{"text": "TIMIT phone recognition", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6562815407911936}]}], "introductionContent": [{"text": "Current state-of-the-art speech recognizers use thousands of hours of training data, collected from a large number of speakers with various backgrounds in order to make the models more robust.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7126626223325729}]}, {"text": "It is well known that one of the simplest ways of improving the accuracy of a recognizer is to increase the amount of training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9980165958404541}]}, {"text": "Moreover, speech recognition systems can benefit from being trained on hand-transcribed data where all the appropriate word level segmentations (i.e., the exact time of the word boundaries) are known.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7603781521320343}]}, {"text": "However, with increasing amounts of raw speech data being made available, it is both time consuming and expensive to accurately segment every word for every given sentence.", "labels": [], "entities": []}, {"text": "Moreover, for languages for which only a small amount of training data is available, it can be expensive and challenging to annotate with precise word transcriptions -the researcher may have no choice but to use partially erroneous training data.", "labels": [], "entities": []}, {"text": "There area number of different ways to label data used to train a speech recognizer.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.6937197893857956}]}, {"text": "First, the most expensive case (from an annotation perspective) is fully supervised training, where both word sequences and time segmentations are completely specified 1 . A second case is most commonly used in speech recognition systems, where only the word sequences of utterances are given, but their precise segmentations are unknown.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 211, "end_pos": 229, "type": "TASK", "confidence": 0.7206186950206757}]}, {"text": "A third case falls under the realm of semi-supervised approaches.", "labels": [], "entities": []}, {"text": "As one possible example, a previously trained recognizer is used to generate transcripts for unlabeled data, which are then used to re-train the recognizer based on some measure of recognizer confidence ().", "labels": [], "entities": []}, {"text": "The above cases do not exhaust the set of possible training scenarios.", "labels": [], "entities": []}, {"text": "In this paper, we show how the notion of virtual evidence (VE) maybe used to obtain the benefits of data with time segmentations but using only partially labeled data.", "labels": [], "entities": []}, {"text": "Our method lies somewhere between the first and second cases above.", "labels": [], "entities": []}, {"text": "This general framework has been successfully applied in the past to the activity recognition domain ().", "labels": [], "entities": [{"text": "activity recognition domain", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.832209845383962}]}, {"text": "Here we make use of the TIMIT phone recognition task as an example to show how VE maybe used to deal with partially labeled speech training data.", "labels": [], "entities": [{"text": "TIMIT phone recognition task", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7363687306642532}, {"text": "VE", "start_pos": 79, "end_pos": 81, "type": "DATASET", "confidence": 0.4888554513454437}]}, {"text": "To the best of our knowledge, this paper presents the first system to express training uncertainty using VE in the speech domain.", "labels": [], "entities": []}, {"text": "text independent (CI) phone recognition.", "labels": [], "entities": [{"text": "text independent (CI) phone recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7957952065127236}]}, {"text": "All observed variables are shaded, deterministic dependences are depicted using solid black lines, value specific dependences are shown using a dot-dash lines, and random dependencies are represented using dashed lines.", "labels": [], "entities": []}, {"text": "In this paper, given any random variable (rv) X, x denotes a particular value of that rv, D X is the domain of X (x \u2208 D X ), and |D X | represents its cardinality.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}