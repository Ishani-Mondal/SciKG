{"title": [], "abstractContent": [{"text": "This paper introduces an unsupervised morphological segmentation algorithm that shows robust performance for four languages with different levels of morphological complexity.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7721191942691803}]}, {"text": "In particular, our algorithm outperforms Goldsmith's Lin-guistica and Creutz and Lagus's Mor-phessor for English and Bengali, and achieves performance that is comparable to the best results for all three PASCAL evaluation datasets.", "labels": [], "entities": [{"text": "PASCAL evaluation datasets", "start_pos": 204, "end_pos": 230, "type": "DATASET", "confidence": 0.6997365554173788}]}, {"text": "Improvements arise from (1) the use of relative corpus frequency and suffix level similarity for detecting incorrect morpheme attachments and (2) the induction of orthographic rules and allomorphs for segmenting words where roots exhibit spelling changes during morpheme attachments.", "labels": [], "entities": [{"text": "detecting incorrect morpheme attachments", "start_pos": 97, "end_pos": 137, "type": "TASK", "confidence": 0.7762590199708939}]}], "introductionContent": [{"text": "Morphological analysis is the task of segmenting a word into morphemes, the smallest meaningbearing elements of natural languages.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9183984994888306}]}, {"text": "Though very successful, knowledge-based morphological analyzers operate by relying on manually designed segmentation heuristics (e.g.), which require a lot of linguistic expertise and are time-consuming to construct.", "labels": [], "entities": [{"text": "knowledge-based morphological analyzers", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.6427215735117594}]}, {"text": "As a result, research in morphological analysis has exhibited a shift to unsupervised approaches, in which a word is typically segmented based on morphemes that are automatically induced from an unannotated corpus.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.8430144488811493}]}, {"text": "Unsupervised approaches have achieved considerable success for English and many European languages (e.g.,,).", "labels": [], "entities": []}, {"text": "The recent PASCAL Challenge on Unsupervised Segmentation of Words into Morphemes 1 has further intensified interest in this problem, selecting as target languages English as well as two highly agglutinative languages, Turkish and Finnish.", "labels": [], "entities": [{"text": "PASCAL Challenge on Unsupervised Segmentation of Words into Morphemes 1", "start_pos": 11, "end_pos": 82, "type": "TASK", "confidence": 0.7109710901975632}]}, {"text": "However, the evaluation of the Challenge reveals that (1) the success of existing unsupervised morphological parsers does not carryover to the two agglutinative languages, and (2) no segmentation algorithm achieves good performance for all three languages.", "labels": [], "entities": []}, {"text": "Motivated by these state-of-the-art results, our goal in this paper is to develop an unsupervised morphological segmentation algorithm that can work well across different languages.", "labels": [], "entities": []}, {"text": "With this goal in mind, we evaluate our algorithm on four languages with different levels of morphological complexity, namely English, Turkish, Finnish and Bengali.", "labels": [], "entities": []}, {"text": "It is worth noting that Bengali is an underinvestigated Indo-Aryan language that is highly inflectional and lies between English and Turkish/Finnish in terms of morphological complexity.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the robustness of our algorithm across languages: it not only outperforms Goldsmith's (2001) Morphessor for English and Bengali, but also compares favorably to the bestperforming PASCAL morphological parsers when evaluated on all three datasets in the Challenge.", "labels": [], "entities": [{"text": "PASCAL morphological parsers", "start_pos": 212, "end_pos": 240, "type": "TASK", "confidence": 0.6550755401452383}]}, {"text": "The performance improvements of our segmentation algorithm over existing morphological analyzers can be attributed to our extending segmentation method, the best performer for English in the aforementioned PASCAL Challenge, with the capability of handling two under-investigated problems: Detecting incorrect attachments.", "labels": [], "entities": [{"text": "Detecting incorrect attachments", "start_pos": 289, "end_pos": 320, "type": "TASK", "confidence": 0.9062679608662924}]}, {"text": "Many existing morphological parsers incorrectly segment \"candidate\" as \"candid\"+\"ate\", since they fail to identify that the morpheme \"ate\" should not attach to the word \"candid\".", "labels": [], "entities": []}, {"text": "work represents one of the few attempts to address this inappropriate morpheme attachment problem, introducing a method that exploits the semantic relatedness between word pairs.", "labels": [], "entities": [{"text": "inappropriate morpheme attachment", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7344741622606913}]}, {"text": "In contrast, we propose two arguably simpler, yet effective techniques that rely on relative corpus frequency and suffix level similarity to solve the problem.", "labels": [], "entities": []}, {"text": "Inducing orthographic rules and allomorphs.", "labels": [], "entities": []}, {"text": "One problem with Keshava and Pitler's algorithm is that it fails to segment words where the roots exhibit spelling changes during attachment to morphemes (e.g. \"denial\" = \"deny\"+\"al\").", "labels": [], "entities": []}, {"text": "To address this problem, we automatically acquire allomorphs and orthographic change rules from an unannotated corpus.", "labels": [], "entities": []}, {"text": "These rules also allow us to output the actual segmentation of the words that exhibit spelling changes during morpheme attachment, thus avoiding the segmentation of \"denial\" as \"deni\"+\"al\", as is typically done in existing morphological parsers.", "labels": [], "entities": [{"text": "morpheme attachment", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7467731535434723}]}, {"text": "In addition to addressing the aforementioned problems, our segmentation algorithm has two appealing features.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.9762762784957886}]}, {"text": "First, it can segment words with any number of morphemes, whereas many analyzers can only be applied to words with one root and one suffix (e.g.,).", "labels": [], "entities": []}, {"text": "Second, it exhibits robust performance even when inducing morphemes from a very large vocabulary, whereas and morphological analyzers perform well only when a small vocabulary is employed, showing deteriorating performance as the vocabulary size increases.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work on unsupervised morphological analysis.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7402676939964294}]}, {"text": "In Section 3, we describe our basic morpheme induction algorithm.", "labels": [], "entities": []}, {"text": "We then show how to exploit the induced morphemes to (1) detect incorrect attachments by using relative corpus frequency (Section 4) and suffix level similarity (Section 5) and (2) induce orthographic rules and allomorphs (Section 6).", "labels": [], "entities": []}, {"text": "Section 7 describes our algorithm for segmenting a word using the induced morphemes.", "labels": [], "entities": []}, {"text": "We present evaluation results in Section 8 and conclude in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will first evaluate our segmentation algorithm for English and Bengali, and then examine its performance on the PASCAL datasets.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9646250605583191}, {"text": "PASCAL datasets", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.9458894431591034}]}, {"text": "We extracted our English vocabulary from the Wall Street Journal corpus of the Penn Treebank and the BLLIP corpus, preprocessing the documents by first tokenizing them and then removing capitalized words, punctuations and numbers.", "labels": [], "entities": [{"text": "Wall Street Journal corpus of the Penn Treebank", "start_pos": 45, "end_pos": 92, "type": "DATASET", "confidence": 0.9129097014665604}, {"text": "BLLIP corpus", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.8664436936378479}]}, {"text": "In addition, we removed words of frequency 1 from BLLIP, because many of them are proper nouns and misspelled words.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7747604250907898}]}, {"text": "The final English vocabulary consists of approximately 60K distinct words.", "labels": [], "entities": []}, {"text": "We applied the same pre-processing steps to five years of articles taken from the Bengali newspaper Prothom Alo to generate our Bengali vocabulary, which consists of 140K words.", "labels": [], "entities": [{"text": "Prothom Alo", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.7578295767307281}]}, {"text": "To create our English test set, we randomly chose \ud97b\udf59000 words from our vocabulary that are at least 4-character long and also appear in CELEX.", "labels": [], "entities": [{"text": "English test set", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.7315107683340708}, {"text": "CELEX", "start_pos": 135, "end_pos": 140, "type": "DATASET", "confidence": 0.9719876050949097}]}, {"text": "Although 95% of the time we adopted the segmentation proposed by CELEX, in some cases the CELEX segmentations are erroneous (e.g. \"rolling\" and \"lodging\" remain unsegmented in CELEX).", "labels": [], "entities": [{"text": "CELEX", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.9694228768348694}, {"text": "CELEX", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.9597233533859253}]}, {"text": "As a result, we cross-check with the online version of Merriam-Webster to make the necessary changes.", "labels": [], "entities": [{"text": "online version of Merriam-Webster", "start_pos": 37, "end_pos": 70, "type": "DATASET", "confidence": 0.6988904178142548}]}, {"text": "To create the Bengali test set, we randomly chose 5000 words from our vocabulary and manually removed proper nouns and misspelled words from the set before giving it to two of our linguists for hand-segmentation.", "labels": [], "entities": [{"text": "Bengali test set", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.754010001818339}]}, {"text": "The final test set contains 4191 words.", "labels": [], "entities": []}, {"text": "We use two standard metrics --exact accuracy and F-score --to evaluate the performance of our segmentation algorithm on the test sets.", "labels": [], "entities": [{"text": "exact", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9927478432655334}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.6168034076690674}, {"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9980577826499939}, {"text": "segmentation algorithm", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.8947145342826843}]}, {"text": "Exact accuracy is the percentage of the words whose proposed segmentation is identical to the correct segmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9971912503242493}]}, {"text": "F-score is the harmonic mean of recall and precision, which are computed based on the placement of morpheme boundaries.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9804998636245728}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.998314619064331}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9990590214729309}]}], "tableCaptions": [{"text": " Table 1: Word-root frequency ratios", "labels": [], "entities": []}, {"text": " Table 2: Hypothesis validation for English", "labels": [], "entities": [{"text": "Hypothesis validation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7229746282100677}]}, {"text": " Table 3: Results (reported in terms of exact accu- racy (A), precision (P), recall (R) and F-score (F))", "labels": [], "entities": [{"text": "exact accu- racy (A)", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.925059837954385}, {"text": "precision (P)", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9538230895996094}, {"text": "recall (R)", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9614522606134415}, {"text": "F-score (F))", "start_pos": 92, "end_pos": 104, "type": "METRIC", "confidence": 0.9569495171308517}]}, {"text": " Table 4: F-scores for the PASCAL gold standards", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9967958331108093}, {"text": "PASCAL gold standards", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.8957124948501587}]}]}