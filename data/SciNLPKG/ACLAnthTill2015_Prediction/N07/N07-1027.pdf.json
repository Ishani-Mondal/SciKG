{"title": [{"text": "Is Question Answering Better Than Information Retrieval? Towards a Task-Based Evaluation Framework for Question Series", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8314010798931122}, {"text": "Information Retrieval", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7197421789169312}, {"text": "Question Series", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.7857024371623993}]}], "abstractContent": [{"text": "This paper introduces a novel evaluation framework for question series and employs it to explore the effectiveness of QA and IR systems at addressing users' information needs.", "labels": [], "entities": []}, {"text": "The framework is based on the notion of recall curves, which characterize the amount of relevant information contained within a fixed-length text segment.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.987398624420166}]}, {"text": "Although it is widely assumed that QA technology provides more efficient access to information than IR systems , our experiments show that a simple IR baseline is quite competitive.", "labels": [], "entities": []}, {"text": "These results help us better understand the role of NLP technology in QA systems and suggest directions for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "The emergence of question answering (QA) has been driven to a large extent by its intuitive appeal.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.9347865104675293}]}, {"text": "Instead of \"hits\", QA technology promises to deliver \"answers\", obviating the user from the tedious task of sorting through lists of potentially-relevant documents.", "labels": [], "entities": []}, {"text": "The success of factoid QA systems, particularly in the NIST-sponsored TREC evaluations, has reinforced the perception about the superiority of QA systems over traditional IR engines.", "labels": [], "entities": [{"text": "NIST-sponsored TREC evaluations", "start_pos": 55, "end_pos": 86, "type": "DATASET", "confidence": 0.8162923057874044}]}, {"text": "However, is QA really better than IR?", "labels": [], "entities": [{"text": "IR", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.5397045016288757}]}, {"text": "This work challenges existing assumptions and critically examines this question, starting with the development of a novel evaluation framework that better models user tasks and preferences.", "labels": [], "entities": []}, {"text": "The framework is then applied to compare top TREC QA systems against an off-the-shelf IR engine.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.6895459592342377}]}, {"text": "Surprisingly, experiments show that the IR baseline is quite competitive.", "labels": [], "entities": [{"text": "IR", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9689024090766907}]}, {"text": "These results help us better understand the added value of NLP technology in QA systems, and are also useful in guiding future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although most question answering systems rely on information retrieval technology, there has always been the understanding that NLP provides significant added value beyond simple IR.", "labels": [], "entities": [{"text": "question answering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8179755210876465}]}, {"text": "Even the earliest open-domain factoid QA systems, which can be traced back to the late nineties), demonstrated the importance and impact of linguistic processing.", "labels": [], "entities": []}, {"text": "Today's top systems deploy a wide range of advanced NLP technology and can answer over three quarters of factoid questions in an open domain.", "labels": [], "entities": []}, {"text": "However, present QA evaluation methodology does not take into account two developments, discussed below.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.8263376653194427}]}, {"text": "First, despite trends to the contrary in TREC evaluations, users don't actually like or want exact answers.", "labels": [], "entities": []}, {"text": "Most question answering systems are designed to pinpoint the exact named entity (person, date, organization, etc.) that answers a particular question-and the development of such technology has been encouraged by the setup of the TREC QA tracks.", "labels": [], "entities": [{"text": "question answering", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.821961522102356}, {"text": "TREC QA tracks", "start_pos": 229, "end_pos": 243, "type": "DATASET", "confidence": 0.6882999738057455}]}, {"text": "However, a study by  shows that users actually prefer answers embedded within some sort of context, e.g., the sentence or the paragraph that the answer was found in.", "labels": [], "entities": []}, {"text": "Context pro- vides a means by which the user can establish the credibility of system responses and also provides a vehicle for \"serendipitous knowledge discovery\"-finding answers to related questions.", "labels": [], "entities": [{"text": "knowledge discovery\"-finding answers to related questions", "start_pos": 142, "end_pos": 199, "type": "TASK", "confidence": 0.8538161814212799}]}, {"text": "As the early TRECs have found, locating a passage that contains an answer is considerably easier than pinpointing the exact answer.", "labels": [], "entities": []}, {"text": "Thus, real-world user preferences may erode the advantage that QA has over IR techniques such as passage retrieval, e.g.,.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.8818648457527161}]}, {"text": "Second, the focus of question answering research has shifted away from isolated factoid questions to more complex information needs embedded within a broader context (e.g., a user scenario).", "labels": [], "entities": [{"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.866640567779541}]}, {"text": "Since 2004, the main task at the TREC QA tracks has consisted of question series organized around topics (called \"targets\")-which can be people, organizations, entities, or events).", "labels": [], "entities": [{"text": "TREC QA tracks", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.6381202936172485}]}, {"text": "Questions in a series inquire about different facets of a target, but are themselves either factoid or list questions.", "labels": [], "entities": []}, {"text": "In addition, each series contains an explicit \"other\" question (always the last one), which can be paraphrased as \"Tell me other interesting things about this target that I don't know enough to ask directly.\"", "labels": [], "entities": []}, {"text": "See for examples of question series.", "labels": [], "entities": []}, {"text": "Separately, NIST has been exploring other types of complex information needs, for example, the relationship task in TREC 2005 and the ciQA (complex, interactive Question Answering) task in TREC 2006 ().", "labels": [], "entities": [{"text": "NIST", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8886411190032959}, {"text": "TREC 2005", "start_pos": 116, "end_pos": 125, "type": "DATASET", "confidence": 0.8378666639328003}, {"text": "Question Answering) task in TREC 2006", "start_pos": 161, "end_pos": 198, "type": "TASK", "confidence": 0.746028619153159}]}, {"text": "One shared feature of these complex questions is that they cannot be answered by simple named entities.", "labels": [], "entities": []}, {"text": "Answers usually span passages, which makes the task very similar to the query-focused summarization task in DUC ().", "labels": [], "entities": []}, {"text": "On these tasks, it is unclear whether QA systems actually outperform baseline IR methods.", "labels": [], "entities": []}, {"text": "As one bit of evidence, in TREC 2003, a simple IR-based sentence ranker outperformed all but the best system on definition questions, the precursor to current \"other\" questions.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.8059597015380859}, {"text": "IR-based sentence ranker", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7201177676518759}]}, {"text": "We believe that QA evaluation methodology has lagged behind these developments and does not adequately characterize the performance of current systems.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9078330993652344}]}, {"text": "In the next section, we present an evaluation framework that takes into account users' desire for context and the structure of more complex QA tasks.", "labels": [], "entities": []}, {"text": "Focusing on question series, we compare the performance of top TREC systems to a baseline IR engine using this evaluation framework.", "labels": [], "entities": []}, {"text": "Question series in TREC represent an attempt at modeling information-seeking dialogues between a user and a system ().", "labels": [], "entities": []}, {"text": "Primarily because dialogue systems are difficult to evaluate, NIST has adopted a setup in which individual questions are evaluated in isolation-this implicitly models a user who types in a question, receives an answer, and then moves onto the next question in the series.", "labels": [], "entities": []}, {"text": "Component scores are aggregated using a weighted average, and no attempt is made to capture dependencies across different question types.", "labels": [], "entities": []}, {"text": "Simultaneously acknowledging the challenges in evaluating dialogue systems and recognizing the similarities between complex QA and query-focused summarization, we propose an alternative framework for QA evaluation that considers the quality of system responses as a whole.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 200, "end_pos": 213, "type": "TASK", "confidence": 0.9099298715591431}]}, {"text": "Instead of generating individual answers to each question, a system might alternatively produce a segment of text (i.e., a summary) that attempts to answer all the questions.", "labels": [], "entities": []}, {"text": "This slightly different conception of QA brings it into better alignment with recent trends in multi-document summarization, which may yield previously untapped synergies (see Section 7).", "labels": [], "entities": []}, {"text": "To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions, which shares similarities with the pyramid evaluation scheme used in summarization (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 203, "end_pos": 216, "type": "TASK", "confidence": 0.9824473261833191}]}, {"text": "A nugget can be described as an \"atomic fact\" that addresses an aspect of an information need.", "labels": [], "entities": []}, {"text": "Instead of the standard nugget F-score, which hides important tradeoffs between precision and recall, we propose to measure nugget recall as a function of response length.", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.7965114116668701}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9989356398582458}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9910596013069153}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9803919196128845}]}, {"text": "The goal is to quantify the number of relevant facts that a user will have encountered after reading a particular amount of text.", "labels": [], "entities": []}, {"text": "Intuitively, we wish to model how quickly a hypothetical user could \"learn\" about a topic by reading system responses.", "labels": [], "entities": []}, {"text": "Within this framework, we compared existing TREC QA systems against an IR baseline.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.5455400347709656}]}, {"text": "Processed outputs from the top-ranked, second-ranked, third-ranked, and median runs in TREC 2004 and TREC 2005 were compared to a baseline IR run generated by Lucene, an off-the-shelf open-source IR engine.", "labels": [], "entities": [{"text": "TREC 2004 and TREC 2005", "start_pos": 87, "end_pos": 110, "type": "DATASET", "confidence": 0.8186505913734436}, {"text": "Lucene", "start_pos": 159, "end_pos": 165, "type": "DATASET", "confidence": 0.9416152238845825}]}, {"text": "Our experiments focused on factoid and \"other\" questions; as the details differ for these two types, we describe each separately and then return to a unified picture.", "labels": [], "entities": []}, {"text": "The evaluation of \"other\" questions closely mirrors the procedure developed for factoid series.", "labels": [], "entities": [{"text": "factoid series", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.866130143404007}]}, {"text": "We employed POURPRE (), a recently developed method for automatically evaluating answers to complex questions.", "labels": [], "entities": [{"text": "POURPRE", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9748820066452026}]}, {"text": "The metric relies on n-gram overlap as a surrogate for manual nugget matching, and has been shown to correlate well with official human judgments.", "labels": [], "entities": [{"text": "nugget matching", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.6960096061229706}]}, {"text": "We modified the POURPRE scoring script to return only the nugget recall (of vital nuggets only).", "labels": [], "entities": [{"text": "POURPRE", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7382451891899109}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.7454025745391846}]}, {"text": "Formally, systems' responses to \"other\" questions consist of unordered sets of answer strings.", "labels": [], "entities": []}, {"text": "We decided to break each system's response into individual answer strings and compute nugget recall on a string-by-string basis.", "labels": [], "entities": [{"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9831651449203491}]}, {"text": "Since these answer strings are for the most part sentences, results are comparable to the factoid series experiments.", "labels": [], "entities": []}, {"text": "Taking answer strings as the basic response unit also makes sense because it respects segment boundaries that are presumably meaningful to the original systems.", "labels": [], "entities": []}, {"text": "Computing POURPRE recall at different response lengths yielded an uninterpolated data series for each topic.", "labels": [], "entities": [{"text": "POURPRE recall", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6571958661079407}]}, {"text": "Results across topics were aggregated in the same manner as the factoid series: first by interpolating to the nearest larger fifty-character increment, and then averaging all topics across each length increment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Official scores of selected TREC 2004 and  TREC 2005 factoid runs.", "labels": [], "entities": [{"text": "TREC 2004 and  TREC 2005 factoid runs", "start_pos": 38, "end_pos": 75, "type": "DATASET", "confidence": 0.7515158908707755}]}, {"text": " Table 3: Official scores of selected TREC 2004 and  TREC 2005 \"other\" runs.", "labels": [], "entities": [{"text": "TREC 2004 and  TREC 2005", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.6742450892925262}]}]}