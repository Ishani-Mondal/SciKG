{"title": [{"text": "Toward Multimedia: A String Pattern-based Passage Ranking Model for Video Question Answering", "labels": [], "entities": [{"text": "Video Question Answering", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6265640258789062}]}], "abstractContent": [{"text": "In this paper, we present anew string pattern matching-based passage ranking algorithm for extending traditional text-based QA toward videoQA.", "labels": [], "entities": [{"text": "string pattern matching-based passage ranking", "start_pos": 31, "end_pos": 76, "type": "TASK", "confidence": 0.6487276673316955}]}, {"text": "Users interact with our videoQA system through natural language questions, while our system returns passage fragments with corresponding video clips as answers.", "labels": [], "entities": []}, {"text": "We collect 75.6 hours videos and 253 Chinese questions for evaluation.", "labels": [], "entities": []}, {"text": "The experimental results showed that our method outperformed six top-performed ranking models.", "labels": [], "entities": []}, {"text": "It is 10.16% better than the second best method (language model) in relatively MRR score and 6.12% in precision rate.", "labels": [], "entities": [{"text": "MRR score", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9590482115745544}, {"text": "precision rate", "start_pos": 102, "end_pos": 116, "type": "METRIC", "confidence": 0.9836763143539429}]}, {"text": "Besides, we also show that the use of a trained Chinese word segmentation tool did decrease the overall videoQA performance where most ranking algorithms dropped at least 10% in relatively MRR, precision, and answer pattern recall rates.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.664800097544988}, {"text": "MRR", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9399805068969727}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9968709349632263}, {"text": "recall", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.8945611715316772}]}], "introductionContent": [{"text": "With the drastic growth of video sources, effective indexing and retrieving video contents has recently been addressed.", "labels": [], "entities": [{"text": "indexing and retrieving video contents", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.7186022996902466}]}, {"text": "The well-known Informedia project () and TREC-VID track) are the two famous examples.", "labels": [], "entities": [{"text": "TREC-VID", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8672062158584595}]}, {"text": "Although text-based question answering (QA) has become a key research issue in past decade, to support multimedia such as video, it is still beginning.", "labels": [], "entities": [{"text": "text-based question answering (QA)", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.8011928846438726}]}, {"text": "Over the past five years, several video QA studies had investigated.", "labels": [], "entities": []}, {"text": "presented an earlier work on combining videoOCR and term weighting models.", "labels": [], "entities": []}, {"text": "proposed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9254400730133057}]}, {"text": "They adopted the term-weighting method) to rank the video segments by weighting the pre-defined keywords.", "labels": [], "entities": []}, {"text": "developed a lexical pattern matching-based ranking method fora domain-specific videoQA.", "labels": [], "entities": []}, {"text": "In the same year, designed a cross-language (Englishto-Chinese) video question answering system based on extracting pre-defined named entity words in captions.", "labels": [], "entities": [{"text": "cross-language (Englishto-Chinese) video question answering", "start_pos": 29, "end_pos": 88, "type": "TASK", "confidence": 0.5918846939291272}]}, {"text": "On the other hand, made use of the simple TFIDF term weighting schema to retrieve the manualsegmented clips for video caption word retrieval.", "labels": [], "entities": [{"text": "TFIDF term weighting", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.6586901545524597}, {"text": "video caption word retrieval", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.7720490843057632}]}, {"text": "They also manually developed the ontology to improve system performance.", "labels": [], "entities": []}, {"text": "In this paper, we present anew string pattern matching-based passage ranking algorithm for video question answering.", "labels": [], "entities": [{"text": "string pattern matching-based passage ranking", "start_pos": 31, "end_pos": 76, "type": "TASK", "confidence": 0.6376574516296387}, {"text": "video question answering", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.5839851995309194}]}, {"text": "We consider that the passage is able to answer questions and also suitable for videos because itself forms a very natural unit.", "labels": [], "entities": []}, {"text": "showed that users prefer passage-level answers over short answer phrases since it contains rich context information.", "labels": [], "entities": []}, {"text": "Our method makes use of the string pattern searching in the suffix trees to find common subsequences between a passage and question.", "labels": [], "entities": []}, {"text": "The proposed term weighting schema is then designed to compute passage score.", "labels": [], "entities": []}, {"text": "In addition, to avoid generating over-length subsequence, we also present two algorithms for re-tokenization and weighting.", "labels": [], "entities": []}], "datasetContent": [{"text": "We should carefully select the use of videoQA collection for evaluation.", "labels": [], "entities": []}, {"text": "Unfortunately, there is no benchmark corpus for this task.", "labels": [], "entities": []}, {"text": "Thus, we develop an annotated collection by following the similar tasks as TREC, CLEF, and NTCIR.", "labels": [], "entities": [{"text": "TREC", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.6946213841438293}, {"text": "NTCIR", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8663989305496216}]}, {"text": "The Discovery videos are one of the popular raw video sources and widely evaluated in many literatures ().", "labels": [], "entities": []}, {"text": "Totally, 75.6 hours of Discovery videos (93 video names) were used.", "labels": [], "entities": [{"text": "Discovery videos", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.9315271079540253}]}, {"text": "lists the statistics of the Discovery films.", "labels": [], "entities": [{"text": "Discovery films", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.940682590007782}]}, {"text": "The questions were created in two different ways: one set (about 73) was collected from previous studies () which came from the \"Project: Assignment of Discovery\"; while the other was derived from areal log from users.", "labels": [], "entities": []}, {"text": "Video collections are difficult to be general-purpose since hundreds hours of videos might take tens of hundreds GB storage space.", "labels": [], "entities": []}, {"text": "Therefore, general questions are quite difficult to be found in the video database.", "labels": [], "entities": []}, {"text": "Hence, we provide a list of short introductions collected from the cover-page of the videos and enable users to browse the descriptions.", "labels": [], "entities": []}, {"text": "Users were then asked for the system with limited to the collected video topics.", "labels": [], "entities": []}, {"text": "We finally filter the (1) keyword-like queries (2) non-Chinese and (3) un-supported questions.", "labels": [], "entities": []}, {"text": "Finally, there were 253 questions for evaluation.", "labels": [], "entities": []}, {"text": "For the answer assessment, we followed the TREC-QA track) and NTCIR to annotate answers in the pool that collected from the outputs of different passage retrieval methods.", "labels": [], "entities": [{"text": "answer assessment", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8728632926940918}, {"text": "TREC-QA track", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.9625604748725891}, {"text": "NTCIR", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.5914978384971619}]}, {"text": "Unlike traditional text QA task, most of the OCR sentences contain a number of OCR error words.", "labels": [], "entities": [{"text": "text QA task", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7072605391343435}]}, {"text": "Furthermore, some sentence did include the answer string but error recognized as different words.", "labels": [], "entities": []}, {"text": "Thus, instead of annotating the recognized transcripts, we used the corresponding video frames for evaluation because users can directly find the answers in the retrieved video clips and recognized text.", "labels": [], "entities": []}, {"text": "Among 253 questions, 56 of which did not have an answer, while 368 passage&frame segments (i.e., answer patterns) in the pool were labeled as answers.", "labels": [], "entities": []}, {"text": "On averagely, there are 1.45 labeled answers for each question.", "labels": [], "entities": []}, {"text": "The MRR) score, precision and pattern-recall are used for evaluation.", "labels": [], "entities": [{"text": "MRR) score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9677584966023763}, {"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9997075200080872}]}, {"text": "We measure the MRR scores for both top1 and top5 ranks, and precision and pattern-recall rates for top5 retrieved answers.", "labels": [], "entities": [{"text": "MRR", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9042107462882996}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9993001222610474}]}], "tableCaptions": [{"text": " Table 1: Statistics of the collected Discovery videos", "labels": [], "entities": [{"text": "collected Discovery videos", "start_pos": 28, "end_pos": 54, "type": "DATASET", "confidence": 0.7485693494478861}]}, {"text": " Table 2: Overall videoQA performance with differ- ent ranking models (using unigram Chinese word)", "labels": [], "entities": []}, {"text": " Table 3: Overall videoQA performance with differ- ent ranking models using word segmentation tools", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.6976450979709625}]}]}