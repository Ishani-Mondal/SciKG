{"title": [{"text": "Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7708653807640076}]}], "abstractContent": [{"text": "The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8466614603996276}]}, {"text": "We propose a method for automatically evaluating the quality of each translation.", "labels": [], "entities": []}, {"text": "In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality.", "labels": [], "entities": []}, {"text": "In English-Japanese translation, for example, prepositions and infinitives must be appropriately translated.", "labels": [], "entities": []}, {"text": "We show several procedures that enable evaluating the quality of a translated sentence more appropriately than using conventional methods.", "labels": [], "entities": []}, {"text": "The first procedure is constructing a test set where the conditions are assigned to each test-set sentence in the form of yes/no questions.", "labels": [], "entities": []}, {"text": "The second procedure is developing a system that determines an answer to each question.", "labels": [], "entities": []}, {"text": "The third procedure is combining a measure based on the questions and conventional measures.", "labels": [], "entities": []}, {"text": "We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals.", "labels": [], "entities": []}], "introductionContent": [{"text": "In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue.", "labels": [], "entities": [{"text": "machine translation (MT) research", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8909260233243307}, {"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9755927324295044}]}, {"text": "In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations () because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9887231588363647}, {"text": "MT evaluations", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.9280345737934113}, {"text": "MT evaluation", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.9265865385532379}, {"text": "MT", "start_pos": 253, "end_pos": 255, "type": "TASK", "confidence": 0.9772008657455444}]}, {"text": "For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9952432513237}, {"text": "MT evaluation", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.8850090503692627}, {"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.9635941982269287}]}, {"text": "This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9958851933479309}, {"text": "MT evaluation", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9070435166358948}]}, {"text": "MT systems can be ranked if a set of MT results for each system and their reference translations are given.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9548068046569824}, {"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9225666522979736}]}, {"text": "Usually, about 300 or more sentences are used to automatically rank MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.8780821561813354}]}, {"text": "However, the quality of a sentence translated by an MT system is difficult to evaluate.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9553301930427551}]}, {"text": "For example, the results of five MTs into Japanese of the sentence \"The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers.\" are shown in.", "labels": [], "entities": [{"text": "MTs", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9840092658996582}]}, {"text": "A conventional automatic evaluation method ranks the fifth MT result first although its human subjective evaluation is the lowest.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.982404887676239}]}, {"text": "This is because conventional methods are based on the similarity between a translated sentence and its reference translation, and they give the translated sentence a high score when the two sentences are globally similar to each other in terms of lexical overlap.", "labels": [], "entities": []}, {"text": "However, in the case of the above example, the most important thing to maintain a high translation quality is to correctly translate \"for\" into the target language, and it would be difficult to detect the importance just by comparing an MT result and its reference translations even if the number of reference translations is increased.", "labels": [], "entities": []}, {"text": "In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we show that constructing a test set where the conditions that are mainly established from a linguistic point of view are assigned to each test-set sentence in the form of yes/no questions, developing a system that determines an answer to each question, and combining a measure based on the questions and conventional measures enable the evaluation of the quality of a translated sentence more appropriately than using conventional methods.", "labels": [], "entities": []}, {"text": "We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, the translation results of three MT systems and their subjective evaluation results were used as a development set for constructing the patterns described in Section 3.2 and for tuning the parameters \u03bb Si , \u03bb Q j , \u03bb Q \ud97b\udf59 j , and \u03bb \ud97b\udf59 in Eq.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9729664325714111}]}, {"text": "The translations and evaluation results of the remaining two MT systems were used as an evaluation set for testing.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9510706663131714}]}, {"text": "In the development set, each test-set sentence has at least one question, at least one reference translation, three MT results, and subjective evaluation results of the three MT results.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9223775863647461}]}, {"text": "The patterns for determining yes/no answers were manually constructed for the questions assigned to the 769 test-set sentences.", "labels": [], "entities": []}, {"text": "There were 917 questions assigned to them.", "labels": [], "entities": []}, {"text": "Among them, the patterns could be constructed for 898 questions assigned to 767 test-set sentences.", "labels": [], "entities": []}, {"text": "The remaining 19 questions were skipped because making simple patterns as described in Section 3.2 was difficult; for example, one of the questions was \"Is the whole sentence translated into one sentence?\".", "labels": [], "entities": []}, {"text": "The yes/no answer determination accuracies obtained by using the patterns are shown in Table 5.", "labels": [], "entities": [{"text": "yes/no answer determination", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6042380273342133}]}, {"text": "We investigated the correlation between the evaluation score, A in Eq.", "labels": [], "entities": [{"text": "A", "start_pos": 62, "end_pos": 63, "type": "METRIC", "confidence": 0.9897281527519226}]}, {"text": "(1) and the subjective evaluations, fluency and adequacy, for the 769 test-set sentences.", "labels": [], "entities": []}, {"text": "First, to maximize the correlation coefficients between the evaluation score, A, and the human subjective evaluations, fluency and adequacy, the optimal values of \u03bb Si , \u03bb Q j , \u03bb Q \ud97b\udf59 j , and \u03bb \ud97b\udf59 in Eq.", "labels": [], "entities": [{"text": "A", "start_pos": 78, "end_pos": 79, "type": "METRIC", "confidence": 0.9538630247116089}]}, {"text": "(1) were investigated using the development set within a framework of multiple linear regression modeling.", "labels": [], "entities": [{"text": "multiple linear regression modeling", "start_pos": 70, "end_pos": 105, "type": "TASK", "confidence": 0.6246034502983093}]}, {"text": "Then, the correlation coefficients were investigated by using the optimal value set.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9221673607826233}]}, {"text": "The results are shown in, 7, and 8.", "labels": [], "entities": []}, {"text": "In these tables, \"Conventional method\" indicates the correlation coefficients obtained when A was calculated by using only similarities Si . \"Conventional method (combination)\" is a combination of existing automatic evaluation methods from the literature.", "labels": [], "entities": []}, {"text": "\"Our method (automatic)\" indicates the correlation coefficients obtained when the results of the automatic determination of yes/no answers were used to calculate Q j and Q \ud97b\udf59 j in Eq.", "labels": [], "entities": [{"text": "correlation", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9572718739509583}]}, {"text": "For the 19 questions for which the patterns could not be constructed, Q j was set at 0.", "labels": [], "entities": []}, {"text": "\"Our method (full automatic)\" indicates the correlation coefficients obtained when the results of the automatic sub-goal generation and determination of rate of accomplish-   in Eq.", "labels": [], "entities": []}, {"text": "Skip word trigrams, skip word bigrams, and skip word unigrams were used for generating the sub-goals according to our preliminary experiments.", "labels": [], "entities": []}, {"text": "\"Our method (upper bound)\" indicates the correlation coefficients obtained when human judgments on the questions were used to calculate Q j and Q \ud97b\udf59 j . As shown in, 7, and 8, our methods significantly outperform the conventional methods from literature.", "labels": [], "entities": []}, {"text": "Note that WER outperformed other individual measures like BLEU and NIST in our experiments, and the combination of existing automatic evaluation methods from the literature outperformed individual lexical similarity measures by themselves in almost all cases.", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9875530004501343}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9939497709274292}, {"text": "NIST", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.852094829082489}]}, {"text": "The differences between the correlation coefficients obtained using our method and the conventional methods are statistically significant at the 5% or less significance level for fluency and adequacy, even if the number of reference translations increases, except in three cases shown in and 8.", "labels": [], "entities": []}, {"text": "This indicates that considering the rate of accomplishment of sub-goals to automatically evaluate the quality of each translation is useful, especially when the number of reference translations is small.", "labels": [], "entities": []}, {"text": "The differences between the correlation coefficients obtained using two automatic methods are not significant.", "labels": [], "entities": [{"text": "correlation", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9545775651931763}]}, {"text": "These results indicate that we can reduce the development cost for constructing sub-goals.", "labels": [], "entities": []}, {"text": "However, there are still significant gaps between the correlation coefficients obtained using a fully automatic method and upper bounds.", "labels": [], "entities": [{"text": "correlation", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.9467878937721252}]}, {"text": "These gaps indicate that we need further improvement in automatic sub-goal generation and automatic estimation of rate of accomplishment of sub-goals, which is our future work.", "labels": [], "entities": [{"text": "sub-goal generation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.6847664266824722}]}, {"text": "Human judgments of adequacy and fluency are known to be noisy, with varying levels of intercoder agreement.", "labels": [], "entities": []}, {"text": "Recent work has tended to apply crossjudge normalization to address this issue (.", "labels": [], "entities": [{"text": "crossjudge normalization", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.833484947681427}]}, {"text": "We would like to evaluate against the normalized data in the future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of conventional automatic evaluations.", "labels": [], "entities": []}, {"text": " Table 3: Examples of subjective evaluations based on yes/no questions.", "labels": [], "entities": []}, {"text": " Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy. (A reference transla- tion is used to calculate S i .)", "labels": [], "entities": [{"text": "evaluation score A", "start_pos": 46, "end_pos": 64, "type": "METRIC", "confidence": 0.5789615710576376}]}, {"text": " Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy. (Three reference  translations are used to calculate S i .)", "labels": [], "entities": [{"text": "evaluation score A", "start_pos": 46, "end_pos": 64, "type": "METRIC", "confidence": 0.612822006146113}]}, {"text": " Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy. (Five reference  translations are used to calculate S i .)", "labels": [], "entities": [{"text": "evaluation score A", "start_pos": 46, "end_pos": 64, "type": "METRIC", "confidence": 0.6035038828849792}]}]}