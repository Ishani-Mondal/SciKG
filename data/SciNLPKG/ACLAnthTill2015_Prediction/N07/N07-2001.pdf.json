{"title": [{"text": "Comparing User Simulation Models For Dialog Strategy Learning", "labels": [], "entities": [{"text": "Comparing User Simulation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5907856523990631}, {"text": "Dialog Strategy", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8770498931407928}]}], "abstractContent": [{"text": "This paper explores what kind of user simulation model is suitable for developing a training corpus for using Markov Decision Processes (MDPs) to automatically learn dialog strategies.", "labels": [], "entities": []}, {"text": "Our results suggest that with sparse training data, a model that aims to randomly explore more dialog state spaces with certain constraints actually performs at the same or better than a more complex model that simulates realistic user behaviors in a statistical way.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, user simulation has been used in the development of spoken dialog systems.", "labels": [], "entities": []}, {"text": "In contrast to experiments with human subjects, which are usually expensive and time consuming, user simulation generates a large corpus of user behaviors in a low-cost and time-efficient manner.", "labels": [], "entities": []}, {"text": "For example, user simulation has been used in evaluation of spoken dialog systems and to learn dialog strategies.", "labels": [], "entities": []}, {"text": "However, these studies do not systematically evaluate how helpful a user simulation is.", "labels": [], "entities": []}, {"text": "( ) propose a set of evaluation measures to assess the realness of the simulated corpora (i.e. how similar are the simulated behaviors and human behaviors).", "labels": [], "entities": []}, {"text": "Nevertheless, how realistic a simulated corpus needs to be for different tasks is still an open question.", "labels": [], "entities": []}, {"text": "We hypothesize that for tasks like system evaluation, a more realistic simulated corpus is preferable.", "labels": [], "entities": [{"text": "system evaluation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7211599946022034}]}, {"text": "Since the system strategies are evaluated and adapted based on the analysis of these simulated dialog behaviors, we would expect that these behaviors are what we are going to see in the test phase when the systems interact with human users.", "labels": [], "entities": []}, {"text": "However, for automatically learning dialog strategies, it is not clear how realistic versus how exploratory) the training corpus should be.", "labels": [], "entities": []}, {"text": "A training corpus needs to be exploratory with respect to the chosen dialog system actions because if a certain action is never tried at certain states, we will not know the value of taking that action in that state.", "labels": [], "entities": []}, {"text": "In (), their system is designed to randomly choose one from the allowed actions with uniform probability in the training phase in order to explore possible dialog state spaces.", "labels": [], "entities": []}, {"text": "In contrast,we use user simulation to generate exploratory training data because in the tutoring system we work with, reasonable tutor actions are largely restricted by student performance.", "labels": [], "entities": []}, {"text": "If certain student actions do not appear, this system would not be able to explore a state space randomly . This paper investigates what kind of user simulation is good for using Markov Decision Processes (MDPs) to learn dialog strategies.", "labels": [], "entities": []}, {"text": "In this study, we compare three simulation models which differ in their efforts on modeling the dialog behaviors in a training corpus versus exploring a potentially larger dialog space.", "labels": [], "entities": []}, {"text": "In addition, we look into the impact of different state space representations and different reward functions on the choice of simulation models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Evaluation of the new policies trained with the three simulation models", "labels": [], "entities": []}]}