{"title": [{"text": "Semi-Supervised Learning for Semantic Parsing using Support Vector Machines", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7366332113742828}]}], "abstractContent": [{"text": "We present a method for utilizing unan-notated sentences to improve a semantic parser which maps natural language (NL) sentences into their formal meaning representations (MRs).", "labels": [], "entities": []}, {"text": "Given NL sentences annotated with their MRs, the initial supervised semantic parser learns the mapping by training Support Vector Machine (SVM) classifiers for every production in the MR grammar.", "labels": [], "entities": []}, {"text": "Our new method applies the learned semantic parser to the unannotated sentences and collects unla-beled examples which are then used to retrain the classifiers using a variant of transductive SVMs.", "labels": [], "entities": []}, {"text": "Experimental results show the improvements obtained over the purely supervised parser, particularly when the annotated training set is small.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing is the task of mapping a natural language (NL) sentence into a complete, formal meaning representation (MR) which a computer program can execute to perform some task, like answering database queries or controlling a robot.", "labels": [], "entities": [{"text": "Semantic parsing is the task of mapping a natural language (NL) sentence into a complete, formal meaning representation (MR) which a computer program can execute to perform some task, like answering database queries or controlling a robot", "start_pos": 0, "end_pos": 238, "type": "Description", "confidence": 0.7992784110612647}]}, {"text": "These MRs are expressed in domain-specific unambiguous formal meaning representation languages (MRLs).", "labels": [], "entities": [{"text": "domain-specific unambiguous formal meaning representation languages (MRLs)", "start_pos": 27, "end_pos": 101, "type": "TASK", "confidence": 0.7088066935539246}]}, {"text": "Given a training corpus of NL sentences annotated with their correct MRs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct MRs.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.728617712855339}]}, {"text": "Several learning systems have been developed for semantic parsing, many of them recently).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.8647552728652954}]}, {"text": "These systems use supervised learning methods which only utilize annotated NL sentences.", "labels": [], "entities": []}, {"text": "However, it requires considerable human effort to annotate sentences.", "labels": [], "entities": []}, {"text": "In contrast, unannotated NL sentences are usually easily available.", "labels": [], "entities": []}, {"text": "Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data).", "labels": [], "entities": []}, {"text": "In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.8531113862991333}]}, {"text": "We modify KRISP, a supervised learning system for semantic parsing presented in), to make a semi-supervised system we call SEMISUP-KRISP.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7483247816562653}]}, {"text": "Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared the performance of SEMISUP-KRISP and KRISP in the GEOQUERY domain for semantic parsing in which the MRL is a functional language used to query a U.S. geography database ().", "labels": [], "entities": [{"text": "GEOQUERY domain", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9266940653324127}, {"text": "semantic parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7340068817138672}]}, {"text": "This domain has been used inmost of the previous work.", "labels": [], "entities": []}, {"text": "The original corpus contains \u00be\u00be\u00bc NL queries collected from undergraduate students and annotated with their correct MRs (.", "labels": [], "entities": [{"text": "MRs", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9340349435806274}]}, {"text": "Later, \ud97b\udf59\u00bf\u00bc additional NL queries were collected from real users of a web-based interface and annotated).", "labels": [], "entities": []}, {"text": "We used this data as unannotated sentences in our current experiments.", "labels": [], "entities": []}, {"text": "We also collected an additional \ud97b\udf59\u00bc\u00bc queries from the same interface, making a total of \u00bd\ud97b\udf59\u00bc\u00bf\u00bf unannotated sentences.", "labels": [], "entities": []}, {"text": "The systems were evaluated using standard 10-fold cross validation.", "labels": [], "entities": []}, {"text": "All the unannotated sentences were used for training in each fold.", "labels": [], "entities": []}, {"text": "Performance was measured in terms of precision (the percentage of generated MRs that were correct) and recall (the percentage of all sentences for which correct MRs were obtained).", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9995225667953491}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9995586276054382}]}, {"text": "An output MR is considered correct if and only if the resulting query retrieves the same answer as the correct MR when submitted to the database.", "labels": [], "entities": []}, {"text": "Since the systems assign confidences to the MRs they generate, the entire range of the precision-recall trade-off can be obtained fora system by measuring precision and recall at various confidence levels.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.9922447800636292}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9983916878700256}, {"text": "recall", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9960376024246216}]}, {"text": "We present learning curves for the best F-measure (harmonic mean of precision and re- call) obtained across the precision-recall trade-off as the amount of annotated training data is increased.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9980530738830566}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.8270245790481567}, {"text": "re- call)", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.959149032831192}]}, {"text": "shows the results for both systems.", "labels": [], "entities": []}, {"text": "The results clearly show the improvement SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences, particularly when the number of annotated sentences is small.", "labels": [], "entities": [{"text": "SEMISUP-KRISP", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.5777648687362671}]}, {"text": "We also show the performance of a hand-built semantic parser GEOBASE) for comparison.", "labels": [], "entities": []}, {"text": "From the figure, it can be seen that, on average, KRISP achieves the same performance as GEOBASE when it is given \u00bd\u00be\u00be annotated examples, while SEMISUP-KRISP reaches this level given only \ud97b\udf59\ud97b\udf59 annotated examples, a \u00be\u00be\ud97b\udf59\ud97b\udf59\u00b1 savings in humanannotation effort.", "labels": [], "entities": [{"text": "GEOBASE", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9003757238388062}]}], "tableCaptions": []}