{"title": [{"text": "Entity Extraction is a Boring Solved Problem -or is it?", "labels": [], "entities": [{"text": "Entity Extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8674644231796265}]}], "abstractContent": [{"text": "This paper presents empirical results that contradict the prevailing opinion that entity extraction is a boring solved problem.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.8620733320713043}]}, {"text": "In particular, we consider data sets that resemble familiar MUC/ACE data, and report surprisingly poor performance for both commercial and research systems.", "labels": [], "entities": [{"text": "MUC/ACE data", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.6764117032289505}]}, {"text": "We then give an error analysis that suggests research challenges for entity extraction that are neither boring nor solved.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.8032759726047516}]}, {"text": "1 Background Entity extraction or named entity recognition, as it is sometimes called, is a known and familiar problem.", "labels": [], "entities": [{"text": "Background Entity extraction", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.6918480396270752}, {"text": "named entity recognition", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.5952519079049429}]}, {"text": "Named entity (NE) tagging has been the subject of numerous shared-task evaluations, including the seminal MUC 6, MUC 7 and MET evaluations, the CoNLL shared task, the SIGHAN bake-offs, and the ACE evaluations.", "labels": [], "entities": [{"text": "Named entity (NE) tagging", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5405760953823725}]}, {"text": "With this track record, and with commercial vendors now selling named-entity tagging fora fee, many naturally consider entity extraction to bean essentially solved problem.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7327555418014526}]}, {"text": "The present paper challenges this view.", "labels": [], "entities": []}, {"text": "The main issue, as we see it, is transfer: NE tag-gers developed fora specific corpus tend not to perform well on other data sets.", "labels": [], "entities": [{"text": "transfer", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.9668389558792114}]}, {"text": "Kosseim and Poibeau (2001), for one, show that the informal language of email or speech transcriptions befud-dles taggers built for journalistic text.", "labels": [], "entities": []}, {"text": "Minkov et al (2005) further explore the systematic differences between journalistic and informal texts, training separate taggers for each text source of interest.", "labels": [], "entities": []}, {"text": "Because named entity taggers are so strongly based on surface features, it isn't surprising to observe poor tagger transfer across texts with significantly different styles or with unrelated content.", "labels": [], "entities": []}, {"text": "In this paper, we report on the more surprising result that transfer issues arise even for texts with closely aligned content or closely aligned styles.", "labels": [], "entities": []}, {"text": "In particular, we consider a range of primarily business-related texts that are, on the face of it, close in style and/or substance to the journalistic stories in existing NE data sets, MUC 6 in particular.", "labels": [], "entities": [{"text": "NE data sets", "start_pos": 172, "end_pos": 184, "type": "DATASET", "confidence": 0.8387909730275472}, {"text": "MUC 6", "start_pos": 186, "end_pos": 191, "type": "DATASET", "confidence": 0.8940041959285736}]}, {"text": "We thus would have expected these texts to support good transfer performance from taggers configured to the MUC task.", "labels": [], "entities": [{"text": "MUC task", "start_pos": 108, "end_pos": 116, "type": "TASK", "confidence": 0.6357280015945435}]}, {"text": "Instead, we found the same kinds of performance drops as Kosseim and Poibeau had noted for informal texts.", "labels": [], "entities": []}, {"text": "Our aim here is to shed light on the how and why of this.", "labels": [], "entities": []}, {"text": "2 Scope of the present study We begin with a disclaimer.", "labels": [], "entities": []}, {"text": "Our goal is not so much to present new technical solutions to NE recognition , as to draw attention to those aspects of the problem that remain unsolved.", "labels": [], "entities": [{"text": "NE recognition", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.9686974287033081}]}, {"text": "We cover two main thrusts: (i) a black-box evaluation of several NE taggers (commercial and research systems); and (ii) an error analysis of system performance.", "labels": [], "entities": [{"text": "NE taggers", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.8559026122093201}]}], "introductionContent": [], "datasetContent": [{"text": "Our evaluation data set contains three distinct sections.", "labels": [], "entities": [{"text": "evaluation data set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7266653776168823}]}, {"text": "The largest component consists of publiclyavailable financial reports filed with the Securities and Exchange Commission (SEC), in particular the 2003 forms 10-K filed by eight Fortune 500 companies.", "labels": [], "entities": []}, {"text": "These corporate annual reports share the same subject matter as much business news: sales, profits, acquisitions, business strategies and the like.", "labels": [], "entities": []}, {"text": "They take, however, a more technical slant and are rich in accounting jargon.", "labels": [], "entities": []}, {"text": "They are also longer, ranging in our study from 22 to 54 pages.", "labels": [], "entities": []}, {"text": "Preliminary exploration with our own MUC 6 tagger showed these SEC filings to be particularly hard to tag.", "labels": [], "entities": [{"text": "MUC 6 tagger", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8473909894625345}, {"text": "SEC filings", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.8198395371437073}]}, {"text": "Because their sheer length and technical emphasis seemed implicated in this poor performance, we assembled a second corpus of forty Web-hosted business stories from such news providers as MS-NBC, CNN Money, and Motley Fool.", "labels": [], "entities": [{"text": "CNN Money", "start_pos": 196, "end_pos": 205, "type": "DATASET", "confidence": 0.8877691924571991}, {"text": "Motley Fool", "start_pos": 211, "end_pos": 222, "type": "DATASET", "confidence": 0.9210208654403687}]}, {"text": "These stories focus on the same eight companies as our 10-K data set, but are shorter and less technical, thus allowing us to isolate length and technicality as factors in tagging business texts.", "labels": [], "entities": [{"text": "10-K data set", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.8185815215110779}]}, {"text": "The final portion of our test set consists often news stories that were selected to closely match the kind of data used in past MUC evaluations.", "labels": [], "entities": [{"text": "MUC evaluations", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.7668124437332153}]}, {"text": "They were drawn from the New York Times (NYT) and Wall Street Journal (WSJ) on-line editions, and focus on current events, thus providing one more comparable dimension of evaluation.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) on-line editions", "start_pos": 50, "end_pos": 92, "type": "DATASET", "confidence": 0.89146688580513}]}, {"text": "1  We attempted to replicate the procedure used in the MUC evaluations, extending it only as required by the characteristics of the taggers.", "labels": [], "entities": [{"text": "MUC evaluations", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.5342970192432404}]}, {"text": "The test data were formatted as in MUC 6, and where SGML markup ran afoul of system I/O characteristics, we remapped the data manually, resolving, e.g., crossing tags that may have strayed into the output.", "labels": [], "entities": [{"text": "MUC 6", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.7364889681339264}]}, {"text": "To provide scores that could be compared with the MUC evaluations, we created MUC6-compliant answer keys, and remapped system output to this standard.", "labels": [], "entities": [{"text": "MUC evaluations", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.7864220142364502}]}, {"text": "We removed system responses that were considered non-taggable in MUC (e.g., URLs) and conflated fine-grained distinctions not made in MUC (e.g., remapping country tags to location).", "labels": [], "entities": [{"text": "MUC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8668960332870483}]}, {"text": "Scores were assessed with the venerable MUC scorer, which provides partial credit for system responses that match the key in type but not extent, or vice-versa.", "labels": [], "entities": [{"text": "MUC scorer", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.5861417651176453}, {"text": "extent", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9773378372192383}]}, {"text": "The scorer also provides a full error analysis, separately characterizing each error in a system response., overleaf, presents our overall findings, aggregated across the three primary entity types: person, organization, and location (the ENAMEX types in the MUC standard).", "labels": [], "entities": [{"text": "MUC standard", "start_pos": 259, "end_pos": 271, "type": "DATASET", "confidence": 0.9395942091941833}]}, {"text": "We generally did not measure the MUC TIMEX (dates, times) and NUMEX types (moneys, percents) because: (i) neither of the statistical systems generate them; (ii) those systems that do generate them tend to do well; (iii) they are overwhelmingly more frequent in the SEC data than in news, thus skewing results.", "labels": [], "entities": [{"text": "MUC", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.4187266528606415}, {"text": "TIMEX", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.5584263801574707}, {"text": "NUMEX types", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9602982401847839}, {"text": "SEC data", "start_pos": 265, "end_pos": 273, "type": "DATASET", "confidence": 0.8841454982757568}]}, {"text": "For completeness' sake, however, does provide all-entity news scores in parentheses for those systems that happened to generate the full set of MUC-6 entities.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.81201171875}]}], "tableCaptions": [{"text": " Table 3: Relative distribution of entity types", "labels": [], "entities": [{"text": "Relative distribution", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7728931903839111}]}]}