{"title": [{"text": "Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.8862530787785848}]}], "abstractContent": [{"text": "We propose three new features for MT evaluation: source-sentence constrained n-gram precision, source-sentence reordering metrics, and discriminative un-igram precision, as well as a method of learning linear feature weights to directly maximize correlation with human judgments.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9669719636440277}]}, {"text": "By aligning both the hypothesis and the reference with the source-language sentence, we achieve better correlation with human judgments than previously proposed metrics.", "labels": [], "entities": []}, {"text": "We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework .", "labels": [], "entities": [{"text": "correlation", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.7931503653526306}]}], "introductionContent": [{"text": "Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations fora given sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7320820093154907}]}, {"text": "The most commonly used metric, BLEU, correlates well overlarge test sets with human judgments (), but does not perform as well on sentence-level evaluation (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9983805418014526}]}, {"text": "Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: \u2022 Metrics based on common loose sequences of MT outputs and references (;).", "labels": [], "entities": []}, {"text": "Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9814221858978271}, {"text": "NIST", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.9415686130523682}]}, {"text": "\u2022 Metrics based on syntactic similarities such as the head-word chain metric (HWCM) ().", "labels": [], "entities": [{"text": "head-word chain metric (HWCM)", "start_pos": 54, "end_pos": 83, "type": "METRIC", "confidence": 0.6537252763907114}]}, {"text": "Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9812259674072266}, {"text": "MT outputs", "start_pos": 201, "end_pos": 211, "type": "TASK", "confidence": 0.9335404634475708}]}, {"text": "\u2022 Metrics based on word alignment between MT outputs and the references ().", "labels": [], "entities": [{"text": "word alignment between MT outputs", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.7074714422225952}]}, {"text": "Such metrics do well inadequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis ().", "labels": [], "entities": [{"text": "fluency evaluation", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7999309599399567}]}, {"text": "\u2022 Combination of metrics based on machine learning.", "labels": [], "entities": []}, {"text": "used SVMs to combine several metrics.", "labels": [], "entities": []}, {"text": "Their method is based on the assumption that higher classification accuracy in discriminating human-from machine-generated translations will yield closer correlation with human judgment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.912594199180603}]}, {"text": "This assumption may not always hold, particularly when classification is difficult.", "labels": [], "entities": [{"text": "classification", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.9785895943641663}]}, {"text": "proposed a log-linear model to combine features, but they only did preliminary experiments based on 2 features.", "labels": [], "entities": []}, {"text": "Following the track of previous work, to improve evaluation performance, one could either propose new metrics, or find more effective ways to combine the metrics.", "labels": [], "entities": []}, {"text": "Much work has been done on computing MT scores based on the pair of MT output/reference, and we aim to investigate whether some other information could be used in the MT evaluation, such as source sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9213858246803284}, {"text": "MT evaluation", "start_pos": 167, "end_pos": 180, "type": "TASK", "confidence": 0.8953398764133453}]}, {"text": "We propose two types of source-sentence related features as well as a feature based on part of speech.", "labels": [], "entities": []}, {"text": "The three new types of feature can be summarized as follows: \u2022 Source-sentence constrained n-gram precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9024057984352112}]}, {"text": "Overlapping n-grams between an MT hypothesis and its references do not necessarily indicate correct translation segments, since they could correspond to different parts of the source sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9694055914878845}]}, {"text": "Thus our constrained n-gram precision counts only overlapping n-grams in MT hypothesis and reference which are aligned to the same words in the source sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8938068747520447}, {"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9221529960632324}]}, {"text": "With the alignment information, we can compare the reorderings of the source sentence in the MT hypothesis and in its references.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9253997206687927}]}, {"text": "Such comparison only considers the aligned positions of the source words in MT hypothesis and references, and thus is oriented towards evaluating the sentence structure.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9607321619987488}]}, {"text": "We divide the normal n-gram precision into many subprecisions according to their part of speech (POS).", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9274945855140686}]}, {"text": "The division gives us flexibility to train the weights of each sub-precision in frameworks such as SVM and Maximum Correlation Training, which will be introduced later.", "labels": [], "entities": []}, {"text": "The motivation behind such differentiation is that different sub-precisions should have different importance in MT evaluation, e.g., subprecision of nouns, verbs, and adjectives should be important for evaluating adequacy, and sub-precision in determiners and conjunctions should mean more in evaluating fluency.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 112, "end_pos": 125, "type": "TASK", "confidence": 0.9641039073467255}]}, {"text": "Along the direction of feature combination, since indirect weight training using SVMs, based on reducing classification error, cannot always yield good performance, we train the weights by directly optimizing the evaluation performance, i.e., maximizing the correlation with the human judgment.", "labels": [], "entities": []}, {"text": "This type of direct optimization is known as Minimum Error Rate Training) in the MT community, and is an essential component in building the stateof-art MT systems.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 45, "end_pos": 72, "type": "METRIC", "confidence": 0.7834564298391342}, {"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9804601073265076}, {"text": "MT", "start_pos": 153, "end_pos": 155, "type": "TASK", "confidence": 0.9782332181930542}]}, {"text": "It would seem logical to apply similar methods to MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9788689613342285}]}, {"text": "What is more, Maximum Correlation Training (MCT) enables us to train the weights based on human fluency judgments and adequacy judgments respectively, and thus makes it possible to make a fluency-oriented or adequacy-oriented metric.", "labels": [], "entities": []}, {"text": "It surpasses previous MT metrics' approach, where a a single metric evaluates both fluency and adequacy.", "labels": [], "entities": [{"text": "MT metrics'", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8981287479400635}]}, {"text": "The rest of the paper is organized as follows: Section 2 gives a brief recap of n-gram precision-based metrics and introduces our three extensions to them; Section 3 introduces MCT for MT evaluation; Section 4 describes the experimental results, and Section 5 gives our conclusion.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 185, "end_pos": 198, "type": "TASK", "confidence": 0.9587250649929047}]}], "datasetContent": [{"text": "Since our source-sentence constrained n-gram precision and discriminative unigram precision are both derived from the normal n-gram precision, it is worth describing the original n-gram precision metric, BLEU ().", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8054491877555847}, {"text": "discriminative unigram precision", "start_pos": 59, "end_pos": 91, "type": "METRIC", "confidence": 0.5528767804304758}, {"text": "BLEU", "start_pos": 204, "end_pos": 208, "type": "METRIC", "confidence": 0.9966632723808289}]}, {"text": "For every MT hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9858346581459045}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9987038373947144}]}, {"text": "The formula for computing BLEU is shown below: where C denotes the set of MT hypotheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9956865906715393}]}, {"text": "Count clip (ngram) denotes the clipped number of n-grams in the candidates which also appear in the references.", "labels": [], "entities": [{"text": "Count clip (ngram)", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9667497277259827}]}, {"text": "BP in the above formula denotes the brevity penalty, which is set to 1 if the accumulated length of the MT outputs is longer than the arithmetic mean of the accumulated length of the references, and otherwise is set to the ratio of the two.", "labels": [], "entities": [{"text": "BP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9878184795379639}, {"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.862324059009552}]}, {"text": "For sentence-level evaluation with BLEU, we compute the score based on each pair of MT hypothesis/reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9963712692260742}]}, {"text": "Later approaches, as described in Section 1, use different ways to manipulate the morphological similarity between the MT hypothesis and its references.", "labels": [], "entities": [{"text": "MT", "start_pos": 119, "end_pos": 121, "type": "TASK", "confidence": 0.9718471169471741}]}, {"text": "Most of them, except NIST, consider the words in MT hypothesis as the same, i.e., as long as the words in MT hypothesis appear in the references, they make no difference to the metrics.", "labels": [], "entities": [{"text": "NIST", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.945466935634613}, {"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.8868764638900757}]}, {"text": "1 NIST computes the n-grams weights as the logarithm of the ratio of the n-gram frequency and its one word lower n-gram frequency.", "labels": [], "entities": [{"text": "NIST", "start_pos": 2, "end_pos": 6, "type": "DATASET", "confidence": 0.8742003440856934}]}, {"text": "From our experiments, NIST is not generally better than BLEU, and the reason, we conjecture, is that it differentiates the n-grams too much and the frequency estimated upon the evaluation corpus is not always reliable.", "labels": [], "entities": [{"text": "NIST", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.4978468418121338}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9982495903968811}]}, {"text": "In this section we will describe two other strategies for differentiating the n-grams, one of which uses the alignments with the source sentence as a further constraint, while the other differentiates the n-gram precisions according to POS.", "labels": [], "entities": [{"text": "POS", "start_pos": 236, "end_pos": 239, "type": "DATASET", "confidence": 0.6316245794296265}]}, {"text": "Maximum Correlation Training (MCT) is an instance of the general approach of directly optimizing the objective function by which a model will ultimately be evaluated.", "labels": [], "entities": [{"text": "Maximum Correlation Training (MCT)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.608900398015976}]}, {"text": "In our case, the model is the linear combination of the component metrics, the parameters are the weights for each component metric, and the objective function is the Pearson's correlation of the combined metric and the human judgments.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 167, "end_pos": 188, "type": "METRIC", "confidence": 0.804429809252421}]}, {"text": "The reason to use the linear combination of the metrics is that the component metrics are usually of the same or similar order of magnitude, and it makes the optimization problem easy to solve.", "labels": [], "entities": []}, {"text": "Using w to denote the weights, and m to denote the component metrics, the combined metric x is computed as: Using hi and x(w) i denote the human judgment and combined metric fora sentence respectively, and N denote the number of sentences in the evaluation set, the objective function is then computed as: Now our task is to find the weights for each component metric so that the correlation of the combined metric with the human judgment is maximized.", "labels": [], "entities": []}, {"text": "It can be formulated as: The function P earson(X(w), H) is differentiable with respect to the vector w, and we compute this derivative analytically and perform gradient ascent.", "labels": [], "entities": [{"text": "P", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9709279537200928}]}, {"text": "Our objective function not always convex (one can easily create a non-convex function by setting the human judgments and individual metrics to some particular value).", "labels": [], "entities": []}, {"text": "Thus there is no guarantee that, starting from a random w, we will get the globally optimal w using optimization techniques such as gradient ascent.", "labels": [], "entities": [{"text": "gradient ascent", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7115980982780457}]}, {"text": "The easiest way to avoid ending up with a bad local optimum to run gradient ascent by starting from different random points.", "labels": [], "entities": [{"text": "gradient ascent", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8032930791378021}]}, {"text": "In our experiments, the difference in each run is very small, i.e., by starting from different random initial values of w, we end up with, not the same, but very similar values for Pearson's correlation.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 181, "end_pos": 202, "type": "METRIC", "confidence": 0.8165115714073181}]}, {"text": "Experiments were conducted to evaluate the performance of the new metrics proposed in this paper, as well as the MCT combination framework.", "labels": [], "entities": []}, {"text": "The data for the experiments are from the MT evaluation workshop at ACL05.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.8233569264411926}, {"text": "ACL05", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.6616672873497009}]}, {"text": "There are seven sets of MT outputs (E09 E11 E12 E14 E15 E17 E22), each of which contains 919 English sentences translated from the same set of Chinese sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9550086855888367}, {"text": "E09 E11 E12 E14 E15 E17 E22", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.690438232251576}]}, {"text": "There are four references (E01, E02, E03, E04) and two sets of human scores for each MT hypothesis.", "labels": [], "entities": [{"text": "MT hypothesis", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.9101583957672119}]}, {"text": "Each human score set contains a fluency and an adequacy score, both of which range from 1 to 5.", "labels": [], "entities": []}, {"text": "We create a set of overall human scores by averaging the human fluency and adequacy scores.", "labels": [], "entities": []}, {"text": "For evaluating the automatic metrics, we compute the Pearson's correlation of the automatic scores and the averaged human scores (over the two sets of available human scores), for overall score, fluency, and adequacy.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.9533118804295858}]}, {"text": "The alignment between the source sentences and the MT hypothesis/references is computed by GIZA++, which is trained on the combined corpus of the evaluation data and a parallel corpus of Chinese-English newswire text.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9679294228553772}, {"text": "GIZA", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.7890340089797974}]}, {"text": "The parallel newswire corpus contains around 75,000 sentence pairs, 2,600,000 English words and 2,200,000 Chinese words.", "labels": [], "entities": []}, {"text": "The stochastic word mapping is trained on a FrenchEnglish parallel corpus containing 700,000 sentence pairs, and, following, we only keep the top 100 most similar words for each English word.", "labels": [], "entities": [{"text": "FrenchEnglish parallel corpus", "start_pos": 44, "end_pos": 73, "type": "DATASET", "confidence": 0.9309331178665161}]}], "tableCaptions": [{"text": " Table 1. As a comparison, we also show the re- sults of BLEU, NIST, METEOR, ROUGE, WER,  and HWCM. For METEOR and ROUGE, WORD- NET and PORTER-STEMMER are enabled, and for  SIA, the decay factor is set to 0.6. The number  in brackets, for BLEU, shows the n-gram length it  counts up to, and for SSCN, shows the length of the  n-gram it uses. In the table, the top 3 results in each  column are marked bold and the best result is also  underlined. The results show that the SSCN2 met- rics are better than the SSCN1 metrics in adequacy  and overall score. This is understandable since what  SSCN metrics need is which words in the source  sentence are aligned to an n-gram in the MT hy- pothesis/references. This is directly modeled in the  alignment used in SSCN2. Though we could also  get such information from the reverse alignment, as  in SSCN1, it is rather an indirect way and could con- tain more noise. It is interesting that SSCN1 gets  better fluency evaluation results than SSCN2. The  SSCN metrics with the unioned constraint, SSCN u,  by combining the strength of SSCN1 and SSCN2,  get even better results in all three aspects. We can  see that SSCN metrics, even without stochastic word  mapping, get significantly better results than their  relatives, BLEU, which indicates the source sen- tence constraints do make a difference. SSCN2 and  SSCN u are also competitive to the state-of-art MT  metrics such as METEOR and SIA. The best SSCN  metric, pSSCN u(2), achieves the best performance  among all the testing metrics in overall and ade- quacy, and the second best performance in fluency,  which is just a little bit worse than the best fluency  metric SIA.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9963662624359131}, {"text": "METEOR", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.7795627117156982}, {"text": "ROUGE", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9266397356987}, {"text": "WER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9714233875274658}, {"text": "SIA", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9698735475540161}, {"text": "BLEU", "start_pos": 239, "end_pos": 243, "type": "METRIC", "confidence": 0.975153386592865}, {"text": "BLEU", "start_pos": 1267, "end_pos": 1271, "type": "METRIC", "confidence": 0.9975996613502502}]}]}