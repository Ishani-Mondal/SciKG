{"title": [{"text": "An Information Retrieval Approach to Sense Ranking", "labels": [], "entities": [{"text": "Information Retrieval Approach to Sense", "start_pos": 3, "end_pos": 42, "type": "TASK", "confidence": 0.70633345246315}]}], "abstractContent": [{"text": "In word sense disambiguation, choosing the most frequent sense for an ambiguous word is a powerful heuristic.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.7225632965564728}]}, {"text": "However, its usefulness is restricted by the availability of sense-annotated data.", "labels": [], "entities": []}, {"text": "In this paper, we propose an information retrieval-based method for sense ranking that does not require annotated data.", "labels": [], "entities": [{"text": "sense ranking", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.9496884047985077}]}, {"text": "The method queries an information retrieval engine to estimate the degree of association between a word and its sense descriptions.", "labels": [], "entities": []}, {"text": "Experiments on the Senseval test materials yield state-of-the-art performance.", "labels": [], "entities": [{"text": "Senseval test materials", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.9058835307757059}]}, {"text": "We also show that the estimated sense frequencies correlate reliably with native speakers' intuitions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7792974511782328}]}, {"text": "Examples include paraphrase acquisition, discourse parsing, or metonymy resolution.", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.9213274717330933}, {"text": "discourse parsing", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7395224124193192}, {"text": "metonymy resolution", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.780857652425766}]}, {"text": "Applications such as machine translation) and information retrieval () have also been shown to benefit from WSD.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8169655501842499}, {"text": "information retrieval", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.8380626738071442}, {"text": "WSD", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.953393816947937}]}, {"text": "Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods.", "labels": [], "entities": [{"text": "computational treatment of sense ambiguity", "start_pos": 108, "end_pos": 150, "type": "TASK", "confidence": 0.7755396366119385}]}, {"text": "Most accurate WSD systems to date are supervised and rely on the availability of training data (see and the references therein).", "labels": [], "entities": [{"text": "WSD", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9744654893875122}]}, {"text": "Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9978284239768982}]}, {"text": "Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth \"the first sense heuristic\"), despite taking local context into account.", "labels": [], "entities": []}, {"text": "One reason for this is the highly skewed distribution of word senses.", "labels": [], "entities": []}, {"text": "A large number of frequent content words is often associated with only one dominant sense.", "labels": [], "entities": []}, {"text": "Obtaining the first sense via annotation is obviously costly and time consuming.", "labels": [], "entities": []}, {"text": "Sense annotated corpora are not readily available for different languages or indeed sense inventories.", "labels": [], "entities": []}, {"text": "Moreover, a word's dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard).", "labels": [], "entities": []}, {"text": "It is therefore not surprising that recent work) attempts to alleviate the annotation bottleneck by inferring the first sense automatically from raw text.", "labels": [], "entities": []}, {"text": "Automatically acquired first senses will undoubtedly be noisy when compared to human annotations.", "labels": [], "entities": []}, {"text": "Nevertheless, they can be usefully employed in two important tasks: (a) to create preliminary annotations, thus supporting the \"annotate automatically, correct manually\" methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with supervised WSD methods that take context into account; for instance, such methods could default to the dominant sense for unseen words or words with uninformative contexts.", "labels": [], "entities": [{"text": "Penn Treebank project", "start_pos": 228, "end_pos": 249, "type": "DATASET", "confidence": 0.9891297419865926}]}, {"text": "This paper focuses on a knowledge-lean sense ranking method that exploits a sense inventory like WordNet and corpus data to automatically induce dominant senses.", "labels": [], "entities": [{"text": "knowledge-lean sense ranking", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.581846296787262}, {"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9640017151832581}]}, {"text": "The proposed method infers the associations between words and sense descriptions automatically by querying an IR engine whose index terms have been compiled from the corpus of interest.", "labels": [], "entities": []}, {"text": "The approach is inexpensive, languageindependent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions.", "labels": [], "entities": []}, {"text": "We evaluate our method on two tasks.", "labels": [], "entities": []}, {"text": "First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 () and) data sets.", "labels": [], "entities": []}, {"text": "Second, we simulate native speakers' intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data.", "labels": [], "entities": []}, {"text": "In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art.", "labels": [], "entities": []}, {"text": "In the following section, we provide an overview of existing work on sense ranking.", "labels": [], "entities": [{"text": "sense ranking", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.8455933034420013}]}, {"text": "In Section 3, we introduce our IR-based method, and describe several sense ranking models.", "labels": [], "entities": [{"text": "IR-based", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.9400352835655212}]}, {"text": "In Section 4, we present our results.", "labels": [], "entities": []}, {"text": "Discussion of our results and future work conclude the paper (Section 5). were the first to propose a computational model for acquiring dominant senses from text corpora.", "labels": [], "entities": []}, {"text": "Key in their approach is the observation that distributionally similar neighbors often provide cues about a word's senses.", "labels": [], "entities": []}, {"text": "The model quantifies the degree of similarity between a word's sense descriptions and its closest neighbors, thus delivering a ranking over senses where the most similar sense is intuitively the dominant sense.", "labels": [], "entities": []}, {"text": "Their method exploits two notions of similarity, distributional and semantic.", "labels": [], "entities": []}, {"text": "Distributionally similar words are acquired from the British National Corpus using an information-theoretic similarity measure) operating over dependency relations (e.g., verb-subject, verb-object).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 53, "end_pos": 76, "type": "DATASET", "confidence": 0.9479275941848755}]}, {"text": "The latter are obtained from the output of parser.", "labels": [], "entities": []}, {"text": "The semantic similarity between neighbors and senses is measured using a manually crafted taxonomy such as WordNet (see Budanitsky and Hirst 2001 for an overview of WordNet-based similarity measures).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9686310291290283}]}, {"text": "propose an algorithm for inferring dominant senses without relying on distributionally similar neighbors.", "labels": [], "entities": []}, {"text": "Their approach capitalizes on the collocational nature of semantically related words.", "labels": [], "entities": []}, {"text": "Assuming a coarsegrained sense inventory (e.g., the Macquarie Thesaurus), it first creates a matrix whose columns represent all categories (senses) c 1 . .", "labels": [], "entities": [{"text": "Macquarie Thesaurus", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.9842511415481567}]}, {"text": "c n in the inventory and rows the ambiguous target words w 1 . .", "labels": [], "entities": []}, {"text": "w m ; the matrix cells record the number of times a target word ti co-occurs with category c j within a window of size s.", "labels": [], "entities": []}, {"text": "Using an appropriate statistical test, they estimate the relative strength of association between an ambiguous word and each of its senses.", "labels": [], "entities": []}, {"text": "The sense with the highest association is the predominant sense.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were driven by three questions:: Results for Senseval-2 data by model instantiation", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for Senseval-2 data by model in- stantiation", "labels": [], "entities": []}, {"text": " Table 2: Results of best model (High, +Norm,  \u2212Hyp) for Senseval-2 data by part of speech  (  *  : sig. diff. from BaseR, # : sig. diff. from BaseS;  p < 0.01 using \u03c7 2 test)", "labels": [], "entities": []}, {"text": " Table 3: Results for 10% of Senseval-2 data by  model instantiation", "labels": [], "entities": [{"text": "Senseval-2 data", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.7919443249702454}]}, {"text": " Table 4: Comparison of results on Senseval-3 data  (  *  : sig. diff. from BaseR, # : sig. diff. from BaseS,   \u2020 : sig. diff. from McCarthy, $ : sig. diff. from IR- Model,  \u2021 : sig. diff. from SemCor; p < 0.01 using  \u03c7 2 test)", "labels": [], "entities": []}, {"text": " Table 5: Results of best model (High, +Norm,  \u2212Hyp) for Senseval-3 data by part of speech  (  *  : sig. diff. from BaseR, # : sig. diff. from BaseS;  p < 0.01 using \u03c7 2 test)", "labels": [], "entities": []}, {"text": " Table 6: Meaning frequencies for act and answer;  normative data from", "labels": [], "entities": []}]}