{"title": [{"text": "Data-Driven Graph Construction for Semi-Supervised Graph-Based Learning in NLP", "labels": [], "entities": [{"text": "Graph Construction", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.6653571724891663}]}], "abstractContent": [{"text": "Graph-based semi-supervised learning has recently emerged as a promising approach to data-sparse learning problems in natural language processing.", "labels": [], "entities": []}, {"text": "All graph-based algorithms rely on a graph that jointly represents labeled and unlabeled data points.", "labels": [], "entities": []}, {"text": "The problem of how to best construct this graph remains largely unsolved.", "labels": [], "entities": []}, {"text": "In this paper we introduce a data-driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classifier.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7701130211353302}]}, {"text": "We apply this technique in the framework of label propagation and evaluate it on two different classification tasks, a multi-class lexicon acquisition task and a word sense disambiguation task.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7869839370250702}, {"text": "multi-class lexicon acquisition task", "start_pos": 119, "end_pos": 155, "type": "TASK", "confidence": 0.7079436033964157}, {"text": "word sense disambiguation", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.6787894169489542}]}, {"text": "Significant improvements are demonstrated over both label propagation using conventional graph construction and state-of-the-art supervised classifiers.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.8282259702682495}]}], "introductionContent": [{"text": "Natural Language Processing (NLP) applications benefit from the availability of large amounts of annotated data.", "labels": [], "entities": []}, {"text": "However, such data is often scarce, particularly for non-mainstream languages.", "labels": [], "entities": []}, {"text": "Semisupervised learning addresses this problem by combining large amounts of unlabeled data with a small set of labeled data in order to learn a classification function.", "labels": [], "entities": []}, {"text": "One class of semi-supervised learning algorithms that has recently attracted increased interest is graph-based learning.", "labels": [], "entities": []}, {"text": "Graph-based techniques represent labeled and unlabeled data points as nodes in a graph with weighted edges encoding the similarity of pairs of samples.", "labels": [], "entities": []}, {"text": "Various techniques are then available for transferring class labels from the labeled to the unlabeled data points.", "labels": [], "entities": []}, {"text": "These approaches have shown good performance in cases where the data is characterized by an underlying manifold structure and samples are judged to be similar by local similarity measures.", "labels": [], "entities": []}, {"text": "However, the question of how to best construct the graph forming the basis of the learning procedure is still an underinvestigated research problem.", "labels": [], "entities": []}, {"text": "NLP learning tasks present additional problems since they often rely on discrete or heterogeneous feature spaces for which standard similarity measures (such as Euclidean or cosine distance) are suboptimal.", "labels": [], "entities": [{"text": "NLP learning tasks", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8588801225026449}]}, {"text": "We propose a two-pass data-driven technique for graph construction in the framework of label propagation ().", "labels": [], "entities": [{"text": "graph construction", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.763683944940567}, {"text": "label propagation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7343422770500183}]}, {"text": "First, we use a supervised classifier trained on the labeled subset to transform the initial feature space (consisting of e.g. lexical, contextual, or syntactic features) into a continuous representation in the form of soft label predictions.", "labels": [], "entities": []}, {"text": "This representation is then used as a basis for measuring similarity among samples that determines the structure of the graph used for the second, semisupervised learning step.", "labels": [], "entities": []}, {"text": "It is important to note that, rather than simply cascading the supervised and the semi-supervised learner, we optimize the combination with respect to the properties required of the graph.", "labels": [], "entities": []}, {"text": "We present several techniques for such optimization, including regularization of the first-pass classifier, biasing by class priors, and linear combi-nation of classifier predictions with known features.", "labels": [], "entities": []}, {"text": "The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 72, "end_pos": 104, "type": "DATASET", "confidence": 0.9634708506720406}, {"text": "SENSEVAL-3 word sense disambiguation task", "start_pos": 117, "end_pos": 158, "type": "TASK", "confidence": 0.8351964235305787}]}, {"text": "In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers).", "labels": [], "entities": [{"text": "label propagation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.796878457069397}]}], "datasetContent": [{"text": "For both tasks we compare the performance of a supervised classifier, label propagation using the standard input features and either Euclidean or cosine distance, and LP using the output from a first-pass supervised classifier.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8045763075351715}, {"text": "LP", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9639912843704224}]}], "tableCaptions": [{"text": " Table 1: Accuracy results of neural classification (NN), LP with discrete features (LP), and combined (NN+LP), over 5 random  samplings of 5000, 10000, and 15000 labeled words in the WSJ lexicon acquisition task. S(G) is the smoothness of the graph", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9697438478469849}, {"text": "WSJ lexicon acquisition task", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.7230355739593506}]}, {"text": " Table 2: Accuracy results of other published systems on  SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly com- parably to our system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9783986210823059}]}, {"text": " Table 3: Accuracy results of support vector machine (SVM), label propagation over discrete features (LP), and label propagation  over SVM outputs (SVM+LP), each trained with 25%, 50%, 75% (5 random samplings each), and 100% of the train set. The  improvements of SVM+LP are significant over LP in the 75% and 100% cases. S(G) is the graph smoothness", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9882630705833435}, {"text": "label propagation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.74113929271698}, {"text": "label propagation", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.6914173811674118}]}]}