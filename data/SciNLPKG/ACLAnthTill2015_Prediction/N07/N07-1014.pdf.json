{"title": [{"text": "A Random Text Model for the Generation of Statistical Language Invariants", "labels": [], "entities": [{"text": "Generation of Statistical Language Invariants", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7512732565402984}]}], "abstractContent": [{"text": "A novel random text generation model is introduced.", "labels": [], "entities": [{"text": "random text generation", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.6611032485961914}]}, {"text": "Unlike in previous random text models, that mainly aim at producing a Zipfian distribution of word frequencies, our model also takes the properties of neighboring co-occurrence into account and introduces the notion of sentences in random text.", "labels": [], "entities": []}, {"text": "After pointing out the deficiencies of related models, we provide a generation process that takes neither the Zipfian distribution on word frequencies nor the small-world structure of the neighboring co-occurrence graph as a constraint.", "labels": [], "entities": []}, {"text": "Nevertheless, these distributions emerge in the process.", "labels": [], "entities": []}, {"text": "The distributions obtained with the random generation model are compared to a sample of natural language data, showing high agreement also on word length and sentence length.", "labels": [], "entities": [{"text": "agreement", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9649844765663147}]}, {"text": "This work proposes a plausible model for the emergence of large-scale characteristics of language without assuming a grammar or semantics.", "labels": [], "entities": []}], "introductionContent": [{"text": "G. K. discovered that if all words in a sample of natural language are arranged in decreasing order of frequency, then the relation between a word's frequency and its rank in the list follows a power-law.", "labels": [], "entities": []}, {"text": "Since then, a significant amount of research in the area of quantitative linguistics has been devoted to the question how this property emerges and what kind of processes generate such Zipfian distributions.", "labels": [], "entities": []}, {"text": "The relation between the frequency of a word at rank rand its rank is given by f(r) \u221d r -z , where z is the exponent of the power-law that corresponds to the slope of the curve in a log plot (cf.).", "labels": [], "entities": []}, {"text": "The exponent z was assumed to be exactly 1 by Zipf; in natural language data, also slightly differing exponents in the range of about 0.7 to 1.2 are observed (cf..", "labels": [], "entities": []}, {"text": "B. provided a formula with a closer approximation of the frequency distributions in language data, noticing that Zipf's law holds only for the medium range of ranks, whereas the curve is flatter for very frequent words and steeper for high ranks.", "labels": [], "entities": []}, {"text": "He also provided a word generation model that produces random words of arbitrary average length in the following way: With a probability w, a word separator is generated at each step, with probability (1-w)/N, a letter from an alphabet of size N is generated, each letter having the same probability.", "labels": [], "entities": [{"text": "word generation", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.752426415681839}]}, {"text": "This is sometimes called the \"monkey at the typewriter\".", "labels": [], "entities": []}, {"text": "The frequency distribution follows a power-law for long streams of words, yet the equiprobability of letters causes the plot to show a step-wise rather than a smooth behavior, as examined by Ferrer i, cf. figure 2.", "labels": [], "entities": []}, {"text": "In the same study, a smooth rank distribution could be obtained by setting the letter probabilities according to letter frequencies in a natural language text.", "labels": [], "entities": []}, {"text": "But the question of how these letter probabilities emerge remains unanswered.", "labels": [], "entities": []}, {"text": "Another random text model was given by, which does not take an alphabet of single letters into consideration.", "labels": [], "entities": []}, {"text": "Instead, at each time step, a previously unseen new word is added to the stream with a probability a, whereas with probability (1-a), the next word is chosen amongst the words at previous positions.", "labels": [], "entities": []}, {"text": "As words with higher frequency in the already generated stream have a higher probability of being added again, this imposes a strong competition among different words, resulting in a frequency distribution that follows a power-law with exponent z=(1-a).", "labels": [], "entities": []}, {"text": "This was taken up by, who slightly modify Simon's model.", "labels": [], "entities": []}, {"text": "They introduce sublinear vocabulary growth by additionally making the new word probability dependent on the time step.", "labels": [], "entities": [{"text": "vocabulary growth", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.691882386803627}]}, {"text": "Furthermore, they introduce a threshold on the maximal probability a previously seen word can be assigned to for generation, being able to modify the exponent z as well as to model the flatter curve for high frequency words.", "labels": [], "entities": []}, {"text": "In (), Zipf's law is extended to words and phrases, showing its validity for syllable-class based languages when conducting the extension.", "labels": [], "entities": []}, {"text": "Neither the Mandelbrot nor the Simon generation model take the sequence of words into account.", "labels": [], "entities": []}, {"text": "Simon treats the previously generated stream as a bag of words, and Mandelbrot does not consider the previous stream at all.", "labels": [], "entities": []}, {"text": "This is certainly an over-simplification, as natural language exhibits structural properties within sentences and texts that are not grasped by bags of words.", "labels": [], "entities": []}, {"text": "The work by is, to our knowledge, the only study to date that takes the word order into account when generating random text.", "labels": [], "entities": []}, {"text": "They show that a 2-parameter Markov process gives rise to a stationary distribution that exhibits the word frequency distribution and the letter frequency distribution characteristics of natural language.", "labels": [], "entities": []}, {"text": "However, the Markov process is initialized such that any state has exactly two successor states, which means that after each word, only two other following words are possible.", "labels": [], "entities": []}, {"text": "This certainly does not reflect natural language properties, wherein fact successor frequencies of words follow a power-law and more successors can be observed for more frequent words.", "labels": [], "entities": []}, {"text": "But even when allowing a more realistic number of successor states, the transition probabilities of a Markov model need to be initialized a priori in a sensible way.", "labels": [], "entities": []}, {"text": "Further, the fixed number of states does not allow for infinite vocabulary.", "labels": [], "entities": []}, {"text": "In the next section we provide a model that does not suffer from all these limitations.", "labels": [], "entities": []}], "datasetContent": [{"text": "To measure agreement with our BNC sample, we generated random text with the sentence generator using w=0.4 and N=26 to match the English average word length and setting s to 0.08 for reaching a comparable sentence length.", "labels": [], "entities": [{"text": "BNC sample", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.8742822408676147}]}, {"text": "The first 50,000 sentences were skipped to reach a relatively stable sentence length throughout the sample.", "labels": [], "entities": []}, {"text": "To make the samples comparable, we used 1 million words totaling 125,345 sentences with an average sentence length of 7.977.", "labels": [], "entities": []}], "tableCaptions": []}