{"title": [{"text": "A Systematic Exploration of the Feature Space for Relation Extraction", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.9451353251934052}]}], "abstractContent": [{"text": "Relation extraction is the task of finding semantic relations between entities from text.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9606436192989349}]}, {"text": "The state-of-the-art methods for relation extraction are mostly based on statistical learning, and thus all have to deal with feature selection, which can significantly affect the classification performance.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.9721166789531708}]}, {"text": "In this paper, we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.900810956954956}]}, {"text": "We present a general definition of feature spaces based on a graphic representation of relation instances, and explore three different representations of relation instances and features of different complexities within this framework.", "labels": [], "entities": []}, {"text": "Our experiments show that using only basic unit features is generally sufficient to achieve state-of-the-art performance, while over-inclusion of complex features may hurt the performance.", "labels": [], "entities": []}, {"text": "A combination of features of different levels of complexity and from different sentence representations, coupled with task-oriented feature pruning , gives the best performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important information extraction task is relation extraction, whose goal is to detect and characterize semantic relations between entities in text.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7705047726631165}, {"text": "relation extraction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.865045428276062}, {"text": "characterize semantic relations between entities in text", "start_pos": 93, "end_pos": 149, "type": "TASK", "confidence": 0.7923562441553388}]}, {"text": "For example, the text fragment \"hundreds of Palestinians converged on the square\" contains the located relation between the Person entity \"hundreds of Palestinians\" and the Bounded-Area entity \"the square\".", "labels": [], "entities": []}, {"text": "Relation extraction has applications in many domains, including finding affiliation relations from web pages and finding protein-protein interactions from biomedical literature.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9681232273578644}]}, {"text": "Recent studies on relation extraction have shown the advantages of discriminative model-based statistical machine learning approach to this problem.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9619393348693848}]}, {"text": "There are generally two lines of work following this approach.", "labels": [], "entities": []}, {"text": "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing) . The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.6824970692396164}]}, {"text": "Of particular interest among the various kernels proposed are the convolution kernels (), because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature.", "labels": [], "entities": []}, {"text": "Apart from their computational efficiency, convolution kernels also implicitly correspond to some feature space.", "labels": [], "entities": []}, {"text": "Therefore, both lines of work rely on an appropriately de-fined set of features.", "labels": [], "entities": []}, {"text": "As in any learning problem, the choice of features can affect the performance significantly.", "labels": [], "entities": []}, {"text": "Despite the importance of feature selection, there has not been any systematic exploration of the feature space for relation extraction, and the choices of features in existing work are somewhat arbitrary.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7005993276834488}, {"text": "relation extraction", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8759559690952301}]}, {"text": "In this paper, we conduct a systematic study of the feature space for relation extraction, and evaluate the effectiveness of different feature subspaces.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8921631276607513}]}, {"text": "First, based on previous studies, we want to identify and characterize the types of features that are potentially useful for relation extraction, and define a relatively complete and structured feature space that can be systematically explored.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.920849472284317}]}, {"text": "Second, we want to compare the effectiveness of different features.", "labels": [], "entities": []}, {"text": "Such a study can guide us to choose the most effective feature set for relation extraction, or to design convolution kernels in the most effective way.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8351326882839203}]}, {"text": "We propose and define a unified graphic representation of the feature space, and experiment with three feature subspaces, corresponding to sequences, syntactic parse trees and dependency parse trees.", "labels": [], "entities": []}, {"text": "Experiment results show that each subspace is effective by itself, with the syntactic parse tree subspace being the most effective.", "labels": [], "entities": []}, {"text": "Combining the three subspaces does not generate much improvement.", "labels": [], "entities": []}, {"text": "Within each feature subspace, using only the basic unit features can already give reasonably good performance.", "labels": [], "entities": []}, {"text": "Adding more complex features may not improve the performance much or may even hurt the performance.", "labels": [], "entities": []}, {"text": "Task-oriented heuristics can be used to prune the feature space, and when appropriately done, can improve the performance.", "labels": [], "entities": []}, {"text": "A combination of features of different levels of complexity and from different sentence representations, coupled with task-oriented feature pruning, gives the best performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the data set from ACE (Automatic Content Extraction) 2004 evaluation to conduct our experiments.", "labels": [], "entities": [{"text": "ACE (Automatic Content Extraction) 2004 evaluation", "start_pos": 26, "end_pos": 76, "type": "TASK", "confidence": 0.5993796437978745}]}, {"text": "This corpus defines 7 types of relations: Physical, Personal / Social, Empolyment / Memebership / Subsidiary, Agent-Artifact, PER / ORG Affiliation, GPE Affiliation and Discourse.", "labels": [], "entities": [{"text": "GPE Affiliation", "start_pos": 149, "end_pos": 164, "type": "METRIC", "confidence": 0.5371608287096024}]}, {"text": "We used Collins parser to parse the sentences in the corpus because Collins parser gives us the head of each syntactic category, which allows us to transform the syntactic parse trees into dependency trees.", "labels": [], "entities": []}, {"text": "We discarded sentences that could not be parsed by Collins parser.", "labels": [], "entities": []}, {"text": "The candidate relation instances were generated by considering all pairs of entities that occur in the same sentence.", "labels": [], "entities": []}, {"text": "We obtained 48625 candidate relation instances in total, among which 4296 instances were positive.", "labels": [], "entities": []}, {"text": "As inmost existing work, instead of using the entire sentence, we used only the sequence of tokens that are inside the minimum complete subtree covering the two arguments.", "labels": [], "entities": []}, {"text": "Presumably, tokens outside of this subtree are not so relevant to the task.", "labels": [], "entities": []}, {"text": "In our graphic representation of relation instances, the attribute set fora token node includes the token itself, its POS tag, and entity type, entity subtype and entity mention type when applicable.", "labels": [], "entities": []}, {"text": "The attribute set fora syntactic category node includes only the syntactic tag.", "labels": [], "entities": []}, {"text": "We used both maximum entropy classifier and SVM for all experiments.", "labels": [], "entities": []}, {"text": "We adopted one vs. others strategy for the multi-class classification problem.", "labels": [], "entities": [{"text": "multi-class classification problem", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.8533057967821757}]}, {"text": "In all experiments, the performance shown was based on 5-fold cross validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison among the three feature sub- spaces and the effect of including larger features.", "labels": [], "entities": []}, {"text": " Table 2: The effect of combining the three feature  subspaces.", "labels": [], "entities": []}, {"text": " Table 3: The effect of various heuristic feature prun- ing methods.", "labels": [], "entities": []}]}