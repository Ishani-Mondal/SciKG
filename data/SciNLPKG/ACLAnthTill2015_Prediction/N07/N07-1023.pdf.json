{"title": [{"text": "Lexicalized Markov Grammars for Sentence Compression *", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.9333267509937286}]}], "abstractContent": [{"text": "We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.76690474152565}]}, {"text": "We define a head-driven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions.", "labels": [], "entities": []}, {"text": "We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora.", "labels": [], "entities": []}, {"text": "Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression addresses the problem of removing words or phrases that are not necessary in the generated output of, for instance, summarization and question answering systems.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9293018877506256}, {"text": "summarization and question answering", "start_pos": 137, "end_pos": 173, "type": "TASK", "confidence": 0.6571475118398666}]}, {"text": "Given the need to ensure grammatical sentences, a number of researchers have used syntax-directed approaches that perform transformations on the output of syntactic parsers.", "labels": [], "entities": []}, {"text": "Some of them) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) * This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023.", "labels": [], "entities": []}, {"text": "Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA.", "labels": [], "entities": [{"text": "DARPA", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.5948628187179565}]}, {"text": "() to extract compression rules from aligned Penn Treebank (PTB) trees.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) trees", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.9638333121935526}]}, {"text": "While their approach proved successful, their reliance on standard maximum likelihood estimators for SCFG productions results in considerable sparseness issues, especially given the relative flat structure of PTB trees; in practice, many SCFG productions are seen only once.", "labels": [], "entities": []}, {"text": "This problem is exacerbated for the compression task, which has only scarce training material available.", "labels": [], "entities": [{"text": "compression task", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.9192711412906647}]}, {"text": "In this paper, we present a head-driven Markovization of SCFG compression rules, an approach that was successfully used in syntactic parsing to alleviate issues intrinsic to relative frequency estimation of treebank productions.", "labels": [], "entities": [{"text": "SCFG compression", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7870927155017853}, {"text": "syntactic parsing", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.7362702786922455}, {"text": "relative frequency estimation of treebank productions", "start_pos": 174, "end_pos": 227, "type": "TASK", "confidence": 0.716325968503952}]}, {"text": "Markovization for sentence compression provides several benefits, including the ability to condition deletions on a flexible amount of syntactic context, to treat head-modifier dependencies independently, and to lexicalize SCFG productions.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7325124591588974}]}, {"text": "Another part of our effort focuses on better alignment models for extracting SCFG compression rules from parallel data, and to improve upon), who could only exploit 1.75% of the Ziff-Davis corpus because of stringent assumptions about human abstractive behavior.", "labels": [], "entities": [{"text": "SCFG compression rules from parallel data", "start_pos": 77, "end_pos": 118, "type": "TASK", "confidence": 0.8318942288557688}]}, {"text": "To alleviate their restrictions, we rely on a robust approach for aligning trees of arbitrary document-abstract sentence pairs.", "labels": [], "entities": []}, {"text": "After accounting for sentence pairs with both substitutions and deletions, we reached a retention of more than 25% of the Ziff-Davis data, which greatly benefited the lexical probabilities incorporated into our Markovized SCFGs.", "labels": [], "entities": [{"text": "retention", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9846151471138}]}, {"text": "Our work provides three main contributions: (1) Our lexicalized head-driven Markovization yields more robust probability estimates, and our compressions outperform) according to automatic and human evaluation.", "labels": [], "entities": []}, {"text": "(2) We provide a comprehensive analysis of the impact of different Markov orders for sentence compression, similarly to a study done for PCFGs (.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.7601760923862457}]}, {"text": "We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the available training resources for syntax-directed sentence compression systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments presented in this section are performed on the Ziff-Davis corpus.", "labels": [], "entities": []}, {"text": "We note first that all probability estimates of our Markovized gramruns in polynomial time.", "labels": [], "entities": [{"text": "Markovized gramruns", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.7178938984870911}]}, {"text": "To determine whether a given one-level tree is an auxiliary, we simply check the following properties: all its leaves but one (the \"foot node\") must be nodes attached to deleted subtrees (e.g., VP and CC in the, and the foot node (VP) must have the same syntactic category as the root node.", "labels": [], "entities": []}, {"text": "Indeed, incorporating lexical dependencies within models trained on data sets as small as 16,000 sentence pairs would be quite futile without incorporating robust smoothing techniques.", "labels": [], "entities": []}, {"text": "Different smoothing techniques were evaluated with our models, and we found that interpolated Witten-Bell discounting was the method that performed best.", "labels": [], "entities": []}, {"text": "We used relative frequency estimates for each of the models presented in Section 2.2 (i.e., p h , pl , pr ), and trained pl separately from pr . We interpolated our most specific models (lexical heads, POS tags, ancestor and sister annotation) with lower-order models.", "labels": [], "entities": []}, {"text": "Automatic evaluation on development sets is performed using word-level classification accuracy, i.e., the number of words correctly classified as being either deleted or not deleted, divided by the total number of words.", "labels": [], "entities": [{"text": "word-level classification", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6480740010738373}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.719672679901123}]}, {"text": "In our first evaluation, we experimented with different horizontal and vertical Markovizations.", "labels": [], "entities": []}, {"text": "First, it appears that vertical annotation is moderately helpful.", "labels": [], "entities": []}, {"text": "It provides gains inaccuracy ranging from .5% to .9% for v = 1 over a simpler models (v = 0), but higher orders (v > 1) have a tendency to decrease performance.", "labels": [], "entities": []}, {"text": "On the other hand, sister annotation of order 1 is much more critical, and provides 4.1% improvement over a simpler model (s = 0, v = 0).", "labels": [], "entities": []}, {"text": "Manual examinations of compression outputs confirmed this analysis: without sister annotation, deletion of punctuation and function words (determiners, coordinate conjunctions, etc.) is often inaccurate, and compressions clearly lack fluency.", "labels": [], "entities": []}, {"text": "This annotation is also helpful for phrasal deletions; for instance, we found that PPs are deleted in 31.4% of cases in Ziff-Davis if they do not immediately follow the head constituent, but this percentage drops to 11.1% for PPs that immediately follow the head.", "labels": [], "entities": []}, {"text": "It seems, however, that increasing sister annotation beyond s > 1 only provide limited improvements.", "labels": [], "entities": []}, {"text": "In our second evaluation reported in, we assessed the usefulness of lexical and POS annotation (setting sand v to 0).", "labels": [], "entities": []}, {"text": "In the table, we use M to denote any of the modifiers Li or R i , and c, t, w respectively represent syntactic constituent, POS, and lexical conditioning.", "labels": [], "entities": []}, {"text": "While POS annotation is clearly advantageous compared to using only syntactic categories, adding lexical variables to the model also helps.", "labels": [], "entities": []}, {"text": "As is shown in the table, it is especially important to know the lexical head of the modifier we are attempting to delete.", "labels": [], "entities": []}, {"text": "The addition of w m to conditioning variables provides an improvement of 1.3% (from 66.5% to 67.8%) on our optimal Ziff-Davis training corpus (ZD-6).", "labels": [], "entities": []}, {"text": "Furthermore, bilexical head-modifier dependencies provide a relatively small improvement of .5% (from 69.8% to 70.3%) over the best model that does not incorporate the lexical head w h . Note that lexical conditioning also helps in the case where the training data is relatively small (ZD-0), though differences are less significant, and bilexical dependencies actually hurt performance.", "labels": [], "entities": []}, {"text": "In subsequent experiments, we experimented with different Markovizations and lexical dependency combination, and finally settled with a model (s = 1 and v = 1) incorporating all conditioning variables listed in the last line of Table 2.", "labels": [], "entities": []}, {"text": "This final tuning was combined with human inspection of generated outputs, since certain modifications that positively impacted output quality seldom changed accuracies.", "labels": [], "entities": []}, {"text": "We finally took the best configuration selected above, and evaluated our model against the noisychannel model of K&M on the 32 test sentences selected by them.", "labels": [], "entities": [{"text": "K&M", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.8597869475682577}]}, {"text": "We performed both automatic and human evaluation against the output produced by Knight and Marcu's original implementation of their noisy channel model).", "labels": [], "entities": []}, {"text": "In the former case, we also provide Simple String Accuracies (SSA).", "labels": [], "entities": []}, {"text": "8 For human evaluation, we hired six native-speaker judges who scored grammaticality and content (importance) with scores from 1 to 5, using instructions as described in K&M.", "labels": [], "entities": [{"text": "K&M", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.6915966669718424}]}, {"text": "Both types of evaluations favored our Markovized model against the noisy channel model.", "labels": [], "entities": []}, {"text": "shows several outputs of our system 8 SSA is defined as: SSA = 1 \u2212 (I + D + S)/R.", "labels": [], "entities": [{"text": "SSA", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9562655091285706}]}, {"text": "The numerator terms are respectively the number of inserts, deletes, and substitutions, and R is the length of the reference compression.", "labels": [], "entities": [{"text": "R", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.9709224700927734}]}], "tableCaptions": [{"text": " Table 1: Markovizations accuracies on Ziff-Davis devel set.", "labels": [], "entities": []}, {"text": " Table 2: Accuracies on Ziff-Davis devel set with different head- modifier annotations.", "labels": [], "entities": []}, {"text": " Table 3: Accuracies on Ziff-Davis test set.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9970322847366333}, {"text": "Ziff-Davis test set", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.786309818426768}]}]}