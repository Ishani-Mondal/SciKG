{"title": [{"text": "Near-Synonym Choice in an Intelligent Thesaurus", "labels": [], "entities": [{"text": "Near-Synonym Choice", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.840187132358551}]}], "abstractContent": [{"text": "An intelligent thesaurus assists a writer with alternative choices of words and orders them by their suitability in the writing context.", "labels": [], "entities": []}, {"text": "In this paper we focus on methods for automatically choosing near-synonyms by their semantic coherence with the context.", "labels": [], "entities": []}, {"text": "Our statistical method uses the Web as a corpus to compute mutual information scores.", "labels": [], "entities": []}, {"text": "Evaluation experiments show that this method performs better than a previous method on the same task.", "labels": [], "entities": []}, {"text": "We also propose and evaluate two more methods, one that uses anti-collocations, and one that uses supervised learning.", "labels": [], "entities": []}, {"text": "To asses the difficulty of the task, we present results obtained by human judges.", "labels": [], "entities": []}], "introductionContent": [{"text": "When composing a text, a writer can access a thesaurus to retrieve words that are similar to a given target word, when there is a need to avoid repeating the same word, or when the word does not seem to be the best choice in the context.", "labels": [], "entities": []}, {"text": "Our intelligent thesaurus is an interactive application that presents the user with a list of alternative words (near-synonyms), and, unlike standard thesauri, it orders the choices by their suitability to the writing context.", "labels": [], "entities": []}, {"text": "We investigate how the collocational properties of near-synonyms can help with choosing the best words.", "labels": [], "entities": []}, {"text": "This problem is difficult because the near-synonyms have senses that are very close to each other, and therefore they occur in similar contexts; we need to capture the subtle differences specific to each near-synonym.", "labels": [], "entities": []}, {"text": "Our thesaurus brings up only alternatives that have the same part-of-speech with the target word.", "labels": [], "entities": []}, {"text": "The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus, dictionaries of synonyms, or clusters acquired from corpora.", "labels": [], "entities": []}, {"text": "In this paper we focus on the task of automatically selecting the best near-synonym that should be used in a particular context.", "labels": [], "entities": []}, {"text": "The natural way to validate an algorithm for this task would be to ask human readers to evaluate the quality of the algorithm's output, but this kind of evaluation would be very laborious.", "labels": [], "entities": []}, {"text": "Instead, we validate the algorithms by deleting selected words from sample sentences, to see whether the algorithms can restore the missing words.", "labels": [], "entities": []}, {"text": "That is, we create a lexical gap and evaluate the ability of the algorithms to fill the gap.", "labels": [], "entities": []}, {"text": "Two examples are presented in.", "labels": [], "entities": []}, {"text": "All the nearsynonyms of the original word, including the word itself, become the choices in the solution set (see the figure for two examples of solution sets).", "labels": [], "entities": []}, {"text": "The task is to automatically fill the gap with the best choice in the particular context.", "labels": [], "entities": []}, {"text": "We present a method of scoring the choices.", "labels": [], "entities": []}, {"text": "The highest scoring near-synonym will be chosen.", "labels": [], "entities": []}, {"text": "In order to evaluate how well our method works we consider that the only correct solution is the original word.", "labels": [], "entities": []}, {"text": "This will cause our evaluation scores to underestimate the performance, as more than one choice will sometimes be a perfect Sentence: This could be improved by more detailed consideration of the processes of .........", "labels": [], "entities": []}, {"text": "propagation inherent in digitizing procedures.", "labels": [], "entities": []}, {"text": "Original near-synonym: error Solution set: mistake, blooper, blunder, boner, contretemps, error, faux pas, goof, slip, solecism Sentence: The day after this raid was the official start of operation strangle, an attempt to completely destroy the .........", "labels": [], "entities": [{"text": "error Solution set", "start_pos": 23, "end_pos": 41, "type": "METRIC", "confidence": 0.8609197934468588}, {"text": "error", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.8065899610519409}, {"text": "slip", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.8281301259994507}]}, {"text": "Original near-synonym: enemy Solution set: opponent, adversary, antagonist, competitor, enemy, foe, rival: Examples of sentences with a lexical gap, and candidate near-synonyms to fill the gap. solution.", "labels": [], "entities": []}, {"text": "Moreover, what we consider to be the best choice is the typical usage in the corpus, but it may vary from writer to writer.", "labels": [], "entities": []}, {"text": "Nonetheless, it is a convenient way of producing test data.", "labels": [], "entities": []}, {"text": "The statistical method that we propose here is based on semantic coherence scores (based on mutual information) of each candidate with the words in the context.", "labels": [], "entities": []}, {"text": "We explore how far such a method can go when using the Web as a corpus.", "labels": [], "entities": []}, {"text": "We estimate the counts by using the Waterloo MultiText 1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler.", "labels": [], "entities": [{"text": "Waterloo MultiText 1 system (Clarke and Terra, 2003b", "start_pos": 36, "end_pos": 88, "type": "DATASET", "confidence": 0.8310159862041473}]}, {"text": "We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The near-synonym groups used in Exp1.", "labels": [], "entities": [{"text": "Exp1", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.8565071821212769}]}, {"text": " Table 2. The network in- cluded second-order co-occurrences. Edmonds used  the WSJ 1987 texts for testing, and reported accura- cies only a little higher than the baseline. The near- synonyms in the seven groups were chosen to have  low polysemy. This means that some sentences with  wrong senses of near-synonyms might be in the  For comparison purposes, in this section we use  the same test data", "labels": [], "entities": [{"text": "WSJ 1987 texts", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9793390035629272}, {"text": "accura- cies", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.9604770342508951}]}, {"text": " Table 5: Comparison between the statistical method  from Section 3 and the anti-collocations method  from Section 4. (Exp2 data set from Section 6.2).", "labels": [], "entities": [{"text": "Exp2 data set from Section 6.2", "start_pos": 119, "end_pos": 149, "type": "DATASET", "confidence": 0.8554004530111948}]}, {"text": " Table 6: Comparative results for the supervised  learning method using various ML learning algo- rithms (Weka), averaged over the seven groups of  near-synonyms from the Exp1 data set.", "labels": [], "entities": [{"text": "Exp1 data set", "start_pos": 171, "end_pos": 184, "type": "DATASET", "confidence": 0.9826982220013937}]}, {"text": " Table 7: Comparison between the unsupervised sta- tistical method from Section 3 and the supervised  method described in Section 5, on the Exp1 data set.", "labels": [], "entities": [{"text": "Exp1 data set", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.9880988995234171}]}, {"text": " Table 8: Results obtained by two human judges on a  random subset of the Exp1 data set.", "labels": [], "entities": [{"text": "Exp1 data set", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9806824127833048}]}, {"text": " Table 9: Accuracies for the first two choices as or- dered by an interactive intelligent thesaurus.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9973886609077454}]}, {"text": " Table 10: Results of the statistical method when  only the left context is considered.", "labels": [], "entities": []}]}