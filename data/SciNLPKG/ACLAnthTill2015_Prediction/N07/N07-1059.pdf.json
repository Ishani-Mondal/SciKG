{"title": [{"text": "Automatic Assessment of Student Translations for Foreign Language Tutoring", "labels": [], "entities": [{"text": "Automatic Assessment of Student Translations", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.5297300338745117}, {"text": "Foreign Language Tutoring", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6150036553541819}]}], "abstractContent": [{"text": "This paper introduces the use of speech translation technology fora new type of voice-interactive Computer Aided Language Learning (CALL) application.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7325134575366974}]}, {"text": "We describe a computer game we have developed , in which the system presents sentences in a student's native language to elicit spoken translations in the target new language.", "labels": [], "entities": []}, {"text": "A critical technology is an algorithm to automatically verify the ap-propriateness of the student's translation using linguistic analysis.", "labels": [], "entities": []}, {"text": "Evaluation results are presented on the system's ability to match human judgment of the cor-rectness of a student's translation, fora set of 1115 utterances collected from 9 learners of Mandarin Chinese translating flight domain sentences.", "labels": [], "entities": []}, {"text": "We also demonstrate the effective use of context information to improve both recognition performance on non-native speech as well as the system's accuracy in judging the translation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9990235567092896}]}], "introductionContent": [{"text": "It is widely recognized that one of the best ways to learn a foreign language is through spoken dialogue with native speakers.", "labels": [], "entities": []}, {"text": "However, this is not a practical method in the classroom setting.", "labels": [], "entities": []}, {"text": "A potential solution to this problem is to rely on computer spoken dialogue systems to role play a tutor and/or a conversational partner.", "labels": [], "entities": []}, {"text": "Ideally, a voice-interactive system can provide the learner with endless opportunities for practice and feedback.", "labels": [], "entities": []}, {"text": "However, while a number of dialogue systems have been developed (or adapted) for language learning purposes (), the issues of speech understanding of the accented and disfluent utterances of a foreign language student typically lead to unacceptable performance.", "labels": [], "entities": [{"text": "speech understanding of the accented and disfluent utterances of a foreign language student", "start_pos": 126, "end_pos": 217, "type": "TASK", "confidence": 0.7664073568124038}]}, {"text": "A relatively successful application of speech processing technology is in the area of pronunciation training.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7875071167945862}, {"text": "pronunciation training", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.9606073200702667}]}, {"text": "In this case, a learner repeats words or sentences prompted by the computer, and receives feedback on the segmental and suprasegmental quality of their speech.", "labels": [], "entities": []}, {"text": "While such systems can be useful, they do not help the student learn how to formulate sentences within the language on their own.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel speech translation game which aims to provide a fun and reliable environment for language learners to gain proficiency in communication.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7234145551919937}]}, {"text": "Our application is complementary to pronunciation training in that we provide the opportunity for the students to practice expressing themselves in the new language.", "labels": [], "entities": [{"text": "pronunciation training", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.929486095905304}]}, {"text": "While we don't explicitly evaluate their pronunciations, we provide implicit feedback in that they must speak well enough for the speech recognizer to successfully process their utterances.", "labels": [], "entities": []}, {"text": "The translation exercise can also serve as a preparation stage in which students can practice relevant vocabulary and sentence constructs, in order to prepare themselves for the more challenging task of interacting with a dialogue system in the same domain.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9759926199913025}]}, {"text": "System: (greet and prompt user) Welcome!", "labels": [], "entities": []}, {"text": "You are playing at level four.", "labels": [], "entities": []}, {"text": "Okay, here's your first sentence to translate: The return date is Tuesday October 25.", "labels": [], "entities": [{"text": "translate", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.957895040512085}]}, {"text": "User: (translate with error) fan3 hui2 ri4 qi1 shi4 si4 yue4 er4 shi2 wu3 hao4 xing1 qi1 er4.", "labels": [], "entities": []}, {"text": "System: (paraphrase in Chinese) fan3 hui2 ri4 qi1 si4 yue4 er4 shi2 wu3 hao4 xing1 qi1 er4.", "labels": [], "entities": []}, {"text": "System: (paraphrase in English) The return date is Tuesday April 25.", "labels": [], "entities": []}, {"text": "User: (repeat date) shi2 yue4 er4 shi2 wu3 hao4.", "labels": [], "entities": []}, {"text": "System: (paraphrase in Chinese) shi2 yue4 er4 shi2 wu3 hao4.", "labels": [], "entities": []}, {"text": "System: (paraphrase in English) October 25.", "labels": [], "entities": []}, {"text": "System: (respond to user) \"Great!", "labels": [], "entities": []}, {"text": "You successfully translated the sentence in separate fragments.", "labels": [], "entities": []}, {"text": "Let's move onto the next sentence: I need to fly to Hong Kong the day after tomorrow.", "labels": [], "entities": []}, {"text": "User: (click on the help button) System: (repeat English prompt) I need to fly to Hong Kong the day after tomorrow.", "labels": [], "entities": []}, {"text": "System: (offer example translation) wo3 xu1 yao4 hou4 tian1 fei1 xiang1 gang3.", "labels": [], "entities": []}, {"text": "User: (imitate the translation) wo3 xu1 yao4 hou4 tian1 fei1 xiang1 gang3.", "labels": [], "entities": []}, {"text": "System: (respond to user) You translated nine out often sentences.", "labels": [], "entities": []}, {"text": "You took on average 1.6 turns per sentence.", "labels": [], "entities": []}, {"text": "You have advanced to level five.", "labels": [], "entities": []}, {"text": "Would you like to play another round?: Example interaction between a user and the system.", "labels": [], "entities": []}, {"text": "Our prototype centers on the task of translating phrases and sentences from English into Chinese, in the flight reservation domain.", "labels": [], "entities": [{"text": "translating phrases and sentences from English", "start_pos": 37, "end_pos": 83, "type": "TASK", "confidence": 0.857763260602951}]}, {"text": "As illustrated by the example dialogue in, the system role plays a language tutor interacting with a Mandarin learner.", "labels": [], "entities": []}, {"text": "The system prompts the student with randomly generated English sentences to elicit spoken Chinese translations from the learner.", "labels": [], "entities": []}, {"text": "The system paraphrases each user utterance in both languages, to implicitly inform the user of the system's internal understanding, and judges whether the student has succeeded in the task.", "labels": [], "entities": []}, {"text": "The system keeps track of how many turns a user takes to complete all the sentences in a game session, and rewards good performance by advancing the student towards higher difficulty levels.", "labels": [], "entities": []}, {"text": "A convenient \"help\" button allows the student to request a translation of the current game sentence, to help them overcome gaps in their knowledge of the linguistic constructs or the vocabulary.", "labels": [], "entities": [{"text": "translation of the current game sentence", "start_pos": 59, "end_pos": 99, "type": "TASK", "confidence": 0.8202741344769796}]}, {"text": "The student can also type any English sentences within the domain to obtain a reference translation.", "labels": [], "entities": []}, {"text": "The system utilizes an interlingua-based bidirectional translation capability, described in detail in ( ).", "labels": [], "entities": [{"text": "interlingua-based bidirectional translation", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.6248653729756674}]}, {"text": "Both Chinese and English sentences are parsed into a common meaning representation, which we loosely refer to as an \"interlingua,\" from which paraphrases in both languages can be automatically generated using formal generation rules.", "labels": [], "entities": []}, {"text": "The key to a successful tutoring system lies in its ability to provide immediate and pertinent feedback on the student's performance, similar to a human tutor.", "labels": [], "entities": []}, {"text": "A central focus of this paper is to address the challenging problem of automatically assessing the appropriateness of a student's translation.", "labels": [], "entities": [{"text": "assessing the appropriateness of a student's translation", "start_pos": 85, "end_pos": 141, "type": "TASK", "confidence": 0.5729228369891644}]}, {"text": "At first glance, our task appears to share much in common with machine translation (MT) evaluation ().", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 63, "end_pos": 98, "type": "TASK", "confidence": 0.8417914013067881}]}, {"text": "Indeed, both are trying to assess the quality of the translation output, whether it is produced by a computer or by a foreign language student.", "labels": [], "entities": []}, {"text": "Nevertheless, there also exist several fundamental distinctions.", "labels": [], "entities": []}, {"text": "Automatic MT evaluation methods, as represented by the well-known Bleu metric (), assume the availability of human reference translations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9611833393573761}, {"text": "Bleu metric", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.8277890980243683}]}, {"text": "The algorithms typically compare MT outputs with reference translations with the goal of producing a quality indicator (on a numeric scale) that correlates with human judgement.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.8855910897254944}]}, {"text": "In contrast, our algorithm operates in the absence of human generated reference translations . Furthermore, our application requires the evaluation algorithm to make accept/reject decisions on each individual translation, in the same way as a language tutor determines whether a translation is acceptable or not.", "labels": [], "entities": []}, {"text": "While our task is more demanding, it is made possible by operating in restricted domains.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present an interlingua-based approach for verifying the correctness of the student's spoken translation.", "labels": [], "entities": [{"text": "verifying the correctness of the student's spoken translation", "start_pos": 59, "end_pos": 120, "type": "TASK", "confidence": 0.642146435048845}]}, {"text": "Section 3 describes the evaluation framework, followed by results and discussions in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we discuss future plans for extending our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given a translation pair, the goal of our algorithm is to make the same accept/reject decision as a human evaluator.", "labels": [], "entities": []}, {"text": "Hence, we can evaluate our algorithm in a classification framework.", "labels": [], "entities": []}, {"text": "In this section, we first present the data collection and labeling effort.", "labels": [], "entities": [{"text": "data collection", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7119701504707336}]}, {"text": "We then describe a baseline system based on a variant of the Bleu metric.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.709507405757904}]}, {"text": "Finally we briefly describe the metrics we used to evaluate our algorithms.", "labels": [], "entities": []}, {"text": "We evaluated the overall system performance on test data using human decisions as ground truth.", "labels": [], "entities": []}, {"text": "Al- We determined empirically that the NIST score works slightly better than the Bleu score in our application.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.6352983117103577}, {"text": "Bleu score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.8083475232124329}]}, {"text": "The scores are computed using the NIST MT scoring tool from: ftp://jaguar.ncsl.nist.gov/mt /resources/mteval-v11b.pl though we cannot generate an ROC curve for our proposed algorithm (because it is a non-parametric method), we plot its performance along with the ROC curve of the baseline system fora more thorough comparison.", "labels": [], "entities": [{"text": "ROC", "start_pos": 263, "end_pos": 266, "type": "METRIC", "confidence": 0.9852095246315002}]}, {"text": "We evaluated the different ASR integration strategies (1-best hypothesis, 10-best hypotheses, using contextual constraints from reference KV) based on sentence classification error rates as well as speech recognition performance.", "labels": [], "entities": [{"text": "ASR integration", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.9868278205394745}, {"text": "sentence classification", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.6670181006193161}]}, {"text": "summarizes the false accept, false reject, and overall classification error rates on unseen test data.", "labels": [], "entities": []}, {"text": "With manual transcripts as inputs, the baseline algorithm using the NIST score achieved a classification error rate of 19.3%, as compared with 25.0% for the trivial case of always accepting the user sentence (Majority classifier).", "labels": [], "entities": [{"text": "NIST score", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9186422228813171}, {"text": "classification error rate", "start_pos": 90, "end_pos": 115, "type": "METRIC", "confidence": 0.7710219422976176}]}, {"text": "The KV-based algorithm achieved a much better performance, with only a 7.1% classification error rate.", "labels": [], "entities": [{"text": "classification error rate", "start_pos": 76, "end_pos": 101, "type": "METRIC", "confidence": 0.7719269096851349}]}, {"text": "This translates into a kappa score of 0.86, which is slightly above the level of agreement initially achieved by the two labellers.", "labels": [], "entities": [{"text": "kappa score", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.974046379327774}]}, {"text": "Note that the performance difference compared to the baseline system is mostly attributed to a large reduction in the \"False Accept\" category.", "labels": [], "entities": [{"text": "False Accept\" category", "start_pos": 119, "end_pos": 141, "type": "METRIC", "confidence": 0.8960155099630356}]}], "tableCaptions": [{"text": " Table 1: Classification results for various evaluation  systems, on both transcripts and automatic speech  recognition (ASR) outputs. Note that the \"KV Con- text\" condition favors a hypothesis that matches the  prompt KV.", "labels": [], "entities": [{"text": "automatic speech  recognition (ASR) outputs", "start_pos": 90, "end_pos": 133, "type": "TASK", "confidence": 0.7757038814680917}]}, {"text": " Table 2: Comparison of speech recognition per- formance in syllable error rates and sentence error  rates, for three different strategies of utterance selec- tion from an N -best list. (ER stands for error rate,  RR stands for relative reduction.)", "labels": [], "entities": [{"text": "sentence error  rates", "start_pos": 85, "end_pos": 106, "type": "METRIC", "confidence": 0.7043363749980927}]}]}