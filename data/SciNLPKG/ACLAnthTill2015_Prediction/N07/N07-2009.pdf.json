{"title": [{"text": "Generalized Graphical Abstractions for Statistical Machine Translation", "labels": [], "entities": [{"text": "Generalized Graphical Abstractions", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7379143635431925}, {"text": "Statistical Machine Translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.8396265506744385}]}], "abstractContent": [{"text": "We introduce a novel framework for the expression, rapid-prototyping, and evaluation of statistical machine-translation (MT) systems using graphical models.", "labels": [], "entities": [{"text": "statistical machine-translation (MT)", "start_pos": 88, "end_pos": 124, "type": "TASK", "confidence": 0.5685392379760742}]}, {"text": "The framework extends dynamic Bayesian networks with multiple connected different-length streams, switching variable existence and dependence mechanisms , and constraint factors.", "labels": [], "entities": []}, {"text": "We have implemented anew general-purpose MT training/decoding system in this framework , and have tested this on a variety of existing MT models (including the 4 IBM models), and some novel ones as well, all using Europarl as a test corpus.", "labels": [], "entities": [{"text": "MT training/decoding", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8831566572189331}, {"text": "Europarl", "start_pos": 214, "end_pos": 222, "type": "DATASET", "confidence": 0.9887155890464783}]}, {"text": "We describe the semantics of our representation , and present preliminary evaluations, showing that it is possible to prototype novel MT ideas in a short amount of time.", "labels": [], "entities": [{"text": "MT ideas", "start_pos": 134, "end_pos": 142, "type": "TASK", "confidence": 0.9231005907058716}]}], "introductionContent": [{"text": "We present a unified graphical model framework based on () for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.7231526871522268}]}, {"text": "Graphical models utilize graphical descriptions of probabilistic processes, and are capable of quickly describing a wide variety of different sets of model assumptions.", "labels": [], "entities": []}, {"text": "In our approach, either phrases or words can be used as the unit of translation, but as a first step, we have only implemented word-based models since our main goal is to show * This material was supported by NSF under Grant No. ISS-0326276.", "labels": [], "entities": [{"text": "NSF under Grant No. ISS-0326276", "start_pos": 209, "end_pos": 240, "type": "DATASET", "confidence": 0.6600455522537232}]}, {"text": "the viability of our graphical model representation and new software system.", "labels": [], "entities": []}, {"text": "There are several important advantages to a unified probabilistic framework for MT including: (1) the same codebase can be used for training and decoding without having to implement a separate decoder for each model; (2) new models can be prototyped quickly; (3) combining models (such as in a speech-MT system) is easier when they are encoded in the same framework; (4) sharing algorithms across different disciplines (e.g., the MT and the constraint-satisfaction community) is facilitated.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9826036095619202}, {"text": "MT", "start_pos": 430, "end_pos": 432, "type": "TASK", "confidence": 0.7519823908805847}]}], "datasetContent": [{"text": "We have developed (in C++) anew entirely selfcontained general-purpose MT training/decoding system based on our framework, of which we provide a preliminary evaluation in this section.", "labels": [], "entities": [{"text": "MT training/decoding", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.8726028352975845}]}, {"text": "Although the framework is perfectly capable of representing phrase-based models, we restrict ourselves to word-based models to show the viability of graphical models for MT and will consider different translation units in future work.", "labels": [], "entities": [{"text": "MT", "start_pos": 170, "end_pos": 172, "type": "TASK", "confidence": 0.9877986907958984}]}, {"text": "We perform MT ex- periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations ().", "labels": [], "entities": [{"text": "MT ex- periments", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.818244531750679}, {"text": "Europarl corpus", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9515851140022278}, {"text": "ACL 2005", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.8121658861637115}, {"text": "SMT evaluations", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.5386421084403992}]}, {"text": "We train an English language model on the whole training set using the SRILM toolkit) and train MT models mainly on a 10k sentence pair subset of the ACL training set.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9125334918498993}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9693026542663574}, {"text": "ACL training set", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.8837080597877502}]}, {"text": "We test on the 2000 sentence test set used for the same evaluations.", "labels": [], "entities": [{"text": "2000 sentence test set", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.6642117574810982}]}, {"text": "For comparison, we use the MT training program, GIZA++, the phrase-base decoder, Pharaoh (, and the wordbased decoder, Rewrite.", "labels": [], "entities": [{"text": "MT training", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8648225367069244}]}, {"text": "For inference we use a backtracking depth-first search inference method with memoization that extends Value Elimination (.", "labels": [], "entities": [{"text": "Value Elimination", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.7718265354633331}]}, {"text": "The same inference engine is used for both training and decoding.", "labels": [], "entities": []}, {"text": "As an admissible heuristic for decoding, we compute, for each node V with Conditional Probability, the largest value of cover all possible configurations of V and its parents compares MT performance between (1) Pharaoh (which uses beam search), (2) our system, and (3) Rewrite (hill-climbing).", "labels": [], "entities": [{"text": "MT", "start_pos": 184, "end_pos": 186, "type": "TASK", "confidence": 0.906442403793335}, {"text": "Rewrite", "start_pos": 269, "end_pos": 276, "type": "DATASET", "confidence": 0.6992509961128235}]}, {"text": "(1) and (2) make use of a fixed lexical table 2 learned using an M-HMM model specified using our tool, and neither uses minimum error rate training.", "labels": [], "entities": []}, {"text": "(3) uses Model 4 parameters learned using GIZA++.", "labels": [], "entities": []}, {"text": "This comparison is informative because Rewrite is a special purpose model 4 decoder and we would expect it to perform at least as well as decoders not written fora specific IBM model.", "labels": [], "entities": [{"text": "Rewrite", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.8786247372627258}]}, {"text": "Pharaoh is more general in that it only requires, as input, a lexical table from any given model.", "labels": [], "entities": []}, {"text": "3 Our MDBN system is not tailored for the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.9317971467971802}]}, {"text": "Pharaoh was able to decode the 2000 sentences of the test set in 5000s on a 3.2GHz machine; Rewrite took 84000s, and we allotted 400000s for our engine (200s per sentence).", "labels": [], "entities": []}, {"text": "We attribute the difference in speed and BLEU score between our system and Pharaoh to the fact Value Elimination searches in a depth-first fashion over the space of partial configurations of RVs, while Pharaoh expands partial translation hypotheses in a best-first search manner.", "labels": [], "entities": [{"text": "speed", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.997676432132721}, {"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9792389869689941}, {"text": "Value Elimination", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7492430508136749}]}, {"text": "Thus, Pharaoh can take advantage of knowledge about the MT problem's hypothesis space while the GM is agnostic with respect to the structure of the problem-something that is desirable from our perspective since generality is a main concern of ours.", "labels": [], "entities": [{"text": "MT problem's hypothesis", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.8852382898330688}]}, {"text": "Moreover, the MDBN's heuristic and caching of previously explored subtrees have not yet proven able to defray the cost, associated with depth-first search, of exploring subtrees that do not contain any \"good\" configurations.", "labels": [], "entities": []}, {"text": "shows BLEU scores of different MT models trained using our system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9990419745445251}, {"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9796817302703857}]}, {"text": "We decode using Pharaoh because the above speed difference in its favor allowed us to run more experiments and focus on the training aspect of different models.", "labels": [], "entities": []}, {"text": "M1, M2, M-HMM, M3, and M4 are the standard IBM models.", "labels": [], "entities": []}, {"text": "M2d and M-Hd are variants in which the distortion between the French and English positions is used instead of the absolute alignment position.", "labels": [], "entities": [{"text": "M2d", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7619357705116272}]}, {"text": "M-Hdd is a second-order M-HMM model (with distortion).", "labels": [], "entities": []}, {"text": "M3H (see is a variant of model 3 that uses first-order dependencies between alignment variables.", "labels": [], "entities": [{"text": "M3H", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9384112358093262}]}, {"text": "M-Hr is another HMM model that uses the relative distortion between the current alignment and the previous one.", "labels": [], "entities": [{"text": "M-Hr", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8182404637336731}]}, {"text": "This is similar to the model implemented by GIZA except we did  not include the English word class dependency.", "labels": [], "entities": []}, {"text": "Finally, model M4H is a simplified model 4, in which only distortions within each tablet are modeled but a Markov dependency is also used between the alignment variables.", "labels": [], "entities": []}, {"text": "also shows BLEU scores obtained by training equivalent IBM models using GIZA and the standard training regimen of initializing higher models with lower ones (we use the same schedules for our GM training, but only transfer lexical tables).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9994634985923767}, {"text": "GIZA", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.7645697593688965}]}, {"text": "The main observation is that GIZA-trained M-HMM, M3 and 4 have about 1% better BLEU scores than their corresponding MDBN versions.", "labels": [], "entities": [{"text": "GIZA-trained M-HMM", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.5412507206201553}, {"text": "BLEU scores", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9780124127864838}]}, {"text": "We attribute the difference in M3/4 scores to the fact we use a Viterbi-like training procedure (i.e., we consider a single configuration of the hidden variables in EM training) while GIZA uses pegging) to sum over a set of likely hidden variable configurations in EM.", "labels": [], "entities": []}, {"text": "While these preliminary results do not show improved MT performance, nor would we expect them to since they are on simulated IBM models, we find very promising the fact that this general-purpose graphical model-based system produces competitive MT results on a computationally challenging task.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.994499683380127}, {"text": "MT", "start_pos": 245, "end_pos": 247, "type": "TASK", "confidence": 0.9933316111564636}]}], "tableCaptions": [{"text": " Table 1: BLEU scores on first 500, 1000, 1500, and  2000 sentences (ordered from shortest to longest) of  the ACL05 English-French 2000 sentence test set us- ing a 700k sent train set. The last row is our MDBN  system's simulation of a M-HMM model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990665316581726}, {"text": "ACL05 English-French 2000 sentence test set", "start_pos": 111, "end_pos": 154, "type": "DATASET", "confidence": 0.8726979494094849}]}, {"text": " Table 2: BLEU scores for various models trained  using GM and GIZA (when applicable). All models  are decoded using Pharaoh.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993157386779785}, {"text": "GM", "start_pos": 56, "end_pos": 58, "type": "DATASET", "confidence": 0.9039739370346069}, {"text": "GIZA", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.7341749668121338}]}]}