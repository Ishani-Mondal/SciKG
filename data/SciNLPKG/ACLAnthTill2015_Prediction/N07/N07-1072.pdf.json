{"title": [{"text": "Computing Semantic Similarity between Skill Statements for Approxi- mate Matching", "labels": [], "entities": [{"text": "Matching", "start_pos": 73, "end_pos": 81, "type": "TASK", "confidence": 0.7320924997329712}]}], "abstractContent": [{"text": "This paper explores the problem of computing text similarity between verb phrases describing skilled human behavior for the purpose of finding approximate matches.", "labels": [], "entities": [{"text": "computing text similarity between verb phrases describing skilled human behavior", "start_pos": 35, "end_pos": 115, "type": "TASK", "confidence": 0.7377029746770859}]}, {"text": "Four parsers are evaluated on a large corpus of skill statements extracted from an enterprise-wide expertise taxon-omy.", "labels": [], "entities": []}, {"text": "A similarity measure utilizing common semantic role features extracted from parse trees was found superior to an information -theoretic measure of similarity and comparable to the level of human agreement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge-intensive industries need to become more efficient at deploying the right expertise as quickly and smoothly as possible, thus it is desired to have systems that can quickly match and deploy skilled individuals to meet customer needs.", "labels": [], "entities": []}, {"text": "The searches inmost of the current matching systems are based on exact matches between skill statements.", "labels": [], "entities": []}, {"text": "However, exact matching is very likely to miss individuals who are very good matches to the job but didn't select the exact skills that appeared in the open job description.", "labels": [], "entities": [{"text": "exact matching", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.730097621679306}]}, {"text": "It is always hard for individuals to find the perfect skills to describe their skill sets.", "labels": [], "entities": []}, {"text": "For example, an individual might not know whether to choose a skill stating that refers to \"maintaining\" a given product or \"supporting\" it or whether to choose a skill about maintaining a \"database\" or about maintaining \"DB2\".", "labels": [], "entities": []}, {"text": "Thus, it is desirable for the job search system to be able to find approximate matches, instead of only exact matches, between available individuals and open job positions.", "labels": [], "entities": []}, {"text": "More specifically, a skill similarity computation is needed to allow searches to be expanded to related skills, and return more potential matches.", "labels": [], "entities": []}, {"text": "In this paper, we present our work on developing a skill similarity computation based upon semantic commonalities between skill statements.", "labels": [], "entities": []}, {"text": "Although there has been much work on text similarity metrics), most approaches treat texts as a bag of words and try to find shared words with certain statistical properties based on corpus frequencies.", "labels": [], "entities": []}, {"text": "As a result, the structural information in the text is ignored in these approaches.", "labels": [], "entities": []}, {"text": "We will describe anew semantic approach that takes the structural information of the text into consideration and matches skill statements on corresponding semantic roles.", "labels": [], "entities": []}, {"text": "We will demonstrate that it can outperform standard statistical text similarity techniques, and reach the level of human agreement.", "labels": [], "entities": [{"text": "statistical text similarity", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6024068593978882}]}, {"text": "In Section 2, we first describe the skill statements we extracted from an enterprise-wide expertise taxonomy.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the performance of a standard statistical approach on this task.", "labels": [], "entities": []}, {"text": "This motivates our semantic approach of matching skill statements on corresponding semantic roles.", "labels": [], "entities": []}, {"text": "We also compare and evaluate the performance of four natural language parsers (the Charniak parser, the Stanford parser, the ESG parser, and MINIPAR) for the purpose of our task.", "labels": [], "entities": []}, {"text": "An inter-rater agreement study and evaluation of our approach will be presented in Section 4.", "labels": [], "entities": []}, {"text": "We end with a discussion and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "While the Charniak parser performed well in our initial verb phrase (VP) test, we decided to compare the Charniak parser's performance with other parsers.", "labels": [], "entities": []}, {"text": "For this evaluation, we compared it with the Stanford parser, the ESG parser, and MINIPAR.", "labels": [], "entities": [{"text": "ESG", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9003119468688965}]}, {"text": "The Stanford parser) is an unlexicalized statistical syntactic parser that was trained on the same corpus as the Charniak parser (the Penn TreeBank).", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.9715788662433624}]}, {"text": "Its parse tree has the same structure as the Charniak parser.", "labels": [], "entities": []}, {"text": "The ESG (English Slot Grammar) parser) is a rule-based parser based on the slot grammar where each phrase has ahead and dependent elements, and is also marked with a syntactic role.", "labels": [], "entities": [{"text": "ESG (English Slot Grammar) parser", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.624855271407536}]}, {"text": "MINIPAR, as a dependency parser, is very similar to the ESG parser in terms of its output.", "labels": [], "entities": []}, {"text": "It represents sentence structures as a set of dependency relationships between head words.", "labels": [], "entities": []}, {"text": "Since our purpose is to use the syntactic parses as inputs to extract semantic role patterns, the correctness of the bracketing of the parses and the syntactic labels of the phrases (e.g., NP, VP, and PP) are the most important information for our purposes, whereas the POS (Part-Of-Speech) labels of individual words (e.g., nouns vs. proper nouns) are not that important (also, there are too many domain-specific terms in our data).", "labels": [], "entities": []}, {"text": "Thus, our evaluation of the parses is only on the correctness of the bracketing and the syntactic labels of the phrases, not the correctness of the entire parse.", "labels": [], "entities": []}, {"text": "For our task, the correctness of the prepositional phrase attachment is especially important for extracting accurate semantic role patterns (].", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.7041149735450745}]}, {"text": "Thus the parser needs to correctly attach \"of IBM E-business Middleware\" to \"Knowledge\" and attach \"to PLM Solutions\" to \"Apply\", not \"Knowledge\".", "labels": [], "entities": []}, {"text": "To evaluate the performance of the parsers, we randomly picked 100 skill statements from our corpus, preprocessed them, and then parsed them using the four different parsers.", "labels": [], "entities": []}, {"text": "We then evaluated the parses using the above evaluation measures.", "labels": [], "entities": []}, {"text": "The parses were rated as corrector incorrect.", "labels": [], "entities": [{"text": "corrector incorrect", "start_pos": 25, "end_pos": 44, "type": "METRIC", "confidence": 0.9655803143978119}]}, {"text": "No partial score was given.", "labels": [], "entities": []}, {"text": "The error analysis reveals four major sources of error for all the parsers, most of which are specific to the domain we are working on: (1) Many domain specific terms and acronyms.", "labels": [], "entities": []}, {"text": "For example, \"SAP\" in \"Employees advise on SAP R/3 logistics basic data.\" was always tagged as a verb by the parsers.", "labels": [], "entities": []}, {"text": "(2) Many long noun phrases.", "labels": [], "entities": []}, {"text": "For example, \"Employees perform JD edwards foundation suite address book.\"", "labels": [], "entities": [{"text": "JD edwards foundation suite address book", "start_pos": 32, "end_pos": 72, "type": "DATASET", "confidence": 0.623473917444547}]}, {"text": "(3) Some specialized use of punctuation.", "labels": [], "entities": []}, {"text": "For example, \"Employees perform business transportation consultant-logistics.sys.\"", "labels": [], "entities": []}, {"text": "(4) Prepositional phrase attachment can be difficult.", "labels": [], "entities": [{"text": "Prepositional phrase attachment", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8627453049023946}]}, {"text": "For example, in \"Employees apply IBM infrastructure knowledge for IDBS\", \"for IDBS\" should attach to \"apply\", but many parsers mistakenly attach it to \"IBM infrastructure knowledge\".", "labels": [], "entities": []}, {"text": "We noticed that MINIPAR performed much worse compared with the other parsers.", "labels": [], "entities": []}, {"text": "The main reason is that it always parses the phrase \"VERB knowledge of Y\" (e.g., \"Employees apply knowledge of web technologies.\") incorrectly --the parse result always mistakenly attaches \"of Y\" (e.g., of web technologies) to the VERB (e.g., apply), not \"knowledge\".", "labels": [], "entities": []}, {"text": "Since there were so many of phrases in the test set and in the corpus, this kind of error significantly reduced the performance for our task.", "labels": [], "entities": []}, {"text": "These kinds of errors on prepositional phrase attachment in MINIPAR were also mentioned in).", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6741617123285929}]}, {"text": "From the evaluation and comparison results we can see that the Charniak parser performs the best for our task among all the four parsers.", "labels": [], "entities": []}, {"text": "This result is consistent with a more thorough evaluation) on a different corpus with a set of different target verbs, which showed the Charniak parser performed the best among three parsers (including the Stanford parser and MINPAR) for labeling semantic roles.", "labels": [], "entities": [{"text": "labeling semantic roles", "start_pos": 238, "end_pos": 261, "type": "TASK", "confidence": 0.8881841897964478}]}, {"text": "We note that although the ESG parser performed a little worse than the Charniak parser, its parses contain much richer syntactic (e.g., subject, object) and semantic (e.g., word senses) slot-filling information, which can be very useful to many natural language applications.", "labels": [], "entities": []}, {"text": "In order to evaluate the two approaches (semantic role parsing and statistical) to computing semantic similarity of skill statements in our domain, we first conducted an experiment to evaluate how humans agree on this task, which also provides us with an upper bound accuracy for the task.", "labels": [], "entities": [{"text": "semantic role parsing", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.6805333296457926}, {"text": "computing semantic similarity of skill statements", "start_pos": 83, "end_pos": 132, "type": "TASK", "confidence": 0.7486469745635986}, {"text": "accuracy", "start_pos": 267, "end_pos": 275, "type": "METRIC", "confidence": 0.9890457391738892}]}, {"text": "We use the 75 skill pairs as test data to evaluate our semantic similarity approach against human judgments.", "labels": [], "entities": []}, {"text": "Considering the reliability of the data, only the coarse-grained (binary) judgments are used.", "labels": [], "entities": []}, {"text": "The gold standard is obtained by majority voting from the three raters, i.e., fora given skill pair, if two or more raters judge it as similar, then the gold standard answer is \"similar\", otherwise it is \"not similar\".", "labels": [], "entities": [{"text": "gold standard", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8853461146354675}]}, {"text": "We first evaluated Lin's statistical approach described in Section 3.1.", "labels": [], "entities": []}, {"text": "Among 75 skill pairs, 53 of them were rated correctly according to the human judgments, that is, 70.67% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9983649849891663}]}, {"text": "The error analysis shows that many of the errors can be corrected if the skills are matched on their corresponding semantic roles.", "labels": [], "entities": []}, {"text": "We then evaluated the utility of the extracted semantic role information to see whether it can outperform the statistical approach.", "labels": [], "entities": []}, {"text": "For simplicity, we will only report on evaluating semantic role matching on the \"concept\" role that specifies the key component of the skills, as introduced in Section 3.2.", "labels": [], "entities": []}, {"text": "There are at least two straightforward ways of performing semantic role matching for the skill similarity computation: 1) match on the entire semantic role; 2) match on the head nouns only.", "labels": [], "entities": [{"text": "semantic role matching", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6269548137982687}]}, {"text": "But both have their drawbacks: the first approach is too strict and will miss many similar skill statements; the second approach may not only miss the similar skill statements, e.g., Perform In order to solve these problems, we used a simple matching criterion from.", "labels": [], "entities": []}, {"text": "The similarity of two texts t 1 and t 2 is determined by: Similarity(t 1 , t 2 ) = This equation states that two texts are similar if shared features area large percentage of the total features.", "labels": [], "entities": []}, {"text": "We set a threshold of 0.5, requiring that at least 50% of the features be shared.", "labels": [], "entities": []}, {"text": "We apply this criterion to the text contained in the \"concept\" role.", "labels": [], "entities": []}, {"text": "The words in the calculation are preprocessed first: abbreviations are expanded, stop-words are excluded (e.g., the and of don't count as shared words), and the remaining words are stemmed (e.g., manager and management are counted as shared words), as was done in our previous information-theoretic approach.", "labels": [], "entities": []}, {"text": "Words connected by punctuation (e.g., e-business, software/hardware) are treated as separate words.", "labels": [], "entities": []}, {"text": "For example, Advise on [Field/Force Management] for Telecom Apply Knowledge of [Basic Field Force Automation] The shared words between the two \"concept\" roles (bracketed) are \"Field\" and \"Force\", and their shared percentage is (2*2)/7 = 57.14% > 50%, so they are similar.", "labels": [], "entities": [{"text": "Telecom Apply Knowledge", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7880316376686096}]}, {"text": "We have also evaluated this approach on our test set with the 75 skill pairs.", "labels": [], "entities": []}, {"text": "Among 75 skill pairs, 60 of them were rated correctly (i.e., 80% accuracy), which significantly outperforms the statistical approach, and is very close to the upper bound accuracy, i.e., human agreement (81%), as shown in..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9954935312271118}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.7463467717170715}, {"text": "agreement", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.8257465958595276}]}, {"text": "Evaluation on Semantic Similarity between Skill Statements The difference between this approach and Lin's information content approach is that this computation is local --no corpus statistics is used.", "labels": [], "entities": []}, {"text": "Also, using this approach, it is easier to set an intuitive threshold (e.g., 50%) fora classification problem (e.g., similar or not for our task).", "labels": [], "entities": []}, {"text": "With this approach, however, there are also cases that are mistagged as similar, for example, Apply Knowledge of [Basic Field Force Automation] Advise on [Sales Force Automation] Although \"Field Force Automation\" and \"Sales Force Automation\" seem similar on their surface form, they are two quite different concepts.", "labels": [], "entities": [{"text": "Apply", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9845272302627563}]}, {"text": "Deeper domain knowledge (such as an ontology) is needed to distinguish such cases.", "labels": [], "entities": []}], "tableCaptions": []}