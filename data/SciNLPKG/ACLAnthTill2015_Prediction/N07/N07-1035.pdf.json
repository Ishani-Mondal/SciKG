{"title": [{"text": "Estimating the Reliability of MDP Policies: A Confidence Interval Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "Past approaches for using reinforcement learning to derive dialog control policies have assumed that there was enough collected data to derive a reliable policy.", "labels": [], "entities": []}, {"text": "In this paper we present a methodology for numerically constructing confidence intervals for the expected cumulative reward fora learned policy.", "labels": [], "entities": []}, {"text": "These intervals are used to (1) better assess the reliability of the expected cumulative reward, and (2) perform a refined comparison between policies derived from different Markov Decision Processes (MDP) models.", "labels": [], "entities": []}, {"text": "We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP state-space.", "labels": [], "entities": []}, {"text": "Our results show that while some of the policies developed in the prior work exhibited very large confidence intervals, the policy developed from the best feature set had a much smaller confidence interval and thus showed very high reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 232, "end_pos": 243, "type": "METRIC", "confidence": 0.9707652926445007}]}], "introductionContent": [{"text": "NLP researchers frequently have to deal with issues of data sparsity.", "labels": [], "entities": []}, {"text": "Whether the task is machine translation or named-entity recognition, the amount of data one has to train or test with can greatly impact the reliability and robustness of one's models, results and conclusions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7656866312026978}, {"text": "named-entity recognition", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7010383009910583}, {"text": "reliability", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.9753996729850769}]}, {"text": "One research area that is particularly sensitive to the data sparsity issue is machine learning, specifically in using Reinforcement Learning (RL) to learn the optimal action fora dialogue system to make given any user state.", "labels": [], "entities": []}, {"text": "Typically this involves learning from previously collected data or interacting in real-time with real users or user simulators.", "labels": [], "entities": []}, {"text": "One of the biggest advantages to this machine learning approach is that it can be used to generate optimal policies for every possible state.", "labels": [], "entities": []}, {"text": "However, this method requires a thorough exploration of the state-space to make reliable conclusions on what the best actions are.", "labels": [], "entities": []}, {"text": "States that are infrequently visited in the training set could be assigned sub-optimal actions, and therefore the resulting dialogue manager may not provide the best interaction for the user.", "labels": [], "entities": []}, {"text": "In this work, we present an approach for estimating the reliability of a policy derived from collected training data.", "labels": [], "entities": [{"text": "reliability", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.945860743522644}]}, {"text": "The key idea is to take into account the uncertainty in the model parameters (MDP transition probabilities), and use that information to numerically construct a confidence interval for the expected cumulative reward for the learned policy.", "labels": [], "entities": []}, {"text": "This confidence interval approach allows us to: (1) better assess the reliability of the expected cumulative reward fora given policy, and (2) perform a refined comparison between policies derived from different MDP models.", "labels": [], "entities": [{"text": "reliability", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.954344630241394}]}, {"text": "We apply the proposed approach to our previous work) in using RL to improve a spoken dialogue tutoring system.", "labels": [], "entities": []}, {"text": "In that work, a dataset of 100 dialogues was used to develop a methodology for selecting which user state features should be included in the MDP state-space.", "labels": [], "entities": []}, {"text": "But are 100 dialogues enough to generate reliable policies?", "labels": [], "entities": []}, {"text": "In this paper we apply our confidence in-terval approach to the same dataset in an effort to investigate how reliable our previous conclusions are, given the amount of available training data.", "labels": [], "entities": []}, {"text": "In the following section, we discuss the prior work and its data sparsity issue.", "labels": [], "entities": []}, {"text": "In section 3, we describe in detail our confidence interval methodology.", "labels": [], "entities": []}, {"text": "In section 4, we show how this methodology works by applying it to the prior work.", "labels": [], "entities": []}, {"text": "In sections 5 and 6, we present our conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The starting point for the proposed methodology is the observation that for each state s j and action a kin the MDP, the set of transition probabilities {p(s i |s j , a k )} i=1..n are modeled as multinomial distributions that are estimated from the transition counts in the training data: where n is the number of states in the model, and is the number of times the system was instate s j , took action a k , and transitioned to state It is important to note that these parameters are just estimates.", "labels": [], "entities": []}, {"text": "The reliability of these estimates clearly depends on the amount of training data, more specifically on the transition counts c(s i , s j , a k ).", "labels": [], "entities": []}, {"text": "For instance, consider a model with 3 states and 2 actions.", "labels": [], "entities": []}, {"text": "Say the model was instate s 1 and took action a 1 ten times.", "labels": [], "entities": []}, {"text": "Out of these, three times the model transitioned back to state s 1 , two times it transitioned to state s 2 , and five times to state s 3 . Then we have: While both sets of transition parameters have the same value, the second set of estimates is more reliable.", "labels": [], "entities": []}, {"text": "The central idea of the proposed approach is to model this uncertainty in the system parameters, and use it to numerically construct confidence intervals for the value of the optimal policy.", "labels": [], "entities": []}, {"text": "Formally, each set of transition probabilities .n is modeled as a multinomial distribution, estimated from data 2 . The uncertainty of multinomial estimates are commonly modeled by means of a Dirichlet distribution.", "labels": [], "entities": []}, {"text": "The Dirichlet distribution is characterized by a set of parameters \u03b1 1 , \u03b1 2 , ..., \u03b1 n , which in this case correspond to the counts {c(s i , s j , a k )} i=1..n . For any given j, the likelihood of the set of multinomial transition parameters {p(s i |s j , a k )} i=1..n is then given by: where Note that the maximum likelihood estimates for the formula above correspond to the frequency count formula we have already described: To capture the uncertainty in the model parameters, we therefore simply need to store the counts of the observed transitions c(s i , s j , a k ).", "labels": [], "entities": []}, {"text": "Based on this model of uncertainty, we can numerically construct a confidence interval for the value of the optimal policy \u03c0 * . Instead of computing the value of the policy based on the maximum likelihood transition estimates\u02c6Testimates\u02c6 estimates\u02c6T M L = {\u02c6p{\u02c6p M L(s i |s j , a k )} k=1..p i,j=1..n , we generate a large number of transition matrices\u02c6Tmatrices\u02c6 matrices\u02c6T 1 , \u02c6 T 1 , ...", "labels": [], "entities": []}, {"text": "\u02c6 Tm by sampling from the Dirichlet distributions corresponding to the counts observed in the training data (in the experiments reported in this paper, we used m = 1000).", "labels": [], "entities": [{"text": "Tm", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9486547708511353}]}, {"text": "We then compute the value of the optimal policy \u03c0 * in each of these models .m . Finally, we numerically construct the 95% confidence interval for the value function based on the resulting value estimates: the bounds for the confidence interval are set at the lowest and highest 2.5 percentile of the resulting distribution of the values for the optimal policy {V \u03c0 * ( \u02c6 Ti )} i=1..m . The algorithm is outlined below: 1.", "labels": [], "entities": []}, {"text": "compute transition counts from the training set: 2.", "labels": [], "entities": []}, {"text": "compute maximum likelihood estimates for transition probability matrix: 3.", "labels": [], "entities": []}, {"text": "use dynamic programming to compute the optimal policy \u03c0 * for mode\u00ee .m , using the Dirichlet distribution for each row: 5.", "labels": [], "entities": []}, {"text": "evaluate the optimal policy \u03c0 * in each of these m models, and obtain V \u03c0 * ( \u02c6 Ti ) 6.", "labels": [], "entities": []}, {"text": "numerically build the 95% confidence interval for V \u03c0 * from these estimates.", "labels": [], "entities": []}, {"text": "To summarize, the central idea is to take into account the reliability of the transition probability estimates and construct a confidence interval for the expected cumulative reward for the learned policy.", "labels": [], "entities": []}, {"text": "In the standard approach, we would compute an estimate for the expected cumulative reward, by simply using the transition probabilities derived from the training set.", "labels": [], "entities": []}, {"text": "Note that these transition probabilities are simply estimates which are more or less accurate, depending on how much data is available.", "labels": [], "entities": []}, {"text": "The proposed methodology does not fully trust these estimates, and asks the question: given that the real world (i.e. real transition probabilities) might actually be a bit different than we think it is, how well can we expect the learned policy to perform?", "labels": [], "entities": []}, {"text": "Note that the confidence interval we construct, and therefore the conclusions we draw, are with respect to the policy learned from the current estimates, i.e. from the current training set.", "labels": [], "entities": []}, {"text": "If more data becomes available, a different optimal policy might emerge, about which we cannot say much.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Feature Comparison Results", "labels": [], "entities": []}, {"text": " Table 3: Confidence Intervals with complete dataset", "labels": [], "entities": [{"text": "Confidence Intervals", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8446964621543884}]}]}