{"title": [{"text": "A Case for Shorter Queries, and Helping Users Create Them", "labels": [], "entities": []}], "abstractContent": [{"text": "Information retrieval systems are frequently required to handle long queries.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7433554828166962}]}, {"text": "Simply using all terms in the query or relying on the underlying retrieval model to appropriately weight terms often leads to ineffective retrieval.", "labels": [], "entities": []}, {"text": "We show that rewriting the query to aversion that comprises a small subset of appropriate terms from the original query greatly improves effectiveness.", "labels": [], "entities": []}, {"text": "Targeting a demonstrated potential improvement of almost 50% on some difficult TREC queries and their associated collections, we develop a suite of automatic techniques to rewrite queries and study their characteristics.", "labels": [], "entities": []}, {"text": "We show that the shortcomings of automatic methods can be ameliorated by some simple user interaction, and report results that are on average 25% better than the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Query expansion has long been a focus of information retrieval research.", "labels": [], "entities": [{"text": "Query expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8670378625392914}, {"text": "information retrieval", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8034396469593048}]}, {"text": "Given an arbitrary short query, the goal was to find and include additional related and suitably-weighted terms to the original query to produce a more effective version.", "labels": [], "entities": []}, {"text": "In this paper we focus on a complementary problem -query re-writing.", "labels": [], "entities": []}, {"text": "Given along query we explore whether there is utility in modifying it to a more concise version such that the original information need is still expressed.", "labels": [], "entities": []}, {"text": "The Y!Q beta 1 search engine allows users to select large portions of text from documents and issue them as queries.", "labels": [], "entities": [{"text": "Y!Q beta 1 search engine", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8774697950908116}]}, {"text": "The search engine is designed to encourage users to submit long queries such as this example from the website \"I need to know the gas mileage for my Audi A8 2004 model\".", "labels": [], "entities": []}, {"text": "The motivation for encouraging this type of querying is that longer queries would provide more information in the form of context (), and this additional information could be leveraged to provide a better search experience.", "labels": [], "entities": []}, {"text": "However, handling such long queries is a challenge.", "labels": [], "entities": []}, {"text": "The use of all the terms from the user's input can rapidly narrow down the set of matching documents, especially if a boolean retrieval model is adopted.", "labels": [], "entities": []}, {"text": "While one would expect the underlying retrieval model to appropriately assign weights to different terms in the query and return only relevant content, it is widely acknowledged that models fail due to a variety of reasons (), and are not suited to tackle every possible query.", "labels": [], "entities": []}, {"text": "Recently, there has been great interest in personalized search, where the query is modified based on a user's profile.", "labels": [], "entities": []}, {"text": "The profile usually consists of documents previously viewed, web sites recently visited, e-mail correspondence and soon.", "labels": [], "entities": []}, {"text": "Common procedures for using this large amount of information usually involve creating huge query vectors with some sort of term-weighting mechanism to favor different portions of the profile.", "labels": [], "entities": []}, {"text": "The queries used in the TREC ad-hoc tracks consist of title, description and narrative sections, of progressively increasing length.", "labels": [], "entities": []}, {"text": "The title, of length ranging from a single term to four terms is considered a concise query, while the description is considered a longer version of the title expressing the same information need.", "labels": [], "entities": []}, {"text": "Almost all research on the TREC ad-hoc retrieval track reports results using only the title portion as the query, and a combination of the title and description as a separate query.", "labels": [], "entities": [{"text": "TREC ad-hoc retrieval track", "start_pos": 27, "end_pos": 54, "type": "DATASET", "confidence": 0.6655402034521103}]}, {"text": "Most reported results show that the latter is more effective than the former, though in the case of some hard collections the opposite is true.", "labels": [], "entities": []}, {"text": "However, as we shall show later, there is tremendous scope for improvement.", "labels": [], "entities": []}, {"text": "Formulating a shorter query from the description can lead to significant improvements in performance.", "labels": [], "entities": []}, {"text": "In the light of the above, we believe there is great utility in creating query-rewriting mechanisms for handling long queries.", "labels": [], "entities": []}, {"text": "This paper is organized in the following way.", "labels": [], "entities": []}, {"text": "We start with some examples and explore ways by which we can create concise high-quality reformulations of long queries in Section 2.", "labels": [], "entities": []}, {"text": "We describe our baseline system in Section 3 and motivate our investigations with experiments in Section 4.", "labels": [], "entities": []}, {"text": "Since automatic methods have shortfalls, we present a procedure in Section 5 to involve users in selecting a good shorter query from a small selection of alternatives.", "labels": [], "entities": []}, {"text": "We report and discuss the results of this approach in Section 6.", "labels": [], "entities": []}, {"text": "Related work is presented in Section 7.", "labels": [], "entities": []}, {"text": "We wrap up with conclusions and future directions in Section 8.", "labels": [], "entities": [{"text": "Section 8", "start_pos": 53, "end_pos": 62, "type": "TASK", "confidence": 0.5516954660415649}]}], "datasetContent": [{"text": "We used version 2.3.2 of the Indri search engine, developed as part of the Lemur 2 project.", "labels": [], "entities": [{"text": "Indri search engine", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.8541629513104757}]}, {"text": "While the inference network-based retrieval framework of Indri permits the use of structured queries, the use of language modeling techniques provides better estimates of probabilities for query evaluation.", "labels": [], "entities": []}, {"text": "The pseudo-relevance feedback mechanism we used is based on relevance models).", "labels": [], "entities": []}, {"text": "To extract named entities from the queries, we used BBN Identifinder ( document AQUAINT collection.", "labels": [], "entities": [{"text": "BBN Identifinder", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.8057666420936584}]}, {"text": "All the documents were from English newswire.", "labels": [], "entities": [{"text": "English newswire", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.9293306767940521}]}, {"text": "We chose these collections because they and their associated queries are known to be hard, and hence present a challenging environment.", "labels": [], "entities": []}, {"text": "We stemmed the collections using the Krovetz stemmer provided as part of Indri, and used a manually-created stoplist of twenty terms (a, an, and, are, at, as, be, for, in, is, it, of, on, or, that, the, to, was, with and what).", "labels": [], "entities": [{"text": "Indri", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.9479284882545471}]}, {"text": "To determine the best query selection procedure, we analyzed 163 queries from the Robust 2004 track, and used 30 and 50 queries from the 2004 and 2005 Robust tracks respectively for evaluation and user studies.", "labels": [], "entities": [{"text": "Robust 2004 track", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.8953118324279785}]}, {"text": "For all systems, we report mean average precision (MAP) and geometric mean average precision (GMAP).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 27, "end_pos": 55, "type": "METRIC", "confidence": 0.9349963665008545}, {"text": "geometric mean average precision (GMAP)", "start_pos": 60, "end_pos": 99, "type": "METRIC", "confidence": 0.8543853759765625}]}, {"text": "MAP is the most widely used measure in Information Retrieval.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.8027937710285187}]}, {"text": "While precision is the fraction of the retrieved documents that are relevant, average precision (AP) is a single value obtained by averaging the precision values at each new relevant document observed.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9989346861839294}, {"text": "average precision (AP)", "start_pos": 78, "end_pos": 100, "type": "METRIC", "confidence": 0.8964074611663818}, {"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.987857460975647}]}, {"text": "MAP is the arithmetic mean of the APs of a set of queries.", "labels": [], "entities": []}, {"text": "Similarly, GMAP is the geometric mean of the APs of a set of queries.", "labels": [], "entities": []}, {"text": "The GMAP measure is more indicative of performance across an entire set of queries.", "labels": [], "entities": []}, {"text": "MAP can be skewed by the presence of a few well-performing queries, and hence is not as good a measure as GMAP from the perspective of measure comprehensive performance.", "labels": [], "entities": []}, {"text": "We first ran two baseline experiments to record the quality of the available long query and the shorter version.", "labels": [], "entities": []}, {"text": "As mentioned in Section 1, we used the description and title sections of each TREC query as surrogates for the long and short versions respectively of a query.", "labels": [], "entities": []}, {"text": "The results are presented in the first two rows, Baseline and Pseudo-relevance Feedback (PRF), of.", "labels": [], "entities": [{"text": "Pseudo-relevance Feedback (PRF)", "start_pos": 62, "end_pos": 93, "type": "METRIC", "confidence": 0.9153206706047058}]}, {"text": "Measured in terms of MAP and GMAP (Section3), using just the title results in better performance than using the description.", "labels": [], "entities": [{"text": "MAP", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.8572577834129333}]}, {"text": "This clearly indicates the existence of terms in the description that while elaborating an information need hurt retrieval performance.", "labels": [], "entities": []}, {"text": "The result of using pseudo-relevance feedback (PRF) on both the title and description show moderate gains -a known fact about this particular collection and associated train-  To show the potential and utility of query rewriting, we first present results that show the upper bound on performance that can obtained by doing so.", "labels": [], "entities": []}, {"text": "We ran retrieval experiments with every combination of query terms.", "labels": [], "entities": []}, {"text": "For a query of length n, there are 2 n combinations.", "labels": [], "entities": []}, {"text": "We limited our experiments to queries of length n \u2264 12.", "labels": [], "entities": []}, {"text": "Selecting the performance obtained by the best sub-query of each query revealed an upper bound in performance almost 50% better than the baseline.", "labels": [], "entities": []}, {"text": "To evaluate the automatic sub-query selection procedures developed in Section 2, we performed retrieval experiments using the sub-queries selected using them.", "labels": [], "entities": []}, {"text": "The results, which are presented in Table 3, show that the automatic sub-query selection process was a failure.", "labels": [], "entities": []}, {"text": "The results of automatic selection were worse than even the baseline, and there was no significant difference between using any of the different sub-query selection procedures.", "labels": [], "entities": []}, {"text": "The failure of the automatic techniques could be attributed to the fact that we were working with the assumption that term co-occurrence could be used to model a user's information need.", "labels": [], "entities": []}, {"text": "To see if there was any general utility in using the procedures to select sub-queries, we selected the best-performing sub-query from the top 10 ranked by each selection procedure.", "labels": [], "entities": []}, {"text": "While the effectiveness in each case as measured by MAP is not close to the best possible MAP, 0.342, they are all significantly better than the baseline of 0.243.", "labels": [], "entities": [{"text": "MAP", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.6050829291343689}, {"text": "MAP", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9640525579452515}]}, {"text": "We conducted an exploratory study with five participants -four of them were graduate students in computer science while the fifth had a background in the social sciences and was reasonably proficient in the use of computers and internet search engines.", "labels": [], "entities": []}, {"text": "The participants worked with 30 queries from Robust 2004, and 50 from Robust 2005 3 . The baseline values reported are automatic runs with the description as the query.", "labels": [], "entities": [{"text": "Robust 2004", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9116055965423584}, {"text": "Robust 2005 3", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9273836414019266}]}, {"text": "shows that all five participants 4 were able to choose sub-queries that led to an improvement in performance over the baseline (TREC title query only).", "labels": [], "entities": []}, {"text": "This improvement is not only on MAP but also on GMAP, indicating that user interaction helped improve a wide spectrum of queries.", "labels": [], "entities": [{"text": "MAP", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.7443397045135498}]}, {"text": "Most notable were the improvements in P@5 and P@10.", "labels": [], "entities": [{"text": "P@5", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.8745390574137369}]}, {"text": "This attested to the fact that the interaction technique we explored was precision-enhancing.", "labels": [], "entities": [{"text": "precision-enhancing", "start_pos": 73, "end_pos": 92, "type": "METRIC", "confidence": 0.9947782754898071}]}, {"text": "Another interesting result, from # sub-queries selected was that participants were able to decide in a large number of cases that re-writing was either not useful fora query, or that none of the options presented to them were better.", "labels": [], "entities": []}, {"text": "Showing context appears to have helped.: # Queries refers to the number of queries that were presented to the participant while # sub-queries selected refers to the number of queries for which the participant chose a sub-query.", "labels": [], "entities": []}, {"text": "All scores including upper bounds were calculated only considering the queries for which the participant selected a sub-query.", "labels": [], "entities": []}, {"text": "An entry in bold means that the improvement in MAP is statistically significant.", "labels": [], "entities": [{"text": "MAP", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.5527588725090027}]}, {"text": "Statistical significance was measured using a paired t-test, with \u03b1 set to 0.05.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.5270975828170776}]}], "tableCaptions": [{"text": " Table 1: The results of using all possible subsets (ex- cluding singletons) of the original query as queries.", "labels": [], "entities": []}, {"text": " Table 2: Results across 163 training queries on the  Robust 2004 collection. Using the best sub-query  results in almost 50% improvement over the baseline", "labels": [], "entities": [{"text": "Robust 2004 collection", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.932127058506012}]}, {"text": " Table 3: Score of the highest rank sub-query by var- ious measures.", "labels": [], "entities": [{"text": "Score", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9785584211349487}]}, {"text": " Table 4: Score of the best sub-query in the top 10  ranked by various measures", "labels": [], "entities": [{"text": "Score", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9547332525253296}]}, {"text": " Table 5: Number of candidates from top 10 that ex- ceeded the baseline", "labels": [], "entities": []}, {"text": " Table 7: # Queries refers to the number of queries that were presented to the participant while # sub-queries  selected refers to the number of queries for which the participant chose a sub-query. All scores including  upper bounds were calculated only considering the queries for which the participant selected a sub-query.  An entry in bold means that the improvement in MAP is statistically significant. Statistical significance was  measured using a paired t-test, with \u03b1 set to 0.05.", "labels": [], "entities": [{"text": "MAP", "start_pos": 374, "end_pos": 377, "type": "METRIC", "confidence": 0.6239984631538391}, {"text": "Statistical significance", "start_pos": 408, "end_pos": 432, "type": "METRIC", "confidence": 0.9006784856319427}]}]}