{"title": [], "abstractContent": [{"text": "This paper presents a maximum entropy machine translation system using a minimal set of translation blocks (phrase-pairs).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6975084245204926}]}, {"text": "While recent phrase-based statistical machine translation (SMT) systems achieve significant improvement over the original source-channel statistical translation models, they 1) use a large inventory of blocks which have significant overlap and 2) limit the use of training to just a few parameters (on the order of ten).", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 13, "end_pos": 63, "type": "TASK", "confidence": 0.732850044965744}]}, {"text": "In contrast , we show that our proposed minimalist system (DTM2) achieves equal or better performance by 1) recasting the translation problem in the traditional statistical modeling approach using blocks with no overlap and 2) relying on training most system parameters (on the order of millions or larger).", "labels": [], "entities": []}, {"text": "The new model is a direct translation model (DTM) formulation which allows easy integration of addi-tional/alternative views of both source and target sentences such as segmentation fora source language such as Arabic, part-of-speech of both source and target, etc.", "labels": [], "entities": []}, {"text": "We show improvements over a state-of-the-art phrase-based decoder in Arabic-English translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation takes a source sequence, S = [s 1 s 2 . .", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7321515679359436}]}, {"text": "s K ], and generates a target sequence, T * = [t 1 t 2 . .", "labels": [], "entities": []}, {"text": "t L ], by finding the most likely translation given by: T * = arg max T p(T |S).", "labels": [], "entities": [{"text": "arg max T p", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9595353752374649}]}], "datasetContent": [{"text": "The UN parallel corpus and the LDC news corpora released as training data for the NIST MT06 evaluation are used for all evaluations presented in this paper.", "labels": [], "entities": [{"text": "UN parallel corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8630942106246948}, {"text": "LDC news corpora", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.8928038080533346}, {"text": "NIST MT06 evaluation", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.775145947933197}]}, {"text": "A variety of test corpora are now available and we use MT03 as development test data, and test results are presented on MT05.", "labels": [], "entities": [{"text": "MT03", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9654766917228699}, {"text": "MT05", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.9841794371604919}]}, {"text": "Results obtained on MT06 are from a blind evaluation.", "labels": [], "entities": [{"text": "MT06", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.6225080490112305}]}, {"text": "For ArabicEnglish, the NIST MT06 training data contains 3.7M sentence pairs from the UN from 1993-2002 and 100K sentences pairs from news sources.", "labels": [], "entities": [{"text": "NIST MT06 training data", "start_pos": 23, "end_pos": 46, "type": "DATASET", "confidence": 0.928836464881897}]}, {"text": "This represents the universe of training data, but for each test set we sample this corpus to train efficiently while also observing slight gains in performance.", "labels": [], "entities": []}, {"text": "The training universe is time sorted and the most recent corpora are sampled first.", "labels": [], "entities": []}, {"text": "Then fora given test set, we obtain the first 20 instances of n-grams from the test that occur in the training universe and the resulting sampled sentences then form the training sample.", "labels": [], "entities": []}, {"text": "The contribution of the sampling technique is to produce a smaller training corpus which reduces the computational load; however, the sampling of the universe of sentences can be viewed as test set domain adaptation which improves performance and is not strictly done due to computational limitations 2 . The 5-gram language model is trained from the English Gigaword corpus and the English portion of the parallel corpus used in the translation model training.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 351, "end_pos": 374, "type": "DATASET", "confidence": 0.7194125056266785}]}, {"text": "The baseline decoder is a phrase-based decoder that employs n-m blocks and uses the same test set specific training corpus described above.", "labels": [], "entities": []}, {"text": "There are 15 individual feature types utilized in the system, but in order to be brief we present the results by feature groups (see", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Bleu scores on MT03-MT06.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9849249720573425}, {"text": "MT03-MT06", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.9491434693336487}]}]}