{"title": [{"text": "Efficient Computation of Entropy Gradient for Semi-Supervised Conditional Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "Entropy regularization is a straightforward and successful method of semi-supervised learning that augments the traditional conditional likelihood objective function with an additional term that aims to minimize the predicted label entropy on unlabeled data.", "labels": [], "entities": [{"text": "Entropy regularization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.925887405872345}]}, {"text": "It has previously been demonstrated to provide positive results in linear-chain CRFs, but the published method for calculating the entropy gradient requires significantly more computation than supervised CRF training.", "labels": [], "entities": []}, {"text": "This paper presents anew derivation and dynamic program for calculating the entropy gradient that is significantly more efficient-having the same asymptotic time complexity as supervised CRF training.", "labels": [], "entities": []}, {"text": "We also present efficient generalizations of this method for calculating the label entropy of all sub-sequences, which is useful for active learning, among other applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semi-supervised learning is of growing importance in machine learning and NLP (.", "labels": [], "entities": []}, {"text": "Conditional random fields (CRFs) () are an appealing target for semi-supervised learning because they achieve state-of-the-art performance across abroad spectrum of sequence labeling tasks, and yet, like many other machine learning methods, training them by supervised learning typically requires large annotated data sets.", "labels": [], "entities": []}, {"text": "Entropy regularization (ER) is a method of semisupervised learning first proposed for classification tasks ().", "labels": [], "entities": [{"text": "Entropy regularization (ER)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8294673323631286}, {"text": "classification tasks", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.8941543400287628}]}, {"text": "In addition to maximizing conditional likelihood of the available labels, ER also aims to minimize the entropy of the predicted label distribution on unlabeled data.", "labels": [], "entities": [{"text": "ER", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9639974236488342}]}, {"text": "By insisting on peaked, confident predictions, ER guides the decision boundary away from dense regions of input space.", "labels": [], "entities": []}, {"text": "It is simple and compelling-no preclustering, no \"auxiliary functions,\" tuning of only one meta-parameter and it is discriminative.", "labels": [], "entities": []}, {"text": "apply this method to linearchain CRFs and demonstrate encouraging accuracy improvements on a gene-name-tagging task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9981328845024109}]}, {"text": "However, the method they present for calculating the gradient of the entropy takes substantially greater time than the traditional supervised-only gradient.", "labels": [], "entities": []}, {"text": "Whereas supervised training requires only classic forward/backward, taking time O(ns 2 ) (sequence length times the square of the number of labels), their training method takes O(n 2 s 3 )-a factor of O(ns) more.", "labels": [], "entities": [{"text": "O", "start_pos": 177, "end_pos": 178, "type": "METRIC", "confidence": 0.9582851529121399}]}, {"text": "This greatly reduces the practicality of using large amounts of unlabeled data, which is exactly the desired use-case.", "labels": [], "entities": []}, {"text": "This paper presents anew, more efficient entropy gradient derivation and dynamic program that has the same asymptotic time complexity as the gradient for traditional CRF training, O(ns 2 ).", "labels": [], "entities": []}, {"text": "In order to describe this calculation, the paper introduces the concept of subsequence constrained entropy-the entropy of a CRF for an observed data sequence when part of the label sequence is fixed.", "labels": [], "entities": []}, {"text": "These methods will allow training on larger unannotated data set sizes than previously possible and support active learning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}