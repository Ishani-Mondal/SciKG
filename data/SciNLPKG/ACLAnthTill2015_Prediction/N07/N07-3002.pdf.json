{"title": [{"text": "Learning Structured Classifiers for Statistical Dependency Parsing", "labels": [], "entities": [{"text": "Statistical Dependency Parsing", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.7021362781524658}]}], "abstractContent": [{"text": "My research is focused on developing machine learning algorithms for inferring dependency parsers from language data.", "labels": [], "entities": []}, {"text": "By investigating several approaches I have developed a unifying perspective that allows me to share advances between both probabilistic and non-probabilistic methods.", "labels": [], "entities": []}, {"text": "First, I describe a generative technique that uses a strictly lexicalised parsing model, where all the parameters are based on words and do not use any part-of-speech (POS) tags nor grammatical categories.", "labels": [], "entities": []}, {"text": "Then, I incorporate two ideas from probabilistic parsing-word similarity smoothing and local estimation-to improve the large margin approach.", "labels": [], "entities": [{"text": "parsing-word similarity smoothing", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.8398738900820414}]}, {"text": "Finally , I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods.", "labels": [], "entities": [{"text": "training dependency parsers", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.738228996594747}]}], "introductionContent": [{"text": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data).", "labels": [], "entities": []}, {"text": "Most of the early work in this area was based on postulating generative probability models of language that included parse structures.", "labels": [], "entities": []}, {"text": "Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems).", "labels": [], "entities": []}, {"text": "Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. \"maximum entropy\") to be applied).", "labels": [], "entities": []}, {"text": "Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (), which demonstrates the state of the art performance in English dependency parsing.", "labels": [], "entities": [{"text": "conditional parsing", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.5491625070571899}, {"text": "English dependency parsing", "start_pos": 172, "end_pos": 198, "type": "TASK", "confidence": 0.5726807514826456}]}, {"text": "Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches.", "labels": [], "entities": []}, {"text": "For example, smoothing methods have played a central role in probabilistic approaches, and yet they are not being used in current large margin training algorithms.", "labels": [], "entities": []}, {"text": "Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach-the \"structured margin loss\")-is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.", "labels": [], "entities": []}, {"text": "I have addressed both of these issues, as well as others in my work.", "labels": [], "entities": []}, {"text": ", that obtains the highest \"score\".", "labels": [], "entities": []}, {"text": "In particular, I follow Eisner (1996) and and assume that the score of a complete spanning tree fora given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair).", "labels": [], "entities": []}, {"text": "In which case, the parsing problem reduces to 7 6 . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models () as well as non-probabilistic models).", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9762338995933533}, {"text": "dependency parsing", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7561211884021759}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison with State of the Art (Depen- dency Accuracy)  Model  Chinese English  Yamada&Matsumoto 03  - 90.3  Nivre&Scholz 04  - 87.3  Wang et al. 05 (Sec. 3)  79.9*  - McDonald et al. 05  - 90.9  McDonald&Pereira 06  82.5*  91.5  Corston-Oliver et al. 06  73.3u  90.8  Structured  86.6*  89.3  Boosting (Sec. 5)  77.6u", "labels": [], "entities": []}]}