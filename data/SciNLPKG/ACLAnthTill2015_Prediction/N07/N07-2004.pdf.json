{"title": [{"text": "JOINT VERSUS INDEPENDENT PHONOLOGICAL FEATURE MODELS WITHIN CRF PHONE RECOGNITION", "labels": [], "entities": [{"text": "VERSUS", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.7253696322441101}, {"text": "INDEPENDENT PHONOLOGICAL FEATURE MODELS WITHIN CRF PHONE RECOGNITION", "start_pos": 13, "end_pos": 81, "type": "METRIC", "confidence": 0.8012992218136787}]}], "abstractContent": [{"text": "We compare the effect of joint modeling of phonological features to independent feature detectors in a Conditional Random Fields framework.", "labels": [], "entities": []}, {"text": "Joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes.", "labels": [], "entities": []}, {"text": "We find that joint modeling provides superior performance to the independent models on the TIMIT phone recognition task.", "labels": [], "entities": [{"text": "TIMIT phone recognition task", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.7959796637296677}]}, {"text": "We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent.", "labels": [], "entities": [{"text": "ASR", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9803563356399536}]}], "introductionContent": [{"text": "Phonological features have received attention as a linguistically-based representation for sub-word information in automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.6393876175085703}]}, {"text": "These sub-phonetic features allow fora more refined representation of speech by allowing for temporal desynchronization between articulators, and help account for some phonological changes common in spontaneous speech, such as devoicing.", "labels": [], "entities": []}, {"text": "A number of methods have been developed for detecting acoustic phonological features and related acoustic landmarks directly from data using Multi-Layer Perceptrons, Support Vector Machines (), or Hidden Markov Models ().", "labels": [], "entities": []}, {"text": "These techniques typically assume that acoustic phonological feature events are independent for ease of modeling.", "labels": [], "entities": []}, {"text": "In one study that broke the independence assumption (), the investigators developed conditional detectors: MLP detectors of acoustic phonological features that are hierarchically dependent on a different phonological class.", "labels": [], "entities": []}, {"text": "In) it was shown that such a conditional training of detectors tended to have correlated frame errors, and that improvements in detection could be obtained by training joint detectors.", "labels": [], "entities": []}, {"text": "For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class.", "labels": [], "entities": []}, {"text": "This was shown only for frame-level classification rather than phone recognition.", "labels": [], "entities": [{"text": "frame-level classification", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7634976208209991}, {"text": "phone recognition", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7895800471305847}]}, {"text": "Posterior estimates of phonological feature classes, as in, particularly those derived from MLPs, have been used as input to), Dynamic Bayesian Networks (DBNs) (, and Conditional Random Fields (CRFs)).", "labels": [], "entities": []}, {"text": "Here we evaluate phonological feature detectors created from MLP phone posterior estimators (joint feature models) rather than the independently trained MLP feature detectors used in previous work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the TIMIT speech corpus for all training and testing (.", "labels": [], "entities": [{"text": "TIMIT speech corpus", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.8834896485010783}]}, {"text": "The acoustic data is manually labeled at the phonetic level, and we propagate this phonetic label information to every frame of data.", "labels": [], "entities": []}, {"text": "For the feature analyses, we employ a lookup table that defines each phone in terms of 8 feature classes, as shown in.", "labels": [], "entities": []}, {"text": "We extract acoustic features in the form of 12th order PLP features plus delta coefficients.", "labels": [], "entities": []}, {"text": "We then use these as inputs to several sets of neural networks using the ICSI QuickNet MLP neural network software), with the 39 acoustic features as input, a varying number of phone or feature class posteriors as output, and 1000 hidden nodes.", "labels": [], "entities": [{"text": "ICSI QuickNet MLP neural network", "start_pos": 73, "end_pos": 105, "type": "DATASET", "confidence": 0.9150083303451538}]}], "tableCaptions": [{"text": " Table 2: Results for Exp. 1: Phone and feature pos- teriors as input to the CRF phone recognition", "labels": [], "entities": [{"text": "CRF phone recognition", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.6689001123110453}]}, {"text": " Table 3: Results of Exp. 2: Removing feature  classes from the input", "labels": [], "entities": []}, {"text": " Table 4: Effect of removing each feature class on  recognition accuracy of vowels and consonants", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9539564847946167}]}]}