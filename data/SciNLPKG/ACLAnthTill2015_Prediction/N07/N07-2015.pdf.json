{"title": [{"text": "Are Very Large N-best Lists Useful for SMT?", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9959546327590942}]}], "abstractContent": [{"text": "This paper describes an efficient method to extract large n-best lists from a word graph produced by a statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.6287409861882528}]}, {"text": "The extraction is based on the k shortest paths algorithm which is efficient even for very large k.", "labels": [], "entities": []}, {"text": "We show that, although we can generate large amounts of distinct translation hypotheses , these numerous candidates are notable to significantly improve overall system performance.", "labels": [], "entities": []}, {"text": "We conclude that large n-best lists would benefit from better discriminating models.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper investigates the properties of large nbest lists in the context of statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 78, "end_pos": 115, "type": "TASK", "confidence": 0.8116610000530878}]}, {"text": "We present a method that allows for fast extraction of very large n-best lists based on the k shortest paths algorithm by).", "labels": [], "entities": []}, {"text": "We will argue that, despite being able to generate a much larger amount of hypotheses than previously reported in the literature, there is no significant gain of such a method in terms of translation quality.", "labels": [], "entities": []}, {"text": "In recent years, phrase-based approaches evolved as the dominating method for feasible machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7235308736562729}]}, {"text": "Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (.", "labels": [], "entities": []}, {"text": "As a by-product of the decoding process, one can extract n-best translations from a word graph and use these fully generated hypotheses for additional reranking.", "labels": [], "entities": []}, {"text": "In the past, several groups report on using n-best lists with n ranging from 1 000 to 10 000.", "labels": [], "entities": []}, {"text": "The advantage of n-best reranking is clear: we can apply complex reranking techniques, based e.g. on syntactic analyses of the candidates or using huge additional language models, since the whole sentence is already generated.", "labels": [], "entities": []}, {"text": "During the generation process, these models would either need hard-to-implement algorithms or large memory requirements.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments in this section are carried out on nbest lists with n going up to 100 000.", "labels": [], "entities": []}, {"text": "We will show that, although we are capable of generating this large amount of hypotheses, the overall performance does not seem to improve significantly beyond a certain threshold.", "labels": [], "entities": []}, {"text": "Or to put it simple: although we generate lots of hypotheses, most of them are not very useful.", "labels": [], "entities": []}, {"text": "As experimental background, we choose the large data track of the Chinese-to-English NIST task, since the length of the sentences and the large vocabulary of the task allow for large n-best lists.", "labels": [], "entities": []}, {"text": "For smaller tasks, e.g. the IWSLT campaign, the domain is rather limited such that it does not make sense to generate lists reaching beyond several thousand hypotheses.", "labels": [], "entities": [{"text": "IWSLT campaign", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.6731747090816498}]}, {"text": "As development data, we use the 2002 eval set, whereas for test, the 2005 eval set is chosen.", "labels": [], "entities": []}, {"text": "The corpus statistics are shown in: Corpus statistics for the Chinese-English NIST MT task.", "labels": [], "entities": [{"text": "NIST MT task", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.5510286291440328}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics for the Chinese-English  NIST MT task.", "labels": [], "entities": [{"text": "NIST MT task", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.615772674481074}]}, {"text": " Table 2: Dev BLEU scores of oracle-best hypothe- ses based on minimum WER/PER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9849542379379272}, {"text": "WER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9816641807556152}, {"text": "PER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.6392045617103577}]}]}