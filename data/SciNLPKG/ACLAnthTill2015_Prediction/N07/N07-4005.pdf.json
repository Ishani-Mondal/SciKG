{"title": [{"text": "Automatic Segmentation and Summarization of Meeting Speech", "labels": [], "entities": [{"text": "Automatic Segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.4924646317958832}, {"text": "Summarization of Meeting Speech", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8617286682128906}]}], "abstractContent": [{"text": "AMI Meeting Facilitator is a system that performs topic segmentation and extractive sum-marisation.", "labels": [], "entities": [{"text": "AMI Meeting Facilitator", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.8658766349156698}, {"text": "topic segmentation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7806351780891418}]}, {"text": "It consists of three components: (1) a segmenter that divides a meeting into a number of locally coherent segments, (2) a summa-rizer that selects the most important utterances from the meeting transcripts. and (3) a compression component that removes the less important words from each utterance based on the degree of compression the user specied.", "labels": [], "entities": []}, {"text": "The goal of the AMI Meeting Facilitator is twofold: rst, we want to provide sucient visual aids for users to interpret what is going on in a recorded meeting; second, we want to support the development of downstream information retrieval and information extraction modules with the information about the topics and summaries in meeting segments.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 242, "end_pos": 264, "type": "TASK", "confidence": 0.7023424655199051}]}, {"text": "2 Component Description 2.1 Segmentation The AMI Meeting Segmenter is trained using a set of 50 meetings that are seperate from the input meeting.", "labels": [], "entities": []}, {"text": "We rst extract features from the audio and video recording of the input meeting in order to train the Maximum Entropy (Max-Ent) models for classifying topic boundaries and non-topic boundaries.", "labels": [], "entities": []}, {"text": "Then we test each utterance in the input meeting on the Segmenter to see if it is a topic boundary or not.", "labels": [], "entities": []}, {"text": "The features we use include the following ve categories: (1) Conversational Feature: These include a set of seven conversational features, including the amount of overlapping speech, the amount of silence between speaker segments, the level of similarity of speaker activity, the number of cue words, and the predictions of LCSEG (i.e., the lexical cohesion statistics, the estimated posterior probability, the predicted class).", "labels": [], "entities": []}, {"text": "(2) Lexical Feature: Each spurt is represented as a vector space of uni-grams, wherein a vector is 1 or 0 depending on whether the cue word appears in the spurt.", "labels": [], "entities": []}, {"text": "(3) Prosodic Feature: These include dialogue-act (DA) rate-of-speech, maximum F0 of the DA, mean energy of the DA, amount of silence in the DA, precedent and subsequent pauses, and duration of the DA.", "labels": [], "entities": [{"text": "F0", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9913500547409058}, {"text": "duration", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9983969330787659}]}, {"text": "(4) Motion Feature: These include the average magnitude of speaker movements, which is measured by the number of pixels changed, over the frames of 40 ms within the spurt.", "labels": [], "entities": []}, {"text": "(5) Contex-tual Feature: These include the dialogue act types and the speaker role (e.g., project manager , marketing expert).", "labels": [], "entities": []}, {"text": "In the dialogue act annotations , each dialogue act is classied as one of the 15 types.", "labels": [], "entities": []}, {"text": "2.2 Summarization The AMI summarizer is trained using a set of 98 scenario meetings.", "labels": [], "entities": [{"text": "AMI summarizer", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.5317323207855225}]}, {"text": "We train a support vector machine (SVM) on these meetings, using 26 features relating to the following categories: (1) Prosodic Features: These include dialogue-act (DA) rate-of-speech, maximum F0 of the DA, mean energy of the DA, amount of silence in the DA, precedent and subsequent pauses, 9", "labels": [], "entities": [{"text": "F0", "start_pos": 194, "end_pos": 196, "type": "METRIC", "confidence": 0.9781671762466431}]}], "introductionContent": [{"text": "AMI Meeting Facilitator is a system that performs topic segmentation and extractive summarisation.", "labels": [], "entities": [{"text": "AMI Meeting Facilitator", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.8786718845367432}, {"text": "topic segmentation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.773876428604126}, {"text": "extractive summarisation", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.8023552000522614}]}, {"text": "It consists of three components: (1) a segmenter that divides a meeting into a number of locally coherent segments, (2) a summarizer that selects the most important utterances from the meeting transcripts. and (3) a compression component that removes the less important words from each utterance based on the degree of compression the user specied.", "labels": [], "entities": []}, {"text": "The goal of the AMI Meeting Facilitator is two-fold: rst, we want to provide sucient visual aids for users to interpret what is going on in a recorded meeting; second, we want to support the development of downstream information retrieval and information extraction modules with the information about the topics and summaries in meeting segments.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 243, "end_pos": 265, "type": "TASK", "confidence": 0.6960898488759995}]}, {"text": "The AMI Meeting Segmenter is trained using a set of 50 meetings that are seperate from the input meeting.", "labels": [], "entities": [{"text": "AMI Meeting Segmenter", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.5752368370691935}]}, {"text": "We rst extract features from the audio and video recording of the input meeting in order to train the Maximum Entropy (MaxEnt) models for classifying topic boundaries and non-topic boundaries.", "labels": [], "entities": []}, {"text": "Then we test each utterance in the input meeting on the Segmenter to see if it is a topic boundary or not.", "labels": [], "entities": []}, {"text": "The features we use include the following ve categories: (1) Conversational Feature: These include a set of seven conversational features, including the amount of overlapping speech, the amount of silence between speaker segments, the level of similarity of speaker activity, the number of cue words, and the predictions of LCSEG (i.e., the lexical cohesion statistics, the estimated posterior probability, the predicted class).", "labels": [], "entities": []}, {"text": "After training the SVM, we test on each meeting of the 20 meeting test set in turn, ranking the dialogue acts from most probable to least probable in terms of being extract-worthy.", "labels": [], "entities": [{"text": "SVM", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8639823198318481}]}, {"text": "Such a ranking allows the user to create a summary of whatever length she desires.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}