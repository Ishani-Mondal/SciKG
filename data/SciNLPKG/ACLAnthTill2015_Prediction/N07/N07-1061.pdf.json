{"title": [{"text": "A Comparison of Pivot Methods for Phrase-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-based Statistical Machine Translation", "start_pos": 34, "end_pos": 78, "type": "TASK", "confidence": 0.7532289177179337}]}], "abstractContent": [{"text": "We compare two pivot strategies for phrase-based statistical machine translation (SMT), namely phrase translation and sentence translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 36, "end_pos": 86, "type": "TASK", "confidence": 0.7387064354760307}, {"text": "phrase translation", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8473365902900696}, {"text": "sentence translation", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.7614879906177521}]}, {"text": "The phrase translation strategy means that we directly construct a phrase translation table (phrase-table) of the source and target language pair from two phrase-tables; one constructed from the source language and English and one constructed from En-glish and the target language.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8672775030136108}, {"text": "phrase translation", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.706638902425766}]}, {"text": "We then use that phrase-table in a phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8860925436019897}]}, {"text": "The sentence translation strategy means that we first translate a source language sentence into n English sentences and then translate these n sentences into target language sentences separately.", "labels": [], "entities": [{"text": "sentence translation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7969698905944824}]}, {"text": "Then, we select the highest scoring sentence from these target sentences.", "labels": [], "entities": []}, {"text": "We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.9960005879402161}, {"text": "SMT", "start_pos": 148, "end_pos": 151, "type": "TASK", "confidence": 0.9845936298370361}]}, {"text": "The phrase translation strategy significantly outperformed the sentence translation strategy.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8271096348762512}, {"text": "sentence translation", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7377307862043381}]}, {"text": "Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9860923886299133}]}], "introductionContent": [{"text": "The rapid and steady progress in corpus-based machine translation has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (, which consists of 11 European languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7355943918228149}, {"text": "Europarl corpus", "start_pos": 229, "end_pos": 244, "type": "DATASET", "confidence": 0.9845825135707855}]}, {"text": "However, large parallel corpora do not exist for many language pairs.", "labels": [], "entities": []}, {"text": "For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora.", "labels": [], "entities": []}, {"text": "Much work has been done to overcome the lack of parallel corpora.", "labels": [], "entities": []}, {"text": "For example, propose mining the web to collect parallel corpora for low-density language pairs.", "labels": [], "entities": []}, {"text": "extract Japanese-English parallel sentences from a noisy-parallel corpus.", "labels": [], "entities": []}, {"text": "extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora.", "labels": [], "entities": []}, {"text": "Researchers can also make the best use of existing (small) parallel corpora.", "labels": [], "entities": []}, {"text": "For example, use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation.", "labels": [], "entities": []}, {"text": "use paraphrases to deal with unknown source language phrases to improve coverage and translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we focus on situations where no parallel corpus is available (except a few hundred parallel sentences for tuning parameters).", "labels": [], "entities": []}, {"text": "To tackle these extremely scarce training data situations, we propose using a pivot language (English) to bridge the source and target languages in translation.", "labels": [], "entities": []}, {"text": "We first translate source language sentences or phrases into English and then translate those English sentences or phrases into the target language, as described in Section 3.", "labels": [], "entities": []}, {"text": "We thus assume that there is a parallel corpus consisting of the source language and English as well as one consisting of English and the target language.", "labels": [], "entities": []}, {"text": "Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language.", "labels": [], "entities": []}, {"text": "In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 28, "end_pos": 78, "type": "TASK", "confidence": 0.7199577987194061}]}, {"text": "This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation () and consists of the Pharaoh decoder), SRILM), GIZA++, mkcls), Carmel, 1 and a phrase model training code.", "labels": [], "entities": [{"text": "NAACL/HLT workshop", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.7893524616956711}, {"text": "statistical machine translation", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.6537435253461202}, {"text": "SRILM", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.6607024669647217}]}], "datasetContent": [{"text": "We conducted controlled experiments using the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.9943574965000153}]}, {"text": "For each language pair described below, the Europarl corpus provides three We use a reranking strategy for the sentence translation strategy.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9915492832660675}, {"text": "sentence translation", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.752216637134552}]}, {"text": "We first obtain n 2 German sentences for each French sentence by applying two independently trained French-English and English-German SMT systems.", "labels": [], "entities": []}, {"text": "Each of the translated German sentences has the sixteen scores as described above.", "labels": [], "entities": []}, {"text": "12 are tuned against reference German sentences by performing minimum error rate training.", "labels": [], "entities": [{"text": "error rate", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.8568808734416962}]}, {"text": "These weights are in general different from those of the original French-English and English-German SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.8930714130401611}]}, {"text": "types of parallel corpora; the source languageEnglish, English-the target language, and the source language-the target language.", "labels": [], "entities": []}, {"text": "This means that we can directly train an SMT system using the source and target language parallel corpus as well as pivot SMT systems using English as the pivot language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9935500025749207}, {"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9272747039794922}]}, {"text": "We use the term Direct to denote directly trained SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9903125762939453}]}, {"text": "For each language pair, we compare four SMT systems; Direct, PhraseTrans, SntTrans15, and SntTrans1.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9837124347686768}]}], "tableCaptions": [{"text": " Table 1: BLEU scores and relative performance", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981222748756409}]}]}