{"title": [{"text": "Generating Case Markers in Machine Translation", "labels": [], "entities": [{"text": "Generating Case Markers", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.779259999593099}, {"text": "Machine Translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.677426666021347}]}], "abstractContent": [{"text": "We study the use of rich syntax-based statistical models for generating grammatical case for the purpose of machine translation from a language which does not indicate case explicitly (English) to a language with a rich system of surface case markers (Japanese).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7180536389350891}]}, {"text": "We propose an extension of n-best re-ranking as a method of integrating such models into a statistical MT system and show that this method substantially outperforms standard n-best re-ranking.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9058599472045898}]}, {"text": "Our best performing model achieves a statistically significant improvement over the baseline MT system according to the BLEU metric.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.983157217502594}, {"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9961726069450378}]}, {"text": "Human evaluation also confirms the results .", "labels": [], "entities": []}], "introductionContent": [{"text": "Generation of grammatical elements such as inflectional endings and case markers is an important component technology for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.8210234940052032}]}, {"text": "Statistical machine translation (SMT) systems, however, have not yet successfully incorporated components that generate grammatical elements in the target language.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8064913252989451}]}, {"text": "Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words, and rely on general-purpose phrasal translations and target language models to generate these elements (e.g.,).", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9322305917739868}]}, {"text": "However, since these grammatical elements in the target language often correspond to long-range dependencies and/or do not have any words corresponding in the source, they maybe difficult to model, and the output of an SMT system is often ungrammatical.", "labels": [], "entities": [{"text": "SMT", "start_pos": 219, "end_pos": 222, "type": "TASK", "confidence": 0.9890871047973633}]}, {"text": "For example, shows an output from our baseline English-to-Japanese SMT system on a sentence from a computer domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9113879799842834}]}, {"text": "The SMT system, trained on this domain, produces a natural lexical translation for the English word patch as correction program, and translates replace into passive voice, which is more appropriate in Japanese.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.969226598739624}]}, {"text": "However, there is a problem in the case marker assignment: the accusative marker wo, which was output by the SMT system, is completely inappropriate when the main verb is passive.", "labels": [], "entities": [{"text": "case marker assignment", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7081457376480103}, {"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9744154214859009}]}, {"text": "This type of mistake in case marker assignment is by no means isolated in our SMT system: a manual analysis showed that 16 out of 100 translations had mistakes solely in the assignment of case markers.", "labels": [], "entities": [{"text": "case marker assignment", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6872014800707499}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9906895160675049}]}, {"text": "A better model of case assignment could therefore improve the quality of an SMT system significantly.", "labels": [], "entities": [{"text": "case assignment", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.8233873546123505}, {"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9962299466133118}]}, {"text": "In this paper, we explore the use of a statistical model for case marker generation in Englishto-Japanese SMT.", "labels": [], "entities": [{"text": "case marker generation", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7916387915611267}, {"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.6944445371627808}]}, {"text": "Though we focus on the generation of case markers in this paper, there are many other surface grammatical phenomena that can be modeled in a similar way, so any SMT system dealing with morpho-syntactically divergent language pairs may benefit from a similar approach to modeling grammatical elements.", "labels": [], "entities": [{"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9919468760490417}]}, {"text": "Our model uses a rich set of syntactic features of both the source (English) and the target (Japanese) sentences, using context which is broader than that utilized by existing SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 176, "end_pos": 179, "type": "TASK", "confidence": 0.9868527054786682}]}, {"text": "We show that the use of such features results in very high case assignment quality and also leads to a notable improvement in MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.9889199137687683}]}, {"text": "Previous work has discussed the building of special-purpose classifiers which generate grammatical elements such as prepositions (, determiners and case markers () with an eye toward improving MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 193, "end_pos": 195, "type": "TASK", "confidence": 0.9962155222892761}]}, {"text": "How-ever, these components have not actually been integrated in an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9751185178756714}]}, {"text": "To our knowledge, this is the first work to integrate a grammatical element production model in an SMT system and to evaluate its impact in the context of end-toend MT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.990559458732605}, {"text": "MT", "start_pos": 165, "end_pos": 167, "type": "TASK", "confidence": 0.8453472256660461}]}, {"text": "A common approach of integrating new models with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system ().", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.8934270143508911}]}, {"text": "In this paper we propose an extension of the n-best re-ranking approach, where we expand n-best candidate lists with multiple case assignment variations, and define new feature functions on this expanded candidate set.", "labels": [], "entities": []}, {"text": "We show that expanding the n-best lists significantly outperforms standard n-best reranking.", "labels": [], "entities": []}, {"text": "We also show that integrating our case prediction model improves the quality of translation according to) and human evaluation.", "labels": [], "entities": [{"text": "case prediction", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8013461828231812}]}], "datasetContent": [{"text": "These results demonstrate that the proposed model is effective at improving the translation quality according to the BLEU score.", "labels": [], "entities": [{"text": "translation", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9521384239196777}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.997732400894165}]}, {"text": "In this section, we report the results of human evaluation to ensure that the improvements in BLEU lead to better translations according to human evaluators.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9937885403633118}, {"text": "translations", "start_pos": 114, "end_pos": 126, "type": "TASK", "confidence": 0.9510611891746521}]}, {"text": "We performed human evaluation on the 20best-10case (n=20, k=10) and 1best-40case (n=1, k=40) models against the baseline using our final test set, the test-2K data.", "labels": [], "entities": []}, {"text": "The performance in BLEU of these models on the full test-2K data was 35.53 for the baseline, 36.09 for the 1best-40case model, and 36.29 for the 20best-10case model, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9994757771492004}, {"text": "test-2K data", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.8083977401256561}]}, {"text": "In our human evaluation, two annotators were asked to evaluate a random set of 100 sentences for which the models being compared produced different translations.", "labels": [], "entities": []}, {"text": "The judges were asked to compare two translations, the baseline output from the original SMT system and the output chosen by the system augmented with the case marker generation component.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9865520596504211}]}, {"text": "Each judge was asked to run two separate evaluations along different evaluation criteria.", "labels": [], "entities": []}, {"text": "In the evaluation of fluency, the judges were asked to decide which translation is more readable/grammatical, ignoring the reference translation.", "labels": [], "entities": []}, {"text": "In the evaluation of adequacy, they were asked to judge which translation more correctly reflects the meaning of the reference translation.", "labels": [], "entities": []}, {"text": "In either setting, they were not given the source sentence.", "labels": [], "entities": []}, {"text": "summarizes the results of the evaluation of the 20best-10case model.", "labels": [], "entities": []}, {"text": "The table shows the results along two evaluation criteria separately, fluency on the left and adequacy on the right.", "labels": [], "entities": []}, {"text": "The evaluation results of Annotator #1 are shown in the columns, while those of Annotator #2 are in the rows.", "labels": [], "entities": []}, {"text": "Each grid in the table shows the number of sentences the annotators classified as the proposed system output better (S), the baseline system better (B) or the translations are of equal quality (E).", "labels": [], "entities": []}, {"text": "Along the diagonal (in boldface) are the judgments that were agreed on by the two annotators: both annotators judged the output of the proposed system to be more fluent in 27 translations, less fluent in 9 translations; they judged that our system output was more adequate in 17 translations and less adequate in 9 translations.", "labels": [], "entities": []}, {"text": "Our system output was thus judged better under both criteria, though according to a sign test, the improvement is statistically significant (p < .01) in fluency, but not inadequacy.", "labels": [], "entities": []}, {"text": "One of the reasons for this inconclusive result is that human evaluation maybe very difficult and can be unreliable when evaluating very different translation candidates, which happens often when comparing the results of models that consider n-best candidates where n>1, as is the case with the 20best-10case model.", "labels": [], "entities": []}, {"text": "Results of human evaluation comparing 20best-10case vs. baseline.", "labels": [], "entities": []}, {"text": "S: proposed system is better; B: baseline is better; E: of equal quality we can see that the raw agreement rate between the two annotators (i.e., number of agreed judgments overall judgments) is only 63% (27+9+27 /100) in fluency and 62% (17+9+36/100) inadequacy.", "labels": [], "entities": []}, {"text": "We therefore performed an additional human evaluation where translations being compared differ only in case markers: the baseline vs. the 1best-40case model output.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "This evaluation has a higher rate of agreement, 74% for fluency and 71% for adequacy, indicating that comparing two translations that differ only minimally (i.e., in case markers) is more reliable.", "labels": [], "entities": [{"text": "agreement", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9975994229316711}]}, {"text": "The improvements achieved by our model are statistically significant in both fluency and adequacy according to a sign test; in particular, it is remarkable that on 42 sentences, the judges agreed that our system was better in fluency, and there were no sentences on which the judges agreed that our system caused degradation.", "labels": [], "entities": []}, {"text": "This means that the proposed system, when choosing among candidates differing only in case markers, can improve the quality of MT output in an extremely precise manner, i.e. making improvements without causing degradations.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9927259683609009}]}], "tableCaptions": [{"text": " Table 2. We will refer to this table as we de- scribe our experiments in later sections.", "labels": [], "entities": []}, {"text": " Table 2: Data set characteristics", "labels": [], "entities": []}, {"text": " Table 2. As we show in Section 5, this re-ranking  method did not result in good performance.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy (%) and BLEU score for case  prediction when given correct context (reference  translations) on the 5K-test set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995115995407104}, {"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9998329877853394}, {"text": "case  prediction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.8547438383102417}, {"text": "5K-test set", "start_pos": 119, "end_pos": 130, "type": "DATASET", "confidence": 0.9623503983020782}]}, {"text": " Table 5. Results of end-to-end experiments on the  dev-1K set", "labels": [], "entities": []}, {"text": " Table 6. Results of human evaluation comparing  20best-10case vs. baseline. S: proposed system is bet- ter; B: baseline is better; E: of equal quality", "labels": [], "entities": []}]}