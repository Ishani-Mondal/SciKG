{"title": [{"text": "Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple Datasets", "labels": [], "entities": [{"text": "German Wordnet", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.8017110824584961}, {"text": "Evaluating Semantic Relatedness", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6432304084300995}]}], "abstractContent": [{"text": "We evaluate semantic relatedness measures on different German datasets showing that their performance depends on: (i) the definition of relatedness that was underlying the construction of the evaluation dataset, and (ii) the knowledge source used for computing semantic relatedness.", "labels": [], "entities": []}, {"text": "We analyze how the underlying knowledge source influences the performance of a measure.", "labels": [], "entities": []}, {"text": "Finally, we investigate the combination of wordnets and Wikipedia to improve the performance of semantic re-latedness measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic similarity (SS) is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist between two words.", "labels": [], "entities": [{"text": "Semantic similarity (SS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7411470413208008}, {"text": "semantic relatedness (SR)", "start_pos": 138, "end_pos": 163, "type": "TASK", "confidence": 0.6281777679920196}]}, {"text": "Many NLP applications, like sense tagging or spelling correction, require knowledge about semantic relatedness rather than just similarity).", "labels": [], "entities": [{"text": "sense tagging", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.8031699955463409}, {"text": "spelling correction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.9425207078456879}]}, {"text": "For these tasks, it is not necessary to know the exact type of semantic relation between two words, but rather if they are closely semantically related or not.", "labels": [], "entities": []}, {"text": "This is also true for the work presented herein, which is part of a project on electronic career guidance.", "labels": [], "entities": [{"text": "electronic career guidance", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.5975554684797922}]}, {"text": "In this domain, it is important to conclude that the words \"baker\" and \"bagel\" are closely related, while the exact type of a semantic relation does not need to be determined.", "labels": [], "entities": []}, {"text": "As we work on German documents, we evaluate a number of SR measures on different German datasets.", "labels": [], "entities": [{"text": "SR", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.5270328521728516}, {"text": "German datasets", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.7051810473203659}]}, {"text": "We show that the performance of measures strongly depends on the underlying knowledge source.", "labels": [], "entities": []}, {"text": "While WordNet) models SR, wordnets for other languages, such as the German wordnet GermaNet, contain only few links expressing SR.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.9310406446456909}, {"text": "SR", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.8920084238052368}]}, {"text": "Thus, they are not well suited for estimating SR.", "labels": [], "entities": [{"text": "estimating SR", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8415243625640869}]}, {"text": "Therefore, we apply the Wikipedia category graph as a knowledge source for SR measures.", "labels": [], "entities": [{"text": "Wikipedia category graph", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.9241418242454529}, {"text": "SR", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9885575771331787}]}, {"text": "We show that Wikipedia based SR measures yield better correlation with human judgments on SR datasets than GermaNet measures.", "labels": [], "entities": [{"text": "SR datasets", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.7457088828086853}]}, {"text": "However, using Wikipedia also leads to a performance drop on SS datasets, as knowledge about classical taxonomic relations is not explicitly modeled.", "labels": [], "entities": []}, {"text": "Therefore, we combine GermaNet with Wikipedia, and yield substantial improvements over measures operating on a single knowledge source.", "labels": [], "entities": []}], "datasetContent": [{"text": "Several German datasets for evaluation of SS or SR have been created so far (see).", "labels": [], "entities": [{"text": "German datasets", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.7561323940753937}, {"text": "SS or SR", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.5001941422621409}]}, {"text": "conducted experiments with a German translation of an English dataset, but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR.", "labels": [], "entities": [{"text": "SR", "start_pos": 173, "end_pos": 175, "type": "TASK", "confidence": 0.6785852313041687}]}, {"text": "Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations).", "labels": [], "entities": []}, {"text": "However, the dataset is biased towards strong classical relations, as word pairs were manually selected.", "labels": [], "entities": []}, {"text": "Thus,  Our results on Gur65 using GermaNet are very close to those published by, ranging from 0.69-0.75.", "labels": [], "entities": []}, {"text": "For Gur350, the performance drops to 0.38-0.50, due to the lower upper bound, and because GermaNet does not model SR well.", "labels": [], "entities": [{"text": "Gur350", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8714308738708496}, {"text": "GermaNet", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.8968941569328308}]}, {"text": "These findings are endorsed by an even more significant performance drop on ZG222.", "labels": [], "entities": [{"text": "ZG222", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9794458746910095}]}, {"text": "The measures based on Wikipedia behave less uniformly.", "labels": [], "entities": []}, {"text": "Les yields acceptable results on Gur350, but is generally not among the best performing measures.", "labels": [], "entities": [{"text": "Gur350", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.6030672192573547}]}, {"text": "LC +Avg yields the best performance on Gur65, but is outperformed on the other datasets by P L +Best, which performs equally good for all datasets.", "labels": [], "entities": [{"text": "Avg", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6851809024810791}, {"text": "Gur65", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9477037191390991}]}, {"text": "If we compare GermaNet based and Wikipedia based measures, we find that the knowledge source has a major influence on performance.", "labels": [], "entities": []}, {"text": "When evaluated on Gur65, that contains pairs connected by SS, GermaNet based measures perform near the upper bound and outperform Wikipedia based measures by a wide margin.", "labels": [], "entities": [{"text": "Gur65", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.9698506593704224}]}, {"text": "On Gur350 containing a mix of SS and SR pairs, most measures perform comparably.", "labels": [], "entities": [{"text": "SR", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9149342775344849}]}, {"text": "Finally, on ZG222, that contains pairs connected by SR, the best Wikipedia based measure outperforms all GermaNet based measures.", "labels": [], "entities": [{"text": "ZG222", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9729199409484863}]}, {"text": "The impressive performance of P Lon the SR datasets cannot be explained with the structural properties of the category graph.", "labels": [], "entities": [{"text": "SR datasets", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.7399741411209106}]}, {"text": "Semantically related terms, that would not be closely related in a taxonomic wordnet structure, are very likely to be categorized under the same Wikipedia category, resulting in short path lengths leading to high SR.", "labels": [], "entities": [{"text": "SR", "start_pos": 213, "end_pos": 215, "type": "METRIC", "confidence": 0.9959818124771118}]}, {"text": "These findings are contrary to that of (), where LC outperformed path length.", "labels": [], "entities": []}, {"text": "They limited the search depth using a manually defined threshold, and did not compute SR between all candidate article pairs.", "labels": [], "entities": [{"text": "SR", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9930282235145569}]}, {"text": "Our results show that judgments on the performance of a measure must always be made with respect to the task at hand: computing SS or SR.", "labels": [], "entities": [{"text": "computing SS", "start_pos": 118, "end_pos": 130, "type": "TASK", "confidence": 0.4474702626466751}, {"text": "SR", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.7262563705444336}]}, {"text": "Depending on this decision, we can choose the best underlying knowledge source.", "labels": [], "entities": []}, {"text": "POS is an alternative combination strategy, where Wikipedia is only used for noun-noun pairs.", "labels": [], "entities": []}, {"text": "GermaNet is used for all other part-of-speech (POS) combinations.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.867184042930603}]}, {"text": "For most datasets, we find a combination strategy that outperforms all single measures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of datasets used for evaluating semantic relatedness.", "labels": [], "entities": []}, {"text": " Table 2: Correlation r of human judgments with SR measures on different datasets.", "labels": [], "entities": [{"text": "SR", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.7886377573013306}]}]}