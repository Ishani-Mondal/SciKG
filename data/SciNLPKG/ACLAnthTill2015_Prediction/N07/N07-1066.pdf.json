{"title": [{"text": "A Probabilistic Framework for Answer Selection in Question Answering", "labels": [], "entities": [{"text": "Answer Selection in Question Answering", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.8383742570877075}]}], "abstractContent": [{"text": "This paper describes a probabilistic answer selection framework for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.888700932264328}]}, {"text": "In contrast with previous work using individual resources such as ontolo-gies and the Web to validate answer candidates , our work focuses on developing a unified framework that not only uses multiple resources for validating answer candidates, but also considers evidence of similarity among answer candidates in order to boost the ranking of the correct answer.", "labels": [], "entities": []}, {"text": "This framework has been used to select answers from candidates generated by four different answer extraction methods.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7124882936477661}]}, {"text": "An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the unified framework.", "labels": [], "entities": [{"text": "TREC factoid questions", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.6648101707299551}]}], "introductionContent": [{"text": "Question answering aims at finding exact answers to a user's natural language question from a large collection of documents.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9118776023387909}]}, {"text": "Most QA systems combine information retrieval with extraction techniques to identify a set of likely candidates and then utilize some selection strategy to generate the final answers (;).", "labels": [], "entities": []}, {"text": "Since answer extractors maybe based on imprecise empirical methods, the selection process can be very challenging, as it often entails identifying correct answer(s) amongst many incorrect ones.", "labels": [], "entities": [{"text": "answer extractors", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.8570941686630249}]}, {"text": "Given the question \"Which city in China has the largest number of foreign financial companies?\", the answer extraction component produces a ranked list of five answer candidates.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7448580265045166}]}, {"text": "Due to imprecision in answer extraction, an incorrect answer (\"Beijing\") was ranked at the top position.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7808669209480286}, {"text": "Beijing", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.6781498789787292}]}, {"text": "The correct answer (\"Shanghai\") was extracted from two documents with different confidence scores and ranked at the third and the fifth positions.", "labels": [], "entities": [{"text": "Shanghai", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.7296079993247986}]}, {"text": "In order to select \"Shanghai\" as the final answer, we need to address two issues: \u2022 Answer Validation.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.8274452686309814}]}, {"text": "How do we identify correct answer(s) amongst incorrect ones?", "labels": [], "entities": []}, {"text": "Validating an answer may involve searching for facts in a knowledge base, e.g. IS-A(Shanghai, city), IS-IN(Shanghai, China).", "labels": [], "entities": []}, {"text": "How do we exploit evidence of similarity among answer candidates?", "labels": [], "entities": []}, {"text": "For example, when there are redundant answers (\"Shanghai\", as above) or several answers which represent a single instance (e.g. \"Clinton, Bill\" and \"William Jefferson Clinton\") in the candidate list, how much should we boost the answer candidate scores?", "labels": [], "entities": []}, {"text": "To address the first issue, several answer selection approaches have used semantic resources.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.919672042131424}]}, {"text": "One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource's hierarchy corresponding to the expected answer type ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9688383340835571}, {"text": "CYC", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.863837718963623}, {"text": "answer validation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8096704483032227}, {"text": "answer reranking", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7462815642356873}]}, {"text": "In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords), and Wikipedia's structured information has been used for answer type checking).", "labels": [], "entities": [{"text": "answer reranking", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.9317367374897003}, {"text": "answer type checking", "start_pos": 225, "end_pos": 245, "type": "TASK", "confidence": 0.8543004989624023}]}, {"text": "To use more than one resource for answer type checking of location questions, combined WordNet with geographical databases.", "labels": [], "entities": [{"text": "answer type checking of location questions", "start_pos": 34, "end_pos": 76, "type": "TASK", "confidence": 0.8559673627217611}, {"text": "WordNet", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9808061122894287}]}, {"text": "However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies broader coverage of location names.", "labels": [], "entities": []}, {"text": "This demonstrates that the method used to combine potential answers may matter as much as the choice of resources.", "labels": [], "entities": []}, {"text": "To address the second issue we must determine how to detect and exploit answer similarity.", "labels": [], "entities": []}, {"text": "As answer candidates are extracted from different documents, they may contain identical, similar or complementary text snippets.", "labels": [], "entities": []}, {"text": "For example, the United States maybe represented by the strings \"U.S.\", \"United States\" or \"USA\" in different documents.", "labels": [], "entities": []}, {"text": "It is important to detect this type of similarity and exploit it to boost answer confidence, especially for list questions that require a set of unique answers.", "labels": [], "entities": []}, {"text": "One approach is to incorporate answer clustering ().", "labels": [], "entities": [{"text": "answer clustering", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8822115957736969}]}, {"text": "For example, we might merge \" into a cluster and then choose one answer as the cluster head.", "labels": [], "entities": []}, {"text": "However, clustering raises new issues: how to choose the cluster head and how to calculate the scores of the clustered answers.", "labels": [], "entities": [{"text": "clustering", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.9616049528121948}]}, {"text": "Although many QA systems individually address these issues in answer selection, there has been little research on generating a generalized probabilistic framework that allows any validation and similarity features to be easily incorporated.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.9385436177253723}]}, {"text": "In this paper we describe a probabilistic answer selection framework to address the two issues.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.837178111076355}]}, {"text": "The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features.", "labels": [], "entities": []}, {"text": "Experimental results on TREC factoid questions show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "labels": [], "entities": [{"text": "TREC factoid questions", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.5939546823501587}, {"text": "answer selection", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.8382917046546936}]}, {"text": "This paper is organized as follows: Section 2 describes our answer selection framework and Section 3 lists the features that generate similarity and validity scores for factoid questions.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.9095028340816498}, {"text": "similarity and validity scores", "start_pos": 134, "end_pos": 164, "type": "METRIC", "confidence": 0.7677927762269974}]}, {"text": "In Section 4, we describe the experimental methodology and the results.", "labels": [], "entities": []}, {"text": "Section 5 describes how we intend to extend our framework to handle complex questions.", "labels": [], "entities": []}, {"text": "Finally Section 6 concludes with suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experiments we used to evaluate our answer selection framework.", "labels": [], "entities": [{"text": "answer selection framework", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.8825472195943197}]}, {"text": "The JAVELIN QA system ( ) was used as a testbed for the evaluation.", "labels": [], "entities": [{"text": "JAVELIN QA", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8220610618591309}]}, {"text": "A total of 1760 factoid questions from the TREC8-12 QA evaluations served as a dataset, with 5-fold cross validation.", "labels": [], "entities": [{"text": "TREC8-12 QA evaluations", "start_pos": 43, "end_pos": 66, "type": "DATASET", "confidence": 0.8792432943979899}]}, {"text": "To better understand how the performance of our framework varies for different extraction techniques, we tested it with four JAVELIN answer extraction modules: FST, LIGHTv1, LIGHTv2 and SVM).", "labels": [], "entities": [{"text": "JAVELIN answer extraction", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6852334042390188}, {"text": "FST", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.6990537643432617}]}, {"text": "FST is an answer extractor based on finite state transducers that incorporate a set of extraction patterns (both manually-created and generalized patterns).", "labels": [], "entities": [{"text": "FST", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6663778424263}, {"text": "answer extractor", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.764242947101593}]}, {"text": "LIGHTv1 is an extractor that selects answer candidates using a non-linear distance heuristic between the keywords and an answer candidate.", "labels": [], "entities": [{"text": "LIGHTv1", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7386398911476135}]}, {"text": "LIGHTv2 is another extractor based on a different distance heuristic, originally developed as part of a multilingual QA system.", "labels": [], "entities": [{"text": "LIGHTv2", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8635764718055725}]}, {"text": "SVM is an extractor that uses Support Vector Machines to discriminate between correct and incorrect answers.", "labels": [], "entities": []}, {"text": "Answer selection performance was measured by average accuracy: the number of correct top answers divided by the number of questions whereat least one correct answer exists in the candidate list provided by an extractor.", "labels": [], "entities": [{"text": "Answer selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9452544450759888}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9942909479141235}]}, {"text": "The baseline was calculated with the answer candidate scores provided by each individual extractor; the answer with the best extractor score was chosen, and no validation or similarity processing was performed.", "labels": [], "entities": []}, {"text": "For Wikipedia, we used aversion downloaded in Nov. 2005, which contained 1,811,554 articles.", "labels": [], "entities": [{"text": "aversion downloaded in Nov. 2005", "start_pos": 23, "end_pos": 55, "type": "DATASET", "confidence": 0.7695492506027222}]}], "tableCaptions": [{"text": " Table 2: Average accuracy of individual features  (Sim: merging similarity features, Val: merging val- idation features, ALL: combination of all features).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9663169384002686}]}]}