{"title": [], "abstractContent": [{"text": "We present a novel machine translation framework based on kernel regression techniques.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7133845835924149}]}, {"text": "In our model, the translation task is viewed as a string-to-string mapping , for which a regression type learning is employed with both the source and the target sentences embedded into their kernel induced feature spaces.", "labels": [], "entities": [{"text": "translation task", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.8915715217590332}]}, {"text": "We report the experiments on a French-English translation task showing encouraging results.", "labels": [], "entities": [{"text": "French-English translation task", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6900038421154022}]}], "introductionContent": [{"text": "illustrates an example of phrase alignment for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.8227891623973846}, {"text": "statistical machine translation (SMT)", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.8075254708528519}]}, {"text": "A rough linear relation is shown by the co-occurences of phrases in bilingual sentence pairs, which motivates us to introduce a novel study on the SMT task: If we define the feature space H x of our source language X as all its possible phrases (i.e. informative blended word n-grams), and define the mapping \u03a6 x : X \u2192 H x , then a sentence x \u2208 X can be expressed by its feature vector \u03a6 x (x) \u2208 H x . Each component of \u03a6 x (x) is indexed by a phrase with the value being the frequency of it in x.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 147, "end_pos": 155, "type": "TASK", "confidence": 0.928803563117981}]}, {"text": "The definition of the feature space H y of our target language Y can be made in a similar way, with corresponding mapping \u03a6 y : Y \u2192 H y . Now in the machine translation task, given S = {(x i , y i ) : xi \u2208 X , y i \u2208 Y, i = 1, . .", "labels": [], "entities": [{"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7998309135437012}]}, {"text": ", m}, a set of sample sentence pairs where y i is the translation of xi , we are trying to learn W a matrix represented linear operator, such that: Comparing with traditional methods, this model gives us a theoretical framework to capture higherdimensional dependencies within the sentences.", "labels": [], "entities": []}, {"text": "To solve the multi-output regression problem, we investigate two models, least squares regression (LSR) similar to the technique presented in (), and maximum margin regression (MMR) introduced in).", "labels": [], "entities": [{"text": "multi-output regression", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.696052223443985}, {"text": "least squares regression (LSR)", "start_pos": 73, "end_pos": 103, "type": "METRIC", "confidence": 0.6730262537797292}, {"text": "maximum margin regression (MMR)", "start_pos": 150, "end_pos": 181, "type": "METRIC", "confidence": 0.7796723743279775}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief review of the regression models.", "labels": [], "entities": []}, {"text": "Section 3 details the solution to the pre-image problem.", "labels": [], "entities": []}, {"text": "We report the experimental results in Section 4, with discussions in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the corpora.", "labels": [], "entities": []}]}