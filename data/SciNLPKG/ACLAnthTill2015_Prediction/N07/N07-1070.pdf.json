{"title": [], "abstractContent": [{"text": "Most research on semantic role labeling (SRL) has been focused on training and evaluating on the same corpus in order to develop the technology.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.8220797777175903}]}, {"text": "This strategy, while appropriate for initiating research, can lead to over-training to the particular corpus.", "labels": [], "entities": []}, {"text": "The work presented in this paper focuses on analyzing the robustness of an SRL system when trained on one genre of data and used to label a different genre.", "labels": [], "entities": [{"text": "SRL", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9812294244766235}]}, {"text": "Our state-of-the-art semantic role labeling system, while performing well on WSJ test data, shows significant performance degradation when applied to data from the Brown corpus.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.6117047667503357}, {"text": "WSJ test data", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.95081494251887}, {"text": "Brown corpus", "start_pos": 164, "end_pos": 176, "type": "DATASET", "confidence": 0.9313146471977234}]}, {"text": "We present a series of experiments designed to investigate the source of this lack of portability.", "labels": [], "entities": []}, {"text": "These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown corpus data.", "labels": [], "entities": [{"text": "PropBanked WSJ data", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.840054452419281}, {"text": "PropBanked Brown corpus data", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.9648774713277817}]}, {"text": "Our results indicate that while syntactic parses and argument identification port relatively well to anew genre, argument classification does not.", "labels": [], "entities": [{"text": "syntactic parses", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7308671325445175}, {"text": "argument identification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.808016836643219}, {"text": "argument classification", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.8512386679649353}]}, {"text": "Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task.", "labels": [], "entities": [{"text": "argument identification task", "start_pos": 204, "end_pos": 232, "type": "TASK", "confidence": 0.7787438829739889}]}], "introductionContent": [{"text": "Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction), Question Answering () and Machine Translation).", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.7850954532623291}, {"text": "Question Answering", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.838832288980484}, {"text": "Machine Translation", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.8574705421924591}]}, {"text": "Semantic Role Labeling (SRL) is the process of producing such a markup.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8068436682224274}]}, {"text": "When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate's semantic arguments.", "labels": [], "entities": []}, {"text": "In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it.", "labels": [], "entities": []}, {"text": "On the Wall Street Journal (WSJ) data, using correct syntactic parses, it is possible to achieve accuracies rivaling human interannotator agreement.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) data", "start_pos": 7, "end_pos": 37, "type": "DATASET", "confidence": 0.9563864299229213}]}, {"text": "However, the performance gap widens when information derived from automatic syntactic parses is used.", "labels": [], "entities": []}, {"text": "So far, most of the work on SRL systems has been focused on improving the labeling performance on a test set belonging to the same genre of text as the training set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9876714944839478}]}, {"text": "Both the Treebank on which the syntactic parser is trained and the PropBank on which the SRL systems are trained represent articles from the year 1989 of the WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.8941525220870972}]}, {"text": "While all these systems perform quite well on the WSJ test data, they show significant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is different than the genre that WSJ represents ().", "labels": [], "entities": [{"text": "WSJ test data", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9278730154037476}, {"text": "F-score)", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9801295399665833}]}, {"text": "Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text -as in the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.9480565190315247}]}, {"text": "These results indicate that the systems are being over-fit to the specific genre of text.", "labels": [], "entities": []}, {"text": "Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus.", "labels": [], "entities": [{"text": "WSJ PropBank corpus", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.9499968489011129}]}, {"text": "For the technology to be widely accepted and useful, it must be robust to change in genre of the data.", "labels": [], "entities": []}, {"text": "Until recently, data tagged with similar semantic argument structure was not available for multiple genres of text.", "labels": [], "entities": []}, {"text": "Recently,, have PropBanked a significant portion of the Treebanked Brown corpus which enables us to perform experiments to analyze the reasons behind the performance degradation, and suggest potential solutions.", "labels": [], "entities": [{"text": "Treebanked Brown corpus", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.9295810063680013}]}], "datasetContent": [{"text": "This section describes experiments that we performed using the PropBanked Brown corpus in an attempt to analyze the factors affecting the portability of SRL systems.", "labels": [], "entities": [{"text": "PropBanked Brown corpus", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.9555777907371521}, {"text": "SRL", "start_pos": 153, "end_pos": 156, "type": "TASK", "confidence": 0.8726950287818909}]}], "tableCaptions": [{"text": " Table 2: Performance of the SRL system on Brown.", "labels": [], "entities": [{"text": "SRL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8367219567298889}, {"text": "Brown", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.9904112815856934}]}, {"text": " Table 3. These numbers are for top one parse for the  Charniak parser, and represent not all parser errors,  but deletion of argument bearing constituent nodes.", "labels": [], "entities": []}, {"text": " Table 3: Constituent deletions in WSJ and Brown.", "labels": [], "entities": [{"text": "WSJ and Brown", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8183296322822571}]}, {"text": " Table 4: Performance of the SRL system using correct Treebank parses.", "labels": [], "entities": [{"text": "SRL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9435415267944336}]}, {"text": " Table 5: Performance on WSJ and Brown using automatic syntactic parses", "labels": [], "entities": [{"text": "WSJ and Brown", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8742262721061707}, {"text": "syntactic parses", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7326012551784515}]}, {"text": " Table 7: Effect of incrementally adding data from a new genre", "labels": [], "entities": []}, {"text": " Table 8: Influence of verb sense feature.", "labels": [], "entities": []}]}