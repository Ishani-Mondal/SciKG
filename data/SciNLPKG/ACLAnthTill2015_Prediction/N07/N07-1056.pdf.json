{"title": [], "abstractContent": [{"text": "The task of selecting and ordering information appears in multiple contexts in text generation and summarization.", "labels": [], "entities": [{"text": "text generation", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7591006457805634}, {"text": "summarization", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9646231532096863}]}, {"text": "For instance , methods for title generation construct a headline by selecting and ordering words from the input text.", "labels": [], "entities": [{"text": "title generation construct a headline", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7945119976997376}]}, {"text": "In this paper , we investigate decoding methods that simultaneously optimize selection and ordering preferences.", "labels": [], "entities": []}, {"text": "We formalize decoding as a task of finding an acyclic path in a directed weighted graph.", "labels": [], "entities": []}, {"text": "Since the problem is NP-hard, finding an exact solution is challenging.", "labels": [], "entities": []}, {"text": "We describe a novel decoding method based on a randomized color-coding algorithm.", "labels": [], "entities": []}, {"text": "We prove bounds on the number of color-coding iterations necessary to guarantee any desired likelihood of finding the correct solution.", "labels": [], "entities": []}, {"text": "Our experiments show that the randomized de-coder is an appealing alternative to a range of decoding algorithms for selection-and-ordering problems, including beam search and Integer Linear Programming.", "labels": [], "entities": [{"text": "beam search", "start_pos": 159, "end_pos": 170, "type": "TASK", "confidence": 0.8492102324962616}]}], "introductionContent": [{"text": "The task of selecting and ordering information appears in multiple contexts in text generation and summarization.", "labels": [], "entities": [{"text": "text generation", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.7591006457805634}, {"text": "summarization", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9646231532096863}]}, {"text": "For instance, atypical multidocument summarization system creates a summary by selecting a subset of input sentences and ordering them into a coherent text.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.5967303216457367}]}, {"text": "Selection and ordering at the word level is commonly employed in lexical realization.", "labels": [], "entities": [{"text": "lexical realization", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7295702695846558}]}, {"text": "For instance, in the task of title generation, the headline is constructed by selecting and ordering words from the input text.", "labels": [], "entities": [{"text": "title generation", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7360436320304871}]}, {"text": "Decoding is an essential component of the selection-and-ordering process.", "labels": [], "entities": []}, {"text": "Given selection and ordering preferences, the task is to find a sequence of elements that maximally satisfies these preferences.", "labels": [], "entities": []}, {"text": "One possible approach for finding such a solution is to decompose it into two tasks: first, select a set of words based on individual selection preferences, and then order the selected units into a well-formed sequence.", "labels": [], "entities": []}, {"text": "Although the modularity of this approach is appealing, the decisions made in the selection step cannot be retracted.", "labels": [], "entities": []}, {"text": "Therefore, we cannot guarantee that selected units can be ordered in a meaningful way, and we may end up with a suboptimal output.", "labels": [], "entities": []}, {"text": "In this paper, we investigate decoding methods that simultaneously optimize selection and ordering preferences.", "labels": [], "entities": []}, {"text": "We formalize decoding as finding a path in a directed weighted graph.", "labels": [], "entities": []}, {"text": "The vertices in the graph represent units with associated selection scores, and the edges represent pairwise ordering preferences.", "labels": [], "entities": []}, {"text": "The desired solution is the highest-weighted acyclic path of a prespecified length.", "labels": [], "entities": []}, {"text": "The requirement for acyclicity is essential because in atypical selection-and-ordering problem, a well-formed output does not include any repeated units.", "labels": [], "entities": []}, {"text": "For instance, a summary of multiple documents should not contain any repeated sentences.", "labels": [], "entities": []}, {"text": "Since the problem is NP-hard, finding an exact solution is challenging.", "labels": [], "entities": []}, {"text": "We introduce a novel randomized decoding algorithm 2 based on the idea of color-coding (.", "labels": [], "entities": []}, {"text": "Although the algorithm is not guaranteed to find the optimal solution on any single run, by increasing the number of runs the algorithm can guarantee an arbitrarily high probability of success.", "labels": [], "entities": []}, {"text": "The paper provides a theoretical analysis that establishes the connection between the required number of runs and the likelihood of finding the correct solution.", "labels": [], "entities": []}, {"text": "Next, we show how to find an exact solution using an integer linear programming (ILP) formulation.", "labels": [], "entities": []}, {"text": "Although ILP is NP-hard, this method is guaranteed to compute the optimal solution.", "labels": [], "entities": []}, {"text": "This allows us to experimentally investigate the trade-off between the accuracy and the efficiency of decoding algorithms considered in the paper.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9995599389076233}]}, {"text": "We evaluate the accuracy of the decoding algorithms on the task of title generation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9994534850120544}, {"text": "title generation", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7544128596782684}]}, {"text": "The decoding algorithms introduced in the paper are compared against beam search, a heuristic search algorithm commonly used for selection-and-ordering and other natural language processing tasks.", "labels": [], "entities": []}, {"text": "Our experiments show that the randomized decoder is an appealing alternative to both beam search and ILP when applied to selection-and-ordering problems.", "labels": [], "entities": [{"text": "beam search", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.7852224111557007}]}], "datasetContent": [{"text": "Task We applied our decoding algorithm to the task of title generation.", "labels": [], "entities": [{"text": "title generation", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7366073280572891}]}, {"text": "This task has been extensively studied over the last six years ().", "labels": [], "entities": []}, {"text": "Title generation is a classic selection-and-ordering problem: during title realization, an algorithm has to take into account both the likelihood of words appearing in the title and their ordering preferences.", "labels": [], "entities": [{"text": "Title generation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8059017360210419}, {"text": "title realization", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7223593294620514}]}, {"text": "In the previous approaches, beam search has been used for decoding.", "labels": [], "entities": [{"text": "beam search", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9065513908863068}]}, {"text": "Therefore, it is natural to explore more sophisticated decoding techniques like the ones described in this paper.", "labels": [], "entities": []}, {"text": "Our method for estimation of selection-andordering preferences is based on the technique described in ().", "labels": [], "entities": []}, {"text": "We compute the likelihood of a word in the document appearing in the title using a maximum entropy classifier.", "labels": [], "entities": []}, {"text": "Every stem is represented by commonly used positional and distributional features, such as location of the first sentence that contains the stem and its TF*IDF.", "labels": [], "entities": [{"text": "TF*IDF", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.8508399526278178}]}, {"text": "We estimate the ordering preferences using a bigram language model with Good-Turing smoothing.", "labels": [], "entities": []}, {"text": "In previous systems, the title length is either provided to a decoder as a parameter, or heuristics are used to determine it.", "labels": [], "entities": []}, {"text": "Since exploration of these heuristics is not the focus of our paper, we provide the decoder with the actual title length (as measured by the number of content words).", "labels": [], "entities": []}, {"text": "Graph Construction We construct a decoding graph in the following fashion.", "labels": [], "entities": [{"text": "Graph Construction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6960862427949905}]}, {"text": "Every unique content word comprises a vertex in the graph.", "labels": [], "entities": []}, {"text": "All the morphological variants of a stem belong to the same equivalence class.", "labels": [], "entities": []}, {"text": "An edge (v, u) in the graph encodes the selection preference of u and the likelihood of the transition from v to u.", "labels": [], "entities": []}, {"text": "Note that the graph does not contain any auxiliary words in its vertices.", "labels": [], "entities": []}, {"text": "We handle the insertion of auxiliary words by inserting additional edges.", "labels": [], "entities": []}, {"text": "For every auxiliary word x, we add one edge representing the transition from v to u via x, and the selection preference of u.", "labels": [], "entities": []}, {"text": "The auxiliary word set consists of 24 prepositions and articles extracted from the corpus.", "labels": [], "entities": []}, {"text": "Corpus Our corpus consists of 547 sections of a commonly used undergraduate algorithms textbook.", "labels": [], "entities": [{"text": "Corpus Our corpus", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8367768128712972}]}, {"text": "The average section contains 609.2 words.", "labels": [], "entities": []}, {"text": "A title, on average, contains 3.7 words, among which 3.0 are content words; the shortest and longest titles have 1 and 13 words respectively.", "labels": [], "entities": []}, {"text": "Our training set consists of the first 382 sections, the remaining 165 sections are used for testing.", "labels": [], "entities": []}, {"text": "The bigram language model is estimated from the body text of all sections in the corpus, consisting of 461,351 tokens.", "labels": [], "entities": []}, {"text": "To assess the importance of the acyclicity constraint, we compute the number of titles that have repeated content words.", "labels": [], "entities": []}, {"text": "The empirical findings support our assumption: 97.9% of the titles do not contain repeated words.", "labels": [], "entities": []}, {"text": "Decoding Algorithms We consider three decoding algorithms: our color-coding algorithm, ILP, and beam search.", "labels": [], "entities": []}, {"text": "The beam search algorithm can only consider vertices which are not already in the path.", "labels": [], "entities": [{"text": "beam search", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8238126933574677}]}, {"text": "To solve the ILP formulations, we employ a Mixed Integer Programming solver lp solve which implements the Branch-and-Bound algorithm.", "labels": [], "entities": [{"text": "Mixed Integer Programming solver", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6057443767786026}]}, {"text": "We implemented the rest of the decoders in Python with the Psyco speed-up module.", "labels": [], "entities": []}, {"text": "We put substantial effort to optimize the performance of all of the algorithms.", "labels": [], "entities": []}, {"text": "The color-coding algorithm is implemented using parallelized computation of coloring iterations.", "labels": [], "entities": []}, {"text": "shows the performance of various decoding algorithms considered in the paper.", "labels": [], "entities": []}, {"text": "We first evaluate each algorithm by the running times it requires to find all the optimal solutions on the test set.", "labels": [], "entities": []}, {"text": "Since ILP is guaranteed to find the optimal solution, we can use its output as a point of comparison.", "labels": [], "entities": []}, {"text": "lists both the average and the median running times.", "labels": [], "entities": []}, {"text": "For some of the decoding algorithms, the difference between the two measurements is striking -6,536 seconds versus 57.3 seconds for ILP.", "labels": [], "entities": [{"text": "striking", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9870328903198242}]}, {"text": "This gap can be explained by outliers which greatly increase the average running time.", "labels": [], "entities": []}, {"text": "For instance, in the worst case, ILP takes an astounding 136 hours to find the optimal solution.", "labels": [], "entities": []}, {"text": "Therefore, we base our comparison on the median running time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Running times in seconds, ROUGE scores, and percentage of optimal solutions found for each of  the decoding algorithms.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9966247081756592}]}]}