{"title": [], "abstractContent": [{"text": "This paper presents an alternative algorithm based on the singular value decomposition (SVD) that creates vector representation for linguistic units with reduced dimensionality.", "labels": [], "entities": []}, {"text": "The work was motivated by an application aimed to represent text segments for further processing in a multi-document summarization system.", "labels": [], "entities": []}, {"text": "The algorithm tries to compensate for SVD's bias towards dominant-topic documents.", "labels": [], "entities": []}, {"text": "Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms-the SVD and the vector space model.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.981279730796814}]}], "introductionContent": [{"text": "We present, in this paper, an alternative algorithm called Clustered Sub-matrix Singular Value Decomposition(CSSVD) algorithm, which applied clustering techniques before basis vector calculation in SVD (.", "labels": [], "entities": []}, {"text": "The work was motivated by an application aimed to provide vector representation for terms and text segments in a document collection.", "labels": [], "entities": []}, {"text": "These vector representations were then used for further preprocessing in a multidocument summarization system.", "labels": [], "entities": []}, {"text": "The SVD is an orthogonal decomposition technique closely related to eigenvector decomposition and factor analysis.", "labels": [], "entities": [{"text": "eigenvector decomposition", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.7274913191795349}, {"text": "factor analysis", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.712809607386589}]}, {"text": "It is commonly used in information retrieval as well as language analysis applications.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8766996562480927}, {"text": "language analysis", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7306907773017883}]}, {"text": "In SVD, areal m-by-n matrix A is decomposed into three matrices, A = UV T . is an m-by-n matrix such that the singular value \u03c3 i = ii is the square root of the i th largest eigenvalue of AA T , and ij = 0 for i = j.", "labels": [], "entities": []}, {"text": "Columns of orthogonal matrices U and V define the orthonormal eigenvectors associated with eigenvalues of AA T and A TA, respectively.", "labels": [], "entities": []}, {"text": "Zeroing out all but the k, k < rank(A), largest singular values yields , which is the closest rank-k matrix to A.", "labels": [], "entities": []}, {"text": "Let Abe a term-document matrix.", "labels": [], "entities": []}, {"text": "Applications such as latent semantic indexing) apply the rank-k approximation A k to the original matrix A, which corresponds to projecting A onto the kdimension subspace spanned by u 1 , u 2 , ..., u k . Because km, in this k-dimension space, minor terms are ignored, so that terms are not independent as they are in the traditional vector space model.", "labels": [], "entities": [{"text": "latent semantic indexing", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7114081581433614}]}, {"text": "This allows semantically related documents to be related to each other even though they may not share terms.", "labels": [], "entities": []}, {"text": "However, SVD tends to wipe out outlier (minority-class) documents as well as minor terms).", "labels": [], "entities": [{"text": "SVD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9435404539108276}]}, {"text": "Consequently, topics underlying outlier documents tend to be lost.", "labels": [], "entities": []}, {"text": "In applications such as multi-document summarization, a set of related documents are used as the information source.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7181470096111298}]}, {"text": "Typically, the documents describe one broad topic from several different view points or sub-topics.", "labels": [], "entities": []}, {"text": "It is important for each of the sub-topics underlying the document collection to be represented well.", "labels": [], "entities": []}, {"text": "Based on the above consideration, we propose the CSSVD algorithm with the intention of compensat-ing for SVD's tendency to wipe out minor topics.", "labels": [], "entities": []}, {"text": "The basic idea is to group the documents into a set of clusters using clustering algorithms.", "labels": [], "entities": []}, {"text": "The SVD is then applied on each of the document clusters.", "labels": [], "entities": []}, {"text": "The algorithm thus selects basis vectors by treating equally each of the topics.", "labels": [], "entities": []}, {"text": "Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the SVD.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9750725626945496}]}], "datasetContent": [{"text": "For the evaluation of the algorithm, 38 topics from the Text REtrieval Conference (TREC) collections were used in our experiments.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC) collections", "start_pos": 56, "end_pos": 100, "type": "DATASET", "confidence": 0.6762115785053798}]}, {"text": "These topics include foreign minorities, behavioral genetics, steel production, etc.", "labels": [], "entities": [{"text": "behavioral genetics", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7160884141921997}]}, {"text": "We deleted documents relevant to more than one topic so that each document is related only to one topic.", "labels": [], "entities": []}, {"text": "The total number of documents used was 2962.", "labels": [], "entities": []}, {"text": "These documents were split into two disjoint groups, called 'pool 1' and 'pool 2'.", "labels": [], "entities": []}, {"text": "The number of documents in 'pool 1' and 'pool 2' were 1453 and 1509, respectively.", "labels": [], "entities": []}, {"text": "Each of the two groups used 19 topics.", "labels": [], "entities": []}, {"text": "We generated training and testing data by simulating the result obtained by a query search.", "labels": [], "entities": []}, {"text": "This simulation is further simplified by selecting documents containing same keywords from each document group.", "labels": [], "entities": []}, {"text": "Thirty document sets were generated from each of the two document groups, i.e. 60 document sets in total.", "labels": [], "entities": []}, {"text": "The number of documents for each set ranges from 51 to 582 with an average of 128; the number of topics ranges from 5 to 19 with an average of 12.", "labels": [], "entities": []}, {"text": "Due to the limited number of the document sets we created, these sets were used both for training and evaluation.", "labels": [], "entities": []}, {"text": "For the evaluation of the documents sets from 'pool 1', 'pool 2' was used for training, and vice versa.", "labels": [], "entities": []}, {"text": "To construct the original term-document matrix, the following operations were performed on each of the documents: 1) filtering out all non-text tags in the documents; 2) converting all the characters into lowercase; 3) removing stop words -a stoplist containing 319 words was used; and 4) term indexing -the tf.idf scheme was used to calculate a term's weight in a document.", "labels": [], "entities": [{"text": "term indexing", "start_pos": 289, "end_pos": 302, "type": "TASK", "confidence": 0.7984181642532349}]}, {"text": "Finally, a document set is represented as a matrix A = [a ij ], where a ij denotes the normalized weight assigned to term i in document j.", "labels": [], "entities": []}, {"text": "Our algorithm was motivated by a multi-document summarization application which is mainly based on measuring the similarities and differences among text segments.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.556803360581398}]}, {"text": "Therefore, the basic requisite is to accurately measure similarities among texts.", "labels": [], "entities": []}, {"text": "Based on this consideration, we used the CSSVD algorithm to create the document vectors in a reduced space for each of the document sets; cosine similarities among these document vectors were computed; and the results were then compared with the TREC relevance judgments.", "labels": [], "entities": [{"text": "TREC", "start_pos": 246, "end_pos": 250, "type": "METRIC", "confidence": 0.8299012780189514}]}, {"text": "As each of the TREC documents we used has one specific topic.", "labels": [], "entities": []}, {"text": "Assume that similarity should be higher for any document pair relevant to the same topic than for any pair relevant to different topics.", "labels": [], "entities": [{"text": "similarity", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9975716471672058}]}, {"text": "The algorithm's accuracy for measuring the similarities among documents was evaluated using average precision taken at various recall levels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.999415397644043}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9001938104629517}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9955642223358154}]}, {"text": "Let pi denote the document pair that has the i th largest similarity value among all pairs of documents in the document set.", "labels": [], "entities": []}, {"text": "The precision for an intra-topic pair pk is calculated by where p j is an intra-topic pair.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995480179786682}]}, {"text": "The average of the precision values overall intra-topic pairs is computed as the average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9966939687728882}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9460688233375549}]}], "tableCaptions": []}