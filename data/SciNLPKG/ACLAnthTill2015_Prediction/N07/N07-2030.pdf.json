{"title": [{"text": "On using Articulatory Features for Discriminative Speaker Adaptation", "labels": [], "entities": [{"text": "Discriminative Speaker Adaptation", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.6999006668726603}]}], "abstractContent": [{"text": "This paper presents away to perform speaker adaptation for automatic speech recognition using the stream weights in a multi-stream setup, which included acoustic models for \"Articulatory Features\" such as ROUNDED or VOICED.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7606951892375946}, {"text": "automatic speech recognition", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6429129242897034}, {"text": "ROUNDED", "start_pos": 205, "end_pos": 212, "type": "METRIC", "confidence": 0.8328984975814819}]}, {"text": "We present supervised speaker adaptation experiments on a spontaneous speech task and compare the above stream-based approach to conventional approaches, in which the models, and not stream combination weights, are being adapted.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6989848613739014}]}, {"text": "In the approach we present, stream weights model the importance of features such as VOICED for word discrimination, which offers a descriptive interpretation of the adaptation parameters.", "labels": [], "entities": [{"text": "word discrimination", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8020883500576019}]}], "introductionContent": [{"text": "Almost all approaches to automatic speech recognition (ASR) using Hidden Markov Models (HMMs) to model the time dependency of speech are also based on phones, or context-dependent sub-phonetic units derived from them, as the atomic unit of speech modeling.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.8129377613464991}, {"text": "speech modeling", "start_pos": 240, "end_pos": 255, "type": "TASK", "confidence": 0.7088113129138947}]}, {"text": "In phonetics, a phone is a shorthand notation fora certain configuration of underlying articulatory features (AFs): /p/ is for example defined as the unvoiced, bi-labial plosive, from which /b/ can be distinguished by its VOICED attribute.", "labels": [], "entities": []}, {"text": "In this sense, instead of describing speech as a single, sequential stream of symbols representing sounds, we can also look at speech as the result of a process involving several parallel streams of information, each of which describes some linguistic or articulatory property as being either absent or present.", "labels": [], "entities": []}, {"text": "A multi-stream architecture is a relatively simple approach to combining several information sources in ASR, because it leaves the basic structure of the Hidden Markov Model and its computational complexity intact.", "labels": [], "entities": [{"text": "ASR", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9742431640625}]}, {"text": "Examples combining different observations are audio-visual speech recognition) and sub-band based speech processing ().", "labels": [], "entities": [{"text": "audio-visual speech recognition", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6520990431308746}]}, {"text": "The same idea can also be used to combine different classifiers on the same observation.", "labels": [], "entities": []}, {"text": "Ina multi-stream HMM setup, log-linear interpolation can be derived as a framework to integrating several independent acoustic models given as Gaussian Mixture Models (GMMs) into the speech recognition process: given a \"weight\" vector \u039b = {\u03bb 0 , \u03bb 1 , \u00b7 \u00b7 \u00b7 , \u03bb M }, a word sequence W , and an acoustic observation o, the posterior probability p(W |o) one wants to optimize is written as: C is a normalization constant, which can be neglected in practice, as long as normalization \u03a3 i \u03bb i = const is observed.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7138141542673111}]}, {"text": "It is now possible to set p(W |o) \u221d p(o|W )) and write a speech recognizer's acoustic model p(o|W ) in this form, which in logarithmic representation reduces to a simple weighted sum of so-called \"scores\" for each individual stream.", "labels": [], "entities": []}, {"text": "The \u03bb i represent the \"im-portance\" of the contribution of each individual information source.", "labels": [], "entities": []}, {"text": "Extending) approach, the log-likelihood score combination method to AF-based ASR can be used to combine information from M different articulatory features while at the same time retaining the \"standard\" acoustic models as stream 0.", "labels": [], "entities": [{"text": "AF-based ASR", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.46237997710704803}]}, {"text": "As an example using M = 2, the acoustic score for /z/ would be computed as a weighted sum of the scores fora (context-dependent sub-)phonetic model z, the score for FRICATIVE and the score for VOICED, while the score for /s/ would be computed as a weighted sum of the scores fora (context-dependent sub-) phonetic model s, the score for FRICA-TIVE and the score for NON VOICED.", "labels": [], "entities": [{"text": "FRICATIVE", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.7395097017288208}, {"text": "FRICA-TIVE", "start_pos": 337, "end_pos": 347, "type": "DATASET", "confidence": 0.884455144405365}, {"text": "NON VOICED", "start_pos": 366, "end_pos": 376, "type": "DATASET", "confidence": 0.8531421720981598}]}, {"text": "The free parameters \u03bb i can be global (G), or they can be made state-dependent (SD) during the optimization process, thus changing the importance of a feature given a specific phonetic context, as long as overall normalization is observed.) discusses this stream setup in more detail.", "labels": [], "entities": []}], "datasetContent": [{"text": "To investigate the performance of the proposed AFbased model, we built acoustic models for 68 articulatory features on 32h of English Spontaneous Scheduling Task ESST data from the Verbmobil project, and integrated them with matching phone-based acoustic models.", "labels": [], "entities": [{"text": "English Spontaneous Scheduling Task ESST data from the Verbmobil project", "start_pos": 126, "end_pos": 198, "type": "DATASET", "confidence": 0.7252551585435867}]}, {"text": "For training robust baseline phone models, 32h from the ESST corpus were merged with 66h Broadcast News '96 data, for which manually annotated speaker labels are available.", "labels": [], "entities": [{"text": "ESST corpus", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9667890965938568}, {"text": "Broadcast News '96 data", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.9216689705848694}]}, {"text": "The system is trained using 6 iterations of ML training and uses 4000 context dependent (CD) acoustic models (HMM states), 32 Gaussians per model with diagonal covariance matrices and a global semi-tied covariance matrix (STC) in a 40-dimensional MFCC-based feature space after LDA.", "labels": [], "entities": []}, {"text": "The characteristics of the training and test sets used in the following experiments are summarized in.", "labels": [], "entities": []}, {"text": "The ESST test vocabulary contains 9400 words including pronunciation variants (7100 base forms) while the language model perplexity is 43.5 with an out of vocabulary (OOV) rate of 1%.", "labels": [], "entities": [{"text": "ESST test vocabulary", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8291755318641663}, {"text": "out of vocabulary (OOV) rate", "start_pos": 148, "end_pos": 176, "type": "METRIC", "confidence": 0.7362291429724012}]}, {"text": "The language model is a tri-gram model trained on ESST data: Data sets used in this work: The ESST test set 1825 is the union of the development set ds2 and the evaluation set xv2.", "labels": [], "entities": [{"text": "ESST data", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9017349481582642}, {"text": "ESST test set 1825", "start_pos": 94, "end_pos": 112, "type": "DATASET", "confidence": 0.9263386875391006}]}, {"text": "containing manually annotated semantic classes for most proper names (persons, locations, numbers).", "labels": [], "entities": []}, {"text": "Generally, systems run in less than 4 times real-time on Pentium 4-class machines.", "labels": [], "entities": []}, {"text": "The baseline Word Error Rate is reported as adaptation \"None\" in Table 2; the system parameters were optimized on the ds2 data set.", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.5792796512444814}, {"text": "adaptation", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9719304442405701}, {"text": "ds2 data set", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.8220014770825704}]}, {"text": "As the stream weight estimation process can introduce a scaling factor for the acoustic model, we verified that the baseline system cannot be improved by widening the beam or by readjusting the weight of the language model vs. the acoustic model.", "labels": [], "entities": []}, {"text": "The baseline system can also not be improved significantly by varying the number of parameters, either by increasing the number of Gaussians per codebook or by increasing the number of codebooks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sets used in this work: The ESST test  set 1825 is the union of the development set ds2  and the evaluation set xv2.", "labels": [], "entities": [{"text": "ESST test  set 1825", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.9281845986843109}]}]}