{"title": [{"text": "A Systematic Comparison of Various Statistical Alignment Models", "labels": [], "entities": [{"text": "Statistical Alignment", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7162750214338303}]}], "abstractContent": [{"text": "We present and compare various methods for computing word alignments using statistical or heuristic models.", "labels": [], "entities": [{"text": "computing word alignments", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6268925368785858}]}, {"text": "We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements.", "labels": [], "entities": []}, {"text": "These statistical models are compared with two heuristic models based on the Dice coefficient.", "labels": [], "entities": []}, {"text": "We present different methods for combining word alignments to perform a symmetriza-tion of directed statistical alignment models.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7057716548442841}]}, {"text": "As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.", "labels": [], "entities": [{"text": "Viterbi alignment", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.9051661491394043}]}, {"text": "We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.", "labels": [], "entities": []}, {"text": "We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.", "labels": [], "entities": []}, {"text": "An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.", "labels": [], "entities": []}, {"text": "In the Appendix, we present an efficient training algorithm for the alignment models presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "We address in this article the problem of finding the word alignment of a bilingual sentence-aligned corpus by using language-independent statistical methods.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7099506258964539}]}, {"text": "There is avast literature on this topic, and many different systems have been suggested to solve this problem.", "labels": [], "entities": []}, {"text": "Our work follows and extends the methods introduced by  by using refined statistical models for the translation process.", "labels": [], "entities": [{"text": "translation process", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.9200127720832825}]}, {"text": "The basic idea of this approach is to develop a model of the translation process with the word alignment as a hidden variable of this process, to apply statistical estimation theory to compute the \"optimal\" model parameters, and to perform alignment search to compute the best word alignment.", "labels": [], "entities": []}, {"text": "So far, refined statistical alignment models have in general been rarely used.", "labels": [], "entities": [{"text": "statistical alignment", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.6570367962121964}]}, {"text": "One reason for this is the high complexity of these models, which makes them difficult to understand, implement, and tune.", "labels": [], "entities": []}, {"text": "Instead, heuristic models are usually used.", "labels": [], "entities": []}, {"text": "In heuristic models, the word alignments are computed by analyzing some association score metric of a link between a source language word and a target language word.", "labels": [], "entities": []}, {"text": "These models are relatively easy to implement.", "labels": [], "entities": []}, {"text": "In this article, we focus on consistent statistical alignment models suggested in the literature, but we also describe a heuristic association metric.", "labels": [], "entities": []}, {"text": "By providing a detailed description and a systematic evaluation of these alignment models, we give the reader various criteria for deciding which model to use fora given task.", "labels": [], "entities": []}, {"text": "* Information Science Institute (USC/ISI), 4029 Via Marina, Suite 1001, Marina del Rey, CA 90292.", "labels": [], "entities": [{"text": "Information Science Institute (USC/ISI), 4029 Via Marina", "start_pos": 2, "end_pos": 58, "type": "DATASET", "confidence": 0.767062596976757}]}, {"text": "\u2020 Lehrstuhl f \u00a8 ur Informatik VI, Computer Science Department, RWTH Aachen-University of Technology, D-52056 Aachen, Germany.", "labels": [], "entities": [{"text": "Lehrstuhl f \u00a8 ur Informatik VI", "start_pos": 2, "end_pos": 32, "type": "METRIC", "confidence": 0.7892298996448517}, {"text": "RWTH Aachen-University of Technology, D-52056 Aachen", "start_pos": 63, "end_pos": 115, "type": "DATASET", "confidence": 0.7879518781389508}]}, {"text": "Another application of word alignments is in the field of word sense disambiguation, word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to languages for which such resources are scarce.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7443155646324158}, {"text": "word sense disambiguation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.718808650970459}, {"text": "word alignment", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7476480305194855}]}], "datasetContent": [{"text": "In the following, we present an annotation scheme for single-word-based alignments and a corresponding evaluation criterion.", "labels": [], "entities": []}, {"text": "It is well known that manually performing a word alignment is a complicated and ambiguous task).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7058119773864746}]}, {"text": "Therefore, in performing the alignments for the research presented here, we use an annotation scheme that explicitly allows for ambiguous alignments.", "labels": [], "entities": []}, {"text": "The persons conducting the annotation are asked to specify alignments of two different kinds: an S (sure) alignment, for alignments that are unambiguous, and a P (possible) alignment, for ambiguous alignments.", "labels": [], "entities": []}, {"text": "The P label is used especially to align words within idiomatic expressions and free translations and missing function words (S \u2286 P).", "labels": [], "entities": []}, {"text": "The reference alignment thus obtained may contain many-to-one and one-to-many relationships.", "labels": [], "entities": []}, {"text": "shows an example of a manually aligned sentence with Sand P labels.", "labels": [], "entities": []}, {"text": "The quality of an alignment A = {(j, a j ) | a j > 0} is then computed by appropriately redefined precision and recall measures: and the following alignment error rate (AER), which is derived from the well-known F-measure: These definitions of precision, recall and the AER are based on the assumption that a recall error can occur only if an S alignment is not found and a precision error can occur only if the found alignment is not even P.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9954549074172974}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9719608426094055}, {"text": "alignment error rate (AER)", "start_pos": 147, "end_pos": 173, "type": "METRIC", "confidence": 0.9592078526814779}, {"text": "F-measure", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9455832839012146}, {"text": "precision", "start_pos": 244, "end_pos": 253, "type": "METRIC", "confidence": 0.9978083968162537}, {"text": "recall", "start_pos": 255, "end_pos": 261, "type": "METRIC", "confidence": 0.9959709048271179}, {"text": "AER", "start_pos": 270, "end_pos": 273, "type": "METRIC", "confidence": 0.9954040050506592}, {"text": "recall", "start_pos": 309, "end_pos": 315, "type": "METRIC", "confidence": 0.9688370823860168}, {"text": "precision", "start_pos": 374, "end_pos": 383, "type": "METRIC", "confidence": 0.9971221089363098}]}, {"text": "The set of sentence pairs for which the manual alignment is produced is randomly selected from the training corpus.", "labels": [], "entities": []}, {"text": "It should be emphasized that all the training of the models is performed in a completely unsupervised way (i.e., no manual alignments are used).", "labels": [], "entities": []}, {"text": "From this point of view, there is no need to have a test corpus separate from the training corpus.", "labels": [], "entities": []}, {"text": "Typically, the annotation is performed by two human annotators, producing sets S 1 , P 1 , S 2 , P 2 . To increase the quality of the resulting reference alignment, the annotators are presented with the mutual errors and asked to improve their alignments where possible.", "labels": [], "entities": []}, {"text": "(Mutual errors of the two annotators A and B are the errors in the alignment of annotator A if we assume the alignment of annotator B as reference and the errors in the alignment of annotator B if we assume the alignment of annotator A as reference.)", "labels": [], "entities": []}, {"text": "From these alignments, we finally generate a reference alignment that contains only those S connections on which both annotators agree and all P connections from both annotators.", "labels": [], "entities": []}, {"text": "This can be accomplished by forming the intersection of the sure alignments (S = S 1 \u2229S 2 ) and the union of the possible alignments (P = P 1 \u222aP 2 ), respectively.", "labels": [], "entities": []}, {"text": "By generating the reference alignment in this way, we obtain an alignment error rate of 0 percent when we compare the S alignments of every single annotator with the combined reference alignment.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 64, "end_pos": 84, "type": "METRIC", "confidence": 0.9545349280039469}]}, {"text": "We present in this section results of experiments involving the Verbmobil and Hansards tasks.", "labels": [], "entities": [{"text": "Verbmobil and Hansards tasks", "start_pos": 64, "end_pos": 92, "type": "DATASET", "confidence": 0.7078375816345215}]}, {"text": "The Verbmobil task) is a (German-English) speech translation task  in the domain of appointment scheduling, travel planning, and hotel reservation.", "labels": [], "entities": [{"text": "German-English) speech translation task", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.7247084379196167}, {"text": "appointment scheduling", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7160047590732574}, {"text": "travel planning", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.687280684709549}, {"text": "hotel reservation", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.7049319893121719}]}, {"text": "The bilingual sentences used in training are correct transcriptions of spoken dialogues.", "labels": [], "entities": []}, {"text": "However, they include spontaneous speech effects such as hesitations, false starts, and ungrammatical phrases.", "labels": [], "entities": []}, {"text": "The French-English Hansards task consists of the debates in the Canadian parliament.", "labels": [], "entities": [{"text": "French-English Hansards task", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.5892093976338705}]}, {"text": "This task has a very large vocabulary of about 100,000 French words and 80,000 English words.", "labels": [], "entities": []}, {"text": "Statistics for the two corpora are shown in.", "labels": [], "entities": []}, {"text": "The number of running words and the vocabularies are based on full-form words and the punctuation marks.", "labels": [], "entities": []}, {"text": "We produced smaller training corpora by randomly choosing 500, 2,000 and 8,000 sentences from the Verbmobil task and 500, 8,000, and 128,000 sentences from the Hansards task.", "labels": [], "entities": [{"text": "Verbmobil", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.7273654341697693}, {"text": "Hansards", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.857650101184845}]}, {"text": "For both tasks, we manually aligned a randomly chosen subset of the training corpus.", "labels": [], "entities": []}, {"text": "From this subset of the corpus, the first 100 sentences are used as the development corpus to optimize the model parameters that are not trained via the EM algorithm (e.g., the smoothing parameters).", "labels": [], "entities": []}, {"text": "The remaining sentences are used as the test corpus.", "labels": [], "entities": []}, {"text": "The sequence of models used and the number of training iterations used for each model is referred to in the following as the training scheme.", "labels": [], "entities": []}, {"text": "Our standard training scheme on Verbmobil is 1 5 H 5 3 3 4 3 6 3 . This notation indicates that five iterations of Model 1, five iterations of HMM, three iterations of Model 3, three iterations of Model 4, and three iterations of Model 6 are performed.", "labels": [], "entities": [{"text": "Verbmobil", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8473918437957764}, {"text": "HMM", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.800823986530304}]}, {"text": "On Hansards, we use 1 5 H 10 3 3 4 3 6 3 . This training scheme typically gives very good results and does not lead to overfitting.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.9286410808563232}]}, {"text": "We use the slightly modified versions of Model 3 and Model 4 described in Section 3.2 and smooth the fertility and the alignment parameters.", "labels": [], "entities": [{"text": "fertility", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9727510213851929}]}, {"text": "In the E-step of the EM algorithm for the fertility-based alignment models, we use the Viterbi alignment and its neighborhood.", "labels": [], "entities": []}, {"text": "Unless stated otherwise, no bilingual dictionary is used in training.", "labels": [], "entities": []}, {"text": "compare the alignment quality achieved using various models and training schemes.", "labels": [], "entities": [{"text": "alignment", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.9296683073043823}]}, {"text": "In general, we observe that the refined models (Models 4, 5, and 6) yield significantly better results than the simple Model 1 or Dice coefficient.", "labels": [], "entities": []}, {"text": "Typically, the best results are obtained with Model 6.", "labels": [], "entities": []}, {"text": "This holds across a wide range of sizes for the training corpus, from an extremely small training corpus of only 500 sentences up to a training corpus of 1.5 million sentences.", "labels": [], "entities": []}, {"text": "The improvement that results from using a larger training corpus is more significant, however, if more refined models are used.", "labels": [], "entities": []}, {"text": "Interestingly, even on a tiny corpus of only 500 sentences, alignment error rates under 30% are achieved for all models, and the best models have error rates somewhat under 20%.", "labels": [], "entities": [{"text": "alignment error", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.7804049253463745}]}], "tableCaptions": [{"text": " Table 2  Corpus characteristics of the Verbmobil task.", "labels": [], "entities": []}, {"text": " Table 3  Corpus characteristics of the Hansards task.", "labels": [], "entities": [{"text": "Hansards task", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8802218735218048}]}, {"text": " Table 4  Comparison of alignment error rate percentages for various training schemes (Verbmobil task;  Dice+C: Dice coefficient with competitive linking).", "labels": [], "entities": [{"text": "alignment error rate percentages", "start_pos": 24, "end_pos": 56, "type": "METRIC", "confidence": 0.750795878469944}, {"text": "Verbmobil", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.7824065089225769}]}, {"text": " Table 5  Comparison of alignment error rate percentages for various training schemes (Hansards task;  Dice+C: Dice coefficient with competitive linking).", "labels": [], "entities": [{"text": "alignment error rate percentages", "start_pos": 24, "end_pos": 56, "type": "METRIC", "confidence": 0.7520170137286186}, {"text": "Hansards task", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.7775315344333649}]}, {"text": " Table 6  Effect of using more alignments in training fertility models on alignment error rate (Verbmobil  task). Body of table presents error rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 74, "end_pos": 94, "type": "METRIC", "confidence": 0.9069583018620809}, {"text": "Verbmobil", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.7811673879623413}]}, {"text": " Table 7  Effect of using more alignments in training fertility models on alignment error rate (Hansards  task). Body of table presents error rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 74, "end_pos": 94, "type": "METRIC", "confidence": 0.9045612613360087}]}, {"text": " Table 8  Computing time on the 34K Verbmobil task (on 600 MHz Pentium III machine).", "labels": [], "entities": []}, {"text": " Table 9  Effect of smoothing on alignment error rate (Verbmobil task, Model 6). Body of table presents  error rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 33, "end_pos": 53, "type": "METRIC", "confidence": 0.7259885867436727}]}, {"text": " Table 10  Effect of smoothing on alignment error rate (Hansards task, Model 6). Body of table presents  error rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 34, "end_pos": 54, "type": "METRIC", "confidence": 0.7412150998910269}]}, {"text": " Table 11  Effect of word classes on alignment error rate (Verbmobil task). Body of table presents error  rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.7011081178983053}]}, {"text": " Table 12  Effect of word classes on alignment error rate (Hansards task). Body of table presents error  rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.7080764273802439}]}, {"text": " Table 13  Effect of using a conventional dictionary on alignment error rate (Verbmobil task). Body of  table presents error rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 56, "end_pos": 76, "type": "METRIC", "confidence": 0.7058226863543192}]}, {"text": " Table 14  Effect of using a conventional dictionary on alignment error rate (Hansards task). Body of  table presents error rate percentages.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 56, "end_pos": 76, "type": "METRIC", "confidence": 0.7203450997670492}]}, {"text": " Table 15  Effect of training corpus size and translation direction on precision, recall, and alignment error  rate (Verbmobil task + dictionary). All figures are percentages.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9994974136352539}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9985948204994202}, {"text": "alignment error  rate", "start_pos": 94, "end_pos": 115, "type": "METRIC", "confidence": 0.939037561416626}]}, {"text": " Table 16  Effect of training corpus size and translation direction on precision, recall, and alignment error  rate (Hansards task + dictionary). All figures are percentages.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9995021820068359}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9986785054206848}, {"text": "alignment error  rate", "start_pos": 94, "end_pos": 115, "type": "METRIC", "confidence": 0.9262277682622274}]}, {"text": " Table 17  Effect of alignment combination on precision, recall, and alignment error rate (Verbmobil task  + dictionary). All figures are percentages.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9994702935218811}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9987457990646362}, {"text": "alignment error rate", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.9491773049036661}, {"text": "Verbmobil task  + dictionary", "start_pos": 91, "end_pos": 119, "type": "METRIC", "confidence": 0.7420128434896469}]}, {"text": " Table 18  Effect of alignment combination on precision, recall, and alignment error rate (Hansards task +  dictionary). All figures are percentages.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9995115995407104}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9988688826560974}, {"text": "alignment error rate", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.9481111168861389}]}]}