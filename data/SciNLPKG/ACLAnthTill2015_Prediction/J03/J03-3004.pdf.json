{"title": [{"text": "wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web", "labels": [], "entities": [{"text": "wEBMT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8653379082679749}, {"text": "Example-Based Machine Translation", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.6537622511386871}]}], "abstractContent": [{"text": "We have developed an example-based machine translation (EBMT) system that uses the World Wide Web for two different purposes: First, we populate the system's memory with translations gathered from rule-based MT systems located on the Web.", "labels": [], "entities": [{"text": "machine translation (EBMT)", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8481667041778564}]}, {"text": "The source strings input to these systems were extracted automatically from an extremely small subset of the rule types in the Penn-II Treebank.", "labels": [], "entities": [{"text": "Penn-II Treebank", "start_pos": 127, "end_pos": 143, "type": "DATASET", "confidence": 0.99251988530159}]}, {"text": "In subsequent stages, the \ud97b\udf59source, target\ud97b\udf59 translation pairs obtained are automatically transformed into a series of resources that render the translation process more successful.", "labels": [], "entities": []}, {"text": "Despite the fact that the output from on-line MT systems is often faulty, we demonstrate in a number of experiments that when used to seed the memories of an EBMT system, they can in fact prove useful in generating translations of high quality in a robust fashion.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.975361704826355}]}, {"text": "In addition, we demonstrate the relative gain of EBMT in comparison to on-line systems.", "labels": [], "entities": []}, {"text": "Second, despite the perception that the documents available on the Web are of questionable quality, we demonstrate in contrast that such resources are extremely useful in automatically postediting translation candidates proposed by our system.", "labels": [], "entities": []}], "introductionContent": [{"text": "In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator's armory.", "labels": [], "entities": [{"text": "translation memory (TM)", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.9146966218948365}]}, {"text": "TM systems store a set of \ud97b\udf59source, target\ud97b\udf59 translation pairs in their databases.", "labels": [], "entities": []}, {"text": "If anew input string cannot be found exactly in the translation database, a search is conducted for close (or \"fuzzy\") matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation.", "labels": [], "entities": []}, {"text": "From this description, it should be clear that TM systems do not translate: Indeed, some researchers consider them to belittle more than a search-and-replace engine, albeit a rather sophisticated one.", "labels": [], "entities": [{"text": "TM", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9594957828521729}]}, {"text": "We can illustrate this with respect to the TM entries in (1), taken from the Canadian Hansards: (1) a.", "labels": [], "entities": [{"text": "Canadian Hansards", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.8495306074619293}]}, {"text": "While most were critical, some contributions were thoughtful and constructive =\u21d2 La plupart ont formu\u00ed e des critiques, mais certains ont fait des observations r \u00b4 efl\u00e9chies et constructives. b. Others were plain meanspirited and some contained errors of fact =\u21d2 D'autres discours comportaient des propos mesquins et m \u02c6 eme des erreurs de fait.", "labels": [], "entities": []}, {"text": "Consider the new source string in (2): (2) While most were critical, some contributions were plain meanspirited.", "labels": [], "entities": []}, {"text": "Despite the fact that this new input in (2) is extremely close to the source strings in the TM entries in (1), no TM system containing just these translation pairs in its database would be able to translate (2); the best they could do would be to identify one or both of the two source sentences in the TM in (1) as fuzzy matches and display these, together with their French translations.", "labels": [], "entities": []}, {"text": "The translator would then manipulate the target strings in the TM into the final translation (3): (3) La plupart ont formul\u00e9 des critiques, mais certains ont fait des observations mesquines.", "labels": [], "entities": []}, {"text": "An alternative translation that might be derived from the TM entries in (1) is that in (4): (4) La plupart ont formul\u00e9 des critiques, mais certains comportaient des observations mesquines.", "labels": [], "entities": []}, {"text": "At all stages in the translation process, therefore, the translators themselves are the integral figures: They are free to accept or reject any suggested matches, they construct the translations, and they mayor may not use any translations proposed by the TM system to formulate the translations in the target document.", "labels": [], "entities": []}, {"text": "Finally, they are free to insert the translations produced into the TM itself as they see fit: that is, either (3) or (4) could be inserted into the TM with the source string (2), or some other translation, if that were preferred.", "labels": [], "entities": []}, {"text": "A prerequisite for TM (and example-based machine translation) applications is a parallel corpus aligned at sentential level.", "labels": [], "entities": [{"text": "TM", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9618381857872009}, {"text": "example-based machine translation)", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7320172563195229}]}, {"text": "Such a corpus maybe presented to translators en bloc, or translators may help construct it themselves.", "labels": [], "entities": []}, {"text": "Here too the translator maintains a large degree of autonomy: Using a tool such as Trados WinAlign, for example, he or she may manually overwrite some of the aligner's decisions by linking \ud97b\udf59source, target\ud97b\udf59 sentence pairs using the graphical interface provided.", "labels": [], "entities": []}, {"text": "Nevertheless, TM systems are currently falling far short of their potential, given the limitation that the smallest accessible translation units are \ud97b\udf59source, target\ud97b\udf59 strings aligned only at sentential level.", "labels": [], "entities": [{"text": "TM", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9663827419281006}]}, {"text": "Consider the fuzzy matching operation, for instance: Translators are able to set a fuzzy match threshold below which no translation pairs are proposed by the TM system.", "labels": [], "entities": [{"text": "fuzzy matching", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7361678183078766}]}, {"text": "If this threshold is set too low, then potentially useful translation pairs will be presented along with a lot of noise, thereby risking that this useful translation information will be obscured (high recall, low precision); if it is set too low, then good matches will be presented, but potentially useful matches will not be (low recall, high precision).", "labels": [], "entities": [{"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9916793704032898}, {"text": "precision", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.9865520000457764}, {"text": "recall", "start_pos": 332, "end_pos": 338, "type": "METRIC", "confidence": 0.971991777420044}, {"text": "precision", "start_pos": 345, "end_pos": 354, "type": "METRIC", "confidence": 0.951888382434845}]}, {"text": "We noted above that faced with the new input in (2), a TM system might be able to present the translator with the fuzzy matches in (1).", "labels": [], "entities": []}, {"text": "However, if a translator were to set the level of fuzzy matching at 80% (a not unreasonable level), then neither of the translation pairs in (1) would be deemed to be a suitably good fuzzy match, as only 7/9 (77%) of the words in (1a) match those in (2) exactly, and only 3/9 (33%) of the words in (1b) match those in (2) exactly.", "labels": [], "entities": []}, {"text": "Indeed, setting an appropriate fuzzy match level is such a difficult problem that some translators switch off this option and use the TM only to find exact matches.", "labels": [], "entities": []}, {"text": "If subsentential alignment could be integrated into the TM databases, more useful fragments could be put at the disposal of the translator.", "labels": [], "entities": [{"text": "subsentential alignment", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7109537273645401}]}, {"text": "If we could fragment the sententially aligned TM examples in (1) so that subsentential chunks were displayed to the user, then the chance of finding exact matches or good fuzzy matches would increase considerably.", "labels": [], "entities": []}, {"text": "This is currently beyond the scope of TM systems.", "labels": [], "entities": [{"text": "TM", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9663251638412476}]}, {"text": "In contrast, EBMT systems have overcome this constraint by storing subsentential translational correspondences in addition to the sententially aligned pairs from which they are derived.", "labels": [], "entities": []}, {"text": "As a consequence, where a TM system can only propose a number of close-scoring matches in its database for the translator to adapt into the final translation, an EBMT system can produce translations itself by automatically combining chunks from different translation examples stored in its memories.", "labels": [], "entities": []}, {"text": "In Section 2, we describe how we automatically obtain a hierarchy of lexical resources that are used sequentially by our EBMT system, wEBMT, to translate new input.", "labels": [], "entities": []}, {"text": "The primary resource gathered is a \"phrasal lexicon,\" constructed by extracting over 200,000 phrases from the Penn Treebank and having them translated into French by three Web-based machine translation (MT) systems.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.9941984415054321}, {"text": "machine translation (MT)", "start_pos": 182, "end_pos": 206, "type": "TASK", "confidence": 0.838958990573883}]}, {"text": "Each set of translations is stored separately, and for each set the \"marker hypothesis\") is used to segment the phrasal lexicon into a \"marker lexicon.\"", "labels": [], "entities": []}, {"text": "The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are \"marked\" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.", "labels": [], "entities": []}, {"text": "That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment.", "labels": [], "entities": [{"text": "phrase-level segmentation of an input sentence", "start_pos": 17, "end_pos": 63, "type": "TASK", "confidence": 0.8119383205970129}]}, {"text": "Consider the following example, selected at random from the Wall Street Journal section of the Penn-II Treebank: The Dearborn, Mich., energy company stopped paying a dividend in the third quarter of 1984 because of troubles at its Midland nuclear plant.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn-II Treebank", "start_pos": 60, "end_pos": 111, "type": "DATASET", "confidence": 0.9419250190258026}, {"text": "Dearborn, Mich., energy company", "start_pos": 117, "end_pos": 148, "type": "DATASET", "confidence": 0.7609327435493469}]}, {"text": "Here we see that three noun phrases start with determiners and one with a possessive pronoun.", "labels": [], "entities": []}, {"text": "The sets of determiners and possessive pronouns are both very small.", "labels": [], "entities": []}, {"text": "Furthermore, there are four prepositional phrases, and the set of prepositions is similarly small.", "labels": [], "entities": []}, {"text": "A further assumption that could be made is that all words that end with -ed are verbs, such as stopped in (5).", "labels": [], "entities": []}, {"text": "The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages.", "labels": [], "entities": []}, {"text": "The marker hypothesis has been used fora number of different language-related tasks, including \u2022 language learning \u2022 monolingual grammar induction \u2022 grammar optimization \u2022 insights into universal grammar \u2022 machine translation With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.", "labels": [], "entities": [{"text": "monolingual grammar induction", "start_pos": 117, "end_pos": 146, "type": "TASK", "confidence": 0.6982241868972778}, {"text": "grammar optimization", "start_pos": 149, "end_pos": 169, "type": "TASK", "confidence": 0.7011265903711319}, {"text": "machine translation", "start_pos": 206, "end_pos": 225, "type": "TASK", "confidence": 0.7939226925373077}]}, {"text": "work showed that artificial languages, both with and without specific marker words, maybe learned more accurately and quickly if such psycholinguistic cues exist.", "labels": [], "entities": []}, {"text": "The research of showed a similar effect due to case marking on pseudowords in such artificial languages, and demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.) work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form.\"", "labels": [], "entities": [{"text": "grammar optimization", "start_pos": 258, "end_pos": 278, "type": "TASK", "confidence": 0.7253710627555847}]}, {"text": "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.", "labels": [], "entities": []}, {"text": "Nevertheless, Juola (1998, page 23) observes that \"a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.\"", "labels": [], "entities": []}, {"text": "Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9911841154098511}]}, {"text": "Following construction of the marker lexicon, the \ud97b\udf59source, target\ud97b\udf59 chunks are generalized further using a methodology based onto permit a limited form of insertion in the translation process.", "labels": [], "entities": []}, {"text": "As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "labels": [], "entities": []}, {"text": "These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.", "labels": [], "entities": []}, {"text": "In Section 3, we describe in detail the segmentation process, together with the procedure whereby target chunks are combined to produce candidate translations.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9839494824409485}]}, {"text": "In Section 4, we report initially on two experiments in which we test different versions of our EBMT system against test sets of NPs and sentences.", "labels": [], "entities": []}, {"text": "We then conduct a set of further experiments which show that using the resources developed from more than one on-line MT system may improve both translation coverage and quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.9728875160217285}, {"text": "translation coverage", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.883661687374115}]}, {"text": "Furthermore, seeding the system databases with more fragments improves translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9561965465545654}]}, {"text": "In addition, we calculate the net gain of our EBMT system by comparing translation quality against that of the three on-line MT systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9223250150680542}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9781770706176758}]}, {"text": "Finally, we comment on the relative strengths and weaknesses of the three on-line MT systems used.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9895002245903015}]}, {"text": "Like most EBMT systems, our approach suffers from the problem of \"boundary friction\": where chunks from different translation examples are recombined, the quality of the resulting translations maybe compromised.", "labels": [], "entities": []}, {"text": "Assume that the aligned examples in (6) are located in the system database: a.", "labels": [], "entities": []}, {"text": "You can attach a phone to the connector =\u21d2 Vous pouvez r \u00b4 elier un t \u00b4 e\u00ed ephone au connecteur. b. Connect only the keyboard and a mouse =\u21d2 Connectez uniquement le clavier et une souris.", "labels": [], "entities": [{"text": "Connectez uniquement", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.5920695811510086}]}, {"text": "Let us now confront the EBMT system with the new input string in (7): You can attach a mouse to the connector.", "labels": [], "entities": []}, {"text": "This could be correctly translated by the EBMT system by isolating the useful translation fragments in (8): (8) a.", "labels": [], "entities": [{"text": "EBMT", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9056394696235657}]}, {"text": "You can attach =\u21d2 Vous pouvez r \u00b4 elier (from (6a)) b. a mouse =\u21d2 une souris (from (6b)) c. to the connector =\u21d2 au connecteur (from (6a)) Recombining the French chunks gives us the correct translation in (9): Vous pouvez r\u00e9lier une souris au connecteur.", "labels": [], "entities": []}, {"text": "However, a number of mistranslations could also ensue, including those in (10): (10) a.", "labels": [], "entities": []}, {"text": "*Vous pouvez r\u00e9lier un souris au connecteur. b. *Vous pouvez r\u00e9lier un souris au le connecteur.", "labels": [], "entities": []}, {"text": "The mistranslation (10a) could be formed via the set of inferences in (11): (11) You can attach a =\u21d2 Vous pouvez r \u00b4 elier un (from (6a)) mouse =\u21d2 souris (from (6b)) to the connector =\u21d2 au connecteur (from (6a)) The mistranslation (10b) could be formed via the set of inferences in: You can attach a =\u21d2 Vous pouvez r \u00b4 elier un (from (6a)) mouse =\u21d2 souris (from (6b)) to =\u21d2 au (from (6a)) the =\u21d2 le (from (6b)) connector =\u21d2 connecteur (from (6a)) It is clear, therefore, that unless the process by which the original \ud97b\udf59source, target\ud97b\udf59 sentence pairs are fragmented is well defined and strictly controlled, chunks maybe combined from different contexts that result in agreement errors such as those in (10).", "labels": [], "entities": []}, {"text": "1 Depending on the input string, our wEBMT system may generate thousands of candidate translations, including many mistranslations like those in (10).", "labels": [], "entities": []}, {"text": "A major advantage of MT systems based on probabilities is that output translations can be ranked (and pruned, if required): One would hope that such systems would rank good translations such as that in (9) more highly than poor ones such as those in.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9842644333839417}]}, {"text": "We demonstrate that in almost all experiments, our EBMT system consistently ranks the \"best\" translation in the top 10 output translations, and always in the top 1% of the translations generated.", "labels": [], "entities": []}, {"text": "In order to minimize errors of boundary friction, in Section 5 we develop a novel, post hoc procedure via the World Wide Web to validate and, if necessary, correct translations prior to their being output to the user.", "labels": [], "entities": []}, {"text": "Finally we conclude and point to areas of future research.", "labels": [], "entities": []}, {"text": "1 Note also that with respect to the translations given in (3) and (4), the translator interacting with the TM has used his or her translation knowledge to avoid a problem of boundary friction: Given the TM entries in (1), the translation of plain meanspirited would appear to be mesquins.", "labels": [], "entities": []}, {"text": "This is correct in this context, as it co-occurs with a masculine plural noun propos.", "labels": [], "entities": []}, {"text": "In translating (2), however, observations is a feminine plural noun, so the adjective mesquines is inserted to maintain agreement throughout the NP.", "labels": [], "entities": [{"text": "translating", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9739853143692017}, {"text": "NP", "start_pos": 145, "end_pos": 147, "type": "DATASET", "confidence": 0.9429755806922913}]}, {"text": "If the translation pair \ud97b\udf59plain meanspirited, mesquines\ud97b\udf59 were not found in the system's memories, then only the mistranslation observations mesquins could be produced by an EBMT system.", "labels": [], "entities": []}, {"text": "2 One of the areas of boundary friction that we use our post hoc validation procedure to correct is that of subject-verb agreement.", "labels": [], "entities": []}, {"text": "Note that with examples such as (18), this is not usually (such) a problem for marker-based approaches to MT as we face here, as verbs are contained within (part of) the same chunk as their subject NPs.", "labels": [], "entities": [{"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.98987877368927}]}, {"text": "However, given that we translate phrases rather than sentences, it is a considerable problem for our approach, yet one that we overcome satisfactorily.", "labels": [], "entities": []}, {"text": "In further work, if we were to store the translations of the VPs with their dummy subject NPs in a sentential lexicon and derive all marker lexicons from this database, the problem of subject-verb agreement would be largely overcome.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report hereon a number of experiments using test sets of 200 sentences and 500 noun phrases.", "labels": [], "entities": []}, {"text": "Some typical examples from the two test sets are given in: Noun phrases: \u2022 the heavy use of management fees last year \u2022 an increase through issues of new shares and convertible bonds \u2022 a space-based defense shield for official acts by the congressman  Here we report on experiments in which the two test sets are tackled by our system when its memory is seeded with translations obtained by the individual on-line MT systems specified in Section 2.1.", "labels": [], "entities": [{"text": "MT", "start_pos": 414, "end_pos": 416, "type": "TASK", "confidence": 0.9578189253807068}]}, {"text": "A parameter that is altered in the experiment on the sentential test set is the nature of the dummy subjects used to gather the initial translation fragments: third-person singular, third-person plural, and both third-person singular and third-person plural.", "labels": [], "entities": []}, {"text": "As far as coverage is concerned, our system wEBMT translated 184 (92%) of the sentences using chunks derived from Systems A and C, and using chunks from system B, our system managed to translate 180 sentences (90%).", "labels": [], "entities": []}, {"text": "The same 16 sentences were not translated by any of the systems owing to their failure to locate one or more words in the sentence within the word-level lexicon.", "labels": [], "entities": []}, {"text": "Recall that despite the fact that all words in the test set were seen by the system in the training phase, only those content (i.e., non-marker) words that occur in bigram marker chunks are inserted into the wordlevel lexicon (cf.).", "labels": [], "entities": []}, {"text": "In cases such as these, in which one or more words cannot be translated by our system, partial translations such as those in (40) are output: A little girl misplaced a full page =\u21d2 Une petite fille misplaced une pleine page.", "labels": [], "entities": []}, {"text": "That is, although misplaced was present in the system's database, it was not present in the correct context.", "labels": [], "entities": []}, {"text": "That is, it appeared in the phrasal lexicon, as shown in: were misplaced =\u21d2 ont\u00e9t\u00e9\u00e9gar\u00e9sont\u00b4ont\u00e9tont\u00e9t\u00b4ont\u00e9t\u00e9ont\u00e9t\u00e9\u00b4ont\u00e9t\u00e9\u00e9garont\u00e9t\u00e9\u00e9gar\u00b4ont\u00e9t\u00e9\u00e9gar\u00e9s The form required in (40) is a simple past-tense verb, but misplaced appears in (41) only as a passive participle.", "labels": [], "entities": []}, {"text": "The word were in is not a marker word, so this fragment cannot be broken down any further by our segmentation method.", "labels": [], "entities": []}, {"text": "8 In such cases we output the partial translation with source equivalents for any untranslated words, as shown in (40).", "labels": [], "entities": []}, {"text": "Ninety-six (48%) of the sentences were translated by combining fragments contained in the original phrasal lexicon or the marker lexicon, 56 (28%) of the translations were obtained by locating single content (i.e., non-marker) words in the word-level lexicon and inserting these into the translation at the appropriate position, and 32 (16%) were produced by inserting marker words into generalized templates.", "labels": [], "entities": []}, {"text": "shows the results obtained from our human evaluators' ratings of the translations produced by our system when it was populated with fragments derived from one of the individual on-line MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 185, "end_pos": 187, "type": "TASK", "confidence": 0.9390983581542969}]}, {"text": "Evaluators rated more than one-third of translations as intelligible and without syntactic errors (score 3), with over 85% of translations deemed intelligible (scores 2 and 3) for all systems.", "labels": [], "entities": []}, {"text": "Unintelligble translations (score 1) ranged from 14% for chunks derived from SDL to just 4.4% for translations formed from knowledge sources created from Logomedia.", "labels": [], "entities": []}, {"text": "These initial results provide some evidence in favor of the hypothesis that Logomedia might be the best system.", "labels": [], "entities": []}, {"text": "Although such evaluation is not a primary focus of our work at the outset, our methodology provides as a spin-off an evaluation of the three MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9741367101669312}]}, {"text": "We discuss this further in Section 4.5.", "labels": [], "entities": []}, {"text": "When the system cannot produce a translation fora particular input, the main reason is an absent word in the word-level lexicon.", "labels": [], "entities": []}, {"text": "Adding more lexical entries would improve translation coverage and would also affect translation quality (possibly adversely, in some cases).", "labels": [], "entities": [{"text": "translation coverage", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.912054717540741}]}, {"text": "We plan to measure the impact of a larger lexicon in future work.", "labels": [], "entities": []}, {"text": "Low-quality translations are almost invariably caused by inappropriate verb forms in the word-level dictionary: For the experiments carried out in which all verbs were third-person plural, any NP with a third-person singular subject in the test set would be accompanied by a third-person plural verb in the translation.", "labels": [], "entities": []}, {"text": "A similar effect is seen where the databases of wEBMT were seeded with third-person singular verbs, of course.", "labels": [], "entities": []}, {"text": "However, we should expect an improvement in translation quality when both sets of verb forms are included in the memories of the system (see Experiment 2).", "labels": [], "entities": []}, {"text": "shows where the \"best\" translation, as defined by a human expert, was ranked among the of translations output by our system.", "labels": [], "entities": []}, {"text": "In over 65% of cases, the system itself had ranked the \"best\" translation first, and the \"best\" translation was never located outside the top five ranked translations.", "labels": [], "entities": []}, {"text": "This is remarkable given that over 2,000 translations are output for certain source sentences.", "labels": [], "entities": []}, {"text": "The results for the previous experiment were obtained when the databases were seeded with third-person plural dummy subjects.", "labels": [], "entities": []}, {"text": "We ran two variations on this experiment: (1) we tested the system by seeding its memories with third-person singular dummy subjects, and (2) We tested the system by seeding its memories with both third-person singular and third-person plural dummy subjects.", "labels": [], "entities": []}, {"text": "shows that translation quality improves when the system databases are seeded with more translation pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9711036682128906}]}, {"text": "We can see that the system does slightly better when it uses third-person plural chunks compared to when it uses their singular counterparts.", "labels": [], "entities": []}, {"text": "When third-person singular dummy subjects are inserted in order to derive the initial translation fragments inserted into our system's memories, the number of translations rated 3 for quality deteriorates by about 5% for systems B and C and by about 3% for system A.", "labels": [], "entities": []}, {"text": "Given a larger number of third-person plural NP subjects in our test set, this was to be expected.", "labels": [], "entities": []}, {"text": "However, a considerable improvement in quality can be seen when fragments from  Translation quality improves when system databases are seeded with more translation pairs: Measuring % translation quality using fragments derived from single on-line MT systems.", "labels": [], "entities": []}, {"text": "The test set comprised 500 noun phrases, with an average NP length of 6.14 words (minimum 3 words, maximum 12).", "labels": [], "entities": [{"text": "NP length", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9167096316814423}]}, {"text": "The noun phrases in the test set also need to be fragmented using our n-gram segmentation method, as it is highly probable that they do not exist en bloc in the phrasal lexicon and therefore need to be analyzed using smaller fragments in the system's databases.", "labels": [], "entities": []}, {"text": "We give results for coverage and translation quality in.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.967510461807251}]}, {"text": "These results are for NPs translated via chunks derived from the three individual on-line MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9651269912719727}]}, {"text": "As with the sentence test set, fragments derived from systems A and C achieve the broadest coverage, producing translations for 474 out of the 500 NPs; those obtained from system B enable 463 of the 500 NPs to be translated.", "labels": [], "entities": []}, {"text": "As for quality, wEBMT clearly performs best when using translation fragments derived from system C: 47.3% of these translations were awarded a quality score of 3, more than 10% better than for chunks derived from system B.", "labels": [], "entities": []}, {"text": "For system C, a total of 452 (96%) of the generated translations were deemed intelligible (scores 2 and 3), that is, 31 (6.6%) more translations than with system B.", "labels": [], "entities": []}, {"text": "On average, about 54% of translations are formed by combining chunks from the phrasal lexicon with those from the marker lexicon, about 9% are produced by inserting marker words into the generalized templates, and about 37% are generated by inserting single non-marker words from the word-level lexicon at the appropriate locations in phrasal chunks.", "labels": [], "entities": []}, {"text": "The major reason that translations fail to be produced in 6% of cases is the absence of a relevant generalized template.", "labels": [], "entities": [{"text": "translations", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.9565147161483765}]}, {"text": "For example, the unseen input her negative TV ads is generalized to: <POSS> negative TV ads However, the nearest relevant generalized template found in the system's memory is (43): That is, the template in (43) allows the insertion of any determiner, but no other marker word.", "labels": [], "entities": []}, {"text": "Deriving translation fragments from more examples would lead to an improvement in coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9761843681335449}]}, {"text": "Alternatively, for marker words that appear in the same relative position, such as determiners and possessive pronouns, we could \"back off\" to a more general marker tag to allow mutual substitution of such words in a subsequent operation to enable translation of examples like these.", "labels": [], "entities": []}, {"text": "This remains an area of investigation in future work.", "labels": [], "entities": []}, {"text": "The results in further substantiate our findings on the sentence test set, namely, that system C maybe the best of the three on-line MT systems used to populate the memories of our EBMT system.", "labels": [], "entities": [{"text": "sentence test set", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.6495847503344218}, {"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9741963744163513}]}, {"text": "We comment further on this in Section 4.5.", "labels": [], "entities": []}, {"text": "In addition, these figures provide strong evidence that our system can indeed translate most noun phrases with which it is confronted and with more than reasonable quality.", "labels": [], "entities": []}, {"text": "The results obtained regarding the ranking of the \"best\" translation appear in.", "labels": [], "entities": []}, {"text": "Our system ranks the \"best\" translation first over 57% of the time, and in over 96% of cases, it ranks it in the top five, and at worst in the top ten.", "labels": [], "entities": []}, {"text": "If the three on-line MT systems translate the phrases extracted from the Penn-II Treebank in different ways, then combining systems to obtain results for AB, AC, BC, and ABC always involves an increase in the number of translations produced, both for sentences and noun phrases.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9702355265617371}, {"text": "Penn-II Treebank", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.9918287694454193}, {"text": "AB", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9844565987586975}, {"text": "BC", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9592662453651428}, {"text": "ABC", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.9269372820854187}]}, {"text": "That is, if an input string receives a translation via chunks derived from the individual on-line systems, when chunks are combined from different systems, more translations will be output for that input string.", "labels": [], "entities": []}, {"text": "As an example, the number of translations produced by each system for the NP a plan for reducing debt over 20 years is shown in.", "labels": [], "entities": [{"text": "NP", "start_pos": 74, "end_pos": 76, "type": "DATASET", "confidence": 0.9161302447319031}]}, {"text": "Whereas the greatest number of translations for this NP produced from chunks from any individual on-line system is 14, when translation fragments from all three systems are merged (ABC), 224 translations are produced.", "labels": [], "entities": []}, {"text": "Combining systems in this way means that all possible combinations of chunks from the systems are produced: That is, the number of translations generated via AB is much larger than those derived from either A or B, as now chunks from A and B maybe combined to produce new translations that could not be generated from the individual knowledge sources.", "labels": [], "entities": []}, {"text": "As a further example, consider the translation of the NP the total at risk a year.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9339452981948853}]}, {"text": "When fragments from systems A and B are combined, the \"best\" translation is comprised of the chunk combination AAB, that is, the three-chunk combination in (44), with the first two chunks obtained from system A, and the last from system B: That is, the translation of this NP improves when the performance of system AB is Translation quality improves when system databases are seeded with more translation pairs and when more knowledge sources are used: Measuring % translation quality using fragments derived from combinations of on-line MT systems.", "labels": [], "entities": [{"text": "AAB", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9578033089637756}]}, {"text": "evaluated: Of course, if we consider (say) three-chunk combinations from either system A or B, the only possibilities are AAA or BBB, respectively.", "labels": [], "entities": [{"text": "AAA", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9960983991622925}, {"text": "BBB", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.7609094381332397}]}, {"text": "However, the number of translations produced by the system is less significant than their quality.", "labels": [], "entities": []}, {"text": "The ranking process outlined in Section 3 classifies the translations produced with regard to their position as the \"best\" translation.", "labels": [], "entities": []}, {"text": "In the sections below, we also discuss the issue of quality and show that it improves when more translation fragments are taken into account.", "labels": [], "entities": []}, {"text": "Furthermore, we show below that despite generating more translations per input string, wEBMT still ranks the \"best\" translation in the top 1% of all output translation candidates.", "labels": [], "entities": [{"text": "wEBMT", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.7686771154403687}]}, {"text": "We saw in Experiment 1 that 16 strings in the test set were left untranslated by systems A, B, and C individually.", "labels": [], "entities": []}, {"text": "When knowledge sources are combined, these 16 strings remain untranslated.", "labels": [], "entities": []}, {"text": "However, as shows, the translation quality improves significantly.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9495015740394592}]}, {"text": "The best individual system performance was 36.5% scoring 3.", "labels": [], "entities": []}, {"text": "This rises to a best performance of 48.9% among pairs of systems combined and improves still further to 50% when chunks from all three knowledge sources are combined.", "labels": [], "entities": []}, {"text": "provides results regarding the relative location of the \"best\" translation for sentences.", "labels": [], "entities": []}, {"text": "For all system combinations, the \"best\" translation is to be found among the top 10 ranked translations in all permutations of combinations of chunks, with at least 54% ranked first.", "labels": [], "entities": []}, {"text": "Despite a corresponding rise in the number of translations produced per input sentence when all three knowledge sources are combined (ABC), in over 97% of cases, the \"best\" translation continues to be found in the top five output candidates.", "labels": [], "entities": []}, {"text": "demonstrates that considerable improvements in translation quality are achieved when the memory of wEBMT is seeded with both third-person singular and third-person plural fragments.", "labels": [], "entities": [{"text": "wEBMT", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.8742637634277344}]}, {"text": "For the pairwise combinations, 78.7% of the translations derived from AB are rated 3 for quality, compared to 80.4% of those derived from AC and BC.", "labels": [], "entities": [{"text": "AB", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.8399563431739807}, {"text": "quality", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9794614315032959}]}, {"text": "The results for ABC improve again, to 81.5%.", "labels": [], "entities": [{"text": "ABC", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.7332062125205994}]}, {"text": "Regarding intelligibility (scores 2 and 3 for quality), we can see from that near perfect results are obtained: AB scores 95.6%, and all other combinations score 96.7%.", "labels": [], "entities": [{"text": "AB", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9991281628608704}]}, {"text": "shows the ranking of the \"best\" translation when multiple knowledge sources are employed and both third-person singular and third-person plural dummy subjects are used to populate the system's memories.", "labels": [], "entities": []}, {"text": "The number of instances in which the \"best\" translation is ranked first by wEBMT deteriorates: by 24% for AB, by 15% for AC, by 20% for BC, and by 27% for ABC.", "labels": [], "entities": [{"text": "wEBMT", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9281279444694519}, {"text": "AB", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9007765054702759}, {"text": "BC", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9150990843772888}]}, {"text": "For all system combinations, shows that the \"best\" translation was ranked no lower than 10th; for the system combinations in, sometimes the \"correct\" translation is ranked as low as 36th.", "labels": [], "entities": []}, {"text": "As expected, the worst ranking results are for system combination ABC, in which all system chunks are combined for both third-person singular and third-person plural dummy subjects.", "labels": [], "entities": []}, {"text": "However, even here the \"best\" translation is ranked in the top five in over 63% of cases, and 72.6% of the time it is located among the top 10 ranked translations.", "labels": [], "entities": []}, {"text": "For this system configuration, the lowest we have to look to find the \"best\" translation is 36th.", "labels": [], "entities": []}, {"text": "For that particular sentence (i.e., the one for which the \"best\" translation is ranked 36), over 4,000 possible translations are generated, so even here the \"best\" translation remains in the top 1% of translation candidates.", "labels": [], "entities": []}, {"text": "Ranking of \"best\" translation for sentences: Chunks derived from combinations of on-line MT systems, third-person singular and third-person plural dummy subjects.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9567688703536987}]}, {"text": "System Ranked 1 Ranked 2-5 Ranked 6-10 Ranked 10-20 Ranked  As we did with sentences, we seeded our EBMT system with fragments derived from the three different on-line MT systems and confronted it with the NP test set.", "labels": [], "entities": [{"text": "MT", "start_pos": 168, "end_pos": 170, "type": "TASK", "confidence": 0.9519975185394287}, {"text": "NP test set", "start_pos": 206, "end_pos": 217, "type": "DATASET", "confidence": 0.8854524691899618}]}, {"text": "clearly shows that as more knowledge sources are added, translation quality improves considerably.", "labels": [], "entities": [{"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9636041522026062}]}, {"text": "The worst-performing individual system scores 3 for quality in just over a third of cases, but when all system chunks are combined, this rises to 77.8%.", "labels": [], "entities": [{"text": "quality", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9874132871627808}]}, {"text": "Note also that, unlike with sentences, we see an increase in coverage when more knowledge sources are used, from a low of 92.6% for system B to a high of 96% when all chunks are combined.", "labels": [], "entities": [{"text": "coverage", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9956822395324707}]}, {"text": "Many more improvements are seen when our post hoc validation and correction methodology, described in Section 5, is used, but the merging of fragments derived from different on-line systems also leads to an improvement in translation quality.", "labels": [], "entities": []}, {"text": "Consider the examples in (45): (45) Input: an old story in common System B: une vieille histoire dans commun System Combination BC: une vieille histoire en commun That is, the optional PP in common was mistranslated by system B as dans commun, but when knowledge from system C is added to that of system B, the improved translation en commun is generated.", "labels": [], "entities": [{"text": "BC", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.7580689191818237}]}, {"text": "We saw in Section 4.1.2 that when translating the NP test set, the \"best\" translation, as adjudged by our human evaluators, was to be found no lower than tenth of all translations output by our system.", "labels": [], "entities": [{"text": "NP test set", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.8522855838139852}]}, {"text": "When knowledge sources are combined, it is Ranking of \"best\" translation for NPs: Chunks derived from more that one on-line MT system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Translation quality for sentences: Chunks  derived from individual on-line MT  systems, third-person plural dummy  subjects.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9684343338012695}]}, {"text": " Table 2  Ranking of \"best\" translation for sentences:  Chunks derived from individual on-line MT  systems, third-person plural dummy subjects.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9667880535125732}]}, {"text": " Table 4  Translation coverage and quality for NPs: Chunks  derived from individual on-line MT systems.", "labels": [], "entities": [{"text": "Translation coverage", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7866724133491516}]}, {"text": " Table 5  Ranking of \"best\" translation for NPs: Chunks derived from  individual on-line MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9547269344329834}]}, {"text": " Table 5. Our system ranks the \"best\" translation first over 57% of the time, and in  over 96% of cases, it ranks it in the top five, and at worst in the top ten.", "labels": [], "entities": []}, {"text": " Table 7  Ranking of \"best\" translation for sentences: Chunks  derived from combinations of on-line MT systems,  third-person plural dummy subjects.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9564211368560791}]}, {"text": " Table 8  Ranking of \"best\" translation for sentences: Chunks derived from combinations of  on-line MT systems, third-person singular and third-person plural dummy  subjects.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9617789387702942}]}, {"text": " Table 9  NPs: Coverage and quality improve when  fragments from different sources are included.", "labels": [], "entities": [{"text": "quality", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.962588369846344}]}, {"text": " Table 10  Ranking of \"best\" translation for NPs: Chunks derived from  more that one on-line MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9633879661560059}]}, {"text": " Table 12  Using the Web to improve noun-verb agreement", "labels": [], "entities": [{"text": "noun-verb agreement", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7497031688690186}]}]}