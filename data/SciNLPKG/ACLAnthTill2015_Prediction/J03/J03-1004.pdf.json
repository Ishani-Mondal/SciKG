{"title": [{"text": "A Machine Learning Approach to Modeling Scope Preferences", "labels": [], "entities": [{"text": "Modeling Scope Preferences", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.8345093925793966}]}], "abstractContent": [{"text": "This article describes a corpus-based investigation of quantifier scope preferences.", "labels": [], "entities": []}, {"text": "Following recent work on multimodular grammar frameworks in theoretical linguistics and along history of combining multiple information sources in natural language processing, scope is treated as a distinct module of grammar from syntax.", "labels": [], "entities": []}, {"text": "This module incorporates multiple sources of evidence regarding the most likely scope reading fora sentence and is entirely data-driven.", "labels": [], "entities": []}, {"text": "The experiments discussed in this article evaluate the performance of our models in predicting the most likely scope reading fora particular sentence, using Penn Treebank data both with and without syntactic annotation.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.9923543930053711}]}, {"text": "We wish to focus attention on the issue of determining scope preferences, which has largely been ignored in theoretical linguistics, and to explore different models of the interaction between syntax and quantifier scope.", "labels": [], "entities": []}, {"text": "1. Overview This article addresses the issue of determining the most accessible quantifier scope reading fora sentence.", "labels": [], "entities": []}, {"text": "Quantifiers are elements of natural and logical languages (such as each, no, and some in English and \u2200 and \u2203 in predicate calculus) that have certain semantic properties.", "labels": [], "entities": []}, {"text": "Loosely speaking, they express that a proposition holds for some proportion of a set of individuals.", "labels": [], "entities": []}, {"text": "One peculiarity of these expressions is that there can be semantic differences that depend on the order in which the quantifiers are interpreted.", "labels": [], "entities": []}, {"text": "These are known as scope differences.", "labels": [], "entities": []}, {"text": "(1) Everyone likes two songs on this album.", "labels": [], "entities": []}, {"text": "As an example of the sort of interpretive differences we are talking about, consider the sentence in (1).", "labels": [], "entities": []}, {"text": "There are two readings of this sentence; which reading is meant depends on which of the two quantified expressions everyone and two songs on this album takes wide scope.", "labels": [], "entities": []}, {"text": "The first reading, in which everyone takes wide scope, simply implies that every person has a certain preference, not necessarily related to anyone else's.", "labels": [], "entities": []}, {"text": "This reading can be paraphrased as \"Pick any person, and that person will like two songs on this album.\"", "labels": [], "entities": []}, {"text": "The second reading, in which everyone takes narrow scope, implies that there are two specific songs on the album of which everyone is fond, say, \"Blue Moon\" and \"My Way.\"", "labels": [], "entities": [{"text": "Blue Moon", "start_pos": 146, "end_pos": 155, "type": "DATASET", "confidence": 0.8592771887779236}]}, {"text": "In theoretical linguistics, attention has been primarily focused on the issue of scope generation.", "labels": [], "entities": [{"text": "scope generation", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.8562755584716797}]}, {"text": "Researchers applying the techniques of quantifier raising and Cooper storage have been concerned mainly with enumerating all of the scope readings fora", "labels": [], "entities": [{"text": "quantifier raising", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8292917609214783}, {"text": "Cooper storage", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.6834745854139328}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 2  Baseline performance, summed over all ten test sets.", "labels": [], "entities": []}, {"text": " Table 3  Performance of the naive Bayes classifier, summed over all 10 test runs.", "labels": [], "entities": []}, {"text": " Table 4  Most active features from naive Bayes classifier.", "labels": [], "entities": []}, {"text": " Table 5  Performance of the maximum-entropy classifier, summed over all 10 test runs.", "labels": [], "entities": []}, {"text": " Table 6  Most active features from maximum-entropy classifier.", "labels": [], "entities": []}, {"text": " Table 7  Performance of the single-layer perceptron, summed over all 10 test runs.", "labels": [], "entities": []}, {"text": " Table 8  Most active features from single-layer perceptron.", "labels": [], "entities": []}, {"text": " Table 9  Summary of classifier results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8021569848060608}]}, {"text": " Table 11  Rules derived from sections 00-20 of the Penn Treebank WSJ corpus. \"TOP\" is a special \"start\"  symbol that may expand to any of the symbols found at the root of a tree in the corpus.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 52, "end_pos": 76, "type": "DATASET", "confidence": 0.9822949320077896}, {"text": "TOP", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9877279996871948}]}, {"text": " Table 12  Performance of models on the unlabeled scope prediction task, summed over all 10 test runs.", "labels": [], "entities": [{"text": "scope prediction task", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8689559499422709}]}]}