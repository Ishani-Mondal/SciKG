{"title": [{"text": "Head-Driven Statistical Models for Natural Language Parsing", "labels": [], "entities": [{"text": "Natural Language Parsing", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.695419987042745}]}], "abstractContent": [{"text": "This article describes three statistical models for natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.6333562731742859}]}, {"text": "The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.", "labels": [], "entities": []}, {"text": "Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment.", "labels": [], "entities": []}, {"text": "All of these preferences are expressed by probabilities conditioned on lexical heads.", "labels": [], "entities": []}, {"text": "The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature.", "labels": [], "entities": [{"text": "Penn Wall Street Journal Treebank", "start_pos": 32, "end_pos": 65, "type": "DATASET", "confidence": 0.964688491821289}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9992396831512451}]}, {"text": "To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9987468719482422}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9196380376815796}]}, {"text": "We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples.", "labels": [], "entities": [{"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9754069447517395}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8856251835823059}]}, {"text": "Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.", "labels": [], "entities": [{"text": "parsing", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.9710370898246765}]}], "introductionContent": [{"text": "Ambiguity is a central problem in natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6330232123533884}]}, {"text": "Combinatorial effects mean that even relatively short sentences can receive a considerable number of parses under a wide-coverage grammar.", "labels": [], "entities": []}, {"text": "Statistical parsing approaches tackle the ambiguity problem by assigning a probability to each parse tree, thereby ranking competing trees in order of plausibility.", "labels": [], "entities": [{"text": "Statistical parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6623417139053345}]}, {"text": "In many statistical models the probability for each candidate tree is calculated as a product of terms, each term corresponding to some substructure within the tree.", "labels": [], "entities": []}, {"text": "The choice of parameterization is essentially the choice of how to represent parse trees.", "labels": [], "entities": []}, {"text": "There are two critical questions regarding the parameterization of a parsing approach:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2  Results on Section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs is the  average number of crossing brackets per sentence. 0 CBs, \u2264 2 CBs are the percentage of  sentences with 0 or \u2264 2 crossing brackets respectively. All the results in this table are for  models trained and tested on the same data, using the same evaluation metric. (Note that  these results show a slight improvement over those in (Collins 97); the main model changes  were the improved treatment of punctuation (section 4.3) together with the addition of the Pp  and Pcc parameters.)", "labels": [], "entities": [{"text": "Section 23 of the WSJ Treebank", "start_pos": 21, "end_pos": 51, "type": "DATASET", "confidence": 0.7940586805343628}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8330860733985901}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9057888984680176}, {"text": "Collins 97)", "start_pos": 427, "end_pos": 438, "type": "DATASET", "confidence": 0.9683769543965658}]}, {"text": " Table 3  Recall and precision for different constituent types, for section 0 of the treebank with model 2.  Label is the nonterminal label; Proportion is the percentage of constituents in the treebank  section 0 that have this label; Count is the number of constituents that have this label.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9996172189712524}, {"text": "Count", "start_pos": 235, "end_pos": 240, "type": "METRIC", "confidence": 0.9975027441978455}]}, {"text": " Table 4  Dependency accuracy on section 0 of the treebank with Model 2. No labels means that only the  dependency needs to be correct; the relation may be wrong; No complements means all  complement (-C) markings are stripped before comparing relations; All means complement  markings are retained on the modifying nonterminal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9306212663650513}]}, {"text": " Table 5  Accuracy of the 50 most frequent dependency types in section 0 of the treebank, as recovered  by model 2.", "labels": [], "entities": []}, {"text": " Table 6  Accuracy for various types/subtypes of dependency (part 1). Only subtypes occurring more  than 10 times are shown.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9885343909263611}]}, {"text": " Table 7  Results on section 0 of the WSJ Treebank. A \"YES\" in the A column means that the adjacency  conditions were used in the distance measure; likewise, a \"YES\" in the V column indicates  that the verb conditions were used in the distance measure. LR = labeled recall; LP = labeled  precision. CBs is the average number of crossing brackets per sentence. 0 CBs \u2264 2 CBs are the  percentages of sentences with 0 and \u2264 2 crossing brackets, respectively.", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9873179197311401}, {"text": "YES", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9487664699554443}, {"text": "YES", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.966237485408783}, {"text": "recall", "start_pos": 266, "end_pos": 272, "type": "METRIC", "confidence": 0.8482697010040283}, {"text": "precision", "start_pos": 288, "end_pos": 297, "type": "METRIC", "confidence": 0.9022610783576965}, {"text": "CBs", "start_pos": 299, "end_pos": 302, "type": "METRIC", "confidence": 0.9399765133857727}]}, {"text": " Table 9  Distribution of nonterminals generated as postmodifiers to a verb within a VP (see tree to the  left), at various distances from the head. A = True means the modifier is adjacent to the head;  V = True means there is a verb between the head and the modifier. The distributions were  calculated from the first 10000 events for each of the distributions in sections 2-21. Auxiliary  verbs (verbs taking a VP complement to their right) were excluded from these statistics.", "labels": [], "entities": []}, {"text": " Table 10  Statistics for rules taken from sections 2-21 of the treebank, with complement markings not  included on nonterminals.", "labels": [], "entities": [{"text": "the treebank", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.7555762231349945}]}, {"text": " Table 11  Statistics for rules taken from sections 2-21 of the treebank, with complement markings  included on nonterminals.", "labels": [], "entities": [{"text": "the treebank", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.7814138531684875}]}, {"text": " Table 12  Results on section 0 of the Treebank. The label restricted means the model is restricted to  recovering rules that have been seen in training data. LR = labeled recall. LP = labeled  precision. CBs is the average number of crossing brackets per sentence. 0 CBs and \u2264 2 CBs are  the percentages of sentences with 0 and \u2264 2 crossing brackets, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.8930636644363403}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.7864645719528198}]}]}