{"title": [{"text": "Word Reordering and a Dynamic Programming Beam Search Algorithm for Statistical Machine Translation", "labels": [], "entities": [{"text": "Word Reordering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7044919580221176}, {"text": "Statistical Machine Translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.8166908224423727}]}], "abstractContent": [{"text": "In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP).", "labels": [], "entities": [{"text": "beam search", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.774511307477951}, {"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.7271847426891327}]}, {"text": "The search algorithm uses the translation model presented in Brown et al.", "labels": [], "entities": []}, {"text": "Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm.", "labels": [], "entities": []}, {"text": "Word reordering restrictions especially useful for the translation direction German to English are presented.", "labels": [], "entities": [{"text": "Word reordering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6978869140148163}]}, {"text": "The restrictions are generalized , and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions.", "labels": [], "entities": []}, {"text": "The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).", "labels": [], "entities": [{"text": "beam search", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9168267548084259}, {"text": "Verbmobil", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.6981043815612793}, {"text": "Canadian Hansards task", "start_pos": 131, "end_pos": 153, "type": "DATASET", "confidence": 0.8529921770095825}]}, {"text": "For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article.", "labels": [], "entities": []}], "introductionContent": [{"text": "This article is about a search procedure for statistical machine translation (MT).", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.8363020320733389}]}, {"text": "The task of the search procedure is to find the most likely translation given a source sentence and a set of model parameters.", "labels": [], "entities": []}, {"text": "Here, we will use a trigram language model and the translation model presented in.", "labels": [], "entities": []}, {"text": "Since the number of possible translations of a given source sentence is enormous, we must find the best output without actually generating the set of all possible translations; instead we would like to focus on the most likely translation hypotheses during the search process.", "labels": [], "entities": []}, {"text": "For this purpose, we present a data-driven beam search algorithm similar to the one used in speech recognition search algorithms ().", "labels": [], "entities": [{"text": "speech recognition search", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.7973553339640299}]}, {"text": "The major difference between the search problem in speech recognition and statistical MT is that MT must take into account the different word order for the source and the target language, which does not enter into speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7548699975013733}, {"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9239911437034607}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.984614908695221}, {"text": "speech recognition", "start_pos": 214, "end_pos": 232, "type": "TASK", "confidence": 0.7471283078193665}]}, {"text": "Tillmann, Vogel, Ney, and  proposes a dynamic programming (DP)-based search algorithm for statistical MT that monotonically translates the input sentence from left to right.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.7746064066886902}]}, {"text": "The word order difference is dealt with using a suitable preprocessing step.", "labels": [], "entities": []}, {"text": "Although the resulting search procedure is very fast, the preprocessing is language specific and requires a lot of manual work.", "labels": [], "entities": []}, {"text": "Currently, most search algorithms for statistical MT proposed in the literature are based on the A * concept.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.65679532289505}]}, {"text": "Here, the word reordering can be easily included in the search procedure, since the input sentence positions can be processed in any order.", "labels": [], "entities": []}, {"text": "The work presented in that is based on the A * concept, however, introduces word reordering restrictions in order to reduce the overall search space.", "labels": [], "entities": []}, {"text": "The search procedure presented in this article is based on a DP algorithm to solve the traveling-salesman problem (TSP).", "labels": [], "entities": [{"text": "traveling-salesman problem (TSP)", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.6824827551841736}]}, {"text": "A data-driven beam search approach is presented on the basis of this DP-based algorithm.", "labels": [], "entities": [{"text": "beam search", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7337010502815247}]}, {"text": "The cities in the TSP correspond to source positions of the input sentence.", "labels": [], "entities": []}, {"text": "By imposing constraints on the possible word reorderings similar to that described in, the DP-based approach becomes more effective: when the constraints are applied, the number of word reorderings is greatly reduced.", "labels": [], "entities": []}, {"text": "The original reordering constraint in is shown to be a special case of a more general restriction scheme in which the word reordering constraints are expressed in terms of simple combinatorical restrictions on the processed sets of source sentence positions.", "labels": [], "entities": []}, {"text": "A set of four parameters is given to control the word reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7069915235042572}]}, {"text": "Additionally, a set of four states is introduced to deal with grammatical reordering restrictions (e.g., for the translation direction German to English, the word order difference between the two languages is mainly due to the German verb group.", "labels": [], "entities": []}, {"text": "In combination with the reordering restrictions, a data-driven beam search organization for the search procedure is proposed.", "labels": [], "entities": []}, {"text": "A beam search pruning technique is conceived that jointly processes partial hypotheses according to two criteria: (1) The partial hypotheses cover the same set of source sentence positions, and (2) the partial hypotheses cover sets C of source sentence positions of equal cardinality.", "labels": [], "entities": []}, {"text": "A partial hypothesis is said to cover a set of source sentence positions when exactly the positions in the set have already been processed in the search process.", "labels": [], "entities": []}, {"text": "To verify the effectiveness of the proposed techniques, we report and analyze results for two translation tasks: the German to English Verbmobil task and French to English Canadian Hansards task.", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a short introduction to the translation model used and reports on other approaches to the search problem in statistical MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.6552223563194275}]}, {"text": "In Section 3, a DP-based search approach is presented, along with appropriate pruning techniques that yield an efficient beam search algorithm.", "labels": [], "entities": [{"text": "DP-based search", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.8330772519111633}]}, {"text": "Section 4 reports and analyzes translation results for the different translation directions.", "labels": [], "entities": []}, {"text": "In Section 5, we conclude with a discussion of the achieved results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Translation experiments are carried out for the translation directions German to English and English to German (Verbmobil task) and for the translation directions French to English and English to French (Canadian Hansards task).", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9368997812271118}, {"text": "Canadian Hansards task", "start_pos": 204, "end_pos": 226, "type": "DATASET", "confidence": 0.8476319313049316}]}, {"text": "Section 4.1 reports on the performance measures used.", "labels": [], "entities": []}, {"text": "Section 4.2 shows translation results for the Verbmobil task.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9659738540649414}, {"text": "Verbmobil task", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.47651346027851105}]}, {"text": "Sections 4.2.1 and 4.2.2 describe that task and the preprocessing steps applied.", "labels": [], "entities": []}, {"text": "In Sections 4.2.3 through 4.2.5, the efficiency of the beam search pruning techniques is shown for German-to-English translation, as the most detailed experiments are conducted for that direction.", "labels": [], "entities": [{"text": "beam search pruning", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8727595210075378}, {"text": "German-to-English translation", "start_pos": 99, "end_pos": 128, "type": "TASK", "confidence": 0.5291609615087509}]}, {"text": "Section 4.2.6 gives translation results for the translation direction English to German.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9414710998535156}, {"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9646229147911072}]}, {"text": "In Section 4.3, translation results for the Canadian Hansards task are reported.", "labels": [], "entities": [{"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.96214759349823}, {"text": "Canadian Hansards task", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.8868393500645956}]}, {"text": "To measure the performance of the translation methods, we use three types of automatic and easy-to-use measures of the translation errors.", "labels": [], "entities": [{"text": "translation", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9607619047164917}]}, {"text": "Additionally, a subjective evaluation involving human judges is carried out ).", "labels": [], "entities": []}, {"text": "The following evaluation criteria are employed: \u2022 WER (word error rate): The WER is computed as the minimum number of substitution, insertion, and deletion operations that have to be performed to convert the generated string into the reference target string.", "labels": [], "entities": [{"text": "WER (word error rate)", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.7985363354285558}, {"text": "WER", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9931625127792358}]}, {"text": "This performance criterion is widely used in speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.879431426525116}]}, {"text": "The minimum is computed using a DP algorithm and is typically referred to as editor Levenshtein distance.", "labels": [], "entities": [{"text": "editor Levenshtein distance", "start_pos": 77, "end_pos": 104, "type": "METRIC", "confidence": 0.7895754178365072}]}, {"text": "\u2022 mWER (multireference WER): We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors.", "labels": [], "entities": [{"text": "multireference WER)", "start_pos": 8, "end_pos": 27, "type": "METRIC", "confidence": 0.6953758597373962}]}, {"text": "For example, on the Verbmobil TEST-331 test set, an average of six reference translations per automatic translation are available.", "labels": [], "entities": [{"text": "Verbmobil TEST-331 test set", "start_pos": 20, "end_pos": 47, "type": "DATASET", "confidence": 0.9157177805900574}]}, {"text": "The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.7333467602729797}, {"text": "Levenshtein distance", "start_pos": 127, "end_pos": 147, "type": "METRIC", "confidence": 0.7537951767444611}]}, {"text": "The resulting measure, the mWER, is more robust than the WER, which takes into account only a single reference translation.", "labels": [], "entities": [{"text": "WER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.884612500667572}]}, {"text": "\u2022 PER (position-independent word error rate): In the casein which only a single reference translation per sentence is available, we introduce as an additional measure the position-independent word error rate (PER).", "labels": [], "entities": [{"text": "PER (position-independent word error rate)", "start_pos": 2, "end_pos": 44, "type": "METRIC", "confidence": 0.7371149999754769}, {"text": "position-independent word error rate (PER)", "start_pos": 171, "end_pos": 213, "type": "METRIC", "confidence": 0.782795033284596}]}, {"text": "This measure compares the words in the two sentences without taking the word order into account.", "labels": [], "entities": []}, {"text": "Words in the reference translation that have no counterpart in the translated sentence are counted as substitution errors.", "labels": [], "entities": []}, {"text": "Depending on whether the translated sentence is longer or shorter than the reference translation, the remaining words result in either insertion (if the translated sentence is longer) or deletion (if the translated sentence is shorter) errors.", "labels": [], "entities": [{"text": "insertion", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.969268262386322}]}, {"text": "The PER is guaranteed to be less than or equal to the WER.", "labels": [], "entities": [{"text": "PER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9986047148704529}, {"text": "WER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9775740504264832}]}, {"text": "The PER is more robust than the WER since it ignores translation errors due to different word order in the translated and reference sentences.", "labels": [], "entities": [{"text": "PER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9849072694778442}, {"text": "WER", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.5041277408599854}]}, {"text": "\u2022 SSER (subjective sentence error rate): For a more fine-grained evaluation of the translation results and to check the validity of the automatic evaluation measures subjective judgments by test persons are carried out ).", "labels": [], "entities": [{"text": "SSER (subjective sentence error rate)", "start_pos": 2, "end_pos": 39, "type": "METRIC", "confidence": 0.7215151573930468}]}, {"text": "The following scale for the error count per sentence is used in these subjective evaluations:.", "labels": [], "entities": [{"text": "error count", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9492843449115753}]}, {"text": "In that task, the goal is the translation of spontaneous speech in faceto-face situations for an appointment scheduling domain.", "labels": [], "entities": [{"text": "translation of spontaneous speech", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.8728849589824677}]}, {"text": "We carryout experiments for both translation directions: German to English and English to German.", "labels": [], "entities": []}, {"text": "Although the Verbmobil task is still a limited-domain task, it is rather difficult in terms of vocabulary size, namely, about 5,000 words or more for each of the two languages; second, the syntactic structures of the sentences are rather unrestricted.", "labels": [], "entities": []}, {"text": "Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this article is mainly the (more or less) correct orthographic transcription of the spoken sentences.", "labels": [], "entities": [{"text": "translation of spoken language", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.8818544745445251}]}, {"text": "Thus, the effects of spontaneous speech are present in the corpus; the effect of speech recognition errors, however, is not covered.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6811282783746719}]}, {"text": "The corpus consists of 58,073 training pairs; its characteristics are given in Categorization: We use some categorization, which consists of replacing a single word by a category.", "labels": [], "entities": []}, {"text": "The only words that are replaced by a category label are proper nouns denoting German cities.", "labels": [], "entities": []}, {"text": "Using the new labeled corpus, all probability models are trained anew.", "labels": [], "entities": []}, {"text": "To produce translations in the \"normal\" language, the categories are translated by rule and are inserted into the target sentence.", "labels": [], "entities": []}, {"text": "Word joining: Target language words are joined using a method similar to the one described in Och,.", "labels": [], "entities": [{"text": "Word joining", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7170623689889908}]}, {"text": "Words are joined to handle cases like the German compound noun \"Zahnarzttermin\" for the English \"dentist's appointment,\" because a single word has to be mapped to two or more target words.", "labels": [], "entities": []}, {"text": "The word joining is applied only to the target language words; the source language sentences remain unchanged.", "labels": [], "entities": [{"text": "word joining", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7066278904676437}]}, {"text": "During the search process several joined target language words maybe generated by a single source language word.", "labels": [], "entities": []}, {"text": "Manual lexicon: To account for unseen words in the test sentences and to obtain a greater number of focused translation probabilities p(f | e), we use a bilingual German-English dictionary.", "labels": [], "entities": []}, {"text": "For each word e in the target vocabulary, we create a list of source translations f according to this dictionary.", "labels": [], "entities": []}, {"text": "The translation probability p dic (f | e) for the dictionary entry (f , e) is defined as where Ne is the number of source words listed as translations of the target word e.", "labels": [], "entities": []}, {"text": "The dictionary probability p dic (f | e) is linearly combined with the automatically trained translation probabilities p aut (f | e) to obtain smoothed probabilities p(f | e): For the translation experiments, the value of the interpolation parameter is fixed at \u03bb = 0.5.", "labels": [], "entities": []}, {"text": "for the translation direction English to German are also carried out.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9810836911201477}]}, {"text": "The results, given  The second corpus on which we perform translation experiments is the Hansard corpus.", "labels": [], "entities": [{"text": "Hansard corpus", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.9791551828384399}]}, {"text": "By law, the proceedings of the Canadian parliament are recorded in both French and English.", "labels": [], "entities": []}, {"text": "(For historical reasons, these proceedings are called \"Hansards.\")", "labels": [], "entities": [{"text": "Hansards", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9443839192390442}]}, {"text": "The remarks of the parliament members are written down in whichever of the two languages they use.", "labels": [], "entities": []}, {"text": "They are then translated into the other language to produce complete sets of the proceedings, one in French and the other in English.", "labels": [], "entities": []}, {"text": "The resulting bilingual data have been sentence-aligned using statistical methods).", "labels": [], "entities": []}, {"text": "Originally, about three million sentences were selected.", "labels": [], "entities": []}, {"text": "Here, we use a subset of the original training data; the details regarding this subset  are given in.", "labels": [], "entities": []}, {"text": "The Hansards corpus presents by far a more difficult task than the Verbmobil corpus in terms of vocabulary size and number of training sentences.", "labels": [], "entities": [{"text": "Hansards corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9633070230484009}, {"text": "Verbmobil corpus", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8991793990135193}]}, {"text": "The training and test sentences are less restrictive than for the Verbmobil task.", "labels": [], "entities": [{"text": "Verbmobil task", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.5646495521068573}]}, {"text": "For the translation experiments on the Hansards corpus, no word joining is carried out.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9775697588920593}, {"text": "Hansards corpus", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9903796017169952}, {"text": "word joining", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.7211619913578033}]}, {"text": "Two target words can be produced by a single source word, as described in Section 3.9.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5  Training and test conditions for the German-to-English Verbmobil corpus (*number of words  without punctuation).", "labels": [], "entities": [{"text": "German-to-English Verbmobil corpus", "start_pos": 47, "end_pos": 81, "type": "DATASET", "confidence": 0.6303834219773611}]}, {"text": " Table 6  Computing time, mWER, and SSER for three different reordering constraints on the TEST-147  test set. During the translation experiments, reordered words are not allowed to cross  punctuation marks.", "labels": [], "entities": [{"text": "SSER", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.998083233833313}, {"text": "TEST-147  test set", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.9041756590207418}]}, {"text": " Table 8  Effect of the coverage pruning threshold tC on the number of search errors and mWER on the  TEST-331 test set (no cardinality pruning carried out: tc = \u221e). A cardinality histogram pruning  of 200,000 is applied to restrict the maximum overall size of the search space. The negative  logarithm of tC is reported.", "labels": [], "entities": [{"text": "mWER", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9682885408401489}, {"text": "TEST-331 test set", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.889166514078776}]}, {"text": " Table 9  Effect of the cardinality pruning threshold tc on the number of search errors and mWER on  the TEST-331 test set (no coverage pruning is carried out: tC = \u221e). A coverage histogram  pruning of 1,000 is applied to restrict the overall size of the search space. The negative  logarithm of tc is shown.", "labels": [], "entities": [{"text": "mWER", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9742937684059143}, {"text": "TEST-331 test set", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.881802479426066}]}, {"text": " Table 10  Effect of observation pruning on the number of search errors and mWER on the TEST-331 test  set (parameter setting: tc = \u221e, tC = 10.0 ). No histogram pruning is applied. The results are  reported for the GE constraint.", "labels": [], "entities": [{"text": "mWER", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9659518599510193}, {"text": "TEST-331 test  set", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.8791596492131551}, {"text": "GE", "start_pos": 215, "end_pos": 217, "type": "METRIC", "confidence": 0.4889559745788574}]}, {"text": " Table 12  Translation results for the translation direction English to German on the TEST-331 test set.  The results are given in terms of computing time, WER, and PER for three different reordering  constraints: MON, EG, and S3.", "labels": [], "entities": [{"text": "translation direction English to German", "start_pos": 39, "end_pos": 78, "type": "TASK", "confidence": 0.9039648413658142}, {"text": "TEST-331 test set", "start_pos": 86, "end_pos": 103, "type": "DATASET", "confidence": 0.9594864845275879}, {"text": "WER", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9982485771179199}, {"text": "PER", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.9981620907783508}, {"text": "MON", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.9086257219314575}, {"text": "EG", "start_pos": 219, "end_pos": 221, "type": "METRIC", "confidence": 0.8563188314437866}]}, {"text": " Table 12. For the English-to-German translation  direction, a single reference translation for each test sentence is used to carry out  the automatic evaluation. The translation task for the translation direction English  to German is more difficult than for the translation direction German to English; the  trigram language model perplexity increases from 38.3 to 68.2 on the TEST-331 test set,  as can be seen in", "labels": [], "entities": [{"text": "English-to-German translation  direction", "start_pos": 19, "end_pos": 59, "type": "TASK", "confidence": 0.6496342420578003}, {"text": "TEST-331 test set", "start_pos": 379, "end_pos": 396, "type": "DATASET", "confidence": 0.9513487219810486}]}, {"text": " Table 14  Training and test conditions for the Hansards task (*number of words without punctuation).", "labels": [], "entities": [{"text": "Hansards task", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.5743557214736938}]}, {"text": " Table 14. The Hansards corpus presents by far a more difficult task than  the Verbmobil corpus in terms of vocabulary size and number of training sentences.  The training and test sentences are less restrictive than for the Verbmobil task. For the  translation experiments on the Hansards corpus, no word joining is carried out. Two  target words can be produced by a single source word, as described in Section 3.9.2.", "labels": [], "entities": [{"text": "Hansards corpus", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.8175368905067444}, {"text": "Verbmobil corpus", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.8655462265014648}, {"text": "Hansards corpus", "start_pos": 281, "end_pos": 296, "type": "DATASET", "confidence": 0.7760482132434845}, {"text": "word joining", "start_pos": 301, "end_pos": 313, "type": "TASK", "confidence": 0.7494691610336304}]}, {"text": " Table 15  Computing time, WER, and PER for the translation direction French to English using the two  reordering constraints MON and S3. An almost \"full\" search is carried out.", "labels": [], "entities": [{"text": "WER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9990142583847046}, {"text": "PER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9991251826286316}, {"text": "translation direction French to English", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.881459903717041}, {"text": "MON", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.7431942224502563}]}, {"text": " Table 16  Computing time, WER, and PER for the translation direction English to French using the two  reordering constraints MON and S3. An almost \"full\" search is carried out.", "labels": [], "entities": [{"text": "WER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9988736510276794}, {"text": "PER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9990987777709961}, {"text": "translation direction English to French", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.8619958519935608}, {"text": "MON", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.7638396620750427}]}]}