{"title": [{"text": "Lexical Processing in the CLARE System", "labels": [], "entities": [{"text": "Lexical Processing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8672427535057068}]}], "abstractContent": [], "introductionContent": [{"text": "In many language processing systems, uncertainty in the boundaries of linguistic units means that data are represented not as a well-defined sequence of units but as a lattice of possibilities.", "labels": [], "entities": []}, {"text": "This is often the casein speech recognition, syntactic parsing and Japanese kana-kanji conversion.", "labels": [], "entities": [{"text": "casein speech recognition", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6161638001600901}, {"text": "syntactic parsing", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7488803863525391}, {"text": "Japanese kana-kanji conversion", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.5743820170561472}]}, {"text": "In contrast, however, it is often assumed that, for languages written with interword spaces, it is sufficient to prepare an input character stream for parsing by grouping it deterministically into a sequence of words, punctuation symbols and perhaps other items.", "labels": [], "entities": []}, {"text": "But for typed input, spaces do not necessarily correspond to boundaries between lexical items, because of errors and other, linguistic, phenomena.", "labels": [], "entities": []}, {"text": "This means that a lattice representation, not a simple sequence, should be used throughout front end (pre-parsing) analysis.", "labels": [], "entities": []}, {"text": "The CLARE system underdevelopment at SRI Cambridge uses such a representation, allowing it to deal straightforwardly with combinations or multiple occurrences of phenomena that would be difficult or impossible to process correctly under a sequence representation.", "labels": [], "entities": [{"text": "SRI Cambridge", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.7864183187484741}]}, {"text": "This paper concentrates on CLARE's ability to deal with typing and spelling errors, which are especially common in interactive use, for which CLARE is designed.", "labels": [], "entities": []}, {"text": "The word identity and word boundary ambiguities encountered in the interpretation of errorful input often require the application of syntactic and semantic knowledge on a phrasal or even sentential scale.", "labels": [], "entities": []}, {"text": "Such knowledge maybe applied as soon as the problem is encountered; however, this brings major problems with it, such as the need for adequate lookahead, and the difficulties of engineering large systems where the processing levels are tightly coupled.", "labels": [], "entities": []}, {"text": "To avoid such problems, CLARE adopts a staged architecture, in which indeterminacy is preserved until the knowledge needed to resolve it is ready to be applied.", "labels": [], "entities": [{"text": "CLARE", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8389102220535278}]}, {"text": "An appropriate representation is of course the key to doing this efficiently.", "labels": [], "entities": []}, {"text": "*CLARE is being developed as part of a collaborative project involving SRI International, British Aerospace, BP Research, British Telecom, Cambridge University, the UK Defence Research Agency, and the UK Department of Trade and Industry.", "labels": [], "entities": [{"text": "SRI International", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.8047890067100525}, {"text": "BP Research", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.8513055145740509}, {"text": "UK Defence Research Agency", "start_pos": 165, "end_pos": 191, "type": "DATASET", "confidence": 0.7867191433906555}]}], "datasetContent": [{"text": "To assess the usefulness of syntactico-semantic constraints in CLARE's spelling correction, the following experiment was carried out.", "labels": [], "entities": [{"text": "CLARE's spelling correction", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6229061409831047}]}, {"text": "Five hundred sentences falling within CLARE's current lexical and grammatical coverage were taken at random from the LOB corpus.", "labels": [], "entities": [{"text": "LOB corpus", "start_pos": 117, "end_pos": 127, "type": "DATASET", "confidence": 0.9207004606723785}]}, {"text": "Although CLARE's core lexicon is fairly small (1600 root forms), it consists of the more frequent words in the language, which tend to be fairly short and therefore have many candidate corrections if misspelled.", "labels": [], "entities": []}, {"text": "The sentences were passed, character by character, through a channel which transmitted a character without alteration with probability 0.99, and with probability 0.01 introduced one of the four kinds of simple error.", "labels": [], "entities": []}, {"text": "This process produced a total of 102 sentences that differed from their originals.", "labels": [], "entities": []}, {"text": "The average length was 6.46 words, and there were 123 corrupted tokens in all.", "labels": [], "entities": []}, {"text": "The corrupted sentence set was then processed by CLARE with only the spelling correction recovery method in force and with no user intervention.", "labels": [], "entities": [{"text": "CLARE", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.899402379989624}, {"text": "spelling correction recovery", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.792380024989446}]}, {"text": "Up to two simple errors were considered per token.", "labels": [], "entities": []}, {"text": "No domainspecific or context-dependent knowledge was used.", "labels": [], "entities": []}, {"text": "Of the 123 corrupted tokens, ten were corrupted into other known words, and so no correction was attempted.", "labels": [], "entities": []}, {"text": "Parsing failed in nine of these cases; in the tenth, the corrupted word made as much sense as the original out of discourse context.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9566353559494019}]}, {"text": "In three further cases, the original token was not among the corrections suggested.", "labels": [], "entities": []}, {"text": "The corrections for two other tokens were not used because a corruption into a known word elsewhere in the same sentence caused parsing to fail.", "labels": [], "entities": []}, {"text": "Only one correction (the right one) was suggested for 59 of the remaining 108 tokens.", "labels": [], "entities": []}, {"text": "Multiple-token correction, involving the manipulation of space characters, took place in 24 of these cases.", "labels": [], "entities": [{"text": "Multiple-token correction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7619116902351379}]}, {"text": "This left 49 tokens for which more than one correction was suggested, requiring syntactic and semantic processing for further disambiguation.", "labels": [], "entities": []}, {"text": "The average number of corrections suggested for these 49 was 4.57.", "labels": [], "entities": [{"text": "corrections", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.887339174747467}]}, {"text": "However, only an average of 1.69 candidates (including, because of the way the corpus was selected, all the right ones) appeared in QLFs satisfying selectional restrictions; thus over 80% of the wrong candidates were rejected.", "labels": [], "entities": [{"text": "QLFs", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.7861894369125366}]}, {"text": "Treating all candidates as equally likely in the absence of frequency information, syntactic and semantic processing reduced the average entropy from 1.92 to 0.54, removing 72% of the uncertainty.", "labels": [], "entities": []}, {"text": "Comparisons of parsing times showed that a lattice could be parsed many times faster than separate alternative strings when the problem token is towards the end of the sentence and/or has several syntactically plausible candidate corrections.", "labels": [], "entities": []}, {"text": "The corpus on which the experiment was carried out consisted only of sentences of which CLARE could parse the uncorrupted versions.", "labels": [], "entities": []}, {"text": "However, the figures presented here give grounds to believe that false positives -a wrong \"correction\" causing a spurious parse of an unparsable original -should be rare.", "labels": [], "entities": []}, {"text": "If the replacement of one word by another only rarely maps one sentence inside coverage to another, then a corresponding replacement on a sentence outside coverage should yield something within coverage even more rarely.", "labels": [], "entities": []}], "tableCaptions": []}