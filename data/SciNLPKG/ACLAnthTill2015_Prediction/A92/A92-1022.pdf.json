{"title": [{"text": "Evaluating Parsing Strategies Using Standardized Parse Files", "labels": [], "entities": []}], "abstractContent": [{"text": "The availability of large files of manually-reviewed parse trees from the University of Pennsylvania \"tree bank\", along with a program for comparing system-generated parses against these \"standard\" parses, provides anew opportunity for evaluating different parsing strategies.", "labels": [], "entities": []}, {"text": "We discuss some of the restructuring required to the output of our parser so that it could be meaningfully compared with these standard parses.", "labels": [], "entities": []}, {"text": "We then describe several heuristics for improving parsing accuracy and coverage, such as closest attachment of mod-ifiers, statistical grammars, and fitted parses, and present a quantitative evaluation of the improvements obtained with each strategy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9747726917266846}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9007202982902527}, {"text": "coverage", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9462352991104126}]}, {"text": "1 The Problem The systematic improvement of parsing strategies requires someway of evaluating competing strategies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9786106944084167}]}, {"text": "We need to understand their effect on both overall system performance and the ability to parse sentences correctly.", "labels": [], "entities": []}, {"text": "In order to evaluate parser output, we will need a file of standard \"correct\" parses and a mechanism for comparing parser output with this standard.", "labels": [], "entities": []}, {"text": "Preparing such a file is a quite time-consuming operation.", "labels": [], "entities": []}, {"text": "In addition, if we would like our comparison of strategies to be meaningful to other researchers and extendible to other systems , we would like a standard and a metric which are, as much as possible, system independent.", "labels": [], "entities": []}, {"text": "One resource which is newly available to meet this need is the \"tree bank\" at the University of Pennsyl-vania.", "labels": [], "entities": [{"text": "Pennsyl-vania", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.8933829069137573}]}, {"text": "This bank includes a large number of parse trees which have been prepared manually following a fairly detailed standard.", "labels": [], "entities": []}, {"text": "At first glance the comparison of parse trees among systems based on different theories would seem to be very difficult.", "labels": [], "entities": []}, {"text": "However, based on an idea proposed by Ezra Black [Black et al., 1991], a group organized by Phil Harrison has developed a metric and a program for performing just such a comparison.", "labels": [], "entities": []}, {"text": "A preliminary assessment of this metric, based on a small \" sample of short sentences (50 13-word sentences), has been quite promising [Harrison et al., 1991].", "labels": [], "entities": []}, {"text": "Our goal, in the work described here, was to extend this effort to larger samples and more complex sentences and to use the metric to compare various parsing heuris-tics which we have employed in our system.", "labels": [], "entities": []}, {"text": "We describe briefly in the next sections the nature of the task and the text to which is was applied, the form of the tree bank, and the comparison metric.", "labels": [], "entities": []}, {"text": "We consider some of the systematic mis-alignments between the standard and our output, and describe how we have reduced these.", "labels": [], "entities": []}, {"text": "Finally , we describe some of our parsing strategies and see how they affect both the parse scores and the overall system performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9684616923332214}]}, {"text": "2 The Application Task The experiments described below were performed on aversion of the PROTEUS system as prepared for MUC-3, the third Message Understanding Conference [Sundheim, 1991].", "labels": [], "entities": [{"text": "MUC-3, the third Message Understanding Conference [Sundheim, 1991]", "start_pos": 120, "end_pos": 186, "type": "TASK", "confidence": 0.5946539292732874}]}, {"text": "The task set before the participants at these Conferences is one of information eztraction: taking free-text input on a particular subject matter, extracting specified types of information, and filling a database with this information.", "labels": [], "entities": []}, {"text": "For MUC-3, the texts consisted of short news reports about terrorist incidents; most are in typical newspaper style, with complex sentences, although transcripts of speeches and radio broadcasts are also included.", "labels": [], "entities": [{"text": "MUC-3", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.68218594789505}]}, {"text": "The PROTEUS system has five basic stages of processing: syntactic analysis, semantic analysis, reference resolution , discourse analysis, and database creation.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7329629361629486}, {"text": "semantic analysis", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7305349409580231}, {"text": "reference resolution", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.8154866993427277}, {"text": "discourse analysis", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7423073053359985}, {"text": "database creation", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.7524248361587524}]}, {"text": "Syntactic analysis consists of parsing with a broad-coverage English grammar and dictionary, followed by syntactic regularization.", "labels": [], "entities": [{"text": "Syntactic analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8995286226272583}]}, {"text": "The regularized parse is then given to the semantic analyzer, which produces a predicate-argument structure.", "labels": [], "entities": []}, {"text": "The semantic analyzer also provides feedback to the parser as to whether the parse satisfies semantic (selectional) constraints.", "labels": [], "entities": []}, {"text": "Thus correct database creation is dependent on correct predicate-argument structures, which are in turn dependent on at least locally correct syntactic structures.", "labels": [], "entities": [{"text": "correct database creation", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.5906895200411478}]}, {"text": "Because of the richness of the text, it is not possible to provide a complete set of semantic patterns.", "labels": [], "entities": []}, {"text": "The semantic patterns included in the system are largely limited to those relevant for filling database entries.", "labels": [], "entities": []}, {"text": "For the terrorist domain, this includes patterns for all sorts of terrorist 156", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The basic idea for the evaluation metric was developed by Ezra Black.", "labels": [], "entities": []}, {"text": "It was then refined and tested at a workshop (and through extensive subsequent electronic communication) organized by Phil Harrison.", "labels": [], "entities": []}, {"text": "This effort has involved computational linguists from eight sites who have applied this metric to compare --for a set of 50 short sentences --the Penn standard with the \"ideal\" parses their systems would generate for these sentences.", "labels": [], "entities": [{"text": "Penn standard", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.9165488183498383}]}, {"text": "At first glance the parses produced by different systems may seem so different as to be incomparable; node labels in particular maybe entirely different.", "labels": [], "entities": []}, {"text": "The metric therefore eliminates node labels and only compares tree structures.", "labels": [], "entities": []}, {"text": "Certain constituents are treated very differently by different systems; for example, auxiliaries are treated as main verbs by some systems but not others.", "labels": [], "entities": []}, {"text": "Several classes of constituents, such as auxiliaries, pre-infinitival \"to\", \"not\", null elements, punctuation, etc., are therefore deleted before trees are compared.", "labels": [], "entities": []}, {"text": "After these eliminations, brackets with no elements inside, brackets around single words, and multiple brackets surrounding the same sequence of words are removed.", "labels": [], "entities": []}, {"text": "Finally, these two bracketed sequences are compared.", "labels": [], "entities": []}, {"text": "We count the number of brackets in the standard output (Std), the total number in the system output (Sys), and the number of matching brackets (M).", "labels": [], "entities": []}, {"text": "We then define --just as we did for system performance --measures of recall (= i / Std) and precision (= M / Sys).", "labels": [], "entities": [{"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9995162487030029}, {"text": "precision (= M / Sys)", "start_pos": 92, "end_pos": 113, "type": "METRIC", "confidence": 0.7973077495892843}]}, {"text": "We also count the number of \"crossings\": the number of cases where a bracketed sequence from the standard overlaps a bracketed sequence from the system output, but neither sequence is properly contained in the other.", "labels": [], "entities": []}, {"text": "Most automatic parsing systems generate considerably more detailed bracketing than that produced by the Treebank; in such cases precision would be reduced, but the recall and crossing count would still accurately reflect the correctness of the system-generated parses.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.8056297898292542}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.999320387840271}, {"text": "recall and crossing count", "start_pos": 164, "end_pos": 189, "type": "METRIC", "confidence": 0.7616404592990875}]}, {"text": "Phil Harrison and Steven Abney have developed and distributed a program which computes these various measures.", "labels": [], "entities": []}, {"text": "For an evaluation of our different parsing heuristics, we took the first 33 of the 356 news reports from the MUC-3 corpus which had been bracketed by the University of Pennsylvania.", "labels": [], "entities": [{"text": "MUC-3 corpus", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.9512902796268463}]}, {"text": "Three of these were deleted because of differences in the way the sentence boundaries were assigned by our system and UPenn.", "labels": [], "entities": [{"text": "UPenn", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.9388565421104431}]}, {"text": "This left 30 reports with 317 sentences for the evaluation runs.", "labels": [], "entities": []}, {"text": "Our syntactic analyzer uses an augmented-contextfree grammar: a context-free core plus a set of constraints.", "labels": [], "entities": []}, {"text": "Some of these are stated as absolute constraints: if the constraint is violated, the analysis is rejected.", "labels": [], "entities": []}, {"text": "Others are stated as preferences: associated with each of these constraints is a penalty to be assessed if the constraint is violated.", "labels": [], "entities": []}, {"text": "Penalties from different constraints are combined additively, and the analysis with the least penalty is selected as the final parse.", "labels": [], "entities": []}, {"text": "The parser uses a best-first search in which, at each point, the hypothesis with the lowest penalty is pursued.", "labels": [], "entities": []}, {"text": "Among the constraints which are realized as preferences in the system are: some grammatical constraints (including adverb position, count noun, and comma constraints), closest attachment of modifiers, statistical preference for productions, and all semantic constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing evaluation over entire corpus, showing number of sentences obtaining one or more parses, average  number of crossings per sentence, and average recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.999370276927948}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9916061162948608}]}, {"text": " Table 2: Parsing evaluation over 206 sentences, showing average  and precision.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9428443908691406}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9996241331100464}]}, {"text": " Table 3: System performance in template filling task.", "labels": [], "entities": [{"text": "template filling task", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8777819077173868}]}]}