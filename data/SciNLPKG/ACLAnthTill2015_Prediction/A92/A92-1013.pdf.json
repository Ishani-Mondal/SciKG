{"title": [{"text": "Computational Lexicons: the Neat Examples and the Odd Exemplars", "labels": [], "entities": []}], "abstractContent": [{"text": "When implementing computational lexicons it is important to keep in mind the texts that a NLP system must deal with.", "labels": [], "entities": []}, {"text": "Words relate to each other in many different, often queer, ways: this information is rarely found in dictionaries, and it is quite hard to be invented a priori, despite the imagination that linguists exhibit at inventing esoteric examples.", "labels": [], "entities": []}, {"text": "In this paper we present the results of an experiment in learning from corpora the frequent selectional restrictions holding between content words.", "labels": [], "entities": []}, {"text": "The method is based on the analysis of word associations augmented with syntactic markers and semantic tags.", "labels": [], "entities": []}, {"text": "Word pairs are extracted by a morphosyntactic analyzer and clustered according to their semantic tags.", "labels": [], "entities": []}, {"text": "A statistical measure is applied to the data to evaluate the significance of a detected relation.", "labels": [], "entities": []}, {"text": "Clustered association data render the study of word associations more interesting with several respects: data are more reliable even for smaller corpora, more easy to interpret, and have many practical applications in NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the fundamental property of computational lexicons is an account of the relations between verbs and its arguments.", "labels": [], "entities": []}, {"text": "Arguments are identified by their position in a predicate-argument structure, or by conceptual relations names (e.g. agent, purpose, location, etc).", "labels": [], "entities": []}, {"text": "Arguments are annotated with selectional restrictions, that impose type constraints on the set of content words that may fill a relation.", "labels": [], "entities": []}, {"text": "Selectional restrictions often do not provide all the semantic information that is necessary in NLP systems, however they are at the basis of the majority of computational approaches to syntactic and semantic disambiguation.", "labels": [], "entities": [{"text": "syntactic and semantic disambiguation", "start_pos": 186, "end_pos": 223, "type": "TASK", "confidence": 0.6305185705423355}]}, {"text": "It has been noticed that representing only the semantics of verbs maybe inadequate.", "labels": [], "entities": []}, {"text": "The notion of spreading the semantic load supports the idea that every content word should be represented in the lexicon as the union of all the situations in which it could potentially participate.", "labels": [], "entities": []}, {"text": "Unfortunately, handwriting selectional restrictions is not an easy matter, because it is time consuming and it is hard to keep consistency among the data when the lexicon has several hundred or thousand words.", "labels": [], "entities": [{"text": "handwriting selectional", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8054966926574707}, {"text": "consistency", "start_pos": 127, "end_pos": 138, "type": "METRIC", "confidence": 0.9719916582107544}]}, {"text": "However the major difficulty is that words relate to each other in many different, often domain dependent ways.", "labels": [], "entities": []}, {"text": "The nowadays vast literature on computational lexicons is filled with neat examples of the eat(animate,food) flavour, but in practice in many language domains selectional constraints between words are quite odd.", "labels": [], "entities": []}, {"text": "It is not just a matter of violating the semantic expectations, such as in \"kill the process\" or \"my car drinks gasoline\", neither it is that kind of fancifulness that linguists exhibit at finding queer sentences.", "labels": [], "entities": []}, {"text": "Rather, there exist statistically relevant linguistic relations that are hard to imagine a-priori, almost never found in dictionaries, and even harder to assign to the appropriate slot in the whatever conceptual structure adopted for lexical representation.", "labels": [], "entities": []}, {"text": "Several examples of such relations are shown throughout this paper.", "labels": [], "entities": []}, {"text": "Ideally, knowledge on word relations should be acquired directly from massive amounts of texts~ rather than from hand-crafted rules.", "labels": [], "entities": []}, {"text": "the basis of many recent studies on word associations.", "labels": [], "entities": [{"text": "word associations", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7688811719417572}]}, {"text": "The results of these studies have important applications in lexicography, to detect lexicosyntactic regularities), such as, for example~ support verbs (e.g. \"make-decision\") prepositional verbs (e.g. \"rely-upon\") idioms, semantic relations (e.g. \"part_of\") and fixed expressions (e.g. \"kick the bucket\").", "labels": [], "entities": []}, {"text": "In) cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification.", "labels": [], "entities": [{"text": "word classification", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7684783637523651}]}, {"text": "All these studies are based on th~ (strong) assumption that syntactic similarity in wor(~ patterns implies semantic similarity.", "labels": [], "entities": []}, {"text": "In (Guthrie el al., 1991), sets of consistently contiguous word~, (\"neighbourhood\") are extracted from machinereadable dictionaries, to help semantic disambiguation in information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.7332017421722412}]}, {"text": "In) statistically collectec associations provide pragmatic cues for lexical choic( in sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7534222900867462}]}, {"text": "For example, we can learr that \"make decision\" is a better choice than, say \"have decision\" or \"take decision\".", "labels": [], "entities": []}, {"text": "( proposes that a syntactic disambiguation criterion can be gathered by comparing the probability of occurrence of nounpreposition and verb-preposition pairs in V NP PP structures.", "labels": [], "entities": []}, {"text": "In general word associations are collected by extracting word pairs in a +-5 window.", "labels": [], "entities": []}, {"text": "In,) the significance of an association (x,y) is measured by the mutual information I(x,y), i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.", "labels": [], "entities": [{"text": "mutual information I", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.5460007389386495}]}, {"text": "In,, the associations are filtered by selecting the word pairs (x,y) whose frequency of occurrence is above f+ks, where f is the average appearance, sis the standard deviation, and k is an empirically determined factor. and use syntactic markers to increase the significance of the data.", "labels": [], "entities": []}, {"text": "uses the subject classification given in machine-readable dictionaries (e.g. economics, engineering, etc.) to reinforce cooccurence links.", "labels": [], "entities": []}, {"text": "Despite the use of these methods to add evidence to the data, the major problem with word-pairs collections is that reliable results are obtained only fora small subset of high-frequency words on very large corpora, otherwise the association ratio becomes unstable.", "labels": [], "entities": []}, {"text": "For example, Church run his experiment on a corpus with over 20-30 millions words, and Hindle reports 6 millions words as not being an adequate corpus.", "labels": [], "entities": []}, {"text": "In many practical NLP/IR applications corpora are not so large, and typically span from 500,000 to a few million words.", "labels": [], "entities": []}, {"text": "The analysis of associations could be done on wider domains, but apart for very general words, it is much more desirable to collect data from the application corpus.", "labels": [], "entities": []}, {"text": "Information collected from other sources could add noise rather than strengthening the data, because inmost applications jargon, technical words, and domain-dependent associations are the norm.", "labels": [], "entities": []}, {"text": "In) it is shown a table of operational pairs like adjective-noun and verb-object, from which clearly emerges the very different nature of the two source domains (Unix Usenet and Jerusalem Post).", "labels": [], "entities": [{"text": "Jerusalem Post", "start_pos": 178, "end_pos": 192, "type": "DATASET", "confidence": 0.8999380767345428}]}, {"text": "For example, the noun-noun pairs with \"tree\" include associations such as \"parse, grammar, decision\" and \"olive, Christmas\".", "labels": [], "entities": []}, {"text": "If the NLP/IR application is about the computer world, associations such as \"olive tree\" or \"Christmas tree\" are (at best) useless.", "labels": [], "entities": []}, {"text": "A second problem with statistically collected word pairs is that an analysis based simply on surface distribution may produce data at a level of granularity too fine.", "labels": [], "entities": []}, {"text": "For example, a purely distributional analysis for word classification, such as those cited above, might place two verbs into distinct classes because one is used primarily with an object olive and the other with the object grape.", "labels": [], "entities": [{"text": "word classification", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7480288147926331}]}, {"text": "This may not be appropriate given the application.", "labels": [], "entities": []}, {"text": "Abstraction via semantic classes (e.g. VEGETABLE), would ensure that the ontology found is appropriate for the domain.", "labels": [], "entities": []}, {"text": "The model of prepositional attachment preference proposed by Hindle is also too weak if applied only to verb-preposition and nounpreposition pairs.", "labels": [], "entities": [{"text": "prepositional attachment preference", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.7169174055258433}]}, {"text": "A preposition mayor may not be related to a verb, even if it frequently cooccurs with it, depending upon the underlying semantic relation.", "labels": [], "entities": []}, {"text": "It is the semantic category of the noun following a preposition that determines the nature of the semantic link (e.g. for+ HUMAN ENTITY = beneficiary, for+ACTION = purpose), and ultimately influences the choice of the proper attachment.", "labels": [], "entities": [{"text": "HUMAN ENTITY", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.8448224365711212}]}, {"text": "Semantic abstraction also renders the data more readable.", "labels": [], "entities": []}, {"text": "Millions of simple word cooccurrences let the experimenter sink in an ocean of data, without providing much insight of the conceptual nature of the detected associations.", "labels": [], "entities": []}, {"text": "In this paper, we present a study on word associations augmented with syntactic markers and semantic tagging.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7333975732326508}]}, {"text": "We call these data clustered associations.", "labels": [], "entities": []}, {"text": "First, statistically meaningful data can be gathered from (relatively) small corpora; -Second, data are presented in a compact form and are much more readable; -Third, clustered association data are useful for many interesting NLP applications, such as conceptual clustering, syntactic and semantic disambiguation, and semi-automatic learning of the relevant selectional restrictions in a given language domain.", "labels": [], "entities": [{"text": "conceptual clustering", "start_pos": 253, "end_pos": 274, "type": "TASK", "confidence": 0.7075447589159012}, {"text": "syntactic and semantic disambiguation", "start_pos": 276, "end_pos": 313, "type": "TASK", "confidence": 0.619498573243618}]}, {"text": "In this paper we discuss the results of an experiment in learning selectional restrictions, to provide support for the design of computational lexicons.", "labels": [], "entities": []}, {"text": "Other results are presented in The method is applied to a corpus of economic enterprise descriptions, registered at the Chambers of Commerce in Italy.", "labels": [], "entities": []}, {"text": "The database of these descriptions (in total over 1,000,000 descriptions, each spanning from 1 to 100-200 words) is managed in Italy by the Company CERVED.", "labels": [], "entities": []}, {"text": "Sentences describe one or several commercial enterprises carried out by a given Company.", "labels": [], "entities": []}, {"text": "Examples of these descriptions are provided throughout the text.", "labels": [], "entities": []}, {"text": "In our experiment, we used only 25,000 descriptions, including about 500,000 words.", "labels": [], "entities": []}, {"text": "A second experiment on a legal corpus is under preparation and will be ready shortly.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}