{"title": [{"text": "A Practical Methodology for the Evaluation of Spoken Language Systems", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature, practical applications rather than \"toy\" implementations.", "labels": [], "entities": []}, {"text": "Evaluation is also crucial to assessing competing claims and identifying promising technical approaches.", "labels": [], "entities": []}, {"text": "While work in speech recognition (SR) has a history of evaluation methodologies that permit comparison among various systems, until recently no methodology existed for either developers of natural language (NL) interfaces or researchers in speech understanding (SU) to evaluate and compare the systems they developed.", "labels": [], "entities": [{"text": "speech recognition (SR)", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.8899659037590026}, {"text": "speech understanding (SU)", "start_pos": 240, "end_pos": 265, "type": "TASK", "confidence": 0.8435777187347412}]}, {"text": "Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology has been put into practice several times in comparative tests of several SLS systems.", "labels": [], "entities": [{"text": "DARPA Spoken Language Systems (SLS)", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.5417876584189278}]}, {"text": "These evaluations are probably the only NL evaluations other than the series of Message Understanding Conferences to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems.", "labels": [], "entities": [{"text": "Message Understanding Conferences", "start_pos": 80, "end_pos": 113, "type": "TASK", "confidence": 0.8067694306373596}]}, {"text": "This paper describes a practical \"black-box\" methodology for automatic evaluation of question-answering NL systems.", "labels": [], "entities": []}, {"text": "While each new application domain will require some development of special resources, the heart of the methodology is domain-independent, and it can be used with either speech or text input.", "labels": [], "entities": []}, {"text": "The particular characteristics of the approach are described in the following section: subsequent sections present its implementation in the DARPA SLS community, and some problems and directions for future development.", "labels": [], "entities": [{"text": "DARPA SLS community", "start_pos": 141, "end_pos": 160, "type": "DATASET", "confidence": 0.709178606669108}]}], "datasetContent": [{"text": "We assume an evaluation architecture like that in.", "labels": [], "entities": []}, {"text": "The shaded components are common resources of the evaluation, and are not part of the system(s) being evaluated.", "labels": [], "entities": []}, {"text": "Specifically, it is assumed there is a common database which all systems use in producing answers, which defines both the data tuples (rows in tables) and the data types for elements of these tuples (string, integer, etc.).", "labels": [], "entities": []}, {"text": "Queries relevant to the database are collected under conditions as realistic as possible (see 2.4).", "labels": [], "entities": []}, {"text": "Answers to the corpus of queries must be provided, expressed in a common standard format (Common Answer Specification, or CAS): one such format is exemplified in Appendix A. Some portion of these pairs of queries and answers is then set aside as a test corpus, and the remainder is provided as training material.", "labels": [], "entities": []}, {"text": "In practice, it has also proved useful to include in the training data the database query expression (for example, an SQL expression) which was used to produce the reference answer: this often makes it possible for system developers to understand what was expected fora query, even if the answer is empty or otherwise limited in content.", "labels": [], "entities": []}, {"text": "The goal of the DARPA Spoken Language Systems program is to further research and demonstrate the potential utility of speech understanding.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.7244799733161926}]}, {"text": "Currently, at least five major sites (AT&T, BBN, CMU, MIT, and SRI) are developing complete SLS systems, and another site (Paramax) is integrating its NL component with other speech systems.", "labels": [], "entities": [{"text": "BBN", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9478906989097595}, {"text": "SLS", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9601262807846069}]}, {"text": "Representatives from these and other organizations meet regularly to discuss program goals and to evaluate progress.", "labels": [], "entities": []}, {"text": "This DARPA SLS community formed a committee on evaluation 4, chaired by David Pallett of the National Institute of Standards and Technology (NIST).", "labels": [], "entities": [{"text": "DARPA SLS", "start_pos": 5, "end_pos": 14, "type": "TASK", "confidence": 0.47721317410469055}]}, {"text": "The committee was to develop a methodology for data collection, training data dissemination, and testing for SLS systems underdevelopment.", "labels": [], "entities": [{"text": "data collection", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7366551458835602}, {"text": "training data dissemination", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6719772617022196}, {"text": "SLS", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9839922785758972}]}, {"text": "The first community-wide evaluation using the first version of this methodology took place in June, 1990, with subsequent evaluations in February 1991 and February 1992.", "labels": [], "entities": []}, {"text": "The emphasis of the committee's work has been on automatic evaluation of queries to an air travel information system (ATIS).", "labels": [], "entities": [{"text": "air travel information system (ATIS)", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.43686596410615103}]}, {"text": "Air travel was chosen as an application that is easy for everyone to understand.", "labels": [], "entities": []}, {"text": "The methodology presented here was originally developed in the context of the need for SLS evaluation, and has been extended in important ways by the community based on the practical experience of doing evaluations.", "labels": [], "entities": [{"text": "SLS evaluation", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.9683662950992584}]}, {"text": "As a result of the ATIS evaluations, a body of resources has now been compiled and is available through NIST.", "labels": [], "entities": [{"text": "ATIS evaluations", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.7981727719306946}, {"text": "NIST", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9818688035011292}]}, {"text": "This includes the ATIS relational database, a corpus of paired queries and answers, protocols for data collection, software for automatic comparison of answers, the \"Principles of Interpretation\" specifying domain-specific meanings of queries, and the CAS format (Appendix A is the current version).", "labels": [], "entities": [{"text": "ATIS relational database", "start_pos": 18, "end_pos": 42, "type": "DATASET", "confidence": 0.727479894955953}, {"text": "data collection", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.7790760695934296}]}, {"text": "Interested parties should contact David Pallet of NIST for more information.", "labels": [], "entities": [{"text": "David Pallet of NIST", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8017950654029846}]}, {"text": "5  This approach to evaluation shares many characteristics with the methods used for the DARPA-sponsored Message Understanding Conferences.", "labels": [], "entities": [{"text": "DARPA-sponsored Message Understanding Conferences", "start_pos": 89, "end_pos": 138, "type": "TASK", "confidence": 0.6087118834257126}]}, {"text": "In particular, both approaches are focused on external (black-box) evaluation of the understanding capabilities of systems using input/output pairs, and there are many similar problems in precisely specifying how NL systems are to satisfy the application task.", "labels": [], "entities": []}, {"text": "Despite these similarities, this methodology probably comes closer to evaluating the actual understanding capabilities of NL systems.", "labels": [], "entities": []}, {"text": "One reason is that the constraints on both input and output are more rigorous.", "labels": [], "entities": []}, {"text": "For database query tasks, virtually every word must be correctly understood to produce a correct answer: by contrast, much of the MUC-3 texts is irrelevant to the application task.", "labels": [], "entities": []}, {"text": "Since this methodology focuses on single queries (.perhaps with additional context), a smaller amount of language is being examined in each individual comparison.", "labels": [], "entities": []}, {"text": "Similarly, for database query, the database itself implicitly constrains the space of possible answers, and each answer is scored as either corrector incorrect.", "labels": [], "entities": []}, {"text": "This differs from the MUC evaluations, where an answer template is a composite of many bits of information, and is scored along the dimensions of recall, precision, and overgeneration.", "labels": [], "entities": [{"text": "MUC evaluations", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7086647748947144}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9992699027061462}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9980977177619934}]}, {"text": "Rome Laboratory has also sponsored a recent effort to define another approach to evaluating NL systems.", "labels": [], "entities": [{"text": "Rome Laboratory", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9639236927032471}]}, {"text": "This methodology is focussed on human evaluation of interactive systems, and is a \"glassbox\" method which looks at the performance of the linguistic components of the system under review.", "labels": [], "entities": []}], "tableCaptions": []}