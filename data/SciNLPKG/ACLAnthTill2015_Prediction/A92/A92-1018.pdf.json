{"title": [], "abstractContent": [{"text": "We present an implementation of a part-of-speech tagger based on a hidden Markov model.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7020612955093384}]}, {"text": "The methodology enables robust and accurate tagging with few resource requirements.", "labels": [], "entities": []}, {"text": "Only a lexicon and some unlabeled training text are required.", "labels": [], "entities": []}, {"text": "We describe implementation strategies and optimizations which result in high-speed operation.", "labels": [], "entities": []}, {"text": "Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.", "labels": [], "entities": [{"text": "phrase recognition", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8888102471828461}, {"text": "word sense disambiguation", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.7279978593190511}, {"text": "grammatical function assignment", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.6350541313489279}]}, {"text": "1 Desiderata Many words are ambiguous in their part of speech.", "labels": [], "entities": []}, {"text": "For example, \"tag\" can be a noun or a verb.", "labels": [], "entities": []}, {"text": "However, when a word appears in the context of other words, the ambiguity is often reduced: in '% tag is a part-of-speech label,\" the word \"tag\" can only be a noun.", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9717578291893005}]}, {"text": "A part-of-speech tagger is a system that uses context to assign parts of speech to words.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 2, "end_pos": 23, "type": "TASK", "confidence": 0.7310256510972977}]}, {"text": "Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora.", "labels": [], "entities": [{"text": "Automatic text tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.613230029741923}]}, {"text": "Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text.", "labels": [], "entities": []}, {"text": "For a tagger to function as a practical component in a language processing system, we believe that a tagger must be: Robust Text corpora contain ungrammatical constructions , isolated phrases (such as titles), and non-linguistic data (such as tables).", "labels": [], "entities": []}, {"text": "Corpora are also likely to contain words that are unknown to the tagger.", "labels": [], "entities": []}, {"text": "It is desirable that a tagger deal gracefully with these situations.", "labels": [], "entities": []}, {"text": "Efficient If a tagger is to be used to analyze arbitrarily large corpora, it must be efficient-performing in time linear in the number of words tagged.", "labels": [], "entities": []}, {"text": "Any training required should also be fast, enabling rapid turnaround with new corpora and new text genres.", "labels": [], "entities": []}, {"text": "Accurate A tagger should attempt to assign the correct part-of-speech tag to every word encountered.", "labels": [], "entities": [{"text": "Accurate", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.989639401435852}]}, {"text": "Tunable A tagger should be able to take advantage of linguistic insights.", "labels": [], "entities": []}, {"text": "One should be able to correct systematic errors by supplying appropriate a priori \"hints.\"", "labels": [], "entities": []}, {"text": "It should be possible to give different hints for different corpora.", "labels": [], "entities": []}, {"text": "Reusable The effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal.", "labels": [], "entities": []}, {"text": "2 Methodology 2.1 Background Several different approaches have been used for building text taggers.", "labels": [], "entities": [{"text": "building text taggers", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.6537292699019114}]}, {"text": "Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and Ku~era, 1982].", "labels": [], "entities": [{"text": "Brown corpus [Francis and Ku~era", "start_pos": 124, "end_pos": 156, "type": "DATASET", "confidence": 0.882216602563858}]}, {"text": "TAGGIT disambiguated 77% of the corpus ; the rest was done manually over a period of several years.", "labels": [], "entities": []}, {"text": "More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [Kosken-niemi, 1990].", "labels": [], "entities": []}, {"text": "Statistical methods have also been used (e.g., [DeRose, 1988], [Garside et al., 1987]).", "labels": [], "entities": []}, {"text": "These provide the capability of resolving ambiguity on the basis of most likely interpretation.", "labels": [], "entities": []}, {"text": "A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words.", "labels": [], "entities": []}, {"text": "Two types of training (i.e., parameter estimation) have been used with this model.", "labels": [], "entities": []}, {"text": "The first makes use of a tagged training corpus.", "labels": [], "entities": []}, {"text": "Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986].", "labels": [], "entities": []}, {"text": "At first, a relatively small amount of text is manually tagged and used to train a partially accurate model.", "labels": [], "entities": []}, {"text": "The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model.", "labels": [], "entities": []}, {"text": "Church uses the tagged Brown corpus for training [Church, 1988].", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8618729412555695}]}, {"text": "These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation.", "labels": [], "entities": []}, {"text": "The second method of training does not require a tagged training corpus.", "labels": [], "entities": []}, {"text": "In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972].", "labels": [], "entities": []}, {"text": "Under this regime the model is called a hidden Markov model (HMM), as state transitions (i.e., part-of-speech categories) are assumed to be unobservable.", "labels": [], "entities": []}, {"text": "Jelinek has used this method for training a text tagger [Jelinek, 1985].", "labels": [], "entities": [{"text": "text tagger", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.7673787474632263}]}, {"text": "Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from second-and first-order models and a uniform probability distribution [Jelinek and Mercer, 1980].", "labels": [], "entities": [{"text": "Parameter smoothing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8058941662311554}]}, {"text": "Kupiec used word equivalence classes (referred to here as ambiguity classes) based on parts of speech, to pool data from individual words [Ku-piec, 1989b].", "labels": [], "entities": []}, {"text": "The most common words are still represented individually, as sufficient data exist for robust estimation.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}