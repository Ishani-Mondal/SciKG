{"title": [{"text": "ROBUST TEXT PROCESSING IN AUTOMATED INFORMATION RETRIEVAL", "labels": [], "entities": [{"text": "ROBUST TEXT PROCESSING IN AUTOMATED INFORMATION RETRIEVAL", "start_pos": 0, "end_pos": 57, "type": "METRIC", "confidence": 0.5631763211318425}]}], "abstractContent": [{"text": "This paper outlines a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval.", "labels": [], "entities": [{"text": "text retrieval", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7306647300720215}, {"text": "statistical document retrieval", "start_pos": 165, "end_pos": 195, "type": "TASK", "confidence": 0.7026200095812479}]}, {"text": "The backbone of our system is a traditional retrieval engine which builds inverted index files from pre-processed documents , and then searches and ranks the documents in response to user queries.", "labels": [], "entities": []}, {"text": "Natural language processing is used to (1) preprocess the documents in order to extract contents-carrying terms, (2) discover inter-term dependencies and build a conceptual hierarchy specific to the database domain, and (3) process user's natural language requests into effective search queries.", "labels": [], "entities": []}, {"text": "The basic assumption of this design is that term-based representation of contents is in principle sufficient to build an effective if not optimal search query out of any user's request.", "labels": [], "entities": []}, {"text": "This has been confirmed by an experiment that compared effectiveness of expert-user prepared queries with those derived automatically from an initial narrative information request.", "labels": [], "entities": []}, {"text": "In this paper we show that large-scale natural language processing (hundreds of millions of words and more) is not only required fora better retrieval, but it is also doable, given appropriate resources.", "labels": [], "entities": []}, {"text": "We report on selected preliminary results of experiments with 500 MByte database of Wall Street Journal articles, as well as some earlier results with a smaller document collection.", "labels": [], "entities": [{"text": "Wall Street Journal articles", "start_pos": 84, "end_pos": 112, "type": "DATASET", "confidence": 0.9281180053949356}]}], "introductionContent": [{"text": "A typical information retrieval OR) task is to select documents from a d~!ahase in response to a user's query, and rank these documents according to relevance.", "labels": [], "entities": [{"text": "information retrieval OR", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7487062712510427}]}, {"text": "This has been usually accomplished using statistical methods (often coupled with manual encoding) that (a) select terms (words, phrases, and other units) from documents that are deemed to best represent their contents, and (b) create an inverted index file (or files) that provide and easy access to documents containing these terms.", "labels": [], "entities": []}, {"text": "An important issue here is that of finding an appropriate combination of term weights which would reflect each term's relative contribution to the information contents of the document.", "labels": [], "entities": []}, {"text": "Among many possible weighting schemes the inverted document frequency OdD has come to be recognized as universally applicable across variety of different text collections.", "labels": [], "entities": [{"text": "OdD", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.5100113749504089}]}, {"text": "Once the index is created, the search process will attempt to match a preprocessed user query (or queries) against representations of documents in each case determining a degree of relevance between the two which depends upon the number and types of matching terms.", "labels": [], "entities": []}, {"text": "Although many sophisticated search and matching methods are available, the crucial problem remains to be that of an adequate representation of contents for both the documents and the queries.", "labels": [], "entities": []}, {"text": "The simplest word-based representations of contents are usually inadequate since single words are rarely specific enough for accurate discrimination, and their grouping is often accidental.", "labels": [], "entities": []}, {"text": "A better method is to identify groups of words that create meaningful phrases, especially if these phrases denote important concepts in database domain.", "labels": [], "entities": []}, {"text": "For example, joint venture is an important term in Wall Street Journal (WSJ henceforth) database, while neither joint nor venture are important by themselves.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ henceforth) database", "start_pos": 51, "end_pos": 96, "type": "DATASET", "confidence": 0.9572484418749809}]}, {"text": "In the retrieval experiments with the WSJ database, we noticed that both joint and venture were dropped from the list of terms by the system because their idf weights were too low.", "labels": [], "entities": [{"text": "WSJ database", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9741460382938385}]}, {"text": "In large databases, such as TIPSTEK/TREC, the use of phrasal terms is not just desirable, it becomes necessary.", "labels": [], "entities": [{"text": "TIPSTEK/TREC", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.7112974524497986}]}, {"text": "The question thus becomes, how to identify the correct phrases in the text?", "labels": [], "entities": []}, {"text": "Both statistical and syntactic methods were used before with only limited success.", "labels": [], "entities": []}, {"text": "Statistical methods based on word cooccurrences and mutual information are prone to high error rates, turning out many unwanted associations.", "labels": [], "entities": []}, {"text": "Syntactic methods suffered from low quality of generated parse structures that could be attributed to limited coverage grammars and the lack of adequate lexicons.", "labels": [], "entities": []}, {"text": "the difficulties encountered in applying computational linguistics technologies to text processing have contributed to a wide-spread belief that automated natural language processing may not be suitable in IR.", "labels": [], "entities": [{"text": "text processing", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7505971789360046}, {"text": "IR", "start_pos": 206, "end_pos": 208, "type": "TASK", "confidence": 0.98136305809021}]}, {"text": "These difficulties included inefficiency, lack of robustness, and prohibitive cost of manual effort required to build lexicons and knowledge bases for each new text domain.", "labels": [], "entities": []}, {"text": "On the other hand, while numerous experiments did not establish the usefulness of linguistic methods in IR, they cannot be considered conclusive because of their limited scale.", "labels": [], "entities": [{"text": "IR", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9953411817550659}]}, {"text": "] The rapid progress in Computational Linguistics over the last few years has changed this equation in various ways.", "labels": [], "entities": [{"text": "Computational Linguistics", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.8250786364078522}]}, {"text": "First of all, large-scale resources became available: on-line lexicons, including Oxford Advanced Learner's Dictionary (OALD), Longman Dictionary of Contemporary English (LDOCE), Webster's Dictionary, Oxford English Dictionary, Collins Dictionary, and others, as well as large text corpora, many of which can now be obtained for research purposes.", "labels": [], "entities": [{"text": "Longman Dictionary of Contemporary English (LDOCE)", "start_pos": 127, "end_pos": 177, "type": "DATASET", "confidence": 0.7381738089025021}, {"text": "Oxford English Dictionary", "start_pos": 201, "end_pos": 226, "type": "DATASET", "confidence": 0.8657364447911581}, {"text": "Collins Dictionary", "start_pos": 228, "end_pos": 246, "type": "DATASET", "confidence": 0.9506833255290985}]}, {"text": "Robust text-oriented software tools have been built, including part of speech taggers (stochastic and otherwise), and fast parsers capable of processing text at speeds of 4200 words per minute or more (e.g., TIP parser developed by the author).", "labels": [], "entities": []}, {"text": "While many of the fast parsers are not very accurate (they are usually partial analyzers by design), 2 some, like TIP, perform in fact no worse than standard full-analysis parsers which are many times slower and far less robust.", "labels": [], "entities": []}, {"text": "3 An accurate syntactic analysis is an essential prerequisite for term selection, but it is by no means sufficient.", "labels": [], "entities": [{"text": "term selection", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.8001298606395721}]}, {"text": "Syntactic parsing of the database contents is usually attempted in order to extract linguistically motivated phrases, which presumably are better indicators of contents than \"statistical phrases\" where words are grouped solely on the basis of physical proximity (e.g., \"college junior\" is not the same as \"junior college').", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6923204213380814}]}, {"text": "However, creation of such compound terms makes term matching process more complex since in addition to the usual problems of synonymy and subsumption, one must deal with their structure (e.g., \"college junior\" is the same as \"junior in college\").", "labels": [], "entities": [{"text": "term matching", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7897241413593292}]}, {"text": "In order to deal with structure, parser's t Standard IR benchmark collectiot~s are statistically too small and the experiments can easily produce cotm~rinmitive results.", "labels": [], "entities": []}, {"text": "For example, Cnmfield collection is only approx. 180,000 English words, while CACM-3204 collection is approx. 200.000 words.", "labels": [], "entities": [{"text": "Cnmfield collection", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.9453455805778503}, {"text": "CACM-3204 collection", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.9097733199596405}]}, {"text": "output needs to be \"normalized\" or \"regularized\" so that complex terms with the same or closely related meanings would indeed receive matching representations.", "labels": [], "entities": []}, {"text": "This goal has been achieved to a certain extent in the present work.", "labels": [], "entities": []}, {"text": "As it will be discussed in more detail below, indexing terms were selected from among head-modifier pairs extracted from predicateargument representations of sentences.", "labels": [], "entities": []}, {"text": "The next important task is to achieve normalization across diferent terms with close or related meaning.", "labels": [], "entities": []}, {"text": "This can be accomplished by discovering various semantic relationships among words and phrases, such as synonymy and subsumption.", "labels": [], "entities": []}, {"text": "For example, the term natural language can be considered, in certain domains at least2 to subsume any term denoting a specific human language, such as English.", "labels": [], "entities": []}, {"text": "Therefore, a query containing the former maybe expected to retrieve documents containing the latter.", "labels": [], "entities": []}, {"text": "The system presented here computes term associations from text on word and fixed phrase level and then uses these associations in query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.6929526031017303}]}, {"text": "A fairly primitive filter is employed to separate synonymy and subsumption relationships from others including antonymy and complementation, some of which are strongly domain-dependent.", "labels": [], "entities": []}, {"text": "This process has led to an increased retrieval precision in experiments with smaller and more cohesive collections (CACM-3204).", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9924350380897522}]}, {"text": "In the following sections we present an overview of our system, with the emphasis on its textprocessing components.", "labels": [], "entities": []}, {"text": "We would like to point out here that the system is completely automated, i.e., all the processing steps, those performed by the statistical core. and these performed by the natural language processing components, are done automatically, and no human intervention or manual encoding is required.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Selecte filtered word similarities (* indicates  the more specific term).", "labels": [], "entities": []}, {"text": " Table 3. Run statistics for CACM-3204 da- tabase: with no NLP; with suffix trimmer,  and with both phrases and similarities.", "labels": [], "entities": [{"text": "CACM-3204 da- tabase", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.886928603053093}]}, {"text": " Table 4. Ran statistics with TIPSTER WSJ database  with top 200 documents considered per each query:", "labels": [], "entities": [{"text": "TIPSTER WSJ database", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.7812667290369669}]}]}