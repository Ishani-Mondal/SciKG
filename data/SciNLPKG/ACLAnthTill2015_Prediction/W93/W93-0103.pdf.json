{"title": [{"text": "Lexical Concept Acquisition From Collocation Map 1", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces an algorithm for automatically acquiring the conceptual structure of each word from corpus.", "labels": [], "entities": []}, {"text": "The concept of a word is defined within the proba-bilistic framework.", "labels": [], "entities": []}, {"text": "A variation of Belief Net named as Collocation Map is used to compute the probabilities.", "labels": [], "entities": []}, {"text": "The Belief Net captures the conditional independences of words, which is obtained from the cooccurrence relations.", "labels": [], "entities": []}, {"text": "The computation in general Belief Nets is known to be NP-hard, so we adopted Gibbs sampling for the approximation of the probabilities.", "labels": [], "entities": []}, {"text": "The use of Belief Net to model the lexical meaning is unique in that the network is larger than expected inmost other applications, and this changes the attitude toward the use of Belief Net.", "labels": [], "entities": []}, {"text": "The lexical concept obtained from the Collocation Map best reflects the subdomain of language usage.", "labels": [], "entities": []}, {"text": "The potential application of conditional probabilities the Collocation Map provides may extend to cover very diverse areas of language processing such as sense disambiguation, thesaurus construction, automatic indexing, and document classification.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.696722000837326}, {"text": "thesaurus construction", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.7348274886608124}, {"text": "document classification", "start_pos": 224, "end_pos": 247, "type": "TASK", "confidence": 0.7906102240085602}]}], "introductionContent": [{"text": "The level of the conceptual representation of words can be very complex in certain contexts, but in this paper we assume rather simple structure in which a concept is a set of weighted associated words.", "labels": [], "entities": []}, {"text": "We propose an automatic concept acquisition framework based on the conditional probabilities suppliedd by a network representation of lexical relations.", "labels": [], "entities": [{"text": "automatic concept acquisition", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.7055056293805441}]}, {"text": "The network is in the spirit of Belief Net, but the probabilities are not necessarily Bayesian.", "labels": [], "entities": []}, {"text": "In fact this variation of Bayesian Net is discussed recently by.", "labels": [], "entities": []}, {"text": "We employed the Belief Net with non Bayesian probabilities as abase for representing the statistical relations among concepts, and implemented the details of the computation.", "labels": [], "entities": []}, {"text": "Belief or Bayesian Nets have been extensively studied \"in the normative expert systems.", "labels": [], "entities": []}, {"text": "Experts provided the network with the Bayesian(subjective) probabilities solely based on his/her technical experiences.", "labels": [], "entities": []}, {"text": "Thus the net has been also known as a Belief Net among a dozen other names that share all or some of the principles of Bayesian net.", "labels": [], "entities": []}, {"text": "The probabilistic model has been also used in the problems of integrating various sources of evidences within sound framework.", "labels": [], "entities": []}, {"text": "One of the powerful features of Belief Net is that the conditional independences of the variables in the model are naturally captured, on which we can derive a form of probabilistic inference.", "labels": [], "entities": []}, {"text": "If we regard the occurrence of a word as a model variable and assume the variables occur within some conditional influences of the variables(words) that previously took place, the Belief approach appears to be appropriate to compute some aspects of lexical relations latent in the texts.", "labels": [], "entities": []}, {"text": "The probabilities on dependent variables are computed from the frequencies, so the probability is now of objective nature rather than Bayesian.", "labels": [], "entities": []}, {"text": "The variation of Belief Net we use is identical to the sigmoid Belief Net by.", "labels": [], "entities": []}, {"text": "In ordinary Belief Nets, 2 ~ probabilities fora parent variable with n children should be specified.", "labels": [], "entities": []}, {"text": "This certainly is a burden in our context in which the net may contain even hundred thousands of variables with heavy interconnections.", "labels": [], "entities": []}, {"text": "Sigmoid interpretation of the connections as in artificial neural networks provides a solution to the problem without damaging the power of the network.", "labels": [], "entities": []}, {"text": "Computing a joint probability is also exponential in an arbitrary Belief network, thus Gibbs sampling which originates from Metropolis algorithm introduced in 50's can be used to approximate the probabilities.", "labels": [], "entities": []}, {"text": "To speedup the convergence of the sampling we adopted simulated annealing algorithm with the sampling.", "labels": [], "entities": []}, {"text": "The simulated annealing is also a descendant of metropolis algorithm, and has been frequently used to compute an optimal state vector of a system of variables.", "labels": [], "entities": []}, {"text": "From the Collocation Map we can compute an arbitrary conditional probabilities of variables.", "labels": [], "entities": []}, {"text": "This is a very powerful utility applicable to every level of language processing.", "labels": [], "entities": []}, {"text": "To name a few automatic indexing, document classification, thesaurus construction, and ambiguity resolution are promising areas.", "labels": [], "entities": [{"text": "document classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8222188651561737}, {"text": "thesaurus construction", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7117434293031693}, {"text": "ambiguity resolution", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.6882482171058655}]}, {"text": "But one big problem with the model is that it cannot be used in real time applications because the Gibbs sampling still requires an ample amount of computation.", "labels": [], "entities": []}, {"text": "Some applications such as automatic indexing and lexical concept acquisition are fortunately not real time bounded tasks.", "labels": [], "entities": [{"text": "automatic indexing", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6001304090023041}, {"text": "lexical concept acquisition", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6528470317522684}]}, {"text": "We are currently undertaking a large scale testing of the model involving one hundred thousand words, which includes the study on the cost of sampling versus the accuracy of probability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9986599683761597}]}, {"text": "To reduce the computational cost in time, the multiprocessor model that is successfully implemented for Hopfield Network can be considered in the context of sampling.", "labels": [], "entities": []}, {"text": "Other options to make the sampling efficient should be actively pursued, and their success is the key to the implementation of the model to the real time problems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}