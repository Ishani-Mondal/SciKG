{"title": [{"text": "Extraction of V-N-Collocations from Text Corpora: A Feasibility Study for German", "labels": [], "entities": []}], "abstractContent": [{"text": "The usefulness of a statistical approach suggested by Church and Hanks (1989) is evaluated for the extraction of verb-noun (V-N) collocations from Ger-man text corpora.", "labels": [], "entities": [{"text": "extraction of verb-noun (V-N) collocations from Ger-man text corpora", "start_pos": 99, "end_pos": 167, "type": "TASK", "confidence": 0.6510950055989352}]}, {"text": "Some motivations for the extraction of V-N collocations from corpora are given and a couple of differences concerning the German language are mentioned that have implications on the applicability of extraction methods developed for English.", "labels": [], "entities": []}, {"text": "We present precision and recall results for V-N collo-cations with support verbs and discuss the consequences for further work on the extraction of collocations from German corpora.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9992678761482239}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9975121021270752}]}, {"text": "Depending on the goal to be achieved, emphasis can be put on a high recall for lexicographic purposes or on high precision for automatic lexical acquisition, in each case leading to a decrease of the corresponding other variable.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9989067316055298}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9972900152206421}, {"text": "automatic lexical acquisition", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.6144730746746063}]}, {"text": "Low recall can still be acceptable if very large corpora (i.e. 50-100 miUion words) are available or if corpora are used for special domains in addition to the data found in machine readable (collocation) dictionaries.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9995501637458801}]}], "introductionContent": [{"text": "Collocations present an area that is important both for lexicography to improve their coverage in modern dictionaries as well as for lexical acquisition in computational linguistics, where the goal is to build either large reusable lexical databases (LDBs) or specific lexica for specialized NLP-applications.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.72502401471138}]}, {"text": "We have tested the statistical approach Mutual Information (MI), brought up by for linguistics, fora (semi-)automatic extraction of verb-noun (V-N) collocations from untagged German text corpora.", "labels": [], "entities": [{"text": "Mutual Information (MI)", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7490295171737671}, {"text": "semi-)automatic extraction of verb-noun (V-N) collocations from untagged German text corpora", "start_pos": 102, "end_pos": 194, "type": "TASK", "confidence": 0.7922914425532023}]}, {"text": "We try to answer the question how much can be done with an untagged corpus and what might be gained by lemmatizing, POS-tagging or even superficial parsing.", "labels": [], "entities": []}, {"text": "Choueka (1988) describes how to automatically extract word combinations from English corpora as a preselection of collocation candidates to ease a lexicographer's search for collocations.", "labels": [], "entities": []}, {"text": "He only uses quantitative selection criteria, no statistical ones, his main extraction criterion being frequency with a lower threshold of at least one occurrence of the collocation in one million words.", "labels": [], "entities": [{"text": "frequency", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9776581525802612}]}, {"text": "He mentions plans to define a \"My thanks go to the ldS that made the two corpora available for research purposes, to Angelika Storrer for her steady encouragement and many fruitful discussions, and to Mats Ftooth and Matthias Heyn who introduced me to the corpora tools.", "labels": [], "entities": []}, {"text": "1 am also greatful to the anonymous reviewers for their helpful comments and constructive criticism.", "labels": [], "entities": []}, {"text": "\"binding degree' on how strong tile words of a collocation attract each other, which would be similar in spirit to what is calculated with MI.", "labels": [], "entities": [{"text": "binding degree", "start_pos": 1, "end_pos": 15, "type": "METRIC", "confidence": 0.7490105330944061}]}, {"text": "The work described in and is along the same lines as ours, though he uses a different statistical calculation, a z-score, and tagged, lemmatized corpora.", "labels": [], "entities": []}, {"text": "Some properties specific to German, however, lead to a type of problem that needs different treatment (section a.a).", "labels": [], "entities": []}, {"text": "Calzolari, Bindi {1990) use MI to extract compounds, fixed expressions and collocations fl'om an Italian corpus, but to our knowledge have not evaluated their results so far.", "labels": [], "entities": []}], "datasetContent": [{"text": "Below, the top bigrams with kommen (come) are shown, and some of the nonsignificant ones (t < 1.65), to illustrate MI and t-scores.", "labels": [], "entities": [{"text": "MI", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.9902364611625671}]}, {"text": "Bigrams with the infinitive form give best results compared to other inflection forms, possibly because this form covers lst/3rd pers. pl. present tense, the infinitive and the nonfinite main verb of complex tenses (modals, conditional, future) at the same time.", "labels": [], "entities": []}, {"text": "Also, the latter two always occur in verb-final position.", "labels": [], "entities": []}, {"text": "To see whether the collocational nouns could be better located directly to the left of the verb rather than within a couple of words, we reduced window-size to 3 words including the verb (this allows one word in between, e.g. 'zu' (to) in infinitival constructions).", "labels": [], "entities": []}, {"text": "As shown in table 1 for BI3 In:f, precision rises about 10%, but with a recall of 72.1%, because those collocations where other arguments or post modifiers occur between N and V are no longer captured.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9996746778488159}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9995935559272766}]}, {"text": "Taking again only significant combinations (BI3/'t In:f) precision rises again slightly.", "labels": [], "entities": [{"text": "BI3", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9668720364570618}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9509990811347961}]}, {"text": "This leads to the conclusion that for German, unless syntactic relations can be determined, a smaller window is preferable to improve a correct detection of preceding object arguments and to exclude unrelated nouns.", "labels": [], "entities": []}, {"text": "Because no lemmatizing program was available we used an additional program on top of the bigram calculations for the inflected forms.", "labels": [], "entities": []}, {"text": "In order to keep the amount of V-N combinations within a magnitude that could still be checked manually for correctness, we restricted search to a 3-word window to the left.", "labels": [], "entities": []}, {"text": "V-N combinations that occurred less than two times with a single inflection forth of the verb were sorted out.", "labels": [], "entities": []}, {"text": "The inflection forms for the infinitive (also lst/3rd pers. pl.), 3rd pers. sg. present and past tense, lst/3rd pets. pl. past and past participle were added up; 1st pers. sg. and 2nd pers.", "labels": [], "entities": []}, {"text": "sg./pl, were so rare thai they could be ignored.", "labels": [], "entities": []}, {"text": "The average results are again presented in table 1 (BI3 3.emma); the number of extracted collocations is maximal, but precision is the lowest of all.", "labels": [], "entities": [{"text": "BI3", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.4938444197177887}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9996546506881714}]}, {"text": "Precision ranges from 33.3% (gehen) to 88.2% (setzen), recall from 50% (erfahren) to 166.7% (setzen).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.99396812915802}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9996304512023926}]}, {"text": "Recall figures are above 100% because the absolute number of collocations found is higher than for BI6 In:f, the basis for the recall calculations.", "labels": [], "entities": [{"text": "BI6", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8854874968528748}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9986222982406616}]}, {"text": "Regarding lemmatization our study shows that one gets more collocations, but at the expense of more uninteresting combinations as well.", "labels": [], "entities": []}, {"text": "One explanation for this is that 3rd pers. sg. present/past and lst/3rd pers. pl. past only occur to the right of their noun argument insubordinate clauses, whereas lst/3rd pers. pl. present are identical with the nonfinite form which additionally occurs in verb-final position in main clauses with a finite auxiliary or modal verb and in infinitive clauses.", "labels": [], "entities": []}, {"text": "For infinitive bringen and lexeme bring-, V-N combinations were also calculated with BI6 fora larger corpus consisting of the MK1 and BZK together.", "labels": [], "entities": [{"text": "BI6", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9127158522605896}, {"text": "BZK", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.8286539912223816}]}, {"text": "For MK1 alone, 31 of 46 combinations are collocations, a precision of 67.4% (recall is set to 100%).", "labels": [], "entities": [{"text": "MK1", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8617203235626221}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9990371465682983}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9994274377822876}]}, {"text": "With the larger corpus the number of found V-N collocations is more than twice as big, with only a slightly lower precision 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.965013861656189}]}, {"text": "Thus, larger corpora would improve results considerably.", "labels": [], "entities": []}, {"text": "Results for the ]exeme with the highest number of collocations at all (73) are along the same lines; however almost, every second V-N combination is no V-N collocation in the sense defined in section 2, i.e. results are much better overall for the infinitive separately.", "labels": [], "entities": []}, {"text": "The complete data for bringen are listed below.", "labels": [], "entities": [{"text": "bringen", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.5619444251060486}]}, {"text": "Tagging possibly be improved by determining syntactic relations as done by Smadja (1991a,b) for English, we conducted another test with bringen, where we manually excluded those uninteresting extracted combinations in which the nouns were in fact used in subject position of the verb.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9512414336204529}, {"text": "bringen", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9035165309906006}]}, {"text": "The results for ~The latest runs with the combined corpus showed that for the infinitives precision even rises slightly on average (82.1%) while recall is almost doubled (134,9%); compared to BI3 Inf in table 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9881672263145447}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9993956089019775}, {"text": "BI3 Inf", "start_pos": 192, "end_pos": 199, "type": "METRIC", "confidence": 0.6291490793228149}]}, {"text": "the two window-sizes, infinitive and lexeme, are shown in table 3.", "labels": [], "entities": []}, {"text": "Precision would rise up to 100t?~, with still a good recall of S7.1t~.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9988405108451843}]}, {"text": "if one could consider syntactic/'elations for the extraction of V-N collocations.", "labels": [], "entities": []}, {"text": "Tile best recall of 43 collocations within 5 words to the left of the lexeme would then still correspond to 78.2c70 precision as compared to 587~, if subjects can/rot be detected.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9928088188171387}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9487934112548828}]}, {"text": "These results point in the same direction as Smadja's who reports an improvement fi'om 40 to 80% precision if syntactic relations are considered, with a 94% recall of all collocations that had been found regardless of syntactic/'elations.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9993720650672913}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.996966540813446}]}, {"text": "However, this cannot as easily be achieved in a large scale for German due to the complicated parsing techniques necessary for the varying word order.  with t-threshold (3tl), 3-word window without t (3I), 6-word window with t (6tl) and without (6I), 6-word window for the enlarged corpus (6I+).", "labels": [], "entities": []}, {"text": "3L stands for '3-word window, lexeme', 3L(oS) means the exclusion of subject nouns; 6L and 6L+ are analogous to the infinitive version.", "labels": [], "entities": []}, {"text": "The result for '61+' implies that larger corpora will improve recall without a serious decline of precision compared to the same method used with the smaller corpus (6I; see also footnote 2).", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9986070990562439}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9991752505302429}]}, {"text": "Whether the recall number should at the cost of a bad precision be pushed even higher by calculating MI for lexemes (6L vs. 6L+) can be decided in view of the application the data are extracted for.", "labels": [], "entities": [{"text": "recall number", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9839782118797302}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9916481971740723}, {"text": "MI", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.997219443321228}]}, {"text": "Once the number of V-N collocations is generally big enough, higher significance and MI thresholds can be used in order to improve precision again.", "labels": [], "entities": [{"text": "significance", "start_pos": 68, "end_pos": 80, "type": "METRIC", "confidence": 0.9681629538536072}, {"text": "MI thresholds", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.9818018674850464}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9987825751304626}]}, {"text": "MI sorts the extracted combinations in such away that the collocations are the better the higher the MI-score is (with a few exceptions which often reflect highly significant, but linguistically uninteresting word combinations from one of the texts; this could hopefully be avoided with a more balanced corpus).", "labels": [], "entities": [{"text": "MI-score", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9655569195747375}]}, {"text": "In general, a trade-off has to be found between the number of extracted collocations (recall) and the number of uninteresting items in between (precision), depending on the application.", "labels": [], "entities": [{"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.997217059135437}, {"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9986608028411865}]}, {"text": "The described approach seems to be a good method for corpora with texts from restricted domains, where a special terminology is used which will thus show up strongly against 'normal' combinations.", "labels": [], "entities": []}, {"text": "Very high precision rates, which are an indispensible requirement for lexical acquisition, can only realistically be envisaged for German with parsed corpora (3L(oS) has the best recall-precision ratio in; otherwise the main advantage lies in a better lexicographical support, which should not be underestimated both for manually built NLP lexica and for printed dictionaries.", "labels": [], "entities": [{"text": "precision rates", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9852809309959412}, {"text": "lexical acquisition", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7074019312858582}, {"text": "recall-precision ratio", "start_pos": 179, "end_pos": 201, "type": "METRIC", "confidence": 0.9853360950946808}]}, {"text": "Lemmatizing does not seem to be always useful, as a comparison of 61+ and 3L shows.", "labels": [], "entities": [{"text": "Lemmatizing", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9700163006782532}]}, {"text": "Possibly the data are blurred because as mentioned on p.", "labels": [], "entities": []}, {"text": "6 the various inflection forms are distributed differently in verb-final and verb-second clauses, at least in the investigated corpus.", "labels": [], "entities": []}, {"text": "Restricted lemmatizing with infinitive (lst/3rd pers. pl.) and past participle fora search to the left, and with 3rd pers. sg. pres./past and lst/3rd pers. pl. past fora search to the right (which is problematic, though) promises to give more precise results, as long as search strategies cannot take into account, the syntactic structure of a sentence.", "labels": [], "entities": []}, {"text": "Work is currently in progress to calculate trigrams to check for prepositions in SVCs or for specific (or no) determiners for phrasemes.", "labels": [], "entities": []}, {"text": "This will give indications to distinguish SVCs and lexicalized, phraseological SVCs from other collocations.", "labels": [], "entities": []}, {"text": "In addition, we plan to consider the variation in span position of the noun within the searched window in order to distinguish fixed phrasemes from flexible ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average figures for varying window-size and lemmatizing", "labels": [], "entities": []}, {"text": " Table 3: Results for bringen if subject nouns are excluded manually", "labels": [], "entities": []}]}