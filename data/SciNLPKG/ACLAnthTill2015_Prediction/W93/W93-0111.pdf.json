{"title": [{"text": "EXPERIMENTS IN SYNTACTIC AND SEMANTIC CLASSIFICATION AND DISAMBIGUATION USING BOOTSTRAPPING*", "labels": [], "entities": [{"text": "SEMANTIC CLASSIFICATION", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.8175529837608337}, {"text": "DISAMBIGUATION USING BOOTSTRAPPING", "start_pos": 57, "end_pos": 91, "type": "METRIC", "confidence": 0.6701133449872335}]}], "abstractContent": [{"text": "Bootstrap methods (unsupervised classification) that generate word classes without requiring pretagging have had notable success in the last few years.", "labels": [], "entities": []}, {"text": "The methods described here strengthen these approaches and produce excellent word classes from a 200,000 word corpus.", "labels": [], "entities": []}, {"text": "The method uses mutual information measures plus positional information from the words in the immediate context of a target word to compute similarities.", "labels": [], "entities": []}, {"text": "Using the similarities, classes are built using hierarchical agglomerative clustering.", "labels": [], "entities": []}, {"text": "At the leaves of the classification tree, words are grouped by syntactic and semantic similarity.", "labels": [], "entities": []}, {"text": "Further up the tree, the classes are primarily syntactic.", "labels": [], "entities": []}, {"text": "Once the initial classes are found, they can be used to classify ambiguous words, i.e., part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7257721722126007}]}, {"text": "This is done by expanding each context word of a target instance into a tightly defined class of similar words, a simsct.", "labels": [], "entities": []}, {"text": "The use of simsets is shown to increase the tagging accuracy from 83% to 92% for the forms \"cloned\" and \"deduced\".", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9874717593193054}]}], "introductionContent": [{"text": "The identification of the syntactic class and the discovery of semantic information for words not contained in any on-line dictionary or thesaurus is an important and challenging * This material is based upon work supported by the National Science Foundation under Grant No.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first experiment classified the 1,000 highest frequency words in the corpus, producing 999 clusters (0-998) during the process.", "labels": [], "entities": []}, {"text": "$pre... and Spas...", "labels": [], "entities": [{"text": "Spas", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9600817561149597}]}, {"text": "words were included in the context set, but not in the target set.", "labels": [], "entities": []}, {"text": "Near the leaves, words clustered by syntax (part of speech) and by semantics.", "labels": [], "entities": []}, {"text": "Later, larger clusters tended to contain words of the same syntactic class, but with less semantic homogeneity.", "labels": [], "entities": []}, {"text": "In each example below, the words listed are the entire contents of the cluster mentioned.", "labels": [], "entities": []}, {"text": "The most striking property of the clusters produced was the classification of words into coherent semantic fields.", "labels": [], "entities": [{"text": "classification of words into coherent semantic fields", "start_pos": 60, "end_pos": 113, "type": "TASK", "confidence": 0.8036754216466632}]}, {"text": "Grefenstette has pointed out) that the Deese antonyms, such as \"large\" and \"small\" or \"hot\" and \"cold\" show up commonly in these analyses.", "labels": [], "entities": []}, {"text": "Our methods discovered entire graded fields, rather than just pairs of opposites.", "labels": [], "entities": []}, {"text": "The following example shows a cluster of seventeen adjectives describing comparative quantity terms, cluster 756, similarity 0.28, decreased, effective, few, greater, high, higher, increased, large, less, low, lower, more, much, no, normal, reduced, short Note that pairs such as \"high\" and \"higher\" and \"low\" and \"lower\" appear.", "labels": [], "entities": []}, {"text": "\"No\", meaning \"none\" in this collection, is located atone extreme.", "labels": [], "entities": []}, {"text": "The somewhat marginal item, \"effective\", entered the cluster late, at cluster 704.", "labels": [], "entities": []}, {"text": "It appears in collocations, such as \"as effective as\" and \"effective than\", in which the other terms also appear.", "labels": [], "entities": []}, {"text": "Comparing the cluster to we find that all the items are in the Roget category Comparative Quantity except for \"effective\" and \"no\".", "labels": [], "entities": []}, {"text": "The cluster item, \"large\" is not in this Roget category but the category does include \"big\", \"huge\" and \"vast\", so the omission is clearly an error in Roget's.", "labels": [], "entities": []}, {"text": "With this correction, 88% (15/17) of the items are in the single Roget category.", "labels": [], "entities": []}, {"text": "The classification of technical terms from genetics and biochemistry is of particular interest, because many of these terms do not appear in available dictionaries or thesauri.", "labels": [], "entities": [{"text": "classification of technical terms", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8240731209516525}]}, {"text": "Cluster 374, similarity 0.37, contains these 18 items, che, cheA, cheB, cheR, cheY, cheZ, double, fla, flaA, taB, flaE, H2, hag, mot, motB, tar, trg, tsr All of these are abbreviations for specific bacterial mutations, except for \"double\".", "labels": [], "entities": []}, {"text": "Its appearance drives home the point that the classification depends entirely on usage.", "labels": [], "entities": [{"text": "classification", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.9641920924186707}]}, {"text": "20 of the 30 occurrences of \"double\" precede the words \"mutant\" or \"mutants\", as do most of the othermutation terms in this cluster.", "labels": [], "entities": []}, {"text": "Cluster 240, similarity 0.4 contains these termS, microscopy, electrophoresis, chromatography Each of these is a noun describing a common technique used in experiments in this domain.", "labels": [], "entities": [{"text": "termS", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.8856534361839294}]}, {"text": "The standard Linnean nomenclature of Genus followed by species, such as Escherichia coli, is reflected by cluster 414, which contains 22 species names, and cluster 510, which contains 9 genus names.", "labels": [], "entities": []}, {"text": "In scientific research, the determination of causal factors and the discovery of essential elements is a major goal.", "labels": [], "entities": []}, {"text": "Here are six concepts in this semantic field comprising cluster 183, similarity 0.43, required, necessary, involved, responsible, essential, important These terms are used almost interchangeably in our corpus, but they don't fare as well in Roget's because of anthropocentric attachments to concepts such as fame, duty and legal liability.", "labels": [], "entities": []}, {"text": "Given the limited context and modest sized corpus, the classification algorithm is bound to make mistakes, though a study of the text concordance will always tell us why the algorithm failed in any specific case.", "labels": [], "entities": []}, {"text": "For example, as the similarity drops to 0.24 at cluster 824 we seethe adverb triple \"greatly\", \"rapidly\" and \"almost\".", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9652467370033264}]}, {"text": "This is still acceptable, but by cluster 836 (similarity 0.24) we seethe triple, \"them\", \"ring\", \"rings\".", "labels": [], "entities": []}, {"text": "At the end there is only a single cluster, 998, which must include all words.", "labels": [], "entities": []}, {"text": "It comes together stubbornly with a negative similarity of-0.51.", "labels": [], "entities": [{"text": "similarity", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.8523626327514648}]}, {"text": "One problem encountered in this work was that the later, larger clusters have less coherence than we would hope for, identifying an important research issue.", "labels": [], "entities": []}, {"text": "Experiment #1 took 20 hours to run on a Symbolics XL1200.", "labels": [], "entities": []}, {"text": "A fundamental problem is to devise decision procedures that will tell us which classes are semantically or syntactically homogeneous; procedures that tell us whereto cut the tree.", "labels": [], "entities": []}, {"text": "The examples shown earlier broke down soon after, when words or clusters which in our judgment were weakly related began to be added.", "labels": [], "entities": []}, {"text": "We are exploring the numerous methods to refine clusters once formed as well as methods to validate clusters for homogeneity.", "labels": [], "entities": []}, {"text": "There are also resampling methods to validate clusters formed by top-down partitioning methods.", "labels": [], "entities": []}, {"text": "All of these methods are computationally demanding but they can result in criteria for when to stop clustering.", "labels": [], "entities": []}, {"text": "On the other hand, we mustn't assume that word relations are so simple that we can legitimately insist on finding neatly separated clusters.", "labels": [], "entities": []}, {"text": "Word relations may simply be too complex and graded for this ever to occur.", "labels": [], "entities": []}, {"text": "The semantic fields we discovered were not confined to synonyms.", "labels": [], "entities": []}, {"text": "To understand why this is the case, consider the sentences, \"The temperature is higher today.\" and, \"The temperature is lower today.\"", "labels": [], "entities": []}, {"text": "There is noway to tell from the syntax which word to expect.", "labels": [], "entities": []}, {"text": "The choice is dependent on the situation in the world; it represents data from the world.", "labels": [], "entities": []}, {"text": "The utterances are informative for just that reason.", "labels": [], "entities": []}, {"text": "Taking this reasoning a step further, information theory would suggest that for two contrasting words to be maximally informative, they should appear about equally often in discourse.", "labels": [], "entities": []}, {"text": "This is born out in our corpus (fhigher=58, i~ower=46) and for the Brown corpus (fhigher=147, fiower=110).", "labels": [], "entities": [{"text": "fhigher", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9011003375053406}, {"text": "Brown corpus", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9784645140171051}]}, {"text": "The same relations are found for many other contrasting pairs, with some bias towards \"positive\" terms.", "labels": [], "entities": []}, {"text": "The most extreme \"positive\" bias in our corpus is fpossible=88, fimpossible=0; \"never say never\" seems to be the catchphrase here --highly appropriate for the field of biology.", "labels": [], "entities": [{"text": "fpossible", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.973610520362854}, {"text": "fimpossible", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9942693710327148}]}, {"text": "Some of the chemical term clusters that were generated are interesting because they contain class terms such as \"sugar\" and \"ion\" along with specific members of the classes (hyponyms), such as \"maltose\" and \"Na +''.", "labels": [], "entities": []}, {"text": "Comparing these in our KWIC concordance suggests that there maybe methodical techniques for identifying some of these generalization hierarchies using machine learning (supervised classification) . For another discussion of attempts to generate generalization hierarchies, see.", "labels": [], "entities": []}, {"text": "As a corpus grows and new words appear, one way to classify them is to find their similarity to the N words for which context vectors have already been computed.", "labels": [], "entities": []}, {"text": "A more efficient method which would probably give the same result would be to successively compare the word to clusters in the tree, starting at the root.", "labels": [], "entities": []}, {"text": "At each node, the child which is most similar to the unclassified word is followed.", "labels": [], "entities": []}, {"text": "This is a logarithmic search technique for finding the best matching class which takes only O(log2N) steps.", "labels": [], "entities": [{"text": "O", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.9494711756706238}]}, {"text": "In such an approach, the hierarchical cluster is being used as a decision tree, which have been much studied in the machine learning literature.", "labels": [], "entities": []}, {"text": "This is an alternate view of the classification approach as the unsupervised learning of a decision tree.", "labels": [], "entities": []}, {"text": "The following experiment is interesting because it shows a specific use for the similarity computations.", "labels": [], "entities": []}, {"text": "They are used hereto increase the accuracy of term disambiguation which means selecting the best tag or class fora potentially ambiguous word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9993680119514465}, {"text": "term disambiguation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6676443666219711}]}, {"text": "Again, this is a bootstrap method; no prior tagging is needed to construct the classes.", "labels": [], "entities": []}, {"text": "But if we do identify the tags fora few items by hand or by using a hand-tagged reference corpus, the tags for all the other items in a cluster can be assumed equal to the known items.", "labels": [], "entities": []}, {"text": "The passive voice is used almost exclusively in the corpus, with some use of the editorial \"We\".", "labels": [], "entities": []}, {"text": "This results in a profusion of participles such as \"detected\", \"sequenced\" and \"identified\".", "labels": [], "entities": []}, {"text": "But such -ed forms can also be simple past tense forms or adjectives.", "labels": [], "entities": []}, {"text": "In addition, we identified their use in a postmodifying participle clause such as, \"...", "labels": [], "entities": []}, {"text": "the value ~ from this measurement.\"", "labels": [], "entities": []}, {"text": "Each one of the 88 instances of \"cloned\" and the 50 instances of \"deduced\" was hand tagged and given a unique ID.", "labels": [], "entities": []}, {"text": "Then clustering was applied to the resulting collection, giving the result shown in.", "labels": [], "entities": []}, {"text": "Experiments #2 and #3 took about 15 minutes each to run.", "labels": [], "entities": []}, {"text": "The resultant clusters are somewhat complex.", "labels": [], "entities": []}, {"text": "There are four tags and we have shown the top four clusters, but two of the clusters contain adjectives exclusively.", "labels": [], "entities": []}, {"text": "The past participle and postmodifier occur together in the same cluster.", "labels": [], "entities": []}, {"text": "(We studied the children of cluster 4, hoping to find better separation, but they are no better.", "labels": [], "entities": []}, {"text": ") The scoring metric we chose was to associate each cluster with the items that were in the majority in the node and score all other items as errors.", "labels": [], "entities": []}, {"text": "This is a good approximation to a situation in which a \"gold standard\" is available to classify the clusters by independent means, such as comparing the clusters to items from a pretagged reference corpus..", "labels": [], "entities": []}, {"text": "Clustering of 88 occurrence of\"cloned\" and 50 occurrences of\"deduced\" into four syntactic categories.", "labels": [], "entities": []}, {"text": "The abbreviations, such as \"JJ\", are based on.", "labels": [], "entities": []}, {"text": "There is a strong admixture of adjectives in cluster 2 and all the postmodifiers are confounded with the past participles in cluster 4.", "labels": [], "entities": []}, {"text": "The total number of errors (minority classes in a cluster) is 23 fora success rate of(138-23)/138 = 83%.", "labels": [], "entities": []}, {"text": "All minority members of a cluster are counted as errors.", "labels": [], "entities": []}, {"text": "This leads to the 83% error rate quoted in the figure caption.", "labels": [], "entities": [{"text": "83% error rate", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.8617450445890427}]}, {"text": "The results shown in can be improved as follows.", "labels": [], "entities": []}, {"text": "Because we are dealing with single occurrences, only one element, or possibly zero, in each of the four context word vectors is filled, with frequency 1.", "labels": [], "entities": []}, {"text": "The other 149 elements have frequency (and mutual information) 0.0.", "labels": [], "entities": [{"text": "frequency", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9721657037734985}]}, {"text": "These sparse vectors will therefore have little or no overlap with vectors from other occurrences.", "labels": [], "entities": []}, {"text": "In order to try to improve the classification, we expanded the context values in an effort to produce more overlap, using the following strategy: We proceed as if the corpus is far larger so that in addition to the actual context words already seen, there are many occurrences of highly similar words in the same positions.", "labels": [], "entities": []}, {"text": "For each non-zero context in each set of 150, we expand it to an ordered class of similar words in the 150, picking words above a fixed similarity threshold (0.3 for the experiments reported here).", "labels": [], "entities": []}, {"text": "Such a class is called a simset, made up of abase word and a sequence of expansion words.", "labels": [], "entities": []}, {"text": "As an example of the expansion of context words via simsets, suppose that the occurrence of the frequency 1 word \"cheA-cheB\" is immediately preceded by \"few\" and the occurrence of the frequency 1 word \"CheA/CheB\" is immediately preceded by \"less\".", "labels": [], "entities": []}, {"text": "The -I C context vectors for each will have l's in different positions so there will be no overlap between them.", "labels": [], "entities": []}, {"text": "If we expanded \"few\" into a large enough simset, the set would eventually contain, \"less\", and vice-versa.", "labels": [], "entities": []}, {"text": "Barring that, each simset might contain a distinct common word such as \"decreased\".", "labels": [], "entities": []}, {"text": "In either case, there would now be some overlap in the context vectors so that the similar use of \"cheA-cheB\" and \"CheA/CheB\" could be detected.", "labels": [], "entities": []}, {"text": "The apparent frequency of each expansion word is based on its corpus frequency relative to the corpus frequency of the word being expanded.", "labels": [], "entities": []}, {"text": "To expand a single context word instance ci appearing with frequency fik in the context of 1 or more occurrences of center word wk, choose all cj such that cj e {set of high-frequency context words} and the similarity S(ci,cj) _> St, a threshold value.", "labels": [], "entities": []}, {"text": "Set the apparent frequency of each expansion word cj to fjk = S(ci,cj)xfik x fj / fi , where fi and fj are the corpus frequencies of ci and cj.", "labels": [], "entities": []}, {"text": "Normalize the total frequency of the context word plus the apparent frequencies of the expansion words to fik.", "labels": [], "entities": []}, {"text": "For the example being discussed here, fik = 1, St=0.3 and the average number of expansion words was 6.", "labels": [], "entities": []}, {"text": "Recomputing the classification of the -ed forms with the expanded context words results in the improved classification shown in.", "labels": [], "entities": []}, {"text": "The number of classification errors is halved, yielding a success rate of 92%.", "labels": [], "entities": [{"text": "classification errors", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7768433392047882}, {"text": "success", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.963405191898346}]}, {"text": "This is comparable in performance to many stochastic tagging algorithms.", "labels": [], "entities": [{"text": "stochastic tagging", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.5379520058631897}]}, {"text": "This analysis is very similar to part-ofspeech tagging.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7382273375988007}]}, {"text": "The simsets of only 6 items are far smaller than the part-of-speech categories conventionally used.", "labels": [], "entities": []}, {"text": "But since we use high frequency words, they represent a substantial portion of the instances.", "labels": [], "entities": []}, {"text": "Also, they have higher specificity than, say, Verb.", "labels": [], "entities": [{"text": "Verb", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8720338344573975}]}, {"text": "Many taggers work sequentially and depend on the left context.", "labels": [], "entities": []}, {"text": "But some words are best classified by their right context.", "labels": [], "entities": []}, {"text": "Clearly this small experiment did not reach the accuracy of the very best taggers, but it performed well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996620416641235}]}, {"text": "This experiment has major ramifications for the future.", "labels": [], "entities": []}, {"text": "The initial classifications found merged all identical word forms together, both as targets and contexts.", "labels": [], "entities": []}, {"text": "But disambiguation techniques such as those in Experiment #2 can be used to differentially tag word occurrences with some degree of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9941717982292175}]}, {"text": "These newly classified items can in turn be used as new target and context items (if their frequencies are adequate) and the analysis can be repeated.", "labels": [], "entities": []}, {"text": "Iterating the method in this way should be able to refine the classes until a fixed point is reached at which no further improvement in classification occurs.", "labels": [], "entities": []}, {"text": "The major challenge in using this approach will be to keep it computationally tractable.", "labels": [], "entities": []}, {"text": "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models, though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum..", "labels": [], "entities": []}, {"text": "Clustering of\"cloned\" and \"deduced\" after expansion of the context words.", "labels": [], "entities": []}, {"text": "The postmodifying form, not isolated before, is fairly well isolated in its own subclass.", "labels": [], "entities": []}, {"text": "The total number of errors is reduced from 23 to 11, fora success rate of 92%.", "labels": [], "entities": [{"text": "errors", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8908002376556396}, {"text": "success rate", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.966231107711792}]}, {"text": "When classifying multiple instances of a single word form as we did in Experiment #2, there are numerous collocations that aid the classification.", "labels": [], "entities": []}, {"text": "For example, 16 of the 50 occurrences of the word \"deduced\" occur in the phrase, \"of the ~ amino acid sequence\".", "labels": [], "entities": []}, {"text": "But with words of frequency 1, we cannot rely on such similarities.", "labels": [], "entities": []}, {"text": "Nevertheless, we experimented with classifying 100 words of corpus frequency 1 with and without expanding the context words.", "labels": [], "entities": []}, {"text": "Though hand scoring the results is difficult, we estimate that there were 8 reasonable pairs found initially and 26 pairs when expansion was used.", "labels": [], "entities": [{"text": "hand scoring", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.7210471332073212}]}, {"text": "Examples of words that paired well without expansion are \"overlaps\" and \"flank\" (due to a preceding \"which\") and \"malB\" and \"cheA-cheB\" (due to the context \"...the [malB, cheA-cheB] region...\").", "labels": [], "entities": []}, {"text": "After expansion, pairs such as \"setting\", \"resetting\" appeared (due in part to the expansion of the preceding \"as\" and \"to\" context words into simsets which both included \"with\", \"in\" and \"by\").", "labels": [], "entities": []}, {"text": "Discussion of Experiment #3.", "labels": [], "entities": []}, {"text": "The amount of information available about frequency 1 words can vary from a lotto nothing at all, and most frequently tends to the latter, viz., \"John and Mary looked at the blork.\"", "labels": [], "entities": []}, {"text": "Nevertheless, such words are prominent, 44% of our corpus' vocabulary.", "labels": [], "entities": []}, {"text": "About half Of them are non-technical and can therefore be analyzed from other corpora or on-line dictionaries.", "labels": [], "entities": []}, {"text": "Word morphology and Latinate morphology in particular, can be helpful.", "labels": [], "entities": [{"text": "Word morphology", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6985495090484619}]}, {"text": "Online chemical databases, supplemented with rules for chemical nomenclature will clarify additional items, e.g., \"2-epoxypropylphosphonic\" or \"phosphoglucomutase-deflcient\".", "labels": [], "entities": []}, {"text": "Furthermore, there are naming conventions for genetic strains and mutants which aid recognition.", "labels": [], "entities": []}, {"text": "The combination of all these methods should lead to a reasonable accuracy in the classification of frequency 1 words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9994391798973083}]}], "tableCaptions": []}