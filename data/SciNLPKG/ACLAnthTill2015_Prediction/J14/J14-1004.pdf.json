{"title": [{"text": "Learning Representations for Weakly Supervised Natural Language Processing Tasks", "labels": [], "entities": [{"text": "Weakly Supervised Natural Language Processing Tasks", "start_pos": 29, "end_pos": 80, "type": "TASK", "confidence": 0.6283078591028849}]}], "abstractContent": [{"text": "Finding the right representations for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce.", "labels": [], "entities": []}, {"text": "This article investigates novel techniques for extracting features from n-gram models, Hidden Markov Models, and other statistical language models, including a novel Partial Lattice Markov Random Field model.", "labels": [], "entities": []}, {"text": "Experiments on part-of-speech tagging and information extraction, among other tasks, indicate that features taken from statistical language models, in combination with more traditional features, outperform traditional representations alone, and that graphical model representations outperform n-gram models, especially on sparse and polysemous words.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7268304228782654}, {"text": "information extraction", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.8008746802806854}]}], "introductionContent": [{"text": "NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve strong performance.", "labels": [], "entities": []}, {"text": "Thus, a part-of-speech (POS) tagger would traditionally use a feature like, \"the previous token is the\" to help classify a given token as a noun or adjective.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagger", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.6320857107639313}]}, {"text": "For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results.", "labels": [], "entities": []}, {"text": "However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication.", "labels": [], "entities": []}, {"text": "These texts have very different characteristics from traditional training corpora in NLP.", "labels": [], "entities": []}, {"text": "Evidence from POS tagging, parsing, and semantic role labeling (SRL), among other NLP tasks, shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7934045195579529}, {"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.7751281261444092}, {"text": "semantic role labeling (SRL)", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.784718910853068}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9983578324317932}]}, {"text": "Collecting labeled training data for each new target domain is typically prohibitively expensive.", "labels": [], "entities": []}, {"text": "In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning when domain-specific labeled training data are scarce.", "labels": [], "entities": []}, {"text": "A growing body of theoretical and empirical evidence suggests that traditional, manually crafted features fora variety of NLP tasks limit systems' performance in this weakly supervised learning for two reasons.", "labels": [], "entities": []}, {"text": "First, feature sparsity prevents systems from generalizing accurately, because many words and features are not observed in training.", "labels": [], "entities": []}, {"text": "Also because word frequencies are Zipf-distributed, this often means that there is little relevant training data fora substantial fraction of parameters (Bikel 2004b), especially in new domains.", "labels": [], "entities": []}, {"text": "For example, word-type features form the backbone of most POS-tagging systems, but types like \"gene\" and \"pathway\" show up frequently in biomedical literature, and rarely in newswire text.", "labels": [], "entities": []}, {"text": "Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features \"gene\" and \"pathway\").", "labels": [], "entities": []}, {"text": "Further, because words are polysemous, word-type features prevent systems from generalizing to situations in which words have different meanings.", "labels": [], "entities": []}, {"text": "For instance, the word type \"signaling\" appears primarily as a present participle (VBG) in Wall Street Journal (WSJ) text, as in, \"Interest rates rose, signaling that . .", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) text", "start_pos": 91, "end_pos": 121, "type": "DATASET", "confidence": 0.8962397064481463}]}, {"text": "\". In biomedical text, however, \"signaling\" appears primarily in the phrase \"signaling pathway,\" where it is considered a noun (NN)); this phrase never appears in the WSJ portion of the Penn Treebank (.", "labels": [], "entities": [{"text": "WSJ portion of the Penn Treebank", "start_pos": 167, "end_pos": 199, "type": "DATASET", "confidence": 0.9409752488136292}]}, {"text": "Our response to the sparsity and polysemy challenges with traditional NLP representations is to seek new representations that allow systems to generalize to previously unseen examples.", "labels": [], "entities": []}, {"text": "That is, we seek representations that permit classifiers to have close to the same accuracy on examples from other domains as they do on the domain of the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9966637492179871}]}, {"text": "Our approach depends on the well-known distributional hypothesis, which states that a word's meaning is identified with the contexts in which it appears).", "labels": [], "entities": []}, {"text": "Our goal is to develop probabilistic statistical language models that describe the contexts of individual words accurately.", "labels": [], "entities": []}, {"text": "We then construct representations, or mappings from word tokens and types to real-valued vectors, from statistical language models.", "labels": [], "entities": []}, {"text": "Because statistical language models are designed to model words' contexts, the features they produce can be used to combat problems with polysemy.", "labels": [], "entities": []}, {"text": "And by careful design of the statistical language models, we can limit the number of features that they produce, controlling how sparse those features are in training data.", "labels": [], "entities": []}, {"text": "Our specific contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We show how to generate representations from a variety of language models, including n-gram models, Brown clusters, and Hidden Markov Models (HMMs).", "labels": [], "entities": []}, {"text": "We also introduce a Partial-Lattice Markov Random Field (PL-MRF), which is a tractable variation of a Factorial Hidden Markov Model (Ghahramani and Jordan 1997) for language modeling, and we show how to produce representations from it.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7836921215057373}]}], "datasetContent": [{"text": "For domain adaptation, we test our representations on two sequence labeling tasks: POS tagging and chunking.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.833391934633255}, {"text": "POS tagging", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.7751873731613159}]}, {"text": "To incorporate learned representation into our models, we follow this general procedure, although the details vary by experiment and are given in the following sections.", "labels": [], "entities": []}, {"text": "First, we collect a set of unannotated text from both the training domain and test domain.", "labels": [], "entities": []}, {"text": "Second, we learn representations on the unannotated text.", "labels": [], "entities": []}, {"text": "We then automatically annotate both the training and test data with features from the learned representation.", "labels": [], "entities": []}, {"text": "Finally, we train a supervised linear-chain CRF model on the annotated training set and apply it to the test set.", "labels": [], "entities": []}, {"text": "A linear-chain CRF is a Markov random field (Darroch, Lauritzen, and Speed 1980) in which the latent variables form a path with edges only between consecutive nodes in the path, and all latent variables are globally conditioned on the observations.", "labels": [], "entities": []}, {"text": "Let X be a random variable over data sequences, and Z be a random variable over corresponding label sequences.", "labels": [], "entities": []}, {"text": "The conditional distribution over the label sequence Z given X has the form where f j (z i\u22121 , z i , x, i) is a real-valued feature function of the entire observation sequence and the labels at positions i and i \u2212 1 in the label sequence, and \u03b8 j is a parameter to be estimated from training data.", "labels": [], "entities": []}, {"text": "We use an open source CRF software package designed by Sunita Sarawagi to train and apply our CRF models.", "labels": [], "entities": []}, {"text": "As is standard, we use two kinds of feature functions: transition and observation.", "labels": [], "entities": []}, {"text": "Transition feature functions indicate, for each pair of labels land l \ud97b\udf59 , whether z i = land z i\u22121 = l \ud97b\udf59 . Boolean observation feature functions indicate, for each label land each feature f provided by a representation, whether z i = land xi has feature f . For each label land each real-valued feature fin representation R, real-valued observation feature functions have value f (x) if z i = l, and are zero otherwise.", "labels": [], "entities": []}, {"text": "In this section, we evaluate our learned representations on their ability to capture semantic, rather than syntactic, information.", "labels": [], "entities": []}, {"text": "Specifically, we investigate a set-expansion task in which we're given a corpus and a few \"seed\" noun phrases from a semantic category (e.g., Superheroes), and our goal is to identify other examples of the category in the corpus.", "labels": [], "entities": []}, {"text": "This is a different type of weakly supervised task from the earlier domain adaptation tasks because we are given only a handful of positive examples from a category, rather than a large sample of positively and negatively labeled training examples from a separate domain.", "labels": [], "entities": []}, {"text": "Existing set-expansion techniques utilize the distributional hypothesis: Candidate noun phrases fora given semantic class are ranked based on how similar their contextual distributions are to those of the seeds.", "labels": [], "entities": []}, {"text": "Here, we measure how performance on the set-expansion task varies when we employ different representations for the contextual distributions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Learned representations, and especially latent-variable statistical language model  representations, significantly outperform a traditional CRF system on domain adaptation for  POS tagging. Percent error is shown for all words and out-of-vocabulary (OOV) words. The  SCL+500bio system was given 500 labeled training sentences from the biomedical domain.  1.8% of tokens in the biomedical test set had POS tags like 'HYPHENATED', which are not  part of the tagset for the training data, and were labeled incorrectly by all systems without  access to labeled data from the biomedical domain. As a result, an error rate of 1.8 + 3.9 = 5.7  serves as a reasonable lower bound for a system that has never seen labeled examples from  the biomedical domain.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 187, "end_pos": 198, "type": "TASK", "confidence": 0.8489423990249634}]}, {"text": " Table 3  Graphical models consistently outperform n-gram models by a larger margin on sparse words  than not-sparse words, and by a larger margin on polysemous words than not-polysemous  words. One exception is the NB-R, which performs worse relative to WEB1T-n-GRAM-R on  polysemous words than non-polysemous words. For each graphical model representation,  we show the difference in performance between that representation and WEB1T-n-GRAM-R  in parentheses. For each representation, differences in accuracy on polysemous and  non-polysemous subsets were statistically significant at p < 0.01 using a two-tailed  Fisher's exact test. Likewise for performance on sparse vs. non-sparse categories.", "labels": [], "entities": [{"text": "WEB1T-n-GRAM-R", "start_pos": 430, "end_pos": 444, "type": "DATASET", "confidence": 0.8979448676109314}, {"text": "accuracy", "start_pos": 502, "end_pos": 510, "type": "METRIC", "confidence": 0.99730384349823}]}, {"text": " Table 4  POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations  outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target  domains, * indicates the performance is statistically significantly better than the Stanford and  TRAD-R baselines at p < 0.05, using a two-tailed \u03c7 2 test; ** indicates significance at p < 0.01.  On the news domain, the Stanford tagger is significantly different from all other systems  using a two-tailed \u03c7 2 test with p < 0.01.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8839501440525055}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9131879806518555}, {"text": "LATTICE-TOKEN-R", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.8125572800636292}]}, {"text": " Table 5  I-HMM-TYPE-R outperforms the other methods, improving performance over a random  baseline by twice as much as either n-GRAM-R or LATTICE-TYPE-R.", "labels": [], "entities": []}, {"text": " Table 6  Graphical models as representations for IE consistently perform better relative to n-gram models  on sparse words, but not necessarily polysemous words.", "labels": [], "entities": [{"text": "IE", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.96323162317276}]}, {"text": " Table 6. Both graphical  model representations outperform the n-gram representation more on sparse words, as  expected. For polysemy, the picture is mixed: The LATTICE-TYPE-R outperforms  n-GRAM-R on polysemous categories, whereas HMM-TYPE-R's performance advan- tage over n-GRAM-R decreases.", "labels": [], "entities": [{"text": "LATTICE-TYPE-R", "start_pos": 161, "end_pos": 175, "type": "METRIC", "confidence": 0.9917187690734863}]}]}