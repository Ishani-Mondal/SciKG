{"title": [{"text": "Feature-Frequency-Adaptive On-line Training for Fast and Accurate Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Training speed and accuracy are two major concerns of large-scale natural language processing systems.", "labels": [], "entities": [{"text": "speed", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.8820085525512695}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999015212059021}]}, {"text": "Typically, we need to make a tradeoff between speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9974913597106934}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9979215264320374}]}, {"text": "It is trivial to improve the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9945969581604004}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9995872378349304}]}, {"text": "Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time, which is the target of this work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9996539354324341}]}, {"text": "To reach this target, we present anew training method, feature-frequency-adaptive on-line training, for fast and accurate training of natural language processing systems.", "labels": [], "entities": []}, {"text": "It is based on the core idea that higher frequency features should have a learning rate that decays faster.", "labels": [], "entities": []}, {"text": "Theoretical analysis shows that the proposed method is convergent with a fast convergence rate.", "labels": [], "entities": []}, {"text": "Experiments are conducted based on well-known benchmark tasks, including named entity recognition, word segmentation, phrase chunking, and sentiment analysis.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6533105770746866}, {"text": "word segmentation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7817817032337189}, {"text": "phrase chunking", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.8022168278694153}, {"text": "sentiment analysis", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9471990168094635}]}, {"text": "These tasks consist of three structured classification tasks and one non-structured classification task, with binary features and real-valued features, respectively.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that the proposed method is faster and at the same time more accurate than existing methods, achieving state-of-the-art scores on the tasks with different characteristics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Training speed is an important concern of natural language processing (NLP) systems.", "labels": [], "entities": [{"text": "speed", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.7199129462242126}]}, {"text": "Large-scale NLP systems are computationally expensive.", "labels": [], "entities": []}, {"text": "In many real-world applications, we further need to optimize high-dimensional model parameters.", "labels": [], "entities": []}, {"text": "For example, the state-of-the-art word segmentation system uses more than 40 million features.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.6904233694076538}]}, {"text": "The heavy NLP models together with high-dimensional parameters lead to a challenging problem on model training, which may require week-level training time even with fast computing machines.", "labels": [], "entities": []}, {"text": "Accuracy is another very important concern of NLP systems.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9808613657951355}]}, {"text": "Nevertheless, usually it is quite difficult to build a system that has fast training speed and at the same time has high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9987199306488037}]}, {"text": "Typically we need to make a tradeoff between speed and accuracy, to trade training speed for higher accuracy or vice versa.", "labels": [], "entities": [{"text": "speed", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9964205026626587}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9969404935836792}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9981136322021484}]}, {"text": "In this work, we have tried to overcome this problem: to improve the training speed and the model accuracy at the same time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9953954815864563}]}, {"text": "There are two major approaches for parameter training: batch and on-line.", "labels": [], "entities": [{"text": "parameter training", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.743758887052536}]}, {"text": "Standard gradient descent methods are normally batch training methods, in which the gradient computed by using all training instances is used to update the parameters of the model.", "labels": [], "entities": []}, {"text": "The batch training methods include, for example, steepest gradient descent, conjugate gradient descent (CG), and quasi-Newton methods like limited-memory BFGS (Nocedal and Wright 1999).", "labels": [], "entities": []}, {"text": "The true gradient is usually the sum of the gradients from each individual training instance.", "labels": [], "entities": []}, {"text": "Therefore, batch gradient descent requires the training method to go through the entire training set before updating parameters.", "labels": [], "entities": [{"text": "batch gradient descent", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7421841422716776}]}, {"text": "This is why batch training methods are typically slow.", "labels": [], "entities": [{"text": "batch training", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8734343647956848}]}, {"text": "On-line learning methods can significantly accelerate the training speed compared with batch training methods.", "labels": [], "entities": []}, {"text": "A representative on-line training method is the stochastic gradient descent method (SGD) and its extensions (e.g., stochastic meta descent) (Bottou 1998;).", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6789287527402242}, {"text": "stochastic meta descent", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.6121087869008383}]}, {"text": "The model parameters are updated more frequently compared with batch training, and fewer passes are needed before convergence.", "labels": [], "entities": []}, {"text": "For large-scale data sets, on-line training methods can be much faster than batch training methods.", "labels": [], "entities": []}, {"text": "However, we find that the existing on-line training methods are still not good enough for training large-scale NLP systems-probably because those methods are not well-tailored for NLP systems that have massive features.", "labels": [], "entities": []}, {"text": "First, the convergence speed of the existing on-line training methods is not fast enough.", "labels": [], "entities": []}, {"text": "Our studies show that the existing on-line training methods typically require more than 50 training passes before empirical convergence, which is still slow.", "labels": [], "entities": []}, {"text": "For large-scale NLP systems, the training time per pass is typically long and fast convergence speed is crucial.", "labels": [], "entities": []}, {"text": "Second, the accuracy of the existing on-line training methods is not good enough.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995599389076233}]}, {"text": "We want to further improve the training accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9969214797019958}]}, {"text": "We try to deal with the two challenges at the same time.", "labels": [], "entities": []}, {"text": "Our goal is to develop anew training method for faster and at the same time more accurate natural language processing.", "labels": [], "entities": []}, {"text": "In this article, we present anew on-line training method, adaptive on-line gradient descent based on feature frequency information (ADF), for very accurate and fast on-line training of NLP systems.", "labels": [], "entities": []}, {"text": "Other than the high training accuracy and fast training speed, we further expect that the proposed training method has good theoretical properties.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9822114109992981}]}, {"text": "We want to prove that the proposed method is convergent and has a fast convergence rate.", "labels": [], "entities": [{"text": "convergence", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9512025713920593}]}, {"text": "In the proposed ADF training method, we use a learning rate vector in the on-line updating.", "labels": [], "entities": [{"text": "ADF training", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.8357753753662109}]}, {"text": "This learning rate vector is automatically adapted based on feature frequency information in the training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.8017161885897318}]}, {"text": "Each model parameter has its own learning rate adapted on feature frequency information.", "labels": [], "entities": []}, {"text": "This proposal is based on the simple intuition that a feature with higher frequency in the training process should have a learning rate that decays faster.", "labels": [], "entities": []}, {"text": "This is because a higher frequency feature is expected to be well optimized with higher confidence.", "labels": [], "entities": []}, {"text": "Thus, a higher frequency feature is expected to have a lower learning rate.", "labels": [], "entities": []}, {"text": "We systematically formalize this intuition into a theoretically sound training algorithm, ADF.", "labels": [], "entities": []}, {"text": "The main contributions of this work are as follows: r On the methodology side, we propose a general purpose on-line training method, ADF.", "labels": [], "entities": []}, {"text": "The ADF method is significantly more accurate than existing on-line and batch training methods, and has faster training speed.", "labels": [], "entities": [{"text": "ADF", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6889553070068359}]}, {"text": "Moreover, theoretical analysis demonstrates that the ADF method is convergent with a fast convergence rate.", "labels": [], "entities": []}, {"text": "r On the application side, for the three well-known tasks, including named entity recognition, word segmentation, and phrase chunking, the proposed simple method achieves equal or even better accuracy than the existing gold-standard systems, which are complicated and use extra resources.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6156702240308126}, {"text": "word segmentation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.8019399344921112}, {"text": "phrase chunking", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.7779164612293243}, {"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9968265891075134}]}], "datasetContent": [{"text": "Our main focus is on training heavily structured classification models.", "labels": [], "entities": []}, {"text": "We evaluate the proposal on three NLP structured classification tasks: biomedical named entity recognition (Bio-NER), Chinese word segmentation, and noun phrase (NP) chunking.", "labels": [], "entities": [{"text": "NLP structured classification", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6254513065020243}, {"text": "biomedical named entity recognition", "start_pos": 71, "end_pos": 106, "type": "TASK", "confidence": 0.6089030802249908}, {"text": "Chinese word segmentation", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.6509019732475281}, {"text": "noun phrase (NP) chunking", "start_pos": 149, "end_pos": 174, "type": "TASK", "confidence": 0.5838839511076609}]}, {"text": "For the structured classification tasks, the ADF training is based on the CRF model.", "labels": [], "entities": [{"text": "structured classification tasks", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6853992342948914}, {"text": "ADF", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8815036416053772}]}, {"text": "Further, to demonstrate that the proposed method is not limited to structured classification tasks, we also perform experiments on a nonstructured binary classification task: sentiment-based text classification.", "labels": [], "entities": [{"text": "sentiment-based text classification", "start_pos": 175, "end_pos": 210, "type": "TASK", "confidence": 0.6385550002257029}]}, {"text": "For the nonstructured classification task, the ADF training is based on the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996).", "labels": [], "entities": [{"text": "nonstructured classification task", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.7148028016090393}, {"text": "ADF", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9364303946495056}]}, {"text": "As for training, we perform gradient descent with the proposed ADF training method.", "labels": [], "entities": [{"text": "gradient descent", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.6677058190107346}]}, {"text": "To compare with existing literature, we choose four popular training methods, a representative batch training method, and three representative on-line training methods.", "labels": [], "entities": []}, {"text": "The batch training method is the limited-memory BFGS (LBFGS) method, which is considered to be one of the best optimizers for log-linear models like CRFs.", "labels": [], "entities": [{"text": "BFGS", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.8325548768043518}]}, {"text": "The on-line training methods include the SGD training method, which we introduced in Section 2.2, the structured perceptron (Perc) training method, and the averaged perceptron (Avg-Perc) training method).", "labels": [], "entities": [{"text": "SGD training", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.7452725768089294}]}, {"text": "The structured perceptron method and averaged perceptron method are non-probabilistic training methods that have very fast training speed due to the avoidance of the computation on gradients.", "labels": [], "entities": []}, {"text": "All training methods, including ADF, SGD, Perc, Avg-Perc, and LBFGS, use the same set of features.", "labels": [], "entities": [{"text": "ADF", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.7491306662559509}, {"text": "LBFGS", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.7626499533653259}]}, {"text": "We also compared the ADF method with the CW method (Dredze, Crammer, and Pereira 2008) and the AROW method.", "labels": [], "entities": [{"text": "AROW", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.4578278362751007}]}, {"text": "The CW and AROW methods are implemented based on the Confidence Weighted Learning Library.", "labels": [], "entities": [{"text": "AROW", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.6597258448600769}, {"text": "Confidence Weighted Learning Library", "start_pos": 53, "end_pos": 89, "type": "DATASET", "confidence": 0.6417241767048836}]}, {"text": "Because the current implementation of the CW and AROW methods do not utilize rich edge features, we removed the rich edge features in our systems to make more fair comparisons.", "labels": [], "entities": []}, {"text": "That is, we removed rich edge features in the CRF-ADF setting, and this simplified method is denoted as ADF-noRich.", "labels": [], "entities": []}, {"text": "The second-order stochastic gradient descent training methods, including the SMD method () and the PSA method (, are not considered in our experiments because we find those methods are quite slow when running on our data sets with high dimensional features.", "labels": [], "entities": [{"text": "SMD", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9510869979858398}]}, {"text": "We find that the settings of q, \u03b1, and \u03b2 in the ADF training method are not sensitive among specific tasks and can be generally set.", "labels": [], "entities": []}, {"text": "We simply set q = n/10 (n is the number of training samples).", "labels": [], "entities": []}, {"text": "It means that feature frequency information is updated 10 times per iteration.", "labels": [], "entities": []}, {"text": "Via cross-validation only on the training data of different tasks, we find that the following setting is sufficient to produce adequate performance for most of the real-world natural language processing tasks: \u03b1 around 0.995 and \u03b2 around 0.6.", "labels": [], "entities": []}, {"text": "This indicates that the feature frequency information has similar characteristics across many different natural language processing tasks.", "labels": [], "entities": []}, {"text": "Thus, we simply use the following setting for all tasks: q = n/10, \u03b1 = 0.995, and \u03b2 = 0.6.", "labels": [], "entities": []}, {"text": "This leaves c (the initial value of the learning rates) as the only hyper-parameter that requires careful tuning.", "labels": [], "entities": []}, {"text": "We perform automatic tuning for c based on the training data via 4-fold cross-validation, testing with c = 0.005, 0.01, 0.05, 0.1, respectively, and the optimal c is chosen based on the best accuracy of cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9958561062812805}]}, {"text": "Via this automatic tuning, we find it is proper to set c = 0.005, 0.1, 0.05, 0.005, for the Bio-NER, word segmentation, phrase chunking, and sentiment classification tasks, respectively.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7504089772701263}, {"text": "phrase chunking", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7753170132637024}, {"text": "sentiment classification", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.9487927854061127}]}, {"text": "To reduce overfitting, we use an L 2 Gaussian weight prior (Chen and Rosenfeld 1999) for the ADF, LBFGS, and SGD training methods.", "labels": [], "entities": [{"text": "ADF", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.861568808555603}]}, {"text": "We vary the \u03c3 with different values (e.g., 1.0, 2.0, and 5.0) for 4-fold cross validation on the training data of different tasks, and finally set \u03c3 = 5.0 for all training methods in the Bio-NER task; \u03c3 = 5.0 for all training methods in the word segmentation task; \u03c3 = 5.0, 1.0, 1.0 for ADF, SGD, and LBFGS in the phrase chunking task; and \u03c3 = 1.0 for all training methods in the sentiment classification task.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 241, "end_pos": 263, "type": "TASK", "confidence": 0.8282233675320944}, {"text": "phrase chunking task", "start_pos": 314, "end_pos": 334, "type": "TASK", "confidence": 0.8407590389251709}, {"text": "sentiment classification task", "start_pos": 380, "end_pos": 409, "type": "TASK", "confidence": 0.8962088425954183}]}, {"text": "Experiments are performed on a computer with an Intel(R) Xeon(R) 2.0-GHz CPU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Summary of the Bio-NER data set.", "labels": [], "entities": [{"text": "Bio-NER data set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8699014584223429}]}, {"text": " Table 2  Feature templates used for the Bio-NER task. w i is the current word token on position i. t i is the  POS tag on position i. o i is the orthography mode on position i. y i is the classification label on  position i. y i\u22121 y i represents label transition. A \u00d7 B represents a Cartesian product between  two sets.", "labels": [], "entities": [{"text": "Bio-NER task", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.8828772902488708}]}, {"text": " Table 4  Results for the Bio-NER, word segmentation, and phrase chunking tasks. The results and the  number of passes are decided based on empirical convergence (with score deviation of adjacent  five passes less than 0.01). For the non-convergent case, we simply report the results based on a  large enough number of training passes. As we can see, the ADF method achieves the best  accuracy with the fastest convergence speed.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8154468834400177}, {"text": "phrase chunking tasks", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.8197442094484965}, {"text": "accuracy", "start_pos": 385, "end_pos": 393, "type": "METRIC", "confidence": 0.9988306164741516}]}, {"text": " Table 5  Comparing our results with some representative state-of-the-art systems.", "labels": [], "entities": []}, {"text": " Table 6  Results on sentiment classification (non-structured binary classification).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.9553151428699493}, {"text": "non-structured binary classification", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.7428992390632629}]}, {"text": " Table 6. As we can see, the proposed method outperforms all of the on-line  and batch baselines in terms of binary classification accuracy. Here again we observe  that the ADF and SGD methods outperform the LBFGS baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9324898719787598}, {"text": "LBFGS baseline", "start_pos": 208, "end_pos": 222, "type": "DATASET", "confidence": 0.8576671183109283}]}]}