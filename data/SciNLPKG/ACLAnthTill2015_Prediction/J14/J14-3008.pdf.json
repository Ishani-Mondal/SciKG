{"title": [{"text": "Pushdown Automata in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6831706364949545}]}], "abstractContent": [{"text": "This article describes the use of pushdown automata (PDA) in the context of statistical machine translation and alignment under asynchronous context-free grammar.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.679866760969162}]}, {"text": "We use PDAs to compactly represent the space of candidate translations generated by the grammar when applied to an input sentence.", "labels": [], "entities": []}, {"text": "General-purpose PDA algorithms for replacement, composition, shortest path, and expansion are presented.", "labels": [], "entities": []}, {"text": "We describe HiPDT, a hierarchical phrase-based decoder using the PDA representation and these algorithms.", "labels": [], "entities": [{"text": "HiPDT", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9028693437576294}]}, {"text": "We contrast the complexity of this decoder with a de-coder based on a finite state automata representation, showing that PDAs provide a more suitable framework to achieve exact decoding for larger synchronous context-free grammars and smaller language models.", "labels": [], "entities": []}, {"text": "We assess this experimentally on a large-scale Chinese-to-English alignment and translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.8417727649211884}]}, {"text": "In translation, we propose a two-pass decoding strategy involving a weaker language model in the first-pass to address the results of PDA complexity analysis.", "labels": [], "entities": [{"text": "PDA complexity analysis", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.7909642259279887}]}, {"text": "We study in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art performance for large-scale SMT.", "labels": [], "entities": [{"text": "HiPDT", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8433209657669067}, {"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9507978558540344}]}], "introductionContent": [{"text": "Synchronous context-free grammars (SCFGs) are now widely used in statistical machine translation, with Hiero as the preeminent example.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.7127471069494883}]}, {"text": "Given an SCFG and an n-gram language model, the challenge is to decode with them, that is, to apply them to source text to generate a target translation.", "labels": [], "entities": []}, {"text": "Decoding is complex in practice, but it can be described simply and exactly in terms of the formal languages and relations involved.", "labels": [], "entities": []}, {"text": "We will use this description to introduce and analyze pushdown automata (PDAs) for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7841574549674988}]}, {"text": "This formal description will allow close comparison of PDAs to existing decoders which are based on other forms of automata.", "labels": [], "entities": []}, {"text": "Decoding can be described in terms of the following steps: 1.", "labels": [], "entities": []}, {"text": "Translation: T = \u03a0 2 ({s}\u2022G) The first step is to compose the finite language {s}, which represents the source sentence to be translated, with the algebraic relation G for the translation grammar G.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.945039689540863}]}, {"text": "The result of this composition projected on the output side is T , a weighted context-free grammar that contains all possible translations of sunder G.", "labels": [], "entities": []}, {"text": "Following the usual definition of Hiero grammars, we assume that G does not allow unbounded insertions so that T is a regular language.", "labels": [], "entities": [{"text": "Hiero grammars", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.6180281043052673}]}], "datasetContent": [{"text": "We now address the following questions: r What are the differences between the FSA and PDA representations as observed in a translation/alignment task?", "labels": [], "entities": [{"text": "FSA", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.46931928396224976}, {"text": "translation/alignment task", "start_pos": 124, "end_pos": 150, "type": "TASK", "confidence": 0.9180397242307663}]}, {"text": "r How do their respective decoding algorithms perform in relation to the complexity analysis described here?", "labels": [], "entities": []}, {"text": "r How many times is exact decoding achievable in each case?", "labels": [], "entities": []}, {"text": "We will discuss the complexity of both HiPDT and HiFST decoders as well as the hypergraph representation, with an emphasis on Hiero-style SCFGs.", "labels": [], "entities": [{"text": "HiPDT", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9356763362884521}, {"text": "HiFST", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7629574537277222}, {"text": "Hiero-style SCFGs", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.8140784204006195}]}, {"text": "We assess our analysis for FSA and PDA representations by contrasting HiFST and HiPDT with large grammars for translation and alignment.", "labels": [], "entities": [{"text": "FSA", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8341119289398193}, {"text": "HiFST", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8985459804534912}, {"text": "HiPDT", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9276981949806213}, {"text": "translation and alignment", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.6452330946922302}]}, {"text": "For convenience, we refer to the hypergraph representation as Th , and to the FSA and PDA representations as T f and T p . We first analyze the complexity of each MT step described in the introduction: In practice, the PDA and FSA representations benefit greatly from the optimizations mentioned previously.", "labels": [], "entities": [{"text": "FSA", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.6204759478569031}, {"text": "MT step", "start_pos": 163, "end_pos": 170, "type": "TASK", "confidence": 0.9082474410533905}]}, {"text": "For the FSA representation, these operations can offset the exponential dependencies in the worstcase complexity analysis.", "labels": [], "entities": [{"text": "FSA", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.5164729952812195}]}, {"text": "For example, in a translation of a 15-word sentence taken at random from the development sets described later, expansion of an RTN yields a WFSA with 174 \u00d7 10 6 states.", "labels": [], "entities": []}, {"text": "By contrast, if the RTN is determinized and minimized prior to expansion, the resulting WFSA has only 34 \u00d7 10 3 states.", "labels": [], "entities": []}, {"text": "Size reductions of this magnitude are typical.", "labels": [], "entities": []}, {"text": "In general, the original RTN, hypergraph, or CFG representation can be exponentially larger than the RTN/PDT optimized as described.", "labels": [], "entities": []}, {"text": "Although our interest is primarily in Hiero-style translation grammars, which have rank 2 and a relatively small number of nonterminals, this complexity analysis can be extended to other grammars.", "labels": [], "entities": [{"text": "Hiero-style translation grammars", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7753336032231649}]}, {"text": "For SCFGs of arbitrary rank l N , translation complexity in time for hypergraphs becomes O(|G||s| l N +1 |M| l N +1 ); with FSAs the time complexity becomes O(e |G||s| l N +1 |M|); and with PDAs the time complexity becomes O(|G||s| l N +1 |M| 3 ).", "labels": [], "entities": [{"text": "FSAs", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.7095394134521484}]}, {"text": "For more complex SCFGs with rules of rank greater than 2, such as SAMT () or GHKM (), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization () or scope pruning.", "labels": [], "entities": [{"text": "GHKM", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.7852849960327148}]}, {"text": "Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here.", "labels": [], "entities": []}, {"text": "We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this article (except when stated otherwise).", "labels": [], "entities": [{"text": "translation grammar", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.9497752785682678}]}, {"text": "In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.8067217469215393}, {"text": "HiPDT", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.8478220105171204}]}, {"text": "The previous complexity analysis suggests that PDAs should excel when used with large translation grammars and relatively small n-gram language models.", "labels": [], "entities": []}, {"text": "In hierarchical phrase-based translation, this is a somewhat unusual scenario: It is far more typical that translation tasks requiring a large translation grammar also require large language models.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.6141975025335947}]}, {"text": "To accommodate these requirements we have developed a twopass decoding strategy in which a weak version of a large language model is applied prior to the expansion of the PDA, after which the full language model is applied to the resulting WFSA in a rescoring pass.", "labels": [], "entities": [{"text": "WFSA", "start_pos": 240, "end_pos": 244, "type": "DATASET", "confidence": 0.8447445034980774}]}, {"text": "An effective way of generating weak language models is by means of entropy pruning under a threshold \u03b8; these are the language models M \u03b8 1 of Section 4.1.", "labels": [], "entities": []}, {"text": "Such a two-pass strategy is widely used in automatic speech recognition).", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.5950745145479838}]}, {"text": "The steps in two-pass translation using entropy-pruned language models are given here, and depicted in.", "labels": [], "entities": []}, {"text": "We translate with M \u03b8 1 and G using the same parameters obtained by MERT for the baseline system, with the exception that the word penalty parameter is adjusted to produce hypotheses of roughly the correct length.", "labels": [], "entities": [{"text": "M \u03b8 1", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.8014516433080038}, {"text": "MERT", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.4749907851219177}]}, {"text": "This produces translation lattices that contain hypotheses with exact scores under G and M \u03b8 1 : Step 2.", "labels": [], "entities": []}, {"text": "These translation lattices are pruned at beamwidth \u03b2: Step 3.", "labels": [], "entities": []}, {"text": "We remove the M \u03b8 1 scores from the pruned translation lattices, reapply the full language model M 1 , and restore the word penalty parameter to the baseline value obtained by MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 176, "end_pos": 180, "type": "DATASET", "confidence": 0.6488044261932373}]}, {"text": "This gives an approximation to \u03a0 2 ({s} \u2022 G) \u2022 M 1 : scores are correctly assigned under G and M 1 , but only hypotheses that survived pruning at Step 2 are included.", "labels": [], "entities": []}, {"text": "We can rescore the lattices produced by the baseline system or by the two-pass system with the larger language model M 2 . If \u03b2 = \u221e or if \u03b8 = 0, the translation lattices obtained in Step 3 should be identical to lattices produced by the baseline system (i.e., the rescoring step is no longer needed).", "labels": [], "entities": []}, {"text": "The aim is to increase \u03b8 to shrink the language model used at Step 1, but \u03b2 will then have to increase accordingly to avoid pruning away desirable hypotheses in Step 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Number of n-grams with explicit conditional probability estimates assigned by the 4-gram  language models M \u03b8  1 after entropy pruning of M 1 at threshold values \u03b8. Perplexities over the  (concatenated) tune-nw reference translations are also reported. The Kneser-Ney and Katz  4-gram LM have 416,190 unigrams, which are not removed by pruning.", "labels": [], "entities": []}, {"text": " Table 3  Success in finding the 1-best translation under G with various M \u03b8  1 under a memory size limit of  10GB as measured over tune-nw (1,755 sentences). We note which operations in translation  exceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection and  Shortest Path operation for HiPDT.", "labels": [], "entities": [{"text": "Expansion", "start_pos": 234, "end_pos": 243, "type": "METRIC", "confidence": 0.9765884876251221}, {"text": "HiFST", "start_pos": 265, "end_pos": 270, "type": "DATASET", "confidence": 0.8807064890861511}, {"text": "HiPDT", "start_pos": 321, "end_pos": 326, "type": "DATASET", "confidence": 0.9176824688911438}]}, {"text": " Table 4  Percentages of success and failure in aligning 2,500 sentence pairs under G ITG with HiFST and  HiPDT. HiPDT finds an alignment whenever it is possible under the translation grammar.", "labels": [], "entities": [{"text": "G ITG", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.8370009362697601}, {"text": "HiFST", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9208555221557617}, {"text": "HiPDT", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.9492064118385315}]}, {"text": " Table 5  Two-pass translation modeling errors as a function of RTN expansion pruning threshold \u03b2. A  modeling error occurs whenever the score of a hypothesis produced by the two-pass translation  differs from the score found by the exact baseline system. Errors are tabulated over systems  reported in", "labels": [], "entities": [{"text": "Two-pass translation modeling", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6723180810610453}]}]}