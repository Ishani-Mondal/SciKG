{"title": [{"text": "Automatic Selection of HPSG-Parsed Sentences for Treebank Construction", "labels": [], "entities": []}], "abstractContent": [{"text": "This article presents an ensemble parse approach to detecting and selecting high-quality linguistic analyses output by a hand-crafted HPSG grammar of Spanish implemented in the LKB system.", "labels": [], "entities": [{"text": "ensemble parse", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7133228778839111}, {"text": "LKB system", "start_pos": 177, "end_pos": 187, "type": "DATASET", "confidence": 0.9244583547115326}]}, {"text": "The approach uses full agreement (i.e., exact syntactic match) along with a MaxEnt parse selection model and a statistical dependency parser trained on the same data.", "labels": [], "entities": []}, {"text": "The ultimate goal is to develop a hybrid corpus annotation methodology that combines fully automatic annotation and manual parse selection, in order to make the annotation task more efficient while maintaining high accuracy and the high degree of consistency necessary for any foreseen uses of a treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9965746402740479}, {"text": "consistency", "start_pos": 247, "end_pos": 258, "type": "METRIC", "confidence": 0.9668676853179932}]}], "introductionContent": [{"text": "Treebanks constitute a crucial resource for theoretical linguistic investigations as well as for NLP applications.", "labels": [], "entities": []}, {"text": "Thus, in the past decades, there has been increasing interest in their construction and both theory-neutral and theory-grounded treebanks have been developed fora great variety of languages.", "labels": [], "entities": []}, {"text": "Descriptions of available annotated corpora can be found in and in the proceedings from the annual editions of the International Workshop on Treebanks and Linguistic Theories.", "labels": [], "entities": [{"text": "International Workshop on Treebanks and Linguistic Theories", "start_pos": 115, "end_pos": 174, "type": "TASK", "confidence": 0.6758120059967041}]}, {"text": "Quantity and quality are two very important objectives when building a treebank, but speed and low labor costs are also required.", "labels": [], "entities": [{"text": "Quantity", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9733702540397644}]}, {"text": "In addition, guaranteeing consistency, that is, that the same phenomena receive the same annotation through the corpus, is crucial for any of the possible uses of the treebank.", "labels": [], "entities": [{"text": "consistency", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9636083841323853}]}, {"text": "The first attempts at treebank projects used manual annotation mainly and devoted many hours of human labor to their construction.", "labels": [], "entities": []}, {"text": "Human annotation is not only slow and expensive, but it also introduces errors and inconsistencies because of the difficulty and tiring nature of the task.", "labels": [], "entities": [{"text": "Human annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6939325332641602}]}, {"text": "1 Therefore, automating parts of the annotation process aims to leverage effectiveness, producing a larger number of high-quality and consistent analyses in shorter time and using fewer resources.", "labels": [], "entities": []}, {"text": "This article presents research that attempts to increase the degree of automation in the annotation process when constructing a large treebank for Spanish (the IULA Spanish LSP Treebank) in the framework of the European project METANET4U (Enhancing the European Linguistic Infrastructure, GA 270893GA).", "labels": [], "entities": [{"text": "IULA Spanish LSP Treebank)", "start_pos": 160, "end_pos": 186, "type": "DATASET", "confidence": 0.8573390960693359}]}, {"text": "The treebank was developed using the following bootstrapping approach, details of which are presented in Sections 3 and 4: r First, we annotated the sentences using the DELPH-IN development framework, in which the annotation process is effected by manually selecting the correct parses from among all the analyses produced by a hand-built symbolic grammar.", "labels": [], "entities": []}, {"text": "r Second, when a number of human-validated parsed sentences were available, we trained a MaxEnt ranker.", "labels": [], "entities": []}, {"text": "r Third, we trained a dependency parser with the human-validated parsed sentences converted to the CoNLL format.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7819892168045044}, {"text": "CoNLL format", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8636151254177094}]}, {"text": "r Fourth, we provided a fully automated chain based on an ensemble method that compared the parse delivered by the dependency parser and the one delivered by the MaxEnt ranker, and then accepted the automatically proposed analysis, but only if both were identical.", "labels": [], "entities": []}, {"text": "r Fifth, sentences rejected by the ensemble were given to human annotators for manual disambiguation.", "labels": [], "entities": []}, {"text": "Obviously, using fully automatic parsing would have been the best solution for speed and consistency, but no statistical parsers for Spanish are good enough yet, and when using symbolic parsers, there is noway to separate good parses from incorrect ones.", "labels": [], "entities": [{"text": "speed", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9837896227836609}, {"text": "consistency", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.978851318359375}]}, {"text": "The ensemble method we propose is away of avoiding monitoring automatic parsing; the error is more than acceptable and recall is expected to be augmented by re-training and the refinement of the different parses.", "labels": [], "entities": [{"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.998960018157959}]}, {"text": "After this introduction, Section 2 presents an overview of related work on automatic parse selection, Section 3 summarizes the set-up, Section 4 presents our experiments and results and, finally, Section 5 concludes.", "labels": [], "entities": [{"text": "automatic parse selection", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7094888289769491}]}], "datasetContent": [{"text": "In our experiments, we tested the ability of the ensemble approach to select only correct parses.", "labels": [], "entities": []}, {"text": "The experiment proceeded as follows: r We divided a set of 15,329 sentences into a training and test set (13,901 and 1,428 sentences, respectively).", "labels": [], "entities": []}, {"text": "Sentence length ranged from 4 to 20 words (longer sentences had not been annotated yet).", "labels": [], "entities": [{"text": "Sentence length", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6902934312820435}]}, {"text": "r We trained the MaxEnt model and MaltParser and ran each of the models on the test set.", "labels": [], "entities": [{"text": "MaxEnt model", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9329865276813507}]}, {"text": "The results we achieved are displayed in. r We compared the outputs of the two models and selected those sentences where both parses produced identical analyses.", "labels": [], "entities": []}, {"text": "The performance of our parser ensemble approach was measured through precision and recall on the task of selecting those sentences for which the first tree proposed by the MaxEnt model was the correct one.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9984812140464783}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9990959167480469}]}, {"text": "shows the confusion matrix resulting from the experiment.", "labels": [], "entities": []}, {"text": "The row predicted ok counts the number of sentences selected by our ensemble method (Malt and MaxEnt delivered parses are identical), and the row predicted nok contains the number of sentences not selected because the parsers disagreed.", "labels": [], "entities": []}, {"text": "Columns gold present the manual evaluation of a MaxEnt model first ranked parse.", "labels": [], "entities": []}, {"text": "From this table, we can compute precision and recall of our sentence selector: 445 sentences were selected out of the 1,428 sentences in the test set (31.2%).", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996053576469421}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9995129108428955}]}, {"text": "Precision (number of correctly selected sentences among all the selected sentences) stood at 90.6% (403/445), and recall (number of correctly selected sentences among all the actually correctly ranked first sentences) was 46.6% (403/864).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9939788579940796}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9996080994606018}]}, {"text": "We compared the results of our ensemble method with two parse selection methods based on: (i) a simple probability-based threshold (baseline) and (ii) a parser uncertainty measure computed as tree entropy as used by.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.888758659362793}]}, {"text": "The baseline consisted of selecting sentences for which the ratio between the probabilities of the two highest ranked analyses delivered by the MaxEnt model was over a given threshold.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 144, "end_pos": 150, "type": "DATASET", "confidence": 0.8713276982307434}]}, {"text": "The idea was that a very high ratio would indicate that the parse ranked first had a large advantage over the others, whereas if the ratio was close to 1, both the first and the second analyses would have similar probabilities, indicating lower confidence of the model in the decision.", "labels": [], "entities": []}, {"text": "Tree entropy takes into account not just the two highest ranked analyses, but all trees proposed by the parser for that sentence.", "labels": [], "entities": []}, {"text": "The rationale is that high entropy indicates a scattered probability distribution among possible trees (and thus less certainty of the model in the prediction), whereas low entropy should indicate that one tree (or a few) gets most of the probability mass.", "labels": [], "entities": [{"text": "certainty", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9914262294769287}]}, {"text": "Results for different thresholds (both for the baseline and tree entropy) are shown in (top).", "labels": [], "entities": []}, {"text": "As we can see, setting a high threshold for the baseline, we can select a small subset of 20% of the sentences with precision similar to that achieved by our parse ensemble approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9990319013595581}]}, {"text": "To select 31% of the sentences (i.e., about the same proportion we obtained with the ensemble approach) we need to set a threshold of 4.5, obtaining a precision of 84%, which is lower than the 90% obtained with the ensemble method.", "labels": [], "entities": [{"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.998219907283783}]}, {"text": "Tree entropy exhibits similar behavior, in that a restrictive threshold can select about 15% of sentences with precision over 90%, while setting a threshold such that about 31% of sentences are selected, we obtain precision of about 75%.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9977778792381287}, {"text": "precision", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9987782835960388}]}, {"text": "Note that although the baseline has an F 1 score slightly higher than the ensemble, our goal is a high precision filter that can be utilized to select correctly parsed sentences.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9698852698008219}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9641971588134766}]}, {"text": "From this point of view, our approach beats both baselines.", "labels": [], "entities": []}, {"text": "The fact that tree entropy yields worse values than the baseline is somehow predictable: Given a sentence with n possible trees (note that n maybe in the order of dozens or even hundreds), if a small number m of those analyses (1 < m << n) concentrate a large portion of probability mass but exhibit small differences between them, the sentence will be rejected by the baseline (because there is not enough distance between the first and second analyses) but will be accepted by tree entropy (because entropy will be relatively low, given the large value of n).", "labels": [], "entities": []}, {"text": "Thus, tree entropy is a good measure for, whose purpose is to select sentences where the model is less confident, but our simple baseline seems to be better when the goal is to select sentences where the first parse is the correct one.", "labels": [], "entities": []}, {"text": "As shown in (bottom), behavior is different for sentences of up to 10 words than for longer sentences.", "labels": [], "entities": []}, {"text": "All three systems have a bias towards selecting short rather than long sentences (because short sentences are more often correctly analyzed by the parser).", "labels": [], "entities": []}, {"text": "The results for short sentences are similar in all three cases, but the ensemble approach is clearly more precise for long sentences, with only a moderate loss in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9968612194061279}]}], "tableCaptions": [{"text": " Table 1  Results of the MaxEnt model and MaltParser as labeled attachment scores, unlabeled  attachment scores, labeled accuracy score, and exact syntactic match.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 121, "end_pos": 135, "type": "METRIC", "confidence": 0.9157513380050659}, {"text": "exact", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9569758176803589}]}, {"text": " Table 2  Confusion matrix used to assess the results in terms of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9996918439865112}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9995402097702026}]}, {"text": " Table 3  Top: Comparative results using different threshold values for the baselines. Bottom: Results per  sentence length when selecting about 31% over all sentences. Thr = threshold; %sel = percentage  of selected sentences; P = precision; R = recall; Len = sentence length.", "labels": [], "entities": [{"text": "Thr", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9982239603996277}, {"text": "precision", "start_pos": 232, "end_pos": 241, "type": "METRIC", "confidence": 0.9093707799911499}, {"text": "R", "start_pos": 243, "end_pos": 244, "type": "METRIC", "confidence": 0.9181059002876282}, {"text": "recall", "start_pos": 247, "end_pos": 253, "type": "METRIC", "confidence": 0.8783705234527588}, {"text": "Len", "start_pos": 255, "end_pos": 258, "type": "METRIC", "confidence": 0.9661377668380737}]}]}