{"title": [], "abstractContent": [{"text": "We describe a structure learning system for unrestricted coreference resolution that explores two key modeling techniques: latent coreference trees and automatic entropy-guided feature induction.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8471588790416718}]}, {"text": "The latent tree modeling makes the learning problem computationally feasible because it incorporates a meaningful hidden structure.", "labels": [], "entities": []}, {"text": "Additionally, using an automatic feature induction method, we can efficiently build enhanced nonlinear models using linear model learning algorithms.", "labels": [], "entities": []}, {"text": "We present empirical results that highlight the contribution of each modeling technique used in the proposed system.", "labels": [], "entities": []}, {"text": "Empirical evaluation is performed on the multilingual unrestricted coreference CoNLL-2012 Shared Task data sets, which comprise three languages: Arabic, Chinese, and English.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task data sets", "start_pos": 79, "end_pos": 111, "type": "DATASET", "confidence": 0.736487889289856}]}, {"text": "We apply the same system to all languages, except for minor adaptations to some language-dependent features such as nested mentions and specific static pronoun lists.", "labels": [], "entities": []}, {"text": "A previous version of this system was submitted to the CoNLL-2012 Shared Task closed track, achieving an official score of 58.69, the best among the competitors.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task closed track", "start_pos": 55, "end_pos": 90, "type": "DATASET", "confidence": 0.9023444056510925}]}, {"text": "The unique enhancement added to the current system version is the inclusion of candidate arcs linking nested mentions for the Chinese language.", "labels": [], "entities": []}, {"text": "By including such arcs, the score increases by almost 4.5 points for that language.", "labels": [], "entities": []}, {"text": "The current system shows a score of 60.15, which corresponds to a 3.5% error reduction, and is the best performing system for each of the three languages.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9862382411956787}]}], "introductionContent": [{"text": "Mentions are textual references to real-world entities or events.", "labels": [], "entities": []}, {"text": "Ina given document, mentions that refer to the same entity are called coreferring mentions and form a mention cluster.", "labels": [], "entities": []}, {"text": "Coreference resolution is the task of identifying the mention clusters in a document and has been a core research topic in natural language processing.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.910422682762146}]}, {"text": "It has wide applications in question answering, machine translation, automatic summarization, and information extraction.", "labels": [], "entities": [{"text": "question answering", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9221426546573639}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8396008312702179}, {"text": "summarization", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.6882849335670471}, {"text": "information extraction", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8971482217311859}]}, {"text": "Coreference resolution systems have been evaluated for several decades, beginning with MUC-6 (.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8866681456565857}, {"text": "MUC-6", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.794597864151001}]}, {"text": "Following those evaluation efforts, the) has been dedicated to the modeling of unrestricted coreference resolution for English text.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.874406486749649}]}, {"text": "The) extends the task to a multilingual scope, considering three languages: Arabic, Chinese, and English.", "labels": [], "entities": []}, {"text": "A singleton is a mention cluster containing exactly one mention.", "labels": [], "entities": []}, {"text": "The unrestricted coreference resolution task consists of identifying the non-singleton mention clusters in a document.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8791899085044861}]}, {"text": "This task is usually split into three subtasks: mention detection, mention clustering, and singleton elimination.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7874150574207306}, {"text": "mention clustering", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7163410186767578}, {"text": "singleton elimination", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.8225110471248627}]}, {"text": "In, we present an illustrative example.", "labels": [], "entities": []}, {"text": "First, ten mentions are detected and shown in bold.", "labels": [], "entities": []}, {"text": "They are sequentially tagged with the numbers 1, 2, . .", "labels": [], "entities": []}, {"text": "Next, four mention clusters are identified by tagging each mention with one of the tags a, b, c, d to indicate its cluster, where a = {1, 2, 8, 9}, b = {3, 7}, c = {4, 5, 6}, d = {10} are the four mention clusters.", "labels": [], "entities": []}, {"text": "Finally, clusters that contain only one mention are ignored, such as the one with Iran as its unique mention.", "labels": [], "entities": []}, {"text": "In this example, we ignore some noun phrases in the mention detection subtask to simplify the illustration.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6932117789983749}]}, {"text": "The final subtask is trivial, given the solution of the previous one.", "labels": [], "entities": []}, {"text": "For the first subtask, several specific heuristics have been proposed, enabling the construction of high-recall mention detectors.", "labels": [], "entities": []}, {"text": "The second subtask is harder, as it requires a complex output.", "labels": [], "entities": []}, {"text": "Most of the current effort toward solving the coreference resolution task is focused on the mention clustering subtask.", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.9466497898101807}, {"text": "mention clustering subtask", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.7635678350925446}]}, {"text": "Here, we propose an approach to unrestricted coreference resolution that is based on two key modeling techniques: latent coreference trees and entropy-guided feature induction.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8640926480293274}, {"text": "entropy-guided feature induction", "start_pos": 143, "end_pos": 175, "type": "TASK", "confidence": 0.619888037443161}]}, {"text": "Our approach is based on a graph whose nodes are the mentions in the given document.", "labels": [], "entities": []}, {"text": "The arcs of this graph link mention pairs that are coreferent candidates.", "labels": [], "entities": []}, {"text": "The resulting structure predictor has similar steps for training and testing.", "labels": [], "entities": []}, {"text": "Predictor training can be summarized as follows:", "labels": [], "entities": [{"text": "Predictor", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.845429003238678}]}], "datasetContent": [{"text": "We evaluate our system using the data sets from the CoNLL-2012 Shared Task.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.8588411609331766}]}, {"text": "In this section, we first briefly describe these data sets.", "labels": [], "entities": []}, {"text": "Next, we detail the system settings adopted in the experiments for different steps of our system, including candidate pair generation, basic feature setting, and entropy-guided feature induction.", "labels": [], "entities": [{"text": "candidate pair generation", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.6254953245321909}, {"text": "entropy-guided feature induction", "start_pos": 162, "end_pos": 194, "type": "TASK", "confidence": 0.6216038763523102}]}, {"text": "We also describe the minor adaptations that are necessary for each language.", "labels": [], "entities": []}, {"text": "Finally, we present the evaluation metrics used to assess our system.", "labels": [], "entities": []}, {"text": "Evaluating coreference systems is a difficult task.", "labels": [], "entities": [{"text": "Evaluating coreference", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7959120571613312}]}, {"text": "The main issue is that coreference information is highly faceted, and the value of each facet varies substantially from one application to another.", "labels": [], "entities": [{"text": "coreference information", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8895174562931061}]}, {"text": "Thus, when reporting and comparing coreference system performances, it is very difficult to define one metric that fits all purposes.", "labels": [], "entities": []}, {"text": "Therefore, we follow the methodology proposed in the CoNLL-2012 Shared Task to assess our systems, as it combines three of the most popular metrics.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.8224519689877828}]}, {"text": "The metrics used are the linkbased MUC metric (, the mention-based B 3 metric (, and the entity-based CEAF e metric).", "labels": [], "entities": [{"text": "MUC metric", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.6982466578483582}, {"text": "mention-based B 3 metric", "start_pos": 53, "end_pos": 77, "type": "METRIC", "confidence": 0.7936434298753738}, {"text": "CEAF e metric", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.6964483857154846}]}, {"text": "All these metrics are based on precision and recall measures, which are combined to produce an F-score value.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9994462132453918}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9975388050079346}, {"text": "F-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9986270666122437}]}, {"text": "The mean F-score of these three metrics gives a unique score for each language.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9942915439605713}]}, {"text": "Additionally, when appropriate, the official CoNLL-2012 Shared Task score is reported, which is the average of the F-scores for all languages.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task score", "start_pos": 45, "end_pos": 73, "type": "DATASET", "confidence": 0.7857126295566559}, {"text": "F-scores", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9736881852149963}]}, {"text": "We denote this metric as the CoNLL score.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.6778075993061066}]}, {"text": "Another important aspect of coreference evaluation is mention matching.", "labels": [], "entities": [{"text": "coreference evaluation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9686839580535889}, {"text": "mention matching", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.6958837807178497}]}, {"text": "Some methodologies, such as the ones used in the MUC and ACE evaluations, consider approximate matching of mention spans.", "labels": [], "entities": [{"text": "MUC", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.5216019153594971}]}, {"text": "However, the CoNLL-2012 Shared Task evaluation considers only exact span matching.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task evaluation", "start_pos": 13, "end_pos": 46, "type": "DATASET", "confidence": 0.7779881209135056}]}, {"text": "We use the CoNLL-2012 Shared Task evaluation scripts version 4, which is the same version used to produce the official ranking in this shared task.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task evaluation scripts", "start_pos": 11, "end_pos": 52, "type": "DATASET", "confidence": 0.8534035325050354}]}, {"text": "We use this version to keep our results comparable with most of the results reported in the literature.", "labels": [], "entities": []}, {"text": "It is important to note that, in early 2014, a committee of researchers revised some of the evaluation metrics and released anew version of these scripts.", "labels": [], "entities": []}, {"text": "Although this revision changes the absolute scores, the ranking of the top performing systems, including ours, remain the same.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Arc data set with some arcs from the document in", "labels": [], "entities": []}, {"text": " Table 2  Description of all 70 basic features.", "labels": [], "entities": []}, {"text": " Table 3  State-of-the-art systems for multilingual unrestricted coreference resolution in the CoNLL-2012  Shared Task data sets. Performances on the test sets for Arabic (AR), Chinese (CH), English (EN),  and the official shared task score.", "labels": [], "entities": [{"text": "multilingual unrestricted coreference resolution", "start_pos": 39, "end_pos": 87, "type": "TASK", "confidence": 0.5733772218227386}, {"text": "CoNLL-2012  Shared Task data sets", "start_pos": 95, "end_pos": 128, "type": "DATASET", "confidence": 0.9063074588775635}]}, {"text": " Table 4  Detailed performance of state-of-the-art systems on the Arabic CoNLL-2012 Shared Task test set.", "labels": [], "entities": [{"text": "Arabic CoNLL-2012 Shared Task test set", "start_pos": 66, "end_pos": 104, "type": "DATASET", "confidence": 0.9454395572344462}]}, {"text": " Table 5  Detailed performance of state-of-the-art systems on the Chinese CoNLL-2012 Shared Task test set.", "labels": [], "entities": [{"text": "Chinese CoNLL-2012 Shared Task test set", "start_pos": 66, "end_pos": 105, "type": "DATASET", "confidence": 0.9296920299530029}]}, {"text": " Table 6  Detailed performance of state-of-the-art systems on the English CoNLL-2012 Shared Task test set.", "labels": [], "entities": [{"text": "English CoNLL-2012 Shared Task test set", "start_pos": 66, "end_pos": 105, "type": "DATASET", "confidence": 0.9419235388437907}]}, {"text": " Table 7  Mention detection performances on the development sets before and after mention clustering.", "labels": [], "entities": []}, {"text": " Table 8  Performances of the Arabic sieves on the development set.", "labels": [], "entities": []}, {"text": " Table 9  Performances of the Chinese sieves on the development set.", "labels": [], "entities": []}, {"text": " Table 11  Impact of EFI on development sets for all languages using all 70 basic features.", "labels": [], "entities": []}, {"text": " Table 13  Latent trees' effect on development set performances.", "labels": [], "entities": []}, {"text": " Table 14  Root loss value effect on development set performances.", "labels": [], "entities": []}, {"text": " Table 15  Impact of nested mentions on the system performance for the Chinese development set.", "labels": [], "entities": [{"text": "Chinese development set", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.809153417746226}]}, {"text": " Table 17  Most frequent errors whenever an incorrect arc (i, j) is predicted instead of the correct arc (i  *  , j).", "labels": [], "entities": [{"text": "errors", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9655611515045166}]}, {"text": " Table 18  Most frequent singleton errors per head POS tag and language.", "labels": [], "entities": []}]}