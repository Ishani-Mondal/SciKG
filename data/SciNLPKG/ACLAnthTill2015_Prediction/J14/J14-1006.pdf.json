{"title": [], "abstractContent": [{"text": "The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-trivial manner from the various spoken regional dialects of Arabic-the true \"native languages\" of Arabic speakers.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 41, "end_pos": 69, "type": "DATASET", "confidence": 0.6658874054749807}]}, {"text": "Those dialects, in turn, differ quite a bit from each other.", "labels": [], "entities": []}, {"text": "However, due to MSA's prevalence in written form, almost all Arabic data sets have predominantly MSA content.", "labels": [], "entities": []}, {"text": "In this article, we describe the creation of a novel Arabic resource with dialect annotations.", "labels": [], "entities": []}, {"text": "We have created a large monolingual data set rich in dialectal Arabic content called the Arabic On-line Commentary Data set (Zaidan and Callison-Burch 2011).", "labels": [], "entities": [{"text": "Arabic On-line Commentary Data set", "start_pos": 89, "end_pos": 123, "type": "DATASET", "confidence": 0.6611011445522308}]}, {"text": "We describe our annotation effort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences from the data set by crowdsourcing the annotation task, and delve into interesting annotator behaviors (like over-identification of one's own dialect).", "labels": [], "entities": []}, {"text": "Using this new annotated data set, we consider the task of Arabic dialect identification: Given the word sequence forming an Arabic sentence, determine the variety of Arabic in which it is written.", "labels": [], "entities": [{"text": "Arabic dialect identification", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.6915758649508158}]}, {"text": "We use the data to train and evaluate automatic classifiers for dialect identification, and establish that classifiers using dialectal data significantly and dramatically outperform baselines that use MSA-only data, achieving near-human classification accuracy.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8119799792766571}, {"text": "accuracy", "start_pos": 252, "end_pos": 260, "type": "METRIC", "confidence": 0.9531717300415039}]}, {"text": "Finally, we apply our classifiers to discover dialectical data from a large Web crawl consisting of 3.5 million pages mined from on-line Arabic newspapers.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this section, we explore using the collected labels to train word-and letter-based DID systems, and show that they outperform other baselines that do not utilize the annotated data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  A summary of the different components of the AOC data set. Overall, 1.4M comments were  harvested from 86.1K articles, corresponding to 52.1M words.", "labels": [], "entities": [{"text": "AOC data set", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9298012256622314}]}, {"text": " Table 3  Some statistics over the labels provided by three spammers. Compared with the typical worker  (right-most column), all workers perform terribly on the MSA control items, and also usually fail  to recognize dialectal content in commentary sentences. Other red flags, such as geographic  location and \"identifying\" unrepresented dialects, are further proof of the spammy behavior.", "labels": [], "entities": []}, {"text": " Table 4  The specific-dialect label distribution (given that a dialect label was provided), shown for each  speaker group.", "labels": [], "entities": []}, {"text": " Table 5  Two annotators with a General label bias, one who uses the label liberally, and one who is more  conservative. Note that in both cases, there is a noticeably smaller percentage of General labels  in the Egyptian newspaper than in the Jordanian and Saudi newspapers.", "labels": [], "entities": []}, {"text": " Table 6  Accuracy rates (%) on several two-way classification tasks (MSA vs. dialect) for various models.  Models in the top part of the table do not utilize the dialect-annotated data, whereas models in  the bottom part do. (For the latter kind of models, the accuracy rates reported are based on a  training set size of 90% of the available data.)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9754543304443359}, {"text": "accuracy", "start_pos": 262, "end_pos": 270, "type": "METRIC", "confidence": 0.9982770681381226}]}, {"text": " Table 7  Confusion matrix in the four-way classification setup. Rows correspond to actual labels, and  columns correspond to predicted labels. For instance, 6.7% of MSA sentences were given a GLF  label (first row, third column). Note that entries within a single row sum to 100%.", "labels": [], "entities": []}, {"text": " Table 8  Predicted label breakdown for the crawled data, over the four varieties of Arabic. All varieties  were given equal priors.", "labels": [], "entities": []}]}