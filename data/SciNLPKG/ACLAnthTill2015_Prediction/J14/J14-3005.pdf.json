{"title": [{"text": "Probabilistic Distributional Semantics with Latent Variable Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a probabilistic framework for acquiring selectional preferences of linguistic predicates and for using the acquired representations to model the effects of context on word meaning.", "labels": [], "entities": []}, {"text": "Our framework uses Bayesian latent-variable models inspired by, and extending, the well-known Latent Dirichlet Allocation (LDA) model of topical structure in documents; when applied to predicate-argument data, topic models automatically induce semantic classes of arguments and assign each predicate a distribution over those classes.", "labels": [], "entities": []}, {"text": "We consider LDA and a number of extensions to the model and evaluate them on a variety of semantic prediction tasks, demonstrating that our approach attains state-of-the-art performance.", "labels": [], "entities": []}, {"text": "More generally, we argue that probabilistic methods provide an effective and flexible methodology for distributional semantics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational models of lexical semantics attempt to represent aspects of word meaning.", "labels": [], "entities": []}, {"text": "For example, a model of the meaning of dog may capture the facts that dogs are animals, that they bark and chase cats, that they are often kept as pets, and soon.", "labels": [], "entities": []}, {"text": "Word meaning is a fundamental component of the way language works: Sentences (and larger structures) consist of words, and their meaning is derived in part from the contributions of their constituent words' lexical meanings.", "labels": [], "entities": [{"text": "Word meaning", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6649019867181778}]}, {"text": "At the same time, words instantiate a mapping between conceptual \"world knowledge\" and knowledge of language.", "labels": [], "entities": []}, {"text": "The relationship between the meanings of an individual word and the larger linguistic structure in which it appears is not unidirectional; while the word contributes to the meaning of the structure, the structure also clarifies the meaning of the word.", "labels": [], "entities": []}, {"text": "Taken on its own a word maybe vague or ambiguous, in the senses of; even when the word's meaning is relatively clear it may still admit specification of additional details that affect its interpretation (e.g., what color/breed was the dog?).", "labels": [], "entities": []}, {"text": "This specification comes through context, which consists of both linguistic and extralinguistic factors but shows a strong effect of the immediate lexical and syntactic environment-the other words surrounding the word of interest and their syntactic relations to it.", "labels": [], "entities": []}, {"text": "These diverse concerns motivate lexical semantic modeling as an important task for all computational systems that must tackle problems of meaning.", "labels": [], "entities": [{"text": "lexical semantic modeling", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.7257527709007263}]}, {"text": "In this article we develop a framework for modeling word meaning and how it is modulated by contextual effects.", "labels": [], "entities": []}, {"text": "Our models are distributional in the sense that their parameters are learned from observed co-occurrences between words and contexts in corpus data.", "labels": [], "entities": []}, {"text": "More specifically, they are probabilistic models that associate latent variables with automatically induced classes of distributional behavior and associate each word with a probability distribution over those classes.", "labels": [], "entities": []}, {"text": "This has a natural interpretation as a model of selectional preference, the semantic phenomenon by which predicates such as verbs or adjectives more plausibly combine with some classes of arguments than with others.", "labels": [], "entities": []}, {"text": "It also has an interpretation as a disambiguation model: The different latent variable values correspond to different aspects of meaning and a word's distribution over those values can be modified by information coming from the context it appears in.", "labels": [], "entities": []}, {"text": "We present a number of specific models within this framework and demonstrate that they can give state-of-the-art performance on tasks requiring models of preference and disambiguation.", "labels": [], "entities": []}, {"text": "More generally, we illustrate that probabilistic modeling is an effective general-purpose framework for distributional semantics and a useful alternative to the popular vector-space framework.", "labels": [], "entities": []}, {"text": "The main contributions of the article are as follows: r We describe the probabilistic approach to distributional semantics, showing how it can be applied as generally as the vector-space approach.", "labels": [], "entities": []}, {"text": "r We present three novel probabilistic selectional preference models and show that they outperform a variety of previously proposed models on a plausibility-based evaluation.", "labels": [], "entities": []}, {"text": "r Furthermore, the representations learned by these models correspond to semantic classes that are useful for modeling the effect of context on semantic similarity and disambiguation.", "labels": [], "entities": []}, {"text": "Section 2 presents background on distributional semantics and an overview of prior work on selectional preference learning and on modeling contextual effects.", "labels": [], "entities": [{"text": "selectional preference learning", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.696995198726654}]}, {"text": "Section 3 introduces the probabilistic latent-variable approach and details the models we use.", "labels": [], "entities": []}, {"text": "Section 4 presents our experimental results on four data sets.", "labels": [], "entities": []}, {"text": "Section 5 concludes and sketches promising research directions for the future.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4  Type and token counts for the BNC and BNC+WIKI corpora.", "labels": [], "entities": [{"text": "BNC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9120137691497803}, {"text": "BNC+WIKI corpora", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.8095358312129974}]}, {"text": " Table 5  Results (Pearson r and Spearman \u03c1 correlations) on Keller and Lapata's (2003) plausibility data.  Asterisks denote performance figures that are taken from the source paper; all other figures are  drawn from our own (re)implementation trained on identical data.", "labels": [], "entities": [{"text": "Pearson r and Spearman \u03c1 correlations", "start_pos": 19, "end_pos": 56, "type": "METRIC", "confidence": 0.7814411272605261}]}, {"text": " Table 6  Aggregate comparisons for the Keller and Lapata (2003) plausibility data set between  latent-variable models (rows) and previously proposed selectional preference models (columns).  Cell entries give the number of evaluations (out of 12) in which the latent-variable model  outperformed the alternative method and the number in which the improvement was  statistically significant.", "labels": [], "entities": [{"text": "Keller and Lapata (2003) plausibility data set", "start_pos": 40, "end_pos": 86, "type": "DATASET", "confidence": 0.6521716283427345}]}, {"text": " Table 7  Most-and least-accurately predicted items for the LDA P\u2192A models using per-item Spearman's \u03c1  pseudo-coefficients on the unseen data set, with gold and predicted rank values.", "labels": [], "entities": []}, {"text": " Table 10  Results (\u03c1 ave averaged across annotators) for the Mitchell and Lapata (2008) similarity data set.", "labels": [], "entities": [{"text": "Mitchell and Lapata (2008) similarity data set", "start_pos": 62, "end_pos": 108, "type": "DATASET", "confidence": 0.6569189230600992}]}, {"text": " Table 11  Results (\u03c1 cat ) for the Mitchell and Lapata (2008) similarity data set.", "labels": [], "entities": [{"text": "Mitchell and Lapata (2008) similarity data set", "start_pos": 36, "end_pos": 82, "type": "DATASET", "confidence": 0.6967741582128737}]}, {"text": " Tables 12 and  Table 13.", "labels": [], "entities": []}, {"text": " Table 13  Results (\u03c1 cat averaged across groups) for the Mitchell and Lapata (2010) similarity data set.", "labels": [], "entities": [{"text": "Mitchell and Lapata (2010) similarity data set", "start_pos": 58, "end_pos": 104, "type": "DATASET", "confidence": 0.670580354001787}]}, {"text": " Table 14  Example sentences for the verb charge from the English Lexical Substitution Task.", "labels": [], "entities": []}, {"text": " Table 15  Results on the English Lexical Substitution Task data set; boldface denotes best performance at  full coverage for each corpus.", "labels": [], "entities": [{"text": "English Lexical Substitution Task data set", "start_pos": 26, "end_pos": 68, "type": "DATASET", "confidence": 0.7631746232509613}]}, {"text": " Table 16  Performance by part of speech, with additional results from Thater, F \u00a8  urstenau, and Pinkal (2010,  2011) and Dinu and Lapata (2010).", "labels": [], "entities": [{"text": "F \u00a8  urstenau", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9179900288581848}]}]}