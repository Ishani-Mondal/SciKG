{"title": [{"text": "Random Walks for Knowledge-Based Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7275568544864655}]}], "abstractContent": [{"text": "Word Sense Disambiguation (WSD) systems automatically choose the intended meaning of a word in context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7672232389450073}]}, {"text": "In this article we present a WSD algorithm based on random walks overlarge Lexical Knowledge Bases (LKB).", "labels": [], "entities": [{"text": "WSD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9785955548286438}]}, {"text": "We show that our algorithm performs better than other graph-based methods when run on a graph built from WordNet and eXtended WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9688835144042969}]}, {"text": "Our algorithm and LKB combination compares favorably to other knowledge-based approaches in the literature that use similar knowledge on a variety of English data sets and a data set on Spanish.", "labels": [], "entities": []}, {"text": "We include a detailed analysis of the factors that affect the algorithm.", "labels": [], "entities": []}, {"text": "The algorithm and the LKBs used are publicly available, and the results easily reproducible.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a key enabling technology that automatically chooses the intended sense of a word in context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.774908572435379}]}, {"text": "It has been the focus of intensive research since the beginning of Natural Language Processing (NLP), and more recently it has been shown to be useful in several tasks such as parsing), machine translation, information retrieval (P\u00e9rez-Ag\u00fceraAg\u00a8Ag\u00fcera and Zaragoza 2008; Zhong and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and summarization (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 176, "end_pos": 183, "type": "TASK", "confidence": 0.9730089902877808}, {"text": "machine translation", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.8237336575984955}, {"text": "information retrieval", "start_pos": 207, "end_pos": 228, "type": "TASK", "confidence": 0.7952934503555298}, {"text": "question answering", "start_pos": 291, "end_pos": 309, "type": "TASK", "confidence": 0.9185130298137665}, {"text": "summarization", "start_pos": 356, "end_pos": 369, "type": "TASK", "confidence": 0.9878453612327576}]}, {"text": "WSD is considered to be a key step in order to approach language understanding beyond keyword matching.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7404801845550537}, {"text": "keyword matching", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7137153148651123}]}, {"text": "The best performing WSD systems are currently those based on supervised learning, as attested in public evaluation exercises), but they need large amounts of hand-tagged data, which is typically very expensive to produce.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9738653302192688}]}, {"text": "Contrary to lexical-sample exercises (where plenty of training and testing examples fora handful of words are provided), all-words exercises (which comprise all words occurring in a running text, and where training data is more scarce) show that only a few systems beat the most frequent sense (MFS) heuristic, with small differences.", "labels": [], "entities": []}, {"text": "For instance, the best system in SensEval-3 scored 65.2 F1, compared to 62.4).", "labels": [], "entities": [{"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9984859824180603}]}, {"text": "The best current state-of-the-art WSD system (, outperforms the MFS heuristic by 5% to 8% in absolute F1 scores on the SensEval and SemEval fine-grained English all words tasks.", "labels": [], "entities": [{"text": "F1", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9429269433021545}]}, {"text": "The causes of the small improvement over the MFS heuristic can be found in the relatively small amount of training data available (sparseness) and the problems that arise when the supervised systems are applied to different corpora from that used to train the system (corpus mismatch).", "labels": [], "entities": []}, {"text": "Note that most of the supervised systems for English are trained over), a half-a-million word subset of the Brown Corpus made available from the WordNet team, and DSO (, comprising 192,800 word occurrences from the Brown and WSJ corpora corresponding to the 191 most frequent nouns and verbs.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.9807382524013519}, {"text": "WordNet team", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9370633065700531}, {"text": "DSO", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9644070267677307}, {"text": "Brown and WSJ corpora", "start_pos": 215, "end_pos": 236, "type": "DATASET", "confidence": 0.7548851370811462}]}, {"text": "Several researchers have explored solutions to sparseness.", "labels": [], "entities": []}, {"text": "For instance, present an unsupervised method to obtain training examples from bilingual data, which was used together with SemCor and DSO to train one of the best performing supervised systems to date.", "labels": [], "entities": [{"text": "DSO", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.8518888354301453}]}, {"text": "In view of the problems of supervised systems, knowledge-based WSD is emerging as a powerful alternative.", "labels": [], "entities": [{"text": "WSD", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.7950904369354248}]}, {"text": "Knowledge-based WSD systems exploit the information in a lexical knowledge base (LKB) to perform WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9702087044715881}]}, {"text": "They currently perform below supervised systems on general domain data, but are attaining performance close or above MFS without access to hand-tagged data.", "labels": [], "entities": []}, {"text": "In this sense, they provide a complementary strand of research which could be combined with supervised methods, as shown for instance in.", "labels": [], "entities": []}, {"text": "In addition, Agirre, L \u00b4 opez show that knowledge-based WSD systems can outperform supervised systems in a domain-specific data set, where MFS from general domains also fails.", "labels": [], "entities": [{"text": "WSD", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9199624061584473}]}, {"text": "In this article, we will focus our attention on knowledge-based methods.", "labels": [], "entities": []}, {"text": "Early work for knowledge-based WSD was based on measures of similarity between pairs of concepts.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8689268827438354}]}, {"text": "In order to maximize pairwise similarity fora sequence of n words where each has up to k senses, the algorithms had to consider up to kn sense sequences.", "labels": [], "entities": []}, {"text": "Greedy methods were often used to avoid the combinatorial explosion.", "labels": [], "entities": []}, {"text": "As an alternative, graph-based methods are able to exploit the structural properties of the graph underlying a particular LKB.", "labels": [], "entities": []}, {"text": "These methods are able to consider all possible combinations of occurring senses on a particular context, and thus offer away to analyze efficiently the inter-relations among them, gaining much attention in the NLP community.", "labels": [], "entities": []}, {"text": "The nodes in the graph represent the concepts (word senses) in the LKB, and edges in the graph represent relations between them, such as subclass and part-of.", "labels": [], "entities": []}, {"text": "Network analysis techniques based on random walks like PageRank ( can then be used to choose the senses that are most relevant in the graph, and thus output those senses.", "labels": [], "entities": []}, {"text": "In order to deal with large knowledge bases containing more than 100,000 concepts, previous algorithms had to extract subsets of the LKB or construct ad hoc graphs for each context to be disambiguated).", "labels": [], "entities": []}, {"text": "An additional reason for the use of custom-built subsets of ad hoc graphs for each context is that if we were using a centrality algorithm like PageRank over the whole graph, it would choose the most important senses in the LKB regardless of context, limiting the applicability of the algorithm.", "labels": [], "entities": []}, {"text": "For instance, the word coach is ambiguous at least between the \"sports coach\" and the \"transport service\" meanings, as shown in the following examples: (1) Nadal is sharing a house with his uncle and coach, Toni, and his physical trainer, Rafael Maymo.", "labels": [], "entities": []}, {"text": "(2) Our fleet comprises coaches from 35 to 58 seats.", "labels": [], "entities": []}, {"text": "If we were to run a centrality algorithm over the whole LKB, with no context, then we would always assign coach to the same concept, and we would thus fail to correctly disambiguate either one of the given examples.", "labels": [], "entities": []}, {"text": "The contributions of this article are the following: (1) A WSD method based on random walks overlarge LKBs.", "labels": [], "entities": [{"text": "WSD", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.957663357257843}]}, {"text": "The algorithm outperforms other graph-based algorithms when using a LKB built from WordNet and eXtended WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.965460479259491}]}, {"text": "The algorithm and LKB combination compares favorably to the state-of-the-art in knowledge-based WSD on a wide variety of data sets, including four English and one Spanish data set.", "labels": [], "entities": []}, {"text": "(2) A detailed analysis of the factors that affect the algorithm.", "labels": [], "entities": []}, {"text": "The algorithm together with the corresponding graphs are publicly available and can be applied easily to sense inventories and knowledge bases different from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.9561392068862915}]}, {"text": "The algorithm for WSD was first presented in . In this article, we present further evaluation on two more recent data sets, analyze the parameters and options of the system, compare it to the state of the art, and discuss the relation of our algorithm with PageRank and the MFS heuristic.", "labels": [], "entities": [{"text": "WSD", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9631638526916504}, {"text": "PageRank", "start_pos": 257, "end_pos": 265, "type": "DATASET", "confidence": 0.9162774682044983}, {"text": "MFS heuristic", "start_pos": 274, "end_pos": 287, "type": "DATASET", "confidence": 0.8985055088996887}]}], "datasetContent": [{"text": "WSD literature has used several measures for evaluation.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8365002870559692}]}, {"text": "Precision is the percentage of correctly disambiguated instances divided by the number of instances disambiguated.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9892895817756653}]}, {"text": "Some systems don't disambiguate all instances, and thus the precision can be high even if the system disambiguates a handful of instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9995044469833374}]}, {"text": "In our case, when a word has Portion of WordNet to illustrate the disambiguation of coach in the sentence Our fleet comprises coaches from 35 to 58 seats.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9500831365585327}]}, {"text": "Each word in the sentence (shown partially) is linked to all its synsets.", "labels": [], "entities": []}, {"text": "The path between trainer#n1 and teacher#1 is omitted for brevity (see.", "labels": [], "entities": []}, {"text": "The left part shows the PPR method, and the right part shows the PPR method.", "labels": [], "entities": []}, {"text": "two senses with the same PageRank value, our algorithm does not return anything, because it abstains from returning a sense in the case of ties.", "labels": [], "entities": []}, {"text": "In contrast, recall measures the percentage of correctly disambiguated instances divided by the total number of instances to be disambiguated.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9992266893386841}]}, {"text": "This measure penalizes systems that are unable to return a solution for all instances.", "labels": [], "entities": []}, {"text": "Finally, the harmonic mean between precision and recall (F1) combines both measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9735027551651001}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9931154847145081}, {"text": "F1)", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9192173480987549}]}, {"text": "F1 is our main measure of evaluation, as it provides a balanced measure between the two extremes.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9945764541625977}]}, {"text": "Note that a system that returns a solution for all instances would have equal precision, recall, and F1 measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9996641874313354}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.999458372592926}, {"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9997928738594055}]}, {"text": "In our experiments we build a context of at least 20 content words for each sentence to be disambiguated, taking the sentences immediately before and after it in the case that the original sentence was too short.", "labels": [], "entities": []}, {"text": "The parameters for the PageRank algorithm were set to 0.85 and 30 iterations following standard practice).", "labels": [], "entities": []}, {"text": "The post hoc impact of those and other parameters has been studied in Section 6.4.", "labels": [], "entities": [{"text": "Section 6.4", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.93377286195755}]}, {"text": "The general domain data sets used in this work are the SensEval-2 (S2AW) (Snyder and Palmer 2004), SensEval-3 (S3AW) (, and SemEval-2007 finegrained (S07AW) () and coarse grained all-words data sets (S07CG).", "labels": [], "entities": []}, {"text": "All data sets have been produced similarly: A few documents were selected for tagging, at least two annotators tagged nouns, verbs, adjectives, and adverbs, intertagger agreement was measured, and the discrepancies between taggers were solved.", "labels": [], "entities": []}, {"text": "The first two data sets are labeled with WordNet 1.7 tags, the third uses WordNet 2.1 tags, and the last one uses coarse-grained senses that group WordNet 2.1 senses.", "labels": [], "entities": [{"text": "WordNet 1.7 tags", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.8851670225461324}, {"text": "WordNet 2.1 tags", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.8856344223022461}]}, {"text": "We run our system using WordNet 1.7 relations and senses for the first two data sets, and WordNet 2.1 for the other two.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9344619512557983}, {"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9481112957000732}]}, {"text": "Section 6.4.3 explores the use of WordNet 3.0 and compares the performance with the use of other versions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9251783490180969}]}, {"text": "Regarding the coarse senses in S07CG, we used the mapping from WordNet 2.1 senses made available by the authors of the data set.", "labels": [], "entities": [{"text": "WordNet 2.1 senses", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.9028800129890442}]}, {"text": "In order to return coarse grainedsenses, we run our algorithm on fine-grained senses, and aggregate the scores for all senses that map to the same coarse-grained sense.", "labels": [], "entities": []}, {"text": "We finally choose the coarsegrained sense with the highest score.", "labels": [], "entities": [{"text": "coarsegrained sense", "start_pos": 22, "end_pos": 41, "type": "DATASET", "confidence": 0.8558695912361145}]}, {"text": "The data sets used in this article contain polysemous and monosemous words, as customary; the percentage of monosemous word occurrences in the S2AW, S3AW, S07AW, and S07CG data sets are 20.7%, 16.9%, 14.4%, and 29.9%, respectively.", "labels": [], "entities": [{"text": "S07CG data sets", "start_pos": 166, "end_pos": 181, "type": "DATASET", "confidence": 0.8964682817459106}]}, {"text": "shows the results as F1 of our random walk WSD systems over these data sets.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.997789740562439}]}, {"text": "We detail overall results, as well as results per part of speech, and whether there is any statistical difference with respect to the best result on each column.", "labels": [], "entities": []}, {"text": "Statistical significance is obtained using the paired bootstrap resampling method, p < 0.01.", "labels": [], "entities": []}, {"text": "Our WSD algorithm can be applied over non-English texts, provided that a LKB for this particular language exists.", "labels": [], "entities": []}, {"text": "We have applied our random walk algorithms to the Spanish WordNet (Atserias,, using the SemEval-2007 Task 09 data set as evaluation gold standard).", "labels": [], "entities": [{"text": "Spanish WordNet", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8425002098083496}, {"text": "SemEval-2007 Task 09 data set", "start_pos": 88, "end_pos": 117, "type": "DATASET", "confidence": 0.8150651454925537}]}, {"text": "The data set contains examples of the 150 most frequent nouns in the CESS-ECE corpus, manually annotated with Spanish WordNet synsets.", "labels": [], "entities": [{"text": "CESS-ECE corpus", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9661691784858704}]}, {"text": "It is split into a train and test part, and has an \"all words\" shape (i.e., input consists of sentences, each one having at least one occurrence of a target noun).", "labels": [], "entities": []}, {"text": "We ran the experiment over the test part (792 instances), and used the train part for calculating the MFS heuristic.", "labels": [], "entities": [{"text": "MFS heuristic", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.6461496353149414}]}, {"text": "The results in are consistent with those for English, with our algorithms approaching MFS performance, and PPR w2w yielding the best results.", "labels": [], "entities": []}, {"text": "Note that for this data set the supervised algorithm could barely improve over the MFS, which performs very well, suggesting that in this particular data set the sense distributions are highly skewed.", "labels": [], "entities": []}, {"text": "Finally, we also show results for the first sense in the Spanish WordNet.", "labels": [], "entities": [{"text": "Spanish WordNet", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8339777588844299}]}, {"text": "In the Spanish WordNet the order of the senses of a word has been assigned directly by the lexicographer (Atserias,, as there is no information of sense frequency from hand-annotated corpora.", "labels": [], "entities": []}, {"text": "This is in contrast to the English WordNet, where the senses are ordered according to their frequency in annotated Results as F1 on the Spanish SemEval07 data set, including first sense, MFS, and the best supervised system in the competition.", "labels": [], "entities": [{"text": "F1", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9895046949386597}, {"text": "Spanish SemEval07 data set", "start_pos": 136, "end_pos": 162, "type": "DATASET", "confidence": 0.8308354765176773}]}, {"text": "* Statistically significant difference with respect to the best of our results (in bold).", "labels": [], "entities": [{"text": "Statistically significant difference", "start_pos": 2, "end_pos": 38, "type": "METRIC", "confidence": 0.9504084984461466}]}], "tableCaptions": [{"text": " Table 2  Results on English data sets (F1). Best results in each column in bold.  *  Statistically significant  with respect to the best result in each column.", "labels": [], "entities": [{"text": "English data sets", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9053784807523092}]}, {"text": " Table 3  Comparison with state-of-the-art results (F1). The top rows report knowledge-based and  unsupervised systems, followed by our system (PPR", "labels": [], "entities": [{"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9946598410606384}]}, {"text": " Table 4  Results for subgraph methods compared with our method (F1). In the Reference column we  mention the reference system that we reimplemented. Best results in each column in bold.  *   Statistically significant with respect to the best result in each column.", "labels": [], "entities": []}, {"text": " Table 5  Comparing WordNet versions. Best result in each row in bold.", "labels": [], "entities": []}, {"text": " Table 6  Comparing XWN and WN3.0 gloss relations, separately and combined. Best result in each row in  bold.", "labels": [], "entities": [{"text": "XWN", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.837341845035553}]}, {"text": " Table 7  Analysis of relation types. The first column shows the performance using just that relation type.  The second shows the combination of TAX and each type. The last column shows all relations  except the corresponding type.", "labels": [], "entities": [{"text": "TAX", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.881747305393219}]}, {"text": " Table 8  Correlation between systems, gold tags, and MFS.", "labels": [], "entities": []}, {"text": " Table 9  Results on three subcorpora as reported in Agirre, L \u00b4  opez de Lacalle, and Soroa (2009),  where Sports and Finance are domain-specific. Best results on each column in bold.", "labels": [], "entities": [{"text": "Agirre", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.904413104057312}, {"text": "L \u00b4  opez de Lacalle", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.8400599598884583}, {"text": "Soroa (2009)", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.8423493206501007}]}, {"text": " Table 10  Combination with MFS (F1). The first two rows correspond to our system with and without  information from MFS. Below that we report systems that also use MFS. Best results in each  column in bold.", "labels": [], "entities": []}]}