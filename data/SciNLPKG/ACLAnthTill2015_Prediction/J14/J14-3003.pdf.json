{"title": [{"text": "A Random Walk-Based Model for Identifying Semantic Orientation", "labels": [], "entities": [{"text": "Identifying Semantic Orientation", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.9045260548591614}]}], "abstractContent": [{"text": "Automatically identifying the sentiment polarity of words is a very important task that has been used as the essential building block of many natural language processing systems such as text classification, text filtering, product review analysis, survey response analysis, and on-line discussion mining.", "labels": [], "entities": [{"text": "Automatically identifying the sentiment polarity of words", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.760104124035154}, {"text": "text classification", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.7766907513141632}, {"text": "text filtering", "start_pos": 207, "end_pos": 221, "type": "TASK", "confidence": 0.7948356568813324}, {"text": "product review analysis", "start_pos": 223, "end_pos": 246, "type": "TASK", "confidence": 0.7371203700701395}, {"text": "survey response analysis", "start_pos": 248, "end_pos": 272, "type": "TASK", "confidence": 0.7171981334686279}, {"text": "on-line discussion mining", "start_pos": 278, "end_pos": 303, "type": "TASK", "confidence": 0.666650116443634}]}, {"text": "We propose a method for identifying the sentiment polarity of words that applies a Markov random walk model to a large word relatedness graph, and produces a polarity estimate for any given word.", "labels": [], "entities": [{"text": "identifying the sentiment polarity of words", "start_pos": 24, "end_pos": 67, "type": "TASK", "confidence": 0.7242445796728134}]}, {"text": "The model can accurately and quickly assign a polarity sign and magnitude to any word.", "labels": [], "entities": []}, {"text": "It can be used both in a semi-supervised setting where a training set of labeled words is used, and in a weakly supervised setting where only a handful of seed words is used to define the two polarity classes.", "labels": [], "entities": []}, {"text": "The method is experimentally tested using a gold standard set of positive and negative words from the General Inquirer lexicon.", "labels": [], "entities": [{"text": "General Inquirer lexicon", "start_pos": 102, "end_pos": 126, "type": "DATASET", "confidence": 0.8711163401603699}]}, {"text": "We also show how our method can be used for three-way classification which identifies neutral words in addition to positive and negative words.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed method outperforms the state-of-the-art methods in the semi-supervised setting and is comparable to the best reported values in the weakly supervised setting.", "labels": [], "entities": []}, {"text": "In addition, the proposed method is faster and does not need a large corpus.", "labels": [], "entities": []}, {"text": "We also present extensions of our methods for identifying the polarity of foreign words and out-of-vocabulary words.", "labels": [], "entities": [{"text": "identifying the polarity of foreign words", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.803530196348826}]}], "introductionContent": [{"text": "Identifying emotions and attitudes from unstructured text has a variety of possible applications.", "labels": [], "entities": [{"text": "Identifying emotions and attitudes from unstructured text", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8616643973759243}]}, {"text": "For example, there has been a large body of work for mining product reputation on the Web (.", "labels": [], "entities": [{"text": "mining product reputation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.8514875769615173}]}, {"text": "have shown how product reputation mining helps with marketing and customer relation management.", "labels": [], "entities": [{"text": "product reputation mining", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6873364051183065}, {"text": "customer relation management", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7425727446873983}]}, {"text": "The Google products catalog and many on-line shopping sites like Amazon.com provide customers not only with comprehensive information and reviews about a product, but also with faceted sentiment summaries.", "labels": [], "entities": [{"text": "Google products catalog", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.7632280588150024}]}, {"text": "Such systems are all supported by a sentiment lexicon, some even in multiple languages.", "labels": [], "entities": []}, {"text": "Another interesting application is mining on-line discussions.", "labels": [], "entities": [{"text": "mining on-line discussions", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8644576470057169}]}, {"text": "An enormous number of discussion groups exist on the Web.", "labels": [], "entities": []}, {"text": "Millions of users post content to these groups covering pretty much every possible topic.", "labels": [], "entities": []}, {"text": "Tracking a participant attitude toward different topics and toward other participants is a very important task that makes use of sentiment lexicons.", "labels": [], "entities": [{"text": "Tracking a participant attitude", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8323302119970322}]}, {"text": "For example, presented the concept of sentiment timelines.", "labels": [], "entities": [{"text": "sentiment timelines", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8847154676914215}]}, {"text": "His system classifies discussion posts about movies as either positive or negative.", "labels": [], "entities": []}, {"text": "This is used to produce a plot of the number of positive and negative sentiment messages overtime.", "labels": [], "entities": []}, {"text": "All these applications would benefit from an automatic way of identifying semantic orientation of words.", "labels": [], "entities": [{"text": "identifying semantic orientation of words", "start_pos": 62, "end_pos": 103, "type": "TASK", "confidence": 0.8660369753837586}]}, {"text": "In this article, we study the task of automatically identifying the semantic orientation of any word by analyzing its relations to other words, Automatically classifying words as positive, negative, or neutral enables us to automatically identify the polarity of larger pieces of text.", "labels": [], "entities": [{"text": "identifying the semantic orientation of any word", "start_pos": 52, "end_pos": 100, "type": "TASK", "confidence": 0.7274571231433323}]}, {"text": "This could be a very useful building block for systems that mine surveys, product reviews, and on-line discussions.", "labels": [], "entities": []}, {"text": "We apply a Markov random walk model to a large semantic relatedness graph, producing a polarity estimate for any given word.", "labels": [], "entities": []}, {"text": "Previous work on identifying the semantic orientation of words has addressed the problem as both a semi-supervised and a weakly supervised (Turney and Littman 2003) learning problem.", "labels": [], "entities": [{"text": "identifying the semantic orientation of words", "start_pos": 17, "end_pos": 62, "type": "TASK", "confidence": 0.8755591412385305}]}, {"text": "In the semisupervised setting, a training set of labeled words is used to train the model.", "labels": [], "entities": []}, {"text": "In the weakly supervised setting, only a handful of seeds are used to define the two polarity classes.", "labels": [], "entities": []}, {"text": "Our proposed method can be used both in a semi-supervised and in a weakly supervised setting.", "labels": [], "entities": []}, {"text": "Empirical experiments on a labeled set of positive and negative words show that the proposed method outperforms the state-of-the-art methods in the semi-supervised setting.", "labels": [], "entities": []}, {"text": "The results in the weakly supervised setting are comparable to the best reported values.", "labels": [], "entities": []}, {"text": "The proposed method has the advantages that it is faster and does not need a large training corpus.", "labels": [], "entities": []}, {"text": "The rest of the article is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review related work on word polarity and subjectivity classification and note applications of the random walk and hitting times framework.", "labels": [], "entities": [{"text": "word polarity", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.7227651476860046}, {"text": "subjectivity classification", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6955349296331406}]}, {"text": "Section 3 presents our method for identifying word polarity.", "labels": [], "entities": [{"text": "identifying word polarity", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.793887734413147}]}, {"text": "We describe how the proposed method can be extended to cover foreign languages in Section 4, and out-of-vocabulary words in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 describes our experimental set-up.", "labels": [], "entities": []}, {"text": "We present our conclusions in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on the gold-standard data set for positive/negative words from the General Inquirer lexicon ().", "labels": [], "entities": [{"text": "General Inquirer lexicon", "start_pos": 92, "end_pos": 116, "type": "DATASET", "confidence": 0.8405698537826538}]}, {"text": "The data set contains 4, 206 words, 1, 915 of which are positive and 2, 291 of which are negative.", "labels": [], "entities": []}, {"text": "Some of the ambiguous words were removed, as in Turney and.", "labels": [], "entities": [{"text": "Turney", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.929789662361145}]}, {"text": "Some examples of positive/negative words are listed in.", "labels": [], "entities": []}, {"text": "We use WordNet as a source of synonyms and hypernyms for the word relatedness graph.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9540543556213379}]}, {"text": "We used the Reuters Corpus, Volume 1 () to generate co-occurrence statistics in the experiments that used them.", "labels": [], "entities": [{"text": "Reuters Corpus, Volume 1", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.9790787100791931}]}, {"text": "We used 10-fold cross-validation for all tests.", "labels": [], "entities": []}, {"text": "We evaluate our results in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9994434714317322}]}, {"text": "Statistical significance was tested using a two-tailed paired t-test.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.7938181757926941}]}, {"text": "All reported results are statistically significant at the 0.05 level.", "labels": [], "entities": []}, {"text": "We perform experiments varying the parameters and the network.", "labels": [], "entities": []}, {"text": "We also look at the performance of the proposed method for different parts of speech, and for different confidence levels.", "labels": [], "entities": []}, {"text": "We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in, the Spin model described in, the shortest path method described in, a re-implementation of the label propagation and Mincut methods described in, and the bootstrapping method described in.", "labels": [], "entities": [{"text": "Semantic Orientation from PMI", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.7219533920288086}]}, {"text": "We now measure the performance of the random walk method when the system is allowed to abstain from classifying the words for which it has low confidence.", "labels": [], "entities": []}, {"text": "We regard the ratio between the hitting time to positive words and hitting time to negative words as a confidence measure and evaluate the top words with the highest confidence level at different values of threshold.", "labels": [], "entities": []}, {"text": "shows the accuracy for 10-fold cross validation and for using only 14 seeds at different thresholds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996271133422852}]}, {"text": "We notice that the accuracy improves by abstaining from classifying the difficult words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999420166015625}]}, {"text": "The figure shows that the top 60% words are classified with accuracy greater than 99% for 10-fold cross validation and 92% with 14 seed words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9993705153465271}]}, {"text": "This maybe compared with the work described in, where they achieve the 92% level when they only consider the top 1,000 words (28%).", "labels": [], "entities": []}, {"text": "shows a learning curve displaying how the performance of both the proposed method and the LP method is affected with varying the labeled set size (i.e., the number of seeds).", "labels": [], "entities": []}, {"text": "We notice that the accuracy exceeds 90% when the training set size rises above 20%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997729659080505}]}, {"text": "The accuracy steadily increases as the size of labeled data increases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.99962317943573}]}, {"text": "We also looked at the classification accuracy for different parts of speech in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9011932611465454}]}, {"text": "We notice that, in the case of 10-fold cross-validation, the performance is consistent across parts of speech.", "labels": [], "entities": []}, {"text": "However, when we only use 14 seeds-all of which are adjectives, similar to Turney and Littman (2003)-we notice that the performance on adjectives is much better than other parts of speech.", "labels": [], "entities": []}, {"text": "When we use 14 seeds but replace some of the adjectives with verbs and nouns such as love, harm, friend, enemy, the performance for nouns and verbs improves considerably at the cost of a small drop in the   performance on adjectives.", "labels": [], "entities": []}, {"text": "Finally, we tried adding edges to the network from glosses and co-occurrence statistics but we did not get any statistically significant improvement.", "labels": [], "entities": []}, {"text": "Some of the words that were very weakly linked benefited from adding new types of links and they were correctly predicted.", "labels": [], "entities": []}, {"text": "Others were misled by the noise and were incorrectly classified.", "labels": [], "entities": []}, {"text": "We had a closer look at the results to find out what are the reasons behind incorrect predictions.", "labels": [], "entities": []}, {"text": "We found two main reasons.", "labels": [], "entities": []}, {"text": "First, some words have more than one sense, possibly with different semantic orientations.", "labels": [], "entities": []}, {"text": "Disambiguating the sense of words given their context before trying to predict their polarity should solve this problem.", "labels": [], "entities": []}, {"text": "The second reason is that some words have very few connections in the thesaurus.", "labels": [], "entities": []}, {"text": "A possible solution to this might be to identify those words and add more links to them from glosses of co-occurrence statistics in the corpus.", "labels": [], "entities": []}, {"text": "6.1.3 General Purpose Three-Way Classification.", "labels": [], "entities": [{"text": "General Purpose Three-Way Classification", "start_pos": 6, "end_pos": 46, "type": "TASK", "confidence": 0.5735240429639816}]}, {"text": "The experiments described so far all use the General Inquirer lexicon, which contains a well-established gold standard data set of positive and negative words.", "labels": [], "entities": [{"text": "General Inquirer lexicon", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.9119812846183777}]}, {"text": "However, in realistic applications, a general purpose list of words will frequently have neutral words that don't express sentiment polarity.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of the random walk method in distinguishing polarized words from neutral words, we constructed a data set of 2, 000 words randomly picked from a standard English dictionary 3 and hand labeled them with three classes: positive, negative, and neutral.", "labels": [], "entities": []}, {"text": "Among the 2, 000 words, 494 were labeled positive, 491 negative, and 1, 015 neutral.", "labels": [], "entities": []}, {"text": "The distribution among different parts of speech is 532 adjectives, 335 verbs, 1, 051 nouns, and 82 others.", "labels": [], "entities": []}, {"text": "We used the semi-supervised setting with the General Inquirer lexicon polarized word list as the training set.", "labels": [], "entities": [{"text": "General Inquirer lexicon polarized word list", "start_pos": 45, "end_pos": 89, "type": "DATASET", "confidence": 0.8565193216005961}]}, {"text": "Because the 2, 000 test set has some portion of polarized words overlapping with the training set, we excluded the words that appear in the test set from the training set.", "labels": [], "entities": []}, {"text": "We performed Algorithm 2 in Section 3.4 with parameters \u03b3 = 0.8, m = 15, k = 1, 000.", "labels": [], "entities": []}, {"text": "The overall accuracy as well as the precision for each class is shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996961355209351}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9997292160987854}]}, {"text": "We can see that the accuracy of the positive class is much lower than the negative class, due to the many positive words classified as neutral.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9994833469390869}]}, {"text": "This means that the average confidence of negative words is higher than positive words.", "labels": [], "entities": []}, {"text": "One factor that could have caused this is the bias originating from the training set.", "labels": [], "entities": []}, {"text": "Because there are more negative seeds than positive ones, the constructed graph has an overall bias towards the negative class.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model,  and the random walks model for 10-fold cross-validation and 14 seeds.", "labels": [], "entities": []}, {"text": " Table 3. We observe  that the random walk method outperforms SO-PMI when SO-PMI uses data sets of  sizes 1 \u00d7 10 7 and 2 \u00d7 10 9 words. The performance of SO-PMI and the random walk  methods are comparable when SO-PMI uses a very large data set (1 \u00d7 10 11 words). The  performance of the spin model approach is also comparable to the other two methods.  The advantages of the random walk method over SO-PMI is that it is faster and it does  not need a very large corpus. Another advantage is that the random walk method can  be used along with the labeled data from the General Inquirer lexicon (", "labels": [], "entities": [{"text": "General Inquirer lexicon", "start_pos": 569, "end_pos": 593, "type": "DATASET", "confidence": 0.921450138092041}]}, {"text": " Table 4  Accuracy for adjectives only for the spin model, the bootstrap method, and the random walk  model.", "labels": [], "entities": []}, {"text": " Table 5  Accuracy for three classes on a general purpose list of 2,000 words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9871343970298767}]}]}