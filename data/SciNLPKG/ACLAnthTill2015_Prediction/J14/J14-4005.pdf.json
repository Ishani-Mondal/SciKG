{"title": [{"text": "A Large-Scale Pseudoword-Based Evaluation Framework for State-of-the-Art Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7209561069806417}]}], "abstractContent": [{"text": "The evaluation of several tasks in lexical semantics is often limited by the lack of large numbers of manual annotations, not only for training purposes, but also for testing purposes.", "labels": [], "entities": []}, {"text": "Word Sense Disambiguation (WSD) is a casein point, as hand-labeled data sets are particularly hard and time-consuming to create.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8152105510234833}]}, {"text": "Consequently, evaluations tend to be performed on a small scale, which does not allow for in-depth analysis of the factors that determine a system's performance.", "labels": [], "entities": []}, {"text": "In this article we address this issue by means of a realistic simulation of large-scale evaluation for the WSD task.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 107, "end_pos": 115, "type": "TASK", "confidence": 0.9311464726924896}]}, {"text": "We do this by providing two main contributions: First, we put forward two novel approaches to the wide-coverage generation of semantically aware pseudowords (i.e., artificial words capable of modeling real polysemous words); second, we leverage the most suitable type of pseudoword to create large pseudosense-annotated corpora, which enable a large-scale experimental framework for the comparison of state-of-the-art supervised and knowledge-based algorithms.", "labels": [], "entities": []}, {"text": "Using this framework, we study the impact of supervision and knowledge on the two major disambiguation paradigms and perform an in-depth analysis of the factors which affect their performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a core research field in computational linguistics dealing with the automatic assignment of senses to words occurring in a given context).", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7898413091897964}, {"text": "automatic assignment of senses to words occurring in a given context", "start_pos": 103, "end_pos": 171, "type": "TASK", "confidence": 0.7836998674002561}]}, {"text": "There are two major paradigms in WSD: supervised and knowledge-based.", "labels": [], "entities": [{"text": "WSD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9666146039962769}]}, {"text": "Supervised WSD starts from a training set and learns a computational model of the word of interest, which is later used attest time to classify new instances of the same word.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9593908786773682}]}, {"text": "Knowledge-based WSD, instead, performs the disambiguation task by using an existing lexical knowledge base-that is, a semantic network to which graph algorithms, for example, can be applied.", "labels": [], "entities": []}, {"text": "However, both disambiguation paradigms have to face the so-called knowledge acquisition bottleneck, namely, the difficulty of capturing knowledge in a computer-usable form.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7316146492958069}]}, {"text": "Unfortunately, providing knowledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest.", "labels": [], "entities": []}, {"text": "Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary () and to 1993 for the SemCor corpus ().", "labels": [], "entities": [{"text": "WordNet dictionary", "start_pos": 151, "end_pos": 169, "type": "DATASET", "confidence": 0.9660969376564026}]}, {"text": "In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk () or voluntary collaborative editing such as in Wikipedia, producing annotated resources manually is still an arduous and understandably infrequent endeavor.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.8584271868069967}]}, {"text": "Despite recent efforts in this direction, including OntoNotes () and MASC (), most work is now aimed either at the automatic acquisition of training data) and lexical knowledge resources, or at the large-scale acquisition of annotations via games () or even video games with a purpose, as recently proposed by.", "labels": [], "entities": [{"text": "MASC", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.6538391709327698}]}, {"text": "As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based ( paradigms in different settings and conditions.", "labels": [], "entities": []}, {"text": "Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available.", "labels": [], "entities": []}, {"text": "All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system's performance.", "labels": [], "entities": []}, {"text": "In this article we address this issue by providing two main contributions: r We first focus on novel, flexible techniques for creating new types of artificial words that model real words by preserving their semantics as much as possible.", "labels": [], "entities": []}, {"text": "Our semantically aware pseudowords can be used to model any word in the lexicon, 1 therefore aiming for wide coverage.", "labels": [], "entities": []}, {"text": "We perform different experiments to show that our semantically aware pseudowords are good at modeling existing ambiguous words in terms of disambiguation difficulty, representativeness, and distinguishability of the artificial senses.", "labels": [], "entities": []}, {"text": "r We leverage our semantically aware pseudowords to create, for the first time, a large-scale evaluation framework for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.8640256524085999}]}, {"text": "Using this framework, we are able to perform an experimental comparison of state-of-the-art systems for supervised and knowledge-based WSD on a very large data set made up of millions of sense-tagged sentences.", "labels": [], "entities": []}, {"text": "Our large-scale framework enables us to carryout an in-depth analysis of the factors and conditions that determine the systems' performance.", "labels": [], "entities": []}, {"text": "In our recent work (Pilehvar and Navigli 2013), we presented an approach for the generation of semantically aware pseudowords, called similarity-based pseudowords.", "labels": [], "entities": []}, {"text": "At the core of this approach was the Personalized PageRank algorithm) on the WordNet graph, which was utilized to find the most semantically similar monosemous representative fora given sense of areal ambiguous word.", "labels": [], "entities": [{"text": "WordNet graph", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9650606215000153}]}, {"text": "The main strength of the similarity-based approach lies in its flexibility, allowing high minimum frequency constraints to beset on its selection of pseudosenses, while maintaining its overall sense modeling quality.", "labels": [], "entities": []}, {"text": "In this article we extend our previous work as follows: 1) we propose anew approach for generating semantically aware pseudowords which leverages topic signatures; 2) we utilize the best type of pseudoword to create a novel framework for largescale evaluation and comparison of WSD systems; 3) based on this framework, we carryout a large-scale comparison of state-of-the-art supervised and knowledge-based WSD algorithms; and 4) we study the impact of the amount of supervision and knowledge on the two major disambiguation paradigms and perform an in-depth analysis of the factors and conditions that determine their performance.", "labels": [], "entities": []}, {"text": "The remainder of this article is organized as follows: In Section 2 we survey related work concerning the impact of the knowledge acquisition bottleneck on WSD and provide an explanation of our pseudoword-based approach.", "labels": [], "entities": [{"text": "WSD", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.8879371881484985}]}, {"text": "In Section 3 we describe pseudowords and overview the existing approaches to their generation.", "labels": [], "entities": []}, {"text": "We then present two new approaches that address the issues associated with existing pseudowords, hence enabling the wide-coverage generation of semantically aware pseudowords.", "labels": [], "entities": [{"text": "wide-coverage generation of semantically aware pseudowords", "start_pos": 116, "end_pos": 174, "type": "TASK", "confidence": 0.6681250383456548}]}, {"text": "In Section 4, we perform various experiments to assess the degree of realism of our proposed pseudowords.", "labels": [], "entities": [{"text": "realism", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9478945732116699}]}, {"text": "We then illustrate how we leverage our pseudowords to generate large sense-tagged data sets in Section 5.", "labels": [], "entities": []}, {"text": "The experimental set-up for pseudoword-based WSD is described in Section 6.", "labels": [], "entities": [{"text": "WSD", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.809810996055603}]}, {"text": "Experimental results as well as the findings are presented and discussed in Section 7.", "labels": [], "entities": []}, {"text": "Finally, we provide concluding remarks in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In Sections 3.2 and 3.3 we presented two techniques for the generation of semantically aware pseudowords that were able to address the coverage and flexibility issues of the vicinity-based approach.", "labels": [], "entities": []}, {"text": "In order to verify the ability of these pseudowords to model various properties of real ambiguous words, we performed three separate evaluations so as to assess them from different perspectives: r Disambiguation difficulty in comparison to real words, where we extrinsically study the impact of the pseudoword quality on the disambiguation performance (Section 4.1).", "labels": [], "entities": []}, {"text": "r Representative power of pseudosenses, where we assess the semantic closeness of pseudosenses to their corresponding real senses (Section 4.2).", "labels": [], "entities": []}, {"text": "r Distinguishability of pseudosenses, where we determine to what extent pseudosenses are specific to a fine-grained real sense rather than covering multiple senses (Section 4.3).", "labels": [], "entities": []}, {"text": "Given that our aim was to leverage these pseudowords for creating large-scale pseudosense-annotated data sets, we performed evaluations on pseudowords generated with minFreq per pseudosense set to a high value of 1,000 (i.e., we can generate 1,000 annotated sentences for each pseudosense) using the English Gigaword corpus (Graff and Cieri 2003).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 300, "end_pos": 323, "type": "DATASET", "confidence": 0.9063752492268881}]}, {"text": "In this article up to this point we have provided the basis for creating large-scale pseudosense-annotated data sets by proposing a flexible approach for generating semantically aware pseudowords that model arbitrary real words.", "labels": [], "entities": []}, {"text": "We have also explained different sampling strategies for distributing pseudosense-annotated sentences according to two different distributions.", "labels": [], "entities": []}, {"text": "We are now ready to setup our experimental framework for large-scale WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.8979847431182861}]}, {"text": "We first describe the text corpus used in our experiments (Section 6.1), then explain how we selected a reliable subset of pseudowords for the experiments (Section 6.2); this is followed by a description of the process of generating training and test data sets (Section 6.3).", "labels": [], "entities": []}, {"text": "In Section 6.4 we introduce the two WSD systems used as representatives of the two main WSD paradigms (i.e., supervised and knowledge-based) in our experiments.", "labels": [], "entities": []}, {"text": "We then provide, in Section 6.5, the details of the method through application of which our knowledge-based system is able to benefit from the training data.", "labels": [], "entities": []}, {"text": "Finally, in Section 6.6 we describe the evaluation measures used in our experiments.", "labels": [], "entities": []}, {"text": "Ng 2010) as the representative supervised WSD system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8755071759223938}]}, {"text": "IMS is a publicly available English all-words WSD system achieving state-of-the-art results on several Senseval and SemEval tasks.", "labels": [], "entities": [{"text": "IMS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9302452206611633}, {"text": "WSD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9125862717628479}]}, {"text": "The system classifies words in context using linear support vector machines.", "labels": [], "entities": []}, {"text": "The context (a sentence in our case) is represented as a standard vector of features including parts of speech, surrounding words, and local collocations (.", "labels": [], "entities": []}, {"text": "For each of the four configurations (see Section 6.3) and for each pseudoword, IMS was trained with the corresponding training set and the learned word expert model was then applied to the test set.", "labels": [], "entities": []}, {"text": "In our experiments, we used the default configuration of IMS where the system adopts a linear SVM classifier with L2-loss function.", "labels": [], "entities": []}, {"text": "As the state-of-the-art knowledge-based WSD system, we used UKB.", "labels": [], "entities": [{"text": "UKB", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9731751084327698}]}, {"text": "12 UKB is a publicly available graph-based WSD system that exploits a preexisting lexical knowledge base (Agirre, Lopez.", "labels": [], "entities": [{"text": "12 UKB", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7549178302288055}]}, {"text": "UKB provides an implementation of the PPR algorithm (Haveliwala 2002), adapted to the task of WSD, as proposed by . PPR is applied to a graph representation of a Lexical Knowledge Base (LKB), which is typically WordNet or an extension of it with additional semantic edges.", "labels": [], "entities": [{"text": "UKB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.988143265247345}, {"text": "WSD", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.6256945729255676}]}, {"text": "We used the w2w variant, which has been shown to perform best (Agirre and Soroa 2009), where PPR is initialized by concentrating the probability mass on the context words other than the target word to be disambiguated.", "labels": [], "entities": []}, {"text": "The most suitable sense of the latter is then chosen by selecting the highest-ranking vertex (i.e., sense) of the word.", "labels": [], "entities": []}, {"text": "Similarly to IMS, we used for UKB the corresponding training set in each trainingtest configuration.", "labels": [], "entities": [{"text": "IMS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.48037439584732056}, {"text": "UKB", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9707248210906982}]}, {"text": "However, atypical knowledge-based WSD system (such as UKB) cannot directly learn from the training data (which, instead, is naturally suited to supervised WSD systems).", "labels": [], "entities": [{"text": "UKB", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9893172383308411}]}, {"text": "In the following section we describe the method used in our experiments to transfer these data into readily available knowledge for UKB.", "labels": [], "entities": [{"text": "UKB", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.9790922403335571}]}, {"text": "Hereafter, we will use IMS and UKB to mean supervised and knowledge-based systems, respectively, since we consider these two systems as state-of-the-art representatives of their corresponding paradigms.", "labels": [], "entities": [{"text": "UKB", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.9480281472206116}]}, {"text": "It is customary in the WSD literature to evaluate the performance of a disambiguation system based on precision, recall, and F1 measure.", "labels": [], "entities": [{"text": "WSD", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.84554123878479}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9995754361152649}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9989626407623291}, {"text": "F1 measure", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9884157776832581}]}, {"text": "Precision calculates the portion of items that are correctly disambiguated from among the total output by the system, and recall measures the portion of the total items in the data set that are correctly disambiguated by the system.", "labels": [], "entities": [{"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9992431402206421}]}, {"text": "F1 is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9955728054046631}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9994508624076843}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9962784647941589}]}, {"text": "Because in our setting all the pseudowords to be disambiguated in the test set are covered in the training data and also included as anode in the LKB, IMS and UKB always provide an answer for each item in the test set.", "labels": [], "entities": [{"text": "LKB", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.9517164826393127}, {"text": "IMS", "start_pos": 151, "end_pos": 154, "type": "DATASET", "confidence": 0.8436828851699829}, {"text": "UKB", "start_pos": 159, "end_pos": 162, "type": "DATASET", "confidence": 0.8864393830299377}]}, {"text": "For such a full-coverage case, the values of precision, recall, and F1 will be equal.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9998486042022705}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.999699592590332}, {"text": "F1", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9998788833618164}]}, {"text": "Hence, in our experiments, we report the recall performance of the systems only.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9971953630447388}]}, {"text": "In addition, throughout this article, we present the results in terms of recall percentage (i.e., the value of recall multiplied by 100).", "labels": [], "entities": [{"text": "recall percentage", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.9920665919780731}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9982088804244995}]}, {"text": "As discussed in the experimental set-up, WSD experiments were carried outwith IMS and UKB while injecting an increasingly higher amount of supervision and knowledge, respectively, that is, from 0 to 800 training sentences (cf. Section 6.3).", "labels": [], "entities": [{"text": "WSD", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9755428433418274}, {"text": "IMS", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8462612628936768}, {"text": "UKB", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9554868340492249}]}, {"text": "We show the overall  Performance of IMS and UKB on the naturally distributed test set when varying the size of the training set per pseudoword (MFS for Nat-Nat = 70.5%).", "labels": [], "entities": [{"text": "UKB", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9587156176567078}, {"text": "MFS", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.7988172769546509}]}, {"text": "Note that we do not report an MFS baseline for Uni-Nat configuration as there is no most frequent sense in the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Noun coverage percentage of vicinity-based pseudowords by degree of polysemy for different  values of minimum frequency.", "labels": [], "entities": []}, {"text": " Table 3  Top five entries of the similarSynsets list for different senses of word coke (we show both  WordNet 3.0 offsets and synsets). The highest-ranking monosemous noun in each list is  shown in bold.", "labels": [], "entities": [{"text": "similarSynsets list", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.9176338613033295}, {"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9167600274085999}]}, {"text": " Table 5  Statistics of averageRank scores of similarity-based pseudowords: we show mean and mode  positions for six different values of minimum occurrence frequency (0, 200, 500, 1,000, 2,000, and  5,000) and for each polysemy degree (we show the average value in the case of multiple modes).", "labels": [], "entities": []}, {"text": " Table 6  Top five words in the topic signatures for different senses of noun coke. The first monosemous  noun for each sense is shown in bold.", "labels": [], "entities": []}, {"text": " Table 7  Recall performance of IMS on the 20 nouns of the Senseval-3 lexical-sample test set (Real  column) compared with the corresponding similarity-based (SB), TS-based (TS), and random  (Rnd) pseudowords. The last three columns show absolute differences between the real setting  and the three pseudoword settings.", "labels": [], "entities": [{"text": "Senseval-3 lexical-sample test set", "start_pos": 59, "end_pos": 93, "type": "DATASET", "confidence": 0.7853979766368866}]}, {"text": " Table 9  Percentage of similarity-based pseudosenses obtained from different types of WordNet relations.", "labels": [], "entities": []}, {"text": " Table 11  Average representativeness scores for pseudosenses of different polysemy classes (scores  range from 1 to 4) and from different WordNet relations. We also show, in the last two rows, the  average scores for only those pseudosenses that are picked from synonyms or directly related  and sibling synsets.", "labels": [], "entities": []}, {"text": " Table 12  Average distinguishability scores for pseudosenses of different polysemy classes (scores range  from 0 to 1).", "labels": [], "entities": []}, {"text": " Table 13  Number of distinct nouns annotated in SemCor at least 1, 10, or 20 times. We also show the total  number of WordNet ambiguous nouns (last row) for different polysemy degrees.", "labels": [], "entities": []}, {"text": " Table 14  Average sense distribution for nouns in SemCor. We select only those nouns for which there exist  at least 10 sense-tagged occurrences in SemCor.", "labels": [], "entities": []}, {"text": " Table 15  Number of pseudowords per degree of polysemy (2 to 12) in our test and tuning sets.", "labels": [], "entities": []}, {"text": " Table 16  Statistics of the averageRank score of the subset of pseudowords selected for our experiments:  we show mean and mode statistics for six different values of minimum occurrence frequency  (minFreq) and for each polysemy degree (average value is presented in the case of multiple  modes).", "labels": [], "entities": [{"text": "minimum occurrence frequency  (minFreq)", "start_pos": 168, "end_pos": 207, "type": "METRIC", "confidence": 0.6670061101516088}]}, {"text": " Table 17  Recall performance of UKB when varying K for all the four data set configurations. The last two  rows show the average performance for each training data distribution. The maximum values  for each configuration are shown in bold.", "labels": [], "entities": [{"text": "UKB", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9048105478286743}]}, {"text": " Table 20  Performance of UKB on Nat-Nat configuration when injected with the pseudoword-specific  (specific) and average sense distribution (average) information as well as the original  performance values for UKB and IMS (as reported earlier in", "labels": [], "entities": [{"text": "UKB", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.945853590965271}, {"text": "UKB", "start_pos": 211, "end_pos": 214, "type": "DATASET", "confidence": 0.967430591583252}]}, {"text": " Table 21  Performance of UKB when additional edges are obtained by exploiting the sentences in the test  data set. We show results for both uniformly and naturally distributed test data and for different  values of K (maximum number of related words per pseudosense used for enriching LKB).", "labels": [], "entities": [{"text": "UKB", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9440571665763855}]}]}