{"title": [{"text": "Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method", "labels": [], "entities": []}], "abstractContent": [{"text": "Linguistic steganography is concerned with hiding information in natural language text.", "labels": [], "entities": [{"text": "Linguistic steganography", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7262404561042786}, {"text": "hiding information in natural language text", "start_pos": 43, "end_pos": 86, "type": "TASK", "confidence": 0.7056876023610433}]}, {"text": "One of the major transformations used in linguistic steganography is synonym substitution.", "labels": [], "entities": [{"text": "linguistic steganography", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.606945663690567}, {"text": "synonym substitution", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.9675230979919434}]}, {"text": "However, few existing studies have studied the practical application of this approach.", "labels": [], "entities": []}, {"text": "In this article we propose two improvements to the use of synonym substitution for encoding hidden bits of information.", "labels": [], "entities": [{"text": "synonym substitution", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.8167486786842346}]}, {"text": "First, we use the Google n-gram corpus for checking the applicability of a synonym in context, and we evaluate this method using data from the SemEval lexical substitution task and human annotated data.", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 18, "end_pos": 38, "type": "DATASET", "confidence": 0.7991802096366882}, {"text": "SemEval lexical substitution task", "start_pos": 143, "end_pos": 176, "type": "TASK", "confidence": 0.8119712620973587}]}, {"text": "Second, we address the problem that arises from words with more than one sense, which creates a potential ambiguity in terms of which bits are represented by a particular word.", "labels": [], "entities": []}, {"text": "We develop a novel method in which words are the vertices in a graph, synonyms are linked by edges, and the bits assigned to a word are determined by a vertex coding algorithm.", "labels": [], "entities": []}, {"text": "This method ensures that each word represents a unique sequence of bits, without cutting out large numbers of synonyms, and thus maintains a reasonable embedding capacity.", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to transmit information through an open channel without detection by anyone other than the receiver, a covert channel can be used.", "labels": [], "entities": []}, {"text": "In information theory, a covert channel is a parasitic communications channel that is hidden within the medium of a legitimate communication channel.", "labels": [], "entities": [{"text": "information theory", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7490557432174683}]}, {"text": "For example, steganography is a form of covert channel in which certain properties of the cover medium are manipulated in an unexpected, unconventional, or unforeseen way so that, with steganographic transmission, the encrypted messages can be camouflaged in a seemly innocent medium and sent to the receiver with less chance of being suspected and attacked.", "labels": [], "entities": []}, {"text": "Ideally, because the changes to the medium are so subtle, anyone not specifically looking fora hidden message is unlikely to notice the changes.", "labels": [], "entities": []}, {"text": "In this article, we aim at concealing secret information in natural language text by manipulating cover words.", "labels": [], "entities": [{"text": "concealing secret information in natural language text", "start_pos": 27, "end_pos": 81, "type": "TASK", "confidence": 0.7272288373538426}]}, {"text": "The proposed steganography system replaces selected cover words with their synonyms, which is the mechanism used to embed information.", "labels": [], "entities": []}, {"text": "In order to ensure the lexical substitutions in the cover text are imperceptible, the system uses the Google n-gram corpus) for checking the applicability of a synonym in context.", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 102, "end_pos": 122, "type": "DATASET", "confidence": 0.8826631704966227}]}, {"text": "In addition, the system assigns codes to acceptable substitutes using a novel vertex coding method in which words are represented as vertices in a graph, synonyms are linked by edges, and the bits assigned to a vertex represent the code of a particular word.", "labels": [], "entities": []}, {"text": "Our lexical substitution-based steganography system was previously published in, in which the system was evaluated automatically by using data from the English lexical substitution task for In this article, we extend the previous work by more closely addressing the practical application of the proposed system.", "labels": [], "entities": []}, {"text": "We present results from anew human evaluation of the system's output and a simple computational steganalysis of wordfrequency statistics which is a more direct evaluation for linguistic steganography.", "labels": [], "entities": []}, {"text": "We also give an extended literature review which will serve as a useful introduction to linguistic steganography for those Computational Linguistics readers not familiar with the problem.", "labels": [], "entities": [{"text": "linguistic steganography", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.6895421296358109}]}], "datasetContent": [{"text": "So far we have introduced different linguistic transformations used to produce alternatives fora cover text as well as some encoding methods that are used to assign a bitstring to a candidate.", "labels": [], "entities": []}, {"text": "The final procedure is text selection, in which an alternative that represents the secret bits is chosen as the stego text.", "labels": [], "entities": [{"text": "text selection", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.7754068970680237}]}, {"text": "We can see that the quality of a stego text mainly relies on the quality of the applied linguistic transformation, typically requiring sophisticated NLP tools and resources to produce a realistic stego text.", "labels": [], "entities": []}, {"text": "However, given the current state-of-the-art, such NLP techniques cannot guarantee the transformation's imperceptibility.", "labels": [], "entities": []}, {"text": "Hence it is important to evaluate a stegosystem.", "labels": [], "entities": []}, {"text": "A stegosystem can be evaluated from two aspects: the security level and the embedding capacity.", "labels": [], "entities": []}, {"text": "The security assessment methods used so far can be classified into two categories: automatic evaluation and human evaluation.", "labels": [], "entities": [{"text": "security assessment", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8297248482704163}]}, {"text": "Topkara, Topkara, and Atallah (2006a) and used machine translation evaluation metrics BLEU ( and NIST (Doddington 2002), automatically measuring how close a stego sentence is to the original.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.7297028203805288}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8810706734657288}, {"text": "NIST (Doddington 2002)", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.8450685262680053}]}, {"text": "admitted that machine translation evaluation metrics are not sufficient for evaluating stegosystems; for example, BLEU relies on word sequences in the stego sentence matching those in the cover sentence and thus is not suitable for evaluating transformations that change the word order significantly.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.8523603081703186}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9922143816947937}]}, {"text": "The other widely adopted evaluation method is based on human judgments. and) asked participants to edit stego text for improving intelligibility and style.", "labels": [], "entities": []}, {"text": "The fewer edit-hits a transformed text received, the higher the reported security level.) first asked subjects to rate the acceptability (in terms of plausibility, grammaticality, and style) of the stego sentences on a seven-point scale.", "labels": [], "entities": []}, {"text": "Then participants were provided with the originals and asked to judge to what extent meaning was preserved, also on a seven-point scale.", "labels": [], "entities": []}, {"text": "In Chang and Clark (2010a) we asked participants to judge whether a paraphrased sentence is grammatical and whether the paraphrasing retains the meaning of the original.", "labels": [], "entities": []}, {"text": "In Chang and Clark (2012a) we asked participants to annotate the naturalness of the resulting sentences after adjective deletions; and in we asked participants to rate the naturalness of sentence permutations on a four-point scale.", "labels": [], "entities": []}, {"text": "For the work presented in this article, we also use human judgments to evaluate the proposed stegosystem, as this is close to the linguistic steganography scenario where we assume the adversary is a human acting passively.", "labels": [], "entities": []}, {"text": "The other aspect of the stegosystem evaluation is to calculate the amount of data capable of being embedded in a stego text, which can be quantified in terms of bits of hidden message per bit transmitted or per language unit (e.g., per word or per sentence).", "labels": [], "entities": []}, {"text": "Payload measurements can be theoretical or empirical.", "labels": [], "entities": []}, {"text": "The theoretical payload measurement only depends on an encoding method and is independent of the quality of a stego text; the empirical measurement takes the applicability of a linguistic transformation, namely, the security of a stego text, into consideration and measures the payload capacity while a certain security level is achieved.", "labels": [], "entities": []}, {"text": "Most of the payload rates reported in existing work are based on empirical measurements.", "labels": [], "entities": []}, {"text": "For the lexical substitution transformation, Topkara, Taskiran, and and Topkara, Topkara, and Atallah (2006b) achieved an average embedding payload of 0.67 bits per sentence, despite the large number of synonyms in English.", "labels": [], "entities": [{"text": "lexical substitution transformation", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.7393483916918436}]}, {"text": "In  Not only the linguistic transformation and the encoding method, but also the choice of cover text, can affect the security level and the payload capacity of a stegosystem.", "labels": [], "entities": [{"text": "linguistic transformation", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.7242524325847626}]}, {"text": "For example, if a newspaper article were chosen as the cover text, then any changes could be easily found in practice by comparing the stego text with the original article, which is likely to be readily available.", "labels": [], "entities": []}, {"text": "In addition, an anomaly introduced by a linguistic transformation maybe more noticeable in a newspaper article than in a blog article.", "labels": [], "entities": []}, {"text": "In terms of payload capacity, a synonym substitution-based stegosystem may find more words that can be substituted in a storybook than in a car repair manual because there are usually many terminologies in a manual which cannot be changed or even cannot be found in a standard dictionary (assuming the system does not happen to have a detailed ontology of car parts).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no study on the practical issue of using different types of cover text for the steganography application.", "labels": [], "entities": [{"text": "steganography", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.9733018279075623}]}, {"text": "Both NGM and NGM DVG assign a score to a word according to the context and the most likely word in the group of alternatives.", "labels": [], "entities": [{"text": "NGM", "start_pos": 5, "end_pos": 8, "type": "DATASET", "confidence": 0.8237963914871216}]}, {"text": "In order to evaluate the performance of the proposed scoring methods, we apply our approaches to a ranking task that requires a system to rank a list of substitute words given an original word and its context.", "labels": [], "entities": []}, {"text": "The task can test whether the proposed methods are capable of assigning higher scores to appropriate substitutes than to unacceptable ones and thus is useful for the steganography application.", "labels": [], "entities": [{"text": "steganography", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.9713581204414368}]}, {"text": "The gold standard data is derived from the English lexical substitution task for and the evaluation measure used is Generalized Average Precision.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.6949570973714193}, {"text": "Generalized Average Precision", "start_pos": 116, "end_pos": 145, "type": "METRIC", "confidence": 0.9513140718142191}]}, {"text": "In this section we first describe the gold standard data used in this evaluation and then provide the results.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.6835005680720011}]}, {"text": "We compare our results with three other models developed by,, and\u00b4Oand\u00b4 and\u00b4O, all of which are designed for measuring word meaning similarity in context.", "labels": [], "entities": []}, {"text": "Note that our substitution checkers do not aim at modeling word similarity, and therefore the result comparison is just trying to show that our substitution checkers are competitive.", "labels": [], "entities": []}, {"text": "Later, we will evaluate the proposed checkers with the human annotated data and see whether our methods would be practical for linguistic steganography.", "labels": [], "entities": [{"text": "linguistic steganography", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.6937708854675293}]}, {"text": "For this evaluation, we use the SemEval-2007 lexical substitution data set as the gold standard.", "labels": [], "entities": [{"text": "SemEval-2007 lexical substitution data set", "start_pos": 32, "end_pos": 74, "type": "DATASET", "confidence": 0.7443077802658081}]}, {"text": "The original purpose of the data set was to develop systems that can automatically find feasible substitutes given a target word in context.", "labels": [], "entities": []}, {"text": "The human annotation data comprises 2,010 sentences selected from the English Internet Corpus), and consists of 201 target words: nouns, verbs, adjectives, and adverbs, each with ten sentences containing that word.", "labels": [], "entities": [{"text": "English Internet Corpus", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.9498330950737}]}, {"text": "The five annotators were asked to provide up to three substitutes fora target word in the context of a sentence, and were permitted to consult a dictionary or thesaurus of their choosing.", "labels": [], "entities": []}, {"text": "After filtering out annotation sentences where the target word is part of a proper name and for which annotators could not think of a good substitute, the data was separated into 298 trial sentences and 1,696 test sentences.", "labels": [], "entities": []}, {"text": "illustrates two examples from the gold standard, both featuring the target word bright.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9032680988311768}]}, {"text": "The right column lists appropriate substitutes of bright in each context, and the numbers in parentheses indicate the number of annotators who provided that substitute.", "labels": [], "entities": []}, {"text": "To allow comparison with previous results reported on the substitution ranking task, following, and\u00b4Oand\u00b4 and\u00b4O S\u00e9aghdha and Korhonen (2011), we pool together the positive substitutes for each target word, considering all contexts, and rank the substitutes using our scoring methods.", "labels": [], "entities": []}, {"text": "For instance, assume in the gold standard there are only two sentences containing the target word bright as shown in.", "labels": [], "entities": []}, {"text": "We merge all the substitutes of bright given by the annotators and derive a large candidate pool {intelligent, clever, colorful, brilliant, gleam, luminous}.", "labels": [], "entities": []}, {"text": "We expect intelligent and clever to be ranked at the top of the list for the first sentence, with colorful, brilliant, gleam, and luminous ranked at the top for the second sentence.", "labels": [], "entities": []}, {"text": "In the SemEval-2007 lexical substitution task participants were asked to discover possible replacements of a target word so the evaluation metrics provided are designed to give credit for each correct guess and do not take the ordering of the guesses into account.", "labels": [], "entities": [{"text": "SemEval-2007 lexical substitution task", "start_pos": 7, "end_pos": 45, "type": "TASK", "confidence": 0.8760228753089905}]}, {"text": "In contrast, in the ranking task a system is already given a fixed pool of substitutes and is asked to recover the order of the list.", "labels": [], "entities": []}, {"text": "Therefore, we use the Generalized Average Precision (GAP) to evaluate the ranked lists rather than the metrics provided in the SemEval-2007 lexical substitution task.", "labels": [], "entities": [{"text": "Generalized Average Precision (GAP)", "start_pos": 22, "end_pos": 57, "type": "METRIC", "confidence": 0.9301879405975342}, {"text": "SemEval-2007 lexical substitution task", "start_pos": 127, "end_pos": 165, "type": "TASK", "confidence": 0.7661163210868835}]}, {"text": "GAP rewards correctly ranked items with respect to their gold standard weights while the traditional average precision is only sensitive to the relative positions of correctly and incorrectly ranked items.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.8489384055137634}]}, {"text": "Let G = g 1 , g 2 , ..., gm be the list of gold substitutions with weights y 1 , y 2 , ..., y m fora target word in context.", "labels": [], "entities": []}, {"text": "In our task, the weight is the frequency of a substitute in the gold standard.", "labels": [], "entities": [{"text": "frequency", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9712309241294861}, {"text": "gold standard", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9200884699821472}]}, {"text": "Let S = s 1 , s 2 , ..., s n be the system ranked substitute list and x 1 , x 2 , ..., x n be the weights associated with them, where m \u2264 n and xi = 0 if s i is not in the gold list and G \u2286 S.", "labels": [], "entities": []}, {"text": "Then where I(x i ) = 1 if xi is larger than zero, zero otherwise; \u00af xi is the average gold weight of the first i system ranked items; \u00af y i is defined analogously.", "labels": [], "entities": []}, {"text": "After experimenting on the trial data, we decided a \u03bb value of 0.6 for the NGM DVG method.", "labels": [], "entities": [{"text": "NGM DVG", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.807624340057373}]}, {"text": "We then applied the proposed NGM and NGM DVG methods to rank pooled substitutes for each sentence in the test data.", "labels": [], "entities": [{"text": "NGM DVG", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7614665329456329}]}, {"text": "summarizes the performances of our approaches, where mean GAP values are reported on the whole test data as well as for different POSs.", "labels": [], "entities": [{"text": "GAP", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9748805165290833}]}, {"text": "We can see that the NGM DVG performs better than the NGM system on the ranking task and achieved a mean GAP of 50.8% on the whole test Two sentences in the SemEval-2007 lexical substitution gold standard.", "labels": [], "entities": [{"text": "NGM DVG", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.8629684746265411}, {"text": "GAP", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9452560544013977}, {"text": "SemEval-2007 lexical substitution gold standard", "start_pos": 156, "end_pos": 203, "type": "DATASET", "confidence": 0.7023256361484528}]}, {"text": "Although the ranking task evaluation gives some indication of how reliable the proposed scoring methods are, for the steganography application we require a system that can correctly distinguish acceptable substitutes from unacceptable ones.", "labels": [], "entities": []}, {"text": "Thus, we conduct a classification task evaluation which is more related to the steganography application.", "labels": [], "entities": [{"text": "classification task", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8984112739562988}]}, {"text": "The task requires a system to determine acceptable substitutes from a group of candidates given the word to be replaced and its context.", "labels": [], "entities": []}, {"text": "Those passed substitutes can then carry different codes and be used as stego words.", "labels": [], "entities": []}, {"text": "Similar to the previous section, we first describe the data and then explain the experimental setup and the evaluation results.", "labels": [], "entities": []}, {"text": "We evaluate the classification performance of the NGM system and the NGM DVG system in terms of accuracy, precision, and recall.", "labels": [], "entities": [{"text": "NGM system", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.9141105711460114}, {"text": "NGM DVG system", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9324105381965637}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9996416568756104}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9993946552276611}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9993846416473389}]}, {"text": "Accuracy is the percentage of correct classification decisions overall acceptable and unacceptable substitutes; precision is the percentage of system accepted substitutes being humanprovided; recall is the percentage of human-provided substitutes being accepted by the system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9947787523269653}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9996181726455688}, {"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9995680451393127}]}, {"text": "Accuracy is less important for the steganography application, and the reasons for using precision and recall were explained in Section 1.4: A higher precision value implies a better security level, and a larger recall value means a greater payload capacity.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.992968738079071}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9992676377296448}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9985684156417847}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9857169389724731}, {"text": "recall", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9939730763435364}]}, {"text": "It is worth noting that, although there will be a decrease in recall if more false negatives are obtained from a system, there will not be a negative effect on the value of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9995449185371399}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9981202483177185}]}, {"text": "That is, from a security perspective, rejecting an acceptable substitute does not damage the quality of stego text.", "labels": [], "entities": []}, {"text": "However, it will lower the payload capacity so more stego text transmission is needed in order to send the secret message, which may raise a security concern.", "labels": [], "entities": []}, {"text": "Both the NGM system and the NGM DVG system require a threshold to decide whether a word is acceptable in context.", "labels": [], "entities": [{"text": "NGM system", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.8903736174106598}, {"text": "NGM DVG system", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.895905335744222}]}, {"text": "In order to derive sensible threshold values for each POS, five-fold cross validation was used for the experiments.", "labels": [], "entities": []}, {"text": "For each fold, 80% of the data is used to find the threshold value which maximizes the accuracy, and that threshold is then applied to the remaining 20% to get the final result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9994309544563293}]}, {"text": "We first test whether the proposed methods would benefit from using only longer n-grams.", "labels": [], "entities": []}, {"text": "We compare the performance of different combinations of n-gram counts, which are frequency counts of bi-to five-grams, tri-to five-grams, four-to five-grams, and five-grams only.", "labels": [], "entities": []}, {"text": "The results show that for both methods the accuracy, precision, and recall values drop when using fewer n-grams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9996508359909058}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.99935382604599}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9996607303619385}]}, {"text": "In other words, among the four combinations, the one including bigram to five-gram frequency counts performs the best across different POS and, therefore, is adopted in the NGM system and the NGM DVG system.", "labels": [], "entities": [{"text": "NGM system", "start_pos": 173, "end_pos": 183, "type": "DATASET", "confidence": 0.9158409535884857}, {"text": "NGM DVG system", "start_pos": 192, "end_pos": 206, "type": "DATASET", "confidence": 0.9212711254755656}]}, {"text": "Next, we try weighting different sized n-grams in the proposed methods, as have Bergsma, Lin, and Goebel (2009) in related work.", "labels": [], "entities": []}, {"text": "According to the preliminary experiments we conducted and the conclusion given by Bergsma, Lin, and Goebel, such a method does not do much better than the simple method using uniform weights for different sized n-grams.", "labels": [], "entities": []}, {"text": "gives the results for the two checking methods and the average threshold values over the five folds.", "labels": [], "entities": []}, {"text": "In addition, for each POS, a simple baseline is derived by always saying a substitute is acceptable.", "labels": [], "entities": []}, {"text": "From the table we can see that both the NGM and the NGM DVG systems have higher precision than the baseline, which performs well in terms of embedding capacity (100% recall) but at the expense of a lower security level (lower precision).", "labels": [], "entities": [{"text": "NGM", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9227558970451355}, {"text": "NGM DVG", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.875782698392868}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9987267851829529}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9980146884918213}, {"text": "precision", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.996813952922821}]}, {"text": "However, in contrast to the results of the ranking task evaluation, the NGM system slightly outperforms the NGM DVG system.", "labels": [], "entities": []}, {"text": "Because imperceptibility is an important issue for steganography, we would prefer a system with a higher precision value.", "labels": [], "entities": [{"text": "steganography", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9766256809234619}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9960566759109497}]}, {"text": "Thus we adopt the NGM method as the linguistic transformation checker in our lexical substitution-based stegosystem.", "labels": [], "entities": [{"text": "linguistic transformation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6866935044527054}]}, {"text": "In addition, we are interested in the effect of the threshold value on the performance of the NGM method.", "labels": [], "entities": []}, {"text": "shows the precision and recall values with respect to different thresholds for each POS.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995449185371399}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9991341233253479}]}, {"text": "From the charts we can clearly seethe trade-off between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9995743632316589}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9969637989997864}]}, {"text": "Although a higher precision can be achieved by using a higher threshold value-for example, noun substitutions reach almost 90% precision with threshold equal to 0.9-the large drop in recall means many applicable substitutes are being eliminated.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9987407326698303}, {"text": "noun substitutions", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7732826173305511}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9959237575531006}, {"text": "recall", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9989883303642273}]}, {"text": "In other words, the trade-off between precision and recall implies the trade-off between imperceptibility and payload capacity for linguistic steganography.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9989529848098755}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9936410784721375}]}, {"text": "Therefore, the practical threshold setting would depend on how steganography users want to trade off imperceptibility for payload.", "labels": [], "entities": []}, {"text": "So far we have presented the performance of our checking methods using two different automatic evaluations, the ranking task and the classification task.", "labels": [], "entities": []}, {"text": "From the ranking task evaluation we can see that the n-gram distributional similarity does have the ability to further eliminate some bad substitutes after applying the basic n-gram  We want to test how reliable the proposed NGM method is if it is used in a lexical substitution-based stegosystem to guard against inappropriate substitutions.", "labels": [], "entities": []}, {"text": "Therefore, apart from the automatic evaluations, we conducted a more direct evaluation of the imperceptibility for the steganography application by asking human judges to evaluate the naturalness of sentences.", "labels": [], "entities": [{"text": "steganography application", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.9318411648273468}]}, {"text": "In the following sections, we explain the evaluation data first and then describe the evaluation setup and results.", "labels": [], "entities": []}, {"text": "We collected a total of 60 sentences from Robert Peston's BBC blog.", "labels": [], "entities": [{"text": "Robert Peston's BBC blog", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.7942822813987732}]}, {"text": "8 For each noun, verb, adjective, and adverb in a sentence, we first group the target word's synset(s) in WordNet and apply the NGM method with a score threshold equal to 0.95 to eliminate bad substitutes.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9641727805137634}]}, {"text": "If more than one substitute passes the check, the one with the lowest score is used to replace the original word.", "labels": [], "entities": []}, {"text": "The reason for choosing the word with the lowest score is because this makes the test more challenging.", "labels": [], "entities": []}, {"text": "This process is applied to a sentence where possible and results in around two changes being made per sentence.", "labels": [], "entities": []}, {"text": "We also generated another version of a sentence changed by random choice of a target word and random choice of a substitute from a target word's synset(s) (in order to provide a baseline comparison).", "labels": [], "entities": []}, {"text": "The number of changes made to a sentence using this random method is the same as that in the version generated by the NGM method.", "labels": [], "entities": []}, {"text": "In this way, it is fair to compare the qualities of the two modified versions because both of them receive the same number of substitutions.", "labels": [], "entities": []}, {"text": "shows lexical substituted sentences generated by our method and by the random method.", "labels": [], "entities": []}, {"text": "We can see that our system replaces four words (in boldface) in the original sentence so the same number of words (in boldface) are randomly selected when applying the random method.", "labels": [], "entities": []}, {"text": "Note that the random method just happens to pick the word big in the original sentence which is also replaced by our system.", "labels": [], "entities": []}, {"text": "We refer to an original sentence as COVER, aversion generated by our method as SYSTEM, and aversion modified by the random method as RANDOM.", "labels": [], "entities": [{"text": "COVER", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9233489632606506}, {"text": "RANDOM", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.5086072087287903}]}, {"text": "The experimental setup follows a Latin square design) with three groups of 10 native English speakers as shown in.", "labels": [], "entities": []}, {"text": "In this table, each row represents a set of annotation sentences fora group of judges, and we can see that each sentence is presented in three different conditions: COVER, SYSTEM, and RANDOM, as shown in a column.", "labels": [], "entities": [{"text": "COVER", "start_pos": 165, "end_pos": 170, "type": "METRIC", "confidence": 0.9906837940216064}, {"text": "SYSTEM", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9136538505554199}, {"text": "RANDOM", "start_pos": 184, "end_pos": 190, "type": "METRIC", "confidence": 0.9677859544754028}]}, {"text": "Subjects in the same group receive the 60 sentences under the same set of conditions, and each subject sees each sentence only once in one of the three conditions.", "labels": [], "entities": []}, {"text": "The annotation process is Web-based.", "labels": [], "entities": []}, {"text": "At the beginning of the annotation task, we describe the aim of the annotation as shown in in the Appendix.", "labels": [], "entities": []}, {"text": "Subjects are asked to rate the naturalness of each sentence Different versions of a cover sentence.", "labels": [], "entities": []}, {"text": "In order to calculate word frequency counts, words in both the cover text and the stego text are first lemmatized using the tools.", "labels": [], "entities": []}, {"text": "In each piece of text, the obtained word frequency count is then mapped to a rank r between 1 and 15, where rank 1 corresponds to low frequency and 15 corresponds to high frequency.", "labels": [], "entities": []}, {"text": "The mapping is defined as: where max and min is the maximum and minimum word frequency counts in the text, respectively, and the rank of max is 15.", "labels": [], "entities": []}, {"text": "Next, we calculate the number of words for each frequency rank in each text.", "labels": [], "entities": []}, {"text": "Finally, we calculate the average number of words for each frequency rank for both the cover text and the stego text as shown in, where the average number of words has been multiplied by r 2 like the results reported by.", "labels": [], "entities": []}, {"text": "From we can seethe two vectors are very close, and the only apparent difference is that in the stego text there are more low-frequency words than in the cover text.", "labels": [], "entities": []}, {"text": "However, unlike the conclusion derived by, our results do not show a substantial difference in the frequency of high-frequency words between the cover text and the stego text.", "labels": [], "entities": []}, {"text": "A possible explanation is that a translation-based stegosystem may combine translations output from different machine translation systems, hence the usage of frequent words may not be consistent, whereas our stegosystem uses the same substitution checker for each synonym replacement so there is a certain consistency achieved in stego text.", "labels": [], "entities": []}, {"text": "After giving both the results of human and computational evaluations in the previous sections, now we would like to show an example of what atypical stego text will look like.", "labels": [], "entities": []}, {"text": "The following paragraphs are generated by the proposed NGM method with the substitution score threshold equal to 0.9, where 24 words have been substituted: The whistleblower, who yesterday gave me the entire recording, told me that the Telegraph's deletion of these sections about Mr Murdoch was a commercial decision, prompted by the fact that the Telegraph -like Mr. Cable -would rather News Corporation does not end up as 100% owner of BskyB.", "labels": [], "entities": [{"text": "Telegraph's deletion of these sections about Mr Murdoch", "start_pos": 236, "end_pos": 291, "type": "TASK", "confidence": 0.4371905028820038}, {"text": "BskyB", "start_pos": 439, "end_pos": 444, "type": "DATASET", "confidence": 0.9322028160095215}]}, {"text": "I of course set this to the Telegraph.", "labels": [], "entities": [{"text": "the Telegraph", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.7585388720035553}]}, {"text": "And quite late in the day, at 19:19 last night to be accurate, the Telegraph's external media adviser sent me a statement attributed to an unidentified \"spokesman for the Daily Telegraph.\"", "labels": [], "entities": [{"text": "the Telegraph's external media adviser", "start_pos": 63, "end_pos": 101, "type": "DATASET", "confidence": 0.76487069328626}, {"text": "Daily Telegraph", "start_pos": 171, "end_pos": 186, "type": "DATASET", "confidence": 0.8660756945610046}]}, {"text": "The statement reads: \"It is complete nonsense to suggest that the Daily Telegraph did not publish comments from Vince Cable on the Rupert Murdoch takeover of BSkyB for commercial reasons.", "labels": [], "entities": []}, {"text": "It was an editorial decision to focus this morning on Cable's comments on the Coalition because they cost of wider interest to our readers.\"", "labels": [], "entities": [{"text": "Cable's comments on the Coalition", "start_pos": 54, "end_pos": 87, "type": "DATASET", "confidence": 0.9190523525079092}]}, {"text": "Well, some would say that was a somewhat eccentric editorial decision for an editor, Tony Gallagher, widely regarded as one of the sharpest in the business.", "labels": [], "entities": []}, {"text": "I rang Mr. Gallagher to discuss this, but he directed me to the Telegraph's national PR spokesperson.", "labels": [], "entities": [{"text": "the Telegraph's national PR spokesperson", "start_pos": 60, "end_pos": 100, "type": "DATASET", "confidence": 0.8989244202772776}]}, {"text": "Also, you may have found that the Telegraph has not even putout any clear and unequivocal statement that it was ever planning to publish Mr Cable's remarks about Mr Murdoch (though it has now released them, after they were set out by the BBC).", "labels": [], "entities": [{"text": "the Telegraph", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.7114989459514618}]}, {"text": "Maybe I am being a bit naive and ridiculous to think any of this matters.", "labels": [], "entities": []}, {"text": "Maybe most of you believe that what we do as reporters is so plain and constantly subject to commercial interference that there is no special benefit to be gained from asking the Telegraph to explain itself in this case.", "labels": [], "entities": []}, {"text": "But really that's not been my experience in 27 years as a hack.", "labels": [], "entities": []}, {"text": "And I still think the question of what news organisations put into the public domain, and how they do it, matters.", "labels": [], "entities": []}, {"text": "Readers can consult Appendix B for those 24 changes made by the proposed NGM method.", "labels": [], "entities": [{"text": "Appendix B", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9369895160198212}]}], "tableCaptions": [{"text": " Table 4  GAP values (%) of the ranking task evaluation.", "labels": [], "entities": [{"text": "GAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9501254558563232}]}, {"text": " Table 5  Statistics of experimental data.", "labels": [], "entities": []}, {"text": " Table 7  Annotation results for negative data.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9771799445152283}]}, {"text": " Table 8  Performance of the NGM and NGM DVG systems on the classification task.", "labels": [], "entities": [{"text": "NGM", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9177305698394775}, {"text": "NGM DVG", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.8073354959487915}, {"text": "classification task", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.9364033937454224}]}, {"text": " Table 11  The average number of words for each word frequency rank (rank 1 is the lowest frequency and  rank 15 is the highest frequency).", "labels": [], "entities": []}, {"text": " Table 12  Statistics of synsets used in our stegosystem.", "labels": [], "entities": []}]}