{"title": [], "abstractContent": [{"text": "Authorship attribution deals with identifying the authors of anonymous texts.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6907916516065598}]}, {"text": "Traditionally, research in this field has focused on formal texts, such as essays and novels, but recently more attention has been given to texts generated by on-line users, such as e-mails and blogs.", "labels": [], "entities": []}, {"text": "Authorship attribution of such on-line texts is a more challenging task than traditional authorship attribution, because such texts tend to be short, and the number of candidate authors is often larger than in traditional settings.", "labels": [], "entities": [{"text": "Authorship attribution of such on-line texts", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.865028977394104}, {"text": "authorship attribution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7309427112340927}]}, {"text": "We address this challenge by using topic models to obtain author representations.", "labels": [], "entities": []}, {"text": "In addition to exploring novel ways of applying two popular topic models to this task, we test our new model that projects authors and documents to two disjoint topic spaces.", "labels": [], "entities": []}, {"text": "Utilizing our model in authorship attribution yields state-of-the-art performance on several data sets, containing either formal texts written by a few authors or informal texts generated by tens to thousands of on-line users.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7200462520122528}]}, {"text": "We also present experimental results that demonstrate the applicability of topical author representations to two other problems: inferring the sentiment polarity of texts, and predicting the ratings that users would give to items such as movies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Authorship attribution has attracted much attention due to its many applications in, for example, computer forensics, criminal law, military intelligence, and humanities research.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7880861163139343}]}, {"text": "The traditional problem, which is the focus of this article, is to attribute anonymous test texts to one of a set of known candidate authors, whose training texts are supplied in advance (i.e., supervised classification).", "labels": [], "entities": []}, {"text": "Whereas most of the early work on authorship attribution focused on formal texts with only a few candidate authors, researchers have recently turned their attention to scenarios involving informal texts and tens to thousands of authors.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7985750138759613}]}, {"text": "In parallel, topic models have gained popularity as a means of discovering themes in such large text corpora.", "labels": [], "entities": []}, {"text": "This article explores authorship attribution with topic models, extending the work presented by Seroussi and colleagues by reporting additional experimental results and applications of topic-based author representations that go beyond traditional authorship attribution.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7384218275547028}]}, {"text": "Topic models work by defining a probabilistic representation of the latent structure of corpora through latent factors called topics, which are commonly associated with distributions over words.", "labels": [], "entities": []}, {"text": "For example, in the popular Latent Dirichlet Allocation (LDA) topic model, each document is associated with a distribution over topics, and each word in the document is generated according to its topic's distribution over words.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA) topic", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.6692456943648202}]}, {"text": "The word distributions often correspond to a human-interpretable notion of topics, but this is not guaranteed, as interpretability depends on the corpus used for training the model.", "labels": [], "entities": []}, {"text": "Indeed, when we ran LDA on a data set of movie reviews and message board posts, we found that some word distributions correspond to authorship style as reflected by authors' vocabulary, with netspeak words such as \"wanna,\" \"alot,\" and \"haha\" assigned to one topic, and words such as \"compelling\" and \"beautifully\" assigned to a different topic.", "labels": [], "entities": []}, {"text": "This finding motivated our use of LDA for authorship attribution . One limitation of LDA is that it does not model authors explicitly.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7405926585197449}]}, {"text": "This led us to use Author-Topic (AT) model to obtain improved authorship attribution results.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7772078216075897}]}, {"text": "However, AT is also limited in that it does not model documents.", "labels": [], "entities": [{"text": "AT", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.8312210440635681}]}, {"text": "We addressed this limitation through the Disjoint Author-Document Topic (DADT) model-a topic model that draws on the strengths of LDA and AT, while addressing their limitations by integrating them into a single model.", "labels": [], "entities": []}, {"text": "Our DADT model extends the model introduced by, which could only be trained on single-authored texts.", "labels": [], "entities": []}, {"text": "In this article, we provide a detailed account of the extended model.", "labels": [], "entities": []}, {"text": "In addition, we offer experimental results for five data sets, extending the results by, which were restricted to two data sets of informal texts with many authors.", "labels": [], "entities": []}, {"text": "Our experiments show that DADT yields state-of-the-art performance on these data sets, which contain either formal texts written by a few authors or informal texts where the number of candidate authors ranges from 62 to about 20,000.", "labels": [], "entities": []}, {"text": "Although our evaluation is focused on single-authored texts, AT and DADT can also be used to model authors based on multi-authored texts, such as research papers.", "labels": [], "entities": [{"text": "AT", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9958325028419495}, {"text": "DADT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9401931166648865}]}, {"text": "To demonstrate the potential utility of this capability of the models, we present the results of a preliminary study, where we use AT and DADT to identify anonymous reviewers based on publicly available information (reviewer lists and the reviewers' publications, which are often multi-authored).", "labels": [], "entities": [{"text": "AT", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.9847276210784912}, {"text": "DADT", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.841369092464447}]}, {"text": "Our results indicate that reviewers maybe identified with moderate accuracy, at least in small conference tracks and workshops.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.998723566532135}]}, {"text": "We hope that these results will help fuel discussions on the issue of reviewer anonymity.", "labels": [], "entities": []}, {"text": "Our finding that topic models yield good authorship attribution performance indicates that they capture aspects of authorship style, which is known to be indicative of author characteristics such as demographic information and personality traits ().", "labels": [], "entities": []}, {"text": "This is in addition to the well-established result that topic models can be used to represent authors' interests (Rosen-).", "labels": [], "entities": []}, {"text": "An implication of these results is that topic models maybe used to obtain text-based representations of users in scenarios where user-generated texts are available.", "labels": [], "entities": []}, {"text": "We demonstrate this by showing how topic models can be utilized to improve the performance of methods we developed to address the popular tasks of polarity inference and rating prediction.", "labels": [], "entities": [{"text": "rating prediction", "start_pos": 170, "end_pos": 187, "type": "TASK", "confidence": 0.840022623538971}]}, {"text": "This article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 surveys related work.", "labels": [], "entities": []}, {"text": "Section 3 discusses LDA, AT, and DADT and the author representations they yield.", "labels": [], "entities": [{"text": "LDA", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.7316034436225891}, {"text": "AT", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9829295873641968}, {"text": "DADT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9376325011253357}]}, {"text": "Section 4 introduces authorship attribution methods, which are evaluated in Section 5.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8173572719097137}]}, {"text": "Section 6 presents applications of our topic-based approach, and Section 7 concludes the article.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the results of our evaluation.", "labels": [], "entities": []}, {"text": "We first describe the data sets we used (Section 5.1) and our experimental setup (Section 5.2), followed by the results of our experiments on the Judgment and PAN'11 data sets (Section 5.3).", "labels": [], "entities": [{"text": "Judgment and PAN'11 data sets", "start_pos": 146, "end_pos": 175, "type": "DATASET", "confidence": 0.8326903223991394}]}, {"text": "Then, we present the results of a more restricted set of experiments on the larger IMDb62, IMDb1M, and Blog data sets (Section 5.4) and summarize our key findings (Section 5.5).", "labels": [], "entities": [{"text": "IMDb62", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9311493635177612}, {"text": "IMDb1M", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.7903892993927002}, {"text": "Blog data sets", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.9568514823913574}]}, {"text": "We used different experimental setups, depending on the data set.", "labels": [], "entities": []}, {"text": "PAN'11 experiments followed the setup of the PAN'11 competition (Argamon and Juola 2011): We trained all the methods on the given training data set, tuned the parameters according to results obtained for the given validation data set, and ran the tuned methods on the given testing data set.", "labels": [], "entities": [{"text": "PAN'11", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9515494704246521}]}, {"text": "For all the other data sets we utilized ten-fold cross validation.", "labels": [], "entities": []}, {"text": "In all cases, we report the overall classification accuracy, that is, the percentage of test texts correctly attributed to their author.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9102845191955566}]}, {"text": "Statistically significant differences are reported when p < 0.05 according to McNemar's test (when reporting results in a table, the best result for each column is in boldface, and several boldface results mean that the differences between them are not statistically significant).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.7519553303718567}]}, {"text": "In our experiments, we used the L2-regularized linear SVM implementation of LIBLINEAR (, which is well suited for large-scale text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.6919203251600266}]}, {"text": "We experimented with cost parameter values from the set {.", "labels": [], "entities": []}, {"text": ", 10 \u22121 , 10 0 , 10 1 , . .", "labels": [], "entities": []}, {"text": ".}, until no accuracy improvement was obtained (starting from 10 0 = 1 and going in both directions).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.999602735042572}]}, {"text": "We report the results obtained with the value that yielded the highest accuracy, which gives an optimistic estimate for the performance of the Token SVM baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9992133378982544}, {"text": "Token SVM baseline", "start_pos": 143, "end_pos": 161, "type": "DATASET", "confidence": 0.7657953302065531}]}, {"text": "We used collapsed Gibbs sampling to train all the topic models, running four chains with a burn-in of 1,000 iterations.", "labels": [], "entities": []}, {"text": "In the Judgment, PAN'11, and IMDb62 experiments, we retained eight samples per chain with a spacing of 100 iterations.", "labels": [], "entities": [{"text": "Judgment", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.8087559342384338}, {"text": "PAN'11", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.6123799681663513}, {"text": "IMDb62", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.8934585452079773}]}, {"text": "In the IMDb1M and Blog experiments, we retained one sample per chain due to runtime constraints.", "labels": [], "entities": [{"text": "IMDb1M", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.9145879745483398}, {"text": "Blog", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.8794618844985962}]}, {"text": "Because we cannot average topic distribution estimates obtained from training samples due to topic exchangeability (Steyvers and Griffiths 2007), we averaged the probabilities calculated from the retained samples.", "labels": [], "entities": []}, {"text": "In the dimensionality reduction experiments, we used the topic distributions from a single training sample to ensure that the number of features is substantially reduced (an alternative approach would be to use the concatenation of all the samples, but this may result in a large number of features, and employing this alternative approach did not improve results in preliminary experiments).", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.7037466615438461}]}, {"text": "For test text sampling, we used a burn-in of 10 iterations and averaged the parameter estimates over the next 10 iterations in a similar manner to the procedure used by.", "labels": [], "entities": [{"text": "test text sampling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6066361864407858}]}, {"text": "We found that these settings yield stable results across different random seed values.", "labels": [], "entities": []}, {"text": "To enable a fair comparison between the topic-based methods and the Token SVM baseline, all methods were trained on the same token representations of the texts.", "labels": [], "entities": [{"text": "Token SVM baseline", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.8242505788803101}]}, {"text": "In most experiments, we did not apply any filters and simply used all the tokens as they appear in the text.", "labels": [], "entities": []}, {"text": "In some cases, as indicated throughout this section, we either retained only stopwords or discarded the stopwords in a preprocessing step that was applied before running the methods.", "labels": [], "entities": []}, {"text": "This allowed us to obtain rough estimates of the effect of considering only style words, considering only content words, and considering both style and content.", "labels": [], "entities": []}, {"text": "However, we note that this is only a crude way of separating style and content, because some stopwords may contain content clues, whereas some words that do not appear in the stopword list maybe seen as indicators of personal style, regardless of content.", "labels": [], "entities": []}, {"text": "We found that the number of topics has a large impact on performance, and the effect of other configurable parameters is smaller (Section 3.1).", "labels": [], "entities": []}, {"text": "Hence, we used symmetric topic priors, setting all the elements of \u03b1 \u03b1 \u03b1 (D) and \u03b1 \u03b1 \u03b1 (A) to min{0.1, 5/T (D) } and min{0.1, 5/T (A) }, respectively.", "labels": [], "entities": []}, {"text": "For all models, we set \u03b2 w = 0.01 for each word was the base measure for the prior of words in topics.", "labels": [], "entities": []}, {"text": "Because DADT allows us to encode our prior knowledge that stopword use is indicative of authorship, we set \u03b2 In addition, we set \u03b7 a = 1 for each author a, yielding smoothed estimates for the corpus distribution of authors \u03c7 \u03c7 \u03c7.", "labels": [], "entities": []}, {"text": "In this section, we present the results of our experiments on the Judgment data set, which contains judgments by three judges, and on the PAN'11 data set, which contains e-mails by 72 authors.", "labels": [], "entities": [{"text": "Judgment data set", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.922419011592865}, {"text": "PAN'11 data set", "start_pos": 138, "end_pos": 153, "type": "DATASET", "confidence": 0.9699555436770121}]}, {"text": "Authorship attribution on the PAN'11 data set is more challenging than on the Judgment data set, because PAN'11 texts are shorter than judgments, and some of the PAN'11 authors wrote only a few e-mails.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7980058789253235}, {"text": "PAN'11 data set", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9394469062487284}, {"text": "Judgment data set", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9503900408744812}]}, {"text": "We first present the results obtained with LDA, followed by the results obtained with AT (with and without fictitious authors), and with our DADT-based methods, which yielded the best performance.", "labels": [], "entities": [{"text": "LDA", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.5665261149406433}, {"text": "AT", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9758476614952087}]}, {"text": "We end this section with experiments that explore the effect of applying stopword filters to the corpus in a preprocessing step.", "labels": [], "entities": []}, {"text": "These experiments demonstrate that our DADT-based approach models authorship indicators other than content words.", "labels": [], "entities": []}, {"text": "As discussed in Section 5.2, we ran ten-fold cross validation on the Judgment data set.", "labels": [], "entities": [{"text": "Judgment data set", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9632167220115662}]}, {"text": "On PAN'11, we tuned the methods on the validation subset and report the results obtained on the testing subset with the settings that yielded the best validation results (i.e., each method was run multiple times on the validation subset and only once on the testing subset).", "labels": [], "entities": [{"text": "PAN'11", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.9074321985244751}]}, {"text": "We present some tuning results together with testing results to illustrate the behavior of the various methods.", "labels": [], "entities": []}, {"text": "It is worth noting that for most methods, PAN'11 testing results are better than the best validation results.", "labels": [], "entities": []}, {"text": "This maybe because on average testing texts are about 10% longer than validation texts (Section 5.1.2).", "labels": [], "entities": []}, {"text": "On the Judgment data set, the best performance obtained by training an SVM classifier on LDA topic distributions (LDA-SVM with 100 topics) was somewhat worse than that obtained by training directly on tokens (Token SVM), but was still much better than a majority baseline (the differences between LDA-SVM and both the Token SVM and majority baselines are statistically significant in all cases).", "labels": [], "entities": [{"text": "Judgment data set", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.888498862584432}]}, {"text": "This indicates that although some authorship indicators are lost when using LDA for dimensionality reduction, many are retained despite the fact that LDA's document representations are much more compact than the raw token representations.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.6963585764169693}]}, {"text": "The ranking of methods on PAN'11 is similar to the ranking on the Judgment data set, though on Judgment the difference between LDA-SVM and Token SVM is much smaller.", "labels": [], "entities": [{"text": "PAN'11", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.8751091361045837}, {"text": "Judgment data set", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9466386238733927}]}, {"text": "The reason for this difference maybe that LDA does not consider authors in the model-building stage.", "labels": [], "entities": [{"text": "LDA", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.781286895275116}]}, {"text": "Although this had a relatively small effect on performance in the three-way judgment attribution scenarios, it appears that accounting for authors is important in scenarios with many authors.", "labels": [], "entities": []}, {"text": "As the rest of this article deals with such scenarios, we decided not to use LDA for modeling authors in subsequent sections.", "labels": [], "entities": []}, {"text": "presents the results of the AT experiment, with Judgment results in and PAN'11 validation and testing results in, respectively.", "labels": [], "entities": [{"text": "AT", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.7161933183670044}, {"text": "Judgment", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9934870004653931}, {"text": "PAN'11 validation", "start_pos": 72, "end_pos": 89, "type": "METRIC", "confidence": 0.7462070286273956}]}, {"text": "In this section, we report the results of our experiments on the IMDb62, IMDb1M, and Blog data sets.", "labels": [], "entities": [{"text": "IMDb62", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9388730525970459}, {"text": "IMDb1M", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.8142577409744263}, {"text": "Blog data sets", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.9675235748291016}]}, {"text": "Both IMDb data sets contain movie reviews and message board posts, with IMDb62 consisting of texts by 62 prolific authors (with at least 1,000 texts each), and IMDb1M consisting of texts by 22,116 authors, who are mostly non-prolific.", "labels": [], "entities": [{"text": "IMDb data sets", "start_pos": 5, "end_pos": 19, "type": "DATASET", "confidence": 0.8931547999382019}, {"text": "IMDb62", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.9002684950828552}, {"text": "IMDb1M", "start_pos": 160, "end_pos": 166, "type": "DATASET", "confidence": 0.9142653942108154}]}, {"text": "The Blog data set contains blog posts by 19,320 authors, and is the largest of the data sets we considered in terms of token count-it contains about 168 million tokens, whereas IMDb62 and IMDb1M contain about 22 and 34 million tokens, respectively.", "labels": [], "entities": [{"text": "Blog data set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.980488141377767}, {"text": "IMDb1M", "start_pos": 188, "end_pos": 194, "type": "DATASET", "confidence": 0.8459905385971069}]}, {"text": "In addition to running experiments on the full Blog data set, we considered a subset that contains all the texts by the 1,000 most prolific authors (this subset contains about 69 million tokens overall in 332,797 posts-about 49% of the posts in the full Blog data set).", "labels": [], "entities": [{"text": "Blog data set", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9057111740112305}, {"text": "Blog data set", "start_pos": 254, "end_pos": 267, "type": "DATASET", "confidence": 0.9370468656222025}]}, {"text": "Due to resource constraints, we performed a more restricted set of experiments on IMDb62, IMDb1M, and Blog than on the Judgment and PAN'11 data sets (which contain about 3 and 0.74 million tokens, respectively).", "labels": [], "entities": [{"text": "IMDb62", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.9498118162155151}, {"text": "IMDb1M", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.888073205947876}, {"text": "Blog", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.8727092742919922}, {"text": "Judgment and PAN'11 data sets", "start_pos": 119, "end_pos": 148, "type": "DATASET", "confidence": 0.7939510345458984}]}, {"text": "We ran only the Token SVM baseline, AT-P, and DADT-P, as these methods yielded the best performance in the PAN'11 experiments.", "labels": [], "entities": [{"text": "Token SVM baseline", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.7520213524500529}, {"text": "AT-P", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.8946847319602966}, {"text": "DADT-P", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9340676069259644}, {"text": "PAN'11", "start_pos": 107, "end_pos": 113, "type": "DATASET", "confidence": 0.7952274084091187}]}, {"text": "We set the overall number of topics of AT and DADT to 200 topics for IMDb62, and 400 topics for IMDb1M and Blog.", "labels": [], "entities": [{"text": "Blog", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.8976638913154602}]}, {"text": "We set DADT's document/author topic split to 50/150 for IMDb62 and 50/350 for IMDb1M and Blog, and used the prior setting that yielded the best PAN'11 results (\u03b4 (D) = 1.222, \u03b4 (A) = 4.889, and = 0.009).", "labels": [], "entities": [{"text": "Blog", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.7309504747390747}, {"text": "\u03b4 (A)", "start_pos": 175, "end_pos": 180, "type": "METRIC", "confidence": 0.8487318903207779}]}, {"text": "As in the PAN'11 experiments, we determined the overall number of topics based on AT-P's performance with 25, 50, 100, 200, and 400 topics.", "labels": [], "entities": []}, {"text": "The document/author topic splits we tested were 10/190, 50/150, and 100/100 for IMDb62, and 10/390, 50/350, and 100/300 for IMDb1M and Blog.", "labels": [], "entities": [{"text": "Blog", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.8886518478393555}]}, {"text": "shows the results of this set of experiments.", "labels": [], "entities": []}, {"text": "As in our previous experiments, DADT-P consistently outperformed AT-P, which indicates that using disjoint sets of document and author topics yields author representations that are more suitable for authorship attribution than using only author topics.", "labels": [], "entities": [{"text": "DADT-P", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.8507082462310791}]}, {"text": "In contrast to the previous experiments, Token SVM outperformed DADT-P in one case: the IMDb62 data set.", "labels": [], "entities": [{"text": "IMDb62 data set", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9715486566225687}]}, {"text": "This maybe because discriminative methods (such as Token SVM) tend to outperform generative methods (such as DADT-P) in scenarios where training data is abundant (, which is the case with IMDb62-it contains at least 900 texts per author in each training fold.", "labels": [], "entities": []}, {"text": "A notable result is that although all the methods yielded relatively low accuracies on the full Blog data set, the topic-based methods experienced a larger drop inaccuracy than Token SVM when transitioning from the prolific author subset to the full data set.", "labels": [], "entities": [{"text": "Blog data set", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9436009128888448}]}, {"text": "This maybe because topic-based methods use a single model, making them more sensitive to the number of authors than Token SVM's one-versus-all setup that uses one model per author (this sensitivity may also explain why DADT-P outperformed Token SVM by a relatively small margin on IMDb1M).", "labels": [], "entities": [{"text": "IMDb1M", "start_pos": 281, "end_pos": 287, "type": "DATASET", "confidence": 0.9377455115318298}]}, {"text": "This result suggests a direction for future work in the form of an ensemble of Token SVM and DADT-P.", "labels": [], "entities": [{"text": "Token SVM", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.8160465657711029}, {"text": "DADT-P", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.8365210294723511}]}, {"text": "The potential of this direction is demonstrated by the fact that a perfect oracle, which chooses the correct answer between Token SVM and DADT-P when they disagree, yields an accuracy of 37.36% on the full Blog data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9991300702095032}, {"text": "Blog data set", "start_pos": 206, "end_pos": 219, "type": "DATASET", "confidence": 0.9772846897443136}]}], "tableCaptions": [{"text": " Table 3  Data set statistics.", "labels": [], "entities": []}, {"text": " Table 4  Stopword experiment results (data sets: Judgment and PAN'11).", "labels": [], "entities": [{"text": "PAN'11", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.6030802130699158}]}, {"text": " Table 5  Large-scale experiment results (data sets: IMDb62, IMDb1M, and Blog).", "labels": [], "entities": [{"text": "IMDb62", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9264136552810669}, {"text": "IMDb1M", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8489826917648315}, {"text": "Blog", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.5474814176559448}]}, {"text": " Table 6  Text-aware rating prediction with AT and DADT (data set: IMDb1M).", "labels": [], "entities": [{"text": "Text-aware rating prediction", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6648942132790884}, {"text": "AT", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9371129870414734}, {"text": "DADT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.8416780829429626}, {"text": "IMDb1M", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.6287156939506531}]}]}