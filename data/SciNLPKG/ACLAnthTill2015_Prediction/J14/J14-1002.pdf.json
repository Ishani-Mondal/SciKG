{"title": [], "abstractContent": [{"text": "Frame semantics is a linguistic theory that has been instantiated for English in the FrameNet lexicon.", "labels": [], "entities": [{"text": "Frame semantics", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7588877081871033}, {"text": "FrameNet lexicon", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.9121302962303162}]}, {"text": "We solve the problem of frame-semantic parsing using a two-stage statistical model that takes lexical targets (i.e., content words and phrases) in their sentential contexts and predicts frame-semantic structures.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7116550654172897}]}, {"text": "Given a target in context, the first stage disambiguates it to a semantic frame.", "labels": [], "entities": []}, {"text": "This model uses latent variables and semi-supervised learning to improve frame disambiguation for targets unseen at training time.", "labels": [], "entities": [{"text": "frame disambiguation", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.6537488400936127}]}, {"text": "The second stage finds the target's locally expressed semantic arguments.", "labels": [], "entities": []}, {"text": "At inference time, a fast exact dual decomposition algorithm collectively predicts all the arguments of a frame at once in order to respect declaratively stated linguistic constraints, resulting in qualitatively better structures than na\u00a8\u0131vena\u00a8\u0131ve local predictors.", "labels": [], "entities": []}, {"text": "Both components are feature-based and discriminatively trained on a small set of annotated frame-semantic parses.", "labels": [], "entities": []}, {"text": "On the SemEval 2007 benchmark data set, the approach, along with a heuristic identifier of frame-evoking targets, outperforms the prior state of the art by significant margins.", "labels": [], "entities": [{"text": "SemEval 2007 benchmark data set", "start_pos": 7, "end_pos": 38, "type": "DATASET", "confidence": 0.8412739038467407}]}, {"text": "Additionally, we present experiments on the much larger FrameNet 1.5 data set.", "labels": [], "entities": [{"text": "FrameNet 1.5 data set", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.9328382462263107}]}, {"text": "We have released our frame-semantic parser as open-source software.", "labels": [], "entities": []}], "introductionContent": [{"text": "FrameNet) is a linguistic resource storing considerable information about lexical and predicate-argument semantics in English.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8658639788627625}]}, {"text": "Grounded in the theory of frame semantics, it suggests-but does not formally define-a semantic representation that blends representations familiar from word-sense disambiguation and semantic role labeling (SRL;.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.6581629912058512}]}, {"text": "Given the limited size of available resources, accurately producing richly structured frame-semantic structures with high coverage will require data-driven techniques beyond simple supervised classification, such as latent variable modeling, semi-supervised learning, and joint inference.", "labels": [], "entities": [{"text": "latent variable modeling", "start_pos": 216, "end_pos": 240, "type": "TASK", "confidence": 0.681394100189209}]}, {"text": "In this article, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicate-argument structures such as those shown in.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7103106081485748}]}, {"text": "We aim to predict a frame-semantic representation with two statistical models rather than a collection of local classifiers, unlike earlier approaches.", "labels": [], "entities": []}, {"text": "We use a probabilistic framework that cleanly integrates the FrameNet lexicon and limited available training data.", "labels": [], "entities": [{"text": "FrameNet lexicon", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9278947114944458}]}, {"text": "The probabilistic framework we adopt is highly amenable to future extension through new features, more relaxed independence assumptions, and additional semi-supervised models.", "labels": [], "entities": []}, {"text": "Carefully constructed lexical resources and annotated data sets from FrameNet, detailed in Section 3, form the basis of the frame structure prediction task.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.8997555375099182}, {"text": "frame structure prediction", "start_pos": 124, "end_pos": 150, "type": "TASK", "confidence": 0.7299321293830872}]}, {"text": "We decompose this task into three subproblems: target identification (Section 4), in which frame-evoking predicates are marked in the sentence; frame identification (Section 5), in which the evoked frame is selected for each predicate; and argument identification (Section 6), in which arguments to each frame are identified and labeled with a role from that frame.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.7079426944255829}, {"text": "argument identification", "start_pos": 240, "end_pos": 263, "type": "TASK", "confidence": 0.750894159078598}]}, {"text": "Experiments demonstrating favorable performance to the previous state of the art on SemEval 2007 and FrameNet data sets are described in each section.", "labels": [], "entities": [{"text": "SemEval 2007", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.7901770174503326}, {"text": "FrameNet data sets", "start_pos": 101, "end_pos": 119, "type": "DATASET", "confidence": 0.8463426232337952}]}, {"text": "Some novel aspects of our approach include a latent-variable model (Section 5.2) and a semisupervised extension of the predicate lexicon (Section 5.5) to facilitate disambiguation of words not in the FrameNet lexicon; a unified model for finding and labeling arguments", "labels": [], "entities": [{"text": "FrameNet lexicon", "start_pos": 200, "end_pos": 216, "type": "DATASET", "confidence": 0.8894788324832916}]}], "datasetContent": [{"text": "Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by each evoked frame.", "labels": [], "entities": []}, {"text": "These correspond to the three subtasks in our parser, each described and evaluated in turn: target identification (Section 4), frame identification (Section 5, not unlike word-sense disambiguation), and argument identification (Section 6, essentially the same as semantic role labeling).", "labels": [], "entities": [{"text": "frame identification", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.6712542623281479}, {"text": "argument identification", "start_pos": 203, "end_pos": 226, "type": "TASK", "confidence": 0.7162304371595383}, {"text": "semantic role labeling", "start_pos": 263, "end_pos": 285, "type": "TASK", "confidence": 0.6500757336616516}]}, {"text": "The standard evaluation script from the SemEval 2007 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.", "labels": [], "entities": [{"text": "SemEval 2007 shared task", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7531226724386215}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9993571639060974}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9990901947021484}, {"text": "F 1 -measure", "start_pos": 99, "end_pos": 111, "type": "METRIC", "confidence": 0.9909014701843262}]}, {"text": "We present precision, recall, and F 1 -measure microaveraged across the test documents, report labelsonly matching scores (spans must match exactly), and do not use named entity labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.999419093132019}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9994742274284363}, {"text": "F 1 -measure microaveraged", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.9809061884880066}]}, {"text": "More details can be found in the task description paper from SemEval 2007.", "labels": [], "entities": [{"text": "task description paper from SemEval 2007", "start_pos": 33, "end_pos": 73, "type": "DATASET", "confidence": 0.5923678775628408}]}, {"text": "For our experiments, statistical significance is measured using a reimplementation of Dan Bikel's randomized parsing evaluation comparator, a stratified shuffling test whose original implementation 11 is accompanied by the following description (quoted verbatim, with explanations of our use of the test given in square brackets): The null hypothesis is that the two models that produced the observed results are the same, such that for each test instance [here, a set of predicate-argument structures fora sentence], the two observed scores are equally likely.", "labels": [], "entities": []}, {"text": "This null hypothesis is tested by randomly shuffling individual sentences' scores between the two models and then re-computing the evaluation metrics [precision, recall or F 1 score in our case].", "labels": [], "entities": [{"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9993239641189575}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.990330696105957}, {"text": "F 1 score", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9687723914782206}]}, {"text": "If the difference in a particular metric after a shuffling is equal to or greater than the original observed difference in that metric, then a counter for that metric is incremented.", "labels": [], "entities": []}, {"text": "Ideally, one would perform all 2 n shuffles, where n is the number of test cases (sentences), but given that this is often prohibitively expensive, the default number of iterations is 10,000 [we use independently sampled 10,000 shuffles].", "labels": [], "entities": []}, {"text": "After all iterations, the likelihood of incorrectly rejecting the null [hypothesis, i.e., the p-value] is simply (nc + 1)/(nt + 1), where nc is the number of random differences greater than the original observed difference, and nt is the total number of iterations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Salient statistics of the data sets used in our experiments. There is a significant overlap between  the two data sets.", "labels": [], "entities": []}, {"text": " Table 2  Breakdown of targets and arguments in the SemEval 2007 training set in terms of part of speech.  The target POS is based on the LU annotation for the frame instance. For arguments, this reflects  the part of speech of the head word (estimated from an automatic dependency parse); the  percentage is out of all overt arguments.", "labels": [], "entities": [{"text": "SemEval 2007 training set", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.8050799518823624}]}, {"text": " Table 3  Target identification results for our system and the baseline on the SemEval'07 data set. Scores in  bold denote significant improvements over the baseline (p < 0.05).", "labels": [], "entities": [{"text": "SemEval'07 data set", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.9332733750343323}]}, {"text": " Table 5  Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.  Precision, recall, and F 1 were evaluated under exact and partial frame matching; see Section 3.3.  Bold indicates best results on the SemEval 2007 data, which are also statistically significant with  respect to the baseline (p < 0.05).", "labels": [], "entities": [{"text": "Frame identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9258474409580231}, {"text": "SemEval 2007 data set", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.7815369740128517}, {"text": "FrameNet 1.5 release", "start_pos": 81, "end_pos": 101, "type": "DATASET", "confidence": 0.8652435938517252}, {"text": "Precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9973552227020264}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9983794689178467}, {"text": "F 1", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9960425794124603}, {"text": "SemEval 2007 data", "start_pos": 239, "end_pos": 256, "type": "DATASET", "confidence": 0.7214759985605875}]}, {"text": " Table 5. Bold indicates best results. UJSF-1,2 produces statistically significant results (p < 0.001)  for all metrics with respect to the supervised baseline for both the unseen LUs as well as the  whole test set. Although the NGF-2 and UJSF-1,2 models are statistically indistinguishable,  it is noteworthy that the UJSF-1,2 objective produces a much smaller lexicon.", "labels": [], "entities": [{"text": "NGF-2", "start_pos": 229, "end_pos": 234, "type": "DATASET", "confidence": 0.9454167485237122}]}]}