{"title": [{"text": "Stochastic Language Generation in Dialogue Using Factored Language Models", "labels": [], "entities": [{"text": "Stochastic Language Generation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7479471762975057}]}], "abstractContent": [{"text": "Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of pre-generated utterances, or (b) using statistics to determine the generation decisions of an existing generator.", "labels": [], "entities": [{"text": "trainable language generation", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.736882209777832}]}, {"text": "Both approaches rely on the existence of a hand-crafted generation component, which is likely to limit their scalability to new domains.", "labels": [], "entities": []}, {"text": "The first contribution of this article is to present BAGEL, a fully data-driven generation method that treats the language generation task as a search for the most likely sequence of semantic concepts and realization phrases, according to Factored Language Models (FLMs).", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9274234175682068}, {"text": "language generation task", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.7956263224283854}]}, {"text": "As domain utterances are not readily available for most natural language generation tasks, a large creative effort is required to produce the data necessary to represent human linguistic variation for nontrivial domains.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.811331590016683}]}, {"text": "This article is based on the assumption that learning to produce paraphrases can be facilitated by collecting data from a large sample of untrained annotators using crowdsourcing-rather than a few domain experts-by relying on a coarse meaning representation.", "labels": [], "entities": []}, {"text": "A second contribution of this article is to use crowdsourced data to show how dialogue naturalness can be improved by learning to vary the output utterances generated fora given semantic input.", "labels": [], "entities": []}, {"text": "Two data-driven methods for generating paraphrases in dialogue are presented: (a) by sampling from the n-best list of realizations produced by BAGEL's FLM reranker; and (b) by learning a structured perceptron predicting whether candidate realizations are valid paraphrases.", "labels": [], "entities": [{"text": "BAGEL's FLM reranker", "start_pos": 143, "end_pos": 163, "type": "DATASET", "confidence": 0.8609271347522736}]}, {"text": "We train BAGEL on a set of 1,956 utterances produced by 137 annotators, which covers 10 types of dialogue acts and 128 semantic concepts in a tourist information system for Cambridge.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.6770580410957336}, {"text": "Cambridge", "start_pos": 173, "end_pos": 182, "type": "DATASET", "confidence": 0.9523415565490723}]}, {"text": "An automated evaluation shows that BAGEL outperforms utterance class LM baselines on this domain.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9138784408569336}]}, {"text": "A human evaluation of 600 resynthesized dialogue extracts shows that BAGEL's FLM output produces utterances comparable to a handcrafted baseline, whereas the perceptron classifier performs worse.", "labels": [], "entities": [{"text": "BAGEL's FLM", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.4660862485567729}]}, {"text": "Interestingly, human judges find the system sampling from the n-best list to be more natural than a system always returning the first-best utterance.", "labels": [], "entities": []}, {"text": "The judges are also more willing to interact with the n-best system in the future.", "labels": [], "entities": []}, {"text": "These results suggest that capturing the large variation found inhuman language using data-driven methods is beneficial for dialogue interaction.", "labels": [], "entities": [{"text": "dialogue interaction", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.9146958887577057}]}], "introductionContent": [{"text": "The field of natural language generation (NLG) was one of the last areas of computational linguistics to embrace statistical methods, perhaps because of the difficulty of collecting semantically annotated corpora.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8158082465330759}]}, {"text": "Over the past decade, statistical NLG has followed two lines of research.", "labels": [], "entities": [{"text": "statistical NLG", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.6551739573478699}]}, {"text": "The first, pioneered by, introduces statistics in the generation process by training a model that reranks candidate outputs of a handcrafted generator.", "labels": [], "entities": []}, {"text": "Their HALOGEN system uses an n-gram language model trained on news articles.", "labels": [], "entities": []}, {"text": "HALOGEN is thus domain-independent, and it was successfully ported to a specific dialogue system domain (.", "labels": [], "entities": [{"text": "HALOGEN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7916421294212341}]}, {"text": "However, its performance depends largely on the granularity of the underlying meaning representation, which typically includes syntactic and lexical information.", "labels": [], "entities": []}, {"text": "A major issue with data-driven NLG systems is that collecting fine-grained semantic annotations requires a large amount of time and expertise.", "labels": [], "entities": []}, {"text": "For most domains, handcrafting templates remains a more cost-effective solution.", "labels": [], "entities": []}, {"text": "More recent work has investigated other types of reranking models, such as hierarchical syntactic language models (, discriminative models trained to replicate user ratings of utterance quality, or language models trained on speaker-specific corpora to model linguistic alignment).", "labels": [], "entities": []}, {"text": "However, a major drawback of the utterancelevel overgenerate and rank approach is its inherent computational cost.", "labels": [], "entities": []}, {"text": "In contrast, this article proposes a method in which local overgeneration can be made tractable through beam pruning.", "labels": [], "entities": []}, {"text": "A second line of research has focused on introducing statistics at the generationdecision level by training models that find the set of generation parameters maximizing an objective function, for example, producing a target linguistic style (, generating the most likely context-free derivations given a corpus, or maximizing the expected reward using reinforcement learning (.", "labels": [], "entities": []}, {"text": "Although such methods do not suffer from the computational cost of an overgeneration phase, they still require a handcrafted generator to define the generation decision space within which statistics can be used to find an optimal solution.", "labels": [], "entities": []}, {"text": "Recently, research has therefore focused on reducing the amount of handcrafting required by learning to infer generation rules from data (see Section 2).", "labels": [], "entities": []}, {"text": "This article presents BAGEL, an NLG system that can be fully trained from utterances aligned with coarse-grained semantic concepts.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.838728129863739}]}, {"text": "BAGEL aims to produce natural utterances within a large dialogue system domain while minimizing the overall development effort.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.694319486618042}]}, {"text": "Because repetitions are common in human-computer interactionsespecially when facing misunderstandings-a secondary objective of this article is to improve dialogue naturalness by learning to generate paraphrases from data.", "labels": [], "entities": []}, {"text": "Although domain experts can be used to annotate data, domain utterances are not readily available for most NLG tasks, hence a creative process is required for generating these utterances as well as matching semantics.", "labels": [], "entities": []}, {"text": "The difficulty of this process is increased for systems aiming at producing a large amount of linguistic variation, because it requires enumerating a large set of paraphrases for each domain input.", "labels": [], "entities": []}, {"text": "This article is based on the assumption that learning to produce paraphrases can be facilitated by collecting data from a large sample of annotators.", "labels": [], "entities": []}, {"text": "However, this requires that the meaning representation should (a) be simple enough to be understood by untrained annotators, and (b) provide useful generalization properties for generating unseen inputs.", "labels": [], "entities": []}, {"text": "Section 3 describes BAGEL's meaning representation, which satisfies both requirements.", "labels": [], "entities": [{"text": "BAGEL's meaning representation", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.527108944952488}]}, {"text": "Section 4 then details how our meaning representation is mapped to a phrase sequence, using cascaded Factored Language Models with back-off smoothing.", "labels": [], "entities": []}, {"text": "Section 5 presents two methods for using BAGEL's probabilistic output for paraphrase generation in dialogue.", "labels": [], "entities": [{"text": "BAGEL's probabilistic output", "start_pos": 41, "end_pos": 69, "type": "DATASET", "confidence": 0.6731299310922623}, {"text": "paraphrase generation in dialogue", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.8312895447015762}]}, {"text": "Section 6 illustrates how semantically aligned training utterances fora large tourist information domain were collected using crowdsourcing.", "labels": [], "entities": []}, {"text": "Section 7 then evaluates the trained models in a dialogue setting, by showing that (a) BAGEL performs comparably to a handcrafted rule-based generator; and (b) human judges prefer systems sampling from the n-best output over systems always selecting the top ranked utterance.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9356216788291931}]}, {"text": "Finally, Section 8 discusses the implication of these results as well as future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section evaluates BAGEL in the tourist information domain, using an automated metric as well as human judgments of resynthesized dialogues.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 23, "end_pos": 28, "type": "TASK", "confidence": 0.6488797664642334}]}, {"text": "Our objective is not only to evaluate the naturalness of the generated utterances for different training methods, but also to assess whether the linguistic variation found in BAGEL's outputs improves the naturalness of the overall dialogue interaction.", "labels": [], "entities": []}, {"text": "Although automated metrics provide useful information for tuning model parameters, they only correlate moderately with human naturalness ratings (.", "labels": [], "entities": []}, {"text": "We therefore evaluate the methods presented in the previous sections through a subjective rating experiment, using Amazon's Mechanical Turk services.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk services", "start_pos": 115, "end_pos": 148, "type": "DATASET", "confidence": 0.8950170516967774}]}, {"text": "For each dialogue act in our unseen test set, we generate a set of paraphrases with each of the following system configurations: (a) using large context reranking FLMs (FLM); (b) using perceptron reranking (perceptron); and (c) using the output of the decoding models directly (no reranking).", "labels": [], "entities": []}, {"text": "In order to validate the paraphrasing FLM threshold analysis presented in Section 5.1, we evaluate utterances generated within a selection beam of 8% and 15% relative to the probability of the top hypothesis (FLM 8 and FLM 15 ), as well as a system returning the top hypothesis only (FLM 0 ).", "labels": [], "entities": [{"text": "FLM threshold", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7800062596797943}]}, {"text": "For each configuration, we either train all decoding and reranking models on distinct data sets for each dialogue act type in, or we train a single realization model on all dialogue act types (global).", "labels": [], "entities": []}, {"text": "Although a global realization model can potentially generalize across dialogue act types (e.g., not requiring each top semantic concept to be seen with each act type during training), performance is likely to be affected by the resulting increase in vocabulary size and the reduction in consistency between training examples.", "labels": [], "entities": []}, {"text": "Concerning the perceptron reranking algorithm, we use a kernelized perceptron with a polynomial kernel of degree 3 as it performed best in preliminary experiments on a subset of our training data.", "labels": [], "entities": []}, {"text": "We evaluate all the paraphrases classified as positive by the model fora given input act.", "labels": [], "entities": []}, {"text": "Our experiment compares two variants of the perceptron model: (a) using the weights of the last perceptron update (Last); and (b) taking the average of each weight update weighted by the number of instances for which the weight vector was left unchanged during training (Avg).", "labels": [], "entities": [{"text": "Avg", "start_pos": 271, "end_pos": 274, "type": "METRIC", "confidence": 0.9958162903785706}]}, {"text": "In order to account for differences in computational resources needed by each system, we set the pruning thresholds such that each paraphrase set is generated within 0.5 seconds on a Pentium 4.2 GHz.", "labels": [], "entities": []}, {"text": "For each input dialogue act, a maximum of 100 realizations were reranked in our experiments.", "labels": [], "entities": []}, {"text": "These were derived from up to five semantic stack sequences, each generating up to 20 realization phrase sequences.", "labels": [], "entities": []}, {"text": "For the purpose of the evaluation, the generated paraphrase sets for all systems are combined and presented in random order, for four dialogue acts at a time.", "labels": [], "entities": []}, {"text": "Participants were told that each utterance was meant to have the same meaning, and they were asked to evaluate their naturalness on a 5-point Likert scale, as illustrated in.", "labels": [], "entities": []}, {"text": "Naturalness is defined as whether the utterance could have been produced by a human.", "labels": [], "entities": []}, {"text": "Each utterance is taken from the test folds of the cross-validation experiment presented in Section 5.1-that is, the models are trained on up to 90% of the data and the training set does not contain any of the generated dialogue acts.", "labels": [], "entities": []}, {"text": "presents the average naturalness rating for each configuration (Nat).", "labels": [], "entities": []}, {"text": "A Wilcoxon rank sum test shows that all systems outperform the FLM system returning the top hypothesis of the search models, with no reranking (p < 0.0001, Evaluation results for different reranking configurations.", "labels": [], "entities": []}, {"text": "Beam = paraphrase selection beam (% of first best probability); Mean n = mean number of paraphrases per act; Total n = total number of paraphrases used for evaluation; Nat = mean naturalness rating over the generated paraphrase set.", "labels": [], "entities": [{"text": "Mean n", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.965833455324173}, {"text": "Nat", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.9790666699409485}]}, {"text": "The last 3 columns indicate the significance of the difference in naturalness according to a two-tailed Wilcoxon rank sum test (*p < 0.05, **p < 0.01, ***p < 0.001).", "labels": [], "entities": []}, {"text": "We find that the best performance is obtained using the FLM reranking models, with an average naturalness of 3.83 when only considering the top hypothesis (FLM 0 ), compared with 3.16 without any reranking (base).", "labels": [], "entities": [{"text": "FLM", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.5733509659767151}, {"text": "FLM 0 )", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.858384350935618}]}, {"text": "Whereas the automated evaluation in Section 5.1 predicted an optimal selection beam of 8%, we find that the average naturalness decreases to 3.78 when taking the average overall paraphrases within that beam; however, the decrease in naturalness is not significant over 1,097 samples (p = 0.33).", "labels": [], "entities": []}, {"text": "Because these results do not take the coverage of the generated paraphrase set into account, such a nonsignificant decrease in naturalness is encouraging, as it suggests that the naturalness of the paraphrases produced are close to the first-best.", "labels": [], "entities": []}, {"text": "Using a larger selection beam of 15% increases coverage further but produces a significantly lower naturalness than both the FLM 0 and FLM 8 systems (p < 0.01 and p < 0.05, respectively).", "labels": [], "entities": [{"text": "coverage", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9925673007965088}, {"text": "FLM 0", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.7585864961147308}, {"text": "FLM 8", "start_pos": 135, "end_pos": 140, "type": "DATASET", "confidence": 0.6968075037002563}]}, {"text": "While we expected that sharing realization models across dialogue act types would help generalize, overall we find that using one realization model per dialogue act type does not perform significantly worse than global realization models (FLM 15 global), although the former greatly reduces the number of model parameters.", "labels": [], "entities": [{"text": "FLM", "start_pos": 239, "end_pos": 242, "type": "METRIC", "confidence": 0.6288434863090515}]}, {"text": "Although a text-based evaluation gives a good insight into the level of naturalness of a generated paraphrase set, it does not evaluate whether differences in naturalness can be perceived in a spoken dialogue context, nor does it evaluate the effect of the linguistic variation resulting from the use of multiple paraphrases within a dialogue.", "labels": [], "entities": []}, {"text": "In this regard, this section evaluates the following three hypotheses: (a) the learned generators can produce language perceived as natural in a dialogue context; (b) varying the paraphrases used throughout the dialogue improves the system's naturalness; and (c) this increase in naturalness makes the user more willing to interact with the system.", "labels": [], "entities": []}, {"text": "We test these hypotheses by conducting a series of observer-based listening tests comparing dialogue extracts in which the system utterances have been regenerated and resynthesized.", "labels": [], "entities": []}, {"text": "The original dialogues were collected over the phone during a task-based evaluation of the Hidden Information State dialogue manager () on the CamInfo domain, using a handcrafted rule-based language generator.", "labels": [], "entities": [{"text": "CamInfo domain", "start_pos": 143, "end_pos": 157, "type": "DATASET", "confidence": 0.9848275482654572}]}, {"text": "Each utterance is synthesized using an HMM-based text-to-speech engine trained on the AWB voice of the ARCTIC data set using the HTS toolkit).", "labels": [], "entities": [{"text": "AWB", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.6614978909492493}, {"text": "ARCTIC data set", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.946341852347056}, {"text": "HTS toolkit", "start_pos": 129, "end_pos": 140, "type": "DATASET", "confidence": 0.89153853058815}]}, {"text": "Our evaluation first compares different generation methods in a pairwise fashion: (a) the FLM reranking method with n-best outputs sampled from an 8% selection beam (FLM n-best); (b) the averaged kernelized perceptron reranking method with uniform sampling over positive predictions (Perceptron); and (c) the single output of the handcrafted rule-based generator (Handcrafted).", "labels": [], "entities": []}, {"text": "The handcrafted generator is an extension of the SPaRKy sentence planner, which associates each dialogue act with a content plan tree combining syntactic templates with rhetorical structure relations.", "labels": [], "entities": [{"text": "SPaRKy sentence planner", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.5973308285077413}]}, {"text": "The syntactic templates are aggregated two-by-two in a bottom-up fashion by trying different clause-combining operations (e.g., by inserting a conjunction, merging identical subjects, or associating each template with distinct sentences).", "labels": [], "entities": []}, {"text": "The aggregated syntactic tree is then converted into a flat string using the RealPro surface realizer (Lavoie and Rambow 1997).", "labels": [], "entities": [{"text": "RealPro surface realizer", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.7552609841028849}]}, {"text": "The handcrafted generator has been tuned over several months to produce natural utterances for all possible input acts; we therefore treat it as a gold standard in our evaluation.", "labels": [], "entities": []}, {"text": "We also compare the FLM reranking approach with n-best outputs with an identical system that always selects the top realization at each turn (FLM first-best).", "labels": [], "entities": [{"text": "FLM reranking", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.6839990019798279}]}, {"text": "In order to maximize the effect of generated linguistic variation, we do not sample paraphrases that were already chosen during the previous dialogue turns, unless there are no remaining paraphrases for that dialogue act.", "labels": [], "entities": []}, {"text": "A total of 255 dialogues were regenerated for each system.", "labels": [], "entities": []}, {"text": "In order to facilitate the listener's task while maintaining some aspect of the dialogue context, the dialogues were split into chunks consisting of the two consecutive system turns, concatenated with the corresponding prerecorded user turn.", "labels": [], "entities": []}, {"text": "In order to make the dialogue extracts more intelligible, regenerated system turns are concatenated with the user turns with no speech overlap.", "labels": [], "entities": []}, {"text": "For each system pair, 600 dialogue extracts were randomly selected for evaluation out of all the regenerated dialogues.", "labels": [], "entities": []}, {"text": "The raters are presented with four pairs of dialogue extracts at a time, which only differ by their system prompts.", "labels": [], "entities": []}, {"text": "For each dialogue pair, they are asked to listen to both sound clips and evaluate (a) which system is the most natural (naturalness score), and (b) which system they would rather interact with (user preference score), as illustrated in.", "labels": [], "entities": []}, {"text": "Participants were native speakers of English recruited through Amazon Mechanical Turk, and geographically restricted to the USA.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.9694520036379496}]}, {"text": "Although the British TTS voice used might affect overall perceptions of naturalness of U.S. judges, it should not introduce any bias within the system comparison as the same voice was used for each system.", "labels": [], "entities": [{"text": "British TTS voice", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.8256768584251404}]}, {"text": "Each dialogue extract was rated by a single participant, and each participant could rate between 4 and 100 dialogue extract pairs.", "labels": [], "entities": []}, {"text": "As a result, between 55 and 64 participants took part in the evaluation of each system pair.", "labels": [], "entities": []}, {"text": "summarizes the results of the preference tests.", "labels": [], "entities": []}, {"text": "The naturalness and user preference scores represent the percentage of times the judges selected a given system over the other.", "labels": [], "entities": []}, {"text": "A binomial test suggests that the judges did not prefer", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Example n-best lists produced by BAGEL with FLM reranking (after normalizing the  probabilities to 1, but before thresholding).", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.5656686425209045}, {"text": "FLM reranking", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.8200223445892334}]}, {"text": " Table 5  BLEU score of the word-based utterance class LMs for different n-gram sizes and different  number of slots included in the utterance class (most frequent first). Best performing parameters  are in bold. The BLEU score is averaged over all cross-validation folds. See figures 12 and 13  for results using other parameter configurations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992758631706238}, {"text": "BLEU", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9992606043815613}]}, {"text": " Table 6  Evaluation results for different reranking configurations. Beam = paraphrase selection beam  (% of first best probability); Mean n = mean number of paraphrases per act; Total n = total  number of paraphrases used for evaluation; Nat = mean naturalness rating over the generated  paraphrase set. The last 3 columns indicate the significance of the difference in naturalness  according to a two-tailed Wilcoxon rank sum test (*p < 0.05, **p < 0.01, ***p < 0.001).", "labels": [], "entities": [{"text": "Beam", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9460022449493408}]}]}