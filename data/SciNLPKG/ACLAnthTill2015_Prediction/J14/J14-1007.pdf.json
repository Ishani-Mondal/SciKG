{"title": [], "abstractContent": [{"text": "We study the problem of sampling trees from forests, in the setting where probabilities for each tree maybe a function of arbitrarily large tree fragments.", "labels": [], "entities": []}, {"text": "This setting extends recent work for sampling to learn Tree Substitution Grammars to the case where the tree structure (TSG derived tree) is not fixed.", "labels": [], "entities": []}, {"text": "We develop a Markov chain Monte Carlo algorithm which corrects for the bias introduced by unbalanced forests, and we present experiments using the algorithm to learn Synchronous Context-Free Grammar rules for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.7843480408191681}]}, {"text": "In this application, the forests being sampled represent the set of Hiero-style rules that are consistent with fixed input word-level alignments.", "labels": [], "entities": []}, {"text": "We demonstrate equivalent machine translation performance to standard techniques but with much smaller grammars.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7207453548908234}]}], "introductionContent": [{"text": "Recent work on learning Tree Substitution Grammars (TSGs) has developed procedures for sampling TSG rules from known derived trees.", "labels": [], "entities": [{"text": "learning Tree Substitution Grammars (TSGs)", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.7800247584070478}]}, {"text": "Here one samples binary variables at each node in the tree, indicating whether the node is internal to a TSG rule or is a split point between two rules.", "labels": [], "entities": []}, {"text": "We consider the problem of learning TSGs in cases where the tree structure is not known, but rather where possible tree structures are represented in a forest.", "labels": [], "entities": []}, {"text": "For example, we may wish to learn from text where treebank annotation is unavailable, but a forest of likely parses can be produced automatically.", "labels": [], "entities": []}, {"text": "Another application on which we focus our attention in this article arises in machine translation, where we want to learn translation rules from a forest representing the phrase decompositions that are consistent with an automatically derived word alignment.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8065586090087891}]}, {"text": "Both these applications involve sampling TSG trees from forests, rather than from fixed derived trees.", "labels": [], "entities": []}, {"text": "Chappelier and Rajman (2000) present a widely used algorithm for sampling trees from forests: One first computes an inside probability for each node bottom-up, and then chooses an incoming hyperedge for each node top-down, sampling according to each hyperedge's inside probability.", "labels": [], "entities": []}, {"text": "Johnson, use this sampling algorithm in a Markov chain Monte Carlo framework for grammar learning.", "labels": [], "entities": [{"text": "grammar learning", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.8887812793254852}]}, {"text": "We can combine the representations used in this algorithm and in the TSG learning algorithm discussed earlier, maintaining two variables at each node of the forest, one for the identity of the incoming hyperedge, and another representing whether the node is internal to a TSG rule or is a split point.", "labels": [], "entities": []}, {"text": "However, computing an inside probability for each node, as in the first phase of the algorithm of Johnson,, becomes difficult because of the exponential number of TSG rules that can apply at any node in the forest.", "labels": [], "entities": []}, {"text": "Not only is the number of possible TSG rules that can apply given a fixed tree structure exponentially large in the size of the tree, but the number of possible tree structures under anode is also exponentially large.", "labels": [], "entities": []}, {"text": "This problem is particularly acute during grammar learning, as opposed to sampling according to a fixed grammar, because any tree fragment is a valid potential rule.", "labels": [], "entities": []}, {"text": "address the large number of valid unseen rules by decomposing the prior over TSG rules into an equivalent probabilistic context-free grammar; however, this technique only applies to certain priors.", "labels": [], "entities": []}, {"text": "In general, algorithms that match all possible rules are likely to be prohibitively slow, as well as unwieldy to implement.", "labels": [], "entities": []}, {"text": "In this article, we design a sampling algorithm that avoids explicitly computing inside probabilities for each node in the forest.", "labels": [], "entities": []}, {"text": "In Section 2, we derive a general algorithm for sampling tree fragments from forests.", "labels": [], "entities": []}, {"text": "We avoid computing inside probabilities, as in the TSG sampling algorithms of and, but we must correct for the bias introduced by the forest structure, a complication that does not arise when the tree structure is fixed.", "labels": [], "entities": []}, {"text": "In order to simplify the presentation of the algorithm, we first set aside the complication of large, TSG-style rules, and describe an algorithm for sampling trees from forests while avoiding computation of inside probabilities.", "labels": [], "entities": []}, {"text": "This algorithm is then generalized to learn the composed rules of TSG in Section 2.3.", "labels": [], "entities": [{"text": "TSG", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.6405083537101746}]}, {"text": "As an application of our technique, we present machine translation experiments in the remainder of the article.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7276232540607452}]}, {"text": "We learn Hiero-style Synchronous Context-Free Grammar (SCFG) rules (Chiang 2007) from bilingual sentences for which a forest of possible minimal SCFG rules has been constructed from fixed word alignments.", "labels": [], "entities": []}, {"text": "The construction of this forest and its properties are described in Section 3.", "labels": [], "entities": []}, {"text": "We make the assumption that the alignments produced by a word-level model are correct in order to simplify the computation necessary for rule learning.", "labels": [], "entities": [{"text": "rule learning", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.8500325083732605}]}, {"text": "This approach seems safe given that the pipeline of alignment followed by rule extraction has generally remained the state of the art despite attempts to learn joint models of alignment and rule decomposition.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.8205463290214539}, {"text": "rule decomposition", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.7153862863779068}]}, {"text": "We apply our sampling algorithm to learn the granularity of rule decomposition in a Bayesian framework, comparing sampling algorithms in Section 4.", "labels": [], "entities": [{"text": "rule decomposition", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7027773559093475}]}, {"text": "The end-to-end machine translation experiments of Section 5 show that our algorithm is able to achieve performance equivalent to the standard technique of extracting all rules, but results in a significantly smaller grammar.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7119985222816467}]}], "datasetContent": [{"text": "We used a Chinese-English parallel corpus available from LDC, 1 composed of newswire text.", "labels": [], "entities": []}, {"text": "The corpus consists of 41K sentence pairs, which is 1M words on the English side.", "labels": [], "entities": []}, {"text": "We used a 392-sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing.", "labels": [], "entities": []}, {"text": "The development set and the test set have sentences with less than 30 words.", "labels": [], "entities": []}, {"text": "A trigram language model was used for all experiments.", "labels": [], "entities": []}, {"text": "BLEU ( was calculated for evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9815654158592224}]}, {"text": "For our baseline system, we extract Hiero translation rules using the heuristic method, with the standard Hiero rule extraction constraints.", "labels": [], "entities": [{"text": "Hiero translation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.600914254784584}, {"text": "Hiero rule extraction", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.6141002277533213}]}, {"text": "We use our in-house SCFG decoder for translation with both the Hiero baseline and our sampled grammars.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9755122065544128}, {"text": "Hiero baseline", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9401697516441345}]}, {"text": "Our features for all experiments include differently normalized rule counts and lexical weightings (Koehn, Och, and Marcu 2003) of each rule.", "labels": [], "entities": []}, {"text": "Weights are tuned using Pairwise Ranking Optimization (Hopkins and May 2011) using the baseline grammar and development set, then used throughout the experiments.", "labels": [], "entities": []}, {"text": "Because our sampling procedure results in a smaller rule table, we also establish a no-singleton baseline to compare our results to a simple heuristic method of reducing rule table size.", "labels": [], "entities": []}, {"text": "The no-singleton baseline discards rules that occur only once and that have more than one word on the Chinese side during the Hiero rule extraction process, before counting the rules and computing feature scores.", "labels": [], "entities": [{"text": "Hiero rule extraction", "start_pos": 126, "end_pos": 147, "type": "TASK", "confidence": 0.7992329994837443}]}, {"text": "For all experiments, we used d = 0.5 for the Pitman-Yor discount parameter, except where we compared the Pitman-Yor process with Dirichlet process (d = 0).", "labels": [], "entities": []}, {"text": "Although we have a separate Pitman-Yor process for each rule length, we used the same \u03b1 = 5 for all rule sizes in all experiments, including Dirichlet process experiments.", "labels": [], "entities": []}, {"text": "For rule length probability, a Poisson distribution where \u03bb = 2 was used for all experiments.", "labels": [], "entities": [{"text": "rule length probability", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8235178589820862}]}, {"text": "The samples are initialized such that all nodes in a forest are set to be segmented, and a random edge is chosen under each node.", "labels": [], "entities": []}, {"text": "For all experiments, we ran the sampler for 100 iterations and took the sample from the last iteration to compare with the baseline.", "labels": [], "entities": []}, {"text": "For stratified sampling, we increased the level we sample at every 10th iteration.", "labels": [], "entities": []}, {"text": "We also tried \"averaging\" samples, where samples from every 10th iteration are merged to a single grammar.", "labels": [], "entities": []}, {"text": "For averaging samples, we took the samples from the 0th iteration (initialization) to the 70th iteration at every 10th iteration.", "labels": [], "entities": []}, {"text": "We decided on the 70th iteration (last iteration of level 7 sampling) as the last iteration because we constrained the sampler not to sample nodes whose span covers more than seven words (for SAMPLECUT only, SAMPLECUT always segments for these nodes), and the likelihood becomes very stable at that point.", "labels": [], "entities": []}, {"text": "Because every tree fragment in the sampled derivation represents a translation rule, we do not need to explicitly extract the rules; we merely need to collect them and count them.", "labels": [], "entities": []}, {"text": "However, derivations include purely nonlexical rules that do not conform to the rule constraints of Hiero, and which are not useful for translation.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.8952668309211731}, {"text": "translation", "start_pos": 136, "end_pos": 147, "type": "TASK", "confidence": 0.9803144931793213}]}, {"text": "To get rid of this type of rule, we prune every rule that has a scope greater than 2.", "labels": [], "entities": []}, {"text": "Whereas Hiero does not allow two adjacent nonterminals in the source side, our pruning criterion allows some rules of scope 2 that are not allowed by Hiero.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9276497960090637}]}, {"text": "For example, the following rule (only source side shown) has scope 2 but is not allowed by Hiero: In order to see if these rules have any positive or negative effects on translation, we compare a rule set that strictly conforms to the Hiero constraints and a rule set that includes all the rules of scope 2 or less.", "labels": [], "entities": []}, {"text": "As a general test of our probability model, we compare the result from initialization and the 100th sample.", "labels": [], "entities": []}, {"text": "The translation performance of the grammar from the 100th iteration of sampling is much higher than that of the initialization state.", "labels": [], "entities": []}, {"text": "This shows that states with higher probability in our Markov chain generally do result in better translation, and that the sampling process is able to learn valuable composed rules.", "labels": [], "entities": [{"text": "translation", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.9377862811088562}]}], "tableCaptions": [{"text": " Table 2  Comparisons of decoding results.", "labels": [], "entities": []}]}