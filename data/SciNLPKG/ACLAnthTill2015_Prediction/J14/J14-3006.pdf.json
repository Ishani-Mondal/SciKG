{"title": [{"text": "Similarity-Driven Semantic Role Induction via Graph Partitioning", "labels": [], "entities": [{"text": "Similarity-Driven Semantic Role Induction", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8568959832191467}, {"text": "Graph Partitioning", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7066942900419235}]}], "abstractContent": [{"text": "As in many natural language processing tasks, data-driven models based on supervised learning have become the method of choice for semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7526661356290182}]}, {"text": "These models are guaranteed to perform well when given sufficient amount of labeled training data.", "labels": [], "entities": []}, {"text": "Producing this data is costly and time-consuming, however, thus raising the question of whether unsupervised methods offer a viable alternative.", "labels": [], "entities": []}, {"text": "The working hypothesis of this article is that semantic roles can be induced without human supervision from a corpus of syntactically parsed sentences based on three linguistic principles: (1) arguments in the same syntactic position (within a specific linking) bear the same semantic role, (2) arguments within a clause bear a unique role, and (3) clusters representing the same semantic role should be more or less lexically and distribu-tionally equivalent.", "labels": [], "entities": []}, {"text": "We present a method that implements these principles and formalizes the task as a graph partitioning problem, whereby argument instances of a verb are represented as vertices in a graph whose edges express similarities between these instances.", "labels": [], "entities": []}, {"text": "The graph consists of multiple edge layers, each one capturing a different aspect of argument-instance similarity, and we develop extensions of standard clustering algorithms for partitioning such multi-layer graphs.", "labels": [], "entities": []}, {"text": "Experiments for English and German demonstrate that our approach is able to induce semantic role clusters that are consistently better than a strong baseline and are competitive with the state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen increased interest in the shallow semantic analysis of natural language text.", "labels": [], "entities": [{"text": "shallow semantic analysis of natural language text", "start_pos": 49, "end_pos": 99, "type": "TASK", "confidence": 0.8247857902731214}]}, {"text": "The term is often used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (.", "labels": [], "entities": [{"text": "identification and labeling of the semantic roles conveyed by sentential constituents", "start_pos": 49, "end_pos": 134, "type": "TASK", "confidence": 0.7068674889477816}]}, {"text": "Semantic roles describe the relations that hold between a predicate and its arguments (e.g., \"who\" did \"what\" to \"whom\", \"when\", \"where\", and \"how\") abstracting over surface syntactic configurations.", "labels": [], "entities": []}, {"text": "This type of semantic information is shallow but relatively straightforward to infer automatically and useful for the development of broad-coverage, domain-independent language understanding systems.", "labels": [], "entities": []}, {"text": "Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction () and question answering, to machine translation (Wu and Fung 2009) and summarization ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.8080048263072968}, {"text": "question answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.8994123339653015}, {"text": "machine translation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.8062683939933777}, {"text": "summarization", "start_pos": 232, "end_pos": 245, "type": "TASK", "confidence": 0.991592526435852}]}, {"text": "In the example sentences below, window occupies different syntactic positions-it is the object of broke in sentences (1a,b), and the subject in (1c).", "labels": [], "entities": []}, {"text": "In all instances, it bears the same semantic role, that is, the patient or physical object affected by the breaking event.", "labels": [], "entities": []}, {"text": "Analogously, ball is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b).", "labels": [], "entities": []}, {"text": "Also notice that all three instances of break in Example (1) have apparently similar surface syntax with a subject and a noun directly following the predicate.", "labels": [], "entities": []}, {"text": "However, in sentence (1a) the subject of break expresses the agent role, in (1b) it expresses the instrument role, and in (1c) the patient role.", "labels": [], "entities": []}, {"text": "The examples illustrate the fact that predicates can license several alternate mappings or linkings between their semantic roles and their syntactic realization.", "labels": [], "entities": []}, {"text": "Pairs of linkings allowed by a single predicate are often called diathesis alternations.", "labels": [], "entities": []}, {"text": "Sentence pair (1a,b) is an example of the instrument subject alternation, and pair (1b,c) illustrates the causative alternation.", "labels": [], "entities": []}, {"text": "Resolving the mapping between the syntactic dependents of a predicate (e.g., subject, object) and the semantic roles that they each express is one of the major challenges faced by semantic role labelers.", "labels": [], "entities": []}, {"text": "The semantic roles in the examples are labeled in the style of PropBank, a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations.", "labels": [], "entities": []}, {"text": "Under the PropBank annotation framework each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate 1 and a set of adjunct roles such as location or time whose interpretation is common across predicates (e.g., last night in sentence (1c)).", "labels": [], "entities": []}, {"text": "The availability of PropBank and related resources (e.g.,) has sparked the development of a variety semantic role labeling systems, most of which conceptualize the task as a supervised learning problem and rely on roleannotated data for model training.", "labels": [], "entities": []}, {"text": "Most of these systems implement a two-stage architecture consisting of argument identification (determining the arguments of the verbal predicate) and argument classification (labeling these arguments with semantic roles).", "labels": [], "entities": [{"text": "argument identification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7287407070398331}, {"text": "argument classification", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.7078572660684586}]}, {"text": "Current approaches deliver reasonably good performance-a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M` arquez et al. for details), although only on languages and domains for which large amounts of role-annotated training data are available.", "labels": [], "entities": []}, {"text": "Unfortunately, the reliance on labeled data, which is both difficult and expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.6668643156687418}]}, {"text": "Although corpora with semantic role annotations exist nowadays in other languages (e.g., German, Spanish, Catalan, Chinese, Korean), they tend to be smaller than their English equivalents and of limited value for modeling purposes.", "labels": [], "entities": []}, {"text": "Even within English, a language for which two major annotated corpora are available, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.887434720993042}]}, {"text": "The data requirements for supervised systems and the current paucity of such data has given impetus to the development of unsupervised methods that learn from unlabeled data.", "labels": [], "entities": []}, {"text": "If successful, unsupervised approaches could lead to significant resource savings and the development of semantic role labelers that require less engineering effort.", "labels": [], "entities": []}, {"text": "Besides being interesting on their own right, from a theoretical and linguistic perspective, unsupervised methods can provide valuable features for downstream (supervised) processing and serve as a preprocessing step for applications that require broad coverage understanding.", "labels": [], "entities": []}, {"text": "In this article we study the potential of unsupervised methods for semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.8134419719378153}]}, {"text": "As in the supervised case, we decompose the problem into an argument identification step and an argument classification step.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7209658175706863}, {"text": "argument classification", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.7207934409379959}]}, {"text": "Our work primarily focuses on argument classification, which we term role induction, because there is no predefined set of semantic roles in the unsupervised case, and these must be induced from data.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8587988913059235}, {"text": "role induction", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.747177928686142}]}, {"text": "The goal is to assign argument instances to clusters such that each cluster contains arguments corresponding to a specific semantic role and each role corresponds to exactly one cluster.", "labels": [], "entities": []}, {"text": "Unsupervised learning is known to be challenging for many natural language processing problems and role induction is no exception.", "labels": [], "entities": [{"text": "role induction", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.8915203809738159}]}, {"text": "Firstly, it is difficult to define a learning objective function whose optimization will yield an accurate model.", "labels": [], "entities": []}, {"text": "This contrasts with the supervised setting, where the objective function can directly reflect training error (i.e., some estimate of the mismatch between model output and the gold standard) and the model can be tuned to replicate human output fora given input under mathematical guarantees regarding the accuracy of the trained model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 304, "end_pos": 312, "type": "METRIC", "confidence": 0.9955867528915405}]}, {"text": "Secondly, it is also more difficult to incorporate rich feature sets into an unsupervised model).", "labels": [], "entities": []}, {"text": "Unless we explicitly know exactly how features interact, more features may not necessarily lead to a more accurate model and may even decrease performance.", "labels": [], "entities": []}, {"text": "In the supervised setting, feature interactions relevant fora particular learning task can be determined to a large extent automatically and thus a large number of them can be included even if their significance is not clear a priori.", "labels": [], "entities": []}, {"text": "The lack of an extensional definition (in the form of training examples) of the target concept makes a strong case for the development of unsupervised methods that use problem specific prior knowledge.", "labels": [], "entities": []}, {"text": "The idea is to derive a strong inductive bias based on this prior knowledge that will guide the learning towards the correct target concept.", "labels": [], "entities": []}, {"text": "For semantic role induction, we propose to build on the following linguistic principles: 1.", "labels": [], "entities": [{"text": "semantic role induction", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8481493592262268}]}, {"text": "Semantic roles are unique within a particular frame.", "labels": [], "entities": []}], "datasetContent": [{"text": "We adopt the general architecture of supervised semantic role labeling systems where argument identification and argument classification are treated separately.", "labels": [], "entities": [{"text": "supervised semantic role labeling", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.688335195183754}, {"text": "argument identification", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.7355396449565887}, {"text": "argument classification", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7383805513381958}]}, {"text": "Our role labeler is fully unsupervised with respect to both tasks-it does not rely on any role annotated data or semantic resources.", "labels": [], "entities": []}, {"text": "However, our system does not learn from raw text.", "labels": [], "entities": []}, {"text": "In common with most semantic role labeling research, we assume that the input is syntactically analyzed.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6446191072463989}]}, {"text": "Our approach is not tied to a specific syntactic representationboth constituent-and dependency-based representations can be used.", "labels": [], "entities": []}, {"text": "The bulk of our experiments focus on English data and a dependency-based representation that simplifies argument identification considerably and is consistent with the CoNLL 2008 benchmark data set used for evaluation in our experiments.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7914365231990814}, {"text": "CoNLL 2008 benchmark data set", "start_pos": 168, "end_pos": 197, "type": "DATASET", "confidence": 0.9687507748603821}]}, {"text": "To show that our method can be applied to other languages and across varying syntactic representations, we also report experiments on German using a constituent-based representation (see Section 6).", "labels": [], "entities": []}, {"text": "Given the parse of a sentence, our system identifies argument instances and assigns them to clusters.", "labels": [], "entities": []}, {"text": "Thereafter, argument instances can be labeled with an identifier corresponding to the cluster they have been assigned to, similar to PropBank core labels (e.g., A0, A1).", "labels": [], "entities": []}, {"text": "We view argument identification as a syntactic processing step that can be largely undertaken deterministically through analysis of the syntactic tree.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7934089004993439}]}, {"text": "We therefore use a small set of rules to detect arguments with high precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9990748167037964}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9986985921859741}]}, {"text": "In the following, we first describe the data set (Section 5.1) on which our experiments were carried out.", "labels": [], "entities": []}, {"text": "Next, we present the argument identification component of our system (Section 5.2) and the method used for comparison with our approach.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7621288001537323}]}, {"text": "Finally, we explain how system output was evaluated (Section 5.4).", "labels": [], "entities": []}, {"text": "In this section we describe how we assess the quality of a role induction method that assigns labels to units that have been identified as likely arguments.", "labels": [], "entities": [{"text": "role induction", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7531924247741699}]}, {"text": "We also discuss how we measure whether differences in model performance are statistically significant.", "labels": [], "entities": []}, {"text": "The applicability of our method to arbitrary languages is important from a theoretical and practical perspective.", "labels": [], "entities": []}, {"text": "On the one hand, linguistic theory calls for models which are universal and generalize across languages.", "labels": [], "entities": []}, {"text": "This is especially true for models operating on the (frame-) semantic level, which is a generalization over surface structure and should therefore be less language specific).", "labels": [], "entities": []}, {"text": "On the other hand, a languageindependent model can be applied to arbitrary languages, genres, and domains and is thus of greater practical benefit.", "labels": [], "entities": []}, {"text": "Because our approach is based on the languageindependent principles discussed in Section 1, we argue that it can easily generalize to other languages.", "labels": [], "entities": []}, {"text": "To test this claim, we further applied our methods to German data.", "labels": [], "entities": [{"text": "German data", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.8496240079402924}]}, {"text": "Although on a high-level, German clauses do not differ drastically from English ones with respect to their frame-semantic make-up, there are differences in terms of how frame elements are mapped onto specific positions on the linear surface structure of a sentence, beyond any variations observed among English verbs.", "labels": [], "entities": []}, {"text": "In general, German places fewer constraints on word order (more precisely phrase order) and instead relies on richer morphology to help disambiguate the grammatical functions of linguistic units.", "labels": [], "entities": []}, {"text": "In particular, verbal nominal arguments are marked with a grammatical case 6 that directly indicates their grammatical function.", "labels": [], "entities": []}, {"text": "Although in main declarative clauses the inflected part of the verb has to occur in second position, German is commonly considered a verb-final language.", "labels": [], "entities": []}, {"text": "This is because the verb often takes the final position insubordinate clauses, as do infinitive verbs (Brigitta 1996).", "labels": [], "entities": []}, {"text": "Although we follow the same experimental set-up as described in Section 5 for English, there are some deviations due to differences in the data sets utilized for the two languages.", "labels": [], "entities": []}, {"text": "Firstly, in contrast to the CoNLL 2008 data set, the SALSA data set (and the underlying TIGER corpus) does not supply automatic parse trees and we therefore conducted our experiments on gold parses only.", "labels": [], "entities": [{"text": "CoNLL 2008 data set", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9757845848798752}, {"text": "SALSA data set", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9234010775883993}, {"text": "TIGER corpus", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.8494991660118103}]}, {"text": "Moreover, because adjunct arguments are not annotated in SALSA, and because argument identification is not the central issue of this work, we chose to also consider only the gold argument identification.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7546411454677582}]}, {"text": "Thus, all our experiments for German were carried out on the gold/gold data set.", "labels": [], "entities": [{"text": "gold/gold data set", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.7559707283973693}]}, {"text": "A substantial linguistic difference between the German and English data sets is the sparsity of the argument head lemmas, which is significantly higher for German than for English: In the CoNLL 2008 data set, the average number of distinct head lemmas per verb is only 3.69, whereas in the SALSA data set it is 20.12.", "labels": [], "entities": [{"text": "CoNLL 2008 data set", "start_pos": 188, "end_pos": 207, "type": "DATASET", "confidence": 0.967728853225708}, {"text": "SALSA data set", "start_pos": 290, "end_pos": 304, "type": "DATASET", "confidence": 0.9257278045018514}]}, {"text": "This is partly due to the fact that the Wall Street Journal text underlying the English data is topically more focused than the Rundschau newspaper text, which covers a broader range of news beyond economics and politics.", "labels": [], "entities": [{"text": "Wall Street Journal text underlying the English data", "start_pos": 40, "end_pos": 92, "type": "DATASET", "confidence": 0.8121798560023308}, {"text": "Rundschau newspaper text", "start_pos": 128, "end_pos": 152, "type": "DATASET", "confidence": 0.844055970509847}]}, {"text": "Moreover, noun compounding is more commonly used in German than in English (Corston-Oliver and Gamon 2004), which leads to higher lexical sparsity.", "labels": [], "entities": [{"text": "noun compounding", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8268859684467316}]}, {"text": "Data sparsity affects our method, which crucially relies on lexical similarity for determining the role-equivalence of clusters.", "labels": [], "entities": []}, {"text": "Therefore, we reduced the number of syntactic cues used for cluster initialization in order to avoid creating too many small clusters for which similarities cannot be reliably computed.", "labels": [], "entities": [{"text": "cluster initialization", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.6953389048576355}]}, {"text": "Specifically, only the syntactic position and function word served as cues to initialize our clusters.", "labels": [], "entities": []}, {"text": "Note that, as in English, the relatively small number of syntactic cues that determine the syntactic position within a linking is a consequence of the size of our evaluation data set (which is rather small) and not an inherent limitation of our method.", "labels": [], "entities": []}, {"text": "On larger data sets, more syntactic cues could and should be incorporated in order to increase performance.", "labels": [], "entities": []}, {"text": "In our experiments we compared the baseline introduced in Section 5.3 against agglomerative partitioning and the label propagation algorithm using both cosine-and avgmax-similarity.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7431157529354095}]}, {"text": "The parameters \u03b1, \u03b2, and \u03b3, which determine the thresholds used in defining overall similarity scores, were set and updated identically as for English (i.e., these parameters can be considered language-independent).", "labels": [], "entities": []}, {"text": "reports results for the baseline and our role induction methods, namely, agglomerative clustering and multi-layered label propagation (using the avgmax and cosine similarity functions) on the SALSA gold/gold data set.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7061994969844818}, {"text": "SALSA gold/gold data set", "start_pos": 192, "end_pos": 216, "type": "DATASET", "confidence": 0.8784783283869425}]}, {"text": "For comparison, we also include results on the English CoNLL-2008 gold/gold data set.", "labels": [], "entities": [{"text": "English CoNLL-2008 gold/gold data set", "start_pos": 47, "end_pos": 84, "type": "DATASET", "confidence": 0.9406383463314602}]}, {"text": "As can be seen, the baseline obtains a similar F1 for German and English, although the contributions of purity and collocation are different for the two languages.", "labels": [], "entities": [{"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9990091323852539}, {"text": "purity", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9983991980552673}]}, {"text": "In English, purity is Results of agglomerative partitioning and label propagation for cosine and avgmax similarity on German.", "labels": [], "entities": [{"text": "purity", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9977301955223083}, {"text": "label propagation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6755048036575317}]}, {"text": "For comparison purposes results for English on the gold/gold data set are also tabulated.", "labels": [], "entities": [{"text": "gold/gold data set", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.8018806338310241}]}, {"text": "All improvements over the baseline are statistically significant at significance level q < 0.001.", "labels": [], "entities": [{"text": "significance level q", "start_pos": 68, "end_pos": 88, "type": "METRIC", "confidence": 0.9689287344614664}]}], "tableCaptions": [{"text": " Table 3  Results for agglomerative partitioning (for avgmax and cosine similarity). F1 improvements  over the baseline are statistically significant in all settings (q < 0.001). Boldface highlights the  best performing system according to purity, collocation, and F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9996658563613892}, {"text": "F1", "start_pos": 265, "end_pos": 267, "type": "METRIC", "confidence": 0.9994124174118042}]}, {"text": " Table 4  Results for multi-layered label propagation (for avgmax and cosine similarity). F1 improvements  over the baseline are statistically significant in all settings (q < 0.001). Boldface highlights the  best performing system according to purity, collocation, and F1.", "labels": [], "entities": [{"text": "multi-layered label propagation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6845277547836304}, {"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9996076226234436}, {"text": "F1", "start_pos": 270, "end_pos": 272, "type": "METRIC", "confidence": 0.9996441602706909}]}, {"text": " Table 5  Results for single-layered label propagation using a heuristic similarity function.  F1 improvements over the baseline are statistically significant (q < 0.001) in the auto/gold  and gold/gold settings. Boldface highlights the best performing system according to purity,  collocation, and F1.", "labels": [], "entities": [{"text": "single-layered label propagation", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6852249900499979}, {"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9994786381721497}, {"text": "F1", "start_pos": 299, "end_pos": 301, "type": "METRIC", "confidence": 0.9993914365768433}]}, {"text": " Table 6  Semantic role induction with graph partitioning and Bayesian clustering.", "labels": [], "entities": [{"text": "Semantic role induction", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7066101133823395}, {"text": "graph partitioning", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7166456580162048}]}, {"text": " Table 8  Results for individual verbs on the auto/auto data set; comparison between the baseline and our  agglomerative clustering algorithm with the cosine similarity function. Boldface highlights the  best performing system according to purity, collocation, and F1.", "labels": [], "entities": [{"text": "purity", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9923542737960815}, {"text": "F1", "start_pos": 265, "end_pos": 267, "type": "METRIC", "confidence": 0.999792754650116}]}, {"text": " Table 9  Five largest clusters created by the baseline and agglomerative partitioning for the verb increase.  Symbols $ and CD are used as placeholders for monetary units and cardinal numbers,  respectively.", "labels": [], "entities": []}, {"text": " Table 10  Results of agglomerative partitioning and label propagation for cosine and avgmax similarity  on German. For comparison purposes results for English on the gold/gold data set are also  tabulated. All improvements over the baseline are statistically significant at significance level  q < 0.001.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.760996550321579}, {"text": "gold/gold data set", "start_pos": 167, "end_pos": 185, "type": "DATASET", "confidence": 0.7506981372833252}, {"text": "significance level  q", "start_pos": 275, "end_pos": 296, "type": "METRIC", "confidence": 0.8972439567248026}]}, {"text": " Table 11  Results for individual verbs on the gold/gold SALSA data set; comparison between the baseline  and the agglomerative clustering algorithm with the cosine similarity function.", "labels": [], "entities": [{"text": "gold/gold SALSA data set", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.7483818133672079}]}, {"text": " Table 12  Results for individual roles on gold/gold SALSA data set; comparison between the baseline and  the agglomerative clustering algorithm with the cosine similarity function.", "labels": [], "entities": [{"text": "gold/gold SALSA data set", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.7478438665469488}]}]}