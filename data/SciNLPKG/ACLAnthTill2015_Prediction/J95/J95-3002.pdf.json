{"title": [{"text": "Robust Learning, Smoothing, and Parameter Tying on Syntactic Ambiguity Resolution", "labels": [], "entities": [{"text": "Parameter Tying", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.8015134930610657}, {"text": "Syntactic Ambiguity Resolution", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8597765366236368}]}], "abstractContent": [{"text": "Statistical approaches to natural language processing generally obtain the parameters by using the maximum likelihood estimation (MLE) method.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6597719887892405}, {"text": "maximum likelihood estimation (MLE", "start_pos": 99, "end_pos": 133, "type": "METRIC", "confidence": 0.7639266669750213}]}, {"text": "The MLE approaches, however, may fail to achieve good performance in difficult tasks, because the discrimination and robustness issues are not taken into consideration in the estimation processes.", "labels": [], "entities": [{"text": "MLE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8644670248031616}]}, {"text": "Motivated by that concern, a discrimination-and robustness-oriented learning algorithm is proposed in this paper for minimizing the error rate.", "labels": [], "entities": []}, {"text": "In evaluating the robust learning procedure on a corpus of 1,000 sentences, 64.3% of the sentences are assigned their correct syntactic structures, while only 53.1% accuracy rate is obtained with the MLE approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9989901185035706}]}, {"text": "In addition, parameters are usually estimated poorly when the training data is sparse.", "labels": [], "entities": []}, {"text": "Smoothing the parameters is thus important in the estimation process.", "labels": [], "entities": [{"text": "estimation", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.977071225643158}]}, {"text": "Accordingly, we use a hybrid approach combining the robust learning procedure with the smoothing method.", "labels": [], "entities": []}, {"text": "The accuracy rate of 69.8% is attained by using this approach.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9834102392196655}]}, {"text": "Finally, a parameter tying scheme is proposed to tie those highly correlated but unreliably estimated parameters together so that the parameters can be better trained in the learning process.", "labels": [], "entities": [{"text": "parameter tying", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.733047366142273}]}, {"text": "With this tying scheme, the number of parameters is reduced by a factor of 2,000 (from 8.7 x 108 to 4.2 x lOS), and the accuracy rate for parse tree selection is improved up to 70.3% when the robust learning procedure is applied on the tied parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9991635084152222}, {"text": "parse tree selection", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.8706251184145609}]}], "introductionContent": [{"text": "Resolution of syntactic ambiguity has been a focus in the field of natural language processing fora longtime.", "labels": [], "entities": [{"text": "Resolution of syntactic ambiguity", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8927316963672638}]}, {"text": "Both rule-based and statistics-based approaches have been proposed to attack this problem in the past.", "labels": [], "entities": []}, {"text": "For rule-based approaches, knowledge is induced by linguistic experts and is encoded in terms of rules.", "labels": [], "entities": []}, {"text": "Since a huge amount of fine-grained knowledge is usually required to solve ambiguity problems, it is quite difficult fora rule-based approach to acquire such kinds of knowledge.", "labels": [], "entities": []}, {"text": "In addition, the maintenance of consistency among the inductive rules is by no means easy.", "labels": [], "entities": [{"text": "consistency", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9757226705551147}]}, {"text": "Therefore, a rule-based approach, in general, fails to attain satisfactory performance for large-scale applications.", "labels": [], "entities": []}, {"text": "In contrast, a statistical approach provides an objective measuring function to evaluate all possible alternative structures in terms of a set of parameters.", "labels": [], "entities": []}, {"text": "Generally, the statistics of parameters are estimated from a training corpus by using well-developed statistical theorems.", "labels": [], "entities": []}, {"text": "The linguistic uncertainty problems can thus be resolved on a solid mathematical basis.", "labels": [], "entities": []}, {"text": "Moreover, the knowledge acquired by a statistical method is always consistent because all the data in the corpus are jointly considered during the acquisition process.", "labels": [], "entities": []}, {"text": "Hence, compared with a rule-based method, the time required for knowledge acquisition and the cost needed to maintain consistency among the acquired knowledge sources are significantly reduced by adopting a statistical approach.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7747968435287476}]}, {"text": "Among the statistical approaches, and proposed a unified scoring function for resolving syntactic ambiguity.", "labels": [], "entities": []}, {"text": "With that scoring function, various knowledge sources can be unified in a uniform formulation.", "labels": [], "entities": []}, {"text": "Previous work has demonstrated that this scoring function is able to provide high discrimination power fora variety of applications.", "labels": [], "entities": []}, {"text": "In this paper, we start with a baseline system based on this scoring function, and then proceed with different proposed enhancement methods.", "labels": [], "entities": []}, {"text": "A test set of 1,000 sentences, extracted from technical manuals, is used for evaluation.", "labels": [], "entities": []}, {"text": "A performance of 53.1% accuracy rate for parse tree selection is obtained for the baseline system, when the parameters are estimated by using the maximum likelihood estimation (MLE) method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.999184787273407}, {"text": "parse tree selection", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.9258586168289185}]}, {"text": "Note that it is the ranking of competitors, instead of the likelihood value, that directly affects the performance of a disambiguation task.", "labels": [], "entities": [{"text": "disambiguation task", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8916608095169067}]}, {"text": "Maximizing the likelihood values on the training corpus, therefore, does not necessarily lead to the minimum error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.8798282742500305}]}, {"text": "In addition, the statistical variations between the training corpus and real tasks are usually not taken into consideration in the estimation procedure.", "labels": [], "entities": []}, {"text": "Thus, minimizing the error rate on the training corpus does not imply minimizing the error rate in the task we are really concerned with.", "labels": [], "entities": [{"text": "error rate", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9448302388191223}]}, {"text": "To deal with the problems described above, a variety of discrimination-based learning algorithms have been adopted extensively in the field of speech recognition (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.8010645508766174}]}, {"text": "Among those approaches, the robustness issue was discussed in detail by in particular, and encouraging results were observed.", "labels": [], "entities": []}, {"text": "In this paper, a discrimination oriented adaptive learning algorithm is first derived based on the scoring function mentioned above and probabilistic gradient descent theory.", "labels": [], "entities": []}, {"text": "The parameters of the scoring function are then learned from the training corpus using the discriminative learning algorithm.", "labels": [], "entities": []}, {"text": "The accuracy rate for parse tree selection is improved to 56.4% when the discriminative learning algorithm is applied.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995131492614746}, {"text": "parse tree selection", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9064843455950419}]}, {"text": "In addition to the discriminative learning algorithm described above, a robust learning procedure is further applied in order to consider the possible statistical variations between the training corpus and the real task.", "labels": [], "entities": []}, {"text": "The robust learning process continues adjusting the parameters even though the input training token has been correctly recognized, until the score difference between the correct candidate and the top competitor exceeds a preset threshold.", "labels": [], "entities": []}, {"text": "The reason for this is to provide a tolerance zone with a large margin for better preserving the correct ranking orders for data in real tasks.", "labels": [], "entities": []}, {"text": "An accuracy rate of 64.3% for parse tree selection is attained after this robust learning algorithm is used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9995428323745728}, {"text": "parse tree selection", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.8147250811258951}]}, {"text": "The above-mentioned robust learning procedure starts with the parameters obtained by the maximum likelihood estimation method.", "labels": [], "entities": []}, {"text": "However, the MLE is notoriously unreliable when there is insufficient training data.", "labels": [], "entities": [{"text": "MLE", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6959881782531738}]}, {"text": "The MLE for the probability of a null event is zero, which is generally inappropriate for most applications.", "labels": [], "entities": [{"text": "MLE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9974467754364014}]}, {"text": "To avoid the sparse training data problem, the parameters are first estimated by various parameter smoothing methods).", "labels": [], "entities": []}, {"text": "An accuracy rate for parse tree selection is improved to 69.8% by applying the robust learning procedure to the smoothed parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9995342493057251}, {"text": "parse tree selection", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.9041101336479187}]}, {"text": "This result demonstrates that a better initial estimate of the parameters gives the robust learning procedure a chance to obtain better results when many local maximal points exist.", "labels": [], "entities": []}, {"text": "Finally, a parameter tying scheme is proposed to reduce the number of parameters.", "labels": [], "entities": [{"text": "parameter tying", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7067722529172897}]}, {"text": "In this approach, some less reliably estimated but highly correlated parameters are tied together, and then trained through the robust learning procedure.", "labels": [], "entities": []}, {"text": "The probabilities of the events that never appear in the training corpus can thus be trained more reliably.", "labels": [], "entities": []}, {"text": "This hybrid (tying + robust learning) approach reduces the number of parameters by a factor of 2,000 (from 8.7 x 108 to 4.2 x 105) and achieves 70.3% accuracy rate for parse tree selection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9990354776382446}, {"text": "parse tree selection", "start_pos": 168, "end_pos": 188, "type": "TASK", "confidence": 0.8287546833356222}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "A unified scoring function used for integrating knowledge from lexical and syntactic levels is introduced in Section 2.", "labels": [], "entities": []}, {"text": "The results of using the unified scoring function are summarized in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, the discrimination-and robustness-oriented learning algorithm is derived.", "labels": [], "entities": []}, {"text": "The effects of the parameter smoothing techniques on the robust learning procedure are investigated in Section 5.", "labels": [], "entities": []}, {"text": "Next, the parameter tying scheme used to enhance parameter training and reduce the number of parameters is described in Section 6.", "labels": [], "entities": [{"text": "parameter tying", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7630017995834351}]}, {"text": "Finally, we discuss our conclusions and describe the direction of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "First of all, 10,000 parsed sentences generated by), a commercialized English-to-Chinese machine translation system designed by Behavior Design Corporation (BDC), were collected.", "labels": [], "entities": [{"text": "English-to-Chinese machine translation", "start_pos": 70, "end_pos": 108, "type": "TASK", "confidence": 0.7127334872881571}]}, {"text": "The domain for this corpus is computer manuals and documents.", "labels": [], "entities": []}, {"text": "The correct parts of speech and parse trees for the collected sentences were verified by linguistic experts.", "labels": [], "entities": []}, {"text": "The corpus was then randomly partitioned into a training set of 9,000 sentences and a test set of the remaining 1,000 sentences to eliminate possible systematic biases.", "labels": [], "entities": []}, {"text": "The average number of words per sentence for the training set and the test set were 13.9 and 13.8, respectively.", "labels": [], "entities": []}, {"text": "In the training set, there were 1,030 unambiguous sentences, while 122 sentences were unambiguous in the test set.", "labels": [], "entities": []}, {"text": "On the average, there were 34.2 alternative parse trees per sentence for the training set, and 31.2 for the test set.", "labels": [], "entities": []}, {"text": "If we excluded those unambiguous sentences, there were 38.49 and 35.38 alternative syntactic structures per sentence for the training set and the test set, respectively.", "labels": [], "entities": []}, {"text": "3.1.1 Lexicon and Phrase Structure Rules.", "labels": [], "entities": []}, {"text": "In the current system, there are 10,418 distinct lexicon entries, extracted from the 10,000-sentence corpus.", "labels": [], "entities": []}, {"text": "The grammar is composed of 1,088 phrase structure rules that are expressed in terms of 35 terminal symbols (parts of speech) and 95 nonterminal symbols.", "labels": [], "entities": []}, {"text": "Usually, a more complex model requires more parameters; hence it frequently introduces more estimation error, although it may lead to less modeling error.", "labels": [], "entities": []}, {"text": "To investigate the effects of model complexity and estimation error on the disambiguation task, the following models, which account for various lexical and syntactic contextual information, were evaluated: 3.1.3 Performance Evaluations.", "labels": [], "entities": []}, {"text": "We will evaluate the above-mentioned models in two measures: accuracy rate and selection power.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.9865220487117767}]}, {"text": "The measure of accuracy rate of parse tree selection has been widely used in the literature.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.9719589650630951}, {"text": "parse tree selection", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8854286273320516}]}, {"text": "However, this measure is unable to identify which model is better if the average number of alternative syntactic structures in various tasks is different.", "labels": [], "entities": []}, {"text": "For example, a language model with 91% accuracy rate fora task with an average of 1.1 alternative syntactic structures per sentence, which corresponds to the performance of random selection, is by no means better than the language model that attains 70% accuracy rate when there are an average of 100 alternative syntactic structures per sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9968032836914062}, {"text": "accuracy", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.9967764019966125}]}, {"text": "Therefore, a measure, namely Selection Power (SP), is proposed in this paper to give additional information for evaluation.", "labels": [], "entities": [{"text": "Selection Power (SP)", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.8977621078491211}]}, {"text": "SP is defined as the average selection factor (SF) of the disambiguation mechanism on the task of interest.", "labels": [], "entities": [{"text": "SP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.83814936876297}, {"text": "average selection factor (SF)", "start_pos": 21, "end_pos": 50, "type": "METRIC", "confidence": 0.8412515024344126}]}, {"text": "The selection factor for an input sentence is defined as the least proportion of all possible alternative structures that includes the selected syntactic structure.", "labels": [], "entities": []}, {"text": "4 A smaller SP value would, in principle, imply better disambiguation power.", "labels": [], "entities": [{"text": "SP", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9778809547424316}]}, {"text": "Formally, SP is expressed as where sf(i) = G is the selection factor for the ith sentence; ni is the total number of alternative syntactic structures for the ith sentence; ri is the rank of the most preferred candidate.", "labels": [], "entities": []}, {"text": "The selection power fora disambiguation mechanism basically serves as an indicator of the selection ability that includes the most preferred candidate within a particular (N-best) region.", "labels": [], "entities": []}, {"text": "A mechanism with a smaller SP value is more likely to include the most preferred candidate for some given N-best hypotheses.", "labels": [], "entities": [{"text": "SP", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.991050124168396}]}, {"text": "In general, the measures of accuracy rate and the selection power are highly correlated.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 28, "end_pos": 41, "type": "METRIC", "confidence": 0.9858103096485138}]}, {"text": "But it is more informative to report performance with both accuracy rate and selection power.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9875398874282837}]}, {"text": "Selection power supplements accuracy rate when two language models to be compared are tested on different tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.999022364616394}]}], "tableCaptions": [{"text": " Table 7  The number of parameters before and after the tying process. Note that the parameters of  P(W I C) are not tied.", "labels": [], "entities": []}, {"text": " Table 7. This table shows that the number of parameters is greatly reduced  after the tying process, especially for the L2 syntactic models.", "labels": [], "entities": []}]}