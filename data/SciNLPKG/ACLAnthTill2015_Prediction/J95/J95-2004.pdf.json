{"title": [{"text": "Deterministic Part-of-Speech Tagging with Finite-State Transducers", "labels": [], "entities": [{"text": "Deterministic Part-of-Speech Tagging", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6861972908178965}]}], "abstractContent": [{"text": "Stochastic approaches to natural language processing have often been preferred to rule-based approaches because of their robustness and their automatic training capabilities.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.68185027440389}]}, {"text": "This was the case for part-of-speech tagging until Brill showed how state-of-the-art part-of-speech tagging can be achieved with a rule-based tagger by inferring rules from a training corpus.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7784424424171448}, {"text": "part-of-speech tagging", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7157222628593445}]}, {"text": "However, current implementations of the rule-based tagger run more slowly than previous approaches.", "labels": [], "entities": []}, {"text": "In this paper, we present a finite-state tagger, inspired by the rule-based tagger, that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in a deterministic finite-state machine.", "labels": [], "entities": []}, {"text": "This result is achieved by encoding the application of the rules found in the tagger as a nondeterministic finite-state transducer and then turning it into a deterministic transducer.", "labels": [], "entities": []}, {"text": "The resulting deterministic transducer yields a part-of-speech tagger whose speed is dominated by the access time of mass storage devices.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7024118304252625}]}, {"text": "We then generalize the techniques to the class of transformation-based systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Finite-state devices have important applications to many areas of computer science, including pattern matching, databases, and compiler technology.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.8432234227657318}]}, {"text": "Although their linguistic adequacy to natural language processing has been questioned in the past, there has recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 218, "end_pos": 245, "type": "TASK", "confidence": 0.6535122692584991}]}, {"text": "This renewal of interest is due to the speed and compactness of finite-state representations.", "labels": [], "entities": []}, {"text": "This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form.", "labels": [], "entities": []}, {"text": "Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation, parsing, phonology and speech recognition.", "labels": [], "entities": [{"text": "morphological analysis and generation", "start_pos": 113, "end_pos": 150, "type": "TASK", "confidence": 0.8189868927001953}, {"text": "parsing", "start_pos": 152, "end_pos": 159, "type": "TASK", "confidence": 0.9527260661125183}, {"text": "speech recognition", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.757889986038208}]}, {"text": "Although finite-state machines have been used for part-of-speech tagging), none of these approaches has the same flexibility as stochastic techniques.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7297909557819366}]}, {"text": "Unlike stochastic approaches to part-of-speech tagging), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.691988155245781}]}, {"text": "Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac-  Since the lexical tagger does not use any contextual information, many words can be tagged incorrectly.", "labels": [], "entities": []}, {"text": "For example, in (1), the word \"killed\" is erroneously tagged as a verb in past participle form, and in (2), \"shot\" is incorrectly tagged as a verb in past tense.", "labels": [], "entities": []}, {"text": "Given the initial tagging obtained by the lexical tagger, the contextual tagger applies a sequence of rules in order and attempts to remedy the errors made by the initial tagging.", "labels": [], "entities": []}, {"text": "For example, the rules in might be found in a contextual tagger.", "labels": [], "entities": []}, {"text": "The first rule says to change tag vbn to vbd if the previous tag is np.", "labels": [], "entities": []}, {"text": "The second rule says to change vbd to tag vbn if the next tag is by.", "labels": [], "entities": []}, {"text": "Once the first rule is applied, the tag for \"killed\" in (1) and (3) is changed from vbn to vbd and the following tagged sentences are obtained:", "labels": [], "entities": []}], "datasetContent": [{"text": "The tagger we constructed has an accuracy identical s to Brill's tagger and comparable to statistical-based methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9994970560073853}]}, {"text": "However, it runs at a much higher speed.", "labels": [], "entities": []}, {"text": "The tagger runs nearly ten times faster than the fastest of the other systems.", "labels": [], "entities": [{"text": "tagger", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.951741635799408}]}, {"text": "Moreover, the finitestate tagger inherits from the rule-based system its compactness compared with a stochastic tagger.", "labels": [], "entities": []}, {"text": "In fact, whereas stochastic taggers have to store word-tag, bigram, and trigram probabilities, the rule-based tagger and therefore the finite-state one only have to encode a small number of rules (between 200 and 300).", "labels": [], "entities": []}, {"text": "We empirically compared our tagger with Eric Brill's implementation of his tagger, and with our implementation of a trigram tagger adapted from the work of Church (1988) that we previously implemented for another purpose.", "labels": [], "entities": []}, {"text": "We ran the three programs on large files and piped their output into a file.", "labels": [], "entities": []}, {"text": "In the times reported, we included the time spent reading the input and writing the output.", "labels": [], "entities": []}, {"text": "All taggers were trained on a portion of the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9512557983398438}]}, {"text": "The experiments were run on an HP720 with 32MB of memory.", "labels": [], "entities": []}, {"text": "In order to conduct a fair comparison, the dictionary lookup part of the stochastic tagger has also been implemented using the techniques described in Section 5.", "labels": [], "entities": []}, {"text": "All three taggers have approximately the same", "labels": [], "entities": []}], "tableCaptions": []}