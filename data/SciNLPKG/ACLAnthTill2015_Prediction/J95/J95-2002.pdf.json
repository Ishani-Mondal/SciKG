{"title": [{"text": "An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities.", "labels": [], "entities": []}, {"text": "Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input.", "labels": [], "entities": []}, {"text": "Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure.", "labels": [], "entities": []}, {"text": "It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm.", "labels": [], "entities": []}, {"text": "Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Context-free grammars are widely used as models of natural language syntax.", "labels": [], "entities": []}, {"text": "In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (; to guide the rule choice efficiently during parsing; to compute island probabilities for non-linear parsing (.", "labels": [], "entities": []}, {"text": "In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models, as well as in non-finite-state acoustic and phonotactic modeling.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7160909473896027}]}, {"text": "In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic, or they are used with context-sensitive and/or semantic probabilities.", "labels": [], "entities": []}, {"text": "Although clearly not a perfect model of natural language, stochastic context-free grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory providing a sound theoretical basis for ranking and pruning of parses, as well as for integration with models for nonsyntactic aspects of language.", "labels": [], "entities": []}, {"text": "All of the applications listed above involve (or could potentially make use of) one or more of the following standard tasks, compiled by . What is the probability that a given string x is generated by a grammar G?", "labels": [], "entities": []}, {"text": "What is the single most likely parse (or derivation) for x?", "labels": [], "entities": []}, {"text": "What is the probability that x occurs as a prefix of some string generated by G (the prefix probability of x)?", "labels": [], "entities": []}, {"text": "How should the parameters (e.g., rule probabilities) in G be chosen to maximize the probability over a training set of strings?", "labels": [], "entities": []}, {"text": "The algorithm described in this article can compute solutions to all four of these problems in a single framework, with a number of additional advantages over previously presented isolated solutions.", "labels": [], "entities": []}, {"text": "Most probabilistic parsers are based on a generalization of bottom-up chart parsing, such as the CYK algorithm.", "labels": [], "entities": []}, {"text": "Partial parses are assembled just as in nonprobabilistic parsing (modulo possible pruning based on probabilities), while substring probabilities (also known as \"inside\" probabilities) can be computed in a straightforward way.", "labels": [], "entities": []}, {"text": "Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4)), as well as.", "labels": [], "entities": [{"text": "CYK chart parser", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7901577154795328}]}, {"text": "While the solution to problem (3) is not a direct extension of CYK parsing, the authors nevertheless present their algorithm in terms of its similarities to the computation of inside probabilities.", "labels": [], "entities": [{"text": "CYK parsing", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.7229270339012146}]}, {"text": "In our algorithm, computations for tasks (1) and (3) proceed incrementally, as the parser scans its input from left to right; in particular, prefix probabilities are available as soon as the prefix has been seen, and are updated incrementally as it is extended.", "labels": [], "entities": []}, {"text": "Tasks (2) and (4) require one more (reverse) pass over the chart constructed from the input.", "labels": [], "entities": []}, {"text": "Incremental, left-to-right computation of prefix probabilities is particularly important since that is a necessary condition for using SCFGs as a replacement for finite-state language models in many applications, such a speech decoding.", "labels": [], "entities": []}, {"text": "As pointed out by, knowing probabilities P (Xo... xi) for arbitrary prefixes Xo...", "labels": [], "entities": []}, {"text": "xi enables probabilistic prediction of possible follow-words Xi+l, as P(xi+l I xo...xi) = P(Xo...xixi+I)/P(xo...xi).", "labels": [], "entities": []}, {"text": "These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function fora stack decoder.", "labels": [], "entities": []}, {"text": "Another application in which prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs (.", "labels": [], "entities": [{"text": "extraction of n-gram probabilities from SCFGs", "start_pos": 77, "end_pos": 122, "type": "TASK", "confidence": 0.732036272684733}]}, {"text": "Here, too, efficient incremental computation saves time, since the work for common prefix strings can be shared.", "labels": [], "entities": []}, {"text": "The key to most of the features of our algorithm is that it is based on the topdown parsing method for nonprobabilistic CFGs developed by.", "labels": [], "entities": []}, {"text": "Earley's algorithm is appealing because it runs with best-known complexity on a number of special classes of grammars.", "labels": [], "entities": []}, {"text": "In particular, Earley parsing is more efficient than the bottom-up methods in cases where top-down prediction can rule out potential parses of substrings.", "labels": [], "entities": [{"text": "Earley parsing", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.43687939643859863}]}, {"text": "The worst-case computational expense of the algorithm (either for the complete input, or incrementally for each new word) is as good as that of the other 1 Their paper phrases these problem in terms of context-free probabilistic grammars, but they generalize in obvious ways to other classes of models.", "labels": [], "entities": []}, {"text": "known specialized algorithms, but can be substantially better on well-known grammar classes.", "labels": [], "entities": []}, {"text": "Earley's parser (and hence ours) also deals with any context-free rule format in a seamless way, without requiring conversions to Chomsky Normal Form (CNF), as is often assumed.", "labels": [], "entities": []}, {"text": "Another advantage is that our probabilistic Earley parser has been extended to take advantage of partially bracketed input, and to return partial parses on ungrammatical input.", "labels": [], "entities": []}, {"text": "The latter extension removes one of the common objections against top-down, predictive (as opposed to bottom-up) parsing approaches (Magerman and Weir 1992).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}