{"title": [], "abstractContent": [], "introductionContent": [{"text": "An important goal of modern linguistic theory is to characterize as narrowly as possible the class of natural languages.", "labels": [], "entities": []}, {"text": "One classical approach to this characterization has been to investigate the generative capacity of grammatical systems specifiable within particular linguistic theories.", "labels": [], "entities": [{"text": "generative capacity", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8848638534545898}]}, {"text": "Formal results along these lines have already been obtained for certain kinds of Transformational Generative Grammars: for example, showed that the theory of Transformational Grammar presented in Chomsky's Aspects of the Theory of Syntax 1965 is powerful enough to allow the specification of grammars for generating any recursively enumerable language, while extended this work by demonstrating that moderately restricted Transformational Grammars (TGs) can generate languages whose recognition time is provably exponential.", "labels": [], "entities": []}, {"text": "1 These moderately restricted theories of Transformational Grammar generate languages whose recognition is widely considered to be computationally intractable.", "labels": [], "entities": [{"text": "Transformational Grammar", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.904684454202652}]}, {"text": "Whether this \"worst case\" complexity analysis has any real import for actual linguistic study has been the subject of some debate (for discussion, see.", "labels": [], "entities": []}, {"text": "Resuits on generative capacity provide only a worst-case bound on the computational resources required to l In Rounds's proof, transformations are subject to a \"terminal length non-decreasing\" condition, as suggested by.", "labels": [], "entities": [{"text": "generative capacity", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.9214455783367157}]}, {"text": "A similar \"terminal length increasing\" constraint (to the author's knowledge first proposed by when coupled with a condition on recoverability of deletions, yields languages that are recursive but not necessary recognizable in exponential time.", "labels": [], "entities": []}, {"text": "2 Usually, the recognition procedures presented actually recover the structural description of sentences in the process of recognition, so that in fact they actually parse sentences, rather than simply recognize them.", "labels": [], "entities": []}, {"text": "recognize the sentences specified by a linguistic theory.", "labels": [], "entities": []}, {"text": "2 But a sentence processor might not have to explicitly reconstruct deep structures in an exact (but inverse) mimicry of a transformation derivation, or even recognize every sentence generable by a particular transformational theory.", "labels": [], "entities": []}, {"text": "For example, as suggested by Fodor, Bever and Garrett 1974, the human sentence processor could simply obey a set of heuristic principles and recover the right representations specified by a linguistic theory, but not according to the rules of that theory.", "labels": [], "entities": []}, {"text": "To say this much is to simply restate a long-standing view that a theory of linguistic performance could well differ from a theory of linguistic competence -and that the relation between the two could vary from one of near isomorphism to the much weaker input/output equivalence implied by the Fodor, Bever, and Garrett position.", "labels": [], "entities": []}, {"text": "3 In short, the study of generative capacity furnishes a mathematical characterization of the computational complexity of a linguistic system.", "labels": [], "entities": [{"text": "generative capacity", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.9545998573303223}]}, {"text": "Whether this mathematical characterization is cognitively relevant is a related, but distinct, question.", "labels": [], "entities": []}, {"text": "Still, the determination of the computational complexity of a linguistic system is an important undertaking.", "labels": [], "entities": []}, {"text": "For one thing, it gives a precise description of the class of languages that the system can generate, and so can tell us whether the linguistic system is in principle descriptively adequate.", "labels": [], "entities": []}, {"text": "This method of argument was used in Chomsky's original rejection of finite-state languages as an adequate characterization of human linguistic competence.", "labels": [], "entities": []}, {"text": "Second, as mentioned, the resource bound on recognition given by a complexity-theoretic analysis tells us how long recognition will take in the worst possible case.", "labels": [], "entities": []}, {"text": "Since unrestricted TGs can generate computationally \"hard\" languages, then plainly, in order to make TGs efficiently parsable, one must supply additional restrictions.", "labels": [], "entities": []}, {"text": "These could be either modifications to the theory of TG itself, or constraints on the parsing mechanism.", "labels": [], "entities": []}, {"text": "For example, the current theory of TG (see contains several restrictions on the way in which displaced constituents such as wh-phrases maybe linked to their \"canonical\" position in predicate-argument structure.", "labels": [], "entities": []}, {"text": "(E.g., Who in Who did Bill kiss is assumed to be linked to a canonical argument position after the verb kiss.)", "labels": [], "entities": []}, {"text": "As an example of a constraint on the parsing mechanism, one could proceed as did Marcus 1980, and posit constraints dictating that TG-generated languages must have parsers that meet certain \"locality conditions\".", "labels": [], "entities": []}, {"text": "4 For instance, the Marcus constraints amount to an extension of Knuth's 1965 LR(k) locality condition to a (restricted) version of a two-stack deterministic pushdown automaton.", "labels": [], "entities": []}, {"text": "5 Recently, anew theory of grammar has been advanced with the explicitly stated aim of meeting the dual demands of learnability and parsability -the Lexical-Functional Grammars (LFGs) of.", "labels": [], "entities": []}, {"text": "The theory of Lexical-Functional Grammar is claimed to beat least as descriptively adequate as Transformational Grammar, if not more so.", "labels": [], "entities": [{"text": "Lexical-Functional Grammar", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.7959932684898376}]}, {"text": "Moreover, it is claimed to have none of TG's com-4 It is important not to confuse the requirement that TGgenerated languages have parsers that meet certain constraints with the claim that such parsers transparently embed TGs.", "labels": [], "entities": []}, {"text": "As stated, the only requirement is one of weak input/output equivalence -i.e., that the parser construct the same (surface string, underlying representation) pairs as the TG.", "labels": [], "entities": []}, {"text": "Actually, one can show that a modified Marcus parsing system goes beyond this requirement and operates according to the same principles as the recent transformational theory of Chomsky.", "labels": [], "entities": [{"text": "Marcus parsing", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.48693157732486725}]}, {"text": "That is, such a modified Marcus parser makes reference to the same base constraints and representational units as the linguistic theory.", "labels": [], "entities": []}, {"text": "Since it abides by the same rules and representations as TG, one is justified in claiming that the model embeds a TG.", "labels": [], "entities": []}, {"text": "Note that the Marcus parser does not mimic earlier theories of TG (as presented in Aspects of the Theory of Syntax); there is no rule-for-rule correspondence between an Aspects grammar and the rules of the Marcus parser.", "labels": [], "entities": []}, {"text": "But neither is there a rule-for-rule correspondence between modern theories of TG and the Aspects theory.", "labels": [], "entities": [{"text": "TG", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.8365890383720398}]}, {"text": "For example, there is no longer a distinct rule of \"passive\" or \"dative movement\".", "labels": [], "entities": []}, {"text": "A detailed demonstration of this claim would go far beyond the purpose of this paper.", "labels": [], "entities": []}, {"text": "See Berwick and Weinberg forthcoming.", "labels": [], "entities": [{"text": "Berwick and Weinberg", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8994837005933126}]}, {"text": "5 The possible need for LR(k)-like restrictions in order to ensure efficient processability was also suggested by putational unruliness, in the sense that it is claimed that there is a \"natural\" embedding of an LFG into a parsing mechanism (a performance model) that accounts for human sentence processing behavior.", "labels": [], "entities": []}, {"text": "In LFG, there are no transformations (as classically described); the work formerly ascribed to transformations such as \"passive\" is shouldered by information stored in lexical entries associated with lexical items.", "labels": [], "entities": []}, {"text": "The underlying representation of surface strings that is built is also different from the deep structures of classical transformational theory; the representation makes reference to functionally defined notions of grammatical terms like \"Subject\", rather than defining them structurally, as was done in classical transformation theory.", "labels": [], "entities": []}, {"text": "The elimination of transformational power and the use of a different kind of underlying representation for sentences naturally gives rise to the hope that a lexical-functional system would be computationally simpler than a transformational one.", "labels": [], "entities": []}, {"text": "An interesting question then is to determine, as has already been done for the case of certain brands of Transformational Grammar, just what the \"worst case\" computational complexity for the recognition of LFG languages is.", "labels": [], "entities": []}, {"text": "If the recognition time complexity for languages generated by the basic LFG theory can be as complex as that for languages generated by a moderately restricted transformational system, then presumably LFG will also have to add additional constraints, beyond those provided in its basic theory, in order to ensure efficient parsability.", "labels": [], "entities": []}, {"text": "Just as with transformational theories, these could be constraints on either the theory or its performance model realization.", "labels": [], "entities": []}, {"text": "The main result of this paper is to show that certain Lexical-Functional Grammars can generate languages whose recognition time is very likely computationally intractable, at least according to our current understanding of algorithmic complexity.", "labels": [], "entities": []}, {"text": "Briefly, the demonstration proceeds by showing how a problem that is widely conjectured to be computationally difficult -namely, whether there exists an assignment of l's and O's (or \"T'\"s and \"F'\"s) to the atoms of a Boolean formula in conjunctive normal form that makes the formula evaluate to \"1\" (or \"true\") -can be re-expressed as the problem of recognizing whether a particular string is or is not a member of the language generated by a certain Lexical-Functional Grammar.", "labels": [], "entities": []}, {"text": "This \"reduction\" shows that in the worst case the recognition of LFG languages can be just as hard as the original Boolean satisfiability problem.", "labels": [], "entities": []}, {"text": "Since it is widely conjectured that there cannot be a polynomial-time algorithm for satisfiability (the problem is NP-complete), there cannot be a polynomialtime recognition algorithm for LFGs in general either.", "labels": [], "entities": []}, {"text": "Note that this results sharpens that in there it is shown only that LFGs (weakly) generate some subset of the class of contextsensitive languages, and, therefore, in the worst case, exponential time is known to be sufficient (though not necessary) to recognize any LFG language.", "labels": [], "entities": []}, {"text": "The result in Kaplan and Bresnan 1981 therefore does not address the question of how much time, in the worst case, is necessary to recognize LFG languages.", "labels": [], "entities": []}, {"text": "6 The result of this paper indicates that in the worst case more than polynomial time will probably be necessary.", "labels": [], "entities": []}, {"text": "(The reason for the hedge \"probably\" will become apparent below; it hinges upon the central unsolved conjecture of current complexity theory.)", "labels": [], "entities": []}, {"text": "In short then, this result places the LFG languages more precisely in the complexity hierarchy of languages.", "labels": [], "entities": []}, {"text": "It also turns out to be instructive to inquire into just why a lexical-functional approach can turnout to be computationally difficult, and how computational tractability maybe guaranteed.", "labels": [], "entities": []}, {"text": "Advocates of lexicalfunctional theories may have thought (and some have explicitly stated) that the banishment of transformations is a computationally wise move because transformations are computationally costly.", "labels": [], "entities": []}, {"text": "Eliminate the transformations, so this causal argument goes, and one has eliminated all computational problems.", "labels": [], "entities": []}, {"text": "Intriguingly though, when one examines the proof to be given below, the ability to express co-occurrence constraints over arbitrary distances across terminal tokens in a string (as in Subject-Verb number agreement), when coupled with the possibility of alternative lexical entries, seems to be all that is required to make the recognition of LFG languages intractable.", "labels": [], "entities": []}, {"text": "This leaves the question posed in the opening paragraph: just what sorts of constraints on natural languages are required in order to ensure efficient parsability?", "labels": [], "entities": []}, {"text": "As it turns out, even though general LFGs may well be computationally intractable, it is easy to imagine a variety of additional constraints for LFG theory that provide away to avoid this problem.", "labels": [], "entities": []}, {"text": "All of these additional restrictions amount to making the LFG theory more restricted, in such away that the reduction argument cannot be made to work.", "labels": [], "entities": []}, {"text": "For example, one effective restriction is to stipulate that there can only be a finite stock of features with which to label lexical items.", "labels": [], "entities": []}, {"text": "In any case, the moral of the story is an unsurprising one: specificity and constraints can absolve a theory of grammar from computational intractability.", "labels": [], "entities": []}, {"text": "What maybe more surprising is that the requisite locality constraints seem to be useful fora variety of theories of grammar, from Transformational Grammar to Lexical-Functional Grammar.", "labels": [], "entities": [{"text": "Lexical-Functional Grammar", "start_pos": 158, "end_pos": 184, "type": "TASK", "confidence": 0.79291832447052}]}], "datasetContent": [], "tableCaptions": []}