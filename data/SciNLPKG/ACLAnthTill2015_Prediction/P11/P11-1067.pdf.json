{"title": [{"text": "Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "Dependency parsing is a central NLP task.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.895157516002655}]}, {"text": "In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6922456473112106}]}, {"text": "We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures.", "labels": [], "entities": []}, {"text": "These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation.", "labels": [], "entities": []}, {"text": "Therefore, the standard evaluation does not provide a true indication of algorithm quality.", "labels": [], "entities": []}, {"text": "We present anew measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon.", "labels": [], "entities": [{"text": "Neutral Edge Direction (NED)", "start_pos": 25, "end_pos": 53, "type": "METRIC", "confidence": 0.6274960388739904}]}], "introductionContent": [{"text": "Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (.", "labels": [], "entities": [{"text": "induction of dependency parsers", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6278011351823807}]}, {"text": "Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated.", "labels": [], "entities": []}, {"text": "However, there are cases in which there is no linguistic consensus as to what the correct annotation is).", "labels": [], "entities": []}, {"text": "Examples include which verb is the head in a verb group structure (e.g., \"can\" or \"eat\" in \"can eat\"), and which noun is the head in a sequence of proper nouns (e.g., \"John\" or \"Doe\" in \"John Doe\").", "labels": [], "entities": []}, {"text": "We refer to such annotations as (linguistically) problematic.", "labels": [], "entities": []}, {"text": "For such cases, evaluation measures should not punish the algorithm for deviating from the gold standard.", "labels": [], "entities": []}, {"text": "In this paper we show that the evaluation measures reported in current works are highly sensitive to the annotation in problematic cases, and propose a simple new measure that greatly neutralizes the problem.", "labels": [], "entities": []}, {"text": "We start from the following observation: for three leading algorithms (), a small set (at most 18 out of a few thousands) of parameters can be found whose modification dramatically improves the standard evaluation measures (the attachment score measure by 9.3-15.1%, and the undirected measure by a smaller but still significant 1.3-7.7%).", "labels": [], "entities": [{"text": "attachment score measure", "start_pos": 228, "end_pos": 252, "type": "METRIC", "confidence": 0.8292436202367147}]}, {"text": "The phenomenon is implementation independent, occurring with several algorithms based on a fundamental probabilistic dependency model . We show that these parameter changes can be mapped to edge direction changes in local structures in the dependency graph, and that these correspond to problematic annotations.", "labels": [], "entities": []}, {"text": "Thus, the standard evaluation measures do not reflect the true quality of the evaluated algorithm.", "labels": [], "entities": []}, {"text": "We explain why the standard undirected evaluation measure is in fact sensitive to such edge direc-tion changes, and present anew evaluation measure, Neutral Edge Direction (NED), which greatly alleviates the problem by ignoring the edge direction in local structures.", "labels": [], "entities": []}, {"text": "Using NED, manual modifications of model parameters always yields small performance differences.", "labels": [], "entities": []}, {"text": "Moreover, NED sometimes punishes such manual parameter tweaking by yielding worse results.", "labels": [], "entities": []}, {"text": "We explain this behavior using an experiment revealing that NED always prefers the structures that are more consistent with the modeling assumptions lying in the basis of the algorithm.", "labels": [], "entities": [{"text": "NED", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9316399097442627}]}, {"text": "When manual parameter modification is done against this preference, the NED results decrease.", "labels": [], "entities": [{"text": "NED", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.6434912085533142}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, we show the impact of a small number of annotation decisions on the performance of unsupervised dependency parsers.", "labels": [], "entities": []}, {"text": "Second, we observe that often these decisions are linguistically controversial and therefore this impact is misleading.", "labels": [], "entities": []}, {"text": "This reveals a problem in the common evaluation of unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.6713600903749466}]}, {"text": "This is further demonstrated by noting that recent papers evaluate the task using three gold standards which differ in such decisions and which yield substantially different results.", "labels": [], "entities": []}, {"text": "Third, we present the NED measure, which is agnostic to errors arising from choosing the non-gold direction in such cases.", "labels": [], "entities": [{"text": "NED measure", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.8834361135959625}]}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the performed parameter modifications.", "labels": [], "entities": []}, {"text": "Section 4 discusses the linguistic controversies in annotating problematic dependency structures.", "labels": [], "entities": []}, {"text": "Section 6 describes experiments with it.", "labels": [], "entities": []}, {"text": "A discussion is given in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we show that NED indeed reduces the performance difference between the original and the modified parameter sets, thus providing empirical evidence for its validity.", "labels": [], "entities": [{"text": "NED", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8828182816505432}]}, {"text": "For brevity, we present results only for the entire WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9325092136859894}]}, {"text": "Results on WSJ10 are similar.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9388858675956726}]}, {"text": "The datasets and decoding algorithms are the same as those used in Section 3.", "labels": [], "entities": []}, {"text": "shows the score differences between the parameter sets using attachment score, undirected evaluation and NED.", "labels": [], "entities": []}, {"text": "A substantial difference persists under undirected evaluation: a gap of 7.7% in cs09, of 3.5% in saj10a and of 1.3% in km04.", "labels": [], "entities": [{"text": "km04", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.9231253266334534}]}, {"text": "The differences are further reduced using NED.", "labels": [], "entities": [{"text": "NED", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8329499959945679}]}, {"text": "This is consistent with our discussion in Section 5, and shows that undirected evaluation only ignores some of the errors inflicted by edge-flips.", "labels": [], "entities": []}, {"text": "For cs09, the difference is substantially reduced, but a 4.2% performance gap remains.", "labels": [], "entities": []}, {"text": "For km04 and saj10a, the original parameters outperform the new ones by 3.6% and 1% respectively.", "labels": [], "entities": []}, {"text": "We can see that even when ignoring edge-flips, some difference remains, albeit not necessarily in the favor of the modified models.", "labels": [], "entities": []}, {"text": "This is because we did not directly perform edge-flips, but rather parameter-flips.", "labels": [], "entities": []}, {"text": "The difference is thus a result of second-order effects stemming from the parameterflips.", "labels": [], "entities": []}, {"text": "In the next section, we explain why the remaining difference is positive for some algorithms (cs09) and negative for others (km04, saj10a).", "labels": [], "entities": []}, {"text": "For completeness, shows a comparison of some of the current state-of-the-art algorithms, using attachment score, undirected evaluation and NED.", "labels": [], "entities": []}, {"text": "The training and test sets are those used in Section 3.", "labels": [], "entities": []}, {"text": "The: A comparison of recent works, using Att (attachment score) U n (undirected evaluation) and NED, on sentences of length \u2264 10 (excluding punctuation) and on all sentences.", "labels": [], "entities": [{"text": "Att (attachment score) U n", "start_pos": 41, "end_pos": 67, "type": "METRIC", "confidence": 0.9208297303744725}]}, {"text": "The gold standard is obtained using the rules of.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8281790316104889}]}], "tableCaptions": [{"text": " Table 2: Results of the original (Orig. columns), the  modified (M od. columns) parameter sets and their dif- ference (\u2206 columns) for the three algorithms.", "labels": [], "entities": []}, {"text": " Table 3: Differences between the modified and original  parameter sets when evaluated using attachment score  (Attach.), undirected evaluation (Undir.), and NED.", "labels": [], "entities": [{"text": "attachment score  (Attach.)", "start_pos": 93, "end_pos": 120, "type": "METRIC", "confidence": 0.8759394884109497}]}, {"text": " Table 4: A comparison of recent works, using Att (at- tachment score) U n (undirected evaluation) and NED, on  sentences of length \u2264 10 (excluding punctuation) and  on all sentences. The gold standard is obtained using  the rules of", "labels": [], "entities": [{"text": "Att (at- tachment score) U n", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.905370467238956}]}, {"text": " Table 5: The first line shows the NED results from  Section 6, when using the original parameters (Orig.  columns) and the modified parameters (Gold columns).  The second line shows the results of the supervised ver- sions of the algorithms using the corpus which agrees  with the unsupervised model in the problematic cases  (Orig.) and the gold standard (Gold).", "labels": [], "entities": []}]}