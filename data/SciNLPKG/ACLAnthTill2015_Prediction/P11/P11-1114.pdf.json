{"title": [{"text": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts", "labels": [], "entities": [{"text": "Detecting Event Role Fillers in Secondary Contexts", "start_pos": 25, "end_pos": 75, "type": "TASK", "confidence": 0.8836643866130284}]}], "abstractContent": [{"text": "The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8529597222805023}]}, {"text": "We propose a multi-layered event extraction architecture that progressively \"zooms in\" on relevant information.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.73255455493927}]}, {"text": "Our extraction model includes a document genre classifier to recognize event narratives , two types of sentence classifiers, and noun phrase classifiers to extract role fillers.", "labels": [], "entities": []}, {"text": "These modules are organized as a pipeline to gradually zero in on event-related information.", "labels": [], "entities": []}, {"text": "We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.", "labels": [], "entities": [{"text": "MUC-4 event extraction data set", "start_pos": 26, "end_pos": 57, "type": "DATASET", "confidence": 0.8227397561073303}]}], "introductionContent": [{"text": "Event extraction is an information extraction (IE) task that involves identifying the role fillers for events in a particular domain.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.752631425857544}, {"text": "information extraction (IE) task", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.8663425544897715}]}, {"text": "For example, the Message Understanding Conferences (MUCs) challenged NLP researchers to create event extraction systems for domains such as terrorism (e.g., to identify the perpetrators, victims, and targets of terrorism events) and management succession (e.g., to identify the people and companies involved incorporate management changes).", "labels": [], "entities": [{"text": "Message Understanding Conferences (MUCs)", "start_pos": 17, "end_pos": 57, "type": "TASK", "confidence": 0.7464864502350489}, {"text": "event extraction", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7423926293849945}]}, {"text": "Most event extraction systems use either a learning-based classifier to label words as role fillers, or lexico-syntactic patterns to extract role fillers from pattern contexts.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.7327786535024643}]}, {"text": "Both approaches, however, generally tackle event recognition and role filler extraction at the same time.", "labels": [], "entities": [{"text": "event recognition", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.9076995849609375}, {"text": "role filler extraction", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9039637645085653}]}, {"text": "In other words, most event extraction systems primarily recognize contexts that explicitly refer to a relevant event.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7502784132957458}]}, {"text": "For example, a system that extracts information about murders will recognize expressions associated with murder (e.g., \"killed\", \"assassinated\", or \"shot to death\") and extract role fillers from the surrounding context.", "labels": [], "entities": []}, {"text": "But many role fillers occur in contexts that do not explicitly mention the event, and those fillers are often overlooked.", "labels": [], "entities": [{"text": "role fillers", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.74554842710495}]}, {"text": "For example, the perpetrator of a murder maybe mentioned in the context of an arrest, an eyewitness report, or speculation about possible suspects.", "labels": [], "entities": []}, {"text": "Victims maybe named in sentences that discuss the aftermath of the event, such as the identification of bodies, transportation of the injured to a hospital, or conclusions drawn from an investigation.", "labels": [], "entities": []}, {"text": "We will refer to these types of sentences as \"secondary contexts\" because they are generally not part of the main event description.", "labels": [], "entities": []}, {"text": "Discourse analysis is one option to explicitly link these secondary contexts to the event, but discourse modelling is itself a difficult problem.", "labels": [], "entities": [{"text": "Discourse analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8550400733947754}, {"text": "discourse modelling", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7130195796489716}]}, {"text": "The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8529597222805023}]}, {"text": "We create a set of classifiers to recognize role-specific contexts that suggest the presence of a likely role filler regardless of whether a relevant event is mentioned or not.", "labels": [], "entities": []}, {"text": "For example, our model should recognize that a sentence describing an arrest probably includes a reference to a perpetrator, even though the crime itself is reported elsewhere.", "labels": [], "entities": []}, {"text": "Extracting information from these secondary contexts can be risky, however, unless we know that the larger context is discussing a relevant event.", "labels": [], "entities": [{"text": "Extracting information", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9066280126571655}]}, {"text": "address this, we adopt a two-pronged strategy for event extraction that handles event narrative documents differently from other documents.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.8079054951667786}]}, {"text": "We define an event narrative as an article whose main purpose is to report the details of an event.", "labels": [], "entities": []}, {"text": "We apply the rolespecific sentence classifiers only to event narratives to aggressively search for role fillers in these stories.", "labels": [], "entities": []}, {"text": "However, other types of documents can mention relevant events too.", "labels": [], "entities": []}, {"text": "The MUC-4 corpus, for example, includes interviews, speeches, and terrorist propaganda that contain information about terrorist events.", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9339150786399841}]}, {"text": "We will refer to these documents as fleeting reference texts because they mention a relevant event somewhere in the document, albeit briefly.", "labels": [], "entities": []}, {"text": "To ensure that relevant information is extracted from all documents, we also apply a conservative extraction process to every document to extract facts from explicit event sentences.", "labels": [], "entities": []}, {"text": "Our complete event extraction model, called TIER, incorporates both document genre and rolespecific context recognition into 3 layers of analysis: document analysis, sentence analysis, and noun phrase (NP) analysis.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7671743929386139}, {"text": "rolespecific context recognition", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.642018179098765}, {"text": "document analysis", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.6831126064062119}, {"text": "sentence analysis", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7026963979005814}, {"text": "noun phrase (NP) analysis", "start_pos": 189, "end_pos": 214, "type": "TASK", "confidence": 0.6250186761220297}]}, {"text": "At the top level, we train a text genre classifier to identify event narrative documents.", "labels": [], "entities": []}, {"text": "At the middle level, we create two types of sentence classifiers.", "labels": [], "entities": []}, {"text": "Event sentence classifiers identify sentences that are associated with relevant events, and role-specific context classifiers identify sentences that contain possible role fillers irrespective of whether an event is mentioned.", "labels": [], "entities": []}, {"text": "At the lowest level, we use role filler extractors to label individual noun phrases as role fillers.", "labels": [], "entities": [{"text": "role filler extractors", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7469354569911957}]}, {"text": "As documents pass through the pipeline, they are analyzed at different levels of granularity.", "labels": [], "entities": []}, {"text": "All documents pass through the event sentence classifier, and event sentences are given to the role filler extractors.", "labels": [], "entities": [{"text": "role filler extractors", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7124461531639099}]}, {"text": "Documents identified as event narratives additionally pass through role-specific sentence classifiers, and the role-specific sentences are also given to the role filler extractors.", "labels": [], "entities": [{"text": "role filler extractors", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.7065389355023702}]}, {"text": "This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7674805819988251}, {"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9946300983428955}]}, {"text": "In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.6989343464374542}, {"text": "MUC-4 data set", "start_pos": 208, "end_pos": 222, "type": "DATASET", "confidence": 0.9491844971974691}]}], "datasetContent": [{"text": "The lower portion of shows the results of a variety of event extraction models that we created using different components of our system.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7365106642246246}]}, {"text": "The AllSent row shows the performance of our Role Filler Extractors when applied to every sentence in every document.", "labels": [], "entities": []}, {"text": "This system produced high recall, but precision was consistently low.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9997630715370178}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9996798038482666}]}, {"text": "The EventSent row shows the performance of our Role Filler Extractors applied only to the event sentences identified by our event sentence classifier.", "labels": [], "entities": []}, {"text": "This boosts precision across all event roles, but with a sharp reduction in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9991647005081177}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.999300479888916}]}, {"text": "We see a roughly 20 point swing from recall to precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9977912902832031}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9985376596450806}]}, {"text": "These results are similar to GLACIER's results on most event roles, which isn't surprising because GLACIER also incorporates event sentence identification.", "labels": [], "entities": [{"text": "event sentence identification", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.6419016718864441}]}, {"text": "The RoleSent row shows the results of our Role Filler Extractors applied only to the role-specific sentences identified by our classifiers.", "labels": [], "entities": []}, {"text": "We see a 12-13 point swing from recall to precision compared to the AllSent row.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9988131523132324}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9945064187049866}, {"text": "AllSent row", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.8583107590675354}]}, {"text": "This result is consistent with our hypothesis that many role fillers exist in rolespecific contexts that are not event sentences.", "labels": [], "entities": [{"text": "role fillers", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.7458260655403137}]}, {"text": "As expected, extracting facts from role-specific contexts that do not necessarily refer to an event is less reliable.", "labels": [], "entities": []}, {"text": "The EventSent+RoleSent row shows the results when information is extracted from both types of sentences.", "labels": [], "entities": []}, {"text": "We see slightly higher recall, which confirms that one set of extractions is not a strict subset of the other, but precision is still relatively low.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9996235370635986}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9996172189712524}]}, {"text": "The next set of experiments incorporates document classification as the third layer of text analysis.", "labels": [], "entities": [{"text": "document classification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7100435644388199}, {"text": "text analysis", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.765740841627121}]}, {"text": "The DomDoc/EventSent+DomDoc/RoleSent row shows the results of applying both types of sentence classifiers only to documents identified as domain-relevant by the Domain-relevant Document (DomDoc) Classifier described in Section 4.4.", "labels": [], "entities": []}, {"text": "Extracting information only from domain-relevant documents improves precision by +6, but also sacrifices 8 points of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9995304346084595}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9934451580047607}]}, {"text": "The EventSent row reveals that information found in event sentences has the highest precision, even without relying on document classification.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993122816085815}]}, {"text": "We concluded that evidence of an event sentence is probably sufficient to warrant role filler extraction irrespective of the style of the document.", "labels": [], "entities": [{"text": "role filler extraction", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.8594555457433065}]}, {"text": "As we discussed in Section 4, many documents contain only a fleeting reference to an event, so it is important to be able to extract information from those isolated event descriptions as well.", "labels": [], "entities": []}, {"text": "Consequently, we created a system, EventSent+DomDoc/RoleSent, that extracts information from event sentences in all documents, but extracts information from role-specific sentences only if they appear in a domain-relevant document.", "labels": [], "entities": []}, {"text": "This architecture captured the best of both worlds: recall improved from 58% to 65% with only a one point drop in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.999690055847168}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9983731508255005}]}, {"text": "Finally, we evaluated the idea of using document genre as a filter instead of domain relevance.", "labels": [], "entities": []}, {"text": "The last row, EventSent+ENarrDoc/RoleSent, shows the results of our final architecture which extracts information from event sentences in all documents, but extracts information from role-specific sentences only in Event Narrative documents.", "labels": [], "entities": []}, {"text": "This architecture produced the best F1 score of 56.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9799580574035645}]}, {"text": "This model increases precision by an additional 4 points and produces the best balance of recall and precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9997445940971375}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9996067881584167}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9985418319702148}]}, {"text": "Overall, TIER's multi-layered extraction architecture produced higher F1 scores than previous systems on four of the five event roles.", "labels": [], "entities": [{"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9994537234306335}]}, {"text": "The improved recall is due to the additional extractions from secondary contexts.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9995110034942627}]}, {"text": "The improved precision comes from our two-pronged strategy of treating event narratives differently from other documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.999151349067688}]}, {"text": "TIER aggressively searches for extractions in event narrative stories but is conservative and extracts information only from event sentences in all other documents.", "labels": [], "entities": [{"text": "TIER", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5861461758613586}]}], "tableCaptions": [{"text": " Table 1: Manual Analysis of Document Types", "labels": [], "entities": [{"text": "Manual Analysis of Document Types", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8263049721717834}]}, {"text": " Table 3: Experimental results, reported as Precision/Recall/F-score", "labels": [], "entities": [{"text": "Precision/Recall/F-score", "start_pos": 44, "end_pos": 68, "type": "METRIC", "confidence": 0.774171245098114}]}]}