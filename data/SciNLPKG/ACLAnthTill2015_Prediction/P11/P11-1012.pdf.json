{"title": [{"text": "Query Weighting for Ranking Model Adaptation", "labels": [], "entities": [{"text": "Ranking Model Adaptation", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6169388989607493}]}], "abstractContent": [{"text": "We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting.", "labels": [], "entities": []}, {"text": "Query weighting is a key step in ranking model adaptation.", "labels": [], "entities": [{"text": "Query weighting", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8359933197498322}, {"text": "ranking model adaptation", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8527475595474243}]}, {"text": "As the learning object of ranking algorithms is divided by query instances, we argue that it's more reasonable to conduct importance weighting at query level than document level.", "labels": [], "entities": []}, {"text": "We present two query weighting schemes.", "labels": [], "entities": [{"text": "query weighting", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6344840377569199}]}, {"text": "The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector.", "labels": [], "entities": []}, {"text": "This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression.", "labels": [], "entities": []}, {"text": "The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation.", "labels": [], "entities": []}, {"text": "Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.", "labels": [], "entities": [{"text": "LETOR3.0 data set", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.8978339831034342}]}], "introductionContent": [{"text": "Learning to rank, which aims at ranking documents in terms of their relevance to user's query, has been widely studied in machine learning and information retrieval communities (.", "labels": [], "entities": []}, {"text": "In general, large amount of training data need to be annotated by domain experts for achieving better ranking performance.", "labels": [], "entities": []}, {"text": "In real applications, however, it is time consuming and expensive to annotate training data for each search domain.", "labels": [], "entities": []}, {"text": "To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation ().", "labels": [], "entities": [{"text": "ranking model adaptation", "start_pos": 262, "end_pos": 286, "type": "TASK", "confidence": 0.6794930299123129}]}, {"text": "Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer.", "labels": [], "entities": [{"text": "cross-domain knowledge transfer", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.662358413139979}]}, {"text": "This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7163912653923035}]}, {"text": "Existing instance weighting schemes mainly focus on the adaptation problem for classification.", "labels": [], "entities": []}, {"text": "Although instance weighting scheme maybe applied to documents for ranking model adaptation, the difference between classification and learning to rank should be highlighted to take careful consideration.", "labels": [], "entities": [{"text": "ranking model adaptation", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.698140561580658}]}, {"text": "Compared to classification, the learning object for ranking is essentially a query, which contains a list of document instances each with a relevance judgement.", "labels": [], "entities": []}, {"text": "Recently, researchers proposed listwise ranking algorithms ( to take the whole query as a learning object.", "labels": [], "entities": []}, {"text": "The benchmark evaluation showed that list-: The information about which document instances belong to the same query is lost in document instance weighting scheme.", "labels": [], "entities": []}, {"text": "To avoid losing this information, query weighting takes the query as a whole and directly measures its importance.", "labels": [], "entities": [{"text": "query weighting", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8186769485473633}]}, {"text": "wise approach significantly outperformed pointwise approach, which takes each document instance as independent learning object, as well as pairwise approach, which concentrates learning on the order of a pair of documents (.", "labels": [], "entities": []}, {"text": "Inspired by the principle of listwise approach, we hypothesize that the importance weighting for ranking model adaptation could be done better at query level rather than document level.", "labels": [], "entities": [{"text": "ranking model adaptation", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.6887595256169637}]}, {"text": "demonstrates the difference between instance weighting and query weighting, where there are two queries q s1 and q s2 in the source domain and q t1 and q t2 in the target domain, respectively, and each query has three retrieved documents.", "labels": [], "entities": []}, {"text": "In(a), source and target domains are represented as a bag of document instances.", "labels": [], "entities": []}, {"text": "It is worth noting that the information about which document instances belong to the same query is lost.", "labels": [], "entities": []}, {"text": "To avoid this information loss, query weighting scheme shown as(b) directly measures importance weight at query level.", "labels": [], "entities": []}, {"text": "Instance weighting makes the importance estimation of document instances inaccurate when documents of the same source query are similar to the documents from different target queries.", "labels": [], "entities": []}, {"text": "as a toy example, where the document instance is represented as a feature vector with four features.", "labels": [], "entities": []}, {"text": "No matter what weighting schemes are used, it makes sense to assign high weights to source queries q s1 and q s2 because they are similar to target queries q t1 and q t2 , respectively.", "labels": [], "entities": []}, {"text": "Meanwhile, the source query q s3 should be weighted lower because it's not quite similar to any of q t1 and q t2 at query level, meaning that the ranking knowledge from q s3 is different from that of q t1 and q t2 and thus less useful for the transfer to the target domain.", "labels": [], "entities": []}, {"text": "Unfortunately, the three source queries q s1 , q s2 and q s3 would be weighted equally by document instance weighting scheme.", "labels": [], "entities": []}, {"text": "The reason is that all of their documents are similar to the two document instances in target domain despite the fact that the documents of q s3 correspond to their counterparts from different target queries.", "labels": [], "entities": []}, {"text": "Therefore, we should consider the source query as a whole and directly measure the query importance.", "labels": [], "entities": []}, {"text": "However, it's not trivial to directly estimate a query's weight because a query is essentially provided as a matrix where each row represents a vector of document features.", "labels": [], "entities": []}, {"text": "In this work, we present two simple but very effective approaches attempting to resolve the problem from distinct perspectives: (1) we compress each query into a query feature vector by aggregating all of its document instances, and then conduct query weighting on these query feature vectors; (2) we measure the similarity between the source query and each target query one by one, and then combine these fine-grained similarity values to calculate its importance to the target domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed two query weighting methods on TREC-2003 and TREC-2004 web track datasets, which were released through LETOR3.0 as a benchmark collection for learning to rank by.", "labels": [], "entities": [{"text": "TREC-2003", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9434834122657776}, {"text": "TREC-2004 web track datasets", "start_pos": 71, "end_pos": 99, "type": "DATASET", "confidence": 0.8752261251211166}]}, {"text": "Originally, different query tasks were defined on different parts of data in the collection, which can be considered as different domains for us.", "labels": [], "entities": []}, {"text": "Adaptation takes place when ranking tasks are performed by using the models trained on the domains in which they were originally defined to rank the documents in other domains.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.970966100692749}]}, {"text": "Our goal is to demonstrate that query weighting can be more effective than the state-of-the-art document instance weighting.", "labels": [], "entities": [{"text": "query weighting", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.8048318326473236}]}, {"text": "Three query tasks were defined in TREC-2003 and TREC-2004 web track, which are homepage finding (HP), named page finding (NP) and topic distillation (TD).", "labels": [], "entities": [{"text": "TREC-2004 web track", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.8487483263015747}, {"text": "homepage finding", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.6971429288387299}, {"text": "page finding (NP)", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7989004015922546}, {"text": "topic distillation (TD)", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.7776968836784363}]}, {"text": "In this dataset, each document instance is represented by 64 features, including low-level features such as term frequency, inverse document frequency and document length, and high-level features such as BM25, language-modeling, PageRank and HITS.", "labels": [], "entities": [{"text": "BM25", "start_pos": 204, "end_pos": 208, "type": "DATASET", "confidence": 0.7074177265167236}]}, {"text": "The number of queries of each task is given in.", "labels": [], "entities": []}, {"text": "The baseline ranking model is an RSVM directly trained on the source domain without using any weighting methods, denoted as no-weight.", "labels": [], "entities": []}, {"text": "We implemented two weighting measures based on domain separator and Kullback-Leibler divergence, referred to DS and KL, respectively.", "labels": [], "entities": [{"text": "DS", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.8797823786735535}]}, {"text": "In DS measure, three document instance weighting methods based on probability principle ( were implemented for comparison, denoted as doc-pair, doc-avg and doc-comb (see Section 2).", "labels": [], "entities": []}, {"text": "In KL measure, there is no probabilistic meaning for KL weight Query and the doc-comb based on KL is not interpretable, and we only present the results of doc-pair and docavg for KL measure.", "labels": [], "entities": []}, {"text": "Our proposed query weighting methods are denoted by query-aggr and querycomp, corresponding to document feature aggregation in query and query comparison across domains, respectively.", "labels": [], "entities": []}, {"text": "All ranking models above were trained only on source domain training data and the labeled data of target domain was just used for testing.", "labels": [], "entities": []}, {"text": "For training the models efficiently, we implemented RSVM with Stochastic Gradient Descent (SGD) optimizer.", "labels": [], "entities": []}, {"text": "The reported performance is obtained by five-fold cross validation.", "labels": [], "entities": []}, {"text": "The task of HP and NP are more similar to each other whereas HP/NP is rather different from TD).", "labels": [], "entities": []}, {"text": "Thus, we carried out HP/NP to TD and TD to HP/NP ranking adaptation tasks.", "labels": [], "entities": [{"text": "HP/NP ranking adaptation", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.5430298388004303}]}, {"text": "Mean Average Precision (MAP)) is used as the ranking performance measure.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP))", "start_pos": 0, "end_pos": 29, "type": "METRIC", "confidence": 0.9556441605091095}]}], "tableCaptions": [{"text": " Table 2: Results of MAP for HP/NP to TD adaptation.  \u2020,  \u2021, \u266f and boldface indicate significantly better than no-weight,  doc-pair, doc-avg and doc-comb, respectively. Confidence level is set at 95%", "labels": [], "entities": [{"text": "MAP", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9262323975563049}, {"text": "TD adaptation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.6373484134674072}, {"text": "\u266f", "start_pos": 61, "end_pos": 62, "type": "METRIC", "confidence": 0.9481662511825562}, {"text": "Confidence", "start_pos": 169, "end_pos": 179, "type": "METRIC", "confidence": 0.998593270778656}]}, {"text": " Table 3: Results of MAP for TD to HP/NP adaptation.  \u2020,  \u2021, \u266f and boldface indicate significantly better than no-weight,  doc-pair, doc-avg and doc-comb, respectively. Confidence level is set as 95%.", "labels": [], "entities": [{"text": "MAP", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9550923705101013}, {"text": "TD to HP/NP adaptation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.5493560234705607}, {"text": "Confidence level", "start_pos": 169, "end_pos": 185, "type": "METRIC", "confidence": 0.9813342690467834}]}, {"text": " Table 4: The Kendall's \u03c4 of R weight and R map in HP/NP to TD adaptation.", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.7928416530291239}]}, {"text": " Table 5: The Kendall's \u03c4 of R weight and R map in TD to HP/NP adaptation.", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.8110453883806864}]}]}