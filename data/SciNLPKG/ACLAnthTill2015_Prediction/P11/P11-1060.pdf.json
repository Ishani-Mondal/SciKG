{"title": [], "abstractContent": [{"text": "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.", "labels": [], "entities": [{"text": "Compositional question answering", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8311332662900289}]}, {"text": "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.", "labels": [], "entities": []}, {"text": "In tackling this challenging learning problem, we introduce anew semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.", "labels": [], "entities": []}, {"text": "On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7523148655891418}, {"text": "GEO", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9429954886436462}, {"text": "JOBS", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.7895041108131409}, {"text": "accuracies", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.989001989364624}]}], "introductionContent": [{"text": "What is the total population of the ten largest capitals in the US?", "labels": [], "entities": []}, {"text": "Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).", "labels": [], "entities": [{"text": "Answering these types of complex questions compositionally", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.6049250662326813}]}, {"text": "Supervised semantic parsers () rely on manual annotation of logical forms, which is expensive.", "labels": [], "entities": []}, {"text": "On the other hand, existing unsupervised semantic parsers) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.", "labels": [], "entities": []}, {"text": "As in, we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.", "labels": [], "entities": []}, {"text": "However, we still model the logical form (now as a latent variable) to capture the complexities of language.", "labels": [], "entities": []}], "datasetContent": [{"text": "Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y.", "labels": [], "entities": []}, {"text": "We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.", "labels": [], "entities": []}, {"text": "We want to induce latent logical forms z (and parameters \u03b8) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.", "labels": [], "entities": []}, {"text": "The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.", "labels": [], "entities": [{"text": "program induction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7991139590740204}]}, {"text": "Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7474069893360138}]}, {"text": "Which one should we use?", "labels": [], "entities": []}, {"text": "The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.", "labels": [], "entities": []}, {"text": "CCG is one instantiation), which is used by many semantic parsers, e.g.,.", "labels": [], "entities": []}, {"text": "However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space.", "labels": [], "entities": [{"text": "program induction", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.771242767572403}]}, {"text": "At the same time, representations such as FunQL (), which was used in 590, are simpler but lack the full expressive power of lambda calculus.", "labels": [], "entities": []}, {"text": "The main technical contribution of this work is anew semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).", "labels": [], "entities": [{"text": "dependency-based compositional semantics (DCS)", "start_pos": 78, "end_pos": 124, "type": "TASK", "confidence": 0.71834929784139}]}, {"text": "The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.", "labels": [], "entities": []}, {"text": "We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).", "labels": [], "entities": [{"text": "GEO", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.7369948625564575}, {"text": "JOBS", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.6873003840446472}]}, {"text": "Our system outperforms all existing systems despite using no annotated logical forms.", "labels": [], "entities": []}, {"text": "We tested our system on two standard datasets, GEO and JOBS.", "labels": [], "entities": [{"text": "GEO", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9474452137947083}, {"text": "JOBS", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8984565734863281}]}, {"text": "In each dataset, each sentence x is annotated with a Prolog logical form, which we use only to evaluate and get an answer y.", "labels": [], "entities": []}, {"text": "This evaluation is done with respect to a world w.", "labels": [], "entities": []}, {"text": "Recall that a world w maps each predicate p \u2208 P to a set of tuples w(p).", "labels": [], "entities": []}, {"text": "There are three types of predicates in P: generic (e.g., argmax), data (e.g., city), and value (e.g., CA).", "labels": [], "entities": []}, {"text": "GEO has 48 non-value predicates and JOBS has 26.", "labels": [], "entities": [{"text": "GEO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9846774339675903}, {"text": "JOBS", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.956572413444519}]}, {"text": "For GEO, w is the standard US geography database that comes with the dataset.", "labels": [], "entities": [{"text": "GEO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8201350569725037}, {"text": "US geography database", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.7910538713137308}]}, {"text": "For JOBS, if we use the standard Jobs database, close to half the y's are empty, which makes it uninteresting.", "labels": [], "entities": [{"text": "JOBS", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.851460874080658}]}, {"text": "We therefore generated a random Jobs database instead as follows: we created 100 job IDs.", "labels": [], "entities": []}, {"text": "For each data predicate p (e.g., language), we add each possible tuple (e.g., (job37, Java)) to w(p) independently with probability 0.8.", "labels": [], "entities": []}, {"text": "We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).", "labels": [], "entities": [{"text": "GEO", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9016545414924622}, {"text": "JOBS", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.845830500125885}]}, {"text": "During development, we further held out a random 30% of the training sets for validation.", "labels": [], "entities": []}, {"text": "Our lexical triggers L include the following: (i) predicates fora small set of \u2248 20 function words (e.g., (most, argmax)), (ii) (x, x) for each value 596 78.9 Our system (DCS with L + ) 87.2: Results on GEO with 250 training and 250 test examples.", "labels": [], "entities": [{"text": "GEO", "start_pos": 203, "end_pos": 206, "type": "DATASET", "confidence": 0.8776648044586182}]}, {"text": "Our results are averaged over 10 random 250+250 splits taken from our 600 training examples.", "labels": [], "entities": []}, {"text": "Of the three systems that do not use logical forms, our two systems yield significant improvements.", "labels": [], "entities": []}, {"text": "Our better system even outperforms the system that uses logical forms.", "labels": [], "entities": []}, {"text": "predicate x in w (e.g., (Boston, Boston)), and (iii) predicates for each POS tag in {JJ, NN, NNS} (e.g., (JJ, size), (JJ, area), etc.).", "labels": [], "entities": [{"text": "Boston, Boston))", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8733191192150116}]}, {"text": "3 Predicates corresponding to verbs and prepositions (e.g., traverse) are not included as overt lexical triggers, but rather in the trace predicates L().", "labels": [], "entities": []}, {"text": "We also define an augmented lexicon L + which includes a prototype word x for each predicate appearing in (iii) above (e.g., (large, size)), which cancels the predicates triggered by x's POS tag.", "labels": [], "entities": []}, {"text": "For GEO, there are 22 prototype words; for JOBS, there are 5.", "labels": [], "entities": [{"text": "GEO", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9211156964302063}, {"text": "JOBS", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8896953463554382}]}, {"text": "Specifying these triggers requires minimal domain-specific supervision.", "labels": [], "entities": []}, {"text": "Results We first compare our system with Clarke et al.", "labels": [], "entities": []}, {"text": "(2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.", "labels": [], "entities": []}, {"text": "shows that our system using lexical triggers L (henceforth, DCS) outperforms SEMRESP (78.9% over 73.2%).", "labels": [], "entities": [{"text": "SEMRESP", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.5466645956039429}]}, {"text": "In fact, although neither DCS nor SEMRESP uses logical forms, DCS uses even less supervision than SEMRESP.", "labels": [], "entities": [{"text": "SEMRESP", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.8166544437408447}, {"text": "SEMRESP", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.8464035987854004}]}, {"text": "SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9303497076034546}]}, {"text": "In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms.", "labels": [], "entities": []}, {"text": "If we add prototype triggers (use L + ), the resulting system (DCS + ) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on GEO with 250 training and 250  test examples. Our results are averaged over 10 random  250+250 splits taken from our 600 training examples. Of  the three systems that do not use logical forms, our two  systems yield significant improvements. Our better sys- tem even outperforms the system that uses logical forms.", "labels": [], "entities": [{"text": "GEO", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8974354267120361}]}, {"text": " Table 3: Accuracy (recall) of systems on the two bench- marks. The systems are divided into three groups. Group  1 uses 10-fold cross-validation; groups 2 and 3 use the in- dependent test set. Groups 1 and 2 measure accuracy of  logical form; group 3 measures accuracy of the answer;  but there is very small difference between the two as seen  from the", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9935317635536194}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9636974930763245}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9975910186767578}, {"text": "accuracy", "start_pos": 261, "end_pos": 269, "type": "METRIC", "confidence": 0.9977961778640747}]}]}