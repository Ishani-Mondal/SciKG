{"title": [{"text": "Collective Classification of Congressional Floor-Debate Transcripts", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores approaches to sentiment classification of U.S. Congressional floor-debate transcripts.", "labels": [], "entities": [{"text": "sentiment classification of U.S. Congressional floor-debate transcripts", "start_pos": 34, "end_pos": 105, "type": "TASK", "confidence": 0.9114812612533569}]}, {"text": "Collective classification techniques are used to take advantage of the informal citation structure present in the debates.", "labels": [], "entities": [{"text": "Collective classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6531513631343842}]}, {"text": "We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms.", "labels": [], "entities": []}, {"text": "Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly out-performing the benchmark technique.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised document classification is a well-studied task.", "labels": [], "entities": [{"text": "Supervised document classification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8716823061307272}]}, {"text": "Research has been performed across many document types with a variety of classification tasks.", "labels": [], "entities": []}, {"text": "Examples are topic classification of newswire articles (, sentiment classification of movie reviews (), and satire classification of news articles.", "labels": [], "entities": [{"text": "topic classification of newswire articles", "start_pos": 13, "end_pos": 54, "type": "TASK", "confidence": 0.8446722149848938}, {"text": "sentiment classification of movie reviews", "start_pos": 58, "end_pos": 99, "type": "TASK", "confidence": 0.8726585507392883}, {"text": "satire classification of news articles", "start_pos": 108, "end_pos": 146, "type": "TASK", "confidence": 0.82742879986763}]}, {"text": "This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems.", "labels": [], "entities": []}, {"text": "This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation.", "labels": [], "entities": [{"text": "supervised document classification", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.6763431827227274}]}, {"text": "Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process.", "labels": [], "entities": [{"text": "document classification", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7162312716245651}]}, {"text": "Web encyclopedias and scholarly publications are two examples of document domains where network structures have been used to assist classification ().", "labels": [], "entities": []}, {"text": "The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effective novel feature models for iterative local classification of debate transcript data.", "labels": [], "entities": [{"text": "collective classification", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.725629597902298}, {"text": "ConVote corpus of congressional debate transcripts", "start_pos": 170, "end_pos": 220, "type": "DATASET", "confidence": 0.9601594706376394}, {"text": "collective document classification", "start_pos": 266, "end_pos": 300, "type": "TASK", "confidence": 0.7305752038955688}, {"text": "collective document classification", "start_pos": 363, "end_pos": 397, "type": "TASK", "confidence": 0.6974975864092509}]}, {"text": "In the next section (Section 2) we provide a formal definition of collective classification and describe the ConVote corpus that is the basis for our experimental evaluation.", "labels": [], "entities": [{"text": "collective classification", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.6901656240224838}, {"text": "ConVote corpus", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.9445874392986298}]}, {"text": "Subsequently, we describe and critique the established benchmark approach for congressional floor-debate transcript classification, before describing approaches based on three alternative collective classification algorithms (Section 3).", "labels": [], "entities": [{"text": "congressional floor-debate transcript classification", "start_pos": 78, "end_pos": 130, "type": "TASK", "confidence": 0.6995081305503845}]}, {"text": "We then present an experimental evaluation (Section 4).", "labels": [], "entities": []}, {"text": "Finally, we describe related work (Section 5) and offer analysis and conclusions (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we compare the performance of our dual-classifier and iterative-classifier approaches.", "labels": [], "entities": []}, {"text": "We also evaluate the performance of the three feature models for local classification.", "labels": [], "entities": [{"text": "local classification", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.6763336658477783}]}, {"text": "All accuracies are given as the percentages of instances correctly classified.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.995470404624939}]}, {"text": "Results are macroaveraged using 10 \u00d7 10-fold cross validation, i.e. 10 runs of 10-fold cross validation using different randomly assigned data splits.", "labels": [], "entities": []}, {"text": "Where quoted, statistical significance has been calculated using a two-tailed paired t-test measured overall 100 pairs with 10 degrees of freedom.", "labels": [], "entities": [{"text": "significance", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.6730259656906128}]}, {"text": "See Bouckaert  Early in our experimental work we noticed that performance often varied greatly depending on the debates that were allocated to training, tuning and testing.", "labels": [], "entities": []}, {"text": "This observation is supported by the per-fold scores that are the basis for the macro-average performance figures reported in, which tend to have large standard deviations.", "labels": [], "entities": []}, {"text": "The absolute standard deviations over the 100 evaluations for the minimum-cut and mean-field methods were 11.19% and 8.94% respectively.", "labels": [], "entities": [{"text": "absolute standard deviations", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.7095924417177836}]}, {"text": "These were significantly larger than the standard deviation for the contentonly baseline, which was 7.34%.", "labels": [], "entities": []}, {"text": "This leads us to conclude that the performance of collective classification methods is highly variable.", "labels": [], "entities": []}, {"text": "Bilgic and Getoor (2008) offer a possible explanation for this.", "labels": [], "entities": []}, {"text": "They note that the cost of incorrectly classifying a given instance can be magnified in collective classification, because errors are propagated throughout the network.", "labels": [], "entities": [{"text": "collective classification", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.6933093070983887}]}, {"text": "The extent to which this happens may depend on the random interaction between base classification accuracy and network structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.8446484804153442}]}, {"text": "There is scope for further work to more fully explain this phenomenon.", "labels": [], "entities": []}, {"text": "From these statistical and theoretical factors we infer that more reliable conclusions can be drawn from collective classification experiments that use cross-validation instead of a single, fixed data split.", "labels": [], "entities": []}, {"text": "use ICA to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts.", "labels": [], "entities": [{"text": "ICA", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6835001707077026}, {"text": "sentiment polarity classification", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.8225780526796976}]}, {"text": "Link features are derived from annotations giving frame relations and target relations.", "labels": [], "entities": []}, {"text": "Respectively, these relate dialogue acts based on the sentiment expressed and the object towards which the sentiment is expressed.", "labels": [], "entities": []}, {"text": "Somasundaran et al. provides another argument for the usefulness of collective classification (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information.", "labels": [], "entities": [{"text": "collective classification", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.699241116642952}, {"text": "ICA)", "start_pos": 108, "end_pos": 112, "type": "TASK", "confidence": 0.7993701994419098}]}, {"text": "propose an unsupervised method for classifying the stance of each contribution to an online debate concerning the merits of competing products.", "labels": [], "entities": []}, {"text": "Concessions to other stances are modeled, but there are no overt citations in the data that could be used to induce the network structure required for collective classification.", "labels": [], "entities": [{"text": "collective classification", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.6237876415252686}]}], "tableCaptions": [{"text": " Table 2 shows overall results for the three collective  classification algorithms. The iterative classifier was  run separately with citation count and context win-", "labels": [], "entities": [{"text": "collective  classification", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.6731083989143372}]}, {"text": " Table 2: Speaker classification accuracies (%) over connected, isolated and all instances. The marked results are  statistically significant over the content only benchmark ( p < .01,  \u2020 p < .001). The mean-field results are statistically  significant over minimum-cut (p < .05).", "labels": [], "entities": [{"text": "Speaker classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7855075001716614}]}]}