{"title": [{"text": "Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "Machine transliteration is defined as automatic phonetic translation of names across languages.", "labels": [], "entities": [{"text": "Machine transliteration", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.751859575510025}, {"text": "automatic phonetic translation of names across languages", "start_pos": 38, "end_pos": 94, "type": "TASK", "confidence": 0.8189527903284345}]}, {"text": "In this paper, we propose synchronous adaptor grammar, a novel nonpara-metric Bayesian learning approach, for machine transliteration.", "labels": [], "entities": [{"text": "machine transliteration", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7013606131076813}]}, {"text": "This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages.", "labels": [], "entities": []}, {"text": "The proposed model outperforms the state-of-the-art EM-based model in the English to Chinese translit-eration task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8316105306148529}, {"text": "crosslingual information retrieval", "start_pos": 92, "end_pos": 126, "type": "TASK", "confidence": 0.726791779200236}]}, {"text": "They are often translated through transliteration, i.e. translation by preserving how words sound in both languages.", "labels": [], "entities": []}, {"text": "In general, machine transliteration is often modelled as monotonic machine translation, the joint source-channel models (, or the sequential labeling problems.", "labels": [], "entities": [{"text": "machine transliteration", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7353266179561615}, {"text": "monotonic machine translation", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.7411902745564779}]}, {"text": "Syllable equivalents acquisition is a critical phase for all these models.", "labels": [], "entities": [{"text": "equivalents acquisition", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7265673577785492}]}, {"text": "Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "However, the EM algorithm may over-fit the training data by memorizing the whole training instances.", "labels": [], "entities": []}, {"text": "To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (.", "labels": [], "entities": []}, {"text": "Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments).", "labels": [], "entities": []}, {"text": "Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures.", "labels": [], "entities": []}, {"text": "They have been successfully applied to many machine learning tasks (.", "labels": [], "entities": []}, {"text": "Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on.", "labels": [], "entities": []}, {"text": "They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history.", "labels": [], "entities": []}, {"text": "Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them.", "labels": [], "entities": []}, {"text": "AGs have been used in various NLP tasks such as topic modeling, perspective modeling (, morphology analysis and word segmentation.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9260031580924988}, {"text": "perspective modeling", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7884241044521332}, {"text": "morphology analysis", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7202651798725128}, {"text": "word segmentation", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7636362612247467}]}, {"text": "In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process).", "labels": [], "entities": []}, {"text": "We also describe how transliteration could be modelled under this formalism.", "labels": [], "entities": []}, {"text": "It should be emphasized that the proposed method is language independent and heuristic-free.", "labels": [], "entities": []}, {"text": "Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task., N is a set of nonterminal symbols, T s /T tare source/target terminal symbols, R is a set of rewrite rules, S \u2208 N is the start symbol, \u0398 is the distribution of rule probabilities, Na \u2286 N is the set of adapted nonterminals, a \u2208 [0, 1], b \u2265 0 are vectors of discount and concentration parameters both indexed by adapted nonterminals, and \u03b1 are Dirichlet prior parameters.", "labels": [], "entities": []}, {"text": "draw cache index z n+1 \u223c P (z|z i<n ), where", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Transliteration data statistics", "labels": [], "entities": []}, {"text": " Table 2: Transliteration results, in the format of word ac- curacy / mean F-score. \"Syl\",\"Word\" and \"Col\" denote  the syllable, word and collocation grammar respectively.", "labels": [], "entities": [{"text": "word ac- curacy / mean F-score", "start_pos": 52, "end_pos": 82, "type": "METRIC", "confidence": 0.7026602412973132}]}]}