{"title": [{"text": "An Efficient Indexer for Large N-Gram Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce anew publicly available tool that implements efficient indexing and retrieval of large N-gram datasets, such as the Web1T 5-gram corpus.", "labels": [], "entities": [{"text": "Web1T 5-gram corpus", "start_pos": 129, "end_pos": 148, "type": "DATASET", "confidence": 0.918140729268392}]}, {"text": "Our tool indexes the entire Web1T dataset with an index size of only 100 MB and performs a retrieval of any N-gram with a single disk access.", "labels": [], "entities": [{"text": "Web1T dataset", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9762044847011566}]}, {"text": "With an increased index size of 420 MB and duplicate data, it also allows users to issue wildcard queries provided that the wild cards in the query are contiguous.", "labels": [], "entities": []}, {"text": "Furthermore, we also implement some of the smoothing algorithms that are designed specifically for large datasets and are shown to yield better language models than the traditional ones on the Web1T 5-gram corpus (Yuret, 2008).", "labels": [], "entities": [{"text": "Web1T 5-gram corpus", "start_pos": 193, "end_pos": 212, "type": "DATASET", "confidence": 0.8569435675938925}]}, {"text": "We demonstrate the effectiveness of our tool and the smoothing algorithms on the English Lexical Substitution task by a simple implementation that gives considerable improvement over a basic language model.", "labels": [], "entities": [{"text": "English Lexical Substitution task", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.6916607469320297}]}], "introductionContent": [{"text": "The goal of statistical language modeling is to capture the properties of a language through a probability distribution so that the probabilities of word sequences can be estimated.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.7680816054344177}]}, {"text": "Since the probability distribution is built from a corpus of the language by computing the frequencies of the N-grams found in the corpus, the data sparsity is always an issue with the language models.", "labels": [], "entities": []}, {"text": "Hence, as it is the case with many statistical models used in Natural Language Processing (NLP), the models give a much better performance with larger data sets.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.6887997984886169}]}, {"text": "However the large data sets, such as the Web1T 5-Gram corpus of (), present a major challenge.", "labels": [], "entities": [{"text": "Web1T 5-Gram corpus of", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.9410666674375534}]}, {"text": "The language models built from these sets cannot fit in memory, hence efficient accessing of the N-gram frequencies becomes an issue.", "labels": [], "entities": []}, {"text": "Trivial methods such as linear or binary search over the entire dataset in order to access a single N-gram prove inefficient, as even a binary search over a single file of 10,000,000 records, which is the case of the Web1T corpus, requires in the worst case log 2 (10, 000, 000) = 24 accesses to the disk drive.", "labels": [], "entities": [{"text": "Web1T corpus", "start_pos": 217, "end_pos": 229, "type": "DATASET", "confidence": 0.9388533234596252}]}, {"text": "Since the access to N-grams is costly for these large data sets, the implementation of further improvements such as smoothing algorithms becomes impractical.", "labels": [], "entities": []}, {"text": "In this paper, we overcome this problem by implementing a novel, publicly available tool 1 that employs an indexing strategy that reduces the access time to any N-gram in the Web1T corpus to a single disk access.", "labels": [], "entities": [{"text": "Web1T corpus", "start_pos": 175, "end_pos": 187, "type": "DATASET", "confidence": 0.9417389929294586}]}, {"text": "We also make a second contribution by implementing some of the smoothing models that take into account the size of the dataset, and are shown to yield up to 31% perplexity reduction on the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 189, "end_pos": 201, "type": "DATASET", "confidence": 0.932662159204483}]}, {"text": "Our implementation is space efficient, and provides a fast access to both the N-gram frequencies, as well as their smoothed probabilities.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the trial data", "labels": [], "entities": []}, {"text": " Table 2: Results on the test data", "labels": [], "entities": []}]}