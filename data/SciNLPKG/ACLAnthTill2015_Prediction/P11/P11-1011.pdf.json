{"title": [], "abstractContent": [{"text": "Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization , and segmentation, is an important part of query processing and understanding in information retrieval systems.", "labels": [], "entities": [{"text": "Marking up search queries", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8355341255664825}, {"text": "query processing", "start_pos": 142, "end_pos": 158, "type": "TASK", "confidence": 0.8588487803936005}]}, {"text": "Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools.", "labels": [], "entities": []}, {"text": "To address this challenge, we propose a probabilistic approach for performing joint query annotation.", "labels": [], "entities": []}, {"text": "First, we derive a robust set of unsupervised independent annotations , using queries and pseudo-relevance feedback.", "labels": [], "entities": []}, {"text": "Then, we stack additional classi-fiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9993616938591003}]}, {"text": "We evaluate our method using a range of queries extracted from a web search log.", "labels": [], "entities": []}, {"text": "Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic mark-up of textual documents with linguistic annotations such as part-of-speech tags, sentence constituents, named entities, or semantic roles is a common practice in natural language processing (NLP).", "labels": [], "entities": [{"text": "Automatic mark-up of textual documents with linguistic annotations such as part-of-speech tags, sentence constituents, named entities, or semantic roles", "start_pos": 0, "end_pos": 152, "type": "Description", "confidence": 0.7274160601875999}, {"text": "natural language processing (NLP)", "start_pos": 177, "end_pos": 210, "type": "TASK", "confidence": 0.7841565112272898}]}, {"text": "It is, however, much less common in information retrieval (IR) applications.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.8523188948631286}]}, {"text": "Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine.", "labels": [], "entities": []}, {"text": "There are several key differences between user queries and the documents used in NLP (e.g., news articles or web pages).", "labels": [], "entities": []}, {"text": "As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora.", "labels": [], "entities": []}, {"text": "The most salient difference between queries and documents is their length.", "labels": [], "entities": []}, {"text": "Most search queries are very short, and even longer queries are usually shorter than the average written sentence.", "labels": [], "entities": []}, {"text": "Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units.", "labels": [], "entities": []}, {"text": "A recent analysis of web query logs by  shows, however, that despite their brevity, queries are grammatically diverse.", "labels": [], "entities": []}, {"text": "Some queries are keyword concatenations, some are semicomplete verbal phrases and some are wh-questions.", "labels": [], "entities": []}, {"text": "It is essential for the search engine to correctly annotate the query structure, and the quality of these query annotations has been shown to be a crucial first step towards the development of reliable and robust query processing, representation and understanding algorithms (.", "labels": [], "entities": [{"text": "representation and understanding", "start_pos": 231, "end_pos": 263, "type": "TASK", "confidence": 0.7905038992563883}]}, {"text": "However, in current query annotation systems, even sentence-like queries are often hard to parse and annotate, as they are prone to contain misspellings and idiosyncratic grammatical structures.", "labels": [], "entities": []}, {"text": "102: Examples of a mark-up scheme for annotating capitalization (L -lowercase, C -otherwise), POS tags (Nnoun, V -verb, X -otherwise) and segmentation (B/I -beginning of/inside the chunk).", "labels": [], "entities": []}, {"text": "They also tend to lack prepositions, proper punctuation, or capitalization, since users (often correctly) assume that these features are disregarded by the retrieval system.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel joint query annotation method to improve the effectiveness of existing query annotations, especially for longer, more complex search queries.", "labels": [], "entities": []}, {"text": "Most existing research focuses on using a single type of annotation for information retrieval such as subject-verb-object dependencies (, namedentity recognition, phrase chunking (, or semantic labeling.", "labels": [], "entities": [{"text": "namedentity recognition", "start_pos": 138, "end_pos": 161, "type": "TASK", "confidence": 0.7355215549468994}, {"text": "phrase chunking", "start_pos": 163, "end_pos": 178, "type": "TASK", "confidence": 0.7822086811065674}, {"text": "semantic labeling", "start_pos": 185, "end_pos": 202, "type": "TASK", "confidence": 0.7024506330490112}]}, {"text": "In contrast, the main focus of this work is on developing a unified approach for performing reliable annotations of different types.", "labels": [], "entities": []}, {"text": "To this end, we propose a probabilistic method for performing a joint query annotation.", "labels": [], "entities": []}, {"text": "This method allows us to exploit the dependency between different unsupervised annotations to further improve the accuracy of the entire set of annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9987527132034302}]}, {"text": "For instance, our method can leverage the information about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9984524250030518}]}, {"text": "We empirically evaluate the joint query annotation method on a range of query types.", "labels": [], "entities": []}, {"text": "Instead of just focusing our attention on keyword queries, as is often done in previous work (, we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (.", "labels": [], "entities": [{"text": "IR", "start_pos": 270, "end_pos": 272, "type": "TASK", "confidence": 0.9562323093414307}]}, {"text": "We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we demonstrate several examples of annotated search queries.", "labels": [], "entities": []}, {"text": "Then, in Section 3, we introduce our joint query annotation method.", "labels": [], "entities": []}, {"text": "In Section 4 we describe two types of independent query annotations that are used as input for the joint query annotation.", "labels": [], "entities": []}, {"text": "Section 5 details the related work and Section 6 presents the experimental results.", "labels": [], "entities": []}, {"text": "We draw the conclusions from our work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluating the performance of our query annotation methods, we use a random sample of 250 queries 2 from a search log.", "labels": [], "entities": []}, {"text": "This sample is manually labeled with three annotations: capitalization, POS tags, and segmentation, according to the description of these annotations in.", "labels": [], "entities": []}, {"text": "In this set of 250 queries, there are 93 questions, 96 phrases contain- ing a verb, and 61 short keyword queries contains a single example of each of these types).", "labels": [], "entities": []}, {"text": "In order to test the effectiveness of the joint query annotation, we compare four methods.", "labels": [], "entities": []}, {"text": "In the first two methods, i-QRY and i-PRF the three annotations are done independently.", "labels": [], "entities": []}, {"text": "Method i-QRY is based on z * (QRY ) Q estimator (Eq. 3).", "labels": [], "entities": []}, {"text": "Method i-PRF is based on the z * (P RF ) Q estimator (Eq. 4).", "labels": [], "entities": [{"text": "z * (P RF ) Q estimator", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.7412559390068054}]}, {"text": "The next two methods, j-QRY and j-PRF, are joint annotation methods, which perform a joint optimization over the entire set of annotations, as described in the algorithm in. j-QRY and j-PRF differ in their choice of the initial independent annotation set Z * (I) Q inline (1) of the algorithm (see).", "labels": [], "entities": []}, {"text": "j-QRY uses only the annotations performed by i-QRY (3 initial independent annotation estimates), while j-PRF combines the annotations performed by i-QRY with the annotations performed by i-PRF (6 initial annotation estimates).", "labels": [], "entities": []}, {"text": "The CRF model training inline (6) of the algorithm is implemented using CRF++ toolkit 3 . 3 http://crfpp.sourceforge.net/ The performance of the joint annotation methods is estimated using a 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "In order to test the statistical significance of improvements attained by the proposed methods we use a two-sided Fisher's randomization test with 20,000 permutations.", "labels": [], "entities": []}, {"text": "Results with p-value < 0.05 are considered statistically significant.", "labels": [], "entities": []}, {"text": "For reporting the performance of our methods we use two measures.", "labels": [], "entities": []}, {"text": "The first measure is classification-oriented -treating the annotation decision for each query term as a classification.", "labels": [], "entities": []}, {"text": "In case of capitalization and segmentation annotations these decisions are binary and we compute the precision and recall metrics, and report F1 -their harmonic mean.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9990247488021851}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9886770248413086}, {"text": "F1 -their harmonic mean", "start_pos": 142, "end_pos": 165, "type": "METRIC", "confidence": 0.927150571346283}]}, {"text": "In case of POS tagging, the decisions are ternary, and hence we report the classification accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.8248051404953003}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9186005592346191}]}, {"text": "We also report an additional, IR-oriented performance measure.", "labels": [], "entities": []}, {"text": "As is typical in IR, we propose measuring the performance of the annotation methods on a per-query basis, to verify that the methods have uniform impact across queries.", "labels": [], "entities": [{"text": "IR", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9816142916679382}]}, {"text": "Accordingly, we report the mean of classification accuracies per query (MQA).", "labels": [], "entities": [{"text": "classification accuracies per query (MQA)", "start_pos": 35, "end_pos": 76, "type": "METRIC", "confidence": 0.6356621086597443}]}, {"text": "Formally, MQA is computed as where acc Q i is the classification accuracy for query Q i , and N is the number of queries.", "labels": [], "entities": [{"text": "MQA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6120892763137817}, {"text": "acc Q i", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9494320551554362}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8869855999946594}]}, {"text": "The empirical evaluation is conducted as follows.", "labels": [], "entities": []}, {"text": "In Section 6.2, we discuss the general performance of the four annotation techniques, and compare the effectiveness of independent and joint annotations.", "labels": [], "entities": []}, {"text": "In Section 6.3, we analyze the performance of the independent and joint annotation methods by query type.", "labels": [], "entities": []}, {"text": "In Section 6.4, we compare the difficulty of performing query annotations for different query types.", "labels": [], "entities": []}, {"text": "Finally, in Section 6.5, we compare the effectiveness of the proposed joint annotation for query segmentation with the existing query segmentation methods.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8338446915149689}]}, {"text": "shows the summary of the performance of the two independent and two joint annotation methods for the entire set of 250 queries.", "labels": [], "entities": []}, {"text": "For independent methods, we see that i-PRF outperforms i-QRY for 107  all annotation types, using both performance measures.", "labels": [], "entities": []}, {"text": "Table 2 also demonstrates that joint annotation has a different impact on various annotations for the same query type.", "labels": [], "entities": []}, {"text": "For instance, j-PRF has a significant positive effect on capitalization and segmentation for keyword queries, but only marginally improves the POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 143, "end_pos": 154, "type": "TASK", "confidence": 0.7023130059242249}]}, {"text": "Similarly, for the verbal phrases, j-PRF has a significant positive effect only for the segmentation annotation.", "labels": [], "entities": []}, {"text": "These variances in the performance of the j-PRF method point to the differences in the structure be-108 tween the query types.", "labels": [], "entities": []}, {"text": "While dependence between the annotations plays an important role for question and keyword queries, which often share a common grammatical structure, this dependence is less useful for verbal phrases, which have a more diverse linguistic structure.", "labels": [], "entities": []}, {"text": "Accordingly, a more in-depth investigation of the linguistic structure of the verbal phrase queries is an interesting direction for future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Detailed analysis of the query annotation performance for capitalization (CAP), POS tagging (TAG) and  segmentation by query type. Numbers in parentheses indicate % of improvement over the i-PRF baseline. Best result  per measure and annotation is boldfaced.  *  denotes statistically significant differences with i-PRF.", "labels": [], "entities": [{"text": "POS tagging (TAG)", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7751929521560669}]}]}