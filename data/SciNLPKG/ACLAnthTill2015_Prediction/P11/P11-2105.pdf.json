{"title": [{"text": "Hierarchical Text Classification with Latent Concepts", "labels": [], "entities": [{"text": "Hierarchical Text Classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6893528004487356}]}], "abstractContent": [{"text": "Recently, hierarchical text classification has become an active research topic.", "labels": [], "entities": [{"text": "hierarchical text classification", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7504756251970927}]}, {"text": "The essential idea is that the descendant classes can share the information of the ancestor classes in a predefined taxonomy.", "labels": [], "entities": []}, {"text": "In this paper, we claim that each class has several latent concepts and its subclasses share information with these different concepts respectively.", "labels": [], "entities": []}, {"text": "Then, we propose a variant Passive-Aggressive (PA) algorithm for hierarchical text classification with latent concepts.", "labels": [], "entities": [{"text": "hierarchical text classification", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.6191439231236776}]}, {"text": "Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms.", "labels": [], "entities": [{"text": "hierarchical classification", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.6636784672737122}]}], "introductionContent": [{"text": "Text classification is a crucial and well-proven method for organizing the collection of large scale documents.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7918227016925812}]}, {"text": "The predefined categories are formed by different criterions, e.g. \"Entertainment\", \"Sports\" and \"Education\" in news classification, \"Junk Email\" and \"Ordinary Email\" in email classification.", "labels": [], "entities": [{"text": "news classification", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.6709072440862656}, {"text": "email classification", "start_pos": 170, "end_pos": 190, "type": "TASK", "confidence": 0.6994268298149109}]}, {"text": "In the literature, many algorithms) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB) and soon.", "labels": [], "entities": []}, {"text": "Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications.", "labels": [], "entities": [{"text": "text classification", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7226774394512177}]}, {"text": "In past serval years, hierarchical text classification has become an active research topic in database area) and machine learning area (.", "labels": [], "entities": [{"text": "hierarchical text classification", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6339874466260275}]}, {"text": "Different with traditional classification, the document collections are organized as hierarchical class structure in many application fields: web taxonomies (i.e. the Yahoo!", "labels": [], "entities": []}, {"text": "Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs.", "labels": [], "entities": []}, {"text": "The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches.", "labels": [], "entities": [{"text": "hierarchical text classification", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.6717214186986288}]}, {"text": "The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy).", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6931610405445099}]}, {"text": "The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories).", "labels": [], "entities": []}, {"text": "The global approach builds only one classifier to discriminate all categories in a hierarchy).", "labels": [], "entities": []}, {"text": "The essential idea of global approach is that the close classes have some common underlying factors.", "labels": [], "entities": []}, {"text": "Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning.", "labels": [], "entities": []}, {"text": "Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain.", "labels": [], "entities": []}, {"text": "However, the taxonomy is defined artificially and is usually very difficult to organize for large scale taxonomy.", "labels": [], "entities": []}, {"text": "The subclasses of the same parent class maybe dissimilar and can be grouped in different concepts, so it bring great challenge to hierarchi-598 cal classification.", "labels": [], "entities": [{"text": "hierarchi-598 cal classification", "start_pos": 130, "end_pos": 162, "type": "TASK", "confidence": 0.6222628951072693}]}, {"text": "For example, the \"Sports\" node in a taxonomy have six subclasses), but these subclass can be grouped into three unobservable concepts.", "labels": [], "entities": []}, {"text": "These concepts can show the underlying factors more clearly.", "labels": [], "entities": []}, {"text": "In this paper, we claim that each class may have several latent concepts and its subclasses share information with these different concepts respectively.", "labels": [], "entities": []}, {"text": "Then we propose a variant Passive-Aggressive (PA) algorithm to maximizes the margins between latent paths.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the basic model of hierarchical classification.", "labels": [], "entities": [{"text": "hierarchical classification", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.7614168524742126}]}, {"text": "Then we propose our algorithm in section 3.", "labels": [], "entities": []}, {"text": "Section 4 gives experimental analysis.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed algorithm on two datasets with hierarchical category structure.", "labels": [], "entities": []}, {"text": "WIPO-alpha dataset The dataset 1 consisted of the 1372 training and 358 testing document comprising the D section of the hierarchy.", "labels": [], "entities": [{"text": "WIPO-alpha dataset", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.978054404258728}]}, {"text": "The number of nodes in the hierarchy was 188, with maximum depth 3.", "labels": [], "entities": []}, {"text": "The dataset was processed into bag-of-words representation with TF\u00b7IDF input : training data set: (x n , y n ), n = 1, \u00b7 \u00b7 \u00b7 , N , and parameters: C, K output: w Initialize: cw \u2190 0,; Here, we use the dry-run dataset(task 1).", "labels": [], "entities": [{"text": "IDF", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.6384477615356445}]}], "tableCaptions": [{"text": " Table 1: Results on WIPO-alpha Dataset.\"-\" means that  the result is not available in the author's paper.", "labels": [], "entities": [{"text": "WIPO-alpha Dataset.", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.8966686129570007}]}, {"text": " Table 2: Results on LSHTC dry-run Dataset", "labels": [], "entities": [{"text": "LSHTC dry-run Dataset", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.8477906584739685}]}]}