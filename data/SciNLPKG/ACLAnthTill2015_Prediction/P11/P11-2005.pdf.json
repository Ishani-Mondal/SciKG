{"title": [{"text": "An Empirical Investigation of Discounting in Cross-Domain Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the empirical behavior of n-gram discounts within and across domains.", "labels": [], "entities": []}, {"text": "When a language model is trained and evaluated on two corpora from exactly the same domain , discounts are roughly constant, matching the assumptions of modified Kneser-Ney LMs.", "labels": [], "entities": []}, {"text": "However, when training and test corpora diverge, the empirical discount grows essentially as a linear function of the n-gram count.", "labels": [], "entities": []}, {"text": "We adapt a Kneser-Ney language model to incorporate such growing discounts, resulting in perplexity improvements over modified Kneser-Ney and Jelinek-Mercer baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discounting, or subtracting from the count of each n-gram, is one of the core aspects of Kneser-Ney language modeling.", "labels": [], "entities": [{"text": "Kneser-Ney language modeling", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.5376109182834625}]}, {"text": "For all but the smallest n-gram counts, Kneser-Ney uses a single discount, one that does not grow with the ngram count, because such constant-discounting was seen in early experiments on held-out data.", "labels": [], "entities": []}, {"text": "However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8438917994499207}]}, {"text": "In particular, modeling crossdomain effects has become increasingly more important, and deployed systems must frequently process data that is out-of-domain from the standpoint of the language model.", "labels": [], "entities": []}, {"text": "In this work, we perform experiments on heldout data to evaluate how discounting behaves in the cross-domain setting.", "labels": [], "entities": []}, {"text": "We find that, when training and testing on corpora that are as similar as possible, empirical discounts indeed do not grow with ngram count, which validates the parametric assumption of Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "ngram count", "start_pos": 128, "end_pos": 139, "type": "METRIC", "confidence": 0.9256110489368439}]}, {"text": "However, when the train and evaluation corpora differ, even slightly, discounts generally exhibit linear growth in the count of the n-gram, with the amount of growth being closely correlated with the corpus divergence.", "labels": [], "entities": []}, {"text": "Finally, we build a language model exploiting a parametric form of the growing discount and show perplexity gains of up to 5.4% over modified Kneser-Ney.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perplexities of the growing discounts language  model (GDLM) and its purely linear variant (GDLM- LIN), which are contributions of this work, versus  the modified Kneser-Ney (MKNLM), basic Kneser-Ney  (KNLM), and Jelinek-Mercer (JMLM) baselines. We  report results for in-domain (NYT00+01) and out-of- domain (AFP02+05+06) training corpora, for two meth- ods of closing the vocabulary.", "labels": [], "entities": [{"text": "NYT00+01", "start_pos": 290, "end_pos": 298, "type": "DATASET", "confidence": 0.9087430238723755}, {"text": "AFP02+05+06) training corpora", "start_pos": 320, "end_pos": 349, "type": "DATASET", "confidence": 0.8736907169222832}]}, {"text": " Table 1. As expected,  for in-domain data, GDLM performs comparably to  MKNLM, since the discounts do not grow and so  there is little to be gained by choosing a param-", "labels": [], "entities": []}]}