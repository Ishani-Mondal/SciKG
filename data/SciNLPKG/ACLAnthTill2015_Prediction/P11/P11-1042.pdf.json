{"title": [{"text": "Unsupervised Word Alignment with Arbitrary Features", "labels": [], "entities": [{"text": "Unsupervised Word Alignment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5630961259206136}]}], "abstractContent": [{"text": "We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al.", "labels": [], "entities": []}, {"text": "In our model, arbitrary, non-independent features maybe freely incorporated , thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional inde-pendencies of the generative process.", "labels": [], "entities": []}, {"text": "However , unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unanno-tated parallel sentences, rather than from supervised word alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.6957538276910782}]}, {"text": "Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative base-lines in a number of language pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.9516667127609253}]}], "introductionContent": [{"text": "Word alignment is an important subtask in statistical machine translation which is typically solved in one of two ways.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7532807290554047}, {"text": "statistical machine translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6925275723139445}]}, {"text": "The more common approach uses a generative translation model that relates bilingual string pairs using a latent alignment variable to designate which source words (or phrases) generate which target words.", "labels": [], "entities": [{"text": "generative translation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.9658716022968292}]}, {"text": "The parameters in these models can be learned straightforwardly from parallel sentences using EM, and standard inference techniques can recover most probable alignments (.", "labels": [], "entities": []}, {"text": "This approach is attractive because it only requires parallel training data.", "labels": [], "entities": []}, {"text": "An alternative to the generative approach uses a discriminatively trained alignment model to predict word alignments in the parallel corpus.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.6966893523931503}]}, {"text": "Discriminative models are attractive because they can incorporate arbitrary, overlapping features, meaning that errors observed in the predictions made by the model can be addressed by engineering new and better features.", "labels": [], "entities": []}, {"text": "Unfortunately, both approaches are problematic, but in different ways.", "labels": [], "entities": []}, {"text": "In the case of discriminative alignment models, manual alignment data is required for training, which is problematic for at least three reasons.", "labels": [], "entities": []}, {"text": "Manual alignments are notoriously difficult to create and are available only fora handful of language pairs.", "labels": [], "entities": []}, {"text": "Second, manual alignments impose a commitment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data) or maybe constrained by requirements of other processing components, such parsers.", "labels": [], "entities": []}, {"text": "Third, the \"correct\" alignment annotation for different tasks may vary: for example, relatively denser or sparser alignments maybe optimal for different approaches to (downstream) translation model induction ().", "labels": [], "entities": [{"text": "translation model induction", "start_pos": 180, "end_pos": 207, "type": "TASK", "confidence": 0.8802708188692728}]}, {"text": "Generative models have a different limitation: the joint probability of a particular setting of the random variables must factorize according to steps in a process that successively \"generates\" the values of the variables.", "labels": [], "entities": []}, {"text": "At each step, the probability of some value being generated may depend only on the generation history (or a subset thereof), and the possible values a variable will take must form a locally normalized conditional probability distribution (CPD).", "labels": [], "entities": []}, {"text": "While these locally normalized CPDs maybe pa-409 rameterized so as to make use of multiple, overlapping features, the requirement that models factorize according to a particular generative process imposes a considerable restriction on the kinds of features that can be incorporated.", "labels": [], "entities": []}, {"text": "When wanted to incorporate a fertility model to create their Models 3 through 5, the generative process used in Models 1 and 2 (where target words were generated one by one from source words independently of each other) had to be abandoned in favor of one in which each source word had to first decide how many targets it would generate.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a discriminatively trained, globally normalized log-linear model of lexical translation that can incorporate arbitrary, overlapping features, and use it to infer word alignments.", "labels": [], "entities": [{"text": "lexical translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7664034366607666}, {"text": "word alignments", "start_pos": 190, "end_pos": 205, "type": "TASK", "confidence": 0.7307614088058472}]}, {"text": "Our model enjoys the usual benefits of discriminative modeling (e.g., parameter regularization, wellunderstood learning algorithms), but is trained entirely from parallel sentences without gold-standard word alignments.", "labels": [], "entities": []}, {"text": "Thus, it addresses the two limitations of current word alignment approaches.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7540955543518066}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "We begin by introducing our model ( \u00a72), and follow this with a discussion of tractability, parameter estimation, and inference using finite-state techniques ( \u00a73).", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.6496745496988297}]}, {"text": "We then describe the specific features we used ( \u00a74) and provide experimental evaluation of the model, showing substantial improvements in three diverse language pairs ( \u00a75).", "labels": [], "entities": []}, {"text": "We conclude with an analysis of related prior work ( \u00a76) and a general discussion ( \u00a78).", "labels": [], "entities": []}], "datasetContent": [{"text": "We now turn to an empirical assessment of our model.", "labels": [], "entities": []}, {"text": "Using various datasets, we evaluate the performance of the models' intrinsic quality and theirtheir alignments' contribution to a standard machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.6956491470336914}]}, {"text": "We make use of parallel corpora from languages with very different typologies: a small (0.8M words) Chinese-English corpus from the tourism and travel domain (), a corpus of Czech-English news commentary (3.1M words), and an Urdu-English corpus (2M words) provided by NIST for the 2009 Open MT Evaluation.", "labels": [], "entities": [{"text": "NIST", "start_pos": 268, "end_pos": 272, "type": "DATASET", "confidence": 0.963512122631073}, {"text": "Open MT Evaluation", "start_pos": 286, "end_pos": 304, "type": "DATASET", "confidence": 0.597549557685852}]}, {"text": "These pairs were selected since each poses different alignment challenges (word or-der in Chinese and Urdu, morphological complexity in Czech, and a non-alphabetic writing system in Chinese), and confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation up and running.", "labels": [], "entities": []}, {"text": "Future work will explore the scalability characteristics and limits of the model.", "labels": [], "entities": []}, {"text": "Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relatively free word order.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7825822234153748}]}, {"text": "For this language pair, we evaluate alignment error rate using the manual alignment corpus described by. summarizes the results.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.7326542238394419}]}, {"text": "Chinese-English poses a different set of problems for alignment.", "labels": [], "entities": [{"text": "alignment", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.9836764931678772}]}, {"text": "While Chinese words have rather simple morphology, the Chinese writing system renders our orthographic features useless.", "labels": [], "entities": []}, {"text": "Despite these challenges, the Chinese re- sults in show the same pattern of results as seen in Czech-English.", "labels": [], "entities": []}, {"text": "Urdu-English is a more challenging language pair for word alignment than the previous two we have considered.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7882936596870422}]}, {"text": "The parallel data is drawn from numerous genres, and much of it was acquired automatically, making it quite noisy.", "labels": [], "entities": []}, {"text": "So our models must not only predict good translations, they must cope with bad ones as well.", "labels": [], "entities": []}, {"text": "Second, there has been no previous work on discriminative modeling of Urdu, since, to our knowledge, no manual alignments have been created.", "labels": [], "entities": [{"text": "discriminative modeling of Urdu", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.7366423457860947}]}, {"text": "Finally, unlike English, Urdu is a head-final language: not only does it have SOV word order, but rather than prepositions, it has post-positions, which follow the nouns they modify, meaning its large scale word order is substantially 415 different from that of English.", "labels": [], "entities": []}, {"text": "demonstrates the same pattern of improving results with our alignment model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Czech-English experimental results. \u02dc  \u03c6 sing. is the  average fertility of singleton source words.", "labels": [], "entities": [{"text": "\u02dc  \u03c6 sing.", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.8168754577636719}]}, {"text": " Table 3: Chinese-English experimental results.  \u02dc  \u03c6 sing. \u2193 # rules \u2191  Model 4  e | f  4.4  f | e  3.9  sym.  3.6  52,323  Our model e | f  3.5  f | e  2.6  sym.  3.1  54,077", "labels": [], "entities": []}, {"text": " Table 4: Urdu-English experimental results.  \u02dc  \u03c6 sing. \u2193 # rules \u2191  Model 4  e | f  6.5  f | e  8.0  sym.  3.2  244,570  Our model e | f  4.8  f | e  8.3  sym.  2.3  260,953", "labels": [], "entities": []}, {"text": " Table 5: The most highly weighted source path bigram  features in the English-Czech, -Chinese, and -Urdu mod- els.", "labels": [], "entities": []}]}