{"title": [{"text": "Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning.", "labels": [], "entities": []}, {"text": "We use unsupervised data to generate informative 'condensed feature represen-tations' from the original feature set used in supervised NLP systems.", "labels": [], "entities": []}, {"text": "The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-of-the-art performance provided by the recently developed high-performance semi-supervised learning technique.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 102, "end_pos": 111, "type": "TASK", "confidence": 0.8739230036735535}]}, {"text": "Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III.", "labels": [], "entities": [{"text": "F-score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.970772922039032}, {"text": "CoNLL-2003 NER data", "start_pos": 133, "end_pos": 152, "type": "DATASET", "confidence": 0.8507491946220398}, {"text": "UAS 93.55", "start_pos": 158, "end_pos": 167, "type": "DATASET", "confidence": 0.8818976879119873}, {"text": "dependency parsing", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.8682031333446503}, {"text": "PTB-III", "start_pos": 229, "end_pos": 236, "type": "DATASET", "confidence": 0.9735027551651001}]}], "introductionContent": [{"text": "In the last decade, supervised learning has become a standard way to train the models of many natural language processing (NLP) systems.", "labels": [], "entities": []}, {"text": "One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data.", "labels": [], "entities": []}, {"text": "Specifically, an approach that involves incorporating 'clusteringbased word representations (CWR)' induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition () and dependency parsing (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 303, "end_pos": 327, "type": "TASK", "confidence": 0.6245527764161428}, {"text": "dependency parsing", "start_pos": 335, "end_pos": 353, "type": "TASK", "confidence": 0.8503897488117218}]}, {"text": "We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality.", "labels": [], "entities": []}, {"text": "The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems.", "labels": [], "entities": []}, {"text": "The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method.", "labels": [], "entities": []}, {"text": "Then, with the iCWR approach, C is induced independently from F, and used in addition to F in supervised learning, i.e., F \u222a C.", "labels": [], "entities": []}, {"text": "In contrast, in our method H is directly induced from F with the help of an existing model already trained by supervised learning with F, and used in place of F in supervised learning.", "labels": [], "entities": []}, {"text": "The largest contribution of our method is that it offers an architecture that can drastically reduce the number of features, i.e., from 10M features in F to less than 1K features in H by constructing 'condensed feature representations (COFER)', which is anew and very unique property that cannot be matched by previous semi-supervised learning methods including the iCWR approach.", "labels": [], "entities": []}, {"text": "One noteworthy feature of our method is that there is no need to handle sparse and high-dimensional feature spaces often used in many supervised NLP systems, which is one of the main causes of the data sparseness problem often encountered when we learn the model with a supervised leaning algorithm.", "labels": [], "entities": []}, {"text": "As a result, NLP systems that are both compact and highperformance can be built by retraining the model with the obtained condensed feature set H.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on two different NLP tasks, namely NER and dependency parsing.", "labels": [], "entities": [{"text": "NER", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.931388795375824}, {"text": "dependency parsing", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8588171601295471}]}, {"text": "To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and and dependency parsing ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.775244265794754}]}, {"text": "For the supervised datasets, we used CoNLL'03 shared task data for NER, and the Penn Treebank III (PTB) corpus for dependency parsing.", "labels": [], "entities": [{"text": "CoNLL'03 shared task data", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.7889344394207001}, {"text": "NER", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.8801555037498474}, {"text": "Penn Treebank III (PTB) corpus", "start_pos": 80, "end_pos": 110, "type": "DATASET", "confidence": 0.9714904597827366}, {"text": "dependency parsing", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.8526559770107269}]}, {"text": "We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison with previous top-line systems on  test data. (#.USD: unsupervised data size. #.AF: the size  of active features in the trained model.)", "labels": [], "entities": [{"text": "USD", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8269798159599304}, {"text": "AF", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9256792664527893}]}]}