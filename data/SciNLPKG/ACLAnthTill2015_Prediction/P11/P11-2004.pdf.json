{"title": [{"text": "Efficient Online Locality Sensitive Hashing via Reservoir Counting", "labels": [], "entities": [{"text": "Reservoir Counting", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7050003409385681}]}], "abstractContent": [{"text": "We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing.", "labels": [], "entities": [{"text": "Reservoir Counting", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7492926716804504}, {"text": "Locality Sensitive Hashing", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.6520187755425771}]}, {"text": "This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9161141514778137}]}], "introductionContent": [{"text": "Feature vectors based on lexical co-occurrence are often of a high dimension, d.", "labels": [], "entities": []}, {"text": "This leads to O(d) operations to calculate cosine similarity, a fundamental tool in distributional semantics.", "labels": [], "entities": [{"text": "O", "start_pos": 14, "end_pos": 15, "type": "METRIC", "confidence": 0.9664224982261658}]}, {"text": "This is improved in practice through the use of data structures that exploit feature sparsity, leading to an expected O(f ) operations, where f is the number of unique features we expect to have non-zero entries in a given vector.", "labels": [], "entities": [{"text": "O", "start_pos": 118, "end_pos": 119, "type": "METRIC", "confidence": 0.9947642087936401}]}, {"text": "showed that the Locality Sensitive Hash (LSH) procedure of, following from Indyk and and, could be successfully used to compress textually derived feature vectors in order to achieve speed efficiencies in large-scale noun clustering.", "labels": [], "entities": [{"text": "noun clustering", "start_pos": 217, "end_pos": 232, "type": "TASK", "confidence": 0.7417051196098328}]}, {"text": "Such LSH bit signatures are constructed using the following hash function, where v \u2208 Rd is a vector in the original feature space, and r is randomly drawn from N (0, 1) d : h( v) = 1 if v \u00b7 r \u2265 0, 0 otherwise.", "labels": [], "entities": [{"text": "LSH bit signatures", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.6289314726988474}]}, {"text": "If h b ( v) is the b-bit signature resulting from b such hash functions, then the cosine similarity between vectors u and v is approximated by: where D(\u00b7, \u00b7) is Hamming distance, the number of bits that disagree.", "labels": [], "entities": [{"text": "D", "start_pos": 150, "end_pos": 151, "type": "METRIC", "confidence": 0.9735133647918701}]}, {"text": "This technique is used when b d, which leads to faster pair-wise comparisons between vectors, and a lower memory footprint.", "labels": [], "entities": []}, {"text": "Van Durme and Lall (2010) observed 1 that if the feature values are additive over a dataset (e.g., when collecting word co-occurrence frequencies), then these signatures maybe constructed online by unrolling the dot-product into a series of local operations: v \u00b7 r i = \u03a3 t v t \u00b7 r i , where v t represents features observed locally at time tin a data-stream.", "labels": [], "entities": []}, {"text": "Since updates maybe done locally, feature vectors do not need to be stored explicitly.", "labels": [], "entities": []}, {"text": "This directly leads to significant space savings, as only one counter is needed for each of the b running sums.", "labels": [], "entities": []}, {"text": "In this work we focus on the following observation: the counters used to store the running sums may themselves bean inefficient use of space, in that they maybe amenable to compression through approximation.", "labels": [], "entities": []}, {"text": "Since the accuracy of this LSH routine is a function of b, then if we were able to reduce the online requirements of each counter, we might afford a larger number of projections.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992330074310303}]}, {"text": "Even if a chance of approximation error were introduced for each hash function, this maybe justified in greater overall fidelity from the resultant increase in b.", "labels": [], "entities": []}, {"text": "Thus, we propose to approximate the online hash function, using a novel technique we call Reservoir Counting, in order to create a space trade-off between the number of projections and the amount of memory each projection requires.", "labels": [], "entities": []}, {"text": "We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9967994689941406}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.985700249671936}]}, {"text": "This result is relevant to work in large-scale distributional semantics), as well as large-scale processing of social media ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average over repeated calls to A and A .", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9980291724205017}]}]}