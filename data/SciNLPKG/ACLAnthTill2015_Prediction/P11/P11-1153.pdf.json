{"title": [{"text": "Structural Topic Model for Latent Topical Structure Analysis", "labels": [], "entities": [{"text": "Latent Topical Structure Analysis", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.7413989380002022}]}], "abstractContent": [{"text": "Topic models have been successfully applied to many document analysis tasks to discover topics embedded in text.", "labels": [], "entities": [{"text": "document analysis tasks", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7611781160036722}]}, {"text": "However, existing topic models generally cannot capture the latent topical structures in documents.", "labels": [], "entities": []}, {"text": "Since languages are intrinsically cohesive and coherent , modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.7215950042009354}]}, {"text": "In this work, we propose anew topic model, Structural Topic Model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly model-ing topical transitions with a latent first-order Markov chain.", "labels": [], "entities": []}, {"text": "Experiment results show that the proposed Structural Topic Model can effectively discover topical structures in text, and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering.", "labels": [], "entities": [{"text": "sentence annotation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.7077666521072388}, {"text": "sentence ordering", "start_pos": 227, "end_pos": 244, "type": "TASK", "confidence": 0.7364504933357239}]}], "introductionContent": [{"text": "A great amount of effort has recently been made in applying statistical topic models) to explore word co-occurrence patterns, i.e. topics, embedded in documents.", "labels": [], "entities": []}, {"text": "Topic models have become important building blocks of many interesting applications (see e.g.,).", "labels": [], "entities": []}, {"text": "In general, topic models can discover word clustering patterns in documents and project each document to a latent topic space formed by such word clusters.", "labels": [], "entities": []}, {"text": "However, the topical structure in a document, i.e., the internal dependency between the topics, is generally not captured due to the exchangeability assumption ( , i.e., the document generation probabilities are invariant to content permutation.", "labels": [], "entities": []}, {"text": "In reality, natural language text rarely consists of isolated, unrelated sentences, but rather collocated, structured and coherent groups of sentences.", "labels": [], "entities": []}, {"text": "Ignoring such latent topical structures inside the documents means wasting valuable clues about topics and thus would lead to non-optimal topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.7128077149391174}]}, {"text": "Taking apartment rental advertisements as an example, when people write advertisements for their apartments, it's natural to first introduce \"size\" and \"address\" of the apartment, and then \"rent\" and \"contact\".", "labels": [], "entities": []}, {"text": "Few people would talk about \"restriction\" first.", "labels": [], "entities": [{"text": "restriction\"", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.8572279810905457}]}, {"text": "If this kind of topical structures are captured by a topic model, it would not only improve the topic mining results, but, more importantly, also help many other document analysis tasks, such as sentence annotation and sentence ordering.", "labels": [], "entities": [{"text": "topic mining", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.7574383914470673}, {"text": "sentence annotation", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.7124843746423721}, {"text": "sentence ordering", "start_pos": 219, "end_pos": 236, "type": "TASK", "confidence": 0.7560834884643555}]}, {"text": "Nevertheless, very few existing topic models attempted to model such structural dependency among topics.", "labels": [], "entities": []}, {"text": "The Aspect HMM model introduced in) combines pLSA) with HMM to perform document segmentation over text streams.", "labels": [], "entities": [{"text": "document segmentation", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.700590655207634}]}, {"text": "However, Aspect HMM separately estimates the topics in the training set and depends on heuristics to infer the transitional relations between topics.", "labels": [], "entities": []}, {"text": "The Hidden Topic Markov Model (HTMM) proposed by () extends the traditional topic models by assuming words in each sentence share the same topic assignment, and topics transit between adjacent sentences.", "labels": [], "entities": []}, {"text": "However, the transitional structures among topics, i.e., how likely one topic would follow another topic, are not captured in this model.", "labels": [], "entities": []}, {"text": "1526 In this paper, we propose anew topic model, named to model and analyze both latent topics and topical structures in text documents.", "labels": [], "entities": []}, {"text": "To do so, strTM assumes: 1) words in a document are either drawn from a content topic or a functional (i.e., background) topic; 2) words in the same sentence share the same content topic; and 3) content topics in the adjacent sentences follow a topic transition that satisfies the first order Markov property.", "labels": [], "entities": []}, {"text": "The first assumption distinguishes the semantics of the occurrence of each word in the document, the second requirement confines the unrealistic \"bag-of-word\" assumption into a tighter unit, and the third assumption exploits the connection between adjacent sentences.", "labels": [], "entities": []}, {"text": "To evaluate the usefulness of the identified topical structures by strTM, we applied strTM to the tasks of sentence annotation and sentence ordering, where correctly modeling the document structure is crucial.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7133244872093201}]}, {"text": "On the corpus of 8,031 apartment advertisements from craiglist () and 1,991 movie reviews from IMDB (), strTM achieved encouraging improvement in both tasks compared with the baseline methods that don't explicitly model the topical structure.", "labels": [], "entities": [{"text": "craiglist", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9175236821174622}, {"text": "IMDB", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.889058530330658}]}, {"text": "The results confirm the necessity of modeling the latent topical structures inside documents, and also demonstrate the advantages of the proposed strTM over existing topic models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we demonstrate the effectiveness of strTM in identifying latent topical structures from documents, and quantitatively evaluate how the mined topic transitions can help the tasks of sentence annotation and sentence ordering.", "labels": [], "entities": [{"text": "identifying latent topical structures from documents", "start_pos": 62, "end_pos": 114, "type": "TASK", "confidence": 0.7426706900199255}, {"text": "sentence annotation", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.7033187001943588}, {"text": "sentence ordering", "start_pos": 222, "end_pos": 239, "type": "TASK", "confidence": 0.7279911637306213}]}], "tableCaptions": [{"text": " Table 1: Summary of evaluation data set", "labels": [], "entities": []}, {"text": " Table 2: Comparison of estimated topic transitions on  Ads data set", "labels": [], "entities": [{"text": "Ads data set", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.8118409017721812}]}, {"text": " Table 3: Sentence annotation performance on Ads data  set", "labels": [], "entities": [{"text": "Sentence annotation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9043107032775879}, {"text": "Ads data  set", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8175840179125468}]}, {"text": " Table 4: Sentence annotation performance on Review  data set", "labels": [], "entities": [{"text": "Sentence annotation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8898677229881287}, {"text": "Review  data set", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.8111919860045115}]}, {"text": " Table 5: Sentence annotation performance according to  structural fitness", "labels": [], "entities": [{"text": "Sentence annotation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9588778913021088}]}, {"text": " Table 5. From this table,  we can find that when the testing documents follow  the regular patterns as in the training data, i.e., top  50 group, strTM performs significantly better than  the other methods; when the testing documents don't", "labels": [], "entities": []}]}