{"title": [{"text": "Automated Whole Sentence Grammar Correction Using a Noisy Channel Model", "labels": [], "entities": [{"text": "Whole Sentence Grammar Correction", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6483702436089516}]}], "abstractContent": [{"text": "Automated grammar correction techniques have seen improvement over the years, but there is still much room for increased performance.", "labels": [], "entities": [{"text": "Automated grammar correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6287035942077637}]}, {"text": "Current correction techniques mainly focus on identifying and correcting a specific type of error, such as verb form misuse or preposition misuse, which restricts the corrections to a limited scope.", "labels": [], "entities": [{"text": "verb form misuse or preposition misuse", "start_pos": 107, "end_pos": 145, "type": "TASK", "confidence": 0.601538727680842}]}, {"text": "We introduce a novel technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections.", "labels": [], "entities": []}, {"text": "We show how to use the EM algorithm to learn the parameters of the noise model, using only a data set of erroneous sentences, given the proper language model.", "labels": [], "entities": []}, {"text": "This frees us from the burden of acquiring a large corpora of corrected sentences.", "labels": [], "entities": []}, {"text": "We also present a cheap and efficient way to provide automated evaluation results for grammar corrections by using BLEU and METEOR, in contrast to the commonly used manual evaluations.", "labels": [], "entities": [{"text": "grammar corrections", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7037055939435959}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9985095858573914}, {"text": "METEOR", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9875301122665405}]}], "introductionContent": [{"text": "The process of editing written text is performed by humans on a daily basis.", "labels": [], "entities": []}, {"text": "Humans work by first identifying the writer's intent, and then transforming the text so that it is coherent and error free.", "labels": [], "entities": []}, {"text": "They can read text with several spelling errors and grammatical errors and still easily identify what the author originally meant to write.", "labels": [], "entities": []}, {"text": "Unfortunately, current computer systems are still far from such capabilities when it comes to the task of recognizing incorrect text input.", "labels": [], "entities": [{"text": "recognizing incorrect text input", "start_pos": 106, "end_pos": 138, "type": "TASK", "confidence": 0.821447879076004}]}, {"text": "Various approaches have been taken, but to date it seems that even many spell checkers such as Aspell do not take context into consideration, which prevents them from finding misspellings which have the same form as valid words.", "labels": [], "entities": [{"text": "spell checkers", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7178009748458862}]}, {"text": "Also, current grammar correction systems are mostly rule-based, searching the text for defined types of rule violations in the English grammar.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7392051815986633}]}, {"text": "While this approach has had some success in finding various grammatical errors, it is confined to specifically defined errors.", "labels": [], "entities": []}, {"text": "In this paper, we approach this problem by modeling various types of human errors using a noisy channel model.", "labels": [], "entities": []}, {"text": "Correct sentences are produced by a predefined generative probabilistic model, and lesioned by the noise model.", "labels": [], "entities": []}, {"text": "We learn the noise model parameters using an expectation-maximization (EM) approach.", "labels": [], "entities": []}, {"text": "Our model allows us to deduce the original intended sentence by looking for the the highest probability parses over the entire sentence, which leads to automated whole sentence spelling and grammar correction based on contextual information.", "labels": [], "entities": [{"text": "whole sentence spelling", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.6185363829135895}, {"text": "grammar correction", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.6740367114543915}]}, {"text": "In Section 2, we discuss previous work, followed by an explanation of our model and its implementation in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "In Section 5 we present a novel technique for evaluating the task of automated grammar and spelling correction, along with the data set we collected for our experiments.", "labels": [], "entities": [{"text": "automated grammar and spelling correction", "start_pos": 69, "end_pos": 110, "type": "TASK", "confidence": 0.7410693883895874}]}, {"text": "Our experiment results and discussion are in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present the data set and evaluation technique used for our experiments.", "labels": [], "entities": []}, {"text": "In the current literature, grammar correction tasks are often manually evaluated for each output correction, or evaluated by taking a set of proper sentences, artificially introducing some error, and seeing how well the algorithm fixes the error.", "labels": [], "entities": [{"text": "grammar correction tasks", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8097030321756998}]}, {"text": "Manual evaluation of automatic corrections maybe the best method forgetting a more detailed evaluation, but to do manual evaluation for every test output requires a large amount of human resources, in terms of both time and effort.", "labels": [], "entities": [{"text": "Manual evaluation of automatic corrections", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5559621632099152}]}, {"text": "In the case where artificial lesioning is introduced, the lesions may not always reflect the actual errors found inhuman data, and it is difficult to replicate the actual tendency of humans to make a variety of different mistakes in a single sentence.", "labels": [], "entities": []}, {"text": "Thus, this method of evaluation, which maybe suitable for evaluating the correction performance of specific grammatical errors, would not befit for evaluating our model's overall performance.", "labels": [], "entities": []}, {"text": "For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU () and METEOR (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7243964523077011}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9977566599845886}, {"text": "METEOR", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9090051054954529}]}, {"text": "Machine translation addresses the problem of changing a sentence in one language to a sentence of another.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.696488544344902}]}, {"text": "The task of correcting erroneous sentences can also bethought of as translating a sentence from a given language A, to another language B, where A is a broken language, and B is the correct language.", "labels": [], "entities": []}, {"text": "Under this context, we can apply machine translation evaluation techniques to evaluate the performance of our system.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8535462617874146}]}, {"text": "Our model's sentence correc-939 tions can bethought of as the output translation to be evaluated.", "labels": [], "entities": []}, {"text": "In order to use BLEU and METEOR, we need to have reference translations on which to score our output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9985242486000061}, {"text": "METEOR", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9558648467063904}]}, {"text": "As we have already explained in section 5.1, we have a collection of erroneous sentences, but no corrections.", "labels": [], "entities": []}, {"text": "To obtain manually corrected sentences for evaluation, the test and evaluation set sentences and were put on Amazon Mechanical Turk as a correction task.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.9751173059145609}]}, {"text": "Workers residing in the US were asked to manually correct the sentences in the two sets.", "labels": [], "entities": []}, {"text": "Workers had a choice of selecting 'Impossible to understand', 'Correct sentence', or 'Incorrect sentence', and were asked to correct the sentences so no spelling errors, grammatical errors, or punctuation errors were present.", "labels": [], "entities": []}, {"text": "Each sentence was given to 8 workers, giving us a set of 8 or fewer corrected sentences for each erroneous sentence.", "labels": [], "entities": []}, {"text": "We asked workers not to completely rewrite the sentences, but to maintain the original structure as much as possible.", "labels": [], "entities": []}, {"text": "Each hit was comprised of 6 sentences, and the reward for each hit was 10 cents.", "labels": [], "entities": []}, {"text": "To ensure the quality of our manually corrected sentences, a native English speaker research assistant went over each of the 'corrected' sentences and marked them as corrector incorrect.", "labels": [], "entities": []}, {"text": "We then removed all the incorrect 'corrections'.", "labels": [], "entities": [{"text": "corrections", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.8958137631416321}]}, {"text": "Using our manually corrected reference sentences, we evaluate our model's correction performance using METEOR and BLEU.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9895321130752563}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9968734383583069}]}, {"text": "Since METEOR and BLEU are fully automated after we have our reference translations (manual corrections), we can run evaluation on our tests without any need for further manual input.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9200589656829834}, {"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9978567957878113}]}, {"text": "While these two evaluation methods were created for machine translation, they also have the potential of being used in the field of grammar correction evaluation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7611573934555054}, {"text": "grammar correction evaluation", "start_pos": 132, "end_pos": 161, "type": "TASK", "confidence": 0.8390357494354248}]}, {"text": "One difference between machine translation and our task is that finding the right lemma is in itself something to be rewarded in MT, but is not sufficient for our task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7787589728832245}, {"text": "MT", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.908804178237915}]}, {"text": "In this respect, evaluation of grammar correction should be more strict.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.6621439456939697}]}, {"text": "Thus, for METEOR, we used the 'exact' module for evaluation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6186365485191345}]}, {"text": "To validate our evaluation method, we ran a simple test by calculating the METEOR and BLEU scores for the observed sentences, and compared them with the scores for the manually corrected sentences, to test for an expected increase.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9949319958686829}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9758555889129639}]}, {"text": "The scores for each correction were evaluated using the set of METEOR BLEU Original ESL sentences 0.8327 0.7540 Manual corrections 0.9179 0.8786  corrected sentences minus the correction sentence being evaluated.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9915295839309692}, {"text": "BLEU Original ESL sentences 0.8327 0.7540 Manual corrections 0.9179 0.8786  corrected sentences", "start_pos": 70, "end_pos": 165, "type": "METRIC", "confidence": 0.8946872303883234}]}, {"text": "For example, let us say we have the observed sentence o, and correction sentences c 1 , c 2 , c 3 and c 4 from Mechanical Turk.", "labels": [], "entities": [{"text": "correction", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.947529137134552}, {"text": "Mechanical Turk", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.8486708104610443}]}, {"text": "We run METEOR and BLEU on both o and c 1 using c 2 , c 3 and c 4 as the reference set.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9608795046806335}, {"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.998426079750061}]}, {"text": "We repeat the process for o and c 2 , using c 1 , c 3 and c 4 as the reference, and soon, until we have run METEOR and BLEU on all 4 correction sentences.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9880883097648621}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9969314336776733}]}, {"text": "With a set of 100 manually labeled sentences, the average METEOR score for the ESL sentences was 0.8327, whereas the corrected sentences had an average score of 0.9179.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9836756885051727}]}, {"text": "For BLEU, the average scores were 0.7540 and 0.8786, respectively, as shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9844706058502197}]}, {"text": "Thus, we have confirmed that the corrected sentences score higher than the ESL sentence.", "labels": [], "entities": [{"text": "ESL sentence", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.7524866163730621}]}, {"text": "It is also notable that finding corrections for the sentences is a much easier task than finding various correct translations, since the task of editing is much easier and can be done by a much larger set of qualified people.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU and METEOR scores for ESL sentences  vs manual corrections on 100 randomly chosen sentences", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993423819541931}, {"text": "METEOR", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9944830536842346}]}, {"text": " Table 2: Aspell vs Spelling noise model", "labels": [], "entities": []}, {"text": " Table 3: Average evaluation scores for various noise models run on 1017 sentences, along with counts of sentences  with increased (\u2191) and decreased (\u2193) scores. All improvements are significant by the binomial test at p < 0.001", "labels": [], "entities": []}]}