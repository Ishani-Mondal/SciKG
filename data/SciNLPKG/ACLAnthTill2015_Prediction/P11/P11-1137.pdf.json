{"title": [{"text": "Discovering Sociolinguistic Associations with Structured Sparsity", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data.", "labels": [], "entities": []}, {"text": "Using aggregate demographic statistics about the authors' geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies.", "labels": [], "entities": []}, {"text": "By imposing a composite 1,\u221e regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero.", "labels": [], "entities": []}, {"text": "We perform two regression studies.", "labels": [], "entities": []}, {"text": "First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics.", "labels": [], "entities": []}, {"text": "Next, we conjoin demographic attributes into features, which we use to predict term frequencies.", "labels": [], "entities": []}, {"text": "The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.", "labels": [], "entities": []}], "introductionContent": [{"text": "How is language influenced by the speaker's sociocultural identity?", "labels": [], "entities": []}, {"text": "Quantitative sociolinguistics usually addresses this question through carefully crafted studies that correlate individual demographic attributes and linguistic variables-for example, the interaction between income and the \"dropped r\" feature of the New York accent.", "labels": [], "entities": []}, {"text": "But such studies require the knowledge to select the \"dropped r\" and the speaker's income, from thousands of other possibilities.", "labels": [], "entities": []}, {"text": "In this paper, we present a method to acquire such patterns from raw data.", "labels": [], "entities": []}, {"text": "Using multi-output regression with structured sparsity, our method identifies a small subset of lexical items that are most influenced by demographics, and discovers conjunctions of demographic attributes that are especially salient for lexical variation.", "labels": [], "entities": []}, {"text": "Sociolinguistic associations are difficult to model, because the space of potentially relevant interactions is large and complex.", "labels": [], "entities": []}, {"text": "On the linguistic side there are thousands of possible variables, even if we limit ourselves to unigram lexical features.", "labels": [], "entities": []}, {"text": "On the demographic side, the interaction between demographic attributes is often non-linear: for example, gender may negate or amplify class-based language differences ().", "labels": [], "entities": []}, {"text": "Thus, additive models which assume that each demographic attribute makes a linear contribution are inadequate.", "labels": [], "entities": []}, {"text": "In this paper, we explore the large space of potential sociolinguistic associations using structured sparsity.", "labels": [], "entities": []}, {"text": "We treat the relationship between language and demographics as a set of multi-input, multioutput regression problems.", "labels": [], "entities": []}, {"text": "The regression coefficients are arranged in a matrix, with rows indicating predictors and columns indicating outputs.", "labels": [], "entities": []}, {"text": "We apply a composite regularizer that drives entire rows of the coefficient matrix to zero, yielding compact, interpretable models that reuse features across different outputs.", "labels": [], "entities": []}, {"text": "If we treat the lexical frequencies as inputs and the author's demographics as outputs, the induced sparsity pattern reveals the set of lexical items that is most closely tied to demographics.", "labels": [], "entities": []}, {"text": "If we treat the demographic attributes as inputs and build a model to predict the text, we can incrementally construct a conjunctive feature space of demographic attributes, capturing key non-linear interactions.", "labels": [], "entities": []}, {"text": "1365 The primary purpose of this research is exploratory data analysis to identify both the most linguistic-salient demographic features, and the most demographically-salient words.", "labels": [], "entities": []}, {"text": "However, this model also enables predictions about demographic features by analyzing raw text, potentially supporting applications in targeted information extraction or advertising.", "labels": [], "entities": [{"text": "targeted information extraction", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.7848009467124939}]}, {"text": "On the task of predicting demographics from text, we find that our sparse model yields performance that is statistically indistinguishable from the full vocabulary, even with a reduction in the model complexity an order of magnitude.", "labels": [], "entities": [{"text": "predicting demographics from text", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.8644165098667145}]}, {"text": "On the task of predicting text from author demographics, we find that our incrementally constructed feature set obtains significantly better perplexity than a linear model of demographic attributes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The ability of the induced demographic features to predict text is evaluated using a traditional perplexity metric.", "labels": [], "entities": []}, {"text": "The same test and training split is used from the vocabulary experiments.", "labels": [], "entities": []}, {"text": "We construct a language model from the induced demographic features by training a multi-output ridge regression, which gives a matrix\u02c6Bmatrix\u02c6 matrix\u02c6B that maps from demographic features to term frequencies across the entire vocabulary.", "labels": [], "entities": []}, {"text": "For each document in the test set, the \"raw\" predicted language model is\u02c6yis\u02c6 --   to unseen words is determined through nested crossvalidation.", "labels": [], "entities": []}, {"text": "We compare against a baseline language model obtained from the training set, again using nested cross-validation to set the probability of unseen terms.", "labels": [], "entities": []}, {"text": "The language models induced from demographic data yield small but statistically significant improvements over the baseline (Wilcoxon signed-rank test, p < .001).", "labels": [], "entities": []}, {"text": "Moreover, the model based on conjunctive features significantly outperforms the model constructed from raw attributes (p < .001).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The demographic attributes used in this research.", "labels": [], "entities": []}, {"text": " Table 2: Correlations between predicted and observed demographic attributes, averaged across cross validation folds.", "labels": [], "entities": []}, {"text": " Table 4: A glossary of non-standard terms from Ta- ble 3. Definitions are obtained by manually inspecting  the context in which the terms appear, and by consulting  www.urbandictionary.com.", "labels": [], "entities": [{"text": "Ta- ble 3", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.7759599983692169}]}, {"text": " Table 5: Word perplexity on test documents, using  language models estimated from induced demographic  features, raw demographic attributes, and a relative- frequency baseline. Lower scores are better.", "labels": [], "entities": []}]}