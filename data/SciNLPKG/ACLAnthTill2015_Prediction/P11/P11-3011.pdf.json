{"title": [{"text": "Optimistic Backtracking A Backtracking Overlay for Deterministic Incremental Parsing", "labels": [], "entities": [{"text": "Deterministic Incremental Parsing", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.5782157778739929}]}], "abstractContent": [{"text": "This paper describes a backtracking strategy for an incremental deterministic transition-based parser for HPSG.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.9177274703979492}]}, {"text": "The method could theoretically be implemented on any other transition-based parser with some adjustments.", "labels": [], "entities": []}, {"text": "In this paper, the algorithm is evaluated on CuteForce, an efficient deterministic shift-reduce HPSG parser.", "labels": [], "entities": [{"text": "CuteForce", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8861541152000427}]}, {"text": "The backtracking strategy may serve to improve existing parsers, or to assess if a deterministic parser would benefit from backtracking as a strategy to improve parsing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Incremental deterministic parsing has received increased awareness over the last decade.", "labels": [], "entities": [{"text": "Incremental deterministic parsing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8375304937362671}]}, {"text": "Processing linguistic data linearly is attractive both from a computational and a cognitive standpoint.", "labels": [], "entities": []}, {"text": "While there is a rich research tradition in statistical parsing, the predominant approach derives from chart parsing and is inherently non-deterministic.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8490466177463531}, {"text": "chart parsing", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.7784228324890137}]}, {"text": "A deterministic algorithm will incrementally expand a syntactic/semantic derivation as it reads the input sentence one word/token at the time.", "labels": [], "entities": []}, {"text": "There area number of attractive features to this approach.", "labels": [], "entities": []}, {"text": "The time-complexity will be linear when the algorithm is deterministic, i.e. it does not allow for later changes to the partial derivation, only extensions to it.", "labels": [], "entities": []}, {"text": "For a number of applications, e.g. speech recognition, the ability to process input on the fly per word, and not per sentence, can also be vital.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8114625811576843}]}, {"text": "However, there are inherent challenges to an incremental parsing algorithm.", "labels": [], "entities": []}, {"text": "Garden paths are the canonical example of sentences that are typically misinterpret due to an early incorrect grammatical assumption.", "labels": [], "entities": []}, {"text": "(1) The horse raced past the barn fell.", "labels": [], "entities": []}, {"text": "The ability to reevaluate an earlier grammatical assumption is disallowed by a deterministic parser.", "labels": [], "entities": []}, {"text": "Optimistic Backtracking is an method designed to locate the incorrect parser decision in an earlier stage if the parser reaches an illegal state, i.e. a state in which a valid parse derivation cannot be retrieved.", "labels": [], "entities": []}, {"text": "The Optimistic Backtracking method will try to locate the first incorrect parsing decision made by the parser, and replace this decision with the correct transition, and resume parsing from this state.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper we trained CuteForce with data from Redwoods Treebank, augmented with derivations from WikiWoods.", "labels": [], "entities": [{"text": "CuteForce", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.940313458442688}, {"text": "Redwoods Treebank", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.9382157325744629}]}, {"text": "The test set contains a random sample of 1794 sentences from the Redwoods Treebank (which was excluded from the training data), with an average length of 14 tokens.", "labels": [], "entities": [{"text": "Redwoods Treebank", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9633117914199829}]}, {"text": "Training data for the backtracker is extracted by parsing derivations from WikiWoods deterministically, and record transition candidates each time parsing fails, labeling the correct backtracking candidate, backtrack to this point, and resume parsing from this state.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results -CFG mode", "labels": [], "entities": [{"text": "CFG", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.6510858535766602}]}, {"text": " Table 2: Results -HPSG unification mode", "labels": [], "entities": []}]}