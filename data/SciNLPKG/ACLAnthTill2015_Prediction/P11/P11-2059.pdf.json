{"title": [{"text": "Language Use: What can it Tell us?", "labels": [], "entities": []}], "abstractContent": [{"text": "For 20 years, information extraction has fo-cused on facts expressed in text.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.8917792439460754}]}, {"text": "In contrast, this paper is a snapshot of research in progress on inferring properties and relationships among participants in dialogs, even though these properties/relationships need not be expressed as facts.", "labels": [], "entities": []}, {"text": "For instance, can a machine detect that someone is attempting to persuade another to action or to change beliefs or is asserting their credibility?", "labels": [], "entities": []}, {"text": "We report results on both English and Arabic discussion forums.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extracting explicitly stated information has been tested in MUC and ACE 2 evaluations.", "labels": [], "entities": [{"text": "Extracting explicitly stated information", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8933964669704437}, {"text": "MUC", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.46868765354156494}]}, {"text": "For example, for the text Mushaima'a, head of the opposition Haq movement, an ACE system extracts the relation LeaderOf(Mushaima'a, HaqMovement).", "labels": [], "entities": []}, {"text": "In TREC QA 3 systems answered questions, e.g. 'When was Mozart born?', for which the answer is contained in one or a few extracted text phrases.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.5591310560703278}]}, {"text": "Sentiment analysis uses implicit meaning of text, but has focused primarily on text known to be rich in opinions (product reviews, editorials) and delves into only one aspect of implicit meaning.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9386480450630188}]}, {"text": "Our long-term goal is to predict social roles in informal group discussion from language uses (LU), even if those roles are not explicitly stated; for example, using the communication during a meeting, identify the leader of a group.", "labels": [], "entities": []}, {"text": "This paper provides a snapshot of preliminary, ongoing research in predicting two classes of language use: Establish-Credibility and Attempt-To-Persuade.", "labels": [], "entities": []}, {"text": "Technical challenges include dealing with the facts that those LUs are rare and subjective and that human judgments have low agreement.", "labels": [], "entities": []}, {"text": "Our hybrid statistical & rule-based approach detects those two LUs in English and Arabic.", "labels": [], "entities": []}, {"text": "Our results are that (1) annotation at the message (turn) level provides training data useful for predicting rare phenomena at the discussion level while reducing the requirement for turn-level predictions to be accurate; (2)weighing subjective judgments overcomes the need for high annotator consistency.", "labels": [], "entities": []}, {"text": "Because the phenomena are rare, always predicting the absence of a LU is a very high baseline.", "labels": [], "entities": []}, {"text": "For English, the system beats those baselines.", "labels": [], "entities": []}, {"text": "For Arabic, more work is required, since only 10-20% of the amount of training data exists so far.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task is to predict for every participant in a given thread, whether the participant exhibits Attempt-to-Persuade and/or Establish-Credibility.", "labels": [], "entities": [{"text": "Attempt-to-Persuade", "start_pos": 97, "end_pos": 116, "type": "METRIC", "confidence": 0.9890496730804443}]}, {"text": "If there is insufficient evidence of an LU fora participant, then the LU value for that poster is negative.", "labels": [], "entities": []}, {"text": "The external evaluation measured LU predictions.", "labels": [], "entities": [{"text": "LU predictions", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.8379645943641663}]}, {"text": "Internally we measured predictions of message-level evidence as well.", "labels": [], "entities": []}, {"text": "For English, 139 threads from Google Groups and LiveJournal have been annotated for Attempt-to-Persuade, and 103 threads for Attempt-to-Establish-Credibility.", "labels": [], "entities": []}, {"text": "For Arabic, threads were collected from al-handasa.net.", "labels": [], "entities": []}, {"text": "31 threads were annotated for both tasks.", "labels": [], "entities": []}, {"text": "Counts of annotated messages appear in.", "labels": [], "entities": []}, {"text": "Due to low annotator agreement, attempting to resolve annotation disagreement by the standard adjudication process was too timeconsuming.", "labels": [], "entities": []}, {"text": "Instead, the evaluation scheme, similar to the pyramid scheme used for summarization evaluation, assigns scores to each example based on its level of agreement among the annotators.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.9550769627094269}]}, {"text": "Specifically, each example is assigned positive and negative scores, p = n + /N and n = n -/N, where n + is the number of annotators that annotate the example as positive, and n -for the negative.", "labels": [], "entities": []}, {"text": "N is the total number of annotators.", "labels": [], "entities": []}, {"text": "A system that outputs positive on the example results in p correct and n incorrect.", "labels": [], "entities": []}, {"text": "The system gets p incorrect and n correct for predicting negative.", "labels": [], "entities": []}, {"text": "Partial accuracy and Fmeasure can then be computed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9697543382644653}, {"text": "Fmeasure", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9992884397506714}]}, {"text": "Formally, let X = {x i } be a set of examples.", "labels": [], "entities": []}, {"text": "Each example xi is associated with positive and negative scores, pi and n i . Let r i = 1 if the system outputs positive for example xi and 0 for negative.", "labels": [], "entities": []}, {"text": "The partial accuracy, recall, precision, and Fmeasure can be computed by: The maximum pA and pF maybe less than 100 when there is disagreement between annotators.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9798139929771423}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995331764221191}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999627947807312}, {"text": "Fmeasure", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9997730851173401}]}, {"text": "To achieve accuracy and F scores on a scale of 100, pA and pF are normalized using the maximum achievable scores with respect to the data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9996503591537476}, {"text": "F scores", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9852540194988251}]}, {"text": "npA = 100\u00d7pA/max(pA) npF = 100\u00d7pF/max(pF)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 includes annotator consistency  at both the evidence (message) and LU level.", "labels": [], "entities": [{"text": "consistency", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.7177247405052185}]}, {"text": " Table 43: Performance on Message Level Evidence  Persuade  Establish Credibility  npA  npF  npA  npF  En  Ar  En  Ar  En  Ar  En  Ar", "labels": [], "entities": [{"text": "Message Level Evidence  Persuade  Establish Credibility  npA  npF  npA  npF  En  Ar  En  Ar  En  Ar  En  Ar", "start_pos": 26, "end_pos": 133, "type": "TASK", "confidence": 0.8149810880422592}]}, {"text": " Table 54: Cross Validation Performance on Poster LUs", "labels": [], "entities": [{"text": "Cross Validation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7881970405578613}]}, {"text": " Table 65: External, Held-Out Results on Poster LUs", "labels": [], "entities": []}]}