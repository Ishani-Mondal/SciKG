{"title": [{"text": "Global Learning of Typed Entailment Rules", "labels": [], "entities": [{"text": "Global Learning of Typed Entailment", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6683978497982025}]}], "abstractContent": [{"text": "Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference.", "labels": [], "entities": []}, {"text": "In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates.", "labels": [], "entities": []}, {"text": "We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs.", "labels": [], "entities": []}, {"text": "We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010).", "labels": [], "entities": []}, {"text": "Our results show that using global transitiv-ity information substantially improves performance over this resource and several base-lines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generic approaches for applied semantic inference from text gained growing attention in recent years, particularly under the Textual Entailment (TE) framework . TE is a generic paradigm for semantic inference, where the objective is to recognize whether a target meaning can be inferred from a given text.", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.7358585894107819}]}, {"text": "A crucial component of inference systems is extensive resources of entailment rules, also known as inference rules, i.e., rules that specify a directional inference relation between fragments of text.", "labels": [], "entities": []}, {"text": "One important type of rule is rules that specify entailment relations between predicates and their arguments.", "labels": [], "entities": []}, {"text": "For example, the rule 'X annex Y \u2192 X control Y' helps recognize that the text 'Japan annexed Okinawa' answers the question 'Which country controls Okinawa?'.", "labels": [], "entities": []}, {"text": "Thus, acquisition of such knowledge received considerable attention in the last decade.", "labels": [], "entities": []}, {"text": "Most past work took a \"local learning\" approach, learning each entailment rule independently of others.", "labels": [], "entities": []}, {"text": "It is clear though, that there are global interactions between predicates.", "labels": [], "entities": []}, {"text": "Notably, entailment is a transitive relation and so the rules A \u2192 B and B \u2192 C imply A \u2192 C.", "labels": [], "entities": []}, {"text": "Recently, proposed a global graph optimization procedure that uses Integer Linear Programming (ILP) to find the best set of entailment rules under a transitivity constraint.", "labels": [], "entities": [{"text": "global graph optimization", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.627305656671524}]}, {"text": "Imposing this constraint raised two challenges.", "labels": [], "entities": []}, {"text": "The first of ambiguity: transitivity does not always hold when predicates are ambiguous, e.g., X buy Y \u2192 X acquire Y and X acquire Y \u2192 X learn Y, but X buy Y X learn Y since these two rules correspond to two different senses of acquire.", "labels": [], "entities": []}, {"text": "The second challenge is scalability: ILP solvers do not scale well since ILP is an NP-complete problem.", "labels": [], "entities": [{"text": "ILP solvers", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8589435815811157}]}, {"text": "Berant et al. circumvented these issues by learning rules where one of the predicate's arguments is instantiated (e.g., 'X reduce nausea \u2192 X affect nausea'), which is useful for learning small graphs on-the-fly, given a target concept such as nausea.", "labels": [], "entities": []}, {"text": "While rules maybe effectively learned when needed, their scope is narrow and they are not useful as a generic knowledge resource.", "labels": [], "entities": []}, {"text": "This paper aims to take global rule learning one step further.", "labels": [], "entities": [{"text": "global rule learning", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.769466241200765}]}, {"text": "To this end, we adopt the representation suggested by, who learned inference rules between typed predicates, i.e., predicates where the argument types (e.g., city or drug) are specified.", "labels": [], "entities": []}, {"text": "Schoenmackers et al. uti-lized typed predicates since they were dealing with noisy and ambiguous web text.", "labels": [], "entities": []}, {"text": "Typing predicates helps disambiguation and filtering of noise, while still maintaining rules of wide-applicability.", "labels": [], "entities": [{"text": "Typing predicates", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.880075067281723}]}, {"text": "Their method employs a local learning approach, while the number of predicates in their data is too large to be handled directly by an ILP solver.", "labels": [], "entities": []}, {"text": "In this paper we suggest applying global optimization learning to open domain typed entailment rules.", "labels": [], "entities": [{"text": "global optimization learning", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.7155285278956095}]}, {"text": "To that end, we show how to construct a structure termed typed entailment graph, where the nodes are typed predicates and the edges represent entailment rules.", "labels": [], "entities": []}, {"text": "We suggest scaling techniques that allow to optimally learn such graphs over a large set of typed predicates by first decomposing nodes into components and then applying incremental ILP ().", "labels": [], "entities": []}, {"text": "Using these techniques, the obtained algorithm is guaranteed to return an optimal solution.", "labels": [], "entities": []}, {"text": "We ran our algorithm over the data set of Schoenmackers et al. and release a resource of 30,000 rules 1 that achieves substantially higher recall without harming precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9990525841712952}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.997946560382843}]}, {"text": "To the best of our knowledge, this is the first resource of that scale to use global optimization for learning predicative entailment rules.", "labels": [], "entities": [{"text": "learning predicative entailment rules", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.7297378256917}]}, {"text": "Our evaluation shows that global transitivity improves the F 1 score of rule learning by 27% over several baselines and that our scaling techniques allow dealing with larger graphs, resulting in improved coverage.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.979221522808075}, {"text": "rule learning", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.825769692659378}]}], "datasetContent": [{"text": "In this section we empirically answer the following questions: (1) Does transitivity improve rule learning over typed predicates?", "labels": [], "entities": []}, {"text": "(Section 5.1) (2) Do Decomposed-ILP and Incremental-ILP improve scalability?", "labels": [], "entities": []}, {"text": "(Section 5.2)  A data set of 1 million TextRunner tuples (, mapped to 10,672 distinct typed predicates over 156 types was provided by.", "labels": [], "entities": []}, {"text": "Readers are referred to their paper for details on mapping of tuples to typed predicates.", "labels": [], "entities": []}, {"text": "Since entailment only occurs between predicates that share the same types, we decomposed predicates by their types (e.g., all predicates with the types place and disease) into 2,303 typed entailment graphs.", "labels": [], "entities": []}, {"text": "The largest graph contains 118 nodes and the total number of potential rules is 263,756.", "labels": [], "entities": []}, {"text": "We generated a training set by applying the procedure described in Section 4.1, yielding 2,644 examples.", "labels": [], "entities": []}, {"text": "We used SVMperf) to train a Gaussian kernel classifier and computed P uv by projecting the classifier output score, S uv , with the sigmoid function: P uv = 1 1+exp(\u2212Suv) . We tuned two SVM parameters using 5-fold cross validation and a development set of two typed entailment graphs.", "labels": [], "entities": []}, {"text": "Next, we used our algorithm to learn rules.", "labels": [], "entities": []}, {"text": "As mentioned in Section 4.2, we integrate background knowledge using the sets A yes and A no that contain predicate pairs for which we know whether entailment holds.", "labels": [], "entities": []}, {"text": "A yes was constructed with syntactic rules: We normalized each predicate by omitting the first word if it is a modal and turning passives to actives.", "labels": [], "entities": []}, {"text": "If two normalized predicates are equal they are synonymous and inserted into A yes . A no was constructed from 3 sources (1) Predicates differing by a single pair of words that are WordNet antonyms Predicates differing by a single word of negation Predicates p(t 1 , t 2 ) and p(t 2 , t 1 ) where p is a transitive verb (e.g., beat) in VerbNet ().", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 336, "end_pos": 343, "type": "DATASET", "confidence": 0.942074179649353}]}, {"text": "We compared our algorithm (termed ILP scale ) to the following baselines.", "labels": [], "entities": []}, {"text": "First, to 10,000 rules released by, where the LHS contains a single predicate (Schoenmackers et al. released 30,000 rules but 20,000 of those have more than one predicate on the LHS, see Section 2), as we learn rules over the same data set.", "labels": [], "entities": []}, {"text": "Second, to distributional similarity algorithms: (a) SR: the score used by  Last, we added to all baselines background knowledge with A yes and A no (adding the subscript X k to their name).", "labels": [], "entities": [{"text": "SR", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9985848665237427}]}, {"text": "To evaluate performance we manually annotated all edges in 10 typed entailment graphs -7 twotypes entailment graphs containing 14, 22, 30, 53, 62, 86 and 118 nodes, and 3 single-type entailment graphs containing 7, 38 and 59 nodes.", "labels": [], "entities": []}, {"text": "This annotation yielded 3,427 edges and 35,585 non-edges, resulting in an empirical edge density of 9%.", "labels": [], "entities": []}, {"text": "We evaluate the algorithms by comparing the set of edges learned by the algorithms to the gold standard edges.", "labels": [], "entities": []}, {"text": "presents the precision-recall curve of the algorithms.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.9981479644775391}]}, {"text": "The curve is formed by varying a score threshold in the baselines and varying the edge prior in ILP scale 5 . For figure clarity, we omit DIRT and SR, since BInc outperforms them.", "labels": [], "entities": [{"text": "ILP scale 5", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.5974489251772562}, {"text": "DIRT", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9935558438301086}, {"text": "SR", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9900485277175903}, {"text": "BInc", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9955631494522095}]}, {"text": "shows micro-recall, precision and F 1 at the point of maximal F 1 , and the Area Under the Curve (AUC) for recall in the range of 0-0.45 for all algorithms, given background knowledge (knowledge consistently improves performance by a few points for all algorithms).", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9996434450149536}, {"text": "F 1", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9954171478748322}, {"text": "maximal F 1", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.7845810055732727}, {"text": "Area Under the Curve (AUC)", "start_pos": 76, "end_pos": 102, "type": "METRIC", "confidence": 0.8896693076406207}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9966331124305725}]}, {"text": "The table also shows results for the rules from Sherlock k .   Results show that using global transitivity information substantially improves performance.", "labels": [], "entities": []}, {"text": "ILP scale is better than all other algorithms by a large margin starting from recall .2, and improves AUC by 29% and the maximal F 1 by 27%.", "labels": [], "entities": [{"text": "ILP scale", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.5437930524349213}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.992508590221405}, {"text": "AUC", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9987496137619019}, {"text": "maximal F 1", "start_pos": 121, "end_pos": 132, "type": "METRIC", "confidence": 0.8303471207618713}]}, {"text": "Moreover, ILP scale doubles recall comparing to the rules from the Sherlock resource, while maintaining comparable precision.", "labels": [], "entities": [{"text": "ILP scale", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8233073651790619}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9994307160377502}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9985024929046631}]}, {"text": "We want to test whether using our scaling techniques, Decomposed-ILP and Incremental-ILP, allows us to reach the optimal solution in graphs that otherwise we could not solve, and consequently increase the number of learned rules and the overall recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 245, "end_pos": 251, "type": "METRIC", "confidence": 0.9983273148536682}]}, {"text": "To check that, we run ILP scale , with and without these scaling techniques (termed ILP \u2212 ).", "labels": [], "entities": []}, {"text": "We used the same data set as in Experiment 1 and learned edges for all 2,303 entailment graphs in the data set.", "labels": [], "entities": []}, {"text": "If the ILP solver was unable to hold the ILP in memory or took more than 2 hours  for some graph, we did not attempt to learn its edges.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.7160103619098663}]}, {"text": "We ran ILP scale and ILP \u2212 in three density modes to examine the behavior of the algorithms for different graph densities: (a) log \u03b7 = \u22120.6: the configuration that achieved the best recall/precision/F 1 of 43.", "labels": [], "entities": [{"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9990102052688599}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.7875645756721497}, {"text": "F 1", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.8984403908252716}]}, {"text": "In each run we counted the number of graphs that could not be learned and the number of rules learned by each algorithm.", "labels": [], "entities": []}, {"text": "In addition, we looked at the 20 largest graphs in our data (49-118 nodes) and measured the ratio r between the size of the largest component after applying Decomposed-ILP and the original size of the graph.", "labels": [], "entities": []}, {"text": "We then computed the average 1\u2212r over the 20 graphs to examine how graph size drops due to decomposition.", "labels": [], "entities": []}, {"text": "Column # unlearned and # rules describe the number of unlearned graphs and the number of learned rules.", "labels": [], "entities": []}, {"text": "Column shows relative increase in the number of rules learned and column Red.", "labels": [], "entities": []}, {"text": "shows the average 1 \u2212 r.", "labels": [], "entities": []}, {"text": "ILP scale increases the number of graphs that we are able to learn: in our best configuration (log \u03b7 = \u22120.6) only 3 graphs could not be handled comparing to 9 graphs when omitting our scaling techniques.", "labels": [], "entities": []}, {"text": "Since the unlearned graphs are among the largest in the data set, this adds 3,500 additional rules.", "labels": [], "entities": []}, {"text": "We compared the precision of rules learned only by ILP scale with that of the rules learned by both, by randomly sampling 100 rules from each and found precision to be comparable.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9987654685974121}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.998897910118103}]}, {"text": "Thus, the additional rules learned translate into a 13% increase in relative recall without harming precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9860922694206238}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9980893731117249}]}, {"text": "Also note that as density increases, the number of rules learned grows and the effectiveness of decomposition decreases.", "labels": [], "entities": []}, {"text": "This shows how Decomposed-ILP is especially useful for sparse graphs.", "labels": [], "entities": []}, {"text": "We re-lease the 29,732 rules learned by the configuration log \u03b7 = \u22120.6 as a resource.", "labels": [], "entities": []}, {"text": "To sum up, our scaling techniques allow us to learn rules from graphs that standard ILP cannot handle and thus considerably increase recall without harming precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9989196062088013}, {"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9970631003379822}]}], "tableCaptions": [{"text": " Table 2: micro-average F 1 and AUC for the algorithms.", "labels": [], "entities": [{"text": "micro-average F 1", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8344778815905253}, {"text": "AUC", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9938048124313354}]}, {"text": " Table 3: Impact of scaling techinques (ILP \u2212 /ILP scale ).", "labels": [], "entities": []}]}