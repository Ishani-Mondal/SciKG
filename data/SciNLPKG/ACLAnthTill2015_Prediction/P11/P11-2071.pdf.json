{"title": [{"text": "Domain Adaptation for Machine Translation by Mining Unseen Words", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6698553413152695}, {"text": "Machine Translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7654401659965515}]}], "abstractContent": [{"text": "We show that unseen words account fora large part of the translation error when moving to new domains.", "labels": [], "entities": []}, {"text": "Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms.", "labels": [], "entities": []}, {"text": "We show several approaches to integrating such translations into a phrase-based translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9136393666267395}]}], "introductionContent": [{"text": "Large amounts of data are currently available to train statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6456603209177653}]}, {"text": "Unfortunately, these training data are often qualitatively different from the target task of the translation system.", "labels": [], "entities": []}, {"text": "In this paper, we consider one specific aspect of domain divergence: the out-of-vocabulary problem.", "labels": [], "entities": [{"text": "domain divergence", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7141427099704742}]}, {"text": "By considering four different target domains (news, medical, movie subtitles, technical documentation) in two source languages (German, French), we: (1) Ascertain the degree to which domain divergence causes increases in unseen words, and the degree to which this degrades translation performance.", "labels": [], "entities": []}, {"text": "(For instance, if all unknown words are names, then copying them verbatim maybe sufficient.)", "labels": [], "entities": []}, {"text": "(2) Extend known methods for mining dictionaries from comparable corpora to the domain adaptation setting, by \"bootstrapping\" them based on known translations from the source domain.", "labels": [], "entities": []}, {"text": "Develop methods for integrating these mined dictionaries into a phrase-based translation system ( . As we shall see, for most target domains, out of vocabulary terms are the source of approximately half of the additional errors made.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.6826641708612442}]}, {"text": "The only exception is the news domain, which is sufficiently similar to parliament proceedings (Europarl) that there are essentially no new, frequent words in news.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9207188487052917}]}, {"text": "By mining a dictionary and naively incorporating it into a translation system, one can only do slightly better than baseline.", "labels": [], "entities": []}, {"text": "However, with a more clever integration, we can close about half of the gap between baseline (unadapted) performance and an oracle experiment.", "labels": [], "entities": []}, {"text": "In most cases this amounts to an improvement of about 1.5 Bleu points () and 1.5 Meteor points ().", "labels": [], "entities": [{"text": "Bleu", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9961917400360107}, {"text": "Meteor", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9303579926490784}]}, {"text": "The specific setting we consider is the one in which we have plentiful parallel (\"labeled\") data in a source domain (eg., parliament) and plentiful comparable (\"unlabeled\") data in a target domain (eg., medical).", "labels": [], "entities": []}, {"text": "We can use the unlabeled data in the target domain to build a good language model.", "labels": [], "entities": []}, {"text": "Finally, we assume access to a very small amount of parallel (\"labeled\") target data, but only enough to evaluate on, or run weight tuning.", "labels": [], "entities": []}, {"text": "All knowledge about unseen words must come from the comparable data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all of our experiments, we use two trigram language models.", "labels": [], "entities": []}, {"text": "The first is trained on the Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9483242928981781}]}, {"text": "The second is trained on the English side of the target domain corpus.", "labels": [], "entities": []}, {"text": "The two language models are traded-off against each other during weight tuning.", "labels": [], "entities": [{"text": "weight tuning", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.6980458050966263}]}, {"text": "In all cases we perform parameter tuning with MERT, and results are averaged over three runs with different random initializations.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7154026329517365}, {"text": "MERT", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.6681128144264221}]}], "tableCaptions": [{"text": " Table 1: For each domain, the percentage of target do- main word tokens that are unseen in the source domain,  together with the most frequent English words in the tar- get domains that do not appear in the source domain. (In  the actual data the subtitles words do not appear cen- sored.)", "labels": [], "entities": []}, {"text": " Table 1.  Here, for each domain, we show the percentage of  words (types) in the target domain that are unseen in  the Parliament data. As we can see, it is markedly  higher in Emea, Subs and PHP than in News.", "labels": [], "entities": [{"text": "Parliament data", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.9238216280937195}, {"text": "Emea", "start_pos": 178, "end_pos": 182, "type": "DATASET", "confidence": 0.9375418424606323}]}, {"text": " Table 2: Random unseen Emea words in German and  their mined translations.", "labels": [], "entities": []}, {"text": " Table 3: Baseline and oracle scores. The last two rows are the change between the baseline and the two types of  oracles, averaged over the two languages.", "labels": [], "entities": []}, {"text": " Table 4: Dictionary-mining system results. The italicized  number beneath each score is the improvement over the  BASELINE approach from Table 3.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9650853872299194}]}, {"text": " Table 4. As we can see, there is a mod- est improvement in Subtitles and PHP, a markedly", "labels": [], "entities": []}]}