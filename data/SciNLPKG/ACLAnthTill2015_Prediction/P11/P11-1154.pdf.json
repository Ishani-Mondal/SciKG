{"title": [], "abstractContent": [{"text": "We propose a method for automatically labelling topics learned via LDA topic models.", "labels": [], "entities": []}, {"text": "We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles.", "labels": [], "entities": []}, {"text": "We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model.", "labels": [], "entities": []}, {"text": "Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic modelling is an increasingly popular framework for simultaneously soft-clustering terms and documents into a fixed number of \"topics\", which take the form of a multinomial distribution over terms in the document collection (.", "labels": [], "entities": [{"text": "Topic modelling", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7719187140464783}]}, {"text": "It has been demonstrated to be highly effective in a wide range of tasks, including multidocument summarisation), word sense discrimination, sentiment analysis, information retrieval () and image labelling.", "labels": [], "entities": [{"text": "multidocument summarisation", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.6779780685901642}, {"text": "word sense discrimination", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.7629693547884623}, {"text": "sentiment analysis", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.9749152660369873}, {"text": "information retrieval", "start_pos": 161, "end_pos": 182, "type": "TASK", "confidence": 0.7959400713443756}, {"text": "image labelling", "start_pos": 190, "end_pos": 205, "type": "TASK", "confidence": 0.7781198620796204}]}, {"text": "One standard way of interpreting a topic is to use the marginal probabilities p(w i |t j ) associated with each term w i in a given topic t j to extract out the 10 terms with highest marginal probability.", "labels": [], "entities": [{"text": "interpreting a topic", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8877016305923462}]}, {"text": "This results in term lists such as: stock market investor fund trading investment firm exchange companies share Here and throughout the paper, we will represent a topic tj via its ranking of top-10 topic terms, based on p(wi|tj).", "labels": [], "entities": [{"text": "stock market investor fund trading investment firm exchange", "start_pos": 36, "end_pos": 95, "type": "TASK", "confidence": 0.691161934286356}]}, {"text": "which are clearly associated with the domain of stock market trading.", "labels": [], "entities": [{"text": "stock market trading", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.6517229576905569}]}, {"text": "The aim of this research is to automatically generate topic labels which explicitly identify the semantics of the topic, i.e. which take us from a list of terms requiring interpretation to a single label, such as STOCK MARKET TRADING in the above case.", "labels": [], "entities": [{"text": "STOCK MARKET TRADING", "start_pos": 213, "end_pos": 233, "type": "METRIC", "confidence": 0.549949179093043}]}, {"text": "The approach proposed in this paper is to first generate a topic label candidate set by: (1) sourcing topic label candidates from Wikipedia by querying with the top-N topic terms; (2) identifying the top-ranked document titles; and (3) further postprocessing the document titles to extract sub-strings.", "labels": [], "entities": [{"text": "sourcing topic label candidates from Wikipedia", "start_pos": 93, "end_pos": 139, "type": "TASK", "confidence": 0.8606197834014893}]}, {"text": "We translate each topic label into features extracted from Wikipedia, lexical association with the topic terms in Wikipedia documents, and also lexical features for the component terms.", "labels": [], "entities": []}, {"text": "This is used as the basis of a support vector regression model, which ranks each topic label candidate.", "labels": [], "entities": []}, {"text": "Our contributions in this work are: (1) the generation of a novel evaluation framework and dataset for topic label evaluation; (2) the proposal of a method for both generating and scoring topic label candidates; and (3) strong in-and cross-domain results across four independent document collections and associated topic models, demonstrating the ability of our method to automatically label topics with remarkable success.", "labels": [], "entities": [{"text": "topic label evaluation", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.8092862367630005}]}], "datasetContent": [{"text": "We conducted topic labelling experiments using document collections constructed from four distinct domains/genres, to test the domain/genre independence of our method: The BLOGS dataset contains blog posts that cover a diverse range of subjects, from product reviews to casual, conversational messages.", "labels": [], "entities": [{"text": "BLOGS dataset", "start_pos": 172, "end_pos": 185, "type": "DATASET", "confidence": 0.8236726820468903}]}, {"text": "The BOOKS topics, coming from public-domain out-of-copyright books (with publication dates spanning more than a century), relate to a wide range of topics including furniture, home decoration, religion and art, and have a more historic feel to them.", "labels": [], "entities": [{"text": "BOOKS", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8029799461364746}]}, {"text": "The NEWS topics reflect the types and range of subjects one might expect in news articles such as health, finance, entertainment, and politics.", "labels": [], "entities": []}, {"text": "The PUBMED topics frequently contain domain-specific terms and are sharply differentiated from the topics for the other corpora.", "labels": [], "entities": []}, {"text": "We are particularly interested in the performance of the method over PUBMED, as it is a highly specialised domain where we may expect lower coverage of appropriate topic labels within Wikipedia.", "labels": [], "entities": []}, {"text": "We took a standard approach to topic modelling each of the four document collections: we tokenised, lemmatised and stopped each document, and created a vocabulary of terms that occurred at least ten times.", "labels": [], "entities": []}, {"text": "From this processed data, we created a bag-of-words representation of each document, and learned topic models with T = 100 topics in each case.", "labels": [], "entities": [{"text": "T", "start_pos": 115, "end_pos": 116, "type": "METRIC", "confidence": 0.9726313352584839}]}, {"text": "To focus our experiments on topics that were relatively more coherent and interpretable, we first used the method of to calculate the average PMI-score for each topic, and filtered all topics that had an average PMI-score lower than 0.4.", "labels": [], "entities": []}, {"text": "We additionally filtered any topics where less than 5 of the top-10 topic terms are default nominal in Wikipedia.", "labels": [], "entities": []}, {"text": "The filtering criteria resulted in 45 topics for BLOGS, 38 topics for BOOKS, 60 topics for NEWS, and 85 topics for PUBMED.", "labels": [], "entities": [{"text": "BLOGS", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7519358992576599}, {"text": "BOOKS", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8268119096755981}, {"text": "NEWS", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.8859673142433167}, {"text": "PUBMED", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.8452800512313843}]}, {"text": "Manual inspection of the discarded topics indicated that they were predominantly hard-to-label junk topics or mixed topics, with limited utility for document/term clustering.", "labels": [], "entities": [{"text": "document/term clustering", "start_pos": 149, "end_pos": 173, "type": "TASK", "confidence": 0.5698629841208458}]}, {"text": "Applying our label candidate generation methodology to these 228 topics produced approximately 6000 labels -an average of 27 labels per topic.: A screenshot of the topic label evaluation task on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "topic label evaluation task", "start_pos": 164, "end_pos": 191, "type": "TASK", "confidence": 0.6909832879900932}, {"text": "Amazon Mechanical Turk", "start_pos": 195, "end_pos": 217, "type": "DATASET", "confidence": 0.9558068116505941}]}, {"text": "This screen constitutes a Human Intelligence Task (HIT); it contains a topic followed by 10 suggested topic labels, which are to be rated.", "labels": [], "entities": []}, {"text": "Note that been would be the stopword label in this example.", "labels": [], "entities": []}, {"text": "In this section we present our experimental results for the topic labelling task, based on both the unsupervised and supervised methods, and the methodology of, which we denote MSZ for the remainder of the paper.", "labels": [], "entities": [{"text": "topic labelling task", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8092760841051737}, {"text": "MSZ", "start_pos": 177, "end_pos": 180, "type": "DATASET", "confidence": 0.7604916095733643}]}, {"text": "We use two basic measures to evaluate the performance of our predictions.", "labels": [], "entities": []}, {"text": "Top-1 average rating is the average annotator rating given to the top-ranked system label, and has a maximum value of 3 (where annotators unanimously rated all top-ranked system labels with a 3).", "labels": [], "entities": []}, {"text": "This is intended to give a sense of the absolute utility of the top-ranked candidates.", "labels": [], "entities": []}, {"text": "The second measure is normalized discounted cumulative gain (nDCG:,), computed for the top-1 (nDCG-1), top-3 (nDCG-3) and top-5 ranked system labels (nDCG-5).", "labels": [], "entities": [{"text": "normalized discounted cumulative gain (nDCG", "start_pos": 22, "end_pos": 65, "type": "METRIC", "confidence": 0.7142111659049988}]}, {"text": "For a given ordered list of scores, this measure is based on the difference between the original order, and the order when the list is sorted by score.", "labels": [], "entities": []}, {"text": "That is, if items are ranked optimally in descending order of score at position N , nDCG-N is equal to 1.", "labels": [], "entities": []}, {"text": "nDCG is a normalised score, and indicates how close the candidate label ranking is to the optimal ranking within the set of annotated candidates, noting that an nDCG-N score of 1 tells us nothing about absolute values of the candidates.", "labels": [], "entities": []}, {"text": "This second evaluation measure is thus intended to reflect the relative quality of the ranking, and complements the top-1 average rating.", "labels": [], "entities": []}, {"text": "Note that conventional precision-and recall-based evaluation is not appropriate for our task, as each label candidate has a real-valued rating.", "labels": [], "entities": [{"text": "precision-and", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9983654618263245}, {"text": "recall-based", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.866136372089386}]}, {"text": "As a baseline for the task, we use the unsupervised label candidate ranking method based on Pearson's \u03c7 2 test, as it was overwhelmingly found to be the pick of the features for candidate ranking.", "labels": [], "entities": [{"text": "candidate ranking", "start_pos": 178, "end_pos": 195, "type": "TASK", "confidence": 0.7958058416843414}]}], "tableCaptions": [{"text": " Table 1: A sample of topics and topic labels, along with the average rating for each label candidate", "labels": [], "entities": []}, {"text": " Table 2: Supervised results for all domains", "labels": [], "entities": []}, {"text": " Table 3: Comparison of results for our proposed supervised ranking method (SVR) and that of MSZ", "labels": [], "entities": [{"text": "MSZ", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.5744457840919495}]}]}