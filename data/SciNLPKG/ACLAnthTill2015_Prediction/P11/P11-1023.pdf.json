{"title": [{"text": "MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility via semantic frames", "labels": [], "entities": [{"text": "MEANT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9666956663131714}]}], "abstractContent": [{"text": "We introduce a novel semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as HTER but at much lower labor cost.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9842982292175293}]}, {"text": "As machine translation systems improve in lexical choice and fluency , the shortcomings of widespread n-gram based, fluency-oriented MT evaluation metrics such as BLEU, which fail to properly evaluate adequacy, become more apparent.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7384725511074066}, {"text": "MT evaluation", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.8926674723625183}, {"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9929080009460449}]}, {"text": "But more accurate, non-automatic adequacy-oriented MT evaluation metrics like HTER are highly labor-intensive, which bottlenecks the evaluation cycle.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9053169190883636}]}, {"text": "We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER.", "labels": [], "entities": [{"text": "MT output", "start_pos": 90, "end_pos": 99, "type": "TASK", "confidence": 0.9176711142063141}, {"text": "HMEANT", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.8559392690658569}, {"text": "BLEU", "start_pos": 257, "end_pos": 261, "type": "METRIC", "confidence": 0.9959707856178284}]}, {"text": "We then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semi-automated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as HTER despite an even lower labor cost for the evaluation procedure.", "labels": [], "entities": []}, {"text": "The results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than HTER.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we show that evaluating machine translation by assessing the translation accuracy of each argument in the semantic role framework correlates with human judgment on translation adequacy as well as HTER, at a significantly lower labor cost.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7655574083328247}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9492080211639404}]}, {"text": "The correlation of this new metric, MEANT, with human judgment is far superior to BLEU and other automatic n-gram based evaluation metrics.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9968530535697937}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9975184202194214}]}, {"text": "We argue that BLEU) and other automatic n-gram based MT evaluation metrics do not adequately capture the similarity in meaning between the machine translation and the reference translation-which, ultimately, is essential for MT output to be useful.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9974307417869568}, {"text": "MT evaluation", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9348565936088562}, {"text": "MT output", "start_pos": 225, "end_pos": 234, "type": "TASK", "confidence": 0.9198927581310272}]}, {"text": "Ngram based metrics assume that \"good\" translations tend to share the same lexical choices as the reference translations.", "labels": [], "entities": []}, {"text": "While BLEU score performs well in capturing the translation fluency, and report cases where BLEU strongly disagree with human judgment on translation quality.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9578883349895477}, {"text": "translation fluency", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8851497173309326}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.980747640132904}]}, {"text": "The underlying reason is that lexical similarity does not adequately reflect the similarity in meaning.", "labels": [], "entities": []}, {"text": "As MT systems improve, the shortcomings of the n-gram based evaluation metrics are becoming more apparent.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9910401701927185}]}, {"text": "State-of-the-art MT systems are often able to output fluent translations that are nearly grammatical and contain roughly the correct words, but still fail to express meaning that is close to the input.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9815776944160461}]}, {"text": "At the same time, although) is more adequacy-oriented, it is only employed in very large scale MT system evaluation instead of day-to-day research activities.", "labels": [], "entities": [{"text": "MT system evaluation", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.9097325007120768}]}, {"text": "The underlying reason is that it requires rigorously trained human experts to make difficult combinatorial decisions on the minimal number of edits so as to make the MT output convey the same meaning as the reference translation-a highly labor-intensive, costly process that bottlenecks the evaluation cycle.", "labels": [], "entities": [{"text": "MT", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.9376810193061829}]}, {"text": "Instead, with MEANT, we adopt at the outset the principle that a good translation is one that is useful, in the sense that human readers may successfully understand at least the basic event structure-\"who did what to whom, when, where and why\")-representing the central meaning of the source utterances.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.419888973236084}]}, {"text": "It is true that limited tasks might exist for which inadequate translations are still useful.", "labels": [], "entities": []}, {"text": "But for meaningful tasks, generally speaking, fora translation to be useful, at least the basic event structure must be correctly understood.", "labels": [], "entities": []}, {"text": "Therefore, our objective is to evaluate translation utility: from a user's point of view, how well is the most essential semantic information being captured by machine translation systems?", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9656323194503784}]}, {"text": "In this paper, we detail the methodology that underlies MEANT, which extends and implements preliminary directions proposed in ( and.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 56, "end_pos": 61, "type": "TASK", "confidence": 0.5326452851295471}]}, {"text": "We present the results of evaluating translation utility by measuring the accuracy within a semantic role labeling (SRL) framework.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9629053473472595}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9992830157279968}, {"text": "semantic role labeling (SRL)", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.7912883659203848}]}, {"text": "We show empirically that our proposed SRL based evaluation metric, which uses untrained monolingual humans to annotate semantic frames in MT output, correlates with human adequacy judgments as well as HTER, and far better than BLEU and other commonly used metrics.", "labels": [], "entities": [{"text": "SRL", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9239208102226257}, {"text": "MT output", "start_pos": 138, "end_pos": 147, "type": "TASK", "confidence": 0.896938145160675}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9959425330162048}]}, {"text": "Finally, we show that replacing the human semantic role labelers with an automatic shallow semantic parser in our proposed metric yields an approximation that is about 80% as closely correlated with human judgment as HTER, at an even lower cost-and is still far better correlated than n-gram based evaluation metrics.", "labels": [], "entities": []}], "datasetContent": [{"text": "A good translation is one from which human readers may successfully understand at least the basic event structure-\"who did what to whom, when, where and why\")-which represents the most essential meaning of the source utterances.", "labels": [], "entities": []}, {"text": "MEANT measures this as follows.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.46861496567726135}]}, {"text": "First, semantic role labeling is performed (either manually or automatically) on both the reference translation and the machine translation.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.6953285932540894}]}, {"text": "The semantic frame structures thus obtained for the MT output are compared to those in the reference translations, frame by frame, argument by argument.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.971176266670227}]}, {"text": "The frame translation accuracy is a weighted sum of the number of correctly translated arguments.", "labels": [], "entities": [{"text": "frame translation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7273117452859879}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.844645619392395}]}, {"text": "Conceptually, MEANT is defined in terms of f-score, with respect to the precision/recall for sentence translation accuracy as calculated by averaging the translation accuracy for all frames in the MT output across the number of frames in the MT output/reference translations.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8926833868026733}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.999360978603363}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9893338084220886}, {"text": "sentence translation", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.7018609642982483}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.4989011585712433}]}, {"text": "We leverage work from Phase 2.5 of the DARPA GALE program in which both a subset of the Chinese source sentences, as well as their English reference, are being annotated with semantic role labels in Propbank style.", "labels": [], "entities": [{"text": "DARPA GALE", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.5108700692653656}]}, {"text": "The corpus also includes three participating stateof-the-art MT systems' output.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9248740077018738}]}, {"text": "For present purposes, we randomly drew 40 sentences from the newswire genre of the corpus to form a meta-evaluation corpus.", "labels": [], "entities": []}, {"text": "To maintain a controlled environment for experiments and consistent comparison, the evaluation corpus is fixed throughout this work.", "labels": [], "entities": []}, {"text": "The first experiment aims to provide a more concrete understanding of one of the key questions as to the upper bounds of the proposed evaluation metric: how well can human annotators perform in reconstructing the semantic frames in MT output?", "labels": [], "entities": [{"text": "MT output", "start_pos": 232, "end_pos": 241, "type": "TASK", "confidence": 0.8708629012107849}]}, {"text": "This is important since MT output is still not close to perfectly grammatical fora good syntactic parsing-applying automatic shallow semantic parsers, which are trained on grammatical input and valid syntactic parse trees, on MT output may significantly underestimate translation utility.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9820370078086853}, {"text": "syntactic parsing-applying automatic shallow semantic parsers", "start_pos": 88, "end_pos": 149, "type": "TASK", "confidence": 0.8391288816928864}]}, {"text": "We performed three variations of the experiments to assess the performance degradation from the automatic approximation of semantic frame reconstruction in each translation (reference translation and MT output): we applied automatic shallow semantic parsing on the MT output only; on the reference translation only; and on both reference translation and MT output.", "labels": [], "entities": [{"text": "semantic frame reconstruction", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.6623339454332987}]}, {"text": "For the semantic) which achieves roughly 87% semantic role labeling accuracy.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.5637164115905762}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9728937149047852}]}, {"text": "shows that the proposed SRL based evaluation metric correlates slightly worse than HTER with a much lower labor cost.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7739640474319458}]}, {"text": "The correlation with human judgment on adequacy of the fully automated SRL annotation version, i.e., applying ASSERT on both the reference translation and the MT output, of the SRL based evaluation metric is about 80% of that of HTER.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.8058448731899261}, {"text": "ASSERT", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9974943399429321}, {"text": "SRL based evaluation", "start_pos": 177, "end_pos": 197, "type": "TASK", "confidence": 0.7798877755800883}, {"text": "HTER", "start_pos": 229, "end_pos": 233, "type": "DATASET", "confidence": 0.8183612823486328}]}, {"text": "The results also show that the correlation with human judgment on adequacy of either one side of translation using automatic SRL is in the 85% to 95% range of that HTER.", "labels": [], "entities": [{"text": "SRL", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.8558858036994934}]}, {"text": "We now show that using monolingual annotators is essentially just as effective as using more expensive bilingual annotators.", "labels": [], "entities": []}, {"text": "We study the cost/benefit trade-off of using human annotators from different language backgrounds for the proposed evaluation metric, and compare whether providing the original source text helps.", "labels": [], "entities": []}, {"text": "Note that this experiment focuses on the SRL annotation step, rather than the judgments of role filler paraphrasing accuracy, because the latter is only a simple three-way decision between \"correct\", \"partial\", and \"incorrect\" that is far less sensitive to the annotators' language backgrounds.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8727268576622009}, {"text": "role filler paraphrasing", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7958442568778992}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.7668441534042358}]}, {"text": "MT output is typically poor.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.965686559677124}]}, {"text": "Therefore, readers of MT output often guess the original meaning in the source input using their own language background knowledge.", "labels": [], "entities": [{"text": "MT output", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9010108113288879}]}, {"text": "Readers' language background thus affects their understanding of the translation, which could affect the accuracy of capturing the key semantic roles in the translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9988583326339722}]}, {"text": "Ref MT bilinguals working on output only 69% 65% monolinguals working on output only 88% 70% bilinguals working on input-output 70% 69% word span in the annotated role fillers with a tolerance of \u00b11 word in mismatch.", "labels": [], "entities": [{"text": "MT bilinguals", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8421446979045868}]}, {"text": "The inter-annotator agreement rate (IAA) on the role identification task is calculated as follows.", "labels": [], "entities": [{"text": "inter-annotator agreement rate (IAA)", "start_pos": 4, "end_pos": 40, "type": "METRIC", "confidence": 0.834436963001887}, {"text": "role identification task", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8819272915522257}]}, {"text": "A 1 and A 2 denote the number of annotated predicates and arguments by annotator 1 and annotator 2 respectively.", "labels": [], "entities": []}, {"text": "M span denotes the number of annotated predicates and arguments with matching word span between annotators.", "labels": [], "entities": [{"text": "M span", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.912151426076889}]}, {"text": "Role classification The agreement of classified roles is counted over the matching of the semantic role labels within two aligned word spans.", "labels": [], "entities": [{"text": "Role classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8209052383899689}]}, {"text": "The IAA on the role classification task is calculated as follows.", "labels": [], "entities": [{"text": "IAA", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9933483004570007}, {"text": "role classification task", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.8890273769696554}]}, {"text": "M label denotes the number of annotated predicates and arguments with matching role label between annotators.", "labels": [], "entities": [{"text": "M label", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8239828646183014}]}, {"text": "In the previous experiment, we showed that the proposed evaluation metric driven by human semantic role annotators performed as well as HTER.", "labels": [], "entities": [{"text": "HTER", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.5151358842849731}]}, {"text": "It is now worth asking a deeper question: can we further reduce the labor cost of MEANT by using automatic shallow semantic parsing instead of humans for semantic role labeling?", "labels": [], "entities": [{"text": "MEANT", "start_pos": 82, "end_pos": 87, "type": "TASK", "confidence": 0.6605143547058105}, {"text": "semantic parsing", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.7440706789493561}, {"text": "semantic role labeling", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.6391608913739523}]}, {"text": "Note that this experiment focuses on understanding the cost/benefit trade-off for the semantic frame reconstruction step.", "labels": [], "entities": [{"text": "semantic frame reconstruction", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.661496510108312}]}, {"text": "For SRL annotation, we replace humans with automatic shallow semantic parsing.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9762430489063263}, {"text": "automatic shallow semantic parsing", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.7369627207517624}]}, {"text": "We decouple this from the ternary judgments of role filler accuracy, which are still made by humans.", "labels": [], "entities": [{"text": "role filler accuracy", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7184411883354187}]}, {"text": "However, we believe the evaluation of role filler accuracy will also be automatable.", "labels": [], "entities": [{"text": "role filler", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.8605953752994537}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9202241897583008}]}], "tableCaptions": [{"text": " Table 3: Sentence-level correlation with human adequacy judg- ments, across the evaluation metrics.", "labels": [], "entities": [{"text": "judg- ments", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.8962700366973877}]}, {"text": " Table 4: Analysis of stability for HMEANT's weight settings,  with RHMEANT rank and Kendall's \u03c4 correlation scores (see text).", "labels": [], "entities": [{"text": "RHMEANT rank", "start_pos": 68, "end_pos": 80, "type": "METRIC", "confidence": 0.986052930355072}, {"text": "Kendall's \u03c4 correlation scores", "start_pos": 85, "end_pos": 115, "type": "METRIC", "confidence": 0.8393733620643615}]}, {"text": " Table 8: Sentence-level correlation with human adequacy judg- ments. *The weights for individual roles in the metric are tuned  by optimizing the correlation.", "labels": [], "entities": []}]}