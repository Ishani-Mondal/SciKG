{"title": [{"text": "Discovery of Topically Coherent Sentences for Extractive Summarization", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6612561345100403}]}], "abstractContent": [{"text": "Extractive methods for multi-document sum-marization are mainly governed by information overlap, coherence, and content constraints.", "labels": [], "entities": []}, {"text": "We present an unsupervised proba-bilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries.", "labels": [], "entities": []}, {"text": "Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems.", "labels": [], "entities": []}, {"text": "Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9748322367668152}, {"text": "DUC-07 test set", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9747144977251688}]}], "introductionContent": [{"text": "A query-focused multi-document summarization model produces a short-summary text of a set of documents, which are retrieved based on a user's query.", "labels": [], "entities": []}, {"text": "An ideal generated summary text should contain the shared relevant content among set of documents only once, plus other unique information from individual documents that are directly related to the user's query addressing different levels of detail.", "labels": [], "entities": []}, {"text": "Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9343185126781464}]}, {"text": "In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences.", "labels": [], "entities": []}, {"text": "Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc,).", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.752652108669281}]}, {"text": "Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (;).", "labels": [], "entities": [{"text": "summarization", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.9669238924980164}]}, {"text": "In particular build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts.", "labels": [], "entities": []}, {"text": "Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building.", "labels": [], "entities": []}, {"text": "A model than can focus on both issues is deemed to be more beneficial fora summarization task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9877147674560547}]}, {"text": "Topical coherence in text involves identifying key concepts, the relationships between these concepts, and linking these relationships into a hierarchy.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel, fully generative Bayesian model of document corpus, which can discover topically coherent sentences that contain key shared information with as little detail and redundancy as possible.", "labels": [], "entities": []}, {"text": "Our model can discover hierarchical latent structure of multi-documents, in which some words are governed by low-level topics (T) and others by high-level topics (H).", "labels": [], "entities": []}, {"text": "The main contributions of this work are: \u2212 construction of a novel bayesian framework to 491 capture higher level topics (concepts) related to summary text discussed in \u00a73, \u2212 representation of a linguistic system as a sequence of increasingly enriched models, which use posterior topic correlation probabilities in sentences to design a novel sentence ranking method in \u00a74 and 5, \u2212 application of the new hierarchical learning method for generation of less redundant summaries discussed in \u00a76.", "labels": [], "entities": []}, {"text": "Our models achieve comparable qualitative results on summarization of multiple newswire documents.", "labels": [], "entities": [{"text": "summarization of multiple newswire documents", "start_pos": 53, "end_pos": 97, "type": "TASK", "confidence": 0.8172244310379029}]}, {"text": "Human evaluations of generated summaries confirm that our model can generate non-redundant and topically coherent summaries.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we qualitatively compare our models against state-of-the art models and later apply an intrinsic evaluation of generated summaries on topical coherence and informativeness.", "labels": [], "entities": []}, {"text": "For a qualitative comparison with the previous state-of-the models, we use the standard summarization datasets on this task.", "labels": [], "entities": []}, {"text": "We train our models on the datasets provided by DUC2005 task and validate the results on DUC 2006 task, which consist of a total of 100 document clusters.", "labels": [], "entities": [{"text": "DUC2005 task", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9246537983417511}, {"text": "DUC 2006 task", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9597348570823669}]}, {"text": "We evaluate the performance of our models on DUC2007 datasets, which comprise of 45 document clusters, each containing 25 news articles.", "labels": [], "entities": [{"text": "DUC2007 datasets", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9701527059078217}]}, {"text": "The task is to create max.", "labels": [], "entities": []}, {"text": "250 word long summary for each document cluster.", "labels": [], "entities": []}, {"text": "ROUGE Evaluations: We train each document cluster as a separate corpus to find the optimum parameters of each model and evaluate on test document clusters.", "labels": [], "entities": []}, {"text": "ROUGE is a commonly used measure, a standard DUC evaluation metric, which computes recall over various n-grams statistics from a model generated summary against a set of human generated summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9667177200317383}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.995236873626709}]}, {"text": "We report results in R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 497 The following models are used as benchmark: (i) PYTHY (: Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models.", "labels": [], "entities": [{"text": "PYTHY", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.8753471970558167}, {"text": "HIERSUM", "start_pos": 254, "end_pos": 261, "type": "METRIC", "confidence": 0.9207911491394043}]}, {"text": "Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence of the generated summary concept distributions from document word-frequency distributions.", "labels": [], "entities": [{"text": "KL-divergence", "start_pos": 105, "end_pos": 118, "type": "METRIC", "confidence": 0.9711620211601257}]}, {"text": "(iii) HybHSum (Celikyilmaz and Hakkani-Tur, 2010): A semisupervised model, which builds a hierarchial LDA to probabilistically score sentences in training dataset as summary or non-summary sentences.", "labels": [], "entities": []}, {"text": "Using these probabilities as output variables, it learns a discriminative classifier model to infer the scores of new sentences in testing dataset.", "labels": [], "entities": []}, {"text": "(iv) PAM () and hPAM (: Two hierarchical topic models to discover high and lowlevel concepts from documents, baselines for synthetic experiments in \u00a74 & \u00a75.", "labels": [], "entities": []}, {"text": "Results of our experiments are illustrated in.", "labels": [], "entities": []}, {"text": "Our unsupervised TTM and ETTM systems yield a 44.1 R-1 (w/ stop-words) outperforming the rest of the models, except HybHSum.", "labels": [], "entities": [{"text": "R-1", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.7612473964691162}, {"text": "HybHSum", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.967456579208374}]}, {"text": "Because HybHSum uses the human generated summaries as supervision during model development and our systems do not, our performance is quite promising considering the generation is completely unsupervised without seeing any human generated summaries during training.", "labels": [], "entities": []}, {"text": "However, the R-2 evaluation (as well as R-4) w/ stop-words does not outperform other models.", "labels": [], "entities": []}, {"text": "This is because R-2 is a measure of bi-gram recall and neither of our models represent bi-grams whereas, for instance, PHTHY includes several bi-gram and higher order n-gram statistics.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9846440553665161}, {"text": "PHTHY", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.7819176912307739}]}, {"text": "For topic models bigrams tend to degenerate due to generating inconsistent bag of bi-grams ().", "labels": [], "entities": []}, {"text": "Manual Evaluations: A common DUC task is to manually evaluate models on the quality of generated summaries.", "labels": [], "entities": []}, {"text": "We compare our best model ETTM to the results of PAM, our benchmark model in synthetic experiments, as well as hybrid hierarchical summarization model, hLDA.", "labels": [], "entities": []}, {"text": "Human annotators are given two sets of summary text for each document set, generated from either one of the two approaches: best ETTM and PAM or best ETTM and HybHSum models.", "labels": [], "entities": []}, {"text": "The annotators are asked to mark the better summary according to five criteria: non-redundancy (which summary is less redundant), coherence (which summary is more coherent), focus and readability (content and no unnecessary details), responsiveness and overall performance.", "labels": [], "entities": []}, {"text": "We asked 3 annotators to rate DUC2007 predicted summaries (45 summary pairs per annotator).", "labels": [], "entities": [{"text": "DUC2007", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.850466787815094}]}, {"text": "A total of 42 pairs are judged for ETTM vs. PAM models and 49 pairs for ETTM vs. HybHSum models.", "labels": [], "entities": [{"text": "ETTM vs. HybHSum", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.6802465716997782}]}, {"text": "The evaluation results in frequencies are shown in.", "labels": [], "entities": []}, {"text": "The participants rated ETTM generated summaries more coherent and focused compared to PAM, where the results are statistically significant (based on t-test on 95% confidence level) indicating that ETTM summaries are rated significantly better.", "labels": [], "entities": []}, {"text": "The results of ETTM are slightly better than HybHSum.", "labels": [], "entities": [{"text": "ETTM", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.6121898889541626}]}, {"text": "We consider our results promising because, being unsupervised, ETTM does not utilize human summaries for model development.", "labels": [], "entities": [{"text": "ETTM", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8283178806304932}]}], "tableCaptions": [{"text": " Table 1: ROUGE results of the best systems on DUC2007  dataset (best results are bolded.)  *  indicate our models.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9891966581344604}, {"text": "DUC2007  dataset", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9873097240924835}]}, {"text": " Table  6. Our unsupervised TTM and ETTM systems yield a  44.1 R-1 (w/ stop-words) outperforming the rest of  the models, except HybHSum. Because HybHSum  uses the human generated summaries as supervision  during model development and our systems do not,", "labels": [], "entities": [{"text": "R-1", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.7397354245185852}, {"text": "HybHSum", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9559043049812317}]}]}