{"title": [{"text": "Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation", "labels": [], "entities": [{"text": "Improve Coordination Disambiguation", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.7935375968615214}]}], "abstractContent": [{"text": "Resolving coordination ambiguity is a classic hard problem.", "labels": [], "entities": [{"text": "Resolving coordination ambiguity", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.903743048508962}]}, {"text": "This paper looks at coordination disambiguation in complex noun phrases (NPs).", "labels": [], "entities": [{"text": "coordination disambiguation in complex noun phrases (NPs)", "start_pos": 20, "end_pos": 77, "type": "TASK", "confidence": 0.7839231093724569}]}, {"text": "Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don't do very well on this problem (79%).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9761339426040649}]}, {"text": "We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Eu-roparl), and (3) unannotated monolingual (e.g. Google N-grams).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9949490129947662}]}, {"text": "Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words.", "labels": [], "entities": []}, {"text": "The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items.", "labels": [], "entities": []}, {"text": "The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation.", "labels": [], "entities": []}, {"text": "We train separate classifiers with monolingual and bilingual features and iteratively improve them via co-training.", "labels": [], "entities": []}, {"text": "The co-trained classifier achieves close to 96% accuracy on Treebank data and makes 20% fewer errors than a supervised system trained with Treebank annotations .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9978615641593933}, {"text": "Treebank data", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9734413027763367}]}], "introductionContent": [{"text": "Determining which words are being linked by a coordinating conjunction is a classic hard problem.", "labels": [], "entities": []}, {"text": "Consider the pair: +ellipsis rocket\\w 1 and mortar\\w 2 attacks\\h \u2212ellipsis asbestos\\w 1 and polyvinyl\\w 2 chloride\\h +ellipsis is about both rocket attacks and mortar attacks, unlike \u2212ellipsis which is not about asbestos chloride.", "labels": [], "entities": []}, {"text": "We use h to refer to the head of the phrase, and w 1 and w 2 to refer to the other two lexical items.", "labels": [], "entities": []}, {"text": "Natural Language Processing applications need to recognize NP ellipsis in order to make sense of new sentences.", "labels": [], "entities": []}, {"text": "For example, if an Internet search engine is given the phrase rocket attacks as a query, it should rank documents containing rocket and mortar attacks highly, even though rocket and attacks are not contiguous in the document.", "labels": [], "entities": []}, {"text": "Furthermore, NPs with ellipsis often require a distinct type of reordering when translated into a foreign language.", "labels": [], "entities": []}, {"text": "Since coordination is both complex and productive, parsers and machine translation (MT) systems cannot simply memorize the analysis of coordinate phrases from training text.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.8282022356987}]}, {"text": "We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9339599609375}]}, {"text": "While the general case of coordination is quite complicated, we focus on the special case of complex NPs.", "labels": [], "entities": []}, {"text": "Errors in NP coordination typically account for the majority of parser coordination errors.", "labels": [], "entities": [{"text": "NP coordination", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8838421106338501}]}, {"text": "The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem.", "labels": [], "entities": []}, {"text": "We first resolve coordinate NP ambiguity in a word-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "In bitexts, both monolingual and bilingual information can indicate NP structure.", "labels": [], "entities": []}, {"text": "We create separate classifiers using monolingual and bilingual feature views.", "labels": [], "entities": []}, {"text": "We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9989051818847656}]}, {"text": "Starting from only two initial labeled examples, we are able to train a highly accurate classifier using only monolingual features.", "labels": [], "entities": []}, {"text": "The monolingual classifier can then be used both within and beyond the aligned bitext.", "labels": [], "entities": []}, {"text": "In particular, it achieves close to 96% accuracy on both bitext data and on out-of-domain examples in the Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9989678859710693}, {"text": "Treebank", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9417166709899902}]}], "datasetContent": [{"text": "We evaluate using accuracy: the percentage of examples classified correctly in held-out test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9994774460792542}]}, {"text": "We compare our systems to a baseline referred to as the Tag-Triple classifier.", "labels": [], "entities": []}, {"text": "This classifier has a single feature: the tag(w 1 ), tag(w 2 ), tag(h) triple.", "labels": [], "entities": []}, {"text": "Tag-Triple is therefore essentially a discriminative, unlexicalized parser for our coordinate NPs.", "labels": [], "entities": []}, {"text": "All classifiers use L2-regularized logistic regression training via LIBLINEAR).", "labels": [], "entities": []}, {"text": "For co-training, we fix regularization at C = 0.1.", "labels": [], "entities": []}, {"text": "For all other classifiers, we optimize the C parameter on the development data.", "labels": [], "entities": []}, {"text": "At each iteration, i, classifier h m annotates 50 new examples for training h b , from a pool of 750 examples, while h b annotates 50 * i new examples for h m , from a pool of 750 * i examples.", "labels": [], "entities": []}, {"text": "This ensures h m gets the majority of automaticallylabeled examples.", "labels": [], "entities": []}, {"text": "We also set k, the number of co-training iterations.", "labels": [], "entities": []}, {"text": "The monolingual, bilingual, and combined classifiers reach their optimum levels of performance after different numbers of iterations.", "labels": [], "entities": []}, {"text": "We therefore set k separately for each, stopping around 16 iterations for the combined, 51 for the monolingual, and 57 for the bilingual classifier.", "labels": [], "entities": []}, {"text": "We evaluate our systems on our held-out bitext data.", "labels": [], "entities": []}, {"text": "The majority class is ellipsis, in 55.8% of examples.", "labels": [], "entities": []}, {"text": "For comparison, we ran two publicly-available broad-coverage parsers and analyzed whether they correctly predicted ellipsis.", "labels": [], "entities": []}, {"text": "The parsers were the C&C parser ( and Minipar.", "labels": [], "entities": [{"text": "C&C parser", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8697258234024048}]}, {"text": "They achieved 78.6% and 77.6%.", "labels": [], "entities": [{"text": "78.6", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9326950311660767}]}, {"text": "3 shows that co-training results in much more accurate classifiers than supervised training alone, regardless of the features or amount of initial training data.", "labels": [], "entities": []}, {"text": "The Tag-Triple system is the weakest system in all cases.", "labels": [], "entities": []}, {"text": "This shows that better monolingual features are very important, but semisupervised training can also make a big difference.", "labels": [], "entities": []}, {"text": "shows the net benefit of our main contributions.", "labels": [], "entities": []}, {"text": "Bilingual features clearly help on this task, but not as much as co-training.", "labels": [], "entities": []}, {"text": "With bilingual features and co-training together, we achieve 96.7% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9966697096824646}]}, {"text": "This combined system could be used to very accurately resolve coordinate ambiguity in parallel data prior to training an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.9842038750648499}]}, {"text": "While we can now accurately resolve coordinate NP ambiguity in parallel text, it would be even better if this accuracy carried over to new domains, where bilingual features are not available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9980342984199524}]}, {"text": "We test the robustness of our co-trained monolingual classifier by evaluating it on our labeled WSJ data.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9236459732055664}]}, {"text": "The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9935901165008545}]}, {"text": "We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with in- and trained the latter on WSJ annotations.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.872738778591156}]}, {"text": "We compare these systems to Tag-Triple and also to a supervised system trained on the WSJ using only our monolingual features (MonoWSJ).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9549948573112488}]}, {"text": "The (out-of-domain) bitext co-trained system is the best system on the WSJ data, both on just the examples where w 1 and w 2 are nouns (Nouns), and on all examples (All) (.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9794448912143707}]}, {"text": "It is statistically significantly better than the prior state-of-the-art Pitler et al. system (McNemar's test, p<0.05) and also exceeds the WSJ-trained system using monolingual features (p<0.2).", "labels": [], "entities": [{"text": "WSJ-trained", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.7712002396583557}]}, {"text": "This domain robustness is less surprising given its key features are derived from webscale N-gram data; such features are known to generalize well across domains ( . We tried co-training without the N-gram features, and performance was worse on the WSJ (85%) than supervised training on WSJ data alone (87%).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 249, "end_pos": 252, "type": "DATASET", "confidence": 0.9448442459106445}, {"text": "WSJ data", "start_pos": 287, "end_pos": 295, "type": "DATASET", "confidence": 0.9622887372970581}]}], "tableCaptions": [{"text": " Table 5: Co-training improves accuracy (%) over stan- dard supervised learning on Bitext test data for different  feature types and number of training examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9994695782661438}, {"text": "Bitext test data", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.9258038202921549}]}, {"text": " Table 6: Net benefits of bilingual features and co-training  on Bitext data, 100-training-example setting. \u2206 = rela- tive error reduction over Monolingual alone.", "labels": [], "entities": [{"text": "Bitext data", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.9502878487110138}, {"text": "rela- tive error reduction", "start_pos": 112, "end_pos": 138, "type": "METRIC", "confidence": 0.9221505045890808}]}, {"text": " Table 7: Coordinate resolution accuracy (%) on WSJ.", "labels": [], "entities": [{"text": "Coordinate resolution", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7889881134033203}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.7351916432380676}, {"text": "WSJ", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9312279224395752}]}]}