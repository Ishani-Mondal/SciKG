{"title": [{"text": "Lost in Translation: Authorship Attribution using Frame Semantics", "labels": [], "entities": [{"text": "Authorship Attribution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.6707912236452103}]}], "abstractContent": [{"text": "We investigate authorship attribution using classifiers based on frame semantics.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7982924282550812}]}, {"text": "The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.785647451877594}, {"text": "authorship attribution of translated texts", "start_pos": 191, "end_pos": 233, "type": "TASK", "confidence": 0.8430064499378205}]}, {"text": "Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that frame-based classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that-contrary to current belief-na\u00efve clas-sifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.", "labels": [], "entities": []}], "introductionContent": [{"text": "Authorship attribution is the following problem: For a given text, determine the author of said text among a list of candidate authors.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8457751274108887}]}, {"text": "Determining authorship is difficult, and a host of methods have been proposed: As of 1998 Rudman estimated the number of metrics used in such methods to beat least 1000).", "labels": [], "entities": []}, {"text": "For comprehensive recent surveys see e.g..", "labels": [], "entities": []}, {"text": "The process of authorship attribution consists of selecting markers (features that provide an indication of the author), and classifying a text by assigning it to an author using some appropriate machine learning technique.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7352979183197021}]}], "datasetContent": [{"text": "For both corpora, authorship attribution experiments were performed using six classifiers, each employing a distinct feature set.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8188167214393616}]}, {"text": "For each feature set the markers were counted in the text and their relative frequencies calculated.", "labels": [], "entities": []}, {"text": "Feature selection was based solely on training data in the inner loop of the crossvalidation cycle.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7729686200618744}]}, {"text": "Two sets of experiments were performed, each with with X = 200 and X = 400 features; the size of the feature vector was kept constant across comparison of methods, due to space constraints only results for 400 features are reported.", "labels": [], "entities": []}, {"text": "The feature sets were: Frequent Words (FW): Frequencies in the text of the X most frequent words . Classification with this feature set is used as baseline.", "labels": [], "entities": [{"text": "Frequent Words (FW)", "start_pos": 23, "end_pos": 42, "type": "METRIC", "confidence": 0.8206117749214172}]}, {"text": "Character N-grams: The X most frequent Ngrams for N = 3, 4, 5.", "labels": [], "entities": []}, {"text": "Frames: The relative frequencies of the X most frequently occurring semantic frames.", "labels": [], "entities": []}, {"text": "Frequent Words and Frames (FWaF): The X/2 most frequent features; words and frames resp.", "labels": [], "entities": [{"text": "Frequent Words and Frames (FWaF)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.4683696244444166}]}, {"text": "combined to a single feature vector of size X.", "labels": [], "entities": []}, {"text": "In order to gauge the impact of translation upon an author's footprint, three different experiments were performed on subsets of Corpus II: The full corpus of 30 texts [Corpus IIa] was used for authorship attribution with an ample mix of authors an translators, several translators having translated texts by more than one author.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 194, "end_pos": 216, "type": "TASK", "confidence": 0.6980113834142685}]}, {"text": "To ascertain how heavily each marker is influenced by translation we also performed translator attribution on a subset of 11 texts [Corpus IIb] with 3 different translators each having translated 3 different authors.", "labels": [], "entities": [{"text": "Corpus IIb]", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9158052206039429}]}, {"text": "If the translator leaves a heavy footprint on the marker, the marker is expected to score better when attributing to translator than to author.", "labels": [], "entities": []}, {"text": "Finally, we reduced the corpus to a set of 18 texts [Corpus IIc] that only includes unique author/translator combinations to see if each marker could attribute correctly to an author if the translator/author combination was not present in the training set.", "labels": [], "entities": [{"text": "Corpus IIc]", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.8927423556645712}]}, {"text": "All classification experiments were conducted using a multi-class winner-takes-all) support vector machine (SVM).", "labels": [], "entities": []}, {"text": "For cross-validation, all experiments used leave-one-out (i.e. N -fold for N texts in the corpus) validation.", "labels": [], "entities": []}, {"text": "All features were scaled to lie in the range before different types of features were combined.", "labels": [], "entities": []}, {"text": "In each step of the cross-validation process, the most frequently occurring features were selected from the training data, and to minimize the effect of skewed training data on the results, oversampling with substitution was used on the training data.", "labels": [], "entities": []}, {"text": "We tested our results for statistical significance using McNemar's test with Yates' correction for continuity against the null hypothesis that the classifier is indistinguishable from a random attribution weighted by the number of author texts in the corpus.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.8785992463429769}, {"text": "continuity", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9918185472488403}]}], "tableCaptions": [{"text": " Table 2: Authorship attribution results", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8520121276378632}]}]}