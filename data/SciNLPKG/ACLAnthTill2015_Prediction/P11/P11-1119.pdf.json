{"title": [{"text": "An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue", "labels": [], "entities": []}], "abstractContent": [{"text": "Dialogue act classification is a central challenge for dialogue systems.", "labels": [], "entities": [{"text": "Dialogue act classification", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7228541572888693}]}, {"text": "Although the importance of emotion inhuman dialogue is widely recognized, most dialogue act classification models make limited or no use of affec-tive channels in dialogue act classification.", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6363348662853241}, {"text": "dialogue act classification", "start_pos": 163, "end_pos": 190, "type": "TASK", "confidence": 0.6503002643585205}]}, {"text": "This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion.", "labels": [], "entities": []}, {"text": "The findings indicate that the affect-enriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue.", "labels": [], "entities": [{"text": "distinguishing user requests for feedback", "start_pos": 92, "end_pos": 133, "type": "TASK", "confidence": 0.8356318235397339}]}, {"text": "The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification.", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.7192502816518148}]}], "introductionContent": [{"text": "Dialogue systems aim to engage users in rich, adaptive natural language conversation.", "labels": [], "entities": []}, {"text": "For these systems, understanding the role of a user's utterance in the broader context of the dialogue is a key challenge.", "labels": [], "entities": []}, {"text": "Central to this endeavor is dialogue act classification, which categorizes the intention behind the user's move (e.g., asking a question, providing declarative information).", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.7316815257072449}]}, {"text": "Automatic dialogue act classification has been the focus of a large body of research, and a variety of approaches, including sequential models (), vector-based models, and most recently, featureenhanced latent semantic analysis, have shown promise.", "labels": [], "entities": [{"text": "Automatic dialogue act classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6823105961084366}, {"text": "featureenhanced latent semantic analysis", "start_pos": 187, "end_pos": 227, "type": "TASK", "confidence": 0.6216692477464676}]}, {"text": "These models maybe further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources.", "labels": [], "entities": []}, {"text": "Users' expressions of emotion are one such source.", "labels": [], "entities": []}, {"text": "Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role.", "labels": [], "entities": []}, {"text": "While the importance of emotional expressions in dialogue is widely recognized, the majority of dialogue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spoken utterances to aid in online dialogue act classification.", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.6361285050710043}, {"text": "online dialogue act classification", "start_pos": 278, "end_pos": 312, "type": "TASK", "confidence": 0.6026008203625679}]}, {"text": "Other research on emotion in dialogue has involved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru,, but this work has not explored leveraging affect information for automatic user dialogue act classification.", "labels": [], "entities": [{"text": "automatic user dialogue act classification", "start_pos": 201, "end_pos": 243, "type": "TASK", "confidence": 0.6159828424453735}]}, {"text": "Outside of dialogue, sentiment analysis within discourse is an active area of research (), but it is generally lim-ited to modeling textual features and not multimodal expressions of emotion such as facial actions.", "labels": [], "entities": [{"text": "sentiment analysis within discourse", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.9164084494113922}]}, {"text": "Such multimodal expressions have only just begun to be explored within corpus-based dialogue research.", "labels": [], "entities": []}, {"text": "This paper presents a novel affect-enriched dialogue act classification approach that leverages knowledge of users' facial expressions during computer-mediated textual human-human dialogue.", "labels": [], "entities": [{"text": "affect-enriched dialogue act classification", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.6375152319669724}]}, {"text": "Intuitively, the user's affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user maybe more likely to ask a question).", "labels": [], "entities": []}, {"text": "We focus specifically on occurrences of students' confusion-related facial actions during taskoriented tutorial dialogue.", "labels": [], "entities": []}, {"text": "Confusion was selected as the focus of this work for several reasons.", "labels": [], "entities": []}, {"text": "First, confusion is known to be prevalent within tutoring, and its implications for student learning are thought to run deep).", "labels": [], "entities": []}, {"text": "Second, while identifying the \"ground truth\" of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular facial action units and confusion during learning.", "labels": [], "entities": []}, {"text": "Finally, automatic facial action recognition technologies are developing rapidly, and confusion-related facial action events are among those that can be reliably recognized automatically (.", "labels": [], "entities": [{"text": "facial action recognition", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.6763183176517487}]}, {"text": "This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems.", "labels": [], "entities": [{"text": "real-time confusion detection", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.7004872361818949}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Excerpt from corpus illustrating annota- tions and interplay between dialogue and task", "labels": [], "entities": []}, {"text": " Table 3. Kappa values for inter-annotator agree- ment on facial action events", "labels": [], "entities": []}, {"text": " Table 4. Classification accuracy and kappa for spe- cialized DA classifiers. Statistically significant  differences (across ten folds, one-tailed t-test) are  shown in bold.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8069543838500977}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9753352999687195}, {"text": "kappa", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9940641522407532}]}]}