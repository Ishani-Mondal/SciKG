{"title": [{"text": "Using Cross-Entity Inference to Improve Event Extraction", "labels": [], "entities": [{"text": "Cross-Entity Inference", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.7304924726486206}, {"text": "Improve Event Extraction", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8549588918685913}]}], "abstractContent": [{"text": "Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7683717906475067}]}, {"text": "The state-of-the-art research on the task is transductive inference (e.g. cross-event inference).", "labels": [], "entities": [{"text": "cross-event inference)", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8038377165794373}]}, {"text": "In this paper, we propose anew method of event extraction by well using cross-entity inference.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.810335248708725}]}, {"text": "In contrast to previous inference methods, we regard entity-type consistency as key feature to predict event mentions.", "labels": [], "entities": []}, {"text": "We adopt this inference method to improve the traditional sentence-level event extraction system.", "labels": [], "entities": [{"text": "sentence-level event extraction", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.6154759327570597}]}, {"text": "Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction.", "labels": [], "entities": [{"text": "trigger (event) identification", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.589749938249588}, {"text": "argument (role) classification", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.68431156873703}, {"text": "ACE event extraction", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.8713292479515076}]}], "introductionContent": [{"text": "The event extraction task in ACE (Automatic Content Extraction) evaluation involves three challenging issues: distinguishing events of different types, finding the participants of an event and determining the roles of the participants.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.710908055305481}, {"text": "ACE (Automatic Content Extraction) evaluation", "start_pos": 29, "end_pos": 74, "type": "TASK", "confidence": 0.6095356345176697}]}, {"text": "The recent researches on the task show the availability of transductive inference, such as that of the following methods: cross-document, crosssentence and cross-event inferences.", "labels": [], "entities": []}, {"text": "Transductive inference is a process to use the known instances to predict the attributes of unknown instances.", "labels": [], "entities": [{"text": "Transductive inference", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9042807519435883}]}, {"text": "As an example, given a target event, the cross-event inference can predict its type by well using the related events co-occurred with it within the same document.", "labels": [], "entities": []}, {"text": "From the sentence: (1)He left the company.", "labels": [], "entities": []}, {"text": "it is hard to tell whether it is a Transport event in ACE, which means that he left the place; or an End-Position event, which means that he retired from the company.", "labels": [], "entities": [{"text": "ACE", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.812868595123291}, {"text": "End-Position", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.958579957485199}]}, {"text": "But cross-event inference can use a related event \"Then he went shopping\" within the same document to identify it as a Transport event correctly.", "labels": [], "entities": [{"text": "cross-event inference", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.857276439666748}]}, {"text": "As the above example might suggest, the availability of transductive inference for event extraction relies heavily on the known evidences of an event occurrence in specific condition.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7665185928344727}]}, {"text": "However, the evidence supporting the inference is normally unclear or absent.", "labels": [], "entities": []}, {"text": "For instance, the relation among events is the key clue for cross-event inference to predict a target event type, as shown in the inference process of the sentence (1).", "labels": [], "entities": [{"text": "cross-event inference", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7935703098773956}]}, {"text": "But event relation extraction itself is a hard task in Information Extraction.", "labels": [], "entities": [{"text": "event relation extraction", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.73787788550059}, {"text": "Information Extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8114261329174042}]}, {"text": "So cross-event inference often suffers from some false evidence (viz., misleading by unrelated events) or lack of valid evidence (viz., unsuccessfully extracting related events).", "labels": [], "entities": [{"text": "cross-event inference", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.9020732939243317}]}, {"text": "In this paper, we propose anew method of transductive inference, named cross-entity inference, for event extraction by well using the relations among entities.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.7336467504501343}]}, {"text": "This method is firstly motivated by the inherent ability of entity types in revealing event types.", "labels": [], "entities": []}, {"text": "From the sentences: (2)He left the bathroom.", "labels": [], "entities": []}, {"text": "(3)He left Microsoft.", "labels": [], "entities": [{"text": "Microsoft", "start_pos": 11, "end_pos": 20, "type": "DATASET", "confidence": 0.9472612142562866}]}, {"text": "it is easy to identify the sentence (2) as a Transport event in ACE, which means that he left the place, because nobody would retire (End-Position type) from a bathroom.", "labels": [], "entities": [{"text": "ACE", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.7187464237213135}]}, {"text": "And compared to the entities in sentence (1) and (2), the entity \"Microsoft\" in (3) would give us more confidence to tag the \"left\" event as an End-Position type, because people are used to giving the full name of the place where they retired.", "labels": [], "entities": []}, {"text": "The cross-entity inference is also motivated by the phenomenon that the entities of the same type often attend similar events.", "labels": [], "entities": [{"text": "cross-entity inference", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7872461676597595}]}, {"text": "That gives us away to predict event type based on entity-type consistency.", "labels": [], "entities": []}, {"text": "From the sentence: (4)Obama beats McCain.", "labels": [], "entities": []}, {"text": "it is hard to identify it as an Elect event in ACE, which means Obama wins the Presidential Election, or an Attack event, which means Obama roughs somebody up.", "labels": [], "entities": [{"text": "ACE", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7809782028198242}]}, {"text": "But if we have the priori knowledge that the sentence \"Bush beats McCain\" is an Elect event, and \"Obama\" was a presidential contender just like \"Bush\" (strict type consistency), we have ample evidence to predict that the sentence (4) is also an Elect event.", "labels": [], "entities": []}, {"text": "Indeed above cross-entity inference for eventtype identification is not the only use of entity-type consistency.", "labels": [], "entities": [{"text": "cross-entity inference", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7750174403190613}, {"text": "eventtype identification", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.762671947479248}]}, {"text": "As we shall describe below, we can make use of it at all issues of event extraction: \ud97b\udf59 For event type: the entities of the same type are most likely to attend similar events.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7447500824928284}]}, {"text": "And the events often use consistent or synonymous trigger.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 For event argument (participant): the entities of the same type normally co-occur with similar participants in the events of the same type.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 For argument role: the arguments of the same type, for the most part, play the same roles in similar events.", "labels": [], "entities": []}, {"text": "With the help of above characteristics of entity, we can perform a step-by-step inference in this order: \ud97b\udf59 Step 1: predicting event type and labeling trigger given the entities of the same type.", "labels": [], "entities": [{"text": "predicting event type", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.8662883241971334}]}, {"text": "\ud97b\udf59 Step 2: identifying arguments in certain event given priori entity type, event type and trigger that obtained by step 1.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 Step 3: determining argument roles in certain event given entity type, event type, trigger and arguments that obtained by step 1 and step 2.", "labels": [], "entities": []}, {"text": "On the basis, we give a blind cross-entity inference method for event extraction in this paper.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7925600409507751}]}, {"text": "In the method, we first regard entities as queries to retrieve their related documents from large-scale language resources, and use the global evidences of the documents to generate entity-type descriptions.", "labels": [], "entities": []}, {"text": "Second we determine the type consistency of entities by measuring the similarity of the type descriptions.", "labels": [], "entities": []}, {"text": "Finally, given the priori attributes of events in the training data, with the help of the entities of the same type, we perform the step-by-step cross-entity inference on the attributes of test events (candidate sentences).", "labels": [], "entities": []}, {"text": "In contrast to other transductive inference methods on event extraction, the cross-entity inference makes every effort to strengthen effects of entities in predicting event occurrences.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7232967764139175}]}, {"text": "Thus the inferential process can benefit from following aspects: 1) less false evidence, viz.", "labels": [], "entities": []}, {"text": "less false entity-type consistency (the key clue of cross-entity inference), because the consistency can be more precisely determined with the help of fully entity-type description that obtained based on the related information from Web; 2) more valid evidence, viz.", "labels": [], "entities": [{"text": "cross-entity inference", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7962015867233276}]}, {"text": "more entities of the same type (the key references for the inference), because any entity never lack its congeners.", "labels": [], "entities": []}], "datasetContent": [{"text": "We followed Liao (2010)'s evaluation and randomly select 10 newswire texts from the ACE 2005 training corpus as our development set, which is used for parameter tuning, and then conduct a blind test on a separate set of 40 ACE 2005 newswire texts.", "labels": [], "entities": [{"text": "ACE 2005 training corpus", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.9418684095144272}, {"text": "parameter tuning", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.7007364481687546}, {"text": "ACE 2005 newswire texts", "start_pos": 223, "end_pos": 246, "type": "DATASET", "confidence": 0.9214272201061249}]}, {"text": "We use the rest of the ACE training corpus (549 documents) as training data for our event extraction system.", "labels": [], "entities": [{"text": "ACE training corpus (549 documents)", "start_pos": 23, "end_pos": 58, "type": "DATASET", "confidence": 0.9373042838914054}, {"text": "event extraction", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.8140447437763214}]}, {"text": "To compare with the reported work on crossevent inference and its sentence-level baseline system, we cross-validate our method on 10 separate sets of 40 ACE texts, and report the optimum, worst and mean performances (see) on the data by using Precision (P), Recall (R) and F-measure (F).", "labels": [], "entities": [{"text": "crossevent inference", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8646274209022522}, {"text": "ACE texts", "start_pos": 153, "end_pos": 162, "type": "DATASET", "confidence": 0.8761904835700989}, {"text": "Precision (P)", "start_pos": 243, "end_pos": 256, "type": "METRIC", "confidence": 0.9352662563323975}, {"text": "Recall (R)", "start_pos": 258, "end_pos": 268, "type": "METRIC", "confidence": 0.9487982541322708}, {"text": "F-measure (F)", "start_pos": 273, "end_pos": 286, "type": "METRIC", "confidence": 0.958690732717514}]}, {"text": "In addition, we also report the performance of two human annotators on 40 ACE newswire texts (a random blind test set): one knows the rules of event extraction; the other knows nothing about it.", "labels": [], "entities": [{"text": "ACE newswire texts", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.9626022577285767}, {"text": "event extraction", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7447030246257782}]}], "tableCaptions": [{"text": " Table 3: Events co-occurring with Population- Center with the conditional probability > 0.05  Actually we find that most entity types appear in  more restricted event mentions than Population- Center entity. For example, Air entity only co- occurs with 5 event types", "labels": [], "entities": []}, {"text": " Table 4: Distribution of entity-event combination  corresponding to different co-occurrence frequency", "labels": [], "entities": []}, {"text": " Table 6: Distribution of entity-role combination  corresponding to different co-occurrence frequency", "labels": [], "entities": []}, {"text": " Table 8: Overall performance on blind test data", "labels": [], "entities": []}, {"text": " Table 10: Performances on visible VS blind", "labels": [], "entities": []}]}