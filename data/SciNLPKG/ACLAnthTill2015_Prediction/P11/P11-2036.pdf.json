{"title": [{"text": "Insertion Operator for Bayesian Tree Substitution Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG).", "labels": [], "entities": [{"text": "Bayesian tree substitution grammars (BTSG)", "start_pos": 62, "end_pos": 104, "type": "TASK", "confidence": 0.7368462937218803}]}, {"text": "Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG.", "labels": [], "entities": [{"text": "Tree insertion", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6764498651027679}, {"text": "BTSG", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.8206651210784912}]}, {"text": "The experimental parsing results show that our model outperforms a standard PCFG and BTSG fora small dataset.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9552626013755798}, {"text": "BTSG", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.6441922783851624}]}, {"text": "For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG.", "labels": [], "entities": [{"text": "BTSG", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.7617078423500061}, {"text": "BTSG", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.9480227828025818}]}], "introductionContent": [{"text": "Tree substitution grammar (TSG) is a promising formalism for modeling language data.", "labels": [], "entities": [{"text": "Tree substitution grammar (TSG)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8354121049245199}]}, {"text": "TSG generalizes context free grammars (CFG) by allowing nonterminal nodes to be replaced with subtrees of arbitrary size.", "labels": [], "entities": [{"text": "context free grammars (CFG)", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6923236052195231}]}, {"text": "A natural extension of TSG involves adding an insertion operator for combining subtrees as in tree adjoining grammars (TAG) or tree insertion grammars (TIG) ().", "labels": [], "entities": []}, {"text": "An insertion operator is helpful for expressing various syntax patterns with fewer grammar rules, thus we expect that adding an insertion operator will improve parsing accuracy and realize a compact grammar size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9295151829719543}]}, {"text": "One of the challenges of adding an insertion operator is that the computational cost of grammar induction is high since tree insertion significantly increases the number of possible subtrees.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7821048200130463}]}, {"text": "Previous work on TAG and TIG induction) has addressed the problem using language-specific heuristics and a maximum likelihood estimator, which leads to overfitting the training data.", "labels": [], "entities": [{"text": "TIG induction", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7154096513986588}]}, {"text": "Instead, we incorporate an insertion operator in a Bayesian TSG (BTSG) model) that learns grammar rules automatically without heuristics.", "labels": [], "entities": []}, {"text": "Our model uses a restricted variant of subtrees for insertion to model the probability distribution simply and train the model efficiently.", "labels": [], "entities": []}, {"text": "We also present an inference technique for handling a tree insertion that makes use of dynamic programming.", "labels": [], "entities": [{"text": "handling a tree insertion", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.731264516711235}]}], "datasetContent": [{"text": "We ran experiments on the British National Corpus (BNC) Treebank 3 and the WSJ English Penn Treebank.", "labels": [], "entities": [{"text": "British National Corpus (BNC) Treebank 3", "start_pos": 26, "end_pos": 66, "type": "DATASET", "confidence": 0.9715319946408272}, {"text": "WSJ English Penn Treebank", "start_pos": 75, "end_pos": 100, "type": "DATASET", "confidence": 0.8781711459159851}]}, {"text": "We did not use a development set since our model automatically updates the hyperparameters for every iteration.", "labels": [], "entities": []}, {"text": "The treebank data was binarized using the CENTER-HEAD method ().", "labels": [], "entities": [{"text": "CENTER-HEAD", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.8085006475448608}]}, {"text": "We replaced lexical words with counts \u2264 1 in the training set with one of three unknown Results from   In small dataset experiments, we used BNC (1k sentences, 90% for training and 10% for testing) and WSJ (section 2 for training and section 22 for testing).", "labels": [], "entities": [{"text": "BNC", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9850727915763855}, {"text": "WSJ", "start_pos": 202, "end_pos": 205, "type": "DATASET", "confidence": 0.8733492493629456}]}, {"text": "This was a small-scale experiment, but large enough to be relevant for low-resource languages.", "labels": [], "entities": []}, {"text": "We trained the model with an MH sampler for 1k iterations.", "labels": [], "entities": []}, {"text": "shows the parsing results for the test set.", "labels": [], "entities": []}, {"text": "We compared our model with standard PCFG and BTSG models implemented by us.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.9305987358093262}, {"text": "BTSG", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.670195460319519}]}, {"text": "Our insertion model successfully outperformed CFG and BTSG.", "labels": [], "entities": [{"text": "CFG", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9200390577316284}, {"text": "BTSG", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.4891934394836426}]}, {"text": "This suggests that adding an insertion operator is helpful for modeling syntax trees accurately.", "labels": [], "entities": []}, {"text": "The BTSG model described in) is similar to ours.", "labels": [], "entities": [{"text": "BTSG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.5700157880783081}]}, {"text": "They reported an F1 score of 78.40 (the score of our BTSG model was.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9849315881729126}, {"text": "BTSG", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8185024261474609}]}, {"text": "We speculate that the performance gap is due to data preprocessing such as the treatment of rare words.: Examples of lexicalized auxiliary trees obtained from our model in the full treebank dataset.", "labels": [], "entities": []}, {"text": "Nonterminal symbols created by binarization are shown with an over-bar.", "labels": [], "entities": []}, {"text": "We also applied our model to the full WSJ Penn Treebank setting (section 2-21 for training and section 23 for testing).", "labels": [], "entities": [{"text": "WSJ Penn Treebank setting", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.9220629334449768}]}, {"text": "The parsing results are shown in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9710865616798401}]}, {"text": "We trained the model with an MH sampler for 3.5k iterations.", "labels": [], "entities": []}, {"text": "For the full treebank dataset, our model obtained nearly identical results to those obtained with BTSG model, making the grammar size approximately 19% smaller than that of BTSG.", "labels": [], "entities": [{"text": "BTSG", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8798155784606934}, {"text": "BTSG", "start_pos": 173, "end_pos": 177, "type": "DATASET", "confidence": 0.9614604711532593}]}, {"text": "We can see that only a small number of auxiliary trees have a great impact on reducing the grammar size.", "labels": [], "entities": []}, {"text": "Surprisingly, there are many fewer auxiliary trees than initial trees.", "labels": [], "entities": []}, {"text": "We believe this to be due to the tree binarization and our restricted assumption of simple auxiliary trees.", "labels": [], "entities": []}, {"text": "shows examples of lexicalized auxiliary trees obtained with our model for the full treebank data.", "labels": [], "entities": []}, {"text": "We can see that punctuation (\"-\", \",\", and \";\") and adverb (RB) tend to be inserted in other trees.", "labels": [], "entities": [{"text": "adverb (RB)", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.6600777208805084}]}, {"text": "Punctuation and adverb appear in various positions in English sentences.", "labels": [], "entities": []}, {"text": "Our results suggest that rather than treat those words as substitutions, it is more reasonable to consider them to be \"insertions\", which is intuitively understandable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Small dataset experiments", "labels": [], "entities": []}, {"text": " Table 3: Full Penn Treebank dataset experiments", "labels": [], "entities": [{"text": "Full Penn Treebank dataset", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.8556664288043976}]}]}