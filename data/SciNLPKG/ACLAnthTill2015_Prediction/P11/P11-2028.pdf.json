{"title": [{"text": "Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?", "labels": [], "entities": [{"text": "Evaluation of Chinese Translation Output", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.5797867715358734}]}], "abstractContent": [{"text": "Word is usually adopted as the smallest unit inmost tasks of Chinese language processing.", "labels": [], "entities": [{"text": "Chinese language processing", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6213167210419973}]}, {"text": "However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible.", "labels": [], "entities": []}, {"text": "So far, there has been no detailed study to compare the correlations of these two approaches with human assessment.", "labels": [], "entities": []}, {"text": "In this paper, we compare word-level metrics with character-level metrics on the submitted output of Eng-lish-to-Chinese translation systems in the IWSLT'08 CT-EC and NIST'08 EC tasks.", "labels": [], "entities": [{"text": "Eng-lish-to-Chinese translation", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.5913122147321701}, {"text": "IWSLT'08 CT-EC", "start_pos": 148, "end_pos": 162, "type": "DATASET", "confidence": 0.8720456957817078}, {"text": "NIST'08 EC tasks", "start_pos": 167, "end_pos": 183, "type": "DATASET", "confidence": 0.800683319568634}]}, {"text": "Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics.", "labels": [], "entities": []}, {"text": "Our analysis suggests several key reasons behind this finding.", "labels": [], "entities": []}], "introductionContent": [{"text": "White space serves as the word delimiter in Latin alphabet-based languages.", "labels": [], "entities": []}, {"text": "However, in written Chinese text, there is no word delimiter.", "labels": [], "entities": []}, {"text": "Thus, in almost all tasks of Chinese natural language processing (NLP), the first step is to segment a Chinese sentence into a sequence of words.", "labels": [], "entities": [{"text": "Chinese natural language processing (NLP)", "start_pos": 29, "end_pos": 70, "type": "TASK", "confidence": 0.7584117991583688}]}, {"text": "This is the task of Chinese word segmentation (CWS), an important and challenging task in Chinese NLP.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.7667126854260763}]}, {"text": "Some linguists believe that word (containing at least one character) is the appropriate unit for Chinese language processing.", "labels": [], "entities": [{"text": "Chinese language processing", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.685455322265625}]}, {"text": "When treating CWS as a standalone NLP task, the goal is to segment a sentence into words so that the segmentation matches the human gold-standard segmentation with the highest F-measure, but without considering the performance of the end-to-end NLP application that uses the segmentation output.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9932479858398438}]}, {"text": "In statistical machine translation (SMT), it can happen that the most accurate word segmentation as judged by the human gold-standard segmentation may not produce the best translation output (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.7991293122371038}, {"text": "word segmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7528457343578339}]}, {"text": "While state-of-the-art Chinese word segmenters achieve high accuracy, some errors still remain.", "labels": [], "entities": [{"text": "Chinese word segmenters", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6570896506309509}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9976481795310974}]}, {"text": "Instead of segmenting a Chinese sentence into words, an alternative is to split a Chinese sentence into characters, which can be readily done with perfect accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9946308135986328}]}, {"text": "However, it has been reported that a Chinese-English phrase-based SMT system () that relied on characters (without CWS) performed slightly worse than when it used segmented words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8714039921760559}]}, {"text": "It has been recognized that varying segmentation granularities are needed for SMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9966017007827759}]}, {"text": "To evaluate the quality of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 ( used the word-level BLEU metric ().", "labels": [], "entities": [{"text": "International Workshop on Spoken Language Translation", "start_pos": 59, "end_pos": 112, "type": "TASK", "confidence": 0.643071174621582}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.936455249786377}]}, {"text": "However, IWSLT'08 and NIST'08 adopted character-level evaluation metrics to rank the submitted systems.", "labels": [], "entities": [{"text": "IWSLT'08", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.847570538520813}, {"text": "NIST'08", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.9293420314788818}]}, {"text": "Although there is much work on automatic evaluation of machine translation (MT), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated.", "labels": [], "entities": [{"text": "automatic evaluation of machine translation (MT)", "start_pos": 31, "end_pos": 79, "type": "TASK", "confidence": 0.6881210170686245}, {"text": "Chinese translation output", "start_pos": 152, "end_pos": 178, "type": "TASK", "confidence": 0.7035266359647115}]}, {"text": "In this paper, we utilize various machine translation evaluation metrics to evaluate the quality of Chinese translation output, and compare their correlation with human assessment when the Chinese translation output is segmented into words versus characters.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.7770045101642609}]}, {"text": "Since there are several CWS tools that can segment Chinese sentences into words and their segmentation results are different, we use four representative CWS tools in our experiments.", "labels": [], "entities": []}, {"text": "Our experimental results reveal that character-level me-trics correlate with human assessment better than word-level metrics.", "labels": [], "entities": []}, {"text": "That is, CWS is not essential for automatic evaluation of Chinese translation output.", "labels": [], "entities": [{"text": "evaluation of Chinese translation output", "start_pos": 44, "end_pos": 84, "type": "TASK", "confidence": 0.6281446516513824}]}, {"text": "Our analysis suggests several key reasons behind this finding.", "labels": [], "entities": []}], "datasetContent": [{"text": "Automatic MT evaluation aims at formulating automatic metrics to measure the quality of MT output.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9663839340209961}, {"text": "MT output", "start_pos": 88, "end_pos": 97, "type": "TASK", "confidence": 0.885202020406723}]}, {"text": "Compared with human assessment, automatic evaluation metrics can assess the quality of MT output quickly and objectively without much human labor..", "labels": [], "entities": [{"text": "MT output", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.8873566091060638}]}, {"text": "An example to show an MT system translation and multiple reference translations being segmented into characters or words.", "labels": [], "entities": [{"text": "MT system translation", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8995284636815389}]}, {"text": "To evaluate English translation output, automatic MT evaluation metrics take an English word as the smallest unit when matching a system translation and a reference translation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9302426874637604}]}, {"text": "On the other hand, to evaluate Chinese translation output, the smallest unit to use in matching can be a Chinese word or a Chinese character.", "labels": [], "entities": []}, {"text": "As shown in, given an English sentence \"how much are the umbrellas?\" a Chinese system translation (or a reference translation) can be segmented into characters) or words).", "labels": [], "entities": []}, {"text": "A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (), NIST), METEOR (exact) (), GTM (, and TER).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9328906536102295}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9988409876823425}, {"text": "NIST", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.5461822152137756}, {"text": "METEOR (exact)", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.882609561085701}, {"text": "TER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.997139573097229}]}, {"text": "Some automatic MT evaluation metrics perform deeper linguistic analysis, such as part-of-speech tagging, synonym matching, semantic role labeling, etc.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9647233784198761}, {"text": "part-of-speech tagging", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7127933353185654}, {"text": "synonym matching", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.9025809168815613}, {"text": "semantic role labeling", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.6261727710564932}]}, {"text": "Since part-of-speech tags are only defined for Chinese words and not for Chinese characters, we restrict the automatic MT evaluation metrics explored in this paper to those metrics listed above which do not require part-ofspeech tagging.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.9472023844718933}]}], "tableCaptions": [{"text": " Table 1. Segment-level consistency on IWSLT'08 CT- EC.", "labels": [], "entities": [{"text": "consistency", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.7703980803489685}, {"text": "IWSLT'08 CT- EC", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.8670972436666489}]}, {"text": " Table 2. Average segment-level correlation on NIST'08  EC.", "labels": [], "entities": [{"text": "correlation", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.5702560544013977}, {"text": "NIST'08  EC", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.932493656873703}]}, {"text": " Table 3. System-level correlation on IWSLT'08 CT-EC.", "labels": [], "entities": [{"text": "correlation", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.8410706520080566}, {"text": "IWSLT'08 CT-EC", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8265014886856079}]}, {"text": " Table 4. System-level correlation on NIST'08 EC.", "labels": [], "entities": [{"text": "correlation", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.8697499632835388}, {"text": "NIST'08 EC", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9387510120868683}]}, {"text": " Table 5. Statistics of semantic relationships on words  sharing some common characters.", "labels": [], "entities": []}]}