{"title": [{"text": "Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity", "labels": [], "entities": [{"text": "Creative Language Retrieval", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6161728302637736}, {"text": "Information Retrieval", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7373903095722198}]}], "abstractContent": [{"text": "Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning.", "labels": [], "entities": [{"text": "Information retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8352829694747925}, {"text": "figurative language processing (FLP)", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.8260131080945333}]}, {"text": "IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved , focusing more on a text's potential relevance than its potential meaning.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.5394952893257141}]}, {"text": "In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways.", "labels": [], "entities": [{"text": "FLP", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6292452812194824}]}, {"text": "There is another key difference: IR is practical , scalable and robust, and in daily use by millions of casual users.", "labels": [], "entities": [{"text": "IR", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9556061625480652}]}, {"text": "FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab.", "labels": [], "entities": [{"text": "FLP", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.537652850151062}]}, {"text": "This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented.", "labels": [], "entities": []}], "introductionContent": [{"text": "Words should not always betaken at face value.", "labels": [], "entities": []}, {"text": "Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial -and perhaps literally nonsensical -reading.", "labels": [], "entities": []}, {"text": "Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see.", "labels": [], "entities": [{"text": "Figurative Language Processing (FLP)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7566718806823095}]}, {"text": "Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of and the collative semantics of) to processes of cross-domain structure alignment (e.g. structure mapping theory; see and) and even structural inversion).", "labels": [], "entities": [{"text": "cross-domain structure alignment", "start_pos": 259, "end_pos": 291, "type": "TASK", "confidence": 0.6497412820657095}, {"text": "structure mapping theory", "start_pos": 298, "end_pos": 322, "type": "TASK", "confidence": 0.7986767888069153}]}, {"text": "Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive demands of creative language: thus, some focus on interdomain mappings (e.g.) while others focus more on intra-domain inference (e.g.).", "labels": [], "entities": [{"text": "FLP", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9459167122840881}]}, {"text": "However, while computationally interesting, none has yet achieved the scalability or robustness needed to make a significant practical impact outside the laboratory.", "labels": [], "entities": []}, {"text": "Moreover, such systems tend to be developed in isolation, and are rarely designed to cohere as part of a larger framework of creative reasoning (e.g..", "labels": [], "entities": []}, {"text": "In contrast, Information Retrieval (IR) is both scalable and robust, and its results translate easily from the laboratory into practical applications (e.g. see).", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.888056755065918}]}, {"text": "Whereas FLP derives its utility and its fragility from its attempts to identify deeper meanings beneath the surface, the widespread applicability of IR stems directly from its superficial treatment of language and meaning.", "labels": [], "entities": [{"text": "IR", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9799246788024902}]}, {"text": "IR does not distinguish between creative and conventional uses of language, or between literal and non-literal meanings.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.982562780380249}]}, {"text": "IR is also remarkably modular: its components are designed to work together interchangeably, from stemmers and indexers to heuristics for query expansion and document ranking.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7574171423912048}, {"text": "query expansion", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.7445542216300964}, {"text": "document ranking", "start_pos": 158, "end_pos": 174, "type": "TASK", "confidence": 0.6879620552062988}]}, {"text": "Yet, because IR treats all language as literal language, it relies on literal matching between queries and the texts that they retrieve.", "labels": [], "entities": [{"text": "IR", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.981063961982727}]}, {"text": "Documents are retrieved precisely because they contain stretches of text that literally resemble the query.", "labels": [], "entities": []}, {"text": "This works well in the main, but it means that IR falls flat when the goal of retrieval is not to identify relevant documents but to retrieve new and creative ways of expressing a given idea.", "labels": [], "entities": [{"text": "IR", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9814634323120117}]}, {"text": "To retrieve creative language, and to be potentially surprised or inspired by the results, one needs to facilitate a non-literal relationship between queries and the texts that they match.", "labels": [], "entities": []}, {"text": "The complementarity of FLP and IR suggests a productive hybrid of both paradigms.", "labels": [], "entities": [{"text": "IR", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.6757002472877502}]}, {"text": "If the most robust elements of FLP are used to provide new non-literal query operators for IR, then IR can be used to retrieve potentially new and creative ways of speaking about a topic from a large text collection.", "labels": [], "entities": []}, {"text": "In return, IR can provide a stable, robust and extensible platform on which to use these operators to build FLP systems that exhibit linguistic creativity.", "labels": [], "entities": []}, {"text": "In the next section we consider the related work on which the current realization of these ideas is founded, before presenting a specific trio of new semantic query operators in section 3.", "labels": [], "entities": []}, {"text": "We describe three simple but practical applications of this creative IR paradigm in section 4.", "labels": [], "entities": [{"text": "IR", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9572790861129761}]}, {"text": "Empirical support for the FLP intuitions that underpin our new operators is provided in section 5.", "labels": [], "entities": []}, {"text": "The paper concludes with some closing observations about future goals and developments in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Though ^ is the most overtly categorical of our wildcards, all three wildcards -?, @ and ^ -are categorical in nature.", "labels": [], "entities": []}, {"text": "Each has a semantic or pragmatic membership function that maps a term onto an expansion set of related members.", "labels": [], "entities": []}, {"text": "The membership functions for specific uses of ^ are created in an ad-hoc fashion by the users that exploit it; in contrast, the membership functions for uses of @ and ? are derived automatically, via pattern-matching and corpus analysis.", "labels": [], "entities": []}, {"text": "Nonetheless, ad-hoc categories in creative IR are often populated with the bindings produced by uses of @ and ? and combinations thereof.", "labels": [], "entities": []}, {"text": "Ina sense, ?X and @X and their variations are themselves ad-hoc categories.", "labels": [], "entities": []}, {"text": "But how well do they serve as categories?", "labels": [], "entities": []}, {"text": "Are they large, but noisy?", "labels": [], "entities": []}, {"text": "Or too small, with limited coverage?", "labels": [], "entities": []}, {"text": "We can evaluate the effectiveness of ? and @, and indirectly that of ^ too, by comparing the use of ? and @ as category builders to a hand-crafted gold standard like WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9729431867599487}]}, {"text": "Other researchers have likewise used WordNet as a gold standard for categorization experiments, and we replicate here the experimental set-up of), which is designed to measure the effectiveness of webacquired conceptual descriptions.", "labels": [], "entities": []}, {"text": "Almuhareb and Poesio choose 214 English nouns from 13 of WordNet's upper-level semantic categories, and proceed to harvest property values for these concepts from the web using the Hearst-like pattern \"a|an|the * C is|was\".", "labels": [], "entities": []}, {"text": "This pattern yields a combined total of 51,045 values for all 214 nouns; these values are primarily adjectives, such as hot and black for coffee, but noun-modifiers of C are also allowed, such as fruit for cake.", "labels": [], "entities": []}, {"text": "They also harvest 8934 attribute nouns, such as temperature and color, using the query \"the * of the C is|was\" . These values and attributes are then used as the basis of a clustering algorithm to partition the 214 nouns back into their original 13 categories.", "labels": [], "entities": []}, {"text": "Comparing these clusters with the original WordNetbased groupings, Almuhareb and Poesio report a cluster accuracy of 71.96% using just values like hot and black, an accuracy of 64.02% using just attributes like temperature and color, and an accuracy of 85.5% using both together (a combined 59,979 features).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8639696836471558}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9969897270202637}, {"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9989413619041443}]}, {"text": "How concisely and accurately does @X describe a noun X for purposes of categorization?", "labels": [], "entities": []}, {"text": "Let ^AP denote the set of 214 WordNet nouns used by Almuhareb and Poesio.", "labels": [], "entities": []}, {"text": "Then @^AP denotes a set of 2,209 adjectival properties; this should be contrasted with the space of 51,045 adjectival values used by Almuhareb and Poesio.", "labels": [], "entities": [{"text": "AP", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.929368793964386}]}, {"text": "Using the same clustering algorithm over this feature set, @ X achieves a clustering accuracy (as measured via cluster purity) of 70.2%, compared to 71.96% for Almuhareb and Poesio.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9442924857139587}, {"text": "cluster purity)", "start_pos": 111, "end_pos": 126, "type": "METRIC", "confidence": 0.8310531179110209}]}, {"text": "However, when @ X is used to harvest a further set of attribute nouns for X, via web queries of the form \"the P * of X \" (where P \u2208 @X), then @ X augmented with this additional set of attributes (like hands for surgeon) produces a larger space of 7,183 features.", "labels": [], "entities": []}, {"text": "This in turn yields a cluster accuracy of 90.2% which contrasts with Almuhareb and Poesio's 85.5% for 59,979 features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.7491276264190674}]}, {"text": "In either case, @X produces comparable clustering quality to Almuhareb and Poesio, with just a small fraction of the features.", "labels": [], "entities": []}, {"text": "So how concisely and accurately does ?X describe a noun X for purposes of categorization?", "labels": [], "entities": []}, {"text": "While @X denotes a set of salient adjectives, ?X denotes a set of comparable nouns.", "labels": [], "entities": []}, {"text": "So this time, ?^AP denotes a set of 8,300 nouns in total, to act as a feature space for the 214 nouns of Almuhareb and Poesio.", "labels": [], "entities": [{"text": "AP", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9808152914047241}]}, {"text": "Remember, the contents of each ?X, and of ?^AP overall, are determined entirely by the contents of the Google 3-grams; the elements of ?X are not ranked in anyway, and all are treated as equals.", "labels": [], "entities": []}, {"text": "When the 8,300 features in ?^AP are clustered into 13 categories, the resulting clusters have a purity of 93.4% relative to WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9743012189865112}]}, {"text": "The pragmatic neighborhood of X, ?X, appears to bean accurate and concise proxy for the meaning of X.", "labels": [], "entities": []}, {"text": "Almuhareb and Poesio's set of 214 words does not contain adjectives, and besides, WordNet does not impose a category structure on its adjectives.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9223209619522095}]}, {"text": "In any case, the role of adjectives in the applications of section 4 is largely an affective one: if X is a noun, then one must have confidence that the adjectives in @X are consonant with our understanding of X, and if P is a property, that the adjectives in ?P evoke much the same mood and sentiment as P.", "labels": [], "entities": []}, {"text": "Our evaluation of @X and ?P should thus bean affective one.", "labels": [], "entities": []}, {"text": "So how well do the properties in @X capture our sentiments about a noun X?", "labels": [], "entities": []}, {"text": "Well enough to estimate the pleasantness of X from the adjectives in @ X, perhaps?", "labels": [], "entities": []}, {"text": "dictionary of affect provides pleasantness ratings fora sizeable number of adjectives and nouns (over 8,000 words in total), allowing us to estimate the pleasantness of X as a weighted average of the pleasantness of each X i in @X (the weights here are web frequencies for the similes that underpin @ in section 3.2).", "labels": [], "entities": []}, {"text": "We thus estimate the affect of all stereotype nouns for which Whissell also records a score.", "labels": [], "entities": []}, {"text": "A twotailed Pearson test (p < 0.05) shows a positive correlation of 0.5 between these estimates and the pleasantness scores assigned by Whissell.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.7612342238426208}]}, {"text": "In contrast, estimates based on the pleasantness of adjectives found in corresponding WordNet glosses show a positive correlation of just 0.278.", "labels": [], "entities": [{"text": "WordNet glosses", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9621134698390961}]}, {"text": "How well do the elements of ?P capture our sentiments toward an adjective P?", "labels": [], "entities": []}, {"text": "After all, we hypothesize that the adjectives in ?P are highly suggestive of P, and vice versa.", "labels": [], "entities": []}, {"text": "Aristotle and the Jigsaw Bard each rely on ?P to suggest adjectives that evoke an unstated property in a metaphor or simile, or to suggest coherent blends of properties.", "labels": [], "entities": [{"text": "Jigsaw Bard", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.9217565357685089}]}, {"text": "When we estimate the pleasantness of each adjective P in Whissell's dictionary via the weighted mean of the pleasantness of adjectives in ?P (again using web frequencies as weights), a two-tailed Pearson test (p < 0.05) shows a correlation of 0.7 between estimates and actual scores.", "labels": [], "entities": [{"text": "Whissell's dictionary", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.6959814329942068}]}, {"text": "It seems ?P does a rather good job of capturing the feel of P.", "labels": [], "entities": []}], "tableCaptions": []}