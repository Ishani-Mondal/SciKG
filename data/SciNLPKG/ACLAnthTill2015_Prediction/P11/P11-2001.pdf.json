{"title": [{"text": "Lexicographic Semirings for Exact Automata Encoding of Sequence Models", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks.", "labels": [], "entities": []}, {"text": "We prove that the semiring allows for exact encoding of backoff models with epsilon transitions.", "labels": [], "entities": []}, {"text": "This allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations.", "labels": [], "entities": []}, {"text": "We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions , the use of the more powerful lexico-graphic semiring is competitive in terms of time of intersection.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "T, T encoded language models For our experiments we used lattices derived from a very large vocabulary continuous speech recognition system, which was built for the 2007 GALE Arabic speech recognition task, and used in the work reported in.", "labels": [], "entities": [{"text": "GALE Arabic speech recognition task", "start_pos": 170, "end_pos": 205, "type": "TASK", "confidence": 0.6932310700416565}]}, {"text": "The lexicographic semiring was evaluated on the development set (2.6 hours of broadcast news and conversations; 18K words).", "labels": [], "entities": []}, {"text": "The 888 word lattices for the development set were generated using a competitive baseline system with acoustic models trained on about 1000 hrs of Arabic broadcast data and a 4-gram language model.", "labels": [], "entities": []}, {"text": "The language model consisting of 122M n-grams was estimated by interpolation of 14 components.", "labels": [], "entities": []}, {"text": "The vocabulary is relatively large at 737K and the associated dictionary has only single pronunciations.", "labels": [], "entities": [{"text": "737K", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.7434205412864685}]}, {"text": "The language model was converted to the automaton topology described earlier, and represented in three ways: first as an approximation of a failure machine using epsilons instead of failure arcs; second as a correct failure machine; and third using the lexicographic construction derived in this paper.", "labels": [], "entities": []}, {"text": "The three versions of the LM were evaluated by intersecting them with the 888 lattices of the development set.", "labels": [], "entities": []}, {"text": "The overall error rate for the systems was 24.8%-comparable to the state-of-theart on this task 1 . For the shortest paths, the failure and lexicographic machines always produced identical lattices (as determined by FST equivalence); in contrast, 81% of the shortest paths from the epsilon approximation are different, at least in terms of weights, from the shortest paths using the failure LM.", "labels": [], "entities": [{"text": "error rate", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9492314159870148}]}, {"text": "For full lattices, 42 (4.7%) of the lexicographic outputs differ from the failure LM outputs, due to small floating point rounding issues; 863 (97%) of the epsilon approximation outputs differ.", "labels": [], "entities": []}, {"text": "In terms of size, the failure LM, with 5.7 million arcs requires 97 Mb.", "labels": [], "entities": []}, {"text": "The equivalent T, Tlexicographic LM requires 120 Mb, due to the doubling of the size of the weights.", "labels": [], "entities": [{"text": "Tlexicographic LM", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.6622911095619202}]}, {"text": "To measure speed, we performed the intersections 1000 times for each of our 888 lattices on a 2993 MHz Intel R Xeon R CPU, and took the mean times for each of our methods.", "labels": [], "entities": []}, {"text": "The 888 lattices were processed with a mean of 1.62 seconds in total (1.8 msec per lattice) using the failure LM; using the T, T -lexicographic LM required 1.8 seconds (2.0 msec per lattice), and is thus about 11% slower.", "labels": [], "entities": []}, {"text": "Epsilon approximation, where the failure arcs are approximated with epsilon arcs took 1.17 seconds (1.3 msec per lattice).", "labels": [], "entities": []}, {"text": "The slightly slower speeds for the exact method using the failure LM, and T, T can be related to the overhead of computing the failure function at runtime, and determinization, respectively.", "labels": [], "entities": []}], "tableCaptions": []}