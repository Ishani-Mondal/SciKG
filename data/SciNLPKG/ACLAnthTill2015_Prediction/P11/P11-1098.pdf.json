{"title": [{"text": "Template-Based Information Extraction without the Templates", "labels": [], "entities": [{"text": "Template-Based Information Extraction", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7686349153518677}]}], "abstractContent": [{"text": "Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template).", "labels": [], "entities": [{"text": "template-based information extraction (IE)", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.8331664105256399}]}, {"text": "This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance.", "labels": [], "entities": [{"text": "IE", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.8176606297492981}]}, {"text": "Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles.", "labels": [], "entities": []}, {"text": "We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents.", "labels": [], "entities": [{"text": "IE task", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9207489788532257}]}, {"text": "We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to hand-created gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.", "labels": [], "entities": [{"text": "MUC-4 terrorism dataset", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.9677834113438925}, {"text": "F1 score", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9845420718193054}]}], "introductionContent": [{"text": "A template defines a specific type of event (e.g., a bombing) with a set of semantic roles (or slots) for the typical entities involved in such an event (e.g., perpetrator, target, instrument).", "labels": [], "entities": []}, {"text": "In contrast to work in relation discovery that focuses on learning atomic facts (, templates can extract a richer representation of a particular domain.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8911887109279633}]}, {"text": "However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain's templates.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8918059170246124}, {"text": "IE", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.8986526727676392}]}, {"text": "Very little work addresses how to learn the template structure itself.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus.", "labels": [], "entities": [{"text": "template filling", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7738896906375885}]}, {"text": "There are many ways to represent events, ranging from role-based representations such as frames () to sequential events in scripts ( and narrative schemas.", "labels": [], "entities": []}, {"text": "Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blowup, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template's event words (e.g., the objects of detonate and explode).", "labels": [], "entities": [{"text": "Bombing Template {detonate, blowup, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template's event words (e.g., the objects of detonate and explode)", "start_pos": 174, "end_pos": 538, "type": "Description", "confidence": 0.813247542778651}]}, {"text": "Our goal is to characterize a domain by learning this template structure completely automatically.", "labels": [], "entities": []}, {"text": "We learn templates by first clustering event words based on their proximity in a training corpus.", "labels": [], "entities": []}, {"text": "We then use a novel approach to role induction that clusters the syntactic functions of these events based on selectional preferences and coreferring arguments.", "labels": [], "entities": [{"text": "role induction", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8378351926803589}]}, {"text": "The induced roles are template-specific (e.g., perpetrator), not universal (e.g., agent or patient) or verb-specific.", "labels": [], "entities": []}, {"text": "After learning a domain's template schemas, we perform the standard IE task of role filling from individual documents, for example: Perpetrator: guerrillas Instrument: dynamite Target: embassy 976 This extraction stage identifies entities using the learned syntactic functions of our roles.", "labels": [], "entities": [{"text": "IE", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9660958051681519}, {"text": "role filling", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.7172082662582397}]}, {"text": "We evaluate on the MUC-4 terrorism corpus with results approaching those of supervised systems.", "labels": [], "entities": [{"text": "MUC-4 terrorism corpus", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.8951257069905599}]}, {"text": "The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure.", "labels": [], "entities": []}, {"text": "We describe how to first expand the small corpus' size, how to cluster its events, and finally how to induce semantic roles.", "labels": [], "entities": []}, {"text": "Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now compare our learned templates to those hand-created by human annotators for the MUC-4 terrorism corpus.", "labels": [], "entities": [{"text": "MUC-4 terrorism corpus", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.9308958848317465}]}, {"text": "The corpus contains 6 template types, but two of them occur in only 4 and 14 of the 1300 training documents.", "labels": [], "entities": []}, {"text": "We thus only evaluate the 4 main templates (bombing, kidnapping, attack, and arson).", "labels": [], "entities": []}, {"text": "The gold slots are shown in.", "labels": [], "entities": []}, {"text": "We evaluate the four learned templates that score highest in the document classification evaluation (to be described in section 5.1), aligned with their MUC-4 types.", "labels": [], "entities": [{"text": "document classification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7371634542942047}]}, {"text": "shows three of our four templates, and two brand new ones that our algorithm learned.", "labels": [], "entities": []}, {"text": "Of the four templates, we learned 12 of the 13 semantic roles as created for MUC.", "labels": [], "entities": [{"text": "MUC", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.6834161281585693}]}, {"text": "In addition, we learned anew role not in MUC for bombings, kidnappings, and arson: the Police or Authorities role.", "labels": [], "entities": [{"text": "MUC for bombings, kidnappings", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6142354786396027}]}, {"text": "The annotators chose not to include this in their labeling, but this knowledge is clearly relevant when understanding such events, so we consider it correct.", "labels": [], "entities": []}, {"text": "There is one additional Bombing and one Arson role that does not align with MUC-4, marked incorrect.", "labels": [], "entities": [{"text": "Bombing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.7410423159599304}, {"text": "Arson", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.7361751794815063}, {"text": "MUC-4", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9457924962043762}]}, {"text": "981 We thus report 92% slot recall, and precision as 14 of 16 (88%) learned slots.", "labels": [], "entities": [{"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9691187143325806}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9997425675392151}]}, {"text": "We only measure agreement with the MUC template schemas, but our system learns other events as well.", "labels": [], "entities": [{"text": "MUC template schemas", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.6632240215937296}]}, {"text": "We show two such examples in: the Weapons Smuggling and Election Templates.", "labels": [], "entities": [{"text": "Weapons Smuggling", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7347085475921631}, {"text": "Election Templates", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7289937734603882}]}, {"text": "The MUC-4 corpus links templates to documents, allowing us to evaluate our document labels.", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8916054666042328}]}, {"text": "We treat each link as a gold label (kidnap, bomb, or attack) for that document, and documents can have multiple labels.", "labels": [], "entities": []}, {"text": "Our learned clusters naturally do not have MUC labels, so we report results on the four clusters that score highest with each label.", "labels": [], "entities": [{"text": "MUC", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.7740610241889954}]}, {"text": "shows the document classification scores.", "labels": [], "entities": [{"text": "document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6200034916400909}]}, {"text": "The bombing template performs best with an F1 score of .72.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9886404275894165}]}, {"text": "Arson occurs very few times, and Attack is lower because it is essentially an agglomeration of diverse events (discussed later).", "labels": [], "entities": [{"text": "Arson", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7537682056427002}, {"text": "Attack", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9953047037124634}]}, {"text": "We trained on the 1300 documents in the MUC-4 corpus and tested on the 200 document TST3 and TST4 test set.", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9564297199249268}, {"text": "TST4 test set", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.8590898116429647}]}, {"text": "We evaluate the four string-based slots: perpetrator, physical target, human target, and instrument.", "labels": [], "entities": []}, {"text": "We merge MUC's two perpetrator slots (individuals and orgs) into one gold Perpetrator slot.", "labels": [], "entities": [{"text": "MUC", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8055872321128845}]}, {"text": "As in, we ignore missed optional slots in computing recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9318512678146362}]}, {"text": "We induced clusters in training, performed IR, and induced the slots.", "labels": [], "entities": [{"text": "IR", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.968663215637207}]}, {"text": "We then extracted entities from the test documents as described in section 5.2.", "labels": [], "entities": []}, {"text": "The standard evaluation for this corpus is to report the F1 score for slot type accuracy, ignoring the template type.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9887945652008057}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9732715487480164}]}, {"text": "For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same.", "labels": [], "entities": []}, {"text": "This allows supervised classifiers to train on all perpetrators at once, rather than template-specific learners.", "labels": [], "entities": []}, {"text": "Although not ideal for our learning goals, we report it for comparison against previous work.", "labels": [], "entities": []}, {"text": "Several supervised approaches have presented results on MUC-4, but unfortunately we cannot compare against them.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 56, "end_pos": 61, "type": "TASK", "confidence": 0.7210307121276855}]}, {"text": "evaluated a random subset of test (they report .60 and .63 F1), and did not evaluate all slot types (they report .57 F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.993417501449585}]}, {"text": "thus shows our results with previous work that is comparable: the fully supervised and  weakly supervised approaches of.", "labels": [], "entities": []}, {"text": "We give two numbers for our system: mapping one learned template to Attack, and mapping five.", "labels": [], "entities": []}, {"text": "Our learned templates for Attack have a different granularity than MUC-4.", "labels": [], "entities": [{"text": "Attack", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.985956072807312}, {"text": "MUC-4", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.8418110013008118}]}, {"text": "Rather than one broad Attack type, we learn several: Shooting, Murder, Coup, General Injury, and Pipeline Attack.", "labels": [], "entities": [{"text": "Shooting", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.9555455446243286}, {"text": "General Injury", "start_pos": 77, "end_pos": 91, "type": "METRIC", "confidence": 0.8302853405475616}, {"text": "Pipeline Attack", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.9141799509525299}]}, {"text": "We see these subtypes as strengths of our algorithm, but it misses the MUC-4 granularity of Attack.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.5760552287101746}]}, {"text": "We thus show results when we apply the best five learned templates to Attack, rather than just one.", "labels": [], "entities": []}, {"text": "The final F1 with these Attack subtypes is .40.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.972979724407196}]}, {"text": "Our precision is as good as (and our F1 score near) two algorithms that require knowledge of the templates and/or labeled data.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996387958526611}, {"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9883264005184174}]}, {"text": "Our algorithm instead learned this knowledge without such supervision.", "labels": [], "entities": []}, {"text": "In order to more precisely evaluate each learned template, we also evaluated per-template performance.", "labels": [], "entities": []}, {"text": "Instead of merging all slots across all template types, we score the slots within each template type.", "labels": [], "entities": []}, {"text": "This is a stricter evaluation than Section 6; for example, bombing victims assigned to attacks were previously deemed correct . gives our results.", "labels": [], "entities": []}, {"text": "Three of the four templates score at or above .42 F1, showing that our lower score from the previous section is mainly due to the Attack template.", "labels": [], "entities": [{"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9974107146263123}]}, {"text": "Arson also unexpectedly These evaluations use the standard TST3 and TST4 test sets, including the documents that are not labeled with any templates.", "labels": [], "entities": [{"text": "Arson", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9208762645721436}, {"text": "TST3", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8932466506958008}, {"text": "TST4 test sets", "start_pos": 68, "end_pos": 82, "type": "DATASET", "confidence": 0.8824736674626669}]}, {"text": "74 of the 200 test documents are unlabeled.", "labels": [], "entities": []}, {"text": "In order to determine where the system's false positives originate, we also measure performance only on the 126 test documents that have at least one template.", "labels": [], "entities": []}, {"text": "presents the results on this subset.", "labels": [], "entities": []}, {"text": "Kidnap improves most significantly in F1 score (7 F1 points absolute), but the others only change slightly.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9371992647647858}, {"text": "F1 points absolute)", "start_pos": 50, "end_pos": 69, "type": "METRIC", "confidence": 0.9546702206134796}]}, {"text": "Most of the false positives in the system thus do not originate from the unlabeled documents (the 74 unlabeled), but rather from extracting incorrect entities from correctly identified documents (the 126 labeled).", "labels": [], "entities": []}], "tableCaptions": []}