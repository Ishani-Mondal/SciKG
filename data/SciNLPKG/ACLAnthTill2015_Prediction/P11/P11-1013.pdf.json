{"title": [{"text": "Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification", "labels": [], "entities": [{"text": "Cross-Domain Sentiment Classification", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.7444238861401876}]}], "abstractContent": [{"text": "Joint sentiment-topic (JST) model was previously proposed to detect sentiment and topic simultaneously from text.", "labels": [], "entities": []}, {"text": "The only supervision required by JST model learning is domain-independent polarity word priors.", "labels": [], "entities": [{"text": "JST model learning", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.922100841999054}]}, {"text": "In this paper, we modify the JST model by incorporating word polarity priors through modifying the topic-word Dirichlet priors.", "labels": [], "entities": [{"text": "JST", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9228244423866272}]}, {"text": "We study the polarity-bearing topics extracted by JST and show that by augmenting the original feature space with polarity-bearing topics, the in-domain supervised classifiers learned from augmented feature representation achieve the state-of-the-art performance of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset.", "labels": [], "entities": []}, {"text": "Furthermore , using feature augmentation and selection according to the information gain criteria for cross-domain sentiment classification, our proposed approach performs either better or comparably compared to previous approaches.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.7410577734311422}]}, {"text": "Nevertheless, our approach is much simpler and does not require difficult parameter tuning .", "labels": [], "entities": []}], "introductionContent": [{"text": "Given apiece of text, sentiment classification aims to determine whether the semantic orientation of the text is positive, negative or neutral.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.9504069685935974}]}, {"text": "Machine learning approaches to this problem (?; ?; ?; ?; ?; ?) typically assume that classification models are trained and tested using data drawn from some fixed distribution.", "labels": [], "entities": []}, {"text": "However, in many practical cases, we may have plentiful labeled examples in the source domain, but very few or no labeled examples in the target domain with a different distribution.", "labels": [], "entities": []}, {"text": "For example, we may have many labeled books reviews, but we are interested in detecting the polarity of electronics reviews.", "labels": [], "entities": []}, {"text": "Reviews for different produces might have widely different vocabularies, thus classifiers trained on one domain often fail to produce satisfactory results when shifting to another domain.", "labels": [], "entities": []}, {"text": "This has motivated much research on sentiment transfer learning which transfers knowledge from a source task or domain to a different but related task or domain.", "labels": [], "entities": [{"text": "sentiment transfer learning", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.9104861815770467}]}, {"text": "Joint sentiment-topic (JST) model (?; ?) was extended from the latent Dirichlet allocation (LDA) model (?) to detect sentiment and topic simultaneously from text.", "labels": [], "entities": []}, {"text": "The only supervision required by JST learning is domain-independent polarity word prior information.", "labels": [], "entities": [{"text": "JST learning", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9404651522636414}]}, {"text": "With prior polarity words extracted from both the MPQA subjectivity lexicon 1 and the appraisal lexicon 2 , the JST model achieves a sentiment classification accuracy of 74% on the movie review data and 71% on the multi-domain sentiment dataset . Moreover, it is also able to extract coherent and informative topics grouped under different sentiment.", "labels": [], "entities": [{"text": "MPQA subjectivity lexicon", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.8600395520528158}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.8517945408821106}, {"text": "movie review data", "start_pos": 181, "end_pos": 198, "type": "DATASET", "confidence": 0.5825123985608419}]}, {"text": "The fact that the JST model does not required any labeled documents for training makes it desirable for domain adaptation in sentiment classification.", "labels": [], "entities": [{"text": "JST", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.7689502835273743}, {"text": "domain adaptation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7157443314790726}, {"text": "sentiment classification", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.9593982100486755}]}, {"text": "Many existing approaches solve the sentiment transfer problem by associating words from different domains which indicate the same sentiment.", "labels": [], "entities": [{"text": "sentiment transfer", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9101015329360962}]}, {"text": "Such an association mapping problem can be naturally solved by the posterior inference in the JST model.", "labels": [], "entities": [{"text": "association mapping", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8110243678092957}, {"text": "JST model", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.8536959588527679}]}, {"text": "Indeed, the polarity-bearing topics extracted by JST essentially capture sentiment associations among words from different domains which effectively overcome the data distribution difference between source and target domains.", "labels": [], "entities": [{"text": "JST", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.8591455817222595}]}, {"text": "The previously proposed JST model uses the sentiment prior information in the Gibbs sampling inference step that a sentiment label will only be sampled if the current word token has no prior sentiment as defined in a sentiment lexicon.", "labels": [], "entities": []}, {"text": "This in fact implies a different generative process where many of the word prior sentiment labels are observed.", "labels": [], "entities": []}, {"text": "The model is no longer \"latent\".", "labels": [], "entities": []}, {"text": "We propose an alternative approach by incorporating word prior polarity information through modifying the topic-word Dirichlet priors.", "labels": [], "entities": []}, {"text": "This essentially creates an informed prior distribution for the sentiment labels and would allow the model to actually be latent and would be consistent with the generative story.", "labels": [], "entities": []}, {"text": "We study the polarity-bearing topics extracted by the JST model and show that by augmenting the original feature space with polarity-bearing topics, the performance of in-domain supervised classifiers learned from augmented feature representation improves substantially, reaching the state-of-the-art results of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset.", "labels": [], "entities": [{"text": "movie review data", "start_pos": 323, "end_pos": 340, "type": "DATASET", "confidence": 0.6012984216213226}, {"text": "multi-domain sentiment dataset", "start_pos": 370, "end_pos": 400, "type": "DATASET", "confidence": 0.6175733506679535}]}, {"text": "Furthermore, using simple feature augmentation, our proposed approach outperforms the structural correspondence learning (SCL) (?) algorithm and achieves comparable results to the recently proposed spectral feature alignment (SFA) method (?).", "labels": [], "entities": [{"text": "structural correspondence learning (SCL)", "start_pos": 86, "end_pos": 126, "type": "TASK", "confidence": 0.6638919313748678}, {"text": "spectral feature alignment (SFA)", "start_pos": 198, "end_pos": 230, "type": "TASK", "confidence": 0.8081923623879751}]}, {"text": "Nevertheless, our approach is much simpler and does not require difficult parameter tuning.", "labels": [], "entities": []}, {"text": "We proceed with a review of related work on sentiment domain adaptation.", "labels": [], "entities": [{"text": "sentiment domain adaptation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8046090801556905}]}, {"text": "We then briefly describe the JST model and present another approach to incorporate word prior polarity information into JST learning.", "labels": [], "entities": [{"text": "JST learning", "start_pos": 120, "end_pos": 132, "type": "TASK", "confidence": 0.9242942631244659}]}, {"text": "We subsequently show that words from different domains can indeed be grouped under the same polarity-bearing topic through an illustration of example topic words extracted by JST before proposing a domain adaptation approach based on JST.", "labels": [], "entities": [{"text": "JST", "start_pos": 175, "end_pos": 178, "type": "DATASET", "confidence": 0.8829230666160583}, {"text": "JST", "start_pos": 234, "end_pos": 237, "type": "DATASET", "confidence": 0.9007044434547424}]}, {"text": "We verify our proposed approach by conducting experiments on both the movie review data and the multi-domain sentiment dataset.", "labels": [], "entities": [{"text": "movie review data", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.6441213190555573}, {"text": "multi-domain sentiment dataset", "start_pos": 96, "end_pos": 126, "type": "DATASET", "confidence": 0.7007221082846323}]}, {"text": "Finally, we conclude our work and outline future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our proposed approach on the two datasets, the movie review (MR) data and the multidomain sentiment (MDS) dataset.", "labels": [], "entities": [{"text": "movie review (MR) data", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.46998515725135803}]}, {"text": "The movie review data consist of 1000 positive and 1000 negative movie reviews drawn from the IMDB movie archive while the multi-domain sentiment dataset contains four different types of product reviews extracted from Amazon.com including Book, DVD, Electronics and Kitchen appliances.", "labels": [], "entities": [{"text": "IMDB movie archive", "start_pos": 94, "end_pos": 112, "type": "DATASET", "confidence": 0.9282985130945841}, {"text": "Book", "start_pos": 239, "end_pos": 243, "type": "DATASET", "confidence": 0.978065550327301}]}, {"text": "Each category Algorithm 2 Adding pseudo-labeled documents.", "labels": [], "entities": []}, {"text": "Input: The target domain data, D t = {x tn \u2208 X : 1 \u2264 n \u2264 N t , N t N s }, document sentiment classification threshold \u03c4 Output: A labeled document pool B 1: Train a JST model parameterized by \u039b on D t 2: for each document x tn \u2208 D t do 3: Infer its sentiment class label from JST as l n = arg max s P (l|x tn ; \u039b)  While the original JST model can produce reasonable results with a simple symmetric Dirichlet prior, here we use asymmetric prior \u03b1 over the topic proportions which is learned directly from data using a fixed-point iteration method (?).", "labels": [], "entities": [{"text": "document sentiment classification", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.6763161619504293}]}, {"text": "In our experiment, \u03b1 was updated every 25 iterations during the Gibbs sampling procedure.", "labels": [], "entities": []}, {"text": "In terms of other priors, we set symmetric prior \u03b2 = 0.01 and \u03b3 = (0.05\u00d7L)/S, where L is the average document length, and the value of 0.05 on average allocates 5% of probability mass for mixing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Supervised sentiment classification accuracy.", "labels": [], "entities": [{"text": "Supervised sentiment classification", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7517274220784506}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.977336049079895}]}, {"text": " Table 3: Adaptation loss with respect to the in-domain  gold standard. The last row shows the average loss over  all the four domains.", "labels": [], "entities": [{"text": "Adaptation loss", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7855063378810883}]}]}