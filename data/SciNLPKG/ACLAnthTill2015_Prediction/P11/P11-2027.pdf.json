{"title": [{"text": "AM-FM: A Semantic Framework for Translation Quality Assessment", "labels": [], "entities": [{"text": "AM-FM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8194478154182434}, {"text": "Translation Quality Assessment", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.9163902997970581}]}], "abstractContent": [{"text": "This work introduces AM-FM, a semantic framework for machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.8815722862879435}]}, {"text": "Based upon this framework, anew evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated.", "labels": [], "entities": []}, {"text": "The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively.", "labels": [], "entities": [{"text": "cross-language latent semantic indexing", "start_pos": 105, "end_pos": 144, "type": "TASK", "confidence": 0.598506972193718}]}, {"text": "Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages.", "labels": [], "entities": []}, {"text": "Finally, the main pros and cons of the proposed framework are discussed along with future research directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation has always been one of the major issues in Machine Translation research, as both human and automatic evaluation methods exhibit very important limitations.", "labels": [], "entities": [{"text": "Machine Translation research", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.9266326030095419}]}, {"text": "On the one hand, although highly reliable, in addition to being expensive and time consuming, human evaluation suffers from inconsistency problems due to inter-and intraannotator agreement issues.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.5854068696498871}]}, {"text": "On the other hand, while being consistent, fast and cheap, automatic evaluation has the major disadvantage of requiring reference translations.", "labels": [], "entities": []}, {"text": "This makes automatic evaluation not reliable in the sense that good translations not matching the available references are evaluated as poor or bad translations.", "labels": [], "entities": []}, {"text": "The main objective of this work is to propose and evaluate AM-FM, a semantic framework for assessing translation quality without the need for reference translations.", "labels": [], "entities": []}, {"text": "The proposed framework is theoretically grounded on the classical concepts of adequacy and fluency, and it is designed to account for these two components of translation quality in an independent manner.", "labels": [], "entities": []}, {"text": "First, a cross-language latent semantic indexing model is used for assessing the adequacy component by directly comparing the output translation with the input sentence it was generated from.", "labels": [], "entities": [{"text": "cross-language latent semantic indexing", "start_pos": 9, "end_pos": 48, "type": "TASK", "confidence": 0.6118930354714394}]}, {"text": "Second, an n-gram based language model of the target language is used for assessing the fluency component.", "labels": [], "entities": []}, {"text": "Both components of the metric are evaluated at the sentence level, providing the means for defining and implementing a sentence-based evaluation metric.", "labels": [], "entities": []}, {"text": "Finally, the two components are combined into a single measure by implementing a weighted harmonic mean, for which the weighting factor can be adjusted for optimizing the metric performance.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2, presents some background work and the specific dataset that has been used in the experimental work.", "labels": [], "entities": []}, {"text": "Section 3, provides details on the proposed AM-FM framework and the specific metric implementation.", "labels": [], "entities": []}, {"text": "Section 4 presents the results of the conducted comparative evaluations.", "labels": [], "entities": []}, {"text": "Finally, section 5 presents the main conclusions and relevant issues to be dealt within future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although BLEU) has become a de facto standard for machine translation evaluation, other metrics such as NIST) and, more recently,), are commonly used too.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9966806769371033}, {"text": "machine translation evaluation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.8891970117886862}, {"text": "NIST", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.8039026260375977}]}, {"text": "Regarding the specific idea of evaluating machine translation without using reference translations, several works have proposed and evaluated different approaches, including round-trip translation, as well as other regression-and classification-based approaches.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7149959653615952}, {"text": "round-trip translation", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.6972191482782364}]}, {"text": "As part of the recent efforts on machine translation evaluation, two workshops have been organizing shared-tasks and evaluation campaigns over the last four years: the NIST Metrics for Machine Translation Challenge 1 (MetricsMATR) and the Workshop on Statistical Machine Translation 2 (WMT); which were actually held as one single event in their most recent edition in 2010.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8791486024856567}, {"text": "NIST Metrics for Machine Translation Challenge", "start_pos": 168, "end_pos": 214, "type": "TASK", "confidence": 0.7039441466331482}, {"text": "Statistical Machine Translation 2 (WMT)", "start_pos": 251, "end_pos": 290, "type": "TASK", "confidence": 0.7850647228104728}]}, {"text": "The dataset used in this work corresponds to WMT-07.", "labels": [], "entities": [{"text": "WMT-07", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9729975461959839}]}, {"text": "This dataset is used, instead of a more recent one, because no human judgments on adequacy and fluency have been conducted in WMT after year 2007, and human evaluation data is not freely available from MetricsMATR.", "labels": [], "entities": [{"text": "WMT", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.6862227916717529}, {"text": "MetricsMATR", "start_pos": 202, "end_pos": 213, "type": "DATASET", "confidence": 0.9148561358451843}]}, {"text": "In this dataset, translation outputs are available for fourteen tasks involving five European languages: English (EN), Spanish (ES), German (DE), French (FR) and Czech (CZ); and two domains: News Commentaries (News) and European Parliament Debates (EPPS).", "labels": [], "entities": []}, {"text": "A complete description on WMT-07 evaluation campaign and dataset is available in.", "labels": [], "entities": [{"text": "WMT-07 evaluation campaign and dataset", "start_pos": 26, "end_pos": 64, "type": "DATASET", "confidence": 0.7873535037040711}]}, {"text": "System outputs for fourteen of the fifteen systems that participated in the evaluation are available.", "labels": [], "entities": []}, {"text": "This accounts for 86 independent system outputs with a total of 172,315 individual sentence translations, from which only 10,754 were rated for both adequacy and fluency by human judges.", "labels": [], "entities": []}, {"text": "The specific vote standardization procedure described in section 5.4 of was applied to all adequacy and fluency scores for removing individual voting patterns and averaging votes.: Domain, source language, target language, system outputs and total amount of sentence translations (with both adequacy and fluency human assessments) included in the WMT-07 dataset  The framework proposed in this work (AM-FM) aims at assessing translation quality without the need for reference translations, while maintaining consistency with human quality assessments.", "labels": [], "entities": [{"text": "WMT-07 dataset", "start_pos": 347, "end_pos": 361, "type": "DATASET", "confidence": 0.9839003384113312}]}, {"text": "Different from other approaches not using reference translations, we rely on a cross-language version of latent semantic indexing () for creating a semantic space where translation outputs and inputs can be directly compared.", "labels": [], "entities": []}, {"text": "A two-component evaluation metric, based on the concepts of adequacy and fluency) is defined.", "labels": [], "entities": []}, {"text": "While adequacy accounts for the amount of source meaning being preserved by the translation (5:all, 4:most, 3:much, 2:little, 1:none), fluency accounts for the quality of the target language in the translation (5:flawless, 4:good, 3:nonnative, 2:disfluent, 1:incomprehensible).", "labels": [], "entities": []}, {"text": "In order to evaluate the AM-FM framework, two comparative evaluations with standard metrics were conducted.", "labels": [], "entities": []}, {"text": "More specifically, BLEU, NIST and Meteor were considered, as they are the metrics most frequently used in machine translation evaluation campaigns.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9944697618484497}, {"text": "NIST", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8611686825752258}, {"text": "Meteor", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8541862964630127}, {"text": "machine translation evaluation", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.8695725997289022}]}], "tableCaptions": [{"text": " Table 2: Pearson's correlation coefficients (com- puted at the system level) between automatic met- rics and human-generated scores", "labels": [], "entities": [{"text": "correlation", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.585821270942688}]}, {"text": " Table 3: Pearson's correlation coefficients (com- puted at the sentence level) between automatic  metrics and human-generated scores", "labels": [], "entities": []}, {"text": " Table 4: Percentage of cases in which each auto- matic metric is able to predict the best, the worst,  and both ranked sentence translations", "labels": [], "entities": []}]}