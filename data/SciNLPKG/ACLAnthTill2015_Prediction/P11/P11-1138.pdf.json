{"title": [{"text": "Local and Global Algorithms for Disambiguation to Wikipedia", "labels": [], "entities": []}], "abstractContent": [{"text": "Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing.", "labels": [], "entities": [{"text": "Disambiguating concepts and entities", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.856788158416748}, {"text": "natural language processing", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.6478584011395773}]}, {"text": "The compre-hensiveness of Wikipedia has made the on-line encyclopedia an increasingly popular target for disambiguation.", "labels": [], "entities": []}, {"text": "Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambigua-tions are compatible.", "labels": [], "entities": [{"text": "Word Sense Disambiguation task", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.7230746299028397}]}, {"text": "In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations fora given document (which we call \"global\" approaches), and compare them to more traditional (local) approaches.", "labels": [], "entities": []}, {"text": "We show that previous approaches for global disambiguation can be improved, but even then the local disam-biguation provides a baseline which is very hard to beat.", "labels": [], "entities": [{"text": "global disambiguation", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.55536749958992}]}], "introductionContent": [{"text": "Wikification is the task of identifying and linking expressions in text to their referent Wikipedia pages.", "labels": [], "entities": [{"text": "Wikification is the task of identifying and linking expressions in text to their referent Wikipedia pages", "start_pos": 0, "end_pos": 105, "type": "Description", "confidence": 0.743312219157815}]}, {"text": "Recently, Wikification has been shown to form a valuable component for numerous natural language processing tasks including text classification (, measuring semantic similarity between texts (, crossdocument co-reference resolution ( , and other tasks ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.8257391452789307}, {"text": "crossdocument co-reference resolution", "start_pos": 194, "end_pos": 231, "type": "TASK", "confidence": 0.6605684459209442}]}, {"text": "Previous studies on Wikification differ with respect to the corpora they address and the subset of expressions they attempt to link.", "labels": [], "entities": []}, {"text": "For example, some studies focus on linking only named entities, whereas others attempt to link all \"interesting\" expressions, mimicking the link structure found in Wikipedia.", "labels": [], "entities": []}, {"text": "Regardless, all Wikification systems are faced with a key Disambiguation to Wikipedia (D2W) task.", "labels": [], "entities": []}, {"text": "In the D2W task, we're given a text along with explicitly identified substrings (called mentions) to disambiguate, and the goal is to output the corresponding Wikipedia page, if any, for each mention.", "labels": [], "entities": []}, {"text": "For example, given the input sentence \"I am visiting friends in <Chicago>,\" we output http://en.wikipedia.org/wiki/Chicago -the Wikipedia page for the city of Chicago, Illinois, and not (for example) the page for the 2002 film of the same name.", "labels": [], "entities": []}, {"text": "Local D2W approaches disambiguate each mention in a document separately, utilizing clues such as the textual similarity between the document and each candidate disambiguation's Wikipedia page.", "labels": [], "entities": []}, {"text": "Recent work on D2W has tended to focus on more sophisticated global approaches to the problem, in which all mentions in a document are disambiguated simultaneously to arrive at a coherent set of disambiguations.", "labels": [], "entities": []}, {"text": "For example, if a mention of \"Michael Jordan\" refers to the computer scientist rather than the basketball player, then we would expect a mention of \"Monte Carlo\" in the same document to refer to the statistical technique rather than the location.", "labels": [], "entities": []}, {"text": "Global approaches utilize the Wikipedia link graph to estimate coherence.", "labels": [], "entities": [{"text": "Wikipedia link graph", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9152305920918783}]}, {"text": "1375 In this paper, we analyze global and local approaches to the D2W task.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: We present a formulation of the D2W task as an optimization problem with local and global variants, and identify the strengths and the weaknesses of each, (2) Using this formulation, we present anew global D2W system, called GLOW.", "labels": [], "entities": [{"text": "GLOW", "start_pos": 259, "end_pos": 263, "type": "DATASET", "confidence": 0.7680895924568176}]}, {"text": "In experiments on existing and novel D2W data sets, 1 GLOW is shown to outperform the previous stateof-the-art system of (, We present an error analysis and identify the key remaining challenge: determining when mentions refer to concepts not captured in Wikipedia.", "labels": [], "entities": [{"text": "GLOW", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.79277503490448}]}], "datasetContent": [{"text": "We evaluate GLOW on four data sets, of which two are from previous work.", "labels": [], "entities": [{"text": "GLOW", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.5828320980072021}]}, {"text": "The first data set, from, is a subset of the AQUAINT corpus of newswire text that is annotated to mimic the hyperlink structure in Wikipedia.", "labels": [], "entities": [{"text": "AQUAINT corpus of newswire text", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.9526626467704773}]}, {"text": "That is, only the first mentions of \"important\" titles were hyperlinked.", "labels": [], "entities": []}, {"text": "Titles deemed uninteresting and redundant mentions of the same title are not linked.", "labels": [], "entities": []}, {"text": "The second data set, from, is taken from MSNBC news and focuses on disambiguating named entities after running NER and co-reference resolution systems on newsire text.", "labels": [], "entities": [{"text": "MSNBC news", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.9269726574420929}, {"text": "NER and co-reference resolution", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.5930748507380486}]}, {"text": "In this case, all mentions of all the detected named entities are linked.", "labels": [], "entities": []}, {"text": "We also constructed two additional data sets.", "labels": [], "entities": []}, {"text": "The first is a subset of the ACE co-reference data set, which has the advantage that mentions and their types are given, and the co-reference is resolved.", "labels": [], "entities": [{"text": "ACE co-reference data set", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.8798304349184036}]}, {"text": "We asked annotators on Amazon's Mechanical Turk to link the first nominal mention of each co-reference chain to Wikipedia, if possible.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.9187745004892349}]}, {"text": "Finding the accuracy of a majority vote of these annotations to be approximately 85%, we manually corrected the annotations to obtain ground truth for our experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9994992017745972}]}, {"text": "The second data set we constructed, Wiki, is a sample of paragraphs from Wikipedia pages.", "labels": [], "entities": [{"text": "Wiki", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8695976734161377}]}, {"text": "Mentions in this data set correspond to existing hyperlinks in the Wikipedia text.", "labels": [], "entities": [{"text": "Wikipedia text", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9273054897785187}]}, {"text": "Because Wikipedia editors explicitly link mentions to Wikipedia pages, their anchor text tends to match the title of the linked-topage-as a result, in the overwhelming majority of 1380 cases, the disambiguation decision is as trivial as string matching.", "labels": [], "entities": []}, {"text": "In an attempt to generate more challenging data, we extracted 10,000 random paragraphs for which choosing the top disambiguation according to P (t|m) results in at least a 10% ranker error rate.", "labels": [], "entities": [{"text": "ranker error rate", "start_pos": 176, "end_pos": 193, "type": "METRIC", "confidence": 0.7200579047203064}]}, {"text": "40 paragraphs of this data was utilized for testing, while the remainder was used for training.", "labels": [], "entities": []}, {"text": "The data sets are summarized in.", "labels": [], "entities": []}, {"text": "The table shows the number of annotated mentions which were hyperlinked to non-null Wikipedia pages, and the number of titles in the documents (without counting repetitions).", "labels": [], "entities": []}, {"text": "For example, the AQUAINT data set contains 727 mentions, 4 all of which refer to distinct titles.", "labels": [], "entities": [{"text": "AQUAINT data set", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.9762571652730306}]}, {"text": "The MSNBC data set contains 747 mentions mapped to non-null Wikipedia pages, but some mentions within the same document refer to the same titles.", "labels": [], "entities": [{"text": "MSNBC data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9814428687095642}]}, {"text": "There are 372 titles in the data set, when multiple instances of the same title within one document are not counted.", "labels": [], "entities": []}, {"text": "To isolate the performance of the individual components of GLOW, we use multiple distinct metrics for evaluation.", "labels": [], "entities": [{"text": "GLOW", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.7852198481559753}]}, {"text": "Ranker accuracy, which measures the performance of the ranker alone, is computed only over those mentions with a non-null gold disambiguation that appears in the candidate set.", "labels": [], "entities": [{"text": "Ranker", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8517602682113647}, {"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.813022792339325}]}, {"text": "It is equal to the fraction of these mentions for which the ranker returns the correct disambiguation.", "labels": [], "entities": []}, {"text": "Thus, a perfect ranker should achieve a ranker accuracy of 1.0, irrespective of limitations of the candidate generator.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9146973490715027}]}, {"text": "Linker accuracy is defined as the fraction of all mentions for which the linker outputs the correct disambiguation (note that, when the title produced by the ranker is incorrect, this penalizes linker accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.8278442621231079}, {"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9486692547798157}]}, {"text": "Lastly, we evaluate our whole system against other baselines using a previously-employed \"bag of titles\" (BOT) evaluation).", "labels": [], "entities": []}, {"text": "In BOT, we compare the set of titles output fora document with the gold set of titles for that document (ignoring duplicates), and utilize standard precision, recall, and F1 measures.", "labels": [], "entities": [{"text": "BOT", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.808971643447876}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9994972944259644}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9976052045822144}, {"text": "F1", "start_pos": 171, "end_pos": 173, "type": "METRIC", "confidence": 0.9997618794441223}]}, {"text": "In BOT, the set of titles is collected from the mentions hyperlinked in the gold annotation.", "labels": [], "entities": [{"text": "BOT", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.5652869343757629}]}, {"text": "That is, if the gold annotation is { (China, People's Republic of China), (Taiwan, Taiwan), (Jiangsu, Jiangsu)}: Percent of \"solvable\" mentions as a function of the number of generated disambiguation candidates.", "labels": [], "entities": []}, {"text": "Listed is the fraction of identified mentions m whose target disambiguation t is among the top k candidates ranked in descending order of P (t|m). and the predicted anotation is: { (China, People's Republic of China), (China, History of China), (Taiwan, null), (Jiangsu, Jiangsu), (republic, Government)} , then the BOT for the gold annotation is: {People's Republic of China, Taiwan, Jiangsu} , and the BOT for the predicted annotation is: {People's Republic of China, History of China, Jiangsu} . The title Government is not included in the BOT for predicted annotation, because its associate mention republic did not appear as a mention in the gold annotation.", "labels": [], "entities": [{"text": "BOT", "start_pos": 316, "end_pos": 319, "type": "METRIC", "confidence": 0.9991975426673889}, {"text": "BOT", "start_pos": 404, "end_pos": 407, "type": "METRIC", "confidence": 0.9990511536598206}, {"text": "BOT", "start_pos": 543, "end_pos": 546, "type": "METRIC", "confidence": 0.9909760355949402}]}, {"text": "Both the precision and the recall of the above prediction is 0.66.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9997960925102234}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9997966885566711}]}, {"text": "We note that in the BOT evaluation, following (Milne and Witten, 2008b) we consider all the titles within a document, even if some the titles were due to mentions we failed to identify.", "labels": [], "entities": [{"text": "BOT", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.5846459865570068}]}, {"text": "5  In this section, we evaluate and analyze GLOW's performance on the D2W task.", "labels": [], "entities": [{"text": "GLOW", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.49564996361732483}]}, {"text": "We begin by evaluating the mention detection component (Step 1 of the algorithm).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7426085323095322}]}, {"text": "The second column of shows how many of the \"non-null\" mentions and corresponding titles we could successfully identify (e.g. out of 747 mentions in the MSNBC data set, only 530 appeared in our anchor-title index).", "labels": [], "entities": [{"text": "MSNBC data set", "start_pos": 152, "end_pos": 166, "type": "DATASET", "confidence": 0.9716661175092062}]}, {"text": "Missing entities were primarily due to especially rare surface forms, or sometimes due to idiosyncratic capitalization in the corpus.", "labels": [], "entities": []}, {"text": "Improving the number of identified mentions substantially is non-trivial; () managed to successfully identify only 59 more entities than we do in the MSNBC data set, using a much more powerful detection method based on search engine query logs.", "labels": [], "entities": [{"text": "MSNBC data set", "start_pos": 150, "end_pos": 164, "type": "DATASET", "confidence": 0.9720154404640198}]}, {"text": "We generate disambiguation candidates fora We evaluate the mention identification stage in Section 6.", "labels": [], "entities": [{"text": "mention identification", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7159147560596466}]}, {"text": "mention musing an anchor-title index, choosing the 20 titles with maximal P (t|m).", "labels": [], "entities": []}, {"text": "evaluates the accuracy of this generation policy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9993390440940857}]}, {"text": "We report the percent of mentions for which the correct disambiguation is generated in the top k candidates (called \"solvable\" mentions).", "labels": [], "entities": []}, {"text": "We see that the baseline prediction of choosing the disambiguation t which maximizes P (t|m) is very strong (80% of the correct mentions have maximal P (t|m) in all data sets except MSNBC).", "labels": [], "entities": []}, {"text": "The fraction of solvable mentions increases until about five candidates per mention are generated, after which the increase is rather slow.", "labels": [], "entities": []}, {"text": "Thus, we believe choosing a limit of 20 candidates per mention offers an attractive trade-off of accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9990556836128235}]}, {"text": "The last column of Table 2 reports the number of solvable mentions and the corresponding number of titles with a cutoff of 20 disambiguation candidates, which we use in our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of mentions and corresponding dis- tinct titles by data set. Listed are (number of men- tions)/(number of distinct titles) for each data set, for each  of three mention types. Gold mentions include all dis- ambiguated mentions in the data set. Identified mentions  are gold mentions whose correct disambiguations exist in  GLOW's author-title index. Solvable mentions are identi- fied mentions whose correct disambiguations are among  the candidates selected by GLOW (see", "labels": [], "entities": [{"text": "GLOW's author-title index", "start_pos": 340, "end_pos": 365, "type": "DATASET", "confidence": 0.9195546656847}, {"text": "GLOW", "start_pos": 479, "end_pos": 483, "type": "DATASET", "confidence": 0.9599891304969788}]}, {"text": " Table 3: Percent of \"solvable\" mentions as a function  of the number of generated disambiguation candidates.  Listed is the fraction of identified mentions m whose  target disambiguation t is among the top k candidates  ranked in descending order of P (t|m).", "labels": [], "entities": []}, {"text": " Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.8916828632354736}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9728790521621704}]}, {"text": " Table 5: Linker performance. The notation X \u2192 Y  means that when linking all mentions, the linking accu- racy is X, while when applying the trained linker, the  performance is Y . The local approaches are better suited  for linking than the global approaches. The linking accu- racy is very sensitive to domain changes.", "labels": [], "entities": []}, {"text": " Table 6: End systems performance -BOT F1. The per- formance of the full system (GLOW) is similar to that of  the local version. GLOW outperforms (Milne and Witten,  2008b) on all data sets.", "labels": [], "entities": [{"text": "BOT F1", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.8962020576000214}]}]}