{"title": [{"text": "Integrating history-length interpolation and classes in language modeling", "labels": [], "entities": [{"text": "language modeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7332042157649994}]}], "abstractContent": [{"text": "Building on earlier work that integrates different factors in language modeling, we view (i) backing off to a shorter history and (ii) class-based generalization as two complementary mechanisms of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation.", "labels": [], "entities": []}, {"text": "This view entails that the classes in a language model should be learned from rare events only and should be preferably applied to rare events.", "labels": [], "entities": []}, {"text": "We construct such a model and show that both training on rare events and preferable application to rare events improve perplexity when compared to a simple direct interpolation of class-based with standard language models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models, probability distributions over strings of words, are fundamental to many applications in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.6477230389912924}]}, {"text": "The main challenge in language modeling is to estimate string probabilities accurately given that even very large training corpora cannot overcome the inherent sparseness of word sequence data.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7683368027210236}]}, {"text": "One way to improve the accuracy of estimation is class-based generalization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.998977780342102}, {"text": "estimation", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9749252796173096}]}, {"text": "The idea is that even though a particular word sequence s may not have occurred in the training set (or too infrequently for accurate estimation), the occurrence of sequences similar to scan help us better estimate p(s).", "labels": [], "entities": []}, {"text": "Plausible though this line of reasoning is, the language models most commonly used today do not incorporate class-based generalization.", "labels": [], "entities": []}, {"text": "This is partially due to the additional cost of creating classes and using classes as part of the model.", "labels": [], "entities": []}, {"text": "But an equally important reason is that most models that integrate class-based information do so byway of a simple interpolation and achieve only a modest improvement in performance.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew type of classbased language model.", "labels": [], "entities": []}, {"text": "The key novelty is that we recognize that certain probability estimates are hard to improve based on classes.", "labels": [], "entities": []}, {"text": "In particular, the best probability estimate for frequent events is often the maximum likelihood estimator and this estimator is hard to improve by using other information sources like classes or word similarity.", "labels": [], "entities": [{"text": "maximum likelihood estimator", "start_pos": 78, "end_pos": 106, "type": "METRIC", "confidence": 0.7862720290819804}]}, {"text": "We therefore design a model that attempts to focus the effect of class-based generalization on rare events.", "labels": [], "entities": []}, {"text": "Specifically, we propose to employ the same strategy for this that history-length interpolated (HI) models use.", "labels": [], "entities": []}, {"text": "We define HI models as models that interpolate the predictions of different-length histories, e.g., p(w 3 |w 1 w 2 ) = \u03bb 1 (w 1 w 2 )p \u2032 (w 3 |w 1 w 2 ) + \u03bb 2 (w 1 w 2 )p \u2032 (w 3 |w 2 ) + (1 \u2212 \u03bb 1 (w 1 w 2 ) \u2212 \u03bb 2 (w 1 w 2 ))p \u2032 (w 3 ) where p \u2032 is a simple estimate; in this section, we use p \u2032 = p ML , the maximum likelihood estimate, as an example. and modified Kneser-Ney ( models are examples of HI models.", "labels": [], "entities": []}, {"text": "HI models address the challenge that frequent events are best estimated by a method close to maximum likelihood by selecting appropriate values for the interpolation weights.", "labels": [], "entities": []}, {"text": "For example, if w 1 w 2 w 3 is frequent, then \u03bb 1 will be close to 1, thus ensuring that p(w 3 |w 1 w 2 ) \u2248 p ML (w 3 |w 1 w 2 ) and that the components p ML (w 3 |w 2 ) and p ML (w 3 ), which are unhelpful in this case, will only slightly change the reliable estimate p ML (w 3 |w 1 w 2 ).", "labels": [], "entities": []}, {"text": "The main contribution of this paper is to propose the same mechanism for class language models.", "labels": [], "entities": []}, {"text": "In fact, we will use the interpolation weights of a KN model to determine how much weight to give to each component of the interpolation.", "labels": [], "entities": []}, {"text": "The difference to a KN model is merely that the lower-order distribution is not the lower-order KN distribution (as in KN), but instead an interpolation of the lower-order KN distribution and a class-based distribution.", "labels": [], "entities": []}, {"text": "We will show that this method of integrating history interpolation and classes significantly increases the performance of a language model.", "labels": [], "entities": []}, {"text": "Focusing the effect of classes on rare events has another important consequence: if this is the right way of using classes, then they should not be formed based on all events in the training set, but only based on rare events.", "labels": [], "entities": []}, {"text": "We show that doing this increases performance.", "labels": [], "entities": []}, {"text": "Finally, we introduce a second discounting method into the model that differs from KN.", "labels": [], "entities": []}, {"text": "This can be motivated by the fact that with two sources of generalization (history-length and classes) more probability mass should be allocated to these two sources than to the single source used in KN.", "labels": [], "entities": []}, {"text": "We propose a polynomial discount and show a significant improvement compared to using KN discounting only.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 reviews the KN model and introduces two models, the DupontRosenfeld model (a \"recursive\" model) and a toplevel interpolated model, that integrate the KN model (a history interpolation model) with a class model.", "labels": [], "entities": []}, {"text": "Section 4 details our experimental setup.", "labels": [], "entities": []}, {"text": "Results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Based on an analysis of strengths and weaknesses of DupontRosenfeld and top-level interpolated models, we present anew polynomial discounting mechanism that does better than either in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run experiments on a Wall Street Journal (WSJ) corpus of 50M words, split 8:1:1 into training, validation and test sets.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus of 50M words", "start_pos": 24, "end_pos": 69, "type": "DATASET", "confidence": 0.9232195675373077}]}, {"text": "The training set contains 256,873 unique unigrams and 4,494,222 unique bigrams.", "labels": [], "entities": []}, {"text": "Unknown words in validation and test sets are mapped to a special unknown word u.", "labels": [], "entities": []}, {"text": "We use the SRILM toolkit) for clustering.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8510322868824005}, {"text": "clustering", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.9657493233680725}]}, {"text": "An important parameter of the classbased model is size |B i | of the base set, i.e., the total number of n-grams (or rather i-grams) to be clustered.", "labels": [], "entities": []}, {"text": "As part of the experiments we vary |B i | systematically to investigate the effect of base set size.", "labels": [], "entities": []}, {"text": "We cluster unigrams (i = 1) and bigrams (i = 2).", "labels": [], "entities": []}, {"text": "For all experiments, |B 1 | = |B 2 | (except in cases where |B 2 | exceeds the number of unigrams, see below).", "labels": [], "entities": []}, {"text": "SRILM does not directly support bigram clustering.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9571276307106018}, {"text": "bigram clustering", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6975110173225403}]}, {"text": "We therefore represent a bigram as a hyphenated word in bigram clustering; e.g., Pan Am is represented as Pan-Am.", "labels": [], "entities": [{"text": "Pan Am", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.9507088363170624}, {"text": "Pan-Am", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.98770672082901}]}, {"text": "The input to the clustering is the vocabulary Bi and the cluster training corpus.", "labels": [], "entities": []}, {"text": "For a particular base set size b, the unigram input vocabulary B 1 is set to the b most frequent unigrams in the training set and the bigram input vocabulary B 2 is set to the b most frequent bigrams in the training set.", "labels": [], "entities": []}, {"text": "In this section, we call the WSJ training corpus the raw corpus and the cluster training corpus the cluster corpus to be able to distinguish them.", "labels": [], "entities": [{"text": "WSJ training corpus", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.8217264413833618}]}, {"text": "We run four different clusterings for each base set size (except for the large sets, see below).", "labels": [], "entities": []}, {"text": "The cluster corpora are constructed as follows.", "labels": [], "entities": []}, {"text": "The cluster corpus is simply the raw corpus.", "labels": [], "entities": []}, {"text": "The cluster corpus is constructed as follows.", "labels": [], "entities": []}, {"text": "A sentence of the raw corpus that contains s words is included twice, once as a sequence of the \u230as/2\u230b bigrams \"w 1 \u2212w 2 w 3 \u2212w 4 w 5 \u2212w 6 . .", "labels": [], "entities": []}, {"text": "\" and once as a sequence of the \u230a(s \u2212 1)/2\u230b bigrams \"w 2 \u2212w 3 w 4 \u2212w 5 w 6 \u2212w 7 . .", "labels": [], "entities": []}, {"text": "\". \u2022 Unique-event unigram clustering.", "labels": [], "entities": [{"text": "Unique-event unigram clustering", "start_pos": 5, "end_pos": 36, "type": "TASK", "confidence": 0.6227733095486959}]}, {"text": "The cluster corpus is the set of all sequences of two unigrams \u2208 B 1 that occur in the raw corpus, one sequence per line.", "labels": [], "entities": []}, {"text": "Each sequence occurs only once in this cluster corpus.", "labels": [], "entities": []}, {"text": "The cluster corpus is the set of all sequences of two bigrams \u2208 B 2 that occur in the training corpus, one sequence per line.", "labels": [], "entities": []}, {"text": "Each sequence occurs only once in this cluster corpus.", "labels": [], "entities": []}, {"text": "As mentioned above, we need both unigram and bigram clusters because we want to incorporate class-based generalization for histories of lengths 1 and 2.", "labels": [], "entities": []}, {"text": "As we will show below this significantly increases performance.", "labels": [], "entities": []}, {"text": "Since the focus of this paper is not on clustering algorithms, reformatting the training corpus as described above (as a sequence of hyphenated bigrams) is a simple way of using SRILM for bigram clustering.", "labels": [], "entities": []}, {"text": "The unique-event clusterings are motivated by the fact that in the Dupont-Rosenfeld model, frequent events are handled by discounted ML estimates.", "labels": [], "entities": []}, {"text": "Classes are only needed in cases where an event was not seen or was not frequent enough in the training set.", "labels": [], "entities": []}, {"text": "Consequently, we should form clusters not based on all events in the training corpus, but only on events that are rare -because this is the type of event that classes will then be applied to in prediction.", "labels": [], "entities": []}, {"text": "The two unique-event corpora can bethought of as reweighted collections in which each unique event receives the same weight.", "labels": [], "entities": []}, {"text": "In practice this means that clustering is mostly influenced by rare events since, on the level of types, most events are rare.", "labels": [], "entities": [{"text": "clustering", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9653851389884949}]}, {"text": "As we will see below, rare-event clusterings perform better than all-event clusterings.", "labels": [], "entities": [{"text": "rare-event clusterings", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.5789065957069397}]}, {"text": "This is not surprising as the class-based component of the model can only benefit rare events and it is therefore reasonable to estimate this component based on a corpus dominated by rare events.", "labels": [], "entities": []}, {"text": "We started experimenting with reweighted corpora because class sizes become very lopsided in regular SRILM clustering as the size of the base set increases.", "labels": [], "entities": [{"text": "SRILM clustering", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.8113639652729034}]}, {"text": "The reason is that the objective function maximizes mutual information.", "labels": [], "entities": []}, {"text": "Highly differentiated classes for frequent words contribute substantially to this objective function whereas putting all rare words in a few large clusters does not hurt the objective much.", "labels": [], "entities": []}, {"text": "However, our focus is on using clustering for improving prediction for rare events; this means that the objective function is counterproductive when contexts are frequency-weighted as they occur in the corpus.", "labels": [], "entities": []}, {"text": "After overweighting rare contexts, the objective function is more in sync with what we use clusters for in our model.", "labels": [], "entities": []}, {"text": "It is important to note that the same intuition underlies unique-event clustering that also motivates using the \"unique-event\" distributions n 1+ (\u2022w 3 2 )/( n 1+ (\u2022w 2 w)) and n 1+ (\u2022w 3 )/( n 1+ (\u2022w)) for the backoff distributions in KN.", "labels": [], "entities": [{"text": "unique-event clustering", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.688439667224884}]}, {"text": "Viewed this way, the basic KN model also uses a unique-event corpus (although a different one) for estimating backoff probabilities.", "labels": [], "entities": []}, {"text": "In all cases, we set the number of clusters to k = 512.", "labels": [], "entities": []}, {"text": "Our main goal in this paper is to compare different ways of setting up history-length/class interpolated models and we do not attempt to optimize k.", "labels": [], "entities": []}, {"text": "We settled on a fixed number of k = 512 because used a total of 1000 classes.", "labels": [], "entities": []}, {"text": "512 unigram classes and 512 bigram classes roughly correspond to this number.", "labels": [], "entities": []}, {"text": "We prefer powers of 2 to facilitate efficient storage of cluster ids (one such cluster id must be stored for each unigram and each bigram) and therefore choose k = 512.", "labels": [], "entities": []}, {"text": "Clustering was performed on an Opteron 8214 processor and took from several minutes for the smallest base sets to more than a week for the largest set of 400,000 items.", "labels": [], "entities": [{"text": "Opteron 8214 processor", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.9407738645871481}]}, {"text": "To estimate n-gram emission probabilities p E , we first introduce an additional cluster for all unigrams that are not in the base set; emission probabilities are then estimated by maximum likelihood.", "labels": [], "entities": []}, {"text": "Cluster transition probabilities p T are computed using addone smoothing.", "labels": [], "entities": []}, {"text": "Both p E and p T are estimated on the raw corpus.", "labels": [], "entities": []}, {"text": "The two class distributions are then defined as follows: where g(v) is the class of the uni-or bigram v.: Optimal parameters for Dupont-Rosenfeld (left) and top-level (right) models on the validation set and perplexity on the validation set.", "labels": [], "entities": []}, {"text": "The two tables compare performance when using a class model trained on all events vs a class model trained on unique events.", "labels": [], "entities": []}, {"text": "|B 1 | = |B 2 | is the number of unigrams and bigrams in the clusters; e.g., lines 1a and 1b are for models that cluster 10,000 unigrams and 10,000 bigrams. is a key to the probability distributions we use.", "labels": [], "entities": []}, {"text": "shows the performance of p DR and p TOP fora range of base set sizes |B i | and for classes trained on all events and on unique events.", "labels": [], "entities": [{"text": "TOP", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8906258344650269}]}, {"text": "Parameters \u03b1 i and \u03bb i are optimized on the validation set.", "labels": [], "entities": []}, {"text": "Perplexity is reported for the validation set.", "labels": [], "entities": []}, {"text": "All following tables also optimize on the validation set and report results on the validation set.", "labels": [], "entities": []}, {"text": "The last table,, also reports perplexity for the test set.", "labels": [], "entities": []}, {"text": "confirms previous findings that classes improve language model performance.", "labels": [], "entities": []}, {"text": "All models have a perplexity that is lower than.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Optimal parameters for Dupont-Rosenfeld (left) and top-level (right) models on the validation set and per- plexity on the validation set. The two tables compare performance when using a class model trained on all events vs a  class model trained on unique events. |B 1 | = |B 2 | is the number of unigrams and bigrams in the clusters; e.g., lines 1a  and 1b are for models that cluster 10,000 unigrams and 10,000 bigrams.", "labels": [], "entities": []}, {"text": " Table 4: Using both unigram and bigram clusters is better than using unigrams only. Results for |B i | = 60,000.", "labels": [], "entities": []}, {"text": " Table 5: Dupont-Rosenfeld and top-level models for  |B i | \u2208 {60000, 100000, 200000, 400000}. Clustering  trained on unique-event corpora.", "labels": [], "entities": []}, {"text": " Table 6: Results for polynomial discounting compared  to p DR and p TOP . |B i | = 400,000, clusters trained on  unique events.", "labels": [], "entities": []}, {"text": " Table 7: Performance of key models on validation and  test sets. tb:l = Table and line the validation result is taken  from. ae/ue = all-event/unique-event. b-= unigrams only.  b+ = bigrams and unigrams.", "labels": [], "entities": []}]}