{"title": [], "abstractContent": [{"text": "While world knowledge has been shown to improve learning-based coreference resolvers, the improvements were typically obtained by incorporating world knowledge into a fairly weak baseline resolver.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.8363617956638336}]}, {"text": "Hence, it is not clear whether these benefits can carryover to a stronger baseline.", "labels": [], "entities": []}, {"text": "Moreover, since there has been no attempt to apply different sources of world knowledge in combination to corefer-ence resolution, it is not clear whether they offer complementary benefits to a resolver.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.9071632325649261}]}, {"text": "We systematically compare commonly-used and under-investigated sources of world knowledge for coreference resolution by applying them to two learning-based coreference models and evaluating them on documents annotated with two different annotation schemes.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.976135641336441}]}], "introductionContent": [{"text": "Noun phrase (NP) coreference resolution is the task of determining which NPs in a text or dialogue refer to the same real-world entity.", "labels": [], "entities": [{"text": "Noun phrase (NP) coreference resolution", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6428435955728803}]}, {"text": "The difficulty of the task stems in part from its reliance on world knowledge.", "labels": [], "entities": []}, {"text": "To exemplify, consider the following text fragment.", "labels": [], "entities": []}, {"text": "Martha Stewart is hoping people don't run out on her.", "labels": [], "entities": []}, {"text": "The celebrity indicted on charges stemming from . .", "labels": [], "entities": []}, {"text": "Having the (world) knowledge that Martha Stewart is a celebrity would be helpful for establishing the coreference relation between the two NPs.", "labels": [], "entities": []}, {"text": "One may argue that employing heuristics such as subject preference or syntactic parallelism (which prefers resolving an NP to a candidate antecedent that has the same grammatical role) in this example would also allow us to correctly resolve the celebrity), thereby obviating the need for world knowledge.", "labels": [], "entities": []}, {"text": "However, since these heuristics are not perfect, complementing them with world knowledge would bean important step towards bringing coreference systems to the next level of performance.", "labels": [], "entities": []}, {"text": "Despite the usefulness of world knowledge for coreference resolution, early learning-based coreference resolvers have relied mostly on morphosyntactic features (e.g.,,,).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.963458389043808}, {"text": "coreference resolvers", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.734238862991333}]}, {"text": "With recent advances in lexical semantics research and the development of large-scale knowledge bases, researchers have begun to employ world knowledge for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.9347297549247742}]}, {"text": "World knowledge is extracted primarily from three data sources, web-based encyclopedia (e.g.,,), unannotated data (e.g.,,), and coreferenceannotated data (e.g.,).", "labels": [], "entities": []}, {"text": "While each of these three sources of world knowledge has been shown to improve coreference resolution, the improvements were typically obtained by incorporating world knowledge (as features) into a baseline resolver composed of a rather weak coreference model (i.e., the mention-pair model) and a small set of features (i.e., the 12 features adopted by knowledge-lean approach).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.9438478052616119}]}, {"text": "As a result, some questions naturally arise.", "labels": [], "entities": []}, {"text": "First, can world knowledge still offer benefits when used in combination with a richer set of features?", "labels": [], "entities": []}, {"text": "Second, since automatically extracted world knowledge is typically noisy, are recently-developed coreference models more noisetolerant than the mention-pair model, and if so, can they profit more from the noisily extracted world knowledge?", "labels": [], "entities": []}, {"text": "Finally, while different world knowl-814 edge sources have been shown to be useful when applied in isolation to a coreference system, do they offer complementary benefits and therefore can further improve a resolver when applied in combination?", "labels": [], "entities": []}, {"text": "We seek answers to these questions by conducting a systematic evaluation of different world knowledge sources for learning-based coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.8221537470817566}]}, {"text": "Specifically, we (1) derive world knowledge from encyclopedic sources that are underinvestigated for coreference resolution, including FrameNet () and, in addition to coreference-annotated data and unannotated data; (2) incorporate such knowledge as features into a richer baseline feature set that we previously employed; and (3) evaluate their utility using two coreference models, the traditional mention-pair model () and the recently developed cluster-ranking model.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.9160657823085785}, {"text": "FrameNet", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.8912695050239563}]}, {"text": "Our evaluation corpus contains 410 documents, which are coreference-annotated using the ACE annotation scheme as well as the OntoNotes annotation scheme).", "labels": [], "entities": [{"text": "ACE annotation scheme", "start_pos": 88, "end_pos": 109, "type": "DATASET", "confidence": 0.8753425478935242}]}, {"text": "By evaluating on two sets of coreference annotations for the same set of documents, we can determine whether the usefulness of world knowledge sources for coreference resolution is dependent on the underlying annotation scheme used to annotate the documents.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.9361436069011688}]}], "datasetContent": [{"text": "We employ two commonly-used scoring programs, B 3 (Bagga and Baldwin, 1998) and CEAF), both of which report results in terms of recall (R), precision (P), and F-measure (F) by comparing the gold-standard (i.e., key) partition, KP , against the system-generated (i.e., response) partition, RP . Briefly, B 3 computes the Rand P values of each NP and averages these values at the end.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.6128231287002563}, {"text": "recall (R)", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9552735984325409}, {"text": "precision (P)", "start_pos": 140, "end_pos": 153, "type": "METRIC", "confidence": 0.9454401284456253}, {"text": "F-measure (F)", "start_pos": 159, "end_pos": 172, "type": "METRIC", "confidence": 0.9566153287887573}]}, {"text": "Specifically, for each NP, NP j , B 3 first computes the number of common NPs in KP j and RP j , the clusters containing NP j in KP and RP , respectively, and then divides this number by |KP j | and |RP j | to obtain the Rand P values of NP j , respectively.", "labels": [], "entities": []}, {"text": "On the other hand, CEAF finds the best one-to-one alignment between the key clusters and the response clusters.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.41112786531448364}]}, {"text": "A complication arises when B 3 is used to score a response partition containing automatically extracted NPs.", "labels": [], "entities": [{"text": "B 3", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9505314528942108}]}, {"text": "Recall that B 3 constructs a mapping between the NPs in the response and those in the key.", "labels": [], "entities": []}, {"text": "Hence, if the response is generated using goldstandard NPs, then every NP in the response is mapped to some NP in the key and vice versa.", "labels": [], "entities": []}, {"text": "In other words, there are no twinless (i.e., unmapped) NPs ().", "labels": [], "entities": []}, {"text": "This is not the case when automatically extracted NPs are used, but the original description of B 3 does not specify how twinless NPs should be scored (.", "labels": [], "entities": []}, {"text": "To address this problem, we set the recall and precision of a twinless NP to zero, regardless of whether the NP appears in the key or the response.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9996277093887329}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9610074162483215}]}, {"text": "Note that CEAF can compare partitions with twinless NPs without any modification, since it operates by finding the best alignment between the clusters in the two partitions.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.7572206854820251}]}, {"text": "Additionally, in order not to over-penalize a response partition, we remove all the twinless NPs in the response that are singletons.", "labels": [], "entities": []}, {"text": "The rationale is simple: since the resolver has successfully identified these NPs as singletons, it should not be penalized, and removing them avoids such penalty.", "labels": [], "entities": [{"text": "resolver", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.9595600366592407}]}, {"text": "Since B 3 and CEAF align NPs/clusters, the lack of singleton clusters in the OntoNotes annotations implies that the resulting scores reflect solely how well a resolver identifies coreference links and do not take into account how well it identifies singleton clusters.", "labels": [], "entities": []}, {"text": "As described in Section 2, we use as our evaluation corpus the 411 documents that are coreferenceannotated using the ACE and OntoNotes annotation schemes.", "labels": [], "entities": [{"text": "ACE", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.9038859009742737}]}, {"text": "Specifically, we divide these documents into five (disjoint) folds of roughly the same size, training the MP model and the CR model using SVM light on four folds and evaluate their performance on the remaining fold.", "labels": [], "entities": []}, {"text": "The linguistic features, as well as the NPs used to create the training and test instances, are computed automatically.", "labels": [], "entities": []}, {"text": "We employ B 3 and CEAF as described in Section 2.3 to score the output of a coreference system.", "labels": [], "entities": [{"text": "B 3", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9611188173294067}, {"text": "CEAF", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8248429894447327}]}], "tableCaptions": [{"text": " Table 1: Results obtained by applying different types of features in isolation to the Baseline system.", "labels": [], "entities": []}, {"text": " Table 2: Results obtained by adding different types of features incrementally to the Baseline system.", "labels": [], "entities": []}]}