{"title": [{"text": "Generalized Interpolation in Decision Tree LM", "labels": [], "entities": [{"text": "Generalized Interpolation in Decision Tree LM", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.6986330300569534}]}], "abstractContent": [{"text": "In the face of sparsity, statistical models are often interpolated with lower order (backoff) models, particularly in Language Modeling.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7474331557750702}]}, {"text": "In this paper, we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective.", "labels": [], "entities": []}, {"text": "We show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context (such as decision tree models), this relation is generally not satisfied.", "labels": [], "entities": []}, {"text": "Based on this insight, we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "A prominent use case for Language Models (LMs) in NLP applications such as Automatic Speech Recognition (ASR) and Machine Translation (MT) is selection of the most fluent word sequence among multiple hypotheses.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.7948136925697327}, {"text": "Machine Translation (MT)", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.8435186386108399}]}, {"text": "Statistical LMs formulate the problem as the computation of the model's probability to generate the word sequence w 1 w 2 . .", "labels": [], "entities": []}, {"text": "w m \u2261 w m 1 , assuming that higher probability corresponds to more fluent hypotheses.", "labels": [], "entities": []}, {"text": "LMs are often represented in the following generative form: p(w i |w i\u22121 1 ) In the following discussion, we will refer to the function p(w i |w i\u22121 1 ) as a language model.", "labels": [], "entities": []}, {"text": "These distributions are typically estimated from observed counts of n-grams w i i\u2212n+1 in the training data.", "labels": [], "entities": []}, {"text": "The context space is still far too large; therefore, the models are recursively smoothed using lower order distributions.", "labels": [], "entities": []}, {"text": "For instance, in a widely used n-gram LM, the probabilities are estimated as follows: where \u03c1 is a discounted probability 1 . In addition to n-gram models, there are many other ways to estimate probability distributions p(w i |w i\u22121 i\u2212n+1 ); in this work, we are particularly interested in models involving decision trees (DTs).", "labels": [], "entities": []}, {"text": "As in n-gram models, DT models also often utilize interpolation with lower order models; however, there are issues concerning the interpolation which arise from the fact that decision trees permit arbitrary clustering of context, and these issues are the main subject of this paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of per- plexity relative to the lower order model of the same type. \"Word-tree\" and \"syntactic\" refer to DT models estimated  using words only (Eq. 2) and words and tags jointly (Eq. 3).", "labels": [], "entities": [{"text": "PTB WSJ section 23", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9258685559034348}]}]}