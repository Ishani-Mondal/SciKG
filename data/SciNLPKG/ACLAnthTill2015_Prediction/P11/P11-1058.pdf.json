{"title": [{"text": "Which Noun Phrases Denote Which Concepts?", "labels": [], "entities": []}], "abstractContent": [{"text": "Resolving polysemy and synonymy is required for high-quality information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7588702142238617}]}, {"text": "We present ConceptResolver, a component for the Never-Ending Language Learner (NELL) (Carlson et al., 2010) that handles both phenomena by identifying the latent concepts that noun phrases refer to.", "labels": [], "entities": []}, {"text": "ConceptResolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7846816976865133}, {"text": "synonym resolution", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.9173629283905029}]}, {"text": "Domain knowledge (the ontology) guides concept creation by defining a set of possible semantic types for concepts.", "labels": [], "entities": [{"text": "concept creation", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7498020827770233}]}, {"text": "Word sense induction is performed by inferring a set of semantic types for each noun phrase.", "labels": [], "entities": [{"text": "Word sense induction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7786814967791239}]}, {"text": "Synonym detection exploits redundant information to train several domain-specific synonym classifiers in a semi-supervised fashion.", "labels": [], "entities": [{"text": "Synonym detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9640043675899506}]}, {"text": "When ConceptResolver is run on NELL's knowledge base, 87% of the word senses it creates correspond to real-world concepts, and 85% of noun phrases that it suggests refer to the same concept are indeed synonyms.", "labels": [], "entities": [{"text": "NELL's knowledge base", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.8050774484872818}]}], "introductionContent": [{"text": "Many information extraction systems construct knowledge bases by extracting structured assertions from free text (e.g., NELL), TextRunner ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.7277341187000275}]}, {"text": "A major limitation of many of these systems is that they fail to distinguish between noun phrases and the underlying concepts they refer to.", "labels": [], "entities": []}, {"text": "As a result, a polysemous phrase like \"apple\" will refer sometimes to the concept Apple Computer (the company), and other times to the concept apple (the fruit).", "labels": [], "entities": []}, {"text": "Furthermore, two synonymous noun phrases like \"apple\" and \"Apple  Computer\" can refer to the same underlying concept.", "labels": [], "entities": []}, {"text": "The result of ignoring this many-to-many mapping between noun phrases and underlying concepts (see) is confusion about the meaning of extracted information.", "labels": [], "entities": []}, {"text": "To minimize such confusion, a system must separately represent noun phrases, the underlying concepts to which they can refer, and the many-to-many \"can refer to\" relation between them.", "labels": [], "entities": []}, {"text": "The relations extracted by systems like NELL actually apply to concepts, not to noun phrases.", "labels": [], "entities": []}, {"text": "Say the system extracts the relation ceoOf(x 1 , x 2 ) between the noun phrases x 1 and x 2 . The correct interpretation of this extracted relation is that there exist concepts c 1 and c 2 such that x 1 can refer to c 1 , x 2 can refer to c 2 , and ceoOf(c 1 , c 2 ).", "labels": [], "entities": []}, {"text": "If the original relation were ceoOf(\"steve\", \"apple\"), then c 1 would be Steve Jobs, and c 2 would be Apple Computer.", "labels": [], "entities": []}, {"text": "A similar interpretation holds for one-place category predicates like person(x 1 ).", "labels": [], "entities": []}, {"text": "We define concept discovery as the problem of (1) identifying concepts like c 1 and c 2 from extracted predicates like ceoOf(x 1 , x 2 ) and (2) mapping noun phrases like x 1 , x 2 to the concepts they can refer to.", "labels": [], "entities": [{"text": "concept discovery", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7545131742954254}]}, {"text": "The main input to ConceptResolver is a set of extracted category and relation instances over noun phrases, like person(x 1 ) and ceoOf(x 1 , x 2 ), produced by running NELL.", "labels": [], "entities": []}, {"text": "Here, any individual noun phrase xi can be labeled with multiple categories and relations.", "labels": [], "entities": []}, {"text": "The output of ConceptResolver is a set of concepts, {c 1 , c 2 , ..., c n }, and a mapping from each noun phrase in the input to the set of concepts it can refer to.", "labels": [], "entities": []}, {"text": "Like many other systems), ConceptResolver represents each output concept c i as a set of synonymous noun phrases, i.e., c i = {x i1 , x i2 , ..., x im }.", "labels": [], "entities": []}, {"text": "For example, shows several concepts output by ConceptResolver; each concept clearly reveals which noun phrases can refer to it.", "labels": [], "entities": []}, {"text": "Each concept also has a semantic type that corresponds to a category in ConceptResolver's ontology; for instance, the first 10 concepts in belong to the category company.", "labels": [], "entities": []}, {"text": "Previous approaches to concept discovery use little prior knowledge, clustering noun phrases based on co-occurrence statistics ).", "labels": [], "entities": [{"text": "concept discovery", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.766507476568222}]}, {"text": "In comparison, ConceptResolver uses a knowledgerich approach.", "labels": [], "entities": []}, {"text": "In addition to the extracted relations, ConceptResolver takes as input two other sources of information: an ontology, and a small number of labeled synonyms.", "labels": [], "entities": []}, {"text": "The ontology contains a schema for the relation and category predicates found in the input instances, including properties of predicates like type restrictions on its domain and range.", "labels": [], "entities": []}, {"text": "The category predicates are used to assign semantic types to each concept, and the properties of relation predicates are used to create evidence for synonym resolution.", "labels": [], "entities": [{"text": "synonym resolution", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.8872595727443695}]}, {"text": "The labeled synonyms are used as training data during synonym resolution, where they are 1.", "labels": [], "entities": [{"text": "synonym resolution", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9439073204994202}]}, {"text": "Use extracted category instances to create one or more senses per noun phrase. ii.", "labels": [], "entities": []}, {"text": "Use argument type constraints to produce relation evidence for synonym resolution.", "labels": [], "entities": [{"text": "synonym resolution", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.9626811444759369}]}], "datasetContent": [{"text": "We perform several experiments to measure ConceptResolver's performance at each of its respective tasks.", "labels": [], "entities": []}, {"text": "The first experiment evaluates word sense induction using Freebase as a canonical set of concepts.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.8067518671353658}]}, {"text": "The second experiment evaluates synonym resolution by comparing ConceptResolver's sense clusters to a gold standard clustering.", "labels": [], "entities": [{"text": "synonym resolution", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.969360888004303}]}, {"text": "For both experiments, we used a knowledge base created by running 140 iterations of NELL.", "labels": [], "entities": []}, {"text": "We preprocessed this knowledge base by removing all noun phrases with zero extracted relations.", "labels": [], "entities": []}, {"text": "As ConceptResolver treats the instances of each category predicate independently, we chose 7 categories from NELL's ontology to use in the evaluation.", "labels": [], "entities": []}, {"text": "The categories were selected on the basis of the number of extracted relations that ConceptResolver could use to detect synonyms.", "labels": [], "entities": []}, {"text": "The number of noun phrases in each category is shown in.", "labels": [], "entities": []}, {"text": "We manually labeled 10 pairs of synonymous senses for each of these categories.", "labels": [], "entities": []}, {"text": "The system automatically synthesized 50 negative examples from the positive examples by assuming each pair represents a distinct concept, so senses in different pairs are not synonyms.", "labels": [], "entities": []}, {"text": "Our first experiment evaluates the performance of ConceptResolver's category-based word sense induction.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.6714224418004354}]}, {"text": "We estimate two quantities: (1) sense precision, the fraction of senses created by our system that correspond to real-world entities, and (2) sense recall, the fraction of real-world entities that ConceptResolver creates senses for.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.619163453578949}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.5912311673164368}]}, {"text": "Sense recall is only measured over entities which are represented by a noun phrase in ConceptResolver's input assertionsit is a measure of ConceptResolver's ability to create senses for the noun phrases it is given.", "labels": [], "entities": []}, {"text": "Sense precision is directly determined by how frequently NELL's extractors propose correct senses for noun phrases, while sense recall is related to the correctness of the one-sense-per-category assumption.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.7984992265701294}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.7594784498214722}]}, {"text": "Precision and recall were evaluated by comparing the senses created by ConceptResolver to concepts in Freebase (.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9834134578704834}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9986238479614258}]}, {"text": "We sampled 100 noun phrases from each category and matched each noun phrase to a set of Freebase concepts.", "labels": [], "entities": []}, {"text": "We interpret each matching Freebase concept as a sense of the noun phrase.", "labels": [], "entities": []}, {"text": "We chose Freebase because it had good coverage for our evaluation categories.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.9742774963378906}]}, {"text": "To align ConceptResolver's senses with Freebase, we first matched each of our categories with a set of similar Freebase categories 2 . We then used a combination of Freebase's search API and Mechanical Turk to align noun phrases with Freebase concepts: we searched for the noun phrase in Freebase, then had Mechanical Turk workers label which of the 2 In Freebase, concepts are called Topics and categories are called Types.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.954557478427887}, {"text": "Freebase", "start_pos": 288, "end_pos": 296, "type": "DATASET", "confidence": 0.9768079519271851}]}, {"text": "For clarity, we use our terminology throughout.: ConceptResolver's word sense induction performance: Empirical distribution of the number of Freebase concepts per noun phrase in each category top 10 resulting Freebase concepts the noun phrase could refer to.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.6490906079610189}, {"text": "Empirical", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9656814336776733}]}, {"text": "After obtaining the list of matching Freebase concepts for each noun phrase, we computed sense precision as the number of noun phrases matching \u2265 1 Freebase concept divided by 100, the total number of noun phrases.", "labels": [], "entities": [{"text": "sense precision", "start_pos": 89, "end_pos": 104, "type": "METRIC", "confidence": 0.8118824064731598}]}, {"text": "Sense recall is the reciprocal of the average number of Freebase concepts per noun phrase.", "labels": [], "entities": [{"text": "recall", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.5950609445571899}]}, {"text": "Noun phrases matching 0 Freebase concepts were not included in this computation.", "labels": [], "entities": []}, {"text": "Our second experiment evaluates synonym resolution by comparing the concepts created by ConceptResolver to a gold standard set of concepts.", "labels": [], "entities": [{"text": "synonym resolution", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.9780414700508118}]}, {"text": "Although this experiment is mainly designed to evaluate ConceptResolver's ability to detect synonyms, it is somewhat affected by the word sense induction process.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.7264893651008606}]}, {"text": "Specifically, the gold standard clustering contains noun phrases that refer to multiple concepts within the same category.", "labels": [], "entities": []}, {"text": "(It is unclear how to create a gold standard clustering without allowing such mappings.)", "labels": [], "entities": []}, {"text": "The word sense induction process produces only one of these mappings, which limits maximum possible recall in this experiment.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.776686946551005}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.997600257396698}]}, {"text": "For this experiment, we report two different measures of clustering performance.", "labels": [], "entities": []}, {"text": "The first measure is the precision and recall of pairwise synonym decisions, typically known as cluster precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9990761280059814}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9944586753845215}, {"text": "cluster precision", "start_pos": 96, "end_pos": 113, "type": "METRIC", "confidence": 0.6546184122562408}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9746092557907104}]}, {"text": "We dub this the clustering metric.", "labels": [], "entities": []}, {"text": "We also adopt the precision/recall measure from Resolver (, which we dub the Resolver metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9992653727531433}, {"text": "recall measure", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.9572668969631195}, {"text": "Resolver", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.752752423286438}]}, {"text": "The Resolver metric aligns each proposed cluster containing \u2265 2 senses with a gold standard cluster (i.e., a real-world concept) by selecting the cluster that a plurality of the senses in the proposed cluster refer to.", "labels": [], "entities": []}, {"text": "Precision is then the fraction of senses in the proposed cluster which are also in the gold standard cluster; recall is computed analogously by swapping the roles of the proposed and gold standard clusters.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9931735396385193}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9989054203033447}]}, {"text": "Resolver precision can be interpreted as the probability that a randomly sampled sense (in a cluster with at least 2 senses) is in a cluster representing its true meaning.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.5791268348693848}]}, {"text": "Incorrect senses were removed from the data set before evaluating precision; however, these senses may still affect performance by influencing the clustering process.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9986826777458191}]}, {"text": "Precision was evaluated by sampling 100 random concepts proposed by ConceptResolver, then manually scoring each concept using both of the metrics above.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9152783751487732}]}, {"text": "This process mimics aligning each sampled concept with its best possible match in a gold standard clustering, then measuring precision with respect to the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9962742328643799}]}, {"text": "Recall was evaluated by comparing the system's output to a manually constructed set of concepts for each category.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.4845619797706604}]}, {"text": "To create this set, we randomly sampled noun phrases from each category and manually matched each noun phrase to one or more real-world entities.", "labels": [], "entities": []}, {"text": "We then found other noun phrases which referred to each entity and created a concept for each entity with at least one unambiguous reference.", "labels": [], "entities": []}, {"text": "This process can create multiple senses fora noun phrase, depending on the real-world entities represented in the input assertions.", "labels": [], "entities": []}, {"text": "We only included concepts containing at least 2 senses in the test set, as singleton concepts do not contribute to either recall metric.", "labels": [], "entities": []}, {"text": "The size of each recall test set is listed in; we created smaller test sets for categories where synonyms were harder to find.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9632037878036499}]}, {"text": "Incorrectly categorized noun phrases were not included in the gold standard as they do not correspond to any real-world entities.", "labels": [], "entities": []}, {"text": "shows the performance of ConceptResolver on each evaluation category.", "labels": [], "entities": []}, {"text": "For each category, we also report the baseline recall achieved by placing each sense in its own cluster.", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9611309766769409}]}, {"text": "ConceptResolver has high precision for several of the categories.", "labels": [], "entities": [{"text": "ConceptResolver", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8671349287033081}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9991121888160706}]}, {"text": "Other categories like athlete and city have somewhat lower precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9988908171653748}]}, {"text": "To make this difference concrete, (first page) shows a random sample of 10 concepts from both company and athlete.", "labels": [], "entities": []}, {"text": "Recall varies even more widely across categories, partly because the categories have varying levels of polysemy, and partly due to differences in average concept size.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8096818923950195}]}, {"text": "The differences in average concept size are reflected in the baseline recall numbers.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9570689797401428}]}, {"text": "We attribute the differences in precision across categories to the different relations available for each category.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9984366297721863}]}, {"text": "For example, none of the relations for athlete uniquely identify a single athlete, and therefore synonymy cannot be accurately represented in the relation view.", "labels": [], "entities": []}, {"text": "Adding more relations to NELL's ontology may improve performance in these cases.", "labels": [], "entities": []}, {"text": "We note that the synonym resolution portion of ConceptResolver is tuned for precision, and that perfect recall is not necessarily attainable.", "labels": [], "entities": [{"text": "synonym resolution", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7453332543373108}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9993795156478882}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9557104706764221}]}, {"text": "Many word senses participate in only one relation, which may not provide enough evidence to detect synonymy.", "labels": [], "entities": []}, {"text": "As NELL continually extracts more knowledge, it is reasonable for ConceptResolver to abstain from these decisions until more evidence is available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ConceptResolver's word sense induction perfor- mance", "labels": [], "entities": []}, {"text": " Table 2: Synonym resolution performance of ConceptResolver", "labels": [], "entities": [{"text": "Synonym resolution", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7661144137382507}, {"text": "ConceptResolver", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.7792348861694336}]}]}