{"title": [{"text": "A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7393412590026855}]}], "abstractContent": [{"text": "This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm.", "labels": [], "entities": []}, {"text": "The composite language model has been trained by performing a con-vergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 183, "end_pos": 198, "type": "TASK", "confidence": 0.7501261532306671}]}, {"text": "The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the BLEU score and \"readability\" when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 171, "end_pos": 181, "type": "METRIC", "confidence": 0.9785768985748291}, {"text": "parsing-based machine translation", "start_pos": 279, "end_pos": 312, "type": "TASK", "confidence": 0.790513257185618}]}], "introductionContent": [{"text": "The Markov chain (n-gram) source models, which predict each word on the basis of previous n-1 words, have been the workhorses of state-of-the-art speech recognizers and machine translators that help to resolve acoustic or foreign language ambiguities by placing higher probability on more likely original underlying word strings.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.7367749810218811}, {"text": "resolve acoustic or foreign language ambiguities", "start_pos": 202, "end_pos": 250, "type": "TASK", "confidence": 0.7979883452256521}]}, {"text": "Research groups ( have shown that using an immense distributed computing paradigm, up to 6-grams can be trained on up to billions and trillions of words, yielding consistent system improvements, but did not observe much improvement beyond 6-grams.", "labels": [], "entities": []}, {"text": "Although the Markov chains are efficient at encoding local word interactions, the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages.", "labels": [], "entities": []}, {"text": "As the machine translation (MT) working groups stated on page 3 of their final report), \"These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8673885703086853}, {"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.9854670763015747}]}, {"text": "There is a dire need for developing novel approaches to language modeling.\" integrated n-gram, structured language model (SLM) and probabilistic latent semantic analysis (PLSA)) under the directed MRF framework () and studied the stochastic properties for the composite language model.", "labels": [], "entities": []}, {"text": "They derived a generalized inside-outside algorithm to train the composite language model from a general EM by following Jelinek's ingenious definition of the inside and outside probabilities for SLM) with 6th order of sentence length time complexity.", "labels": [], "entities": []}, {"text": "Unfortunately, there are no experimental results reported.", "labels": [], "entities": []}, {"text": "In this paper, we study the same composite language model.", "labels": [], "entities": []}, {"text": "Instead of using the 6th order generalized inside-outside algorithm proposed in (), we train this composite model by a convergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 239, "end_pos": 254, "type": "TASK", "confidence": 0.7837288677692413}]}, {"text": "We conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens and compare perplexity results with n-grams (n=3,4,5 respectively) on these three corpora, we obtain drastic perplexity reductions.", "labels": [], "entities": []}, {"text": "Finally, we ap-201 ply our language models to the task of re-ranking the N-best list from Hiero (, a state-of-the-art parsing-based MT system, we achieve significantly better translation quality measured by the BLEU score and \"readability\".", "labels": [], "entities": [{"text": "Hiero", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.8885617852210999}, {"text": "BLEU score", "start_pos": 211, "end_pos": 221, "type": "METRIC", "confidence": 0.9790291786193848}]}], "datasetContent": [{"text": "We have trained our language models using three different training sets: one has 44 million tokens, another has 230 million tokens, and the other has 1.3 billion tokens.", "labels": [], "entities": []}, {"text": "An independent test set which has 354 k tokens is chosen.", "labels": [], "entities": []}, {"text": "The independent check data set used to determine the linear interpolation coefficients has 1.7 million tokens for the 44 million tokens training corpus, 13.7 million tokens for both 230 million and 1.3 billion tokens training corpora.", "labels": [], "entities": []}, {"text": "All these data sets are taken from the LDC English Gigaword corpus with non-verbalized punctuation and we remove all punctuation.", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 39, "end_pos": 66, "type": "DATASET", "confidence": 0.9358505755662918}]}, {"text": "gives the detailed information on how these data sets are chosen from the LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 74, "end_pos": 101, "type": "DATASET", "confidence": 0.9313780814409256}]}, {"text": "The vocabulary sizes in all three cases are: \u2022 word (also WORD-PREDICTOR operation) vocabulary: 60 k, open -all words outside the vocabulary are mapped to the <unk> token, these 60 k words are chosen from the most frequently occurred words in 44 millions tokens corpus; \u2022 POS tag (also TAGGER operation) vocabulary: 69, closed; \u2022 non-terminal tag vocabulary: 54, closed; \u2022 CONSTRUCTOR operation vocabulary: 157, closed.", "labels": [], "entities": []}, {"text": "Similar to SLM), after the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences.", "labels": [], "entities": [{"text": "SLM", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9096808433532715}, {"text": "TAGGER", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9429534077644348}]}, {"text": "We use the \"openNLP\" software) to parse a large amount of sentences in the LDC English Gigaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in).", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 75, "end_pos": 102, "type": "DATASET", "confidence": 0.881661668419838}, {"text": "Upenn Treebank", "start_pos": 232, "end_pos": 246, "type": "DATASET", "confidence": 0.9761956632137299}]}, {"text": "For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that 206 contain 230 million tokens, then use them to initialize model parameters.", "labels": [], "entities": []}, {"text": "The parser at \"openNLP\" is trained by Upenn treebank with 1 million tokens and there is a mismatch between Upenn treebank and LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "Upenn treebank", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.9824435114860535}, {"text": "Upenn treebank", "start_pos": 107, "end_pos": 121, "type": "DATASET", "confidence": 0.9580527245998383}, {"text": "LDC English Gigaword corpus", "start_pos": 126, "end_pos": 153, "type": "DATASET", "confidence": 0.7633136510848999}]}, {"text": "Nevertheless, experimental results show that this approach is effective to provide initial values of model parameters.", "labels": [], "entities": []}, {"text": "As we have explained, the proposed EM algorithms can be naturally cast into a MapReduce framework, see more discussion in (.", "labels": [], "entities": []}, {"text": "If we have access to a large cluster of machines with Hadoop installed that are powerful enough to process a billion tokens level corpus, we just need to specify a map function and a reduce function etc., Hadoop will automatically parallelize and execute programs written in this functional style.", "labels": [], "entities": []}, {"text": "Unfortunately, we don't have this kind of resources available.", "labels": [], "entities": []}, {"text": "Instead, we have access to a supercomputer at a supercomputer center with MPI installed that has more than 1000 core processors usable.", "labels": [], "entities": []}, {"text": "Thus we implement our algorithms using C++ under MPI on the supercomputer, where we have to write C++ codes for Map part and Reduce part, and the MPI is used to take care of massage passing, scheduling, synchronization, etc. between clients and servers.", "labels": [], "entities": [{"text": "massage passing", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.7792572677135468}]}, {"text": "This involves a fair amount of programming work, even though our implementation under MPI is not as reliable asunder Hadoop but it is more efficient.", "labels": [], "entities": []}, {"text": "We use up to 1000 core processors to train the composite language models for 1.3 billion tokens corpus where 900 core processors are used to store the parameters alone.", "labels": [], "entities": []}, {"text": "We decide to use linearly smoothed trigram as the baseline model for 44 million token corpus, linearly smoothed 4-gram as the baseline model for 230 million token corpus, and linearly smoothed 5-gram as the baseline model for 1.3 billion token corpus.", "labels": [], "entities": []}, {"text": "Model size is a big issue, we have to keep only a small set of topics due to the consideration in both computational time and resource demand.", "labels": [], "entities": [{"text": "Model size", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.7142304480075836}]}, {"text": "shows the perplexity results and computation time of composite n-gram/PLSA language models that are trained on three corpora when the pre-defined number of total topics is 200 but different numbers of most likely topics are kept for each document in PLSA, the rest are pruned.", "labels": [], "entities": []}, {"text": "For composite 5-gram/PLSA model trained on 1.3 billion tokens corpus, 400 cores have to be used to keep top 5 most likely topics.", "labels": [], "entities": []}, {"text": "For composite trigram/PLSA model trained on 44M tokens corpus, the computation time increases drastically with less than 5% percent perplexity improvement.", "labels": [], "entities": [{"text": "44M tokens corpus", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.8433238665262858}]}, {"text": "So in the following experiments, we keep top 5 topics for each document from total 200 topics and all other 195 topics are pruned.", "labels": [], "entities": []}, {"text": "All composite language models are first trained by performing N-best list approximate EM algorithm until convergence, then EM algorithm fora second stage of parameter re-estimation for WORD-PREDICTOR and SEMANTIZER until convergence.", "labels": [], "entities": [{"text": "EM", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9122655987739563}]}, {"text": "We fix the size of topics in PLSA to be 200 and then prune to 5 in the experiments, where the unpruned 5 topics in general account for 70% probability in p(g|d).", "labels": [], "entities": []}, {"text": "shows comprehensive perplexity results fora variety of different models such as composite n-gram/m-SLM, n-gram/PLSA, m-SLM/PLSA, their linear combinations, etc., where we use online EM with fixed learning rate to reestimate the parameters of the SEMANTIZER of test document.", "labels": [], "entities": [{"text": "SEMANTIZER of test document", "start_pos": 246, "end_pos": 273, "type": "DATASET", "confidence": 0.6667554602026939}]}, {"text": "The m-SLM performs competitively with its counterpart n-gram (n=m+1) on large scale corpus.", "labels": [], "entities": []}, {"text": "In, for composite n-gram/m-SLM model (n = 3, m = 2 and n = 4, m = 3) trained on 44 million tokens and 230 million tokens, we cutoff its fractional expected counts that are less than a threshold 0.005, this significantly reduces the number of predictor's types by 85%.", "labels": [], "entities": []}, {"text": "When we train the composite language on 1.3 billion tokens corpus, we have to both aggressively prune the parameters of WORD-PREDICTOR and shrink the order of n-gram and m-SLM in order to store them in a supercomputer having 1000 cores.", "labels": [], "entities": []}, {"text": "In particular, for composite 5-gram/4-SLM model, its size is too big to store, thus we use its approximation, a linear combination of 5-gram/2-SLM and 2-gram/4-SLM, and for 5-gram/2-SLM or 2-gram/4-SLM, again we cutoff its fractional expected counts that are less than a threshold 0.005, this significantly reduces the number of predictor's types by 85%.", "labels": [], "entities": []}, {"text": "For composite 4-SLM/PLSA model, we cutoff its fractional expected counts that are less than a threshold 0.002, again this significantly reduces the number of predictor's types by 85%.", "labels": [], "entities": []}, {"text": "For composite 4-SLM/PLSA model or its linear combination with models, we ignore all the tags and use only the words in the 4 head words.", "labels": [], "entities": []}, {"text": "In this: Perplexity results for various language models on test corpus, where + denotes linear combination, / denotes composite model; n denotes the order of n-gram and m denotes the order of SLM; the topic nodes are pruned from 200 to 5.", "labels": [], "entities": []}, {"text": "too big to store in the supercomputer.", "labels": [], "entities": []}, {"text": "The composite n-gram/m-SLM/PLSA model gives significant perplexity reductions over baseline n-grams, n = 3, 4, 5 and m-SLMs, m = 2, 3, 4.", "labels": [], "entities": []}, {"text": "The majority of gains comes from PLSA component, but when adding SLM component into n-gram/PLSA, there is a further 10% relative perplexity reduction.", "labels": [], "entities": []}, {"text": "We have applied our composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA language model that is trained by 1.3 billion word corpus for the task of re-ranking the N -best list in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 173, "end_pos": 204, "type": "TASK", "confidence": 0.7045312921206156}]}, {"text": "We used the same 1000-best list that is used by.", "labels": [], "entities": []}, {"text": "This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (, a state-of-the-art parsing-based translation model.", "labels": [], "entities": [{"text": "MT03 Chinese-English evaluation set", "start_pos": 50, "end_pos": 85, "type": "DATASET", "confidence": 0.8789905458688736}, {"text": "parsing-based translation", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.7100454568862915}]}, {"text": "Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing) on a 200 million tokens corpus.", "labels": [], "entities": []}, {"text": "Each translation has 11 features and language model is one of them.", "labels": [], "entities": []}, {"text": "We substitute our language model and use MERT to optimize the BLEU score ().", "labels": [], "entities": [{"text": "MERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9918911457061768}, {"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9801500141620636}]}, {"text": "We partition the data into ten pieces, 9 pieces are used as training data to optimize the BLEU score () by MERT, a remaining single piece is used to re-rank the 1000-best list and obtain the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9847998321056366}, {"text": "MERT", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.995311439037323}, {"text": "BLEU score", "start_pos": 191, "end_pos": 201, "type": "METRIC", "confidence": 0.9777974784374237}]}, {"text": "The cross-validation process is then repeated 10 times (the folds), with each of the 10 pieces used exactly once as the validation data.", "labels": [], "entities": []}, {"text": "The 10 results from the folds then can be averaged (or otherwise combined) to produce a single estimation for BLEU score.", "labels": [], "entities": [{"text": "estimation", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9774462580680847}, {"text": "BLEU score", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9824413657188416}]}, {"text": "shows the BLEU scores through 10-fold cross-validation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993220567703247}]}, {"text": "The composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA language model gives 1.57% BLEU score improvement over the baseline and 0.79% BLEU score improvement over the 5-gram.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9761800169944763}, {"text": "BLEU score", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.9772703349590302}]}, {"text": "This is because there is not much diversity on the 1000-best list, and essentially only 20 \u223c 30 distinct sentences are therein the 1000-best list.", "labels": [], "entities": []}, {"text": "Chiang (2007) studied the performance of machine translation on Hiero, the BLEU score is 33.31% when n-gram is used to re-rank the N -best list, however, the BLEU score becomes significantly higher 37.09% when the n-gram is embedded directly into Hiero's one pass decoder, this is because there is not much diversity in the N -best list.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7906148433685303}, {"text": "Hiero", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8347821235656738}, {"text": "BLEU score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9834226965904236}, {"text": "BLEU score", "start_pos": 158, "end_pos": 168, "type": "METRIC", "confidence": 0.9833135306835175}]}, {"text": "It is expected that putting the our composite language into a one pass decoder of both phrase-based ( and parsing-based ( MT systems should result in much improved BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9977864027023315}]}, {"text": "SYSTEM MODEL MEAN (%) BASELINE 31.75 5-GRAM 32.53 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 5-GRAM/PLSA 33.01 5-GRAM/2-SLM+2-GRAM/4-SLM 33.32 +5-GRAM/PLSA: 10-fold cross-validation BLEU score results for the task of re-ranking the N -best list.", "labels": [], "entities": [{"text": "MODEL", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.8548875451087952}, {"text": "MEAN", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.6806314587593079}, {"text": "BASELINE", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9798985719680786}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9897319674491882}]}, {"text": "Besides reporting the BLEU scores, we look at the \"readability\" of translations similar to the study conducted by.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9994279742240906}]}, {"text": "The translations are sorted into four groups: good/bad syntax crossed with good/bad meaning by human judges, see Table 5.", "labels": [], "entities": []}, {"text": "We find that many more sentences are perfect, many more are grammatically correct, and many more are semantically correct.", "labels": [], "entities": []}, {"text": "The syntactic language model only improves translations to have good grammar, but does not improve translations to preserve meaning.", "labels": [], "entities": []}, {"text": "The composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA language model improves both significantly.", "labels": [], "entities": []}, {"text": "Bear in mind that integrated Charniak's language model with the syntaxbased translation model to rescore a tree-to-string translation forest, whereas we use only our language model for N -best list re-ranking.", "labels": [], "entities": []}, {"text": "Also, in the same study in, they found that the outputs produced using the n-grams received higher scores from BLEU; ours did not.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9986407160758972}]}, {"text": "The difference between human judgments and BLEU scores indicate that closer agreement maybe possible by incorporating syntactic structure and semantic information into the BLEU score evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9972119927406311}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9555080533027649}]}, {"text": "For example, semantically similar words like \"insure\" and \"ensure\" in the example of BLEU paper () should be substituted in the formula, and there is a weight to measure the goodness of syntactic structure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9800818562507629}]}, {"text": "This modification will lead to a better metric and such information can be provided by our composite language models.: Results of \"readability\" evaluation on 919 translated sentences, P: perfect, S: only semantically correct, G: only grammatically correct, W: wrong.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The corpora used in our experiments are selected  from the LDC English Gigaword corpus and specified in  this table, AFP, AFW, NYT, XIN and CNA denote the  sections of the LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "LDC English Gigaword corpus", "start_pos": 69, "end_pos": 96, "type": "DATASET", "confidence": 0.8473625034093857}, {"text": "AFP", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9806844592094421}, {"text": "AFW", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.8691885471343994}, {"text": "NYT", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.5001770853996277}, {"text": "XIN", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.9000449776649475}, {"text": "LDC English Gigaword corpus", "start_pos": 182, "end_pos": 209, "type": "DATASET", "confidence": 0.8403822183609009}]}, {"text": " Table 2: Perplexity (ppl) results and time consumed of composite n-gram/PLSA language model trained on three  corpora when different numbers of most likely topics are kept for each document in PLSA.", "labels": [], "entities": []}, {"text": " Table 3: Perplexity results for various language models on test corpus, where + denotes linear combination, / denotes  composite model; n denotes the order of n-gram and m denotes the order of SLM; the topic nodes are pruned from  200 to 5.", "labels": [], "entities": []}, {"text": " Table 5: Results of \"readability\" evaluation on 919 trans- lated sentences, P: perfect, S: only semantically correct,  G: only grammatically correct, W: wrong.", "labels": [], "entities": []}]}