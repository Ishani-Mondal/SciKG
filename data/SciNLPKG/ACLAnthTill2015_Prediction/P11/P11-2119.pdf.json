{"title": [{"text": "Unary Constraints for Efficient Context-Free Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel pruning method for context-free parsing that increases efficiency by disallowing phrase-level unary productions in CKY chart cells spanning a single word.", "labels": [], "entities": [{"text": "context-free parsing", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.609475627541542}]}, {"text": "Our work is orthogonal to recent work on \"closing\" chart cells, which has focused on multi-word constituents, leaving span-1 chart cells unpruned.", "labels": [], "entities": []}, {"text": "We show that a simple dis-criminative classifier can learn with high accuracy which span-1 chart cells to close to phrase-level unary productions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9952957034111023}]}, {"text": "Eliminating these unary productions from the search can have a large impact on downstream processing , depending on implementation details of the search.", "labels": [], "entities": []}, {"text": "We apply our method to four parsing architectures and demonstrate how it is complementary to the cell-closing paradigm, as well as other pruning methods such as coarse-to-fine, agenda, and beam-search pruning .", "labels": [], "entities": []}], "introductionContent": [{"text": "While there have been great advances in the statistical modeling of hierarchical syntactic structure in the past 15 years, exact inference with such models remains very costly and most rich syntactic modeling approaches resort to heavy pruning, pipelining, or both.", "labels": [], "entities": []}, {"text": "Graph-based pruning methods such as best-first and beam-search have both be used within context-free parsers to increase their efficiency.", "labels": [], "entities": []}, {"text": "Pipeline systems make use of simpler models to reduce the search space of the full model.", "labels": [], "entities": []}, {"text": "For example, the well-known Charniak parser) uses a simple grammar to prune the search space fora richer model in a second pass.", "labels": [], "entities": []}, {"text": "have recently shown that using a finite-state tagger to close cells within the CKY chart can reduce the worst-case and average-case complexity of context-free parsing, without reducing accuracy.", "labels": [], "entities": [{"text": "CKY chart", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9357154667377472}, {"text": "context-free parsing", "start_pos": 146, "end_pos": 166, "type": "TASK", "confidence": 0.4830383211374283}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9959486722946167}]}, {"text": "In their work, word positions are classified as beginning and/or ending multi-word constituents, and all chart cells not conforming to these constraints can be pruned. and both extend this approach by classifying chart cells with a finer granularity.", "labels": [], "entities": []}, {"text": "Pruning based on constituent span is straightforwardly applicable to all parsing architectures, yet the methods mentioned above only consider spans of length two or greater.", "labels": [], "entities": []}, {"text": "Lexical and unary productions spanning a single word are never pruned, and these can, in many cases, contribute significantly to the parsing effort.", "labels": [], "entities": []}, {"text": "In this paper, we investigate complementary methods to prune chart cells with finite-state preprocessing.", "labels": [], "entities": []}, {"text": "Informally, we use a tagger to restrict the number of unary productions with nonterminals on the right-hand side that can be included in cells spanning a single word.", "labels": [], "entities": []}, {"text": "We term these single word constituents (SWCs) (see Section 2 fora formal definition).", "labels": [], "entities": []}, {"text": "Disallowing SWCs alters span-1 cell population from potentially containing all nonterminals to just pre-terminal part-of-speech (POS) non-terminals.", "labels": [], "entities": [{"text": "SWCs", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9687173366546631}]}, {"text": "In practice, this decreases the number of active states in span-1 chart cells by 70%, significantly reducing the number of allowable constituents in larger spans.", "labels": [], "entities": []}, {"text": "Span-1 chart cells are also the most frequently queried cells in the CKY algorithm.", "labels": [], "entities": []}, {"text": "The search over possible midpoints will always include two cells spanning a single word -one as the first left child and one as the last right child.", "labels": [], "entities": []}, {"text": "It is therefore critical that the number of active states 676 in these cells be minimized so that the number of grammar access requests is also minimized.", "labels": [], "entities": []}, {"text": "Note, however, that some methods of grammar accesssuch as scanning through the rules of a grammar and looking for matches in the chart -achieve less of a speedup from diminished cell population than others, something we investigate in this paper.", "labels": [], "entities": []}, {"text": "Importantly, our method is orthogonal to prior work on tagging chart constraints and we expect efficiency gains to be additive.", "labels": [], "entities": [{"text": "tagging chart", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.891630619764328}]}, {"text": "In what follows, we will demonstrate that a finite-state tagger can learn, with high accuracy, which span-1 chart cells can be closed to SWCs, and how such pruning can increase the efficiency of context-free parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9927740097045898}]}], "datasetContent": [{"text": "To evaluate the effectiveness of unary constraints, we apply our technique to four parsers: an exhaustive CKY chart parser; the Charniak parser), which uses agenda-based two-level coarse-to-fine pruning; the Berkeley parser (Petrov and Klein, 2007a), a multilevel coarse-to-fine parser; and the BUBS parser), a single-pass beam-search parser with a figure-of-merit constituent ranking function.", "labels": [], "entities": [{"text": "CKY chart parser", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7586757938067118}, {"text": "BUBS", "start_pos": 295, "end_pos": 299, "type": "DATASET", "confidence": 0.9487625956535339}]}, {"text": "The Berkeley and BUBS parsers both parse with the Berkeley latent-variable grammar, while the Charniak parser uses a lexicalized grammar, and the exhaustive CKY algorithm is run with a simple Markov order-2 grammar.", "labels": [], "entities": [{"text": "BUBS", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.9020195007324219}]}, {"text": "All grammars are induced from the same data: sections 2-21 of the WSJ treebank.", "labels": [], "entities": [{"text": "WSJ treebank", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9865215718746185}]}, {"text": "contrasts the merit of unary constraints on the three high-accuracy parsers, and several interesting comparisons emerge.", "labels": [], "entities": []}, {"text": "First, as recall is traded for precision within the tagger, each parser reacts quite differently to the imposed constraints.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9975630044937134}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9982712268829346}]}, {"text": "We apply constraints to the Berkeley parser during the initial coarse-pass search, which is simply an exhaustive CKY search with a coarse grammar.", "labels": [], "entities": []}, {"text": "Applying unary and cell-closing constraints at this point in the coarse-to-fine pipeline speeds up the initial coarsepass significantly, which accounted for almost half of the total parse time in the Berkeley parser.", "labels": [], "entities": []}, {"text": "In addition, all subsequent fine-pass searches also benefit from additional pruning as their search is guided by the remaining constituents of the previous pass, which is the intersection of standard coarse-to-fine pruning and our imposed constraints.", "labels": [], "entities": []}, {"text": "We apply constraints to the Charniak parser during the first-pass agenda-based search.", "labels": [], "entities": []}, {"text": "Because an agenda-based search operates at a constituent level instead of a cell/span level, applying unary constraints alters the search frontier instead of reducing the absolute number of constituents placed in the chart.", "labels": [], "entities": []}, {"text": "We jointly tune lambda and the internal search parameters of the Charniak parser until accuracy degrades.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9990608096122742}]}, {"text": "Application of constraints to the CKY and BUBS parsers is straightforward as they are both single pass parsers -any constituent violating the constraints is pruned.", "labels": [], "entities": [{"text": "BUBS", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.7567388415336609}]}, {"text": "We also note that the CKY and 679 BUBS parsers both employ the cross-product grammar access method discussed in Section 2, while the Berkeley parser uses the grammar loop method.", "labels": [], "entities": [{"text": "CKY and 679 BUBS parsers", "start_pos": 22, "end_pos": 46, "type": "DATASET", "confidence": 0.6958447694778442}]}, {"text": "This grammar access difference dampens the benefit of unary constraints for the Berkeley parser.", "labels": [], "entities": []}, {"text": "Referring back to, we see that both speed and accuracy increase in all but the Berkeley parser.", "labels": [], "entities": [{"text": "speed", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9988131523132324}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9994441866874695}]}, {"text": "Although it is unusual that pruning leads to higher accuracy during search, it is not unexpected here as our finite-state tagger makes use of lexical relationships that the PCFG does not.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9969032406806946}, {"text": "PCFG", "start_pos": 173, "end_pos": 177, "type": "DATASET", "confidence": 0.9601479172706604}]}, {"text": "By leveraging this new information to constrain the search space, we are indirectly improving the quality of the model.", "labels": [], "entities": []}, {"text": "Finally, there is an obvious operating point for each parser at which the unary constraints are too severe and accuracy deteriorates rapidly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.998939573764801}]}, {"text": "For test conditions, we set the tuning parameter \u03bb based on the development set results to prune as much of the search space as possible before reaching this degradation point.", "labels": [], "entities": []}, {"text": "Using lambda-values optimized for each parser, we parse the unseen section 23 test data and present results in.", "labels": [], "entities": [{"text": "section 23 test data", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.6156364604830742}]}, {"text": "We see that in all cases, unary constraints improve the efficiency of parsing without significant accuracy loss.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9817615151405334}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9982132911682129}]}, {"text": "As one might expect, exhaustive CKY parsing benefits the most from unary constraints since no other pruning is applied.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7281880676746368}]}, {"text": "But even heavily pruned parsers using graph-based and pipelining techniques still see substantial speedups The Berkeley parser does maintain meta-information about where non-terminals have been placed in the chart, giving it some of the advantages of cross-product grammar access.  with the additional application of unary constraints.", "labels": [], "entities": []}, {"text": "Furthermore, unary constraints consistently provide an additive efficiency gain when combined with cellclosing constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unary production counts from sections 2-21 of the", "labels": [], "entities": [{"text": "Unary production", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8343976736068726}]}, {"text": " Table 2: Grammar statistics and averaged span-1 active state", "labels": [], "entities": []}, {"text": " Table 3: Test set results applying unary constraints (UC) and", "labels": [], "entities": []}]}