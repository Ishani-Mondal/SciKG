{"title": [{"text": "A Scalable Probabilistic Classifier for Language Modeling", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7218749821186066}]}], "abstractContent": [{"text": "We present a novel probabilistic classifier, which scales well to problems that involve a large number of classes and require training on large datasets.", "labels": [], "entities": []}, {"text": "A prominent example of such a problem is language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.8075924515724182}]}, {"text": "Our classifier is based on the assumption that each feature is associated with a predictive strength, which quantifies how well the feature can predict the class by itself.", "labels": [], "entities": []}, {"text": "The predictions of individual features can then be combined according to their predictive strength, resulting in a model, whose parameters can be reliably and efficiently estimated.", "labels": [], "entities": []}, {"text": "We show that a generative language model based on our classifier consistently matches modified Kneser-Ney smoothing and can outperform it if sufficiently rich features are incorporated.", "labels": [], "entities": [{"text": "generative language", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8898075222969055}]}], "introductionContent": [{"text": "A Language Model (LM) is an important component within many natural language applications including speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7444886863231659}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7879610955715179}]}, {"text": "The task of a generative LM is to assign a probability p(w) to a sequence of words w = w 1 . .", "labels": [], "entities": [{"text": "generative LM", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.9298840165138245}]}, {"text": "w L . It is common to factorize this probability as Thus, the central problem that arises from this formulation consists of estimating the probability p(w i |w i\u2212N +1 . .", "labels": [], "entities": []}, {"text": "w i\u22121 ).", "labels": [], "entities": []}, {"text": "This can be viewed as a classification problem in which the target word W i corresponds to the class that must be predicted, based on features extracted from the conditioning context, e.g. a word occurring in the context.", "labels": [], "entities": []}, {"text": "This paper describes a novel approach for modeling such conditional probabilities.", "labels": [], "entities": []}, {"text": "We propose a classifier which is based on the assumption that each feature has a predictive strength, quantifying how well the feature can predict the class (target word) by itself.", "labels": [], "entities": []}, {"text": "Then the predictions made by individual features can be combined into a mixture model, in which the prediction of each feature is weighted according to its predictive strength.", "labels": [], "entities": []}, {"text": "This reflects the fact that certain features (e.g. certain context words) are much more predictive than others but the predictive strength fora particular feature often doesn't vary much across classes and can thus be assumed constant.", "labels": [], "entities": []}, {"text": "The main advantage of our model is that it is straightforward to incorporate rich features without sacrificing scalability or reliability of parameter estimation.", "labels": [], "entities": []}, {"text": "In addition, it is simple to implement and no feature selection is required.", "labels": [], "entities": []}, {"text": "Section 3 shows that a generative 1 LM built with our classifier is competitive to modified Kneser-Ney smoothing and can outperform it if sufficiently rich features are incorporated.", "labels": [], "entities": []}, {"text": "The classification-based approach to language modeling was introduced by who proposed an optimized variant of the maximumentropy classifier) for the task.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7066904753446579}]}, {"text": "Unfortunately, data sparsity resulting from the large number of classes makes it difficult to obtain reliable parameter estimates, even on large datasets and the high computational costs make it difficult train models on large datasets in the first place 2 . Scal-ability is however very important, since moving to larger datasets is often the simplest way to obtain a better model.", "labels": [], "entities": []}, {"text": "Similarly, neural probabilistic LMs () don't scale very well to large datasets.", "labels": [], "entities": []}, {"text": "Even the more scalable variant proposed by is trained on a dataset consisting of only 14M words, also using a vocabulary of around 20000 words.", "labels": [], "entities": []}, {"text": "Van den proposes a decision-tree classifier which has been applied to training datasets with more than 100M words.", "labels": [], "entities": []}, {"text": "However, his model is non-probabilistic and thus a standard comparison with probabilistic models in terms of perplexity isn't possible.", "labels": [], "entities": []}, {"text": "N-Gram models) obtain estimates for p(w i |w i\u2212N +1 . .", "labels": [], "entities": []}, {"text": "w i\u22121 ) using counts of N-Grams.", "labels": [], "entities": []}, {"text": "Because directly using the maximumlikelihood estimate would result in poor predictions, smoothing techniques are applied.", "labels": [], "entities": []}, {"text": "A modified interpolated form of Kneser-Ney smoothing) was shown to consistently outperform a variety of other smoothing techniques) and currently constitutes a stateof-the-art 3 generative LM.", "labels": [], "entities": [{"text": "generative LM", "start_pos": 178, "end_pos": 191, "type": "TASK", "confidence": 0.8085457682609558}]}], "datasetContent": [{"text": "All experiments were conducted using the SRI Language Modeling Toolkit (SRILM, Stolcke), i.e. we implemented 5 the VMM within SRILM and compared to default N-Gram models supplied with SRILM.", "labels": [], "entities": [{"text": "SRI Language Modeling Toolkit (SRILM", "start_pos": 41, "end_pos": 77, "type": "DATASET", "confidence": 0.6543744206428528}, {"text": "SRILM", "start_pos": 126, "end_pos": 131, "type": "DATASET", "confidence": 0.8854311108589172}, {"text": "SRILM", "start_pos": 184, "end_pos": 189, "type": "DATASET", "confidence": 0.9395063519477844}]}, {"text": "The experiments were run on a 64-bit, 2.2 GHz dual-core machine with 8GB RAM.", "labels": [], "entities": []}, {"text": "Data The experiments were carried out on data from the Reuters Corpus Version 1 (Lewis et al., The code can be downloaded from http://code.", "labels": [], "entities": [{"text": "Reuters Corpus Version 1", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.9800251871347427}]}, {"text": "google.com/p/variable-mixture-model .), which was split into sentences, tokenized and converted to lowercase, not removing punctuation.", "labels": [], "entities": []}, {"text": "All our models were built with the same 30367-word vocabulary, which includes the sentence-end symbol and a special symbol for out-of-vocabulary words (UNK).", "labels": [], "entities": []}, {"text": "The vocabulary was compiled by selecting all words which occur more than four times in the data of week 31, which was not otherwise used for training or testing.", "labels": [], "entities": []}, {"text": "As development set we used the articles of week 50 (4.1M words) and as test set the articles of week 51 (3.8M words).", "labels": [], "entities": []}, {"text": "For training we used datasets of four different sizes: D1 (week 1, 3.1M words), D2 (weeks 1-3, 10M words), D3 (weeks 1-10, 37M words) and D4 (weeks 1-30, 113M words).", "labels": [], "entities": []}, {"text": "Features We use three different feature sets in our experiments.", "labels": [], "entities": []}, {"text": "The first feature set (basic, BA) consists of all features also used in standard N-Gram models, i.e. all subsequences up to a length N \u2212 1 immediately preceding the target word.", "labels": [], "entities": [{"text": "BA", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9778096079826355}]}, {"text": "The second feature set (short-range, SR) consists of all basic features as well as all skip N-Grams () that can be formed with the N \u2212 1 length context.", "labels": [], "entities": [{"text": "SR", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9525327086448669}]}, {"text": "Moreover, all words occurring in the context are included as bag features, i.e. as features which indicate the occurrence of a word but not the particular position.", "labels": [], "entities": []}, {"text": "The third feature set (long-range, LR) is an extension of SR which also includes longerdistance features.", "labels": [], "entities": [{"text": "SR", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.6663172245025635}]}, {"text": "Specifically, this feature set additionally includes all unigram bag features up to a distance d = 9.", "labels": [], "entities": []}, {"text": "The feature types and examples of extracted features are given in.", "labels": [], "entities": []}, {"text": "Model Comparison We compared the VMM to modified Kneser-Ney (KN, see Section 1).", "labels": [], "entities": [{"text": "VMM", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7732424139976501}]}, {"text": "The order of a VMM is defined through the length of the context from which the basic and short-range features are extracted.", "labels": [], "entities": []}, {"text": "In particular, VM-BA of a certain order uses the same features as the N-Gram models of the same order and VM-SR uses the same conditioning context as the N-Gram models of the same order.", "labels": [], "entities": []}, {"text": "VM-LR in addition contains longerdistance features, beyond the order of the corresponding N-Gram models.", "labels": [], "entities": [{"text": "VM-LR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9555632472038269}]}, {"text": "The order of the models was varied between N = 2 . .", "labels": [], "entities": []}, {"text": "5, however, for the larger two datasets D3 and D4 the order 5 models would not fit into the available RAM which is why for order 5 we can only report scores for D1 and D2.", "labels": [], "entities": []}, {"text": "We could resort to pruning, but since this would have an effect on performance it would invalidate a direct comparison, which we want to avoid.", "labels": [], "entities": []}, {"text": "628  Model Parametrization We used the development set to determine the values for the absolute discounting parameter D (defined in Section 2.1) and the number of iterations for stochastic gradient ascent.", "labels": [], "entities": [{"text": "absolute discounting parameter D", "start_pos": 87, "end_pos": 119, "type": "METRIC", "confidence": 0.7489510700106621}]}, {"text": "This resulted in a value D = 0.1.", "labels": [], "entities": [{"text": "D", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9950124621391296}]}, {"text": "Stochastic gradient yields best results with a single pass through all instances.", "labels": [], "entities": []}, {"text": "More iterations result in overfitting, i.e. decrease training data log-likelihood but increase the log-likelihood on the development data.", "labels": [], "entities": []}, {"text": "The step size was kept fixed at \u03b7 = 1.0.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The test set perplexities of the models for orders  N=2..5 on training datasets D1-D4.", "labels": [], "entities": []}]}