{"title": [{"text": "A Latent Topic Extracting Method based on Events in a Document and its Application", "labels": [], "entities": [{"text": "Latent Topic Extracting", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.7266782522201538}]}], "abstractContent": [{"text": "Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis.", "labels": [], "entities": [{"text": "topic analysis", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.667068600654602}, {"text": "text analysis", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.8807385563850403}]}, {"text": "However, those methods basically assign topics to words, but do not account for the events in a document.", "labels": [], "entities": []}, {"text": "With this background, in this paper, we propose a latent topic extracting method which assigns topics to events.", "labels": [], "entities": [{"text": "topic extracting", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.6906828284263611}]}, {"text": "We also show that our proposed method is useful to generate a document summary based on a latent topic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, several latent topic analysis methods such as Latent Semantic Indexing (LSI)), Probabilistic LSI (pLSI), and Latent Dirichlet Allocation (LDA) () have been widely used for text analysis.", "labels": [], "entities": [{"text": "latent topic analysis", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.6177506049474081}, {"text": "text analysis", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.8948643505573273}]}, {"text": "However, those methods basically assign topics to words, but do not account for the events in a document.", "labels": [], "entities": []}, {"text": "Here, we define a unit of informing the content of document at the level of sentence as an \"Event\" , and propose a model that treats a document as a set of Events.", "labels": [], "entities": []}, {"text": "We use LDA as a latent topic analysis method, and assign topics to Events in a document.", "labels": [], "entities": []}, {"text": "To examine our proposed method's performance on extracting latent topics from a document, we compare the accuracy of our method to that of the conventional methods through a common document retrieval task.", "labels": [], "entities": [{"text": "extracting latent topics from a document", "start_pos": 48, "end_pos": 88, "type": "TASK", "confidence": 0.7803287108739217}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9992890357971191}]}, {"text": "Furthermore, as an application of our method, we apply it to a query-biased document summarization (Tombros and Sanderson, For the definition of an Event, see 1998;) to verify that the method is useful for various applications.", "labels": [], "entities": []}, {"text": "proposed a flexible latent topics inference in which topics are assigned to phrases in a document.", "labels": [], "entities": []}, {"text": "showed that the accuracy of document classification will be improved by introducing a feature dealing with the dependency relationships among words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.999029278755188}, {"text": "document classification", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7166877090930939}]}], "datasetContent": [{"text": "Through a common document retrieval task, we compare our method with the conventional method and evaluate both of them.", "labels": [], "entities": []}, {"text": "In concrete, we regard the documents which have a similar topic distribution to a query's topic distribution as the result of retrieval, and then examine whether or not the estimated topic distribution can represent the latent semantics of each document based on the accuracy of retrieval results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 267, "end_pos": 275, "type": "METRIC", "confidence": 0.9952801465988159}]}, {"text": "Henceforth, we call the conventional word-based LDA \"wordLDA\" and our proposed event-based LDA \"eventLDA\".", "labels": [], "entities": []}, {"text": "In the experiment, we use a data set provided at NT-CIR4 (NII Test Collection for IR Systems 4) TSC3 (Text Summarization Challenge 3) . The data consists of 30 topic sets of documents in which each set has about 10 Japanese newspaper articles, and the total number of the sentences in the data is 3587.", "labels": [], "entities": [{"text": "NT-CIR4 (NII Test Collection for IR Systems 4) TSC3", "start_pos": 49, "end_pos": 100, "type": "DATASET", "confidence": 0.781507362018932}, {"text": "Text Summarization Challenge", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.727104256550471}]}, {"text": "In order to make evaluation for the result provided by our method easier, we compile a set of questions, provided by the data sets for evaluating the result of summarization, as a query, and then use it as a query for query-biased summarization.", "labels": [], "entities": []}, {"text": "As an evaluation method, we adopt precision and coverage used at TSC3 (, and the number of extracted sentences is the same as used in TSC3.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9996483325958252}, {"text": "coverage", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9939265847206116}, {"text": "TSC3", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.919098973274231}]}, {"text": "Precision is an evaluation measure which indicates the ratio of the number of correct sentences to that of the sentences generated by the system.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9768303632736206}]}, {"text": "Coverage is an evaluation measure which indicates the degree of how the system output is close to the summary generated by a human, taking account of the redundancy.", "labels": [], "entities": []}, {"text": "Moreover, to examine the characteristics of the proposed method, we compare both methods in terms of the number of topics and the proper measure to estimate similarity.", "labels": [], "entities": []}, {"text": "The number of trials is 20 at each condition.", "labels": [], "entities": []}, {"text": "5 sets of documents selected at random from 30 sets of documents are used in the trials, and all the trials are totally averaged.", "labels": [], "entities": []}, {"text": "As a target for comparison with the proposed method, we also conduct an experiment using wordLDA.", "labels": [], "entities": [{"text": "wordLDA", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9291278123855591}]}], "tableCaptions": [{"text": " Table 1: Result based on the number of topics.", "labels": [], "entities": []}, {"text": " Table 2: Performance under various measures.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the number of topics.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of each method.", "labels": [], "entities": []}]}