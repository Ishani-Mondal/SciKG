{"title": [{"text": "Coreference for Learning to Extract Relations: Yes, Virginia, Coreference Matters", "labels": [], "entities": [{"text": "Learning to Extract Relations", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6204830333590508}]}], "abstractContent": [{"text": "As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples.", "labels": [], "entities": [{"text": "bootstrapping relation extraction", "start_pos": 97, "end_pos": 130, "type": "TASK", "confidence": 0.7080453932285309}]}, {"text": "Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates.", "labels": [], "entities": []}, {"text": "Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation.", "labels": [], "entities": []}, {"text": "We explore whether coreference can improve the learning process.", "labels": [], "entities": [{"text": "coreference", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9596710205078125}]}, {"text": "That is, if the algorithm considered examples such as his sister, would accuracy be improved?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9993528723716736}]}, {"text": "With co-reference, we see on average a 2-fold increase in F-Score.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9972864389419556}]}, {"text": "Despite using potentially errorful machine coreference, we see significant increase in recall on all relations.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9994478821754456}]}, {"text": "Precision increases in four cases and decreases in six.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.992584228515625}]}], "introductionContent": [{"text": "As an alternative to requiring substantial supervised relation training data (e.g. the ~300k words of detailed, exhaustive annotation in Automatic Content Extraction (ACE) evaluations 1 ) many have explored bootstrapping relation extraction from a few (~20) seed instances of a relation.", "labels": [], "entities": [{"text": "bootstrapping relation extraction", "start_pos": 207, "end_pos": 240, "type": "TASK", "confidence": 0.6866009732087454}]}, {"text": "Key to such approaches is a large body of unannotated text that can be iteratively processed as follows: 1.", "labels": [], "entities": []}, {"text": "Find sentences containing the seed instances.", "labels": [], "entities": []}, {"text": "2. Induce patterns of context from the sentences.", "labels": [], "entities": []}, {"text": "3. From those patterns, find more instances.", "labels": [], "entities": []}, {"text": "4. Go to 2 until some condition is reached.", "labels": [], "entities": []}, {"text": "Most techniques assume that relation instances, like hasBirthDate(Wolfgang Amadeus Mozart, http://www.nist.gov/speech/tests/ace/ 1756), are realized in the corpus as relation texts with easily spotted anchors like Wolfgang Amadeus Mozart was born in 1756.", "labels": [], "entities": []}, {"text": "In this paper we explore whether using coreference can improve the learning process.", "labels": [], "entities": []}, {"text": "That is, if the algorithm considered texts like his birth in 1756 for the above relation, would performance of the learned patterns be better?", "labels": [], "entities": []}], "datasetContent": [{"text": "Estimating recall for bootstrapped relation learning is a challenge except for corpora small enough for complete annotation to be feasible, e.g., the ACE corpora.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9934781789779663}, {"text": "bootstrapped relation learning", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6907519400119781}, {"text": "ACE corpora", "start_pos": 150, "end_pos": 161, "type": "DATASET", "confidence": 0.942541241645813}]}, {"text": "ACE typically had a test set of ~30,000 words and ~300k for training.", "labels": [], "entities": [{"text": "ACE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6633053421974182}]}, {"text": "Yet, with a small corpus, rare relations will be inadequately represented.", "labels": [], "entities": []}, {"text": "Macro-reading evaluations (e.g. Mitchell, 2009) have not estimated recall, but have measured precision by sampling system output and determining whether the extracted fact is true in the world.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9988248944282532}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.998977541923523}]}, {"text": "An instance like hasChild(his father, he) would be useful neither during training nor (without coreference) at runtime.", "labels": [], "entities": []}, {"text": "An average of 12,583 matches versus 2,256 matches.", "labels": [], "entities": []}, {"text": "If multiple mentions expressing an argument occur in one sentence, each match is counted, inflating the difference.", "labels": [], "entities": []}, {"text": "Despite being selected to be rich in the 18 ACE relation subtypes, the 10 most frequent subtypes account for over 90% of the relations with the 4 most frequent accounting for 62%; the 5 least frequent relation subtypes occur less than 50 times.", "labels": [], "entities": []}, {"text": "Here we extend this idea to both precision and recall in a micro-reading context.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9994325041770935}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9984982013702393}]}, {"text": "Precision is measured by running the system over the background corpus and randomly sampleing 100 texts that the system believes entail each relation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9434162378311157}]}, {"text": "From the mentions matching the argument slots of the patterns, we build a relation instance.", "labels": [], "entities": []}, {"text": "If these mentions are not names (only possible for +Coref), they are resolved to names using system coreference.", "labels": [], "entities": []}, {"text": "For example, given the passage in and the pattern '(Y, poss:X)', the system would match the mentions X=her and Y=son, and build the relation instance hasChild(Ethel Kennedy, Robert F. Kennedy Jr.).", "labels": [], "entities": [{"text": "Ethel Kennedy, Robert F. Kennedy Jr.)", "start_pos": 159, "end_pos": 196, "type": "DATASET", "confidence": 0.7694133147597313}]}, {"text": "During assessment, the annotator is asked whether, in the context of the whole document, a given sentence entails the relation instance.", "labels": [], "entities": []}, {"text": "We thus treat both incorrect relation extraction and incorrect reference resolution as mistakes.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.6967594027519226}, {"text": "reference resolution", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.6831268668174744}]}, {"text": "To measure recall, we select 20 test relation instances and search the corpus for sentences containing all arguments of a test instance (explicitly or via coreference).", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9950889945030212}]}, {"text": "We randomly sampled from this set, choosing at most 10 sentences for each test instance, to form a collection of at most 200 sentences likely to be texts expressing the desired relation.", "labels": [], "entities": []}, {"text": "These sentences were then manually annotated in the same manner as the precision annotation.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9947308301925659}]}, {"text": "Sentences that did not correctly convey the relation instance were removed, and the remaining set of sentences formed a recall set.", "labels": [], "entities": []}, {"text": "We consider a recall set instance to be found by a system if the system finds a relation of the correct type in the sentence.", "labels": [], "entities": []}, {"text": "We intentionally chose to sample 10 sentences from each test example, rather than sampling from the set of all sentences found.", "labels": [], "entities": []}, {"text": "This prevents one or two very commonly expressed instances from dominating the recall set.", "labels": [], "entities": []}, {"text": "As a result, the recall test set is biased away from \"true\" recall, because it places a higher weight on the \"long tail\" of instances.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9947720170021057}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9145617485046387}]}, {"text": "However, this gives a more accurate indication of the system's ability to find novel instances of a relation.", "labels": [], "entities": []}, {"text": "gives results for precision, recall, and F for +Coref (+) and -Coref (-).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9996213912963867}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9994787573814392}, {"text": "F", "start_pos": 41, "end_pos": 42, "type": "METRIC", "confidence": 0.9996739625930786}]}, {"text": "In all cases removing coreference causes a drop in recall, ranging from only 33%(hasBirthPlace) to over 90% (GPEEmploys).", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9995259046554565}, {"text": "hasBirthPlace", "start_pos": 81, "end_pos": 94, "type": "METRIC", "confidence": 0.789476215839386}, {"text": "GPEEmploys", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.732229471206665}]}, {"text": "The median drop is 68%.", "labels": [], "entities": [{"text": "median drop", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9368330836296082}]}], "tableCaptions": [{"text": " Table 1: Precision, Recall, and F scores", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.997941792011261}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9908260703086853}, {"text": "F", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.9833190441131592}]}, {"text": " Table 2: Number of test seeds where at least one  instance is found in the evaluation.", "labels": [], "entities": []}]}