{"title": [{"text": "Rule Markov Models for Fast Tree-to-String Translation", "labels": [], "entities": [{"text": "Fast Tree-to-String Translation", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6974702676137289}]}], "abstractContent": [{"text": "Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 5, "end_pos": 36, "type": "TASK", "confidence": 0.6334840854008993}]}, {"text": "Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.987380862236023}]}, {"text": "Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules.", "labels": [], "entities": []}, {"text": "Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (mea-sured using Bleu) as composed rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule, for example, a phrase pair in phrase-based translation or a tree-to-string rule in tree-to-string translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7509564459323883}]}, {"text": "These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context.", "labels": [], "entities": [{"text": "translation", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.9688926935195923}]}, {"text": "To alleviate this problem, most state-of-the-art systems rely on composed rules, which are larger rules that can be formed out of smaller rules (including larger phrase pairs that can be formerd out of smaller phrase pairs), as opposed to minimal rules, which are rules that cannot be formed out of other rules.", "labels": [], "entities": []}, {"text": "Although this approach does improve translation quality dramatically by weakening the independence assumptions in the translation model, they suffer from two main problems.", "labels": [], "entities": [{"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9786821603775024}]}, {"text": "First, composition can cause a combinatorial explosion in the number of rules.", "labels": [], "entities": []}, {"text": "To avoid this, ad-hoc limits are placed during composition, like upper bounds on the number of nodes in the composed rule, or the height of the rule.", "labels": [], "entities": []}, {"text": "Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar.", "labels": [], "entities": []}, {"text": "Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down.", "labels": [], "entities": []}, {"text": "Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules.", "labels": [], "entities": []}, {"text": "For example, initially built a syntax-based system using only minimal rules, and subsequently reported () that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold.", "labels": [], "entities": []}, {"text": "The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context.", "labels": [], "entities": []}, {"text": "In this work, we restrict a rule's context to the vertical chain of ancestors of the rule.", "labels": [], "entities": []}, {"text": "This ancestral context would play the same role as the context formerly provided by rule composition.", "labels": [], "entities": [{"text": "rule composition", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.8134377598762512}]}, {"text": "The dependency treelet model developed by takes such an approach within the framework of dependency translation.", "labels": [], "entities": [{"text": "dependency translation", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.8280889391899109}]}, {"text": "However, their study leaves unanswered whether a rule Markov model can take the place of composed rules.", "labels": [], "entities": []}, {"text": "In this work, we investigate the use of rule Markov models in the context of tree-856 to-string translation ().", "labels": [], "entities": [{"text": "tree-856 to-string translation", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.776787002881368}]}, {"text": "We make three new contributions.", "labels": [], "entities": []}, {"text": "First, we carryout a detailed comparison of rule Markov models with composed rules.", "labels": [], "entities": []}, {"text": "Our experiments show that, using trigram rule Markov models, we achieve an improvement of 2.2 Bleu over a baseline of minimal rules.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.7959644198417664}]}, {"text": "When we compare against vertically composed rules, we find that our rule Markov model has the same accuracy, but our model is much smaller and decoding with our model is 30% faster.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9993919134140015}]}, {"text": "When we compare against full composed rules, we find that our rule Markov model still often reaches the same level of accuracy, again with savings in space and time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9993897676467896}]}, {"text": "Second, we investigate methods for pruning rule Markov models, finding that even very simple pruning criteria actually improve the accuracy of the model, while of course decreasing its size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9984935522079468}]}, {"text": "Third, we present a very fast decoder for tree-tostring grammars with rule Markov models.", "labels": [], "entities": []}, {"text": "have recently introduced an efficient incremental decoding algorithm for tree-to-string translation, which operates top-down and maintains a derivation history of translation rules encountered.", "labels": [], "entities": [{"text": "tree-to-string translation", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.7830385863780975}]}, {"text": "This history is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model, which makes it an ideal decoder for our model.", "labels": [], "entities": []}, {"text": "We start by describing our rule Markov model (Section 2) and then how to decode using the rule Markov model (Section 3).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same  level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for  both the full model and the model filtered for the concatenation of the development and test sets (dev+test).", "labels": [], "entities": []}, {"text": " Table 2: For rule bigrams, RM-B with D 1 = 0.4 gives the  best results on the development set.", "labels": [], "entities": []}, {"text": " Table 3: For rule bigrams, RM-A with D 1 , D 2 = 0.5 gives  the best results on the development set.", "labels": [], "entities": []}, {"text": " Table 6: Adding rule Markov models to composed-rule grammars improves their translation performance.", "labels": [], "entities": []}, {"text": " Table 4: RM-A is robust to different settings of D n on the  development set.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of vertically composed rules using  various settings (maximum rule height 7).", "labels": [], "entities": []}]}