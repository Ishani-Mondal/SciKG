{"title": [{"text": "Semi-Supervised SimHash for Efficient Document Similarity Search", "labels": [], "entities": [{"text": "Efficient Document Similarity Search", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.6276292130351067}]}], "abstractContent": [{"text": "Searching documents that are similar to a query document is an important component in modern information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.721027061343193}]}, {"text": "Some existing hashing methods can be used for efficient document similarity search.", "labels": [], "entities": [{"text": "document similarity search", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7785395582516988}]}, {"text": "However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing.", "labels": [], "entities": []}, {"text": "Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative.", "labels": [], "entities": []}, {"text": "This paper proposes a novel (semi-)supervised hash-ing method named Semi-Supervised SimHash (S 3 H) for high-dimensional data similarity search.", "labels": [], "entities": [{"text": "data similarity search", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.6995683113733927}]}, {"text": "The basic idea of S 3 H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes.", "labels": [], "entities": [{"text": "S 3 H", "start_pos": 18, "end_pos": 23, "type": "TASK", "confidence": 0.7798494895299276}]}, {"text": "We evaluate our method with several state-of-the-art methods on two large datasets.", "labels": [], "entities": []}, {"text": "All the results show that our method gets the best performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document Similarity Search (DSS) is to find similar documents to a query doc in a text corpus or on the web.", "labels": [], "entities": [{"text": "Document Similarity Search (DSS)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8836158315340678}]}, {"text": "It is an important component in modern information retrieval since DSS can improve the traditional search engines and user experience).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7481822371482849}]}, {"text": "Traditional search engines accept several terms submitted by a user as a query and return a set of docs that are relevant to the query.", "labels": [], "entities": []}, {"text": "However, for those users who are not search experts, it is always difficult to accurately specify some query terms to express their search purposes.", "labels": [], "entities": []}, {"text": "Unlike short-query based search, DSS queries by a full (long) document, which allows users to directly submit a page or a document to the search engines as the description of their information needs.", "labels": [], "entities": [{"text": "DSS queries", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9284586608409882}]}, {"text": "Meanwhile, the explosion of information has brought great challenges to traditional methods.", "labels": [], "entities": []}, {"text": "For example, Inverted List (IL) which is a primary key-term access method would return a very large set of docs fora query document, which leads to the time-consuming post-processing.", "labels": [], "entities": []}, {"text": "Therefore, anew effective algorithm is required.", "labels": [], "entities": []}, {"text": "Hashing methods can perform highly efficient but approximate similarity search, and have gained great success in many applications such as Content-Based Image Retrieval (CBIR) (), near-duplicate data detection, etc.", "labels": [], "entities": [{"text": "approximate similarity search", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6981903612613678}, {"text": "Content-Based Image Retrieval (CBIR)", "start_pos": 139, "end_pos": 175, "type": "TASK", "confidence": 0.7901016672452291}, {"text": "near-duplicate data detection", "start_pos": 180, "end_pos": 209, "type": "TASK", "confidence": 0.7304973403612772}]}, {"text": "Hashing methods project high-dimensional objects to compact binary codes called fingerprints and make similar fingerprints for similar objects.", "labels": [], "entities": []}, {"text": "The similarity search in the Hamming space 1 is much more efficient than in the original attribute space (.", "labels": [], "entities": []}, {"text": "Recently, several hashing methods have been proposed.", "labels": [], "entities": []}, {"text": "Specifically, SimHash (SH)) uses random projections to hash data.", "labels": [], "entities": []}, {"text": "Although it works well with long fingerprints, SH has poor discrimination power for short fingerprints.", "labels": [], "entities": []}, {"text": "A kernelized variant of SH, called Kernelized Locality Sensitive Hashing (KLSH) (, is proposed to handle non-linearly separable data.", "labels": [], "entities": [{"text": "Kernelized Locality Sensitive Hashing (KLSH)", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.6567514198166984}]}, {"text": "These methods are unsupervised thus cannot incorporate prior knowledge for better hashing.", "labels": [], "entities": []}, {"text": "Moti-vated by this, some supervised methods are proposed to derive effective hash functions from prior knowledge, i.e., Spectral Hashing ( and Semi-Supervised Hashing (SSH) ().", "labels": [], "entities": [{"text": "Spectral Hashing", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.8459176123142242}]}, {"text": "Regardless of different objectives, both methods derive hash functions via Principle Component Analysis (PCA).", "labels": [], "entities": []}, {"text": "However, PCA is computationally expensive, which limits their usage for high-dimensional data.", "labels": [], "entities": []}, {"text": "This paper proposes a novel (semi-)supervised hashing method, Semi-Supervised SimHash (S 3 H), for high-dimensional data similarity search.", "labels": [], "entities": [{"text": "data similarity search", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.6975212494532267}]}, {"text": "Unlike SSH that tries to find a sequence of hash functions, S 3 H fixes the random projection directions and seeks the optimal feature weights from prior knowledge to relocate the objects such that similar objects have similar fingerprints.", "labels": [], "entities": []}, {"text": "This is implemented by maximizing the empirical accuracy on the prior knowledge (labeled data) and the entropy of hash functions (estimated over labeled and unlabeled data).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9715165495872498}]}, {"text": "The proposed method avoids using PCA which is computationally expensive especially for high-dimensional data, and leads to an efficient Quasi-Newton based solution.", "labels": [], "entities": []}, {"text": "To evaluate our method, we compare with several state-ofthe-art hashing methods on two large datasets, i.e., 20 Newsgroups (20K points) and Open Directory Project (ODP) (2.4 million points).", "labels": [], "entities": []}, {"text": "All experiments show that S 3 H gets the best search performance.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 briefly introduces the background and some related works.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our proposed SemiSupervised SimHash (S 3 H).", "labels": [], "entities": []}, {"text": "Section 4 provides experimental validation on two datasets.", "labels": [], "entities": []}, {"text": "The conclusions are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two datasets 20 Newsgroups and Open Directory Project (ODP) in our experiments.", "labels": [], "entities": []}, {"text": "Each document is represented as a vector of occurrence numbers of the terms within it.", "labels": [], "entities": []}, {"text": "The class information of docs is considered as prior knowledge that two docs within a same class should have more similar fingerprints while two docs within different classes should have dissimilar fingerprints.", "labels": [], "entities": []}, {"text": "We will demonstrate that our S 3 H can effectively incorporate this prior knowledge to improve the DSS performance.", "labels": [], "entities": [{"text": "DSS", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9139660596847534}]}, {"text": "We use Inverted List (IL)) as the baseline.", "labels": [], "entities": [{"text": "Inverted List (IL))", "start_pos": 7, "end_pos": 26, "type": "METRIC", "confidence": 0.8695357441902161}]}, {"text": "In fact, given a query doc, IL returns all the docs that contain any term within it.", "labels": [], "entities": []}, {"text": "We also compare our method with three state-ofthe-art hashing methods, i.e., KLSH, SSH and SH.", "labels": [], "entities": [{"text": "KLSH", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.702854335308075}]}, {"text": "In KLSH, we adopt the RBF kernel \u03ba( ), where the scaling factor \u03b4 2 takes 0.5 and the other two parameters p and tare set to be 500 and 50 respectively.", "labels": [], "entities": [{"text": "KLSH", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.696483850479126}, {"text": "RBF kernel \u03ba", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.733134925365448}]}, {"text": "The parameter \u03bb in SSH is set to 1.", "labels": [], "entities": [{"text": "SSH", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8623898029327393}]}, {"text": "For S 3 H, we simply set the parameters \u03bb 1 and \u03bb 2 in Equation (8) to 4 and 0.5 respectively.", "labels": [], "entities": [{"text": "Equation", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.975177526473999}]}, {"text": "To objectively reflect the performance of S 3 H, we evaluate our S 3 H with and without Feature Contribution Calculation algorithm (FCC) (Algorithm 1).", "labels": [], "entities": [{"text": "Feature Contribution Calculation algorithm (FCC)", "start_pos": 88, "end_pos": 136, "type": "METRIC", "confidence": 0.7342856270926339}]}, {"text": "Specifically, FCC-free S 3 H (denoted as S 3 Hf ) is just a simplification when Gs in S 3 H are simply set to Ds.", "labels": [], "entities": []}, {"text": "For quantitative evaluation, as in literature (, we calculate the precision under two scenarios: hash lookup and hash ranking.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9994333386421204}]}, {"text": "For hash lookup, the proportion of good neighbors (have the same class label as the query) among the searched objects within a given Hamming radius is calculated as precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9989416003227234}]}, {"text": "Similarly to (, fora query document, if no neighbors within the given Hamming radius can be found, it is considered as zero precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9805871248245239}]}, {"text": "Note that, the precision of IL is the proportion of good neighbors among the whole searched objects.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9995101690292358}]}, {"text": "For hash ranking, all the objects in X are ranked in terms of their Hamming distance from the query document, and the top K nearest neighbors are returned as the result.", "labels": [], "entities": [{"text": "hash ranking", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8221218287944794}]}, {"text": "Then, Mean Averaged Precision (MAP)) is calculated.", "labels": [], "entities": [{"text": "Mean Averaged Precision (MAP))", "start_pos": 6, "end_pos": 36, "type": "METRIC", "confidence": 0.9502379099527994}]}, {"text": "We also calculate the averaged intra-and inter-class Hamming distance for various hashing methods.", "labels": [], "entities": []}, {"text": "In- tuitively, a good hashing method should have small intra-class distance while large inter-class distance.", "labels": [], "entities": []}, {"text": "We test all the methods on a PC with a 2.66 GHz processor and 12GB RAM.", "labels": [], "entities": [{"text": "RAM", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9448619484901428}]}, {"text": "All experiments repeate 10 times and the averaged results are reported.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Averaged intra-and inter-class Hamming dis- tance of 20 Newsgroups for 32-bit fingerprint. \u2206 is the  difference between the averaged inter-and intra-class  Hamming distance. Large \u2206 implies good hashing.", "labels": [], "entities": [{"text": "Hamming dis- tance", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.796996183693409}]}, {"text": " Table 2: Averaged intra-and inter-class Hamming dis- tance of ODP for 32-bit fingerprint (860K features). \u2206  is the difference between averaged intra-and inter-class  Hamming distance.", "labels": [], "entities": [{"text": "Hamming dis- tance", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.8338084518909454}, {"text": "ODP", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.8261170983314514}, {"text": "Hamming", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.9038399457931519}]}]}