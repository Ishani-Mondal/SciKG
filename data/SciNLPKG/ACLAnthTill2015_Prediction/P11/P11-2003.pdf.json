{"title": [{"text": "Temporal Restricted Boltzmann Machines for Dependency Parsing", "labels": [], "entities": [{"text": "Temporal Restricted Boltzmann Machines", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8119349926710129}, {"text": "Dependency Parsing", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.6660670042037964}]}], "abstractContent": [{"text": "We propose a generative model based on Temporal Restricted Boltzmann Machines for transition based dependency parsing.", "labels": [], "entities": [{"text": "transition based dependency parsing", "start_pos": 82, "end_pos": 117, "type": "TASK", "confidence": 0.6624994948506355}]}, {"text": "The parse tree is built incrementally using a shift-reduce parse and an RBM is used to model each decision step.", "labels": [], "entities": []}, {"text": "The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information.", "labels": [], "entities": []}, {"text": "Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively , which compare well with similar previous models and the state-of-the-art.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (, Bayesian networks (, and Deep Belief Networks ().", "labels": [], "entities": []}, {"text": "In this paper, we investigate how these models can be applied to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8675906658172607}]}, {"text": "We focus on Shift-Reduce transition-based parsing proposed by.", "labels": [], "entities": [{"text": "Shift-Reduce transition-based parsing", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.7791516979535421}]}, {"text": "In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree.", "labels": [], "entities": []}, {"text": "To assign probabilities to these actions, previous work has proposed memory-based classifiers (), SVMs (), and Incremental Sigmoid Belief Networks (ISBN).", "labels": [], "entities": []}, {"text": "Ina related earlier work, proposed a maximum entropy model for transition-based constituency parsing.", "labels": [], "entities": [{"text": "transition-based constituency parsing", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.6129128138224283}]}, {"text": "Of these approaches, only ISBNs induce highdimensional latent representations to encode parse history, but suffer from either very approximate or slow inference procedures.", "labels": [], "entities": []}, {"text": "We propose to address the problem of inference in a high-dimensional latent space by using an undirected graphical model, Restricted Boltzmann Machines (RBMs), to model the individual parsing decisions.", "labels": [], "entities": []}, {"text": "Unlike the Sigmoid Belief Networks (SBNs) used in ISBNs, RBMs have tractable inference procedures for both forward and backward reasoning, which allows us to efficiently infer both the probability of the decision given the latent variables and vice versa.", "labels": [], "entities": []}, {"text": "The key structural difference between the two models is that the directed connections between latent and decision vectors in SBNs become undirected in RBMs.", "labels": [], "entities": []}, {"text": "A complete parsing model consists of a sequence of RBMs interlinked via directed edges, which gives us a form of Temporal Restricted Boltzmann Machines (TRBM), but with the incrementally specified model structure required by parsing.", "labels": [], "entities": []}, {"text": "In this paper, we analyze and contrast ISBNs with TRBMs and show that the latter provide an accurate and theoretically sound model for parsing with highdimensional latent variables.", "labels": [], "entities": [{"text": "parsing", "start_pos": 135, "end_pos": 142, "type": "TASK", "confidence": 0.9734019041061401}]}], "datasetContent": [{"text": "We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task dataset", "start_pos": 63, "end_pos": 93, "type": "DATASET", "confidence": 0.8925768971443176}]}, {"text": "Standard splits of training, development and test sets were used.", "labels": [], "entities": []}, {"text": "To handle word sparsity, we replaced all the (POS, word) pairs with frequency less than 20 in the training set with (POS, UNKNOWN), giving us only 4530 tag-word pairs.", "labels": [], "entities": []}, {"text": "Since our model can work only with projective trees, we used MaltParser () to projectivize/deprojectivize the training input/test output.", "labels": [], "entities": []}, {"text": "87.07 89.95 f . Malt \u2212 \u2192 AE 85.96 88.64 g.", "labels": [], "entities": [{"text": "AE", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.7761930227279663}]}, {"text": "MST Malt 87.45 90.22 h.", "labels": [], "entities": [{"text": "MST Malt 87.45 90.22 h", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.9616797804832459}]}, {"text": "89.88 unknown on adding the features (row b) shows that the feed forward inference procedure for ISBNs relies heavily on these feature connections to compensate for the lack of backward inference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 lists the labeled (LAS) and unlabeled (UAS)  attachment scores. Row a shows that a simple ISBN  model without features, using feed forward infer- ence procedure, does not work well. As explained  in section 2, this is expected since in the absence of  explicit features, the latent variables in a given layer  do not take into account the observations in the pre- vious layers. The huge improvement in performance", "labels": [], "entities": []}, {"text": " Table 2: K-means clustering of words according to their  TRBM latent representations. Duplicate words in the  same cluster are not shown.", "labels": [], "entities": []}, {"text": " Table 3: Wordnet similarity scores for clusters given by  different models.", "labels": [], "entities": [{"text": "Wordnet similarity scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6901981830596924}]}]}