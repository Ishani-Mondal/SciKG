{"title": [{"text": "A Class of Submodular Functions for Document Summarization", "labels": [], "entities": [{"text": "Document Summarization", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.895955353975296}]}], "abstractContent": [{"text": "We design a class of submodular functions meant for document summarization tasks.", "labels": [], "entities": [{"text": "document summarization tasks", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.7881008386611938}]}, {"text": "These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity.", "labels": [], "entities": []}, {"text": "Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality.", "labels": [], "entities": []}, {"text": "When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization.", "labels": [], "entities": [{"text": "DUC 2004-2007 corpora", "start_pos": 18, "end_pos": 39, "type": "DATASET", "confidence": 0.9435209234555563}]}, {"text": "Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions area natural fit for document summarization.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6426147818565369}, {"text": "submodular function optimization", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.7563940286636353}, {"text": "document summarization", "start_pos": 210, "end_pos": 232, "type": "TASK", "confidence": 0.66280597448349}]}], "introductionContent": [{"text": "In this paper, we address the problem of generic and query-based extractive summarization from collections of related documents, a task commonly known as multi-document summarization.", "labels": [], "entities": [{"text": "query-based extractive summarization from collections of related documents", "start_pos": 53, "end_pos": 127, "type": "TASK", "confidence": 0.7618600651621819}, {"text": "multi-document summarization", "start_pos": 154, "end_pos": 182, "type": "TASK", "confidence": 0.7066468596458435}]}, {"text": "We treat this task as monotone submodular function maximization (to be defined in Section 2).", "labels": [], "entities": []}, {"text": "This has a number of critical benefits.", "labels": [], "entities": []}, {"text": "On the one hand, there exists a simple greedy algorithm for monotone submodular function maximization where the summary solution obtained (say\u02c6Ssay\u02c6 say\u02c6S) is guaranteed to be almost as good as the best possible solution (say S opt ) according to an objective F.", "labels": [], "entities": [{"text": "monotone submodular function maximization", "start_pos": 60, "end_pos": 101, "type": "TASK", "confidence": 0.6692505925893784}]}, {"text": "More precisely, the greedy algorithm is a constant factor approximation to the cardinality constrained version of the problem, so that . This is particularly attractive since the quality of the solution does not depend on the size of the problem, so even very large size problems do well.", "labels": [], "entities": []}, {"text": "It is also important to note that this is a worst case bound, and inmost cases the quality of the solution obtained will be much better than this bound suggests.", "labels": [], "entities": []}, {"text": "Of course, none of this is useful if the objective function F is inappropriate for the summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9189956188201904}]}, {"text": "In this paper, we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for document summarization.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.6154006719589233}]}, {"text": "We show, in fact, that many well-established methods for summarization correspond to submodular function optimization, a property not explicitly mentioned in these publications.", "labels": [], "entities": [{"text": "summarization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.98464035987854}, {"text": "submodular function optimization", "start_pos": 85, "end_pos": 117, "type": "TASK", "confidence": 0.7481469511985779}]}, {"text": "We take this fact, however, as testament to the value of submodular functions for summarization: if summarization algorithms are repeatedly developed that, by chance, happen to bean instance of a submodular function optimization, this suggests that submodular functions area natural fit.", "labels": [], "entities": [{"text": "summarization", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.981086015701294}]}, {"text": "On the other hand, other authors have started realizing explicitly the value of submodular functions for summarization (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9819424748420715}]}, {"text": "Submodular functions share many properties in common with convex functions, one of which is that they are closed under a number of common combination operations (summation, certain compositions, restrictions, and so on).", "labels": [], "entities": [{"text": "summation", "start_pos": 162, "end_pos": 171, "type": "TASK", "confidence": 0.9577243328094482}]}, {"text": "These operations give us the tools necessary to design a powerful submodular objective for submodular document summarization that extends beyond any previous work.", "labels": [], "entities": [{"text": "submodular document summarization", "start_pos": 91, "end_pos": 124, "type": "TASK", "confidence": 0.6352132161458334}]}, {"text": "We demonstrate this by carefully crafting a class of submodular func-510 tions we feel are ideal for extractive summarization tasks, both generic and query-focused.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7670794129371643}]}, {"text": "In doing so, we demonstrate better than existing state-of-the-art performance on a number of standard summarization evaluation tasks, namely DUC-04 through to DUC-07.", "labels": [], "entities": [{"text": "DUC-04", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.9663212895393372}, {"text": "DUC-07", "start_pos": 159, "end_pos": 165, "type": "DATASET", "confidence": 0.9777656197547913}]}, {"text": "We believe our work, moreover, might act as a springboard for researchers in summarization to consider the problem of \"how to design a submodular function\" for the summarization task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9931105375289917}, {"text": "summarization task", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.9367041289806366}]}, {"text": "In Section 2, we provide a brief background on submodular functions and their optimization.", "labels": [], "entities": []}, {"text": "Section 3 describes how the task of extractive summarization can be viewed as a problem of submodular function maximization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6524300575256348}, {"text": "submodular function maximization", "start_pos": 91, "end_pos": 123, "type": "TASK", "confidence": 0.6896123886108398}]}, {"text": "We also in this section show that many standard methods for summarization are, in fact, already performing submodular function optimization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9885681867599487}, {"text": "submodular function optimization", "start_pos": 107, "end_pos": 139, "type": "TASK", "confidence": 0.7521326343218485}]}, {"text": "In Section 4, we present our own submodular functions.", "labels": [], "entities": []}, {"text": "Section 5 presents results on both generic and query-focused summarization tasks, showing as far as we know the best known ROUGE results for DUC-04 through DUC-06, and the best known precision results for DUC-07, and the best recall DUC-07 results among those that do not use a web search engine.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.832683801651001}, {"text": "ROUGE", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9850717782974243}, {"text": "precision", "start_pos": 183, "end_pos": 192, "type": "METRIC", "confidence": 0.9923343062400818}, {"text": "recall", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.9861225485801697}]}, {"text": "Section 6 discusses implications for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Automatic evaluation of summary quality is important for the research of document summarization as it avoids the labor-intensive and potentially inconsistent human evaluation.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.5846454799175262}]}, {"text": "ROUGE) is widely used for summarization evaluation and it has been shown that ROUGE-N scores are highly correlated with human evaluation).", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.9625251591205597}, {"text": "ROUGE-N", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.953392744064331}]}, {"text": "Interestingly, ROUGE-N is monotone submodular, adding further evidence that monotone submodular functions are natural for document summarization.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9272041916847229}, {"text": "document summarization", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.6255379617214203}]}, {"text": "By definition), ROUGE-N is the n-gram recall between a candidate summary and a set of reference summaries.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9970466494560242}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9065080285072327}]}, {"text": "Precisely, let S be the candidate summary (a set of sentences extracted from the ground set V ), c e : 2 V \u2192 Z + be the number of times n-gram e occurs in summary S, and R i be the set of n-grams contained in the reference summary i (suppose we have K reference summaries, i.e., i = 1, \u00b7 \u00b7 \u00b7 , K).", "labels": [], "entities": []}, {"text": "Then ROUGE-N can be written as the following set function: where r e,i is the number of times n-gram e occurs in reference summary i.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.9724999070167542}]}, {"text": "Since c e (S) is monotone modular and min(x, a) is a concave non-decreasing function of x, min(c e (S), r e,i ) is monotone submodular by Theorem 1.", "labels": [], "entities": []}, {"text": "Since summation preserves submodularity, and the denominator is constant, we see that F ROUGE-N is monotone submodular.", "labels": [], "entities": [{"text": "summation", "start_pos": 6, "end_pos": 15, "type": "TASK", "confidence": 0.9680070877075195}, {"text": "ROUGE-N", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.8430159687995911}]}, {"text": "Since the reference summaries are unknown, it is of course impossible to optimize F ROUGE-N directly.", "labels": [], "entities": [{"text": "F", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9830151796340942}, {"text": "ROUGE-N", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.6425568461418152}]}, {"text": "Therefore, some approaches (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010) instead define \"concepts\".", "labels": [], "entities": []}, {"text": "Alter-513 natively, we herein propose a class of monotone submodular functions that naturally models the quality of a summary while not depending on an explicit notion of concepts, as we will see in the following section.", "labels": [], "entities": []}, {"text": "The document understanding conference (DUC) (http://duc.nist.org) was the main forum providing benchmarks for researchers working on document summarization.", "labels": [], "entities": [{"text": "document understanding conference (DUC)", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.8576928675174713}, {"text": "document summarization", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.5750745534896851}]}, {"text": "The tasks in DUC evolved from single-document summarization to multi-document summarization, and from generic summarization) to query-focused summarization.", "labels": [], "entities": []}, {"text": "As ROUGE) has been officially adopted for DUC evaluations since 2004, we also take it as our main evaluation criterion.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.9935269355773926}, {"text": "DUC evaluations", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7131772935390472}]}, {"text": "We evaluated our approaches on DUC data, and demonstrate results on both generic and query-focused summarization.", "labels": [], "entities": [{"text": "DUC data", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.7632770538330078}]}, {"text": "In all experiments, the modified greedy algorithm was used for summary generation.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.9241845905780792}]}], "tableCaptions": [{"text": " Table 1: ROUGE-1 recall (R) and F-measure (F) results  (%) on DUC-04. DUC-03 was used as development set.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9863587021827698}, {"text": "recall (R)", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.892676830291748}, {"text": "F-measure (F)", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9632024466991425}, {"text": "DUC-04", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.9462203979492188}, {"text": "DUC-03", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.9180718660354614}]}, {"text": " Table 2: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-05, where DUC-05 was used as training set.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9915287494659424}, {"text": "recall (R)", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9034025222063065}, {"text": "F-measure (F)", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9619227051734924}, {"text": "DUC-05", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.9590317010879517}, {"text": "DUC-05", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9487829804420471}]}, {"text": " Table 3: ROUGE-2 recall (R) and F-measure (F) results  on DUC-05 (%). We used DUC-06 as training set.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9943287968635559}, {"text": "recall (R)", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9119454920291901}, {"text": "F-measure (F)", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.965432807803154}, {"text": "DUC-05", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.8919200301170349}, {"text": "DUC-06", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.9561469554901123}]}, {"text": " Table 4: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-06, where DUC-05 was used as training set.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9941158294677734}, {"text": "recall (R)", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9083676636219025}, {"text": "F-measure (F)", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9625378996133804}, {"text": "DUC-06", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.9603432416915894}, {"text": "DUC-05", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9516406655311584}]}, {"text": " Table 5: ROUGE-2 recall (R) and F-measure (F) re- sults (%) on DUC-07. DUC-05 was used as training  set for objective L 1 (S) + \u03bbR Q (S). DUC-05 and DUC- 06 were used as training sets for objective", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9840826392173767}, {"text": "recall (R)", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.885498583316803}, {"text": "F-measure (F) re- sults", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.9595394730567932}, {"text": "DUC-07", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9365164041519165}]}]}