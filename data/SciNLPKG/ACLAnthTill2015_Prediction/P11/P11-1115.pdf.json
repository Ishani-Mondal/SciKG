{"title": [{"text": "Knowledge Base Population: Successful Approaches and Challenges", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference.", "labels": [], "entities": [{"text": "Knowledge Base Population (KBP) track", "start_pos": 41, "end_pos": 78, "type": "DATASET", "confidence": 0.7031432475362506}, {"text": "Text Analysis Conference", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7348923484484354}]}, {"text": "The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts.", "labels": [], "entities": []}, {"text": "This is done through two tasks, Entity Linking-linking names in context to entities in the KB-and Slot Filling-adding information about an entity to the KB.", "labels": [], "entities": [{"text": "Entity Linking-linking names in context to entities in the KB-and Slot Filling-adding information about an entity to the KB", "start_pos": 32, "end_pos": 155, "type": "TASK", "confidence": 0.8606090827992088}]}, {"text": "A large source collection of newswire and web documents is provided from which systems are to discover information.", "labels": [], "entities": []}, {"text": "Attributes (\"slots\") derived from Wikipedia infoboxes are used to create the reference KB.", "labels": [], "entities": []}, {"text": "In this paper we provide an overview of the techniques which can serve as a basis fora good KBP system, layout the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges.", "labels": [], "entities": [{"text": "Information Extraction (IE) and Question Answering (QA)", "start_pos": 167, "end_pos": 222, "type": "TASK", "confidence": 0.7250637547536329}]}], "introductionContent": [{"text": "Traditional information extraction (IE) evaluations, such as the Message Understanding Conferences (MUC) and Automatic Content Extraction (ACE), assess the ability to extract information from individual documents in isolation.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8846720099449158}, {"text": "Message Understanding Conferences (MUC)", "start_pos": 65, "end_pos": 104, "type": "TASK", "confidence": 0.6834257046381632}, {"text": "Automatic Content Extraction (ACE)", "start_pos": 109, "end_pos": 143, "type": "TASK", "confidence": 0.7357164223988851}]}, {"text": "In practice, however, we may need to gather information about a person or organization that is scattered among the documents of a large collection.", "labels": [], "entities": []}, {"text": "This requires the ability to identify the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly in conflict, coming from these documents.", "labels": [], "entities": []}, {"text": "Furthermore, we may want to use the extracted information to augment an existing database.", "labels": [], "entities": []}, {"text": "This requires the ability to link individuals mentioned in a document, and information about these individuals, to entries in the database.", "labels": [], "entities": []}, {"text": "On the other hand, traditional Question Answering (QA) evaluations made limited efforts at disambiguating entities in queries (e.g.), and limited use of relation/event extraction in answer search (e.g..", "labels": [], "entities": [{"text": "Question Answering (QA) evaluations", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.8457464675108591}, {"text": "relation/event extraction", "start_pos": 153, "end_pos": 178, "type": "TASK", "confidence": 0.6529420390725136}, {"text": "answer search", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.7888568639755249}]}, {"text": "The Knowledge Base Population (KBP) shared task, conducted as part of the NIST Text Analysis Conference, aims to address and evaluate these capabilities, and bridge the IE and QA communities to promote research in discovering facts about entities and expanding a knowledge base with these facts.", "labels": [], "entities": [{"text": "NIST Text Analysis Conference", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.7327540665864944}]}, {"text": "KBP is done through two separate subtasks, Entity teams submitted results for one or both sub-tasks.", "labels": [], "entities": [{"text": "KBP", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5992521047592163}]}, {"text": "A variety of approaches have been proposed to address both tasks with considerable success; nevertheless, there are many aspects of the task that remain unclear.", "labels": [], "entities": []}, {"text": "What are the fundamental techniques used to achieve reasonable performance?", "labels": [], "entities": []}, {"text": "What is the impact of each novel method?", "labels": [], "entities": []}, {"text": "What types of problems are represented in the current KBP paradigm compared to traditional IE and QA?", "labels": [], "entities": []}, {"text": "In which way have the current testbeds and evaluation methodology affected our perception of the task difficulty?", "labels": [], "entities": []}, {"text": "Have we reached a performance ceiling with current state of the art techniques?", "labels": [], "entities": []}, {"text": "What are the remaining challenges and what are the possible ways to address these challenges?", "labels": [], "entities": []}, {"text": "In this paper we aim to answer some of these questions based on our detailed analysis of evaluation results.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section will summarize the tasks conducted at KBP 2010.", "labels": [], "entities": [{"text": "KBP 2010", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8604971170425415}]}, {"text": "The overall goal of KBP is to automatically identify salient and novel entities, link them to corresponding Knowledge Base (KB) entries (if the linkage exists), then discover attributes about the entities, and finally expand the KB with any new attributes.", "labels": [], "entities": []}, {"text": "In the Entity Linking task, given a person (PER), organization (ORG) or geo-political entity (GPE, a location with a government) query that consists of a name string and a background document containing that name string, the system is required to provide the ID of the KB entry to which the name refers; or NIL if there is no such KB entry.", "labels": [], "entities": [{"text": "Entity Linking task", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8732162714004517}, {"text": "NIL", "start_pos": 307, "end_pos": 310, "type": "METRIC", "confidence": 0.9264756441116333}]}, {"text": "The background document, drawn from the KBP corpus, serves to disambiguate ambiguous name strings.", "labels": [], "entities": [{"text": "KBP corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9274436831474304}]}, {"text": "In selecting among the KB entries, a system could make use of the Wikipedia text associated with each entry as well as the structured fields of each entry.", "labels": [], "entities": []}, {"text": "In addition, there was an optional task where the system could only make use of the structured fields; this was intended to be representative of applications where no backing text was available.", "labels": [], "entities": []}, {"text": "Each site could submit up to three runs with different parameters.", "labels": [], "entities": []}, {"text": "The goal of Slot Filling is to collect from the corpus information regarding certain attributes of an entity, which maybe a person or some type of organization.", "labels": [], "entities": [{"text": "Slot Filling", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9772515296936035}]}, {"text": "Each query in the Slot Filling task consists of the name of the entity, its type (person or organization), a background document containing the name (again, to disambiguate the query in case there are multiple entities with the same name), its node ID (if the entity appears in the knowledge base), and the attributes which need not be filled.", "labels": [], "entities": [{"text": "Slot Filling task", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.9530648986498514}]}, {"text": "Attributes are excluded if they are already filled in the reference database and can only take on a single value.", "labels": [], "entities": [{"text": "Attributes", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9644060134887695}]}, {"text": "Along with each slot fill, the system must provide the ID of a document which supports the correctness of this fill.", "labels": [], "entities": []}, {"text": "If the corpus does not provide any information fora given attribute, the system should generate a NIL response (and no document ID).", "labels": [], "entities": []}, {"text": "KBP2010 defined 26 types of attributes for persons (such as the age, birthplace, spouse, children, job title, and employing organization) and 16 types of attributes for organizations (such as the top employees, the founder, the year founded, the headquarters location, and subsidiaries).", "labels": [], "entities": [{"text": "KBP2010", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.944776177406311}]}, {"text": "Some of these attributes are specified as only taking a single value (e.g., birthplace), while some can take multiple values (e.g., top employees).", "labels": [], "entities": []}, {"text": "The reference KB includes hundreds of thousands of entities based on articles from an October 2008 dump of English Wikipedia which includes 818,741 nodes.", "labels": [], "entities": []}, {"text": "The source collection includes 1,286,609 newswire documents, 490,596 web documents and hundreds of transcribed spoken documents.", "labels": [], "entities": []}, {"text": "To score Entity Linking, we take each query and check whether the KB node ID (or NIL) returned by a system is corrector not.", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.8555217683315277}]}, {"text": "Then we compute the Micro-averaged Accuracy, computed across all queries.", "labels": [], "entities": [{"text": "Micro-averaged", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.9504978060722351}, {"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.7781134843826294}]}, {"text": "To score Slot Filling, we first pool all the system responses (as is done for information retrieval evaluations) together with a set of manuallyprepared slot fills.", "labels": [], "entities": [{"text": "Slot Filling", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9444495737552643}]}, {"text": "These responses are then assessed by hand.", "labels": [], "entities": []}, {"text": "Equivalent answers (such as \"Bill Clinton\" and \"William Jefferson Clinton\") are grouped into equivalence classes.", "labels": [], "entities": [{"text": "Bill Clinton\" and \"William Jefferson Clinton\")", "start_pos": 29, "end_pos": 75, "type": "DATASET", "confidence": 0.7609333627753787}]}, {"text": "Each system response is rated as correct, wrong, or redundant (a response which is equivalent to another response for the same slot or an entry already in the knowledge base).", "labels": [], "entities": []}, {"text": "Given these judgments, we count Correct = total number of non-NIL system output slots judged correct System = total number of non-NIL system output slots Reference = number of single-valued slots with a correct non-NIL response + number of equivalence classes for all list-", "labels": [], "entities": [{"text": "Correct", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9761034846305847}]}], "tableCaptions": [{"text": " Table 2. Impact of Semantic Features on Entity  Linking (Micro-Averaged Accuracy %)", "labels": [], "entities": [{"text": "Entity  Linking", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7163771688938141}, {"text": "Micro-Averaged Accuracy %)", "start_pos": 58, "end_pos": 84, "type": "METRIC", "confidence": 0.78461754322052}]}]}