{"title": [{"text": "Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering", "labels": [], "entities": [{"text": "Multi-document Summarization", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.7577072083950043}, {"text": "Question Answering", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7604975998401642}]}], "abstractContent": [{"text": "We propose anew method for query-oriented extractive multi-document summarization.", "labels": [], "entities": [{"text": "query-oriented extractive multi-document summarization", "start_pos": 27, "end_pos": 81, "type": "TASK", "confidence": 0.6471872180700302}]}, {"text": "To enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms.", "labels": [], "entities": []}, {"text": "We then formulate the sum-marization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words.", "labels": [], "entities": []}, {"text": "Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline using Maximal Marginal Relevance.", "labels": [], "entities": [{"text": "NTCIR ACLIA question answering test collections", "start_pos": 25, "end_pos": 72, "type": "DATASET", "confidence": 0.9018403192361196}, {"text": "F3-score", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9479932188987732}]}], "introductionContent": [{"text": "Automatic text summarization aims at reducing the amount of text the user has to read while preserving important contents, and has many applications in this age of digital information overload).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6470208317041397}]}, {"text": "In particular, query-oriented multi-document summarization is useful for helping the user satisfy his information need efficiently by gathering important pieces of information from multiple documents.", "labels": [], "entities": [{"text": "query-oriented multi-document summarization", "start_pos": 15, "end_pos": 58, "type": "TASK", "confidence": 0.5701173643271128}]}, {"text": "In this study, we focus on extractive summarization (, in particular, on sentence selection from a given set of source documents that contain relevant sentences.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.5193318426609039}]}, {"text": "One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences.", "labels": [], "entities": []}, {"text": "Hence, to enrich the information need representation, we build a co-occurrence graph to obtain words that augment the original query terms.", "labels": [], "entities": []}, {"text": "We call this method Query Snowball.", "labels": [], "entities": [{"text": "Query", "start_pos": 20, "end_pos": 25, "type": "TASK", "confidence": 0.888404905796051}, {"text": "Snowball", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.4802553653717041}]}, {"text": "Another challenge in sentence selection for query-oriented multi-document summarization is how to avoid redundancy so that diverse pieces of information (i.e. nuggets) can be covered.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7183544635772705}, {"text": "query-oriented multi-document summarization", "start_pos": 44, "end_pos": 87, "type": "TASK", "confidence": 0.5203700562318166}]}, {"text": "For penalizing redundancy across sentences, using single words as the basic unit may not always be appropriate, because different nuggets fora given information need often have many words in common.", "labels": [], "entities": [{"text": "penalizing redundancy", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.9427529871463776}]}, {"text": "shows an example of this word overlap problem from the NTCIR-8 ACLIA2 Japanese question answering test collection.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.6773512810468674}, {"text": "NTCIR-8 ACLIA2 Japanese question answering test collection", "start_pos": 55, "end_pos": 113, "type": "DATASET", "confidence": 0.8868814962250846}]}, {"text": "Here, two gold-standard nuggets for the question \"Sen to Chihiro no Kamikakushi (Spirited Away) is a fulllength animated movie from Japan.", "labels": [], "entities": []}, {"text": "The user wants to know how it was received overseas.\"", "labels": [], "entities": []}, {"text": "(in English translation) is shown.", "labels": [], "entities": []}, {"text": "Each nugget represents a particular award that the movie received, and the two Japanese nugget strings have as many as three words in common: \" (review/critic)\", \" (animation)\" and \" (award).\"", "labels": [], "entities": []}, {"text": "Thus, if we use single words as the basis for penalising redundancy in sentence selection, it would be difficult to cover both of these nuggets in the summary because of the word overlaps.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.6919436901807785}]}, {"text": "We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.9803655743598938}]}, {"text": "This problem is an optimization problem that maximizes the total score of words covered by a summary under a summary length limit.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: ACLIA dataset statistics", "labels": [], "entities": [{"text": "ACLIA dataset", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.8720289766788483}]}, {"text": " Table 2: ACLIA2 test data results", "labels": [], "entities": [{"text": "ACLIA2 test data", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8456037640571594}]}, {"text": " Table 3: F3-scores for each question type (ACLIA2 test)", "labels": [], "entities": [{"text": "F3-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9829164147377014}]}]}