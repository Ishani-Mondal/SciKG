{"title": [{"text": "Model-Portability Experiments for Textual Temporal Analysis", "labels": [], "entities": [{"text": "Textual Temporal Analysis", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.8077165087064108}]}], "abstractContent": [{"text": "We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms.", "labels": [], "entities": [{"text": "time expression recognition", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.7187536160151163}]}, {"text": "We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach.", "labels": [], "entities": []}, {"text": "We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia.", "labels": [], "entities": [{"text": "time expression recognition", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7235808372497559}]}, {"text": "We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus.", "labels": [], "entities": [{"text": "LWLM", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.9054170250892639}, {"text": "Reuters corpus", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9577238857746124}, {"text": "Wikipedia corpus", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.9713171124458313}]}, {"text": "We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9158962965011597}, {"text": "LWLM", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9458985924720764}, {"text": "WordNet", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9269478917121887}]}], "introductionContent": [{"text": "The recognition of time expressions such as April 2011, mid-September and early next week is a crucial first step for applications like question answering that must be able to handle temporally anchored queries.", "labels": [], "entities": [{"text": "question answering", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.8861493766307831}]}, {"text": "This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task, the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task ().", "labels": [], "entities": [{"text": "Message Understanding Conference named entity task", "start_pos": 97, "end_pos": 147, "type": "TASK", "confidence": 0.841879685719808}, {"text": "Automatic Content Extraction time normalization task", "start_pos": 153, "end_pos": 205, "type": "TASK", "confidence": 0.7050312558809916}, {"text": "TempEval 2010 time expression task", "start_pos": 250, "end_pos": 284, "type": "TASK", "confidence": 0.5956957340240479}]}, {"text": "Many researchers competed in these tasks, applying both rule-based and machine-learning approaches, and achieving F1 measures as high as 0.86 for recognizing temporal expressions.", "labels": [], "entities": [{"text": "F1", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9993627667427063}]}, {"text": "Yet inmost of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire.", "labels": [], "entities": []}, {"text": "Thus we know little about how well time expression recognition systems generalize to other sorts of text.", "labels": [], "entities": [{"text": "time expression recognition", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6596846580505371}]}, {"text": "We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia.", "labels": [], "entities": [{"text": "time recognizer", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.6504403799772263}, {"text": "TempEval 2010", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.8843958079814911}]}, {"text": "At the same time, we are interested in helping the model recognize more types of time expressions than are available explicitly in the newswire training data.", "labels": [], "entities": [{"text": "newswire training data", "start_pos": 135, "end_pos": 157, "type": "DATASET", "confidence": 0.8425948818524679}]}, {"text": "We therefore introduce a semisupervised approach for expanding the training data, where we take words from temporal expressions in the data, substitute these words with likely synonyms, and add the generated examples to the training set.", "labels": [], "entities": []}, {"text": "We select synonyms both via WordNet, and via predictions from the Latent Words Language Model (LWLM).", "labels": [], "entities": []}, {"text": "We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary.", "labels": [], "entities": [{"text": "TempEval, Reuters and Wikipedia test sets", "start_pos": 50, "end_pos": 91, "type": "DATASET", "confidence": 0.7842426385198321}]}], "datasetContent": [{"text": "The tested model is trained on the official TempEval 2010 training data with 53450 tokens and 2117 annotated TIMEX3 tokens.", "labels": [], "entities": [{"text": "TempEval 2010 training data", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.9259448796510696}]}, {"text": "For testing the portability of the model to other domains we annotated two small target domain document collections with TIMEX3 tags.", "labels": [], "entities": [{"text": "TIMEX3", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.748157799243927}]}, {"text": "The first corpus is 12 Reuters news articles from the Reuters corpus", "labels": [], "entities": [{"text": "Reuters news articles from the Reuters corpus", "start_pos": 23, "end_pos": 68, "type": "DATASET", "confidence": 0.8333236149379185}]}], "tableCaptions": [{"text": " Table 1: Precision, recall and F1 scores for all models on the source (TempEval 2010) and target (Reuters  and Wikipedia) domains. Bootstrapped models were asked to generate between one and ten additional train- ing examples per instance. The maximum P, R, F1 and the number of synonyms at which this maximum  was achieved are given in the P, R, F1 and # Syn rows. F1 scores more than 0.010 above the Basic Tem- pEval Model are marked in bold.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9957512617111206}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9962567090988159}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9987753033638}, {"text": "F1", "start_pos": 258, "end_pos": 260, "type": "METRIC", "confidence": 0.9963840246200562}, {"text": "F1", "start_pos": 347, "end_pos": 349, "type": "METRIC", "confidence": 0.990284264087677}, {"text": "F1", "start_pos": 366, "end_pos": 368, "type": "METRIC", "confidence": 0.9989401698112488}]}]}