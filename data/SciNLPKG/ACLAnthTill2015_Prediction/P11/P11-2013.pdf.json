{"title": [{"text": "Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision", "labels": [], "entities": [{"text": "Normalizing Text Messages", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8636060158411661}, {"text": "Supervision", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.8447368144989014}]}], "abstractContent": [{"text": "Most text message normalization approaches are based on supervised learning and rely on human labeled training data.", "labels": [], "entities": [{"text": "text message normalization", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.7674018144607544}]}, {"text": "In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type.", "labels": [], "entities": []}, {"text": "In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision.", "labels": [], "entities": [{"text": "letter transformation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.8104027211666107}]}, {"text": "Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits.", "labels": [], "entities": []}, {"text": "To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel web-based approach and performed character-level alignment for model training.", "labels": [], "entities": [{"text": "hand labeling", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.7492403090000153}, {"text": "character-level alignment", "start_pos": 175, "end_pos": 200, "type": "TASK", "confidence": 0.7449845969676971}]}, {"text": "Experiments on both Twitter and SMS messages show that our system significantly outperformed the state-of-the-art deletion-based abbreviation system and the jazzy spellchecker (absolute accuracy gain of 21.69% and 18.16% over jazzy spellchecker on the two test sets respectively).", "labels": [], "entities": [{"text": "accuracy gain", "start_pos": 186, "end_pos": 199, "type": "METRIC", "confidence": 0.9532037675380707}]}], "introductionContent": [{"text": "Recent years have witnessed the explosive growth of text message usage, including the mobile phone text messages (SMS), chat logs, emails, and status updates from the social network websites such as Twitter and Facebook.", "labels": [], "entities": []}, {"text": "These text message collections serve as valuable information sources, yet the nonstandard contents within them often degrade togetha tgthr togeda 2getha togather t0gether toqethaa 2gthr togehter togeter 2getter 2qetha togethor tagether 2gtr (6) the existing language processing systems, calling the need of text normalization before applying the traditional information extraction, retrieval, sentiment analysis, or summarization techniques.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 307, "end_pos": 325, "type": "TASK", "confidence": 0.7072577476501465}, {"text": "information extraction, retrieval, sentiment analysis", "start_pos": 358, "end_pos": 411, "type": "TASK", "confidence": 0.8007847496441433}, {"text": "summarization", "start_pos": 416, "end_pos": 429, "type": "TASK", "confidence": 0.9172365665435791}]}, {"text": "Text message normalization is also of crucial importance for building text-tospeech (TTS) systems, which need to determine pronunciation for nonstandard words.", "labels": [], "entities": [{"text": "Text message normalization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7388292551040649}]}, {"text": "Text message normalization aims to replace the non-standard tokens that carry significant meanings with the context-appropriate standard English words.", "labels": [], "entities": [{"text": "Text message normalization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7077979445457458}]}, {"text": "This is a very challenging task due to the vast amount and wide variety of existing nonstandard tokens.", "labels": [], "entities": []}, {"text": "We found more than 4 million distinct out-of-vocabulary tokens in the English tweets of the Edinburgh Twitter corpus (see Section 2.2).", "labels": [], "entities": [{"text": "Edinburgh Twitter corpus", "start_pos": 92, "end_pos": 116, "type": "DATASET", "confidence": 0.9647476077079773}]}, {"text": "shows examples of nonstandard tokens originated from the word \"together\".", "labels": [], "entities": []}, {"text": "We can see that some variants can be generated by dropping letters from the original word (\"tgthr\") or substituting letters with digit (\"2gether\"); however, many variants are generated by combining the letter insertion, deletion, and substitution operations (\"toqethaa\", \"2gthr\").", "labels": [], "entities": []}, {"text": "This shows that it is difficult to divide the nonstandard tokens into exclusive categories.", "labels": [], "entities": []}, {"text": "Among the literature of text normalization(for text messages or other domains),, employed the noisy channel model to find the most probable word sequence given the observed noisy message.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7956529557704926}]}, {"text": "Their approaches first classified the nonstandard tokens into various categories (e.g., abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category.", "labels": [], "entities": []}, {"text": "developed a hidden Markov model using hand annotated training data., Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word.", "labels": [], "entities": []}, {"text": "addressed the phonetic substitution problem by extending the initial letter-to-phone model., viewed the text message normalization as a statistical machine translation process from the texting language to standard English.", "labels": [], "entities": [{"text": "phonetic substitution", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7582893669605255}, {"text": "text message normalization", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.6877376834551493}, {"text": "statistical machine translation", "start_pos": 136, "end_pos": 167, "type": "TASK", "confidence": 0.679699699083964}]}, {"text": "experimented with the weighted finitestate machines for normalizing French SMS messages.", "labels": [], "entities": [{"text": "normalizing French SMS messages", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.8522328585386276}]}, {"text": "Most of the above approaches rely heavily on the hand annotated data and involve categorizing the nonstandard tokens in the first place, which gives rise to three problems: (1) the labeled data is very expensive and time consuming to obtain; (2) it is hard to establish a standard taxonomy for categorizing the tokens found in text messages; (3) the lack of optimized way to integrate various category-specific models often compromises the system performance, as confirmed by.", "labels": [], "entities": []}, {"text": "In this paper, we propose a general letter transformation approach that normalizes nonstandard tokens without categorizing them.", "labels": [], "entities": [{"text": "letter transformation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7368128895759583}]}, {"text": "A large set of noisy training word pairs were automatically collected via a novel web-based approach and aligned at the character level for model training.", "labels": [], "entities": []}, {"text": "The system was tested on both Twitter and SMS messages.", "labels": [], "entities": []}, {"text": "Results show that our system significantly outperformed the jazzy spellchecker and the state-of-the-art deletion-based abbreviation system, and also demonstrated good cross-domain portability.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the system performance on both Twitter and SMS message test sets.", "labels": [], "entities": [{"text": "SMS message test sets", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.6600493416190147}]}, {"text": "The SMS data was used in previous work.", "labels": [], "entities": [{"text": "SMS data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7377429008483887}]}, {"text": "It consists of 303 distinct nonstandard tokens and their corresponding dictionary words.", "labels": [], "entities": []}, {"text": "We developed our own Twitter message test set consisting of 6,150 tweets manually annotated via the Amazon Mechanical Turk.", "labels": [], "entities": []}, {"text": "3 to 6 turkers were required to convert the nonstandard tokens in the tweets to the standard English words.", "labels": [], "entities": []}, {"text": "We extract the nonstandard tokens whose most frequently normalized word consists of letters/digits/apostrophe, and is different from the token itself.", "labels": [], "entities": []}, {"text": "This results in 3,802 distinct nonstandard tokens that we use as the test set.", "labels": [], "entities": []}, {"text": "147 (3.87%) of them have more than one corresponding standard English words.", "labels": [], "entities": []}, {"text": "Similar to prior work, we use isolated nonstandard tokens without any context, that is, the LM probabilities P (S) are based on unigrams.", "labels": [], "entities": []}, {"text": "We compare our system against three approaches.", "labels": [], "entities": []}, {"text": "The first one is a comprehensive list of chat slangs, abbreviations, and acronyms collected by InternetSlang.com; it contains normalized word forms for 6,105 commonly used slangs.", "labels": [], "entities": []}, {"text": "The second is the word-abbreviation lookup table generated by the supervised deletion-based abbreviation approach proposed in.", "labels": [], "entities": []}, {"text": "It contains 477,941 (word, abbreviation) pairs automatically generated for 54,594 CMU dictionary words.", "labels": [], "entities": []}, {"text": "The third is the jazzy spellchecker based on the Aspell algorithm (.", "labels": [], "entities": []}, {"text": "It integrates the phonetic matching algorithm (DoubleMetaphone) and Levenshtein distance that enables the interchanging of two adjacent letters, and changing/deleting/adding of letters.", "labels": [], "entities": [{"text": "phonetic matching", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7074147760868073}, {"text": "Levenshtein distance", "start_pos": 68, "end_pos": 88, "type": "METRIC", "confidence": 0.7280821204185486}]}, {"text": "The system performance is measured using the n-best accuracy (n=1,3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.7181510329246521}]}, {"text": "For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: N-best performance on Twitter and SMS data  sets using different systems.", "labels": [], "entities": [{"text": "N-best", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9544215798377991}, {"text": "SMS data  sets", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.7269709606965383}]}]}