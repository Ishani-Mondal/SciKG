{"title": [{"text": "Is Machine Translation Ripe for Cross-lingual Sentiment Classification?", "labels": [], "entities": [{"text": "Machine Translation Ripe", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7282927135626475}, {"text": "Cross-lingual Sentiment Classification", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.797056237856547}]}], "abstractContent": [{"text": "Recent advances in Machine Translation (MT) have brought forth anew paradigm for building NLP applications in low-resource scenarios.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.8525067746639252}]}, {"text": "To build a sentiment classifier fora language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text.", "labels": [], "entities": []}, {"text": "This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7293073683977127}]}, {"text": "Various prior work have achieved positive results using this approach.", "labels": [], "entities": []}, {"text": "In this opinion piece, we take a step back and make some general statements about cross-lingual adaptation problems.", "labels": [], "entities": [{"text": "cross-lingual adaptation", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.7592750489711761}]}, {"text": "First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.950762152671814}, {"text": "accuracy degradation", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.9751620888710022}]}, {"text": "Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolin-gual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered.", "labels": [], "entities": []}, {"text": "This paper will describe a series of carefully-designed experiments that led us to these conclusions.", "labels": [], "entities": []}, {"text": "1 Summary Question 1: If MT gave perfect translations (seman-tically), do we still have a domain adaptation challenge in cross-lingual sentiment classification?", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.8574870228767395}, {"text": "cross-lingual sentiment classification", "start_pos": 121, "end_pos": 159, "type": "TASK", "confidence": 0.7765524983406067}]}, {"text": "The reason is that while many translations of a word maybe valid, the MT system might have a systematic bias.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9533390998840332}]}, {"text": "For example, the word \"awe-some\" might be prevalent in English reviews, but in translated reviews, the word \"excellent\" is generated instead.", "labels": [], "entities": []}, {"text": "From the perspective of MT, this translation is correct and preserves sentiment polarity.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.7037310600280762}, {"text": "sentiment polarity", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7814878821372986}]}, {"text": "But from the perspective of a classifier, there is a domain mis-match due to differences in word distributions.", "labels": [], "entities": []}, {"text": "Question 2: Can we apply standard adaptation algorithms developed for other (monolingual) adaptation problems to cross-lingual adaptation?", "labels": [], "entities": []}, {"text": "Answer: No. It appears that the interaction between target unlabeled data and source data can be rather unexpected in the case of cross-lingual adaptation.", "labels": [], "entities": []}, {"text": "We do not know the reason, but our experiments show that the accuracy of adaptation algorithms in cross-lingual scenarios have much higher variance than monolingual scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9989051818847656}]}, {"text": "The goal of this opinion piece is to argue the need to better understand the characteristics of domain adaptation in cross-lingual problems.", "labels": [], "entities": []}, {"text": "We invite the reader to disagree with our conclusion (that the true barrier to good performance is not insufficient MT quality, but inappropriate domain adaptation methods).", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.975538969039917}]}, {"text": "Here we present a series of experiments that led us to this conclusion.", "labels": [], "entities": []}, {"text": "First we describe the experiment design (\u00a72) and baselines (\u00a73), before answering Question 1 (\u00a74) and Question 2 (\u00a75).", "labels": [], "entities": []}, {"text": "2 Experiment Design The cross-lingual setup is this: we have labeled data from source domain Sand wish to build a sentiment classifier for target domain T.", "labels": [], "entities": []}, {"text": "Domain mismatch can arise from language differences (e.g. English vs. translated text) or market differences (e.g. DVD vs. Book reviews).", "labels": [], "entities": []}, {"text": "Our experiments will involve fixing 429", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The cross-lingual setup is this: we have labeled data from source domain Sand wish to build a sentiment classifier for target domain T . Domain mismatch can arise from language differences (e.g. English vs. translated text) or market differences (e.g. DVD vs. Book reviews).", "labels": [], "entities": []}, {"text": "Our experiments will involve fixing 429 T to a common testset and varying S.", "labels": [], "entities": [{"text": "S", "start_pos": 74, "end_pos": 75, "type": "METRIC", "confidence": 0.9665632247924805}]}, {"text": "This allows us to experiment with different settings for adaptation.", "labels": [], "entities": []}, {"text": "We use the Amazon review dataset of Prettenhofer (2010) 1 , due to its wide range of languages (English, Japanese, French, German) and markets (music, DVD, books).", "labels": [], "entities": [{"text": "Amazon review dataset of Prettenhofer (2010) 1", "start_pos": 11, "end_pos": 57, "type": "DATASET", "confidence": 0.9229964878824022}]}, {"text": "Unlike Prettenhofer (2010), we reverse the direction of cross-lingual adaptation and consider English as target.", "labels": [], "entities": [{"text": "cross-lingual adaptation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7154668718576431}]}, {"text": "English is not a low-resource language, but this setting allows for more comparisons.", "labels": [], "entities": []}, {"text": "Each source dataset has 2000 reviews, equally balanced between positive and negative.", "labels": [], "entities": []}, {"text": "The target has 2000 test samples, large unlabeled data (25k, 30k, 50k samples respectively for Music, DVD, and Books), and an additional 2000 labeled data reserved for oracle experiments.", "labels": [], "entities": []}, {"text": "Texts in JP, FR, and DE are translated word-by-word into English with Google Translate.", "labels": [], "entities": [{"text": "FR", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.4944668412208557}, {"text": "DE", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.6702507138252258}]}, {"text": "We perform three sets of experiments, shown in. lists all the results; we will interpret them in the following sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Test accuracies (%) for English Music/DVD/Book reviews. Each column is an adaptation scenario using  different source data. The source data may vary by language or by market. For example, the first row shows that for  the target of Music-EN, the accuracy of a SVM trained on translated JP reviews (in the same market) is 68.5, while the  accuracy of a SVM trained on DVD reviews (in the same language) is 76.8. \"Oracle\" indicates training on the same  market and same language domain as the target. \"JP+FR+DE\" indicates the concatenation of JP, FR, DE as source  data. Boldface shows the winner of Supervised vs. Adapted.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.49681052565574646}, {"text": "English Music/DVD/Book reviews", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.5195665189198085}, {"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.9986308217048645}, {"text": "accuracy", "start_pos": 348, "end_pos": 356, "type": "METRIC", "confidence": 0.997886598110199}, {"text": "FR+DE\"", "start_pos": 513, "end_pos": 519, "type": "METRIC", "confidence": 0.8898504078388214}, {"text": "FR", "start_pos": 555, "end_pos": 557, "type": "METRIC", "confidence": 0.9469801783561707}]}]}