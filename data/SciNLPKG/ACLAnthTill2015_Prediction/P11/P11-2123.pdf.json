{"title": [{"text": "Improving Dependency Parsing with Semantic Classes", "labels": [], "entities": [{"text": "Improving Dependency Parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8571483294169108}]}], "abstractContent": [{"text": "This paper presents the introduction of WordNet semantic classes in a dependency parser, obtaining improvements on the full Penn Treebank for the first time.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.9932948052883148}]}, {"text": "We tried different combinations of some basic semantic classes and word sense disambigua-tion algorithms.", "labels": [], "entities": []}, {"text": "Our experiments show that selecting the adequate combination of semantic features on development data is key for success.", "labels": [], "entities": []}, {"text": "Given the basic nature of the semantic classes and word sense disam-biguation algorithms used, we think there is ample room for future improvements.", "labels": [], "entities": []}, {"text": "1 Introduction Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994).", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9722922444343567}, {"text": "parsing", "start_pos": 229, "end_pos": 236, "type": "TASK", "confidence": 0.9693772792816162}]}, {"text": "Although there have been some significant results (see Section 2), this issue continues to be elusive.", "labels": [], "entities": []}, {"text": "In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8452387154102325}]}, {"text": "We present a set of experiments using semantic classes in dependency parsing of the Penn Tree-bank (PTB).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8021771609783173}, {"text": "Penn Tree-bank (PTB)", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.9697487592697144}]}, {"text": "We extend the tests made in Agirre et al.", "labels": [], "entities": []}, {"text": "(2008), who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 164, "end_pos": 184, "type": "TASK", "confidence": 0.8354393541812897}]}, {"text": "As our baseline parser, we use MaltParser (Nivre, 2006).", "labels": [], "entities": [{"text": "MaltParser (Nivre, 2006)", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.8624706168969473}]}, {"text": "We will evaluate the parser on both the full PTB (Marcus et al.", "labels": [], "entities": [{"text": "PTB", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.6950979232788086}]}, {"text": "1993) and on a sense-annotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al.", "labels": [], "entities": [{"text": "Brown Corpus portion of PTB", "start_pos": 45, "end_pos": 72, "type": "DATASET", "confidence": 0.9263577461242676}]}, {"text": "2 Related Work Agirre et al.", "labels": [], "entities": []}, {"text": "(2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes.", "labels": [], "entities": []}, {"text": "This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized.", "labels": [], "entities": [{"text": "Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003", "start_pos": 87, "end_pos": 147, "type": "TASK", "confidence": 0.5781653265158335}]}, {"text": "This simple method allowed incorporating lexical semantic information into the parser.", "labels": [], "entities": []}, {"text": "They tested the parsers in both a full parsing and a PP attachment context.", "labels": [], "entities": []}, {"text": "The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.", "labels": [], "entities": []}, {"text": "This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9450938105583191}, {"text": "Penn Treebank", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9937210083007812}, {"text": "parsing", "start_pos": 117, "end_pos": 124, "type": "TASK", "confidence": 0.9674839377403259}]}, {"text": "Collins (2000) tested a combined parsing/word sense disambiguation model based in WordNet which did not obtain improvements in parsing.", "labels": [], "entities": [{"text": "parsing/word sense disambiguation", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.7172525644302368}, {"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.951856791973114}]}, {"text": "(2008) presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features.", "labels": [], "entities": []}, {"text": "They demonstrate the effectiveness of the approach in a series of dependency parsing experiments on PTB and the Prague Dependency Treebank, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7260481864213943}, {"text": "PTB", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.9812361001968384}, {"text": "Prague Dependency Treebank", "start_pos": 112, "end_pos": 138, "type": "DATASET", "confidence": 0.9553702473640442}]}, {"text": "(2009) also experiment with the same method combined with semi-supervised learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9717034697532654}, {"text": "parsing", "start_pos": 214, "end_pos": 221, "type": "TASK", "confidence": 0.9714267253875732}]}, {"text": "Although there have been some significant results (see Section 2), this issue continues to be elusive.", "labels": [], "entities": []}, {"text": "In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8452387154102325}]}, {"text": "We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8041943609714508}, {"text": "Penn Treebank (PTB)", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.9749630451202392}]}, {"text": "We extend the tests made in, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 185, "end_pos": 205, "type": "TASK", "confidence": 0.8351667523384094}]}, {"text": "As our baseline parser, we use MaltParser).", "labels": [], "entities": []}, {"text": "We will evaluate the parser on both the full PTB () and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in.", "labels": [], "entities": [{"text": "PTB", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9348728060722351}, {"text": "Brown Corpus portion of PTB", "start_pos": 90, "end_pos": 117, "type": "DATASET", "confidence": 0.9103597402572632}]}], "datasetContent": [{"text": "In this section we will briefly describe the datadriven parser used for the experiments (subsection 3.1), followed by the PTB-based datasets (subsection 3.2).", "labels": [], "entities": [{"text": "PTB-based datasets", "start_pos": 122, "end_pos": 140, "type": "DATASET", "confidence": 0.974125862121582}]}, {"text": "Finally, we will describe the types of semantic representation used in the experiments.", "labels": [], "entities": []}, {"text": "We used two different datasets: the full PTB and the Semcor/PTB intersection ().", "labels": [], "entities": [{"text": "PTB", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.9501776099205017}, {"text": "Semcor/PTB intersection", "start_pos": 53, "end_pos": 76, "type": "DATASET", "confidence": 0.7322473078966141}]}, {"text": "The full PTB allows for comparison with the stateof-the-art, and we followed the usual train-test split.", "labels": [], "entities": [{"text": "PTB", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7434912323951721}]}, {"text": "The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing.", "labels": [], "entities": []}, {"text": "We use the same train-test split of, with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data.", "labels": [], "entities": []}, {"text": "This dataset is available on request to the research community.", "labels": [], "entities": []}, {"text": "We will evaluate the parser via Labeled Attachment Score (LAS).", "labels": [], "entities": [{"text": "Labeled Attachment Score (LAS)", "start_pos": 32, "end_pos": 62, "type": "METRIC", "confidence": 0.8993111153443655}]}, {"text": "We will use Bikel's randomized parsing evaluation comparator to test the statistical significance of the results using word sense information, relative to the respective baseline parser using only standard features.", "labels": [], "entities": []}, {"text": "We used PennConverter ( to convert constituent trees in the Penn Treebank annotation style into dependency trees.", "labels": [], "entities": [{"text": "PennConverter", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9697777032852173}, {"text": "Penn Treebank annotation style", "start_pos": 60, "end_pos": 90, "type": "DATASET", "confidence": 0.973078042268753}]}, {"text": "Although in general the results from parsing Pennconverter's output are lower than with other conversions, claim that this conversion is better suited for semantic processing, with a richer structure and a more finegrained set of dependency labels.", "labels": [], "entities": [{"text": "parsing Pennconverter", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7485655546188354}]}, {"text": "For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing () as our baseline.", "labels": [], "entities": [{"text": "CoNLL 2007 Shared Task on Dependency Parsing", "start_pos": 71, "end_pos": 115, "type": "TASK", "confidence": 0.8587151765823364}]}], "tableCaptions": [{"text": " Table 1. Evaluation results on the test set for the  Semcor-Penn intersection. Individual semantic  features and best combination.", "labels": [], "entities": []}, {"text": " Table 1. Evaluation results (LAS) on the test  set for the full PTB. Individual features and  best combination.", "labels": [], "entities": [{"text": "Evaluation results (LAS)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7568479299545288}, {"text": "PTB", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.5733415484428406}]}]}