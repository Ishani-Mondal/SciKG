{"title": [{"text": "Exploring Entity Relations for Named Entity Disambiguation", "labels": [], "entities": [{"text": "Named Entity Disambiguation", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6854379574457804}]}], "abstractContent": [{"text": "Named entity disambiguation is the task of linking an entity mention in a text to the correct real-world referent predefined in a knowledge base, and is a crucial subtask in many areas like information retrieval or topic detection and tracking.", "labels": [], "entities": [{"text": "Named entity disambiguation is the task of linking an entity mention in a text to the correct real-world referent predefined in a knowledge base", "start_pos": 0, "end_pos": 144, "type": "Description", "confidence": 0.7750161625444889}, {"text": "information retrieval", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.7696722745895386}, {"text": "topic detection and tracking", "start_pos": 215, "end_pos": 243, "type": "TASK", "confidence": 0.7568854317069054}]}, {"text": "Named entity disambigua-tion is challenging because entity mentions can be ambiguous and an entity can be refer-enced by different surface forms.", "labels": [], "entities": [{"text": "Named entity disambigua-tion", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.608016441265742}]}, {"text": "We present an approach that exploits Wikipedia relations between entities co-occurring with the ambiguous form to derive a range of novel features for classifying candidate referents.", "labels": [], "entities": []}, {"text": "We find that our features improve disambiguation results significantly over a strong popularity baseline, and are especially suitable for recognizing entities not contained in the knowledge base.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9560492038726807}]}, {"text": "Our system achieves state-of-the-art results on the TAC-KBP 2009 dataset.", "labels": [], "entities": [{"text": "TAC-KBP 2009 dataset", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.9489941398302714}]}], "introductionContent": [{"text": "Identifying the correct real-world referents of named entities (NE) mentioned in text (such as people, organizations, and geographic locations) plays an important role in various natural language processing and information retrieval tasks.", "labels": [], "entities": [{"text": "Identifying the correct real-world referents of named entities (NE) mentioned in text (such as people, organizations, and geographic locations)", "start_pos": 0, "end_pos": 143, "type": "TASK", "confidence": 0.8509635758399964}, {"text": "information retrieval", "start_pos": 211, "end_pos": 232, "type": "TASK", "confidence": 0.680945560336113}]}, {"text": "The goal of Named Entity Disambiguation (NED) is to label a surface form denoting an NE in text with one of multiple predefined NEs from a knowledge base (KB), or to detect that the surface form refers to an out-of-KB entity, which is known as NIL detection.", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8101063370704651}, {"text": "NIL detection", "start_pos": 244, "end_pos": 257, "type": "TASK", "confidence": 0.8000211119651794}]}, {"text": "NED has become a popular research field recently, as the growth of large-scale publicly available encyclopedic knowledge resources such as Wikipedia has stimulated research on linking NEs in text to their entries in these KBs (.", "labels": [], "entities": []}, {"text": "The disambiguation of named entities raises several challenges: Surface forms in text can be ambiguous, and the same entity can be referred to by different surface forms.", "labels": [], "entities": []}, {"text": "For example, the surface form \"George Bush\" may denote either of two former U.S. presidents, and the later president can be referred to by \"George W. Bush\" or with his nickname \"Dubya\".", "labels": [], "entities": []}, {"text": "Thus, a many-to-many mapping between surface forms and entities has to be resolved.", "labels": [], "entities": []}, {"text": "In addition, entity mentions may not have a matching entity in the KB, which is often the case for nonpopular entities.", "labels": [], "entities": []}, {"text": "Typical approaches to NED combine the use of document context knowledge with entity information stored in the KB in order to disambiguate entities.", "labels": [], "entities": [{"text": "NED", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9417802691459656}]}, {"text": "Many systems represent document context and KB information as word or concept vectors, and rank entities using vector space similarity metrics.", "labels": [], "entities": []}, {"text": "Other authors employ supervised machine learning algorithms to classify or rank candidate entities (.", "labels": [], "entities": []}, {"text": "Common features include popularity metrics based on Wikipedia's graph structure or on name mention frequency (, similarity metrics exploring Wikipedia's concept relations, and string similarity features.", "labels": [], "entities": []}, {"text": "Recent work also addresses the task of NIL detection.", "labels": [], "entities": [{"text": "NIL detection", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9698978364467621}]}, {"text": "While previous research has largely focused on disambiguating each entity mention in a documentseparately, we explore an approach that is driven by the observation that entities normally co-occur in texts.", "labels": [], "entities": []}, {"text": "Documents often discuss several different entities related to each other, e.g. a news article may report on a meeting of political leaders from different countries.", "labels": [], "entities": []}, {"text": "Analogously, entries in a KB such as Wikipedia are linked to other, related entries.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9534154534339905}]}, {"text": "Our Contributions In this paper, we evaluate a range of novel disambiguation features that exploit the relations between NEs identified in a document and in the KB.", "labels": [], "entities": []}, {"text": "Our goal is to explore the usefulness of Wikipedia's link structure as source of relations between entities.", "labels": [], "entities": []}, {"text": "We propose a method for candidate selection that is based on an inverted index of surface forms and entities (Section 3.2).", "labels": [], "entities": [{"text": "candidate selection", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6757190674543381}]}, {"text": "Instead of a bag-of-words approach we use co-occurring NEs in text for describing an ambiguous surface form.", "labels": [], "entities": []}, {"text": "We introduce several different disambiguation features that exploit the relations between entities derived from the graph structure of Wikipedia (Section 3.3).", "labels": [], "entities": []}, {"text": "Finally, we combine our disambiguation features and achieve state-of-the-art results with a Support Vector Machine (SVM) classifier (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments on the 2009 Knowledge Base Population (KBP) dataset of the Text Analysis Conference (TAC)).", "labels": [], "entities": [{"text": "2009 Knowledge Base Population (KBP) dataset of the Text Analysis Conference (TAC))", "start_pos": 34, "end_pos": 117, "type": "DATASET", "confidence": 0.8854238837957382}]}, {"text": "The dataset consists of a KB derived from a 2008 snapshot of the English Wikipedia, and a collection of newswire, weblog and newsgroup documents.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.8453875184059143}]}, {"text": "A set of 3904 surface form-document pairs (queries) is constructed from these sources, encompassing 560 unique entities.", "labels": [], "entities": []}, {"text": "The majority of queries (57%) are NIL queries, of the KB queries, 69% are for organizations and 15% each for persons and geopolitical entities.", "labels": [], "entities": []}, {"text": "For each query the surface form appearing in the given document has to be disambiguated against the KB.", "labels": [], "entities": []}, {"text": "We randomly split the 3904 queries to perform 10-fold cross-validation, and stratify the resulting folds to ensure a similar distribution of KB and NIL queries in our training data.", "labels": [], "entities": []}, {"text": "After normalizing feature values to be in, we train a candidate and a NIL classifier on 90% of the queries in each iteration, and test using the remaining 10%.", "labels": [], "entities": []}, {"text": "Results reported in this paper are then averaged across the test folds.", "labels": [], "entities": []}, {"text": "compares the micro-averaged accuracy of our approach on KB and NIL queries for different feature sets, and lists the results of two other stateof-the-art systems (, as well as the best and median reported performance of the 2009 TAC-KBP track . Micro-averaged accuracy is calculated as the fraction of correct queries, and is the official TAC-KBP evaluation measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8287593126296997}, {"text": "Micro-averaged accuracy", "start_pos": 245, "end_pos": 268, "type": "METRIC", "confidence": 0.6758957803249359}]}, {"text": "As a baseline we use a feature set consisting of the BOW and SFP features.", "labels": [], "entities": [{"text": "BOW", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.8304441571235657}, {"text": "SFP", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.685096025466919}]}, {"text": "The best feature set in our experiments comprises all features except for the LC-all and CR features.", "labels": [], "entities": [{"text": "CR", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9603907465934753}]}, {"text": "Our best accuracy of 0.84 compares favorably with other state-of-the-art systems on this dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995824694633484}]}, {"text": "Using the best feature set improves the disambiguation accuracy by 6.2% over the baseline feature set, which is significant at p = 0.05.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9850468635559082}]}, {"text": "For KB queries our system's accuracy is higher than that of Dredze et al., but lower than the accuracy reported by Zheng et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996882677078247}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9990701079368591}]}, {"text": "One striking result is the high accuracy for NIL queries, where our approach outperforms all previously reported results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9994308352470398}, {"text": "NIL queries", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.6799065172672272}]}, {"text": "displays the performance of our approach when iteratively adding features.", "labels": [], "entities": []}, {"text": "We can see that the novel entity features contribute to a higher overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9992197751998901}]}, {"text": "Including the candidate selection score (CS) improves accuracy by 3.6% over the baseline.", "labels": [], "entities": [{"text": "candidate selection score (CS)", "start_pos": 14, "end_pos": 44, "type": "METRIC", "confidence": 0.7595738371213278}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9994242191314697}]}, {"text": "The Wikipedia link-based features provide additional gains, however differences are quite 21 small (1.0 \u2212 1.5%).", "labels": [], "entities": []}, {"text": "We find that there is hardly any difference in performance between using the LCall and LC-in features.", "labels": [], "entities": [{"text": "LCall", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8864346742630005}]}, {"text": "The Candidate Rank (CR) feature slightly decreases the overall accuracy.", "labels": [], "entities": [{"text": "Candidate Rank (CR)", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.848001378774643}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9988803267478943}]}, {"text": "A manual inspection of the CR feature shows that often candidates cannot be distinguished by the classifier because they are assigned the same PageRank scores.", "labels": [], "entities": [{"text": "CR", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.8934317827224731}]}, {"text": "We assume this results from our use of uniform priors for the edges and vertices of the document graphs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Micro-averaged accuracy for TAC-KBP 2009  data compared for different feature sets. The best feature  set contains all features except for LC-all and CR. Our  system outperforms previously reported results on NIL  queries, and compares favorably on all queries.", "labels": [], "entities": [{"text": "Micro-averaged", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9123753309249878}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8952186703681946}, {"text": "TAC-KBP 2009  data", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.8645780682563782}]}]}