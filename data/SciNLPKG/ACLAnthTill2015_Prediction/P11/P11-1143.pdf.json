{"title": [{"text": "Improving Question Recommendation by Exploiting Information Need", "labels": [], "entities": [{"text": "Improving Question Recommendation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9114834864934286}]}], "abstractContent": [{"text": "In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users' information needs.", "labels": [], "entities": [{"text": "question recommendation from large archives of community question answering", "start_pos": 40, "end_pos": 115, "type": "TASK", "confidence": 0.7931210862265693}]}, {"text": "Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation.", "labels": [], "entities": []}, {"text": "We show that translation model can be effectively utilized to predict the information need given only the user's query question.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9728773832321167}]}, {"text": "Experiments show that the proposed information need prediction approach can improve the performance of question recommendation.", "labels": [], "entities": [{"text": "information need prediction", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6443615655104319}, {"text": "question recommendation", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.8186047077178955}]}], "introductionContent": [{"text": "There has recently been a rapid growth in the number of community question answering (CQA) services such as Yahoo!", "labels": [], "entities": [{"text": "question answering (CQA)", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.83036789894104}]}, {"text": "Answers 1 , Askville and WikiAnswer 3 where people answer questions posted by other users.", "labels": [], "entities": [{"text": "Askville", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.9214743971824646}]}, {"text": "These CQA services have built up very large archives of questions and their answers.", "labels": [], "entities": []}, {"text": "They provide a valuable resource for question answering research. is an example from Yahoo!", "labels": [], "entities": [{"text": "question answering research.", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.9311321377754211}]}, {"text": "In the CQA archives, the title part is the user's query question, and the user's information need is usually expressed as natural language statements mixed with questions expressing their interests in the question body part.", "labels": [], "entities": [{"text": "CQA archives", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9612202942371368}]}, {"text": "In order to avoid the lag time involved with waiting fora personal response and to enable high quali-ty answers from the archives to be retrieved, we need to search CQA archives of previous questions that are closely associated with answers.", "labels": [], "entities": [{"text": "CQA archives", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9557474851608276}]}, {"text": "If a question is found to be interesting to the user, then a previous answer can be provided with very little delay.", "labels": [], "entities": []}, {"text": "Question search and question recommendation are proposed to facilitate finding highly relevant or potentially interesting questions.", "labels": [], "entities": [{"text": "Question search", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7575473785400391}, {"text": "question recommendation", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7896853387355804}]}, {"text": "Given a user's question as the query, question search tries to return the most semantically similar questions from the question archives.", "labels": [], "entities": []}, {"text": "As the complement of question search, we define question recommendation as recommending questions whose information need is the same or similar to the user's original question.", "labels": [], "entities": []}, {"text": "For example, the question \"What aspects of my computer do I need to upgrade ...\" with the information need \"...", "labels": [], "entities": []}, {"text": "making a skate movie, my computer freezes, ...\" and the question \"What is the most cost effective way to expend memory space ...\" with information need \"...", "labels": [], "entities": []}, {"text": "in need of more space for music and pictures ...\" are both good recommendation questions for the user in.", "labels": [], "entities": []}, {"text": "So the recommended questions are not necessarily identical or similar to the query question.", "labels": [], "entities": []}, {"text": "In this paper, we discuss methods for question recommendation based on using the similarity between information need in the archive.", "labels": [], "entities": [{"text": "question recommendation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.8086957037448883}]}, {"text": "We also propose two models to predict the information need based on the query question even if there's no information need expressed in the body of the question.", "labels": [], "entities": []}, {"text": "We show that with the proposed models it is possible to recommend questions that have the same or similar information need. lows.", "labels": [], "entities": []}, {"text": "In section 2, we briefly describe the related work on question search and recommendation.", "labels": [], "entities": [{"text": "question search and recommendation", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.8319630175828934}]}, {"text": "Section 3 addresses in detail how we measure the similarity between short texts.", "labels": [], "entities": []}, {"text": "Section 4 describes two models for information need prediction that we use for the experiment.", "labels": [], "entities": [{"text": "information need prediction", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6589299142360687}]}, {"text": "Section 5 tests the performance of the proposed models for the task of question recommendation.", "labels": [], "entities": [{"text": "question recommendation", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.785684734582901}]}, {"text": "Section 7 is the conclusion of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each question (query question) in 'Test t' or 'Test c', we used the words in the question title part as the main search query and the other words in the information need part as search query expansion to retrieve candidate recommended questions from Yahoo!", "labels": [], "entities": []}, {"text": "We obtained an average of 154 resolved questions under 'travel' or 'computers&internet' category, and three assessors were involved in the manual judgments.", "labels": [], "entities": []}, {"text": "Given a question returned by a recommendation 1429 method, two assessors are asked to label it with 'good' or 'bad'.", "labels": [], "entities": []}, {"text": "The third assessor will judge the conflicts.", "labels": [], "entities": []}, {"text": "The assessors are also asked to read the information need and answer parts.", "labels": [], "entities": []}, {"text": "If a recommended question is considered to express the same or similar information need, the assessor will label it 'good'; otherwise, the assessor will label it as 'bad'.", "labels": [], "entities": []}, {"text": "Three measures for evaluating the recommendation performance are utilized.", "labels": [], "entities": []}, {"text": "They are Mean Reciprocal Rank (MRR), top five prediction accuracy (precision@5) and top ten prediction accuracies (precision@10) (.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 9, "end_pos": 35, "type": "METRIC", "confidence": 0.961534341176351}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.7105516195297241}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9509646892547607}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9497305154800415}]}, {"text": "In MRR the reciprocal rank of a query question is the multiplicative inverse of the rank of the first 'good' recommended question.", "labels": [], "entities": [{"text": "MRR", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.7319456934928894}]}, {"text": "The top five prediction accuracy fora query question is the number of 'good' recommended questions out of the top five ranked questions and the top ten accuracy is calculated out of the top ten ranked questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.5197438597679138}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9752137064933777}]}], "tableCaptions": [{"text": " Table 2: Question recommendation results without information need prediction", "labels": [], "entities": [{"text": "Question recommendation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8057377636432648}]}, {"text": " Table 3: Question recommendation results with information need predicted by translation model", "labels": [], "entities": [{"text": "Question recommendation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8046220242977142}]}]}