{"title": [{"text": "Does Size Matter -How Much Data is Required to Train a REG Algorithm?", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we investigate how much data is required to train an algorithm for attribute selection, a subtask of Referring Expressions Generation (REG).", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7141712605953217}, {"text": "Referring Expressions Generation (REG)", "start_pos": 115, "end_pos": 153, "type": "TASK", "confidence": 0.7686959058046341}]}, {"text": "To enable comparison between different-sized training sets, a systematic training method was developed.", "labels": [], "entities": []}, {"text": "The results show that depending on the complexity of the domain, training on 10 to 20 items may already lead to a good performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "There are many ways in which we can refer to objects and people in the real world.", "labels": [], "entities": []}, {"text": "A chair, for example, can be referred to as red, large, or seen from the front, while men maybe singled out in terms of their pogonotrophy (facial hairstyle), clothing and many other attributes.", "labels": [], "entities": []}, {"text": "This poses a problem for algorithms that automatically generate referring expressions: how to determine which attributes to use?", "labels": [], "entities": []}, {"text": "One solution is to assume that some attributes are preferred over others, and this is indeed what many Referring Expressions Generation (REG) algorithms do.", "labels": [], "entities": [{"text": "Referring Expressions Generation (REG)", "start_pos": 103, "end_pos": 141, "type": "TASK", "confidence": 0.7725322792927424}]}, {"text": "A classic example is the Incremental Algorithm (IA), which postulates the existence of a complete ranking of relevant attributes.", "labels": [], "entities": [{"text": "Incremental Algorithm (IA)", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.618131422996521}]}, {"text": "The IA essentially iterates through this list of preferred attributes, selecting an attribute for inclusion in a referring expression if it helps singling out the target from the other objects in the scene (the distractors).", "labels": [], "entities": [{"text": "IA", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8986219167709351}]}, {"text": "Crucially, Dale and Reiter do not specify how the ranking of attributes should be determined.", "labels": [], "entities": []}, {"text": "They refer to psycholinguistic research suggesting that, in general, absolute attributes (such as color) are preferred over relative ones (such as size), but stress that constructing a preference order is essentially an empirical question, which will differ from one domain to another.", "labels": [], "entities": []}, {"text": "Many other REG algorithms similarly rely on preferences.", "labels": [], "entities": []}, {"text": "The graph-based based REG algorithm (, for example, models preferences in terms of costs, with cheaper properties being more preferred.", "labels": [], "entities": []}, {"text": "Various ways to compute costs are possible; they can be defined, for instance, in terms of log probabilities, which makes frequently encountered properties cheap, and infrequent ones more expensive.", "labels": [], "entities": []}, {"text": "argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive).", "labels": [], "entities": []}, {"text": "This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge ().", "labels": [], "entities": [{"text": "REG Challenge", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.5156970918178558}]}, {"text": "Many other attribute selection algorithms also rely on training data to determine preferences in one form or another.", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7318235337734222}]}, {"text": "Unfortunately, suitable data is hard to come by.", "labels": [], "entities": []}, {"text": "It has been argued that determining which properties to include in a referring expression requires a \"semantically transparent\" corpus): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target.", "labels": [], "entities": []}, {"text": "Obviously, text corpora tend not to meet this requirement, which is why semantically transparent corpora are often collected using human participants who are asked to produce referring expressions for targets in controlled visual scenes fora given domain.", "labels": [], "entities": []}, {"text": "Since this is a time consuming exercise, it will not be surprising that such corpora are thin on the ground (and are often only available for English).", "labels": [], "entities": []}, {"text": "An important question therefore is how many human-produced references are needed to achieve a certain level of performance.", "labels": [], "entities": []}, {"text": "Do we really need hundreds of instances, or can we already make informed decisions about preferences on a few or even one training instance?", "labels": [], "entities": []}, {"text": "In this paper, we address this question by systematically training the graph-based REG algorithm on a number of \"semantically transparent\" data sets of various sizes and evaluating on a held-out test set.", "labels": [], "entities": []}, {"text": "The graph-based algorithm seems a good candidate for this exercise, in view of its performance in the REG challenges.", "labels": [], "entities": [{"text": "REG", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9046483635902405}]}, {"text": "For the sake of comparison, we also follow the evaluation methodology of the REG challenges, training and testing on two domains (a furniture and a people domain), and using two automatic metrics (Dice and accuracy) to measure human-likeness.", "labels": [], "entities": [{"text": "REG", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.7772687673568726}, {"text": "Dice", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9419227838516235}, {"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.7999743819236755}]}, {"text": "One hurdle needs to betaken beforehand.", "labels": [], "entities": []}, {"text": "manually assigned one of three costs to properties, loosely based on corpus frequencies.", "labels": [], "entities": []}, {"text": "For our current evaluation experiments, this would hamper comparison across data sets, because it is difficult to do it in a manner that is both consistent and meaningful.", "labels": [], "entities": []}, {"text": "Therefore we first experiment with a more systematic way of assigning a limited number of frequency-based costs to properties using k-means clustering.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experiment with kmeans clustering to derive property costs from English and Dutch corpus data.", "labels": [], "entities": [{"text": "kmeans clustering", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8516642153263092}, {"text": "Dutch corpus data", "start_pos": 108, "end_pos": 125, "type": "DATASET", "confidence": 0.7507135470708212}]}, {"text": "For this experiment we looked at both English and Dutch, to make sure the chosen method does not only work well for English.", "labels": [], "entities": []}, {"text": "To find out how much training data is required to achieve an acceptable attribute selection perfor- We used slightly different property orders than, leading to minor differences in our FN results.", "labels": [], "entities": []}, {"text": "mance, in the second experiment we derived cost functions and property orders from different sized training sets, and evaluated them on our test data.", "labels": [], "entities": []}, {"text": "For this experiment, we only used English data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for k-means costs with k = 2 and the  FN costs of Theune et al. (2010) on Dutch and English.", "labels": [], "entities": [{"text": "FN", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.8506325483322144}]}, {"text": " Table 2: Mean results for the different set sizes.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9867908954620361}]}]}