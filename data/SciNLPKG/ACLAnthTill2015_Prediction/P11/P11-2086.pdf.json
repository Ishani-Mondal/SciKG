{"title": [{"text": "Automatic Assessment of Coverage Quality in Intelligence Reports", "labels": [], "entities": []}], "abstractContent": [{"text": "Common approaches to assessing document quality look at shallow aspects, such as grammar and vocabulary.", "labels": [], "entities": []}, {"text": "For many real-world applications, deeper notions of quality are needed.", "labels": [], "entities": []}, {"text": "This work represents a first step in a project aimed at developing computational methods for deep assessment of quality in the domain of intelligence reports.", "labels": [], "entities": []}, {"text": "We present an automated system for ranking intelligence reports with regard to coverage of relevant material.", "labels": [], "entities": []}, {"text": "The system employs methodologies from the field of automatic summarization, and achieves performance on a par with human judges, even in the absence of the underlying information sources.", "labels": [], "entities": [{"text": "summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7860437631607056}]}], "introductionContent": [{"text": "Distinguishing between high-and low-quality documents is an important skill for humans, and a challenging task for machines.", "labels": [], "entities": []}, {"text": "The majority of previous research on the subject has focused on low-level measures of quality, such as spelling, vocabulary and grammar.", "labels": [], "entities": []}, {"text": "However, in many real-world situations, it is necessary to employ deeper criteria, which look at the content of the document and the structure of argumentation.", "labels": [], "entities": []}, {"text": "One example where such criteria are essential is decision-making in the intelligence community.", "labels": [], "entities": []}, {"text": "This is also a domain where computational methods can play an important role.", "labels": [], "entities": []}, {"text": "Ina typical situation, an intelligence officer faced with an important decision receives reports from a team of analysts on a specific topic of interest.", "labels": [], "entities": []}, {"text": "Each decision may involve several areas of interest, resulting in several collections of reports.", "labels": [], "entities": []}, {"text": "Additionally, the officer maybe engaged in many decision processes within a small window of time.", "labels": [], "entities": []}, {"text": "Given the nature of the task, it is vital that the limited time be used effectively, i.e., that the highest-quality information be handled first.", "labels": [], "entities": []}, {"text": "Our project aims to provide a system that will assist intelligence officers in the decision making process by quickly and accurately ranking reports according to the most important criteria for the task.", "labels": [], "entities": [{"text": "decision making process", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.9219188690185547}]}, {"text": "In this paper, as a first step in the project, we focus on content-related criteria.", "labels": [], "entities": []}, {"text": "In particular, we chose to start with the aspect of \"coverage\".", "labels": [], "entities": [{"text": "coverage", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.8939822912216187}]}, {"text": "Coverage is perhaps the most important element in a time-sensitive scenario, where an intelligence officer may need to choose among several reports while ensuring no relevant and important topics are overlooked.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a gold standard, we use the average of the scores given to each report by the human of two reports on completely different subjects, based on external knowledge.", "labels": [], "entities": []}, {"text": "For our usage scenario, this is not an issue.", "labels": [], "entities": []}, {"text": "We also experimented with unigram and trigram representations, which did not do as well as the bigram representation (as suggested by . judges 3 . Since we are interested in ranking reports by coverage, we convert the scores from the original numerical scale to a ranked list.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the algorithms (and of the individual judges) using Kendall's Tau to measure concordance with the gold standard.", "labels": [], "entities": []}, {"text": "Kendall's Tau coefficient (\u03c4 k ) is commonly used (e.g., to compare rankings, and looks at the number of pairs of ranked items that agree or disagree with the ordering in the gold standard.", "labels": [], "entities": [{"text": "Kendall's Tau coefficient (\u03c4 k )", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.8236112967133522}]}, {"text": "Let T = {(a i , a j ) : a i g a j } denote the set of pairs ordered in the gold standard (a i precedes a j ).", "labels": [], "entities": []}, {"text": "Let R = {(a l , am ) : a l r am } denote the set of pairs ordered by a ranking algorithm.", "labels": [], "entities": []}, {"text": "C = T \u2229R is the set of concordant pairs, i.e., pairs ordered the same way in the gold standard and in the ranking, and D = T \u2229 R is the set of discordant pairs.", "labels": [], "entities": []}, {"text": "Kendall's rank correlation coefficient \u03c4 k is defined as follows: The value of \u03c4 k ranges from -1 (reversed ranking) to 1 (perfect agreement), with 0 being equivalent to a random ranking (50% agreement).", "labels": [], "entities": [{"text": "rank correlation coefficient \u03c4 k", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.7657779574394226}, {"text": "perfect agreement)", "start_pos": 123, "end_pos": 141, "type": "METRIC", "confidence": 0.8256609241167704}]}, {"text": "As a simple baseline system, we rank the reports according to their length in words, which asserts that a longer document has \"more coverage\".", "labels": [], "entities": []}, {"text": "For comparison, we also examine agreement between individual human judges and the gold standard.", "labels": [], "entities": []}, {"text": "In each scenario, we calculate the average agreement (Tau value) between an individual judge and the gold standard, and also look at the highest and lowest Tau value from among the individual judges.", "labels": [], "entities": [{"text": "average agreement (Tau value)", "start_pos": 35, "end_pos": 64, "type": "METRIC", "confidence": 0.7837972442309061}]}, {"text": "presents the results of our ranking experiments on each of the eight scenarios.", "labels": [], "entities": []}], "tableCaptions": []}