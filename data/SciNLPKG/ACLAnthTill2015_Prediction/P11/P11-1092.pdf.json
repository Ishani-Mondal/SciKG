{"title": [{"text": "Grammatical Error Correction with Alternating Structure Optimization", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8095800081888834}]}], "abstractContent": [{"text": "We present a novel approach to grammatical error correction based on Alternating Structure Optimization.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7018309434254965}, {"text": "Alternating Structure Optimization", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.6036544839541117}]}, {"text": "As part of our work, we introduce the NUS Corpus of Learner En-glish (NUCLE), a fully annotated one million words corpus of learner English available for research purposes.", "labels": [], "entities": [{"text": "NUS Corpus of Learner En-glish (NUCLE)", "start_pos": 38, "end_pos": 76, "type": "DATASET", "confidence": 0.9440064579248428}]}, {"text": "We conduct an extensive evaluation for article and preposition errors using various feature sets.", "labels": [], "entities": []}, {"text": "Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively.", "labels": [], "entities": []}, {"text": "Our approach also outperforms two commercial grammar checking software packages.", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.8753636479377747}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) has been recognized as an interesting as well as commercially attractive problem in natural language processing (NLP), in particular for learners of English as a foreign or second language (EFL/ESL).", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8618588745594025}, {"text": "natural language processing (NLP)", "start_pos": 119, "end_pos": 152, "type": "TASK", "confidence": 0.7779858012994131}]}, {"text": "Despite the growing interest, research has been hindered by the lack of a large annotated corpus of learner text that is available for research purposes.", "labels": [], "entities": []}, {"text": "As a result, the standard approach to GEC has been to train an off-the-shelf classifier to re-predict words in non-learner text.", "labels": [], "entities": [{"text": "GEC", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.6906701922416687}]}, {"text": "Learning GEC models directly from annotated learner corpora is not well explored, as are methods that combine learner and non-learner text.", "labels": [], "entities": []}, {"text": "Furthermore, the evaluation of GEC has been problematic.", "labels": [], "entities": [{"text": "GEC", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.4828821122646332}]}, {"text": "Previous work has either evaluated on artificial test instances as a substitute for real learner errors or on proprietary data that is not available to other researchers.", "labels": [], "entities": []}, {"text": "As a consequence, existing methods have not been compared on the same test set, leaving it unclear where the current state of the art really is.", "labels": [], "entities": []}, {"text": "In this work, we aim to overcome both problems.", "labels": [], "entities": []}, {"text": "First, we present a novel approach to GEC based on Alternating Structure Optimization (ASO) (Ando and).", "labels": [], "entities": [{"text": "GEC", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9130871891975403}]}, {"text": "Our approach is able to train models on annotated learner corpora while still taking advantage of large non-learner corpora.", "labels": [], "entities": []}, {"text": "Second, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes.", "labels": [], "entities": [{"text": "NUS Corpus of Learner English (NUCLE)", "start_pos": 25, "end_pos": 62, "type": "DATASET", "confidence": 0.9535342752933502}]}, {"text": "We conduct an extensive evaluation for article and preposition errors using six different feature sets proposed in previous work.", "labels": [], "entities": []}, {"text": "We compare our proposed ASO method with two baselines trained on non-learner text and learner text, respectively.", "labels": [], "entities": [{"text": "ASO", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9034520387649536}]}, {"text": "To the best of our knowledge, this is the first extensive comparison of different feature sets on real learner text which is another contribution of our work.", "labels": [], "entities": []}, {"text": "Our experiments show that our proposed ASO algorithm significantly improves over both baselines.", "labels": [], "entities": [{"text": "ASO", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8473975658416748}]}, {"text": "It also outperforms two commercial grammar checking software packages in a manual evaluation.", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.8316801190376282}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the tasks.", "labels": [], "entities": []}, {"text": "Section 4 formulates GEC as a classification problem.", "labels": [], "entities": [{"text": "formulates GEC", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.5686709582805634}, {"text": "classification", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.953395426273346}]}, {"text": "Section 5 extends this to the ASO algorithm.", "labels": [], "entities": [{"text": "ASO", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.7689145803451538}]}, {"text": "The experiments are presented in Section 6 and the results in Section 7.", "labels": [], "entities": []}, {"text": "Section 8 contains a more detailed analysis of the results.", "labels": [], "entities": []}, {"text": "Section 9 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "For experiments on non-learner text, we report accuracy, which is defined as the number of correct predictions divided by the total number of test instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.976689338684082}]}, {"text": "For experiments on learner text, we report F 1 -measure where precision is the number of suggested corrections that agree with the human annotator divided by the total number of proposed corrections by the system, and recall is the number of suggested corrections that agree with the human annotator divided by the total number of errors annotated by the human annotator.", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.977531909942627}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9984878301620483}, {"text": "recall", "start_pos": 218, "end_pos": 224, "type": "METRIC", "confidence": 0.9995614886283875}]}, {"text": "The first set of experiments investigates predicting articles and prepositions in non-learner text.", "labels": [], "entities": [{"text": "predicting articles and prepositions in non-learner text", "start_pos": 42, "end_pos": 98, "type": "TASK", "confidence": 0.8790561386517116}]}, {"text": "This primarily serves as a reference point for the correction task described in the next section.", "labels": [], "entities": [{"text": "correction task", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8801378011703491}]}, {"text": "We train classifiers as described in Section 4 on the Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9347606301307678}]}, {"text": "We train with up to 10 million training instances, which corresponds to about 37 million words of text for articles and 112 million words of text for prepositions.", "labels": [], "entities": []}, {"text": "The test instances are extracted from section 23 of the WSJ and no text from the WSJ is included in the training data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9547781348228455}, {"text": "WSJ", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9802054166793823}]}, {"text": "The observed article or preposition choice of the writer is the class 1 LDC2009T13 2 www.nltk.org 3 opennlp.sourceforge.net we want to predict.", "labels": [], "entities": []}, {"text": "Therefore, the article or preposition cannot be part of the input features.", "labels": [], "entities": []}, {"text": "Our proposed ASO method is not included in these experiments, as it uses the observed article or preposition as a feature which is only applicable when testing on learner text.", "labels": [], "entities": [{"text": "ASO", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.7561635971069336}]}, {"text": "The second set of experiments investigates the primary goal of this work: to automatically correct grammatical errors in learner text.", "labels": [], "entities": []}, {"text": "The test instances are extracted from NUCLE.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9615470767021179}]}, {"text": "In contrast to the previous selection task, the observed word choice of the writer can be different from the correct class and the observed word is available during testing.", "labels": [], "entities": []}, {"text": "We investigate two different baselines and our ASO method.", "labels": [], "entities": [{"text": "ASO", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8154467344284058}]}, {"text": "The first baseline is a classifier trained on the Gigaword corpus in the same way as described in the selection task experiment.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.9555766582489014}]}, {"text": "We use a simple thresholding strategy to make use of the observed word during testing.", "labels": [], "entities": []}, {"text": "The system only flags an error if the difference between the classifier's confidence for its first choice and the confidence for the observed word is higher than a threshold t.", "labels": [], "entities": []}, {"text": "The threshold parameter t is tuned on the NUCLE development data for each feature set.", "labels": [], "entities": [{"text": "NUCLE development data", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9200212955474854}]}, {"text": "In our experiments, the value fort is between 0.7 and 1.2.", "labels": [], "entities": [{"text": "fort", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9514577388763428}]}, {"text": "The second baseline is a classifier trained on NU-CLE.", "labels": [], "entities": [{"text": "NU-CLE", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.9334943294525146}]}, {"text": "The classifier is trained in the same way as the Gigaword model, except that the observed word choice of the writer is included as a feature.", "labels": [], "entities": []}, {"text": "The correct class during training is the correction provided by the human annotator.", "labels": [], "entities": [{"text": "correction", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9579716324806213}]}, {"text": "As the observed word is part of the features, this model does not need an extra thresholding step.", "labels": [], "entities": []}, {"text": "Indeed, we found that thresholding is harmful in this case.", "labels": [], "entities": []}, {"text": "During training, the instances that do not contain an error greatly outnumber the instances that do contain an error.", "labels": [], "entities": []}, {"text": "To reduce this imbalance, we keep all instances that contain an error and retain a random sample of q percent of the instances that do not contain an error.", "labels": [], "entities": []}, {"text": "The undersample parameter q is tuned on the NUCLE development data for each data set.", "labels": [], "entities": [{"text": "NUCLE development data", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9390315612157186}]}, {"text": "In our experiments, the value for q is between 20% and 40%.", "labels": [], "entities": []}, {"text": "Our ASO method is trained in the following way.", "labels": [], "entities": [{"text": "ASO", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8867687582969666}]}, {"text": "We create binary auxiliary problems for articles or prepositions, i.e., there are 3 auxiliary problems for articles and 36 auxiliary problems for prepositions.", "labels": [], "entities": []}, {"text": "We train the classifiers for the auxiliary problems on the complete 10 million instances from Gigaword in the same ways as in the selection task experiment.", "labels": [], "entities": []}, {"text": "The weight vectors of the auxiliary problems form the matrix U . We perform SVD to get U = V 1 DV T 2 . We keep all columns of V 1 to form \u0398.", "labels": [], "entities": []}, {"text": "The target problems are again binary classification problems for each article or preposition, but this time trained on NUCLE.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.6945974230766296}, {"text": "NUCLE", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.9458884596824646}]}, {"text": "The observed word choice of the writer is included as a feature for the target problems.", "labels": [], "entities": []}, {"text": "We again undersample the instances that do not contain an error and tune the parameter q on the NUCLE development data.", "labels": [], "entities": [{"text": "NUCLE development data", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.945581316947937}]}, {"text": "The value for q is between 20% and 40%.", "labels": [], "entities": []}, {"text": "We also experimented with a classifier that is trained on the concatenated data from NUCLE and Gigaword.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.9391101598739624}, {"text": "Gigaword", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9108691215515137}]}, {"text": "This model always performed worse than the better of the individual baselines.", "labels": [], "entities": []}, {"text": "The reason is that the two data sets have different feature spaces which prevents simple concatenation of the training data.", "labels": [], "entities": []}, {"text": "We therefore omit these results from the paper.", "labels": [], "entities": []}, {"text": "We carried out a manual evaluation of the best ASO models and compared their output with two commercial grammar checking software packages which we call System A and System B. We randomly sampled 1000 test instances for articles and 2000 test instances for prepositions and manually categorized each test instance into one of the following categories: (1) Correct means that both human and system flag an error and suggest the same correction.", "labels": [], "entities": [{"text": "Correct", "start_pos": 356, "end_pos": 363, "type": "METRIC", "confidence": 0.9936189651489258}]}, {"text": "If the system's correction differs from the human but is equally acceptable, it is considered (2) Both Ok.", "labels": [], "entities": []}, {"text": "If the system identifies an error but fails to correct it, we consider it (3) Both Wrong, as both the writer and the system are wrong.", "labels": [], "entities": []}, {"text": "(4) Other Error means that the system's correction does not result in a grammatical sentence because of another grammatical error that is outside the scope of article or preposition errors, e.g., a noun number error as in \"all the dog\".", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9744853973388672}]}, {"text": "If the system corrupts a previously correct sentence it is a (5) False Flag.", "labels": [], "entities": [{"text": "False", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9883973002433777}]}, {"text": "If the human flags an error but the system does not, it is a (6) Miss.", "labels": [], "entities": [{"text": "Miss", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9390650391578674}]}, {"text": "No Flag means that neither the human annotator nor the system flags an error.", "labels": [], "entities": []}, {"text": "We calculate precision by dividing the count of category (1) by the sum of counts of categories (1), (3), and (5), and recall by dividing the count of category (1) by the sum of counts of categories (1), (3), and (6).", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9995337724685669}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9922665953636169}]}, {"text": "The results are shown in  tion shows that even commercial software packages achieve low F 1 -measure for article and preposition errors, which confirms the difficulty of these tasks.", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.9915699362754822}]}], "tableCaptions": [{"text": " Table 1: Best results for the correction task on NU- CLE test data. Improvements for ASO over either  baseline are statistically significant (p < 0.001) for  both tasks.", "labels": [], "entities": [{"text": "NU- CLE test data", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.8878207802772522}, {"text": "ASO", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.7218831777572632}]}, {"text": " Table 2. Our ASO method outperforms  both commercial software packages. Our evalua-", "labels": [], "entities": [{"text": "ASO", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8447989821434021}]}]}