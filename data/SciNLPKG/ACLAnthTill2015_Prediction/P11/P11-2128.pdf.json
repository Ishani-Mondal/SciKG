{"title": [{"text": "Entity Set Expansion using Topic information", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes three modules based on latent topics of documents for alleviating \"se-mantic drift\" in bootstrapping entity set expansion.", "labels": [], "entities": [{"text": "bootstrapping entity set expansion", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.6670765057206154}]}, {"text": "These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning.", "labels": [], "entities": [{"text": "topic feature generation", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6866572101910909}, {"text": "negative example selection", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.6213019589583079}]}, {"text": "In this study, we model latent topics with LDA (Latent Dirichlet Allocation) in an unsuper-vised way.", "labels": [], "entities": [{"text": "LDA (Latent Dirichlet Allocation)", "start_pos": 43, "end_pos": 76, "type": "METRIC", "confidence": 0.757013181845347}]}, {"text": "Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2% depending on the domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9992871880531311}]}], "introductionContent": [{"text": "The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (.", "labels": [], "entities": [{"text": "entity set expansion", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.6549758513768514}]}, {"text": "For example, the user inputs a few words \"Apple\", \"Google\" and \"IBM\" , and the system outputs \"Microsoft\", \"Facebook\" and \"Intel\".", "labels": [], "entities": []}, {"text": "Many set expansion algorithms are based on bootstrapping algorithms, which iteratively acquire new entities.", "labels": [], "entities": []}, {"text": "These algorithms suffer from the general problem of \"semantic drift\".", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7610219717025757}]}, {"text": "Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9994114637374878}]}, {"text": "proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8452033996582031}]}, {"text": "Espresso alleviates semanticdrift by a sophisticated scoring system based on * Presently with Okayama Prefectural University pointwise mutual information (PMI)., and also proposed original score functions with the goal of reducing semantic-drift.", "labels": [], "entities": [{"text": "Okayama Prefectural University pointwise mutual information (PMI).", "start_pos": 94, "end_pos": 160, "type": "DATASET", "confidence": 0.8836889333195157}]}, {"text": "Our purpose is also to reduce semantic drift.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7856759130954742}]}, {"text": "For achieving this goal, we use a discriminative method instead of a scoring function and incorporate topic information into it.", "labels": [], "entities": []}, {"text": "Topic information means the genre of each document as estimated by statistical topic models.", "labels": [], "entities": []}, {"text": "In this paper, we effectively utilize topic information in three modules: the first generates the features of the discriminative models; the second selects negative examples; the third prunes incorrect examples from candidate examples for new entities.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposal improves the accuracy of the extracted entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9982878565788269}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we illustrate discriminative bootstrapping algorithms and describe their problems.", "labels": [], "entities": []}, {"text": "Our proposal is described in Section 3 and experimental results are shown in Section 4.", "labels": [], "entities": []}, {"text": "Related works are described in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 provides our conclusion and describes future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use 30M Japanese blog articles crawled in May 2008.", "labels": [], "entities": [{"text": "Japanese blog articles crawled in May 2008", "start_pos": 11, "end_pos": 53, "type": "DATASET", "confidence": 0.8461620381900242}]}, {"text": "The documents were tokenized by JTAG (, chunked, and labeled with IREX 8 Named Entity types by CRFs using Minimum Classification Error rate (), and transformed into features.", "labels": [], "entities": [{"text": "JTAG", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.7961642742156982}]}, {"text": "The context features were defined using the template \"(head) entity (mid.) attribute (tail)\".", "labels": [], "entities": []}, {"text": "The words included in each part were used as surface, part-of-speech and Named Entity label features added position information.", "labels": [], "entities": []}, {"text": "Maximum word number of each part was set at 2 words.", "labels": [], "entities": []}, {"text": "The features have to appear in both the positive and negative training data at least 5 times.", "labels": [], "entities": []}, {"text": "In the experiments, we used three domains, car (\"CAR\"), broadcast program (\"PRG\") and sports organization (\"SPT\").", "labels": [], "entities": []}, {"text": "The adjustment numbers for basic settings are N s = 10, Na = 10, N n = 100.", "labels": [], "entities": []}, {"text": "After running 10 iterations, we obtained 1000 entities in total.", "labels": [], "entities": []}, {"text": "SV M light (Joachims, 1999) with second order polynomial kernel was used as the discriminative model.", "labels": [], "entities": []}, {"text": "Parallel LDA, which is LDA with MPI ( Bold font indicates that the difference between accuracy of the methods in the row and the previous row is significant (P < 0.05 by binomial test) and italic font indicates (P < 0.1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9987590312957764}]}, {"text": "14 days of blog articles.", "labels": [], "entities": []}, {"text": "In the Markov-chain Monte Carlo (MCMC) method, sampling was iterated 200 times for training with a burn-in taking 50 iterations.", "labels": [], "entities": []}, {"text": "These parameters were selected based on the results of a preliminary experiment.", "labels": [], "entities": []}, {"text": "Four experimental settings were examined.", "labels": [], "entities": []}, {"text": "First is Baseline; it is described in Section 3.1.", "labels": [], "entities": []}, {"text": "Second is the first method with the addition of topic features.", "labels": [], "entities": []}, {"text": "Third is the second method with the addition of a negative example selection module.", "labels": [], "entities": []}, {"text": "Fourth is the third method with the addition of a candidate pruning module (equals the entire shaded part in.", "labels": [], "entities": []}, {"text": "Each extracted entity is labeled with corrector incorrect by two evaluators based on the results of a commercial search engine.", "labels": [], "entities": []}, {"text": "The \u03ba score for agreement between evaluators was 0.895.", "labels": [], "entities": [{"text": "\u03ba score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9630962908267975}]}, {"text": "Because the third evaluator checked the two evaluations and confirmed that the examples which were judged as correct by either one of the evaluators were correct, those examples were counted as correct.", "labels": [], "entities": []}, {"text": "shows the accuracy and significance for each domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995918869972229}, {"text": "significance", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.939581573009491}]}, {"text": "Using topic features significantly improves accuracy in the CAR and SPT domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9990224838256836}]}, {"text": "The negative example selection module improves accuracy in the CAR and PRG domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9990584254264832}]}, {"text": "This means the method could reduce the risk of selecting false-negative examples.", "labels": [], "entities": []}, {"text": "Also, the candidate pruning method is effective for the CAR and PRG domains.", "labels": [], "entities": []}, {"text": "The CAR domain has lower accuracy than the others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9991482496261597}]}, {"text": "This is because similar entities such as motorcycles are extracted; they have not only the same context but also the same topic as the CAR domain.", "labels": [], "entities": []}, {"text": "In the SPT domain, the method with topic features offer significant improvements inaccuracy and no further improvement was achieved by the other two modules.", "labels": [], "entities": [{"text": "SPT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8712276816368103}]}, {"text": "To confirm whether our modules work properly, we show some characteristic words belonging to each topic that is similar and not similar to target domain in shows characteristic words for one positive topic z hand two negative topics z land z e , defined as follow.", "labels": [], "entities": []}, {"text": "\u2022 z h (the second row) is the topic that maximizes PT (z), which is used as a positive topic.", "labels": [], "entities": [{"text": "PT (z)", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9313980638980865}]}, {"text": "\u2022 z l (the fourth row) is the topic that minimizes PT (z), which is used as a negative topic.", "labels": [], "entities": [{"text": "PT (z)", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9328538030385971}]}, {"text": "\u2022 z e (the fifth row) is a topic that, we consider, effectively eliminates \"drifted entities\" extracted by the baseline method.", "labels": [], "entities": []}, {"text": "z e is eventually included in the lower half of topic list sorted by PT (z).", "labels": [], "entities": [{"text": "PT", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9961839318275452}]}, {"text": "For a given topic, z, we chose topmost three words in terms of topic-word score.", "labels": [], "entities": []}, {"text": "The topic-word score of a word, v, is defined as p(v|z)/p(v), where p(v) is the unigram probability of v, which was estimated by maximum likelihood estimation.", "labels": [], "entities": []}, {"text": "For utilizing candidate pruning, near topics including z h must be similar to the domain.", "labels": [], "entities": []}, {"text": "By contrast, for utilizing negative example selection, the lower half of topics, z l , z e and other negative topics, must be far from the domain.", "labels": [], "entities": []}, {"text": "Our system succeeded in achieving this.", "labels": [], "entities": []}, {"text": "As shown in \"CAR\" in, the nearest topic includes \"shaken\" (automobile inspection) and the farthest topic includes \"naika\" (internal medicine) which satisfies our expectation.", "labels": [], "entities": []}, {"text": "Furthermore, the effective negative topic is similar to the topic of drifted entity sets (digital device).", "labels": [], "entities": []}, {"text": "This indicates that our method successfully eliminated drifted entities.", "labels": [], "entities": []}, {"text": "We can confirm that the other domains trend in the same direction as \"CAR\" domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The experimental results for the three domains.", "labels": [], "entities": []}]}