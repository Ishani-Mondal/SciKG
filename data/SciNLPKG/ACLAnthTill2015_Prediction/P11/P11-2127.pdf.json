{"title": [{"text": "The Surprising Variance in Shortest-Derivation Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.46167775988578796}]}], "abstractContent": [{"text": "We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments.", "labels": [], "entities": [{"text": "full-scale shortest-derivation parsing (SDP)", "start_pos": 15, "end_pos": 59, "type": "TASK", "confidence": 0.7737037142117819}]}, {"text": "Shortest derivation parsing exhibits an unusual range of behaviors.", "labels": [], "entities": [{"text": "Shortest derivation parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.736301859219869}]}, {"text": "At one extreme, in the fully unpruned case, it is neither fast nor accurate.", "labels": [], "entities": []}, {"text": "At the other extreme , when pruned with a coarse unlexical-ized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.9201909303665161}]}, {"text": "Our analysis includes an investigation of tie-breaking and associated dynamic programs.", "labels": [], "entities": []}, {"text": "At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9994779229164124}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9997174143791199}, {"text": "English WSJ task", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8704536358515421}, {"text": "F1", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9993971586227417}]}], "introductionContent": [{"text": "One guiding intuition in parsing, and data-driven NLP more generally, is that, all else equal, it is advantageous to memorize large fragments of training examples.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9748603701591492}]}, {"text": "Taken to the extreme, this intuition suggests shortest derivation parsing (SDP), wherein a test sentence is analyzed in away which uses as few training fragments as possible.", "labels": [], "entities": [{"text": "shortest derivation parsing (SDP)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7452441106239954}]}, {"text": "SDP certainly has appealing properties: it is simple and parameter free -there need not even bean explicit lexicon.", "labels": [], "entities": []}, {"text": "However, SDP maybe too simple to be competitive.", "labels": [], "entities": [{"text": "SDP", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9386165738105774}]}, {"text": "In this paper, we consider SDP in both its pure form and with several direct modifications, finding a range of behaviors.", "labels": [], "entities": [{"text": "SDP", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9646632075309753}]}, {"text": "In its pure form, with no pruning or approximation, SDP is neither fast nor accurate, achieving less than 70% F1 on the English WSJ task.", "labels": [], "entities": [{"text": "approximation", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9429430365562439}, {"text": "SDP", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9602533578872681}, {"text": "F1", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.999724805355072}, {"text": "English WSJ task", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.8305761615435282}]}, {"text": "Moreover, basic tie-breaking variants and lexical augmentation are insufficient to achieve competitive accuracies.", "labels": [], "entities": []}, {"text": "On the other hand, SDP is dramatically improved in both speed and accuracy when a simple, unlexicalized PCFG is used for coarseto-fine pruning (and tie-breaking).", "labels": [], "entities": [{"text": "speed", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9880437254905701}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9980720281600952}]}, {"text": "On the English WSJ, the coarse PCFG and the fine SDP together achieve 87% F1 with basic treebank annotation (see) and up to 90% F1 with richer treebank annotation (see).", "labels": [], "entities": [{"text": "English WSJ", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.888283908367157}, {"text": "F1", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9997220635414124}, {"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.9996412992477417}]}, {"text": "The main contribution of this work is to analyze the behavior of shortest derivation parsing, showing both when it fails and when it succeeds.", "labels": [], "entities": [{"text": "shortest derivation parsing", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.7028122742970785}]}, {"text": "Our final parser, which combines a simple PCFG coarse pass with an otherwise pure SPD fine pass, can be quite accurate while being straightforward to implement.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy (F1) and exact match (EX) for", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993587136268616}, {"text": "F1)", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9632929861545563}, {"text": "exact match (EX)", "start_pos": 28, "end_pos": 44, "type": "METRIC", "confidence": 0.9266600489616394}]}, {"text": " Table 2: Our primary results on the WSJ task. SDP is the  basic unpruned shortest derivation parser. PCFG results are  with one level of parent annotation and horizontal markoviza- tion. PCFG+SDP incorporates the coarse PCFG posteriors into  SDP. See end of Section 5 for a comparison to other parsing  approaches.", "labels": [], "entities": [{"text": "WSJ task", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.8267298638820648}]}, {"text": " Table 3: Results for training and testing on the Brown and  German treebanks. Gildea (2001) uses the lexicalized Collins'  Model 1 (Collins, 1999).", "labels": [], "entities": [{"text": "Brown and  German treebanks", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.7771369814872742}, {"text": "Collins'  Model 1 (Collins, 1999)", "start_pos": 114, "end_pos": 147, "type": "DATASET", "confidence": 0.9225014820694923}]}, {"text": " Table 4: Results with richer WSJ-annotations from Stanford  and Berkeley parsers.", "labels": [], "entities": []}]}