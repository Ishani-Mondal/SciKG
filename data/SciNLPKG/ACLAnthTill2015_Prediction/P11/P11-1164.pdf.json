{"title": [{"text": "Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification", "labels": [], "entities": [{"text": "Extracting Comparative Entities and Predicates from Texts", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.897251912525722}]}], "abstractContent": [{"text": "The automatic extraction of comparative information is an important text mining problem and an area of increasing interest.", "labels": [], "entities": [{"text": "automatic extraction of comparative information", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.8199985325336456}, {"text": "text mining", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.8725631237030029}]}, {"text": "In this paper, we study how to build a Korean comparison mining system.", "labels": [], "entities": [{"text": "Korean comparison mining", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.5583149294058481}]}, {"text": "Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates.", "labels": [], "entities": []}, {"text": "We perform various experiments to find relevant features and learning techniques.", "labels": [], "entities": []}, {"text": "As a result, we achieve outstanding performance enough for practical use.", "labels": [], "entities": []}], "introductionContent": [{"text": "Almost everyday, people are faced with a situation that they must decide upon one thing or the other.", "labels": [], "entities": []}, {"text": "To make better decisions, they probably attempt to compare entities that they are interesting in.", "labels": [], "entities": []}, {"text": "These days, many web search engines are helping people look for their interesting entities.", "labels": [], "entities": []}, {"text": "It is clear that getting information from a large amount of web data retrieved by the search engines is a much better and easier way than the traditional survey methods.", "labels": [], "entities": []}, {"text": "However, it is also clear that directly reading each document is not a perfect solution.", "labels": [], "entities": []}, {"text": "If people only have access to a small amount of data, they may get a biased point of view.", "labels": [], "entities": []}, {"text": "On the other hand, investigating large amounts of data is a timeconsuming job.", "labels": [], "entities": []}, {"text": "Therefore, a comparison mining system, which can automatically provide a summary of comparisons between two (or more) entities from a large quantity of web documents, would be very useful in many areas such as marketing.", "labels": [], "entities": [{"text": "comparison mining", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8752632141113281}]}, {"text": "We divide our work into two tasks to effectively build a comparison mining system.", "labels": [], "entities": [{"text": "comparison mining", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8771331310272217}]}, {"text": "The first task is related to a sentence classification problem and the second is related to an information extraction problem.", "labels": [], "entities": [{"text": "sentence classification problem", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.8083217541376749}, {"text": "information extraction", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.8122734129428864}]}], "datasetContent": [{"text": "The experiments are conducted on 7,384 sentences collected from the web by three trained human labelers.", "labels": [], "entities": []}, {"text": "Firstly, two labelers annotated the corpus.", "labels": [], "entities": []}, {"text": "A Kappa value of 0.85 showed that it was safe to say that the two labelers agreed in their judgments.", "labels": [], "entities": []}, {"text": "Secondly, the third labeler annotated the conflicting part of the corpus.", "labels": [], "entities": []}, {"text": "All three labelers discussed any conflict, and finally reached an agreement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Final results in comparative sentence  extraction (%)", "labels": [], "entities": [{"text": "comparative sentence  extraction", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7056291798750559}]}, {"text": " Table 5: Evaluation of threshold option (%);", "labels": [], "entities": []}, {"text": " Table 7: Portion (%) of multiple-word comparative  elements", "labels": [], "entities": []}, {"text": " Table 8: Error rate (%) in CE-candidate detection", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9871895611286163}]}, {"text": " Table  9. We calculated each probability of CE-candidates  using MEM and SVM. Both MEM and SVM  showed outstanding performance; there was no  significant difference between the two machine  learning methods (SVM and MEM). Hence, we  only report the results of SVM. Note that many  sentences do not contain any OE. To identify such  sentences, if SVM tagged every \"N\" in a sentence  as \"not OE\", we tagged the sentence as \"no OE\".", "labels": [], "entities": [{"text": "OE", "start_pos": 426, "end_pos": 428, "type": "METRIC", "confidence": 0.9628686308860779}]}, {"text": " Table 9: Final results of Task 2 (Accuracy, %)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9991869330406189}]}]}