{"title": [{"text": "A Simple Measure to Assess Non-response", "labels": [], "entities": []}], "abstractContent": [{"text": "There are several tasks where is preferable not responding than responding incorrectly.", "labels": [], "entities": []}, {"text": "This idea is not new, but despite several previous attempts there isn't a commonly accepted measure to assess non-response.", "labels": [], "entities": []}, {"text": "We study here an extension of accuracy measure with this feature and a very easy to understand interpretation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9995146989822388}]}, {"text": "The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties.", "labels": [], "entities": [{"text": "sensitivity", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9563080668449402}]}, {"text": "We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unan-swered.", "labels": [], "entities": []}, {"text": "This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.", "labels": [], "entities": [{"text": "Reading Comprehension tests", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7643626928329468}]}], "introductionContent": [{"text": "There is some tendency to consider that an incorrect result is simply the absence of a correct one.", "labels": [], "entities": []}, {"text": "This is particularly true in the evaluation of Information Retrieval systems where, in fact, the absence of results sometimes is the worse output.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.8010140359401703}]}, {"text": "However, there are scenarios where we should consider the possibility of not responding, because this behavior has more value than responding incorrectly.", "labels": [], "entities": []}, {"text": "For example, during the process of introducing new features in a search engine it is important to preserve users' confidence in the system.", "labels": [], "entities": []}, {"text": "Thus, a system must decide whether it should give or not a result in the new fashion or keep on with the old kind of output.", "labels": [], "entities": []}, {"text": "A similar example is the decision about showing or not ads related to the query.", "labels": [], "entities": []}, {"text": "Showing wrong ads harms the business model more than showing nothing.", "labels": [], "entities": []}, {"text": "A third example more related to Natural Language Processing is the Machine Reading evaluation through reading comprehension tests.", "labels": [], "entities": [{"text": "Machine Reading evaluation", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.8378934860229492}]}, {"text": "In this case, where multiple choices fora question are offered, choosing a wrong option should be punished against leaving the question unanswered.", "labels": [], "entities": []}, {"text": "In the latter case, the use of utility functions is a very common option.", "labels": [], "entities": []}, {"text": "However, utility functions give arbitrary value to not responding and ignore the system's behavior showed when it responds (see Section 2).", "labels": [], "entities": []}, {"text": "To avoid this, we present c@1 measure (Section 2.2), as an extension of accuracy (the proportion of correctly answered questions).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9989161491394043}]}, {"text": "In Section 3 we show that no other extension produces a sensible measure.", "labels": [], "entities": []}, {"text": "In Section 4 we evaluate c@1 in terms of stability, discrimination power and sensibility, and some real examples of its behavior are given in the context of Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7959062159061432}]}, {"text": "Related work is discussed in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "When anew measure is proposed, it is important to study the reliability of the results obtained using that measure.", "labels": [], "entities": [{"text": "reliability", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9847169518470764}]}, {"text": "For this purpose, we have chosen the method described by for assessing the stability and discrimination power, as well as the method described by for examining the sensitivity of our measure.", "labels": [], "entities": []}, {"text": "These methods have been used for studying IR metrics (showing similar results with the methods based on statistics)), as well as for evaluating the reliability of other QA measures different to the ones studied here.", "labels": [], "entities": [{"text": "IR", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9669961929321289}]}, {"text": "We have compared the results over c@1 with the ones obtained using both accuracy and the utility function (UF) defined in Formula (1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.999521017074585}, {"text": "utility function (UF)", "start_pos": 89, "end_pos": 110, "type": "METRIC", "confidence": 0.7842840075492858}]}, {"text": "This comparison is useful to show how confident can a researcher be with the results obtained using each evaluation measure.", "labels": [], "entities": []}, {"text": "In the following subsections we will first show the data used for our study.", "labels": [], "entities": []}, {"text": "Then, the experiments about stability and sensitivity will be described.", "labels": [], "entities": [{"text": "sensitivity", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9631581902503967}]}, {"text": "In addition to the theoretical study, we undertook a study to interpret the results obtained by real systems in areal scenario.", "labels": [], "entities": []}, {"text": "The aim is to compare the results of the proposed c@1 measure with accuracy in order to compare their behavior.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9995187520980835}]}, {"text": "For this purpose we inspected the real systems runs in the data set.", "labels": [], "entities": []}, {"text": "shows a couple of examples where two systems have answered correctly a similar number of questions.", "labels": [], "entities": []}, {"text": "For example, this is the case of icia091ro and uaic092ro that, therefore, obtain almost the same accuracy value.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9989799857139587}]}, {"text": "However, icia091ro has returned less incorrect answers by not responding some questions.", "labels": [], "entities": []}, {"text": "This is the kind of behavior we want to measure and reward.", "labels": [], "entities": []}, {"text": "shows how accuracy is sensitive only to the number of correct answers whereas c@1 is able to distinguish when systems keep the number of correct answers but reduce the number of incorrect ones by not responding to some.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991870522499084}]}, {"text": "The same reasoning is applicable to loga092de compared to base092de for German.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results obtained applying the swap method to  accuracy, c@1 and UF at 95% of confidence, with c =  250: (i) Absolute difference required; (ii) Highest value  obtained; (iii) Relative difference required ((i)/(ii)); (iv)  percentage of comparisons that accomplish the required  difference (sensitivity)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9993876218795776}, {"text": "UF", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.8766242861747742}, {"text": "Absolute difference", "start_pos": 118, "end_pos": 137, "type": "METRIC", "confidence": 0.9773489832878113}, {"text": "Relative difference", "start_pos": 184, "end_pos": 203, "type": "METRIC", "confidence": 0.964417964220047}]}, {"text": " Table 3: Example of system results in QA@CLEF 2009.  (i) number of questions correctly answered; (ii) number  of questions incorrectly answered; (iii) number of unan- swered questions.", "labels": [], "entities": [{"text": "QA@CLEF 2009", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.7803320735692978}]}]}