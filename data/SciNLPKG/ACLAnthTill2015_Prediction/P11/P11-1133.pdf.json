{"title": [{"text": "Rare Word Translation Extraction from Aligned Comparable Documents", "labels": [], "entities": [{"text": "Rare Word Translation Extraction from Aligned Comparable Documents", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8104525916278362}]}], "abstractContent": [{"text": "We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification.", "labels": [], "entities": [{"text": "rare word bilingual extraction", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.610087163746357}]}, {"text": "We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach.", "labels": [], "entities": []}, {"text": "We test our hypothesis on different pairs of languages and corpora.", "labels": [], "entities": []}, {"text": "We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1 to 5 occurrences).", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.999283492565155}]}, {"text": "Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9995997548103333}, {"text": "classification of Chinese-English translations", "start_pos": 152, "end_pos": 198, "type": "TASK", "confidence": 0.8066965043544769}]}, {"text": "Our method is therefore even potentially applicable to low resources languages without training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rare words have long been a challenge to translate automatically using statistical methods due to their low occurrences.", "labels": [], "entities": []}, {"text": "However, the Zipf's Law claims that, for any corpus of natural language text, the frequency of a word w n (n being its rank in the frequency table) will be roughly twice as high as the frequency of word w n+1 . The logical consequence is that in any corpus, there are very few frequent words and many rare words.", "labels": [], "entities": []}, {"text": "We propose a novel approach to extract rare word translations from comparable corpora, relying on two main features.", "labels": [], "entities": [{"text": "extract rare word translations from comparable corpora", "start_pos": 31, "end_pos": 85, "type": "TASK", "confidence": 0.8450840200696673}]}, {"text": "The first feature is the context-vector similarity: each word is characterized by its context in both source and target corpora, words in translation should have similar context in both languages.", "labels": [], "entities": []}, {"text": "The second feature follows the assumption that specific terms and their translations should appear together often in documents on the same topic, and rarely in non-related documents.", "labels": [], "entities": []}, {"text": "This is the general assumption behind early work on bilingual lexicon extraction from parallel documents using sentence boundary as the context window size for cooccurrence computation, we suggest to extend it to aligned comparable documents using document as the context window.", "labels": [], "entities": [{"text": "bilingual lexicon extraction from parallel documents", "start_pos": 52, "end_pos": 104, "type": "TASK", "confidence": 0.7845493505398432}]}, {"text": "This document context is too large for co-occurrence computation of functional words or high frequency content words, but we show through observations and experiments that this window size is appropriate for rare words.", "labels": [], "entities": []}, {"text": "Both these features are unreliable when the number of occurrences of words are low.", "labels": [], "entities": []}, {"text": "We suggest however that they are complementary and can be used together in a machine learning approach.", "labels": [], "entities": []}, {"text": "Moreover, we suggest that the model trained for one pair of languages can be successfully applied to extract translations from another pair of languages.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we discuss the challenge of rare lexicon extraction, explaining the reasons why classic approaches on comparable corpora fail at dealing with rare words.", "labels": [], "entities": [{"text": "rare lexicon extraction", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.6625416576862335}]}, {"text": "We then discuss in section 3 the concept of aligned comparable documents and how we exploited those documents for bilingual lexicon extraction in section 4.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.6432362596193949}]}, {"text": "We present our resources and implementation in section 5 then carryout and comment several experiments in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our approach, we needed evaluation lists of terms for which translations are already known.", "labels": [], "entities": []}, {"text": "We used the Medical Subject Headlines, from the UMLS meta-thesaurus 6 which provides a lexicon of specialized, medical terminology, notably in Spanish, English and French.", "labels": [], "entities": [{"text": "UMLS meta-thesaurus 6", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9652338624000549}]}, {"text": "We used the LDC lexicon presented in the previous section for ChineseEnglish.", "labels": [], "entities": [{"text": "ChineseEnglish", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9587012529373169}]}, {"text": "From these resources, we selected all the source words that appears from 1 to 5 times in the corpora in order to build the evaluation lists.", "labels": [], "entities": []}, {"text": "We ran three different experiments.", "labels": [], "entities": []}, {"text": "Experiment I compares the accuracy of the context-vector similarity and the co-occurrence model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9995645880699158}]}, {"text": "Experiment II uses supervised classification with both features.", "labels": [], "entities": []}, {"text": "Experiment III extracts translation from a pair of languages, using a classifier trained on another pair of languages.", "labels": [], "entities": []}, {"text": "We split the French-English part of the Wikipedia corpus into different samples: the first sample contains 500 pairs of documents.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8565009236335754}]}, {"text": "We then aggregated more documents to this initial sample to test different sizes of corpora.", "labels": [], "entities": []}, {"text": "We built the sample in order to ensure hapaxes in the whole corpus are hapaxes in all subsets.", "labels": [], "entities": []}, {"text": "That is, we ensured the 431 hapaxes in the evaluation lists are represented in the 500 documents subset.", "labels": [], "entities": []}, {"text": "We extracted translations in two different ways: 1.", "labels": [], "entities": []}, {"text": "using the co-occurrence model; 2.", "labels": [], "entities": []}, {"text": "using the context-vector based approach, with the same evaluation lists.", "labels": [], "entities": []}, {"text": "The accuracy is computed on 1,000 pairs of translations from the set of oracle translations, and measures the amount of correct translations found for the 10 best ranks (T op 10 ) after ranking the candidates according to their score (context-vector similarity or co-occurrence model).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995204210281372}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "We can draw two conclusions out of these results.", "labels": [], "entities": []}, {"text": "First, the size of the corpus influences the quality of the bilingual lexicon extraction when using the co-occurrence model.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.7170963088671366}]}, {"text": "This is especially interesting with hapaxes, for which frequency does not change with the increase of the size of the corpora.", "labels": [], "entities": []}, {"text": "The accuracy is improved by adding more information to the corpus, even if this additional information does not cover the pairs of translations we are looking for.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994233846664429}]}, {"text": "The added documents will weaken the association of incorrect translations, without changing the association for rare terms translations.", "labels": [], "entities": []}, {"text": "For example, the precision for hapaxes using the co-occurrence model ranges from less than 1% when using only 500 pairs of documents, to about 13% when using all documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995952248573303}]}, {"text": "The second conclusion is that the co-occurrence model outperforms the context-vector similarity.", "labels": [], "entities": []}, {"text": "However, both these approaches still perform poorly.", "labels": [], "entities": []}, {"text": "In the next experiment, we propose to combine them using supervised classification.", "labels": [], "entities": []}, {"text": "For each corpus or combination of corporaEnglish-Spanish, English-French, Spanish-French and Chinese-English, we ran three experiments, using the following features for supervised learning of translations: \u2022 the context-vector similarity; \u2022 the co-occurrence model; \u2022 both features together.", "labels": [], "entities": []}, {"text": "The parameters are discussed in section 4.3.", "labels": [], "entities": []}, {"text": "We used all the oracle translations to train the positive values.", "labels": [], "entities": []}, {"text": "Results are presented in table 2, they are computed using a 10-folds cross validation.", "labels": [], "entities": []}, {"text": "Class T refers to \"Translation\", \u00acT to \"Non-Translation\".", "labels": [], "entities": []}, {"text": "The evaluation of precision/recall/F-Measure for the class \"Translation\" are given in equation 4 to 6.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9995705485343933}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9855344295501709}, {"text": "F-Measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9976119995117188}, {"text": "Translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9527190923690796}]}, {"text": "These results show first that one feature is generally not discriminatory enough to discern correct translation and non-translation pairs.", "labels": [], "entities": []}, {"text": "For example with Spanish-English, by using context-vector similarity only, we obtained very high recall/precision for the classification of \"Non-Translation\", but null precision/recall for the classification of \"Translation\".", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9981780052185059}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.8525903224945068}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.7849768996238708}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9703714847564697}]}, {"text": "In some other cases, we obtained high precision but poor recall with one feature only, which is not a usefully result as well since most of the correct translations are still labeled as \"Non-Translation\".", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.998202919960022}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9985644221305847}]}, {"text": "However, when using both features, the precision is strongly improved up to 98% (English-Spanish or French-Spanish) with a high recall of about 90% for class T.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9997239708900452}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9994564652442932}]}, {"text": "We also achieved about 86%/75% precision/recall in the case of Chinese-English, even though they are very distant languages.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9694031476974487}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9771686792373657}]}, {"text": "This last result is also very promising since it has been obtained from a fully automatically built corpus.", "labels": [], "entities": []}, {"text": "shows some examples of correctly labeled \"Translation\".", "labels": [], "entities": [{"text": "Translation\"", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.8397281467914581}]}, {"text": "The decision trees obtained indicate that, in general, word pairs with very high co-occurrence model scores are translations, and that the context-vector similarity disambiguate candidates with lower cooccurrence model scores.", "labels": [], "entities": []}, {"text": "Interestingly, the trained decision trees are very similar between the different pairs of languages, which inspired the next experiment.", "labels": [], "entities": []}, {"text": "In the last experiment, we focused on using the knowledge acquired with a given pair of languages to recognize proper translation pairs using a different pair of languages.", "labels": [], "entities": []}, {"text": "For this experiment, we used the data from one corpus to train the classifier, and used the data from another combination of languages as the test set.", "labels": [], "entities": []}, {"text": "Results are displayed in table 4.", "labels": [], "entities": []}, {"text": "These last results are of great interest because they show that translation pairs can be correctly classified even with a classifier trained on another pair of languages.", "labels": [], "entities": []}, {"text": "This is very promising because it allows one to prospect new languages using knowledge acquired on a known pairs of languages.", "labels": [], "entities": []}, {"text": "As an example, we reached a 77% F-Measure for Chinese-English alignment using a classifier trained on Spanish-French features.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9931922554969788}]}, {"text": "This not only confirms the precision/recall of our approach in general, but also shows that the model obtained by training tends to be very stable and accurate across different pairs of languages and different corpora.: Experiment II and III: examples of rare word translations found by our algorithm.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9994007349014282}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9896925091743469}]}, {"text": "Note that even though some words such as \"kindergarten\" are not rare in general, they occur with very low frequency in the test corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for all parts of all corpora.", "labels": [], "entities": []}, {"text": " Table 2: Experiment II: results of binary classification for  \"Translation\" and \"Non-Translation\".", "labels": [], "entities": []}, {"text": " Table 4: Experiment III: Precision/Recall/F-Measure for label \"Translation\", obtained for all training/testing set com- binations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9827519059181213}, {"text": "Recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.7206237316131592}, {"text": "F-Measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.8880857825279236}, {"text": "Translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.7816458344459534}]}]}