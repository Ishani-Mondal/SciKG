{"title": [], "abstractContent": [{"text": "The challenges of Named Entities Recognition (NER) for tweets lie in the insufficient information in a tweet and the unavailabil-ity of training data.", "labels": [], "entities": [{"text": "Named Entities Recognition (NER)", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.8116938372453054}]}, {"text": "We propose to combine a K-Nearest Neighbors (KNN) classi-fier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to tackle these challenges.", "labels": [], "entities": []}, {"text": "The KNN based classifier conducts pre-labeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet.", "labels": [], "entities": []}, {"text": "The semi-supervised learning plus the gazetteers alleviate the lack of training data.", "labels": [], "entities": []}, {"text": "Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semi-supervised learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entities Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (.", "labels": [], "entities": [{"text": "Named Entities Recognition (NER)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7999560634295145}, {"text": "identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations", "start_pos": 72, "end_pos": 200, "type": "TASK", "confidence": 0.7248728904459212}]}, {"text": "Proposed solutions to NER fall into three categories: 1) The rule-based (); 2) the machine learning based ; and 3) hybrid methods (Jansche and).", "labels": [], "entities": [{"text": "NER", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9393113255500793}]}, {"text": "With the availability of annotated corpora, such as ACE05,) and * This work has been done while the author was visiting Microsoft Research Asia.", "labels": [], "entities": [{"text": "ACE05", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9391477704048157}]}, {"text": "CoNLL03, the data driven methods now become the dominating methods.", "labels": [], "entities": [{"text": "CoNLL03", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9365590810775757}]}, {"text": "However, current NER mainly focuses on formal text such as news articles).", "labels": [], "entities": [{"text": "NER", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9485851526260376}]}, {"text": "Exceptions include studies on informal text such as emails, blogs, clinical notes.", "labels": [], "entities": []}, {"text": "Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, anew genre of text, which are short, informal, ungrammatical and noise prone.", "labels": [], "entities": []}, {"text": "For example, the average F1 of the Stanford NER () , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8%) to 45.8% on tweets.", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9958000779151917}, {"text": "Stanford NER", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8903398215770721}, {"text": "CoNLL03 shared task data set", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.8808080315589905}]}, {"text": "Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules.", "labels": [], "entities": []}, {"text": "However, manually creating them is tedious and prohibitively unaffordable.", "labels": [], "entities": []}, {"text": "Proposed solutions to alleviate this issue include: 1) Domain adaption, which aims to reuse the knowledge of the source domain in a target domain.", "labels": [], "entities": [{"text": "Domain adaption", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.733513280749321}]}, {"text": "Two recent examples are , which uses data that is informative about the target domain and also easy to be labeled to bridge the two domains, and, which introduces a high-level rule language, called NERL, to build the general and domain specific NER systems; and 2) semi-supervised learning, which aims to use the abundant unlabeled data to compensate for the lack of annotated data. is one such example.", "labels": [], "entities": []}, {"text": "Another challenge is the limited information in tweet.", "labels": [], "entities": []}, {"text": "Two factors contribute to this difficulty.", "labels": [], "entities": []}, {"text": "One 359 is the tweet's informal nature, making conventional features such as part-of-speech (POS) and capitalization not reliable.", "labels": [], "entities": []}, {"text": "The performance of current NLP tools drops sharply on tweets.", "labels": [], "entities": []}, {"text": "For example, OpenNLP 1 , the state-of-the-art POS tagger, gets only an accuracy of 74.0% on our test data set.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.6661265045404434}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9992947578430176}, {"text": "test data set", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.8269074757893881}]}, {"text": "The other is the tweet's short nature, leading to the excessive abbreviations or shorthand in tweets, and the availability of very limited context information.", "labels": [], "entities": []}, {"text": "Tackling this challenge, ideally, requires adapting related NLP tools to fit tweets, or normalizing tweets to accommodate existing tools, both of which are hard tasks.", "labels": [], "entities": []}, {"text": "We propose a novel NER system to address these challenges.", "labels": [], "entities": []}, {"text": "Firstly, a K-Nearest Neighbors (KNN) based classifier is adopted to conduct word level classification, leveraging the similar and recently labeled tweets.", "labels": [], "entities": [{"text": "word level classification", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6862776676813761}]}, {"text": "Following the two-stage prediction aggregation methods (), such pre-labeled results, together with other conventional features used by the state-of-the-art NER systems, are fed into a linear Conditional Random Fields (CRF) () model, which conducts fine-grained tweet level NER.", "labels": [], "entities": []}, {"text": "Furthermore, the KNN and CRF model are repeatedly retrained with an incrementally augmented training set, into which high confidently labeled tweets are added.", "labels": [], "entities": []}, {"text": "Indeed, it is the combination of KNN and CRF under a semi-supervised learning framework that differentiates ours from the existing.", "labels": [], "entities": []}, {"text": "Finally, following, 30 gazetteers are used, which cover common names, countries, locations, temporal expressions, etc.", "labels": [], "entities": []}, {"text": "These gazetteers represent general knowledge across domains.", "labels": [], "entities": []}, {"text": "The underlying idea of our method is to combine global evidence from KNN and the gazetteers with local contextual information, and to use common knowledge and unlabeled tweets to makeup for the lack of training data.", "labels": [], "entities": [{"text": "KNN", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9101022481918335}]}, {"text": "12,245 tweets are manually annotated as the test data set.", "labels": [], "entities": []}, {"text": "Experimental results show that our method outperforms the baselines.", "labels": [], "entities": []}, {"text": "It is also demonstrated that integrating KNN classified results into the CRF model and semi-supervised learning considerably boost the performance.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows.", "labels": [], "entities": []}, {"text": "1 http://sourceforge.net/projects/opennlp/ 1.", "labels": [], "entities": []}, {"text": "We propose to a novel method that combines a KNN classifier with a conventional CRF based labeler under a semi-supervised learning framework to combat the lack of information in tweet and the unavailability of training data.", "labels": [], "entities": []}, {"text": "2. We evaluate our method on a human annotated data set, and show that our method outperforms the baselines and that both the combination with KNN and the semi-supervised learning strategy are effective.", "labels": [], "entities": []}, {"text": "The rest of our paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we introduce related work.", "labels": [], "entities": []}, {"text": "In Section 3, we formally define the task and present the challenges.", "labels": [], "entities": []}, {"text": "In Section 4, we detail our method.", "labels": [], "entities": []}, {"text": "In Section 5, we evaluate our method.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our method on a manually annotated data set and show that our system outperforms the baselines.", "labels": [], "entities": []}, {"text": "The contributions of the combination of KNN and CRF as well as the semisupervised learning are studied, respectively.", "labels": [], "entities": []}, {"text": "For every type of named entity, Precision (Pre.), recall (Rec.) and F1 are used as the evaluation metrics.", "labels": [], "entities": [{"text": "Precision (Pre.)", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.8677174150943756}, {"text": "recall (Rec.)", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.9488237649202347}, {"text": "F1", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9974216222763062}]}, {"text": "Precision is a measure of what percentage the output labels are correct, and recall tells us to what percentage the labels in the gold-standard data set are correctly labeled, while F1 is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9928452372550964}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9993090629577637}, {"text": "gold-standard data set", "start_pos": 130, "end_pos": 152, "type": "DATASET", "confidence": 0.8323349555333456}, {"text": "F1", "start_pos": 182, "end_pos": 184, "type": "METRIC", "confidence": 0.9979143738746643}, {"text": "precision", "start_pos": 209, "end_pos": 218, "type": "METRIC", "confidence": 0.9992714524269104}, {"text": "recall", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9959467053413391}]}, {"text": "For the overall performance, we use the average Precision, Recall and F1, where the weight of each name entity type is proportional to the number of entities of that type.", "labels": [], "entities": [{"text": "Precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.998525083065033}, {"text": "Recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9839147329330444}, {"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9919860363006592}]}, {"text": "These metrics are widely used by existing NER systems to evaluate their performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9679053425788879}]}], "tableCaptions": [{"text": " Table 1: Overall experimental results.", "labels": [], "entities": []}, {"text": " Table 2: Experimental results on PERSON.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.46672749519348145}]}, {"text": " Table 3: Experimental results on PRODUCT.", "labels": [], "entities": [{"text": "PRODUCT", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.8214048743247986}]}, {"text": " Table 4: Experimental results on LOCATION.", "labels": [], "entities": [{"text": "LOCATION", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.6893270015716553}]}, {"text": " Table 5: Experimental results on ORGANIZATION.", "labels": [], "entities": [{"text": "ORGANIZATION", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.968343198299408}]}, {"text": " Table 6: Overall performance of our system with and  without the KNN classifier, respectively.", "labels": [], "entities": []}, {"text": " Table 7: Overview performance of the CRF labeler (com- bined with KNN) with different feature sets.", "labels": [], "entities": []}, {"text": " Table 8: Performance of our system with and without  semi-supervised learning, respectively.", "labels": [], "entities": []}]}