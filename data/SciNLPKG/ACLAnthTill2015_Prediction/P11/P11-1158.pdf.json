{"title": [{"text": "Efficient CCG Parsing: A* versus Adaptive Supertagging", "labels": [], "entities": [{"text": "Supertagging", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.4312339723110199}]}], "abstractContent": [{"text": "We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG).", "labels": [], "entities": [{"text": "parsing of Combinatory Categorial Grammar (CCG)", "start_pos": 94, "end_pos": 141, "type": "TASK", "confidence": 0.758526299148798}]}, {"text": "First we consider adap-tive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser's search space using a separate sequence model.", "labels": [], "entities": []}, {"text": "Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG.", "labels": [], "entities": []}, {"text": "In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time.", "labels": [], "entities": [{"text": "parser", "start_pos": 57, "end_pos": 63, "type": "TASK", "confidence": 0.9603925347328186}, {"text": "A* parsing", "start_pos": 130, "end_pos": 140, "type": "TASK", "confidence": 0.6717246572176615}]}, {"text": "By itself , A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline.", "labels": [], "entities": [{"text": "A", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9709330201148987}]}, {"text": "Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Efficient parsing of Combinatorial Categorial Grammar) is a longstanding problem in computational linguistics.", "labels": [], "entities": [{"text": "Efficient parsing of Combinatorial Categorial Grammar", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7565128008524576}]}, {"text": "Even with practical CCG that are strongly context-free, parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants.", "labels": [], "entities": [{"text": "parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.968646228313446}, {"text": "Penn Treebank-style context-free grammars", "start_pos": 93, "end_pos": 134, "type": "DATASET", "confidence": 0.9077450633049011}]}, {"text": "Where atypical Penn Treebank grammar may have fewer than 100 nonterminals), we found that a CCG grammar derived from CCGbank contained nearly 1600.", "labels": [], "entities": [{"text": "Penn Treebank grammar", "start_pos": 15, "end_pos": 36, "type": "DATASET", "confidence": 0.9825882315635681}]}, {"text": "The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations.", "labels": [], "entities": []}, {"text": "The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexical categories considered for each word, using the output of a supertagger, a sequence model over these categories ().", "labels": [], "entities": [{"text": "parsing of CCG", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.8696568608283997}]}, {"text": "Variations on this approach drive the widelyused, broad coverage C&C parser.", "labels": [], "entities": [{"text": "broad coverage C&C parser", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.5220702737569809}]}, {"text": "However, pruning means approximate search: if a lexical category used by the highest probability derivation is pruned, the parser will not find that derivation ( \u00a72).", "labels": [], "entities": []}, {"text": "Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation.", "labels": [], "entities": []}, {"text": "Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss inaccuracy ( \u00a73).", "labels": [], "entities": []}, {"text": "Can we improve on this tradeoff?", "labels": [], "entities": []}, {"text": "The line of investigation we pursue in this paper is to consider more efficient exact algorithms.", "labels": [], "entities": []}, {"text": "In particular, we test different variants of the classical A* algorithm, which has met with success in Penn Treebank parsing with context-free grammars (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9600139558315277}]}, {"text": "We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su- pertagging.", "labels": [], "entities": []}, {"text": "Our empirical results show that on the unpruned set of lexical categories, heuristics employed for context-free grammars show substantial speedups in hardware-independent metrics of parser effort ( \u00a74).", "labels": [], "entities": []}, {"text": "To understand how this compares to the CKY baseline, we conduct a carefully controlled set of timing experiments.", "labels": [], "entities": [{"text": "CKY baseline", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9389581382274628}]}, {"text": "Although our results show that improvements on hardware-independent metrics do not always translate into improvements in CPU time due to increased processing costs that are hidden by these metrics, they also show that when the lexical categories are pruned using the output of a supertagger, then we can still improve efficiency by 15% with A* techniques ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we used the generative CCG parser of.", "labels": [], "entities": [{"text": "generative CCG parser", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8713229695955912}]}, {"text": "Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.", "labels": [], "entities": []}, {"text": "Although not quite as accurate as the discriminative parser of  in our preliminary experiments, this parser is still quite competitive.", "labels": [], "entities": []}, {"text": "It is written in Java and implements the CKY algorithm with a global pruning threshold of 10 \u22124 for the models we use.", "labels": [], "entities": []}, {"text": "We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG; and HWDep, a headword dependency model which is the best performing model of the parser.", "labels": [], "entities": []}, {"text": "The PCFG model simply generates a treetop down and uses very simple structural probabilities while the HWDep model conditions node expansions on headwords and their lexical categories.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9147225618362427}, {"text": "HWDep", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.8438052535057068}]}, {"text": "For supertagging we used Dennis Mehay's implementation, which follows Clark.", "labels": [], "entities": []}, {"text": "Due to differences in smoothing of the supertagging and parsing models, we occasionally drop supertags returned by the supertagger because they do not appear in the parsing model . Evaluation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.965112566947937}]}, {"text": "All experiments were conducted on CCGBank, a right-most normal-form CCG version of the Penn Treebank.", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9537455439567566}, {"text": "Penn Treebank", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9947713017463684}]}, {"text": "Models were trained on sections 2-21, tuned on section 00, and tested on section 23.", "labels": [], "entities": []}, {"text": "Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery); we evaluate on all sentences and thus penalise lower coverage.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.8349316120147705}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8583905696868896}, {"text": "predicate argument structure recovery", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.6893142908811569}]}, {"text": "All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs 4 .  To compare approaches, we extended our baseline parser to support A* search.", "labels": [], "entities": []}, {"text": "Following) we restrict our experiments to sentences on which we can perform exact search via using the same subset of section 00 as in \u00a73.2.", "labels": [], "entities": []}, {"text": "Before considering CPU time, we first evaluate the amount of work done by the parser using three hardwareindependent metrics.", "labels": [], "entities": []}, {"text": "We measure the number of edges pushed (Pauls and Klein, 2009a) and edges popped, corresponding to the insert/decrease-key operations and remove operation of the priority queue, respectively.", "labels": [], "entities": []}, {"text": "Finally, we measure the number of traversals, which counts the number of edge weights computed, regardless of whether the weight is discarded due to the prior existence of a better weight.", "labels": [], "entities": []}, {"text": "This latter metric seems to be the most accurate account of the work done by the parser.", "labels": [], "entities": []}, {"text": "Due to differences in the PCFG and HWDep models, we considered different A* variants: for the PCFG model we use a simple A* with a precom-puted heuristic, while for the the more complex HWDep model, we used a hierarchical A* algorithm () based on a simple grammar projection that we designed.", "labels": [], "entities": []}, {"text": "Hardware-independent metrics are useful for understanding agenda-based algorithms, but what we actually care about is CPU time.", "labels": [], "entities": []}, {"text": "We were not aware of any past work that measures A* parsers in terms of CPU time, but as this is the real objective we feel that experiments of this type are important.", "labels": [], "entities": []}, {"text": "This is especially true in real implementations because the savings in edges processed by an agenda parser come at a cost: operations on the priority queue data structure can add significant runtime.", "labels": [], "entities": []}, {"text": "Timing experiments of this type are very implementation-dependent, so we took care to implement the algorithms as cleanly as possible and to reuse as much of the existing parser code as we could.", "labels": [], "entities": []}, {"text": "An important implementation decision for agenda-based algorithms is the data structure used to implement the priority queue.", "labels": [], "entities": []}, {"text": "Preliminary experiments showed that a Fibonacci heap implementation outperformed several alternatives: Brodal queues, binary heaps, binomial heaps, and pairing heaps.", "labels": [], "entities": []}, {"text": "We carried out timing experiments on the best A* parsers for each model, comparing them with our CKY implementation and the agenda-based CKY simulation EXH; we used the same data as in \u00a73.2.", "labels": [], "entities": [{"text": "CKY simulation EXH", "start_pos": 137, "end_pos": 155, "type": "DATASET", "confidence": 0.6817978620529175}]}, {"text": "presents the cumulative running times with and without adaptive supertagging average over ten runs, while reports F-scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9967437982559204}]}, {"text": "Although the timing results of the agenda-based parsers track the hardware-independent metrics, they start at a significant disadvantage to exhaustive CKY with a simple control loop.", "labels": [], "entities": []}, {"text": "This is most evident when looking at the timing results for EXH, which in the case of the full PCFG model requires more than twice the time than the CKY algorithm that it simulates.", "labels": [], "entities": [{"text": "timing", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9790184497833252}, {"text": "EXH", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.7879367470741272}]}, {"text": "A*    makes modest CPU-time improvements in parsing the full space of the HWDep model.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9523353576660156}, {"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9836132526397705}, {"text": "HWDep", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9135355949401855}]}, {"text": "Although this decreases the time required to obtain the highest accuracy, it is still a substantial tradeoff in speed compared with AST.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.998325765132904}, {"text": "speed", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9948728680610657}, {"text": "AST", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.8535825610160828}]}, {"text": "On the other hand, the AST tradeoff improves significantly: by combining AST with A* we observe a decrease in running time of 15% for the A* NULL parser of the HWDep model over CKY.", "labels": [], "entities": [{"text": "AST", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9496814012527466}, {"text": "CKY", "start_pos": 177, "end_pos": 180, "type": "DATASET", "confidence": 0.9261625409126282}]}, {"text": "As the CKY baseline with AST is very strong, this result shows that A* holds real promise for CCG parsing.", "labels": [], "entities": [{"text": "A", "start_pos": 68, "end_pos": 69, "type": "METRIC", "confidence": 0.9673588275909424}, {"text": "CCG parsing", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.8473441898822784}]}], "tableCaptions": [{"text": " Table 2: Headword dependency model factorisation, backoff levels are denoted by '#' between conditioning variables:  A # B # C indicates that\u02c6Pthat\u02c6 that\u02c6P (. . . |A, B, C) is interpolated with\u02c6Pwith\u02c6 with\u02c6P (. . . |A, B), which is an interpolation of\u02c6Pof\u02c6 of\u02c6P . . . |A, B)  and\u02c6Pand\u02c6 and\u02c6P (. . . |A). Variables c P and w P represent, respectively, the head lexical category and headword of category P .", "labels": [], "entities": []}, {"text": " Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.", "labels": [], "entities": [{"text": "precision (LP/UP)", "start_pos": 201, "end_pos": 218, "type": "METRIC", "confidence": 0.9077740013599396}, {"text": "recall", "start_pos": 220, "end_pos": 226, "type": "METRIC", "confidence": 0.9984742999076843}, {"text": "F-score (LF/UF)", "start_pos": 240, "end_pos": 255, "type": "METRIC", "confidence": 0.911906232436498}]}, {"text": " Table 5: Results on CCGbank section 23 when applying adaptive supertagging (AST) to two models of a CCG parser.", "labels": [], "entities": []}, {"text": " Table 6: Breakdown of the number of sentences parsed  for the HWDep (AST) model (see", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9654892683029175}, {"text": "HWDep (AST) model", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.8567880511283874}]}, {"text": " Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.", "labels": [], "entities": [{"text": "PCFG grammar", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.9306109547615051}]}, {"text": " Table 8: Parsing time in seconds of CKY and agenda- based parsers with and without adaptive supertagging.", "labels": [], "entities": []}, {"text": " Table 9: Labelled F-score of exact CKY and agenda- based parsers with/without adaptive supertagging. All  parses have the same probabilities, thus variances are due  to implementation-dependent differences in tiebreaking.", "labels": [], "entities": [{"text": "F-score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9205619692802429}]}]}