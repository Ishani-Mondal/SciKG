{"title": [{"text": "Joint Hebrew Segmentation and Parsing using a PCFG-LA Lattice Parser", "labels": [], "entities": [{"text": "Hebrew Segmentation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7719821631908417}, {"text": "PCFG-LA Lattice Parser", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.8774625261624655}]}], "abstractContent": [{"text": "We experiment with extending a lattice parsing methodology for parsing Hebrew (Gold-berg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser.", "labels": [], "entities": [{"text": "parsing Hebrew", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.8748555779457092}, {"text": "PCFG-LA Berkeley Parser", "start_pos": 178, "end_pos": 201, "type": "DATASET", "confidence": 0.9460011919339498}]}, {"text": "We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unseg-mented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.9985492825508118}, {"text": "error reduction", "start_pos": 203, "end_pos": 218, "type": "METRIC", "confidence": 0.9702566564083099}]}, {"text": "This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.", "labels": [], "entities": [{"text": "lattice parsing", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.6689650118350983}, {"text": "parsing over uncertain inputs", "start_pos": 100, "end_pos": 129, "type": "TASK", "confidence": 0.8189135044813156}]}], "introductionContent": [{"text": "Most work on parsing assumes that the lexical items in the yield of a parse tree are fully observed, and correspond to space delimited tokens, perhaps after a deterministic preprocessing step of tokenization.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9674056172370911}]}, {"text": "While this is mostly the case for English, the situation is different in languages such as Chinese, in which word boundaries are not marked, and the Semitic languages of Hebrew and Arabic, in which various particles corresponding to function words are agglutinated as affixes to content bearing words, sharing the same space-delimited token.", "labels": [], "entities": []}, {"text": "For example, the Hebrew token bcl 1 can be interpreted as the single noun meaning \"onion\", or as a sequence of a preposition and a noun b-cl meaning \"in (the) shadow\".", "labels": [], "entities": []}, {"text": "In such languages, the sequence of lexical We adopt here the transliteration scheme of items corresponding to an input string is ambiguous, and cannot be determined using a deterministic procedure.", "labels": [], "entities": []}, {"text": "In this work, we focus on constituency parsing of Modern Hebrew (henceforth Hebrew) from raw unsegmented text.", "labels": [], "entities": [{"text": "constituency parsing of Modern Hebrew (henceforth Hebrew) from raw unsegmented text", "start_pos": 26, "end_pos": 109, "type": "TASK", "confidence": 0.8622459219052241}]}, {"text": "A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process, in which the input string is pre-segmented prior to handing it to a parser.", "labels": [], "entities": []}, {"text": "The shortcoming of this method, as noted by, is that many segmentation decisions cannot be resolved based on local context alone.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.972126841545105}]}, {"text": "Rather, they may depend on long distance relations and interact closely with the syntactic structure of the sentence.", "labels": [], "entities": []}, {"text": "Thus, segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9760347604751587}, {"text": "parsing process", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.8986198902130127}]}, {"text": "demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text.", "labels": [], "entities": [{"text": "lattice parsing", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6551846265792847}, {"text": "parsing of Hebrew text", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8540280610322952}]}, {"text": "They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies.", "labels": [], "entities": []}, {"text": "showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9632075428962708}, {"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9531930685043335}]}, {"text": "Recently, demonstrated the effectiveness of lattice-parsing for parsing Arabic.", "labels": [], "entities": [{"text": "parsing Arabic", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.919583797454834}]}, {"text": "Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser ().", "labels": [], "entities": [{"text": "lattice parsing", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.698173850774765}]}], "datasetContent": [{"text": "Data In all the experiments we use Ver.2 of the Hebrew treebank (, which was converted to use the tagset of the MILA morphological analyzer ().", "labels": [], "entities": [{"text": "Ver.2", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.7625539302825928}, {"text": "Hebrew treebank", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9203937351703644}, {"text": "MILA morphological analyzer", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.580491324265798}]}, {"text": "We use the same splits as in previous work, with a training set of 5240 sentences (484-5724) and a test set of 483 sentences . During development, we evaluated on a random subset of 100 sentences from the training set.", "labels": [], "entities": []}, {"text": "Unless otherwise noted, we used the basic non-terminal categories, without any extended information available in them.", "labels": [], "entities": []}, {"text": "Gold Segmentation and Tagging To assess the adequacy of the Berkeley parser for Hebrew, we performed baseline experiments in which either gold segmentation and tagging or just gold segmentation were available to the parser.", "labels": [], "entities": []}, {"text": "The numbers are very high: an F-measure of about 88.8% for the gold segmentation and tagging, and about 82.8% for gold segmentation only.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9996485710144043}, {"text": "gold segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7144699990749359}, {"text": "gold segmentation", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.6468732506036758}]}, {"text": "This shows the adequacy of the PCFG-LA methodology for parsing the Hebrew treebank, but also goes to show the highly ambiguous nature of the tagging.", "labels": [], "entities": [{"text": "parsing the Hebrew treebank", "start_pos": 55, "end_pos": 82, "type": "DATASET", "confidence": 0.8656273931264877}]}, {"text": "Our baseline lattice parsing experiment (without the lexicon) results in an F-score of around 76%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9995749592781067}]}, {"text": "4 Segmentation \u2192 Parsing pipeline As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMM pos-tagger ( . We then ignore the produced tagging, and pass the resulting segmented text as input to the PCFG-LA parsing model as a deterministic input (here the lattice representation is used while tagging, but the parser sees a deterministic, segmented input).", "labels": [], "entities": []}, {"text": "In the pipeline setting, we either allow the parser to assign all possible POS-tags, or restrict it to POS-tags licensed by the lexicon.", "labels": [], "entities": []}, {"text": "Lattice Parsing Experiments Our initial lattice parsing experiments with the Berkeley parser were disappointing.", "labels": [], "entities": [{"text": "Lattice Parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6153944879770279}, {"text": "lattice parsing", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.6836320161819458}]}, {"text": "The lattice seemed too permissive, allowing the parser to chose weird analyses.", "labels": [], "entities": []}, {"text": "Error analysis suggested the parser failed to distinguish among the various kinds of VPs: finite, non-finite and modals.", "labels": [], "entities": []}, {"text": "Once we annotate the treebank verbs into finite, non-finite and modals , results improve a lot.", "labels": [], "entities": []}, {"text": "Further improvement was gained by specifically marking the subject-NPs.", "labels": [], "entities": []}, {"text": "The parser was notable to correctly learn these splits on its own, but once they were manually provided it did a very good job utilizing this information.", "labels": [], "entities": []}, {"text": "8 Marking object NPs did not help on their own, and slightly degraded the performance when both subjects and objects were marked.", "labels": [], "entities": [{"text": "Marking object NPs", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.8622404336929321}]}, {"text": "It appears that the learning procedure managed to learn the structure of objects without our help.", "labels": [], "entities": []}, {"text": "In all the experiments, the use of the morphological analyzer in producing the lattice was crucial for parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.9875512719154358}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9221380949020386}]}, {"text": "Results Our final configuration (marking verbal forms and subject-NPs, using the analyzer to construct the lattice and training the parser for 5 iterations) produces remarkable parsing accuracy when parsing from unsegmented text: an F-score of 79.9% (prec: 82.3 rec: 77.6) and seg+tagging F of 93.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9248808026313782}, {"text": "parsing from unsegmented text", "start_pos": 199, "end_pos": 228, "type": "TASK", "confidence": 0.8313635289669037}, {"text": "F-score", "start_pos": 233, "end_pos": 240, "type": "METRIC", "confidence": 0.9979667663574219}, {"text": "seg+tagging F", "start_pos": 277, "end_pos": 290, "type": "METRIC", "confidence": 0.7263450920581818}]}, {"text": "The pipeline systems with the same grammar achieve substantially lower F-scores of 75.2% (without the lexicon) and 77.3 (with the lexicon).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9979212880134583}]}, {"text": "For comparison, the previous best results for parsing Hebrew are 84.1%F assuming gold segmentation and tagging  The strengths of the system can be attributed to three factors: (1) performing segmentation, tagging and parsing jointly using lattice parsing, (2) relying on an external resource (lexicon / morphological analyzer) instead of on the Treebank to provide lexical coverage and (3) using a strong syntactic model.", "labels": [], "entities": [{"text": "parsing Hebrew", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.8985451459884644}, {"text": "F", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9975329637527466}]}], "tableCaptions": [{"text": " Table 1: Parsing scores of the various systems", "labels": [], "entities": []}]}