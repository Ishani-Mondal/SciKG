{"title": [{"text": "Identifying Word Translations from Comparable Corpora Using Latent Topic Models", "labels": [], "entities": [{"text": "Identifying Word Translations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9193337559700012}]}], "abstractContent": [{"text": "A topic model outputs a set of multinomial distributions over words for each topic.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources.", "labels": [], "entities": []}, {"text": "Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space.", "labels": [], "entities": []}, {"text": "The best results, obtained by combining knowledge from word-topic distributions with similarity measures in the original space, are also reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generative models for documents such as Latent Dirichlet Allocation (LDA) ( are based upon the idea that latent variables exist which determine how words in documents might be generated.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.524260992805163}]}, {"text": "Fitting a generative model means finding the best set of those latent variables in order to explain the observed data.", "labels": [], "entities": []}, {"text": "Within that setting, documents are observed as mixtures of latent topics, where topics are probability distributions over words.", "labels": [], "entities": []}, {"text": "Our goal is to model and test the capability of probabilistic topic models to identify potential translations from document-aligned text collections.", "labels": [], "entities": []}, {"text": "A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the same topic, but strongly varying in style, length and even vocabulary, while still sharing a certain amount of main concepts (or topics).", "labels": [], "entities": []}, {"text": "We try to establish a connection between such latent topics and an idea known as the distributional hypothesis -words with a similar meaning are often used in similar contexts.", "labels": [], "entities": []}, {"text": "Besides the obvious context of direct cooccurrence, we believe that topic models are an additional source of knowledge which might be used to improve results in the quest for translation candidates extracted without the availability of a translation dictionary and linguistic knowledge.", "labels": [], "entities": []}, {"text": "We designed several methods, all derived from the core idea of using word distributions over topics as an extra source of contextual knowledge.", "labels": [], "entities": []}, {"text": "Two words are potential translation candidates if they are often present in the same cross-lingual topics and not observed in other cross-lingual topics.", "labels": [], "entities": []}, {"text": "In other words, a word w 2 from a target language is a potential translation candidate fora word w 1 from a source language, if the distribution of w 2 over the target language topics is similar to the distribution of w 1 over the source language topics.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes related work, focusing on previous attempts to use topic models to recognize potential translations.", "labels": [], "entities": []}, {"text": "Section 3 provides a short summary of the BiLDA model used in the experiments, presents all main ideas behind our work and gives an overview and a theoretical background of the methods.", "labels": [], "entities": [{"text": "BiLDA", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.7274410724639893}]}, {"text": "Section 4 evaluates and discusses initial results.", "labels": [], "entities": []}, {"text": "Finally, section 5 proposes several extensions and gives a summary of the current work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision@1 scores for the test subset of the IT- EN Wikipedia corpus (baseline precision score: 0.5031)", "labels": [], "entities": [{"text": "Precision@1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9549323320388794}, {"text": "IT- EN Wikipedia corpus", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.9104080319404602}, {"text": "precision score", "start_pos": 90, "end_pos": 105, "type": "METRIC", "confidence": 0.911419004201889}]}, {"text": " Table 2: MRR scores for the test subset of the IT-EN  Wikipedia corpus (baseline MRR score: 0.5890)", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6227666735649109}, {"text": "IT-EN  Wikipedia corpus", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.9501974781354269}]}]}