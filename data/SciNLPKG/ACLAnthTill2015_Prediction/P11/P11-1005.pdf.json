{"title": [{"text": "Evaluating the Impact of Coder Errors on Active Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification.", "labels": [], "entities": [{"text": "Active Learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7076196134090423}, {"text": "supervised classification", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.6851684898138046}]}, {"text": "While various simulation studies fora number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets.", "labels": [], "entities": []}, {"text": "This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process.", "labels": [], "entities": [{"text": "AL", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9338472485542297}]}, {"text": "We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when applied to noisy data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised machine learning techniques are still the mainstay for many NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 71, "end_pos": 80, "type": "TASK", "confidence": 0.8785603940486908}]}, {"text": "There is, however, a well-known bottleneck for these approaches: the amount of high-quality data needed for training, mostly obtained by human annotation.", "labels": [], "entities": []}, {"text": "Active Learning (AL) has been proposed as a promising approach to reduce the amount of time and cost for human annotation.", "labels": [], "entities": [{"text": "Active Learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.669518631696701}]}, {"text": "The idea behind active learning is quite intuitive: instead of annotating a large number of randomly picked instances we carefully select a small number of instances that are maximally informative for the machine learning classifier.", "labels": [], "entities": []}, {"text": "Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9993027448654175}]}, {"text": "Active learning has been applied to several NLP tasks like part-of-speech tagging (, chunking), syntactic parsing (), Named Entity Recognition (, Word Sense Disambiguation (;), text classification () or statistical machine translation, and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7290820479393005}, {"text": "syntactic parsing", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7376056015491486}, {"text": "Named Entity Recognition", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.6178826987743378}, {"text": "Word Sense Disambiguation", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.5468220810095469}, {"text": "text classification", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.7933456897735596}, {"text": "statistical machine translation", "start_pos": 203, "end_pos": 234, "type": "TASK", "confidence": 0.6975308656692505}]}, {"text": "Most of these studies, however, have only simulated the active learning process using goldstandard data.", "labels": [], "entities": []}, {"text": "This setting is crucially different from areal world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators.", "labels": [], "entities": []}, {"text": "While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense Disambiguation (WSD) with fine-grained sense distinctions, AL can actually harm the learning process.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 169, "end_pos": 200, "type": "TASK", "confidence": 0.7543954253196716}]}, {"text": "Dang suggests that the lack of a positive effect of AL might be due to inconsistencies in the human annotations and that AL cannot efficiently be applied to tasks which need double blind annotation with adjudication to insure a sufficient data quality.", "labels": [], "entities": []}, {"text": "Even if we take a more optimistic view and assume that AL might still be useful even for tasks featuring a high degree of ambiguity, it remains crucial to address the problem of annotation noise and its impact on AL.", "labels": [], "entities": []}, {"text": "43 In this paper we present a thorough evaluation of the impact of annotation noise on AL.", "labels": [], "entities": [{"text": "AL", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.8648823499679565}]}, {"text": "We simulate different types of coder errors and assess the effect on the learning process.", "labels": [], "entities": []}, {"text": "We propose a method to detect inconsistencies and remove them from the training data, and show that our method does alleviate the problem of annotation noise in our experiments.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reports on recent research on the impact of annotation noise in the context of supervised classification.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.693551778793335}]}, {"text": "Section 3 describes the experimental setup of our simulation study and presents results.", "labels": [], "entities": []}, {"text": "In Section 4 we present our filtering approach and show its impact on AL performance.", "labels": [], "entities": []}, {"text": "Section 5 concludes and outlines future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To make sure that the data we use in our simulation is as close to real-world data as possible, we do not create an artificial data set as done in () but use real data from a WSD task for the German verb drohen (threaten).", "labels": [], "entities": []}, {"text": "1 Drohen has three different word senses which can be disambiguated by humans with a high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9952811598777771}]}, {"text": "This point is crucial to our setup.", "labels": [], "entities": []}, {"text": "To control the amount of noise in the data, we need to be sure that the initial data set is noise-free.", "labels": [], "entities": []}, {"text": "For classification we use a maximum entropy classifier.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9651651978492737}]}, {"text": "Our sampling method is uncertainty sampling (, a standard sampling heuristic for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label.", "labels": [], "entities": []}, {"text": "As a measure of uncertainty we use Shannon entropy (1) () and the margin metric (2) (.", "labels": [], "entities": []}, {"text": "The first measure considers the model's predictions q for each class c and selects those instances from the pool where the Shannon entropy is highest.", "labels": [], "entities": []}, {"text": "The second measure looks at the difference between the largest two values in the prediciton vector q, namely the two predicted classes c, c \u2032 which are, according to our model, the most likely ones for instance x n , and selects those instances where the difference (margin) between the two predicted probabilities is the smallest.", "labels": [], "entities": []}, {"text": "We discuss some details of this metric in Section 4.", "labels": [], "entities": []}, {"text": "The features we use for WSD area combination of context features (word token with window size 11 and POS context with window size 7), syntactic features based on the output of a dependency parser and semantic features based on GermaNet hyperonyms.", "labels": [], "entities": [{"text": "WSD area", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.8223942816257477}]}, {"text": "These settings were tuned to the target verb by).", "labels": [], "entities": []}, {"text": "All results reported below are averages over a 5-fold cross validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of word senses in pool and test sets", "labels": [], "entities": [{"text": "Distribution of word senses", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8138332515954971}]}, {"text": " Table 2: Accuracy for the different sampling methods without and with filtering after adding 500 instances to the seed  data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998960018157959}]}, {"text": " Table 3: Error analysis of the instances rejected by the  filtering approach", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.919842541217804}]}]}