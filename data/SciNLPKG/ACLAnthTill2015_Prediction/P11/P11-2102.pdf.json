{"title": [{"text": "Identifying Sarcasm in Twitter: A Closer Look", "labels": [], "entities": [{"text": "Identifying Sarcasm", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8598880767822266}]}], "abstractContent": [{"text": "Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite.", "labels": [], "entities": []}, {"text": "We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author.", "labels": [], "entities": [{"text": "sarcasm", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9727219343185425}]}, {"text": "We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm.", "labels": [], "entities": []}, {"text": "We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task.", "labels": [], "entities": [{"text": "identifying sarcastic utterances", "start_pos": 97, "end_pos": 129, "type": "TASK", "confidence": 0.7977821628252665}]}, {"text": "Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic detection of sarcasm is still in its infancy.", "labels": [], "entities": [{"text": "Automatic detection of sarcasm", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8630906194448471}]}, {"text": "One reason for the lack of computational models has been the absence of accurately-labeled naturally occurring utterances that can be used to train machine learning systems.", "labels": [], "entities": []}, {"text": "Microblogging platforms such as Twitter, which allow users to communicate feelings, opinions and ideas in short messages and to assign labels to their own messages, have been recently exploited in sentiment and opinion analysis.", "labels": [], "entities": [{"text": "sentiment and opinion analysis", "start_pos": 197, "end_pos": 227, "type": "TASK", "confidence": 0.8413238227367401}]}, {"text": "In Twitter, messages can be annotated with hashtags such as #bicycling, #happy and #sarcasm.", "labels": [], "entities": []}, {"text": "We use these hashtags to build a labeled corpus of naturally occurring sarcastic, positive and negative tweets.", "labels": [], "entities": []}, {"text": "In this paper, we report on an empirical study on the use of lexical and pragmatic factors to distinguish sarcasm from positive and negative sentiments expressed in Twitter messages.", "labels": [], "entities": []}, {"text": "The contributions of this paper include i) creation of a corpus that includes only sarcastic utterances that have been explicitly identified as such by the composer of the message; ii) a report on the difficulty of distinguishing sarcastic tweets from tweets that are straight-forwardly positive or negative.", "labels": [], "entities": []}, {"text": "Our results suggest that lexical features alone are not sufficient for identifying sarcasm and that pragmatic and contextual features merit further study.", "labels": [], "entities": [{"text": "identifying sarcasm", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.9061836302280426}]}], "datasetContent": [{"text": "In this section we investigate the usefulness of lexical and pragmatic features in machine learning to classify sarcastic, positive and negative Tweets.", "labels": [], "entities": [{"text": "classify sarcastic, positive and negative Tweets", "start_pos": 103, "end_pos": 151, "type": "TASK", "confidence": 0.7101068496704102}]}, {"text": "We used two standard classifiers often employed in sentiment classification: support vector machine with sequential minimal optimization (SMO) and logistic regression (LogR).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9524405598640442}]}, {"text": "For features we used: 1) unigrams; 2) presence of dictionary-based lexical and pragmatic factors (LIWC + _P); and 3) frequency of dictionary-based lexical and pragmatic factors (LIWC + _F).", "labels": [], "entities": [{"text": "LIWC + _P)", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.654822313785553}, {"text": "frequency", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9776003956794739}, {"text": "LIWC + _F)", "start_pos": 178, "end_pos": 188, "type": "METRIC", "confidence": 0.5632624506950379}]}, {"text": "We also trained our models with bigrams and trigrams; however, results using these features did not report better results than unigrams and LICW + . The classifiers were trained on balanced datasets (900 instances per class) and tested through five-fold cross-validation.", "labels": [], "entities": []}, {"text": "In, shaded cells indicate the best accuracies for each class, while bolded values indicate the best accuracies per row.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.977272629737854}]}, {"text": "In the three-way classification (S-P-N), SMO with unigrams as features outperformed SMO with LIWC + _P and LIWC + _F as features.", "labels": [], "entities": [{"text": "SMO", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9449359178543091}]}, {"text": "The best accuracy of 57% is an indication of the difficulty of the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9996078610420227}]}, {"text": "We also performed several two-way classification experiments.", "labels": [], "entities": []}, {"text": "For the S-NS classification the best results were again obtained using SMO with  unigrams as features (65.44%).", "labels": [], "entities": [{"text": "S-NS classification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8500042855739594}, {"text": "SMO", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.6292969584465027}]}, {"text": "For S-P and S-N the best accuracies were close to 70%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9968057870864868}]}, {"text": "Overall, our best result (75.89%) was achieved in the polaritybased classification P-N.", "labels": [], "entities": []}, {"text": "It is intriguing that the machine learning systems have roughly equal difficulty in separating sarcastic tweets from positive tweets and from negative tweets.", "labels": [], "entities": []}, {"text": "These results indicate that the lexical and pragmatic features considered in this paper do not provide sufficient information to accurately differentiate sarcastic from positive and negative tweets.", "labels": [], "entities": []}, {"text": "This maybe due to the inherent difficulty of distinguishing short utterances in isolation, without use of contextual evidence.", "labels": [], "entities": []}, {"text": "In the next section we explore the inherent difficulty of identifying sarcastic utterances by comparing human performance and classifier performance.", "labels": [], "entities": [{"text": "identifying sarcastic utterances", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.8146486878395081}]}], "tableCaptions": [{"text": " Table 2: Classifiers accuracies using 5-fold cross- validation, in percent.", "labels": [], "entities": []}, {"text": " Table 3: Classifiers accuracies against humans' accuracies in three classification tasks.", "labels": [], "entities": []}]}