{"title": [{"text": "Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction", "labels": [], "entities": [{"text": "Document Selection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8772131502628326}, {"text": "Event Extraction", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7540787160396576}]}], "abstractContent": [{"text": "Annotating training data for event extraction is tedious and labor-intensive.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.8719651997089386}]}, {"text": "Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7262773662805557}]}, {"text": "In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping.", "labels": [], "entities": []}, {"text": "Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection.", "labels": [], "entities": []}, {"text": "We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection.", "labels": [], "entities": []}, {"text": "We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference.", "labels": [], "entities": [{"text": "trigger labeling", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7436120510101318}, {"text": "role labeling", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7766315340995789}, {"text": "role labeling", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.7441538870334625}]}], "introductionContent": [{"text": "The goal of event extraction is to identify instances of a class of events in text.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7410983443260193}]}, {"text": "In addition to identifying the event itself, it also identifies all of the participants and attributes of each event; these are the entities that are involved in that event.", "labels": [], "entities": []}, {"text": "The same event might be presented in various expressions, and an expression might represent different events in different contexts.", "labels": [], "entities": []}, {"text": "Moreover, for each event type, the event participants and attributes may also appear in multiple forms and exemplars of the different forms maybe required.", "labels": [], "entities": []}, {"text": "Thus, event extraction is a difficult task and requires substantial training data.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.9114901423454285}]}, {"text": "However, annotating events for training is a tedious task.", "labels": [], "entities": []}, {"text": "Annotators need to read the whole sentence, possibly several sentences, to decide whether there is a specific event or not, and then need to identify the event participants (like Agent and Patient), and attributes (like place and time) to complete an event annotation.", "labels": [], "entities": []}, {"text": "As a result, for event extraction tasks like MUC4, MUC6 and ACE2005, from one to several hundred annotated documents were needed.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7150162309408188}, {"text": "MUC4", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.8529145121574402}, {"text": "MUC6", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8525058627128601}, {"text": "ACE2005", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8079352974891663}]}, {"text": "In this paper, we apply a novel self-training process on an existing state-of-the-art baseline system.", "labels": [], "entities": []}, {"text": "Although traditional self-training on normal newswire does notwork well for this specific task, we managed to use information retrieval (IR) to select a better corpus for bootstrapping.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.7775074601173401}]}, {"text": "Also, taking advantage of properties of this corpus, cross-document inference is applied to obtain more \"informative\" probabilities.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to apply information retrieval and global inference to semi-supervised learning for event extraction.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.751403421163559}, {"text": "event extraction", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.7774254381656647}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3. Performance (F score) with different  self-training strategies after 10 iterations", "labels": [], "entities": [{"text": "F score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9106774032115936}]}]}