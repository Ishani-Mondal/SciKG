{"title": [], "abstractContent": [{"text": "An attractive property of attribute-value grammars is their reversibility.", "labels": [], "entities": []}, {"text": "Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.9038979411125183}]}, {"text": "We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.9567998945713043}]}], "introductionContent": [{"text": "Reversible grammars were introduced as early as 1975 by Martin.", "labels": [], "entities": []}, {"text": "In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature.", "labels": [], "entities": [{"text": "attribute-value grammars (AVG)", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6890600442886352}]}, {"text": "Later, AVG were enriched with a statistical component: stochastic AVG (SAVG).", "labels": [], "entities": []}, {"text": "Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences).", "labels": [], "entities": []}, {"text": "Various parsers based on this approach now exist for various languages.", "labels": [], "entities": []}, {"text": "SAVG can be applied for generation to select the most fluent realization from the set of possible realizations ().", "labels": [], "entities": []}, {"text": "In this case, the stochastic model is conditioned on the input logical forms.", "labels": [], "entities": []}, {"text": "Such generators exist for various languages as well.", "labels": [], "entities": []}, {"text": "If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9810583591461182}]}, {"text": "To some extent this is reasonable, because some features are only relevant in a certain direction.", "labels": [], "entities": []}, {"text": "For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 121, "end_pos": 128, "type": "TASK", "confidence": 0.962273895740509}]}, {"text": "Similarly, features which describe aspects of the logical form are important for parsing, but irrelevant for generation.", "labels": [], "entities": []}, {"text": "Yet, there are also many features that are relevant in both directions.", "labels": [], "entities": []}, {"text": "For instance, for Dutch, a very effective feature signals a direct object NP in fronted position in main clauses.", "labels": [], "entities": []}, {"text": "If a main clause is parsed which starts with a NP, the disambiguation component will favor a subject reading of that NP.", "labels": [], "entities": []}, {"text": "In generation, the fluency component will favor subject fronting over object fronting.", "labels": [], "entities": [{"text": "object fronting", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7089330554008484}]}, {"text": "Clearly, such shared preferences are not accidental.", "labels": [], "entities": []}, {"text": "In this paper we propose reversible SAVG in which a single stochastic component is applied both in parsing and generation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9686379432678223}]}, {"text": "We provide experimental evidence that such reversible SAVG achieve similar performance as their directional counterparts.", "labels": [], "entities": []}, {"text": "A single, reversible model is to be preferred over two distinct models because it explains why preferences in a disambiguation component and a fluency component, such as the preference for subject fronting over object fronting, are shared.", "labels": [], "entities": []}, {"text": "A single, reversible model is furthermore of practical interest for its simplicity, compactness, and maintainability.", "labels": [], "entities": []}, {"text": "As an important additional advantage, reversible models are applicable for tasks which combine aspects of parsing and generation, such as word-graph parsing and paraphrasing.", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7614221970240275}, {"text": "word-graph parsing", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.6896146982908249}]}, {"text": "In situations where only a small amount of training data is available for parsing or generation, cross-pollination improves the perfor-mance of a model.", "labels": [], "entities": [{"text": "parsing or generation", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7638206680615743}]}, {"text": "If preferences are shared between parsing and generation, it follows that a generator could benefit from parsing data and vice versa.", "labels": [], "entities": []}, {"text": "We present experimental results indicating that in such a bootstrap scenario a reversible model achieves better performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate reversible SAVG, we conduct experiments in the context of the Alpino system for Dutch.", "labels": [], "entities": [{"text": "SAVG", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.8890368938446045}]}, {"text": "Alpino provides a wide-coverage grammar, lexicon and parser).", "labels": [], "entities": [{"text": "Alpino", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9031288027763367}]}, {"text": "Recently, a sentence realizer has been added that uses the same grammar and lexicon).", "labels": [], "entities": [{"text": "sentence realizer", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7281491607427597}]}, {"text": "In the experiments, the cdbl part of the Alpino Treebank (van der) is used as training data (7,154 sentences).", "labels": [], "entities": [{"text": "cdbl part of the Alpino Treebank (van der)", "start_pos": 24, "end_pos": 66, "type": "DATASET", "confidence": 0.7367382764816284}]}, {"text": "The WR-P-P-H part (2,267 sentences) of the LASSY corpus , which consists of text from the Trouw 2001 newspaper, is used for testing.", "labels": [], "entities": [{"text": "LASSY corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.7327352613210678}, {"text": "Trouw 2001 newspaper", "start_pos": 90, "end_pos": 110, "type": "DATASET", "confidence": 0.9433740973472595}]}], "tableCaptions": [{"text": " Table 1: Size of the training data for each model", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9790984392166138}]}, {"text": " Table 2: Concept Accuracy scores and f-scores in terms  of named dependency relations for the parsing-specific  model versus the reversible model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8966551423072815}]}, {"text": " Table 3: General Text Matcher scores for fluency ranking  using various models.", "labels": [], "entities": [{"text": "General Text Matcher", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6483681599299113}, {"text": "fluency ranking", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7116489112377167}]}]}