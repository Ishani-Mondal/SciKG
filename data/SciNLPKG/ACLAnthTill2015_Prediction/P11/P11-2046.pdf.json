{"title": [{"text": "Relation Guided Bootstrapping of Semantic Lexicons", "labels": [], "entities": [{"text": "Relation", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9365536570549011}]}], "abstractContent": [{"text": "State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift.", "labels": [], "entities": []}, {"text": "Unfortunately, their use introduces a substantial amount of supervised knowledge.", "labels": [], "entities": []}, {"text": "We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift.", "labels": [], "entities": []}, {"text": "This removes the necessity for manually craft-ing category and relationship constraints, and manually generating negative categories.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework ().", "labels": [], "entities": []}, {"text": "These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (.", "labels": [], "entities": []}, {"text": "Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns ( , which can be reduced by semantically constraining the candidates.", "labels": [], "entities": []}, {"text": "Multicategory bootstrappers, such as NOMEN) and WMEB), reduce semantic drift by extracting multiple categories simultaneously in competition.", "labels": [], "entities": [{"text": "NOMEN", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8479104042053223}, {"text": "WMEB", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.7258290648460388}, {"text": "semantic drift", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7784920334815979}]}, {"text": "The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories ().", "labels": [], "entities": []}, {"text": "For example, female names are often bootstrapped with the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) ( . Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsupervised framework.", "labels": [], "entities": []}, {"text": "made some progress towards automatically learning useful negative categories during bootstrapping.", "labels": [], "entities": []}, {"text": "In this work we identify an unsupervised source of semantic constraints inspired by the Coupled Pattern Learner (CPL,).", "labels": [], "entities": []}, {"text": "In CPL, relation bootstrapping is coupled with lexicon bootstrapping in order to control semantic drift in the target relation's arguments.", "labels": [], "entities": []}, {"text": "Semantic constraints on categories and relations are manually crafted in CPL.", "labels": [], "entities": []}, {"text": "For example, a candidate of the relation IS-CEOOF will only be extracted if its arguments can be extracted into the ceo and company lexicons and a ceo is constrained to not be a celebrity or politician.", "labels": [], "entities": []}, {"text": "Negative examples such as IS-CEOOF(Sergey Brin, Google) are also introduced to clarify boundary conditions.", "labels": [], "entities": []}, {"text": "CPL employs a large number of these manually-crafted constraints to improve precision at the expense of recall (only 18 IS-CEOOF instances were extracted).", "labels": [], "entities": [{"text": "CPL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8924893140792847}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9989821314811707}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9993708729743958}]}, {"text": "In our approach, we exploit open relation bootstrapping to minimise semantic drift, without any manual seeding of relations or pre-defined category lexicon combinations.", "labels": [], "entities": []}, {"text": "Orthogonal to these seeded and constraint-based methods is the relation-independent Open Information Extraction (OPENIE) paradigm.", "labels": [], "entities": [{"text": "relation-independent Open Information Extraction (OPENIE)", "start_pos": 63, "end_pos": 120, "type": "TASK", "confidence": 0.7172898607594627}]}, {"text": "OPENIE systems, such as TEXTRUNNER (, define neither lexicon categories nor predefined relationships.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.6286857724189758}]}, {"text": "They extract relation tuples by exploiting broad syntactic patterns that are likely to indicate relations.", "labels": [], "entities": []}, {"text": "This enables the extraction of interesting and unanticipated relations from text.", "labels": [], "entities": []}, {"text": "However these patterns are often too broad, resulting in the extraction of tuples that do not represent relations at all.", "labels": [], "entities": []}, {"text": "As a result, heavy (supervised) postprocessing or use of supervised information is necessary.", "labels": [], "entities": []}, {"text": "For example, improve TEXTRUNNER precision by using deep parsing information via semantic role labelling.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.659773588180542}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.8510249257087708}]}], "datasetContent": [{"text": "To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of.", "labels": [], "entities": []}, {"text": "Note however the method is equally applicable to any corpus and set of semantic categories.", "labels": [], "entities": []}, {"text": "The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009).", "labels": [], "entities": []}, {"text": "The text was tokenised and POS-tagged using bio-specific NLP tools (), and parsed using the biomedical C&C CCG parser (.", "labels": [], "entities": [{"text": "biomedical C&C CCG parser", "start_pos": 92, "end_pos": 117, "type": "DATASET", "confidence": 0.5449825376272202}]}, {"text": "The term extraction data is formed from the raw 5-grams (t 1 , t 2 , t 3 , t 4 , t 5 ), where the set of candidate terms correspond to the middle tokens (t 3 ) and the patterns are formed from the surrounding tokens (t 1 , t 2 , t 4 , t 5 ).", "labels": [], "entities": [{"text": "term extraction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6713314205408096}]}, {"text": "The relation extraction data is also formed from the 5-grams.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8190416097640991}]}, {"text": "The candidate tuples correspond to the tokens (t 1 , t 5 ) and the patterns are formed from the intervening tokens (t 2 , t 3 , t 4 ).", "labels": [], "entities": []}, {"text": "The second relation dataset (5gm + 4gm), also includes length 2 patterns formed from 4-grams.", "labels": [], "entities": []}, {"text": "The final relation dataset (5gm + DC) includes dependency chains up to length 5 as the patterns between terms ().", "labels": [], "entities": []}, {"text": "These chains are formed using the Stanford dependencies generated by the parser.", "labels": [], "entities": []}, {"text": "All candidates occurring less than 10 times were filtered.", "labels": [], "entities": []}, {"text": "The sizes of the resulting datasets are shown in  We follow in using the 10 biomedical semantic categories and their hand-picked seeds in, and manually crafted negative categories: amino acid, animal, body part and organism.", "labels": [], "entities": []}, {"text": "Our evaluation process involved manually judging each extracted term and we calculate the average precision of the top-1000 terms over the 10 target categories.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9724175930023193}]}, {"text": "We do not calculate recall, due to the open-ended nature of the categories.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9985805749893188}]}, {"text": "compares the performance of WMEB and RGB, with and without the negative categories.", "labels": [], "entities": [{"text": "WMEB", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.9021192789077759}, {"text": "RGB", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8358919024467468}]}, {"text": "For RGB, we compare intra-, inter-and mixed relation types, and use the 5gm format of tuples and relation patterns.", "labels": [], "entities": []}, {"text": "In WMEB, drift dominates in the later iterations with \u223c19% precision drop between the first and last 500 terms.", "labels": [], "entities": [{"text": "WMEB", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.4768270254135132}, {"text": "drift", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9963474869728088}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9989639520645142}]}, {"text": "The manually-crafted negative categories give a substantial boost in precision on both the first and last 500 terms (+11.5% overall).", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9995110034942627}]}], "tableCaptions": [{"text": " Table 1: Statistics of three filtered MEDLINE datasets", "labels": [], "entities": [{"text": "MEDLINE datasets", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.7704577445983887}]}, {"text": " Table 3: Performance comparison of WMEB and RGB", "labels": [], "entities": [{"text": "WMEB", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7350754141807556}, {"text": "RGB", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.6456592679023743}]}, {"text": " Table 4: Comparison of different relation pattern types", "labels": [], "entities": []}]}