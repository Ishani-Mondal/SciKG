{"title": [{"text": "A Bayesian Model for Unsupervised Semantic Parsing", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6710791885852814}]}], "abstractContent": [{"text": "We propose a non-parametric Bayesian model for unsupervised semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.6980099827051163}]}, {"text": "Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7472025156021118}]}, {"text": "We use hierarchical Pitman-Yor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations.", "labels": [], "entities": []}, {"text": "We develop a modification of the Metropolis-Hastings split-merge sampler, resulting in an efficient inference algorithm for the model.", "labels": [], "entities": []}, {"text": "The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7772954106330872}]}], "introductionContent": [{"text": "Statistical approaches to semantic parsing have recently received considerable attention.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9122092425823212}]}, {"text": "While some methods focus on predicting a complete formal representation of meaning (, others consider more shallow forms of representation.", "labels": [], "entities": [{"text": "predicting a complete formal representation of meaning", "start_pos": 28, "end_pos": 82, "type": "TASK", "confidence": 0.79085887329919}]}, {"text": "However, most of this research has concentrated on supervised methods requiring large amounts of labeled data.", "labels": [], "entities": []}, {"text": "Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage, motivating the need for unsupervised or semi-supervised techniques.", "labels": [], "entities": []}, {"text": "Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.9510058462619781}]}, {"text": "These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7672081291675568}]}, {"text": "The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7398523390293121}]}, {"text": "While a similar direction has been previously explored in, the recent work of) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.", "labels": [], "entities": [{"text": "predicting predicate-argument structure of a sentence", "start_pos": 117, "end_pos": 170, "type": "TASK", "confidence": 0.8285389343897501}]}, {"text": "For example, fora pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions \"Steelers\" and \"the Pittsburgh team\" to the same semantic class Steelers, and group expressions \"defeated\" and \"secured the victory over\".", "labels": [], "entities": []}, {"text": "Such semantic representation can be useful for entailment or question answering tasks, as an entailment model can abstract away from specifics of syntactic and lexical realization relying instead on the induced semantic representation.", "labels": [], "entities": [{"text": "entailment or question answering tasks", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.7062816560268402}]}, {"text": "For example, the two sentences in have identical semantic representation, and therefore can be hypothesized to be equivalent.", "labels": [], "entities": []}, {"text": "From the statistical modeling point of view, joint learning of predicate-argument structure and discovery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (.", "labels": [], "entities": []}, {"text": "In this respect our model is similar to recent LDA-based models of selectional preference (, and can even be regarded as their recursive and non-parametric extension.", "labels": [], "entities": []}, {"text": "In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7654661238193512}]}, {"text": "Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of which have to perform model selection and use heuristics to penalize more complex models of semantics.", "labels": [], "entities": []}, {"text": "Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets.", "labels": [], "entities": []}, {"text": "More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009) and our non-parametric method is presented in Section 3.", "labels": [], "entities": []}, {"text": "Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7345679700374603}]}, {"text": "However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (, or the number of adapted productions in the adaptor grammar) was not very large.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.8795608878135681}]}, {"text": "In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 253, "end_pos": 265, "type": "DATASET", "confidence": 0.9468910098075867}]}, {"text": "This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move.", "labels": [], "entities": []}, {"text": "Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes but has not previously been used in hierarchical setting.", "labels": [], "entities": []}, {"text": "We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example, and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate.", "labels": [], "entities": [{"text": "argument role-syntax alignment", "start_pos": 171, "end_pos": 201, "type": "TASK", "confidence": 0.6388289431730906}]}, {"text": "Evaluating unsupervised models is a challenging task.", "labels": [], "entities": []}, {"text": "We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.7745485405127207}]}, {"text": "In both cases, we follow () in using the corpus of biomedical abstracts.", "labels": [], "entities": []}, {"text": "Our model achieves favorable results significantly outperforming the baselines, including state-of-theart methods for relation extraction, and achieves scores comparable to those of the MLN model.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8871090710163116}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 begins with a definition of the semantic parsing task.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7808152139186859}]}, {"text": "Sections 3 and 4 give background on the MLN model and the Pitman-Yor processes, respectively.", "labels": [], "entities": [{"text": "MLN model", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.7455596327781677}]}, {"text": "In Sections 5 and 6, we describe our model and the inference method.", "labels": [], "entities": []}, {"text": "Section 7 provides both qualitative and quantitative evaluation.", "labels": [], "entities": []}, {"text": "Finally, ad-ditional related work is presented in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We induced a semantic representation over a collection of texts and evaluated it by answering questions about the knowledge contained in the corpus.", "labels": [], "entities": []}, {"text": "We used the GENIA corpus (, a dataset of 1999 biomedical abstracts, and a set of questions produced by.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.9733554720878601}]}, {"text": "A example question is shown in.", "labels": [], "entities": []}, {"text": "All model hyperpriors were set to maximize the posterior, except for w (A) and w (C) , which were set to 1.e \u2212 10 and 1.e \u2212 35, respectively.", "labels": [], "entities": []}, {"text": "Inference was run for around 300,000 sampling iterations until the percentage of accepted split-merge moves became lower than 0.05%.", "labels": [], "entities": []}, {"text": "Let us examine some of the induced semantic classes   Argument types of the induced classes also show a tendency to correspond to semantic roles.", "labels": [], "entities": []}, {"text": "For example, an argument type of class 2 is modeled as a distribution over two argument parts, prep of and prep from.", "labels": [], "entities": []}, {"text": "The corresponding arguments define the origin of the cells (transgenic mouse, smoker, volunteer, donor, . .", "labels": [], "entities": []}, {"text": "). We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in.", "labels": [], "entities": [{"text": "USP-BAYES", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.5990576148033142}]}, {"text": "The first set of baselines looks for answers by attempting to match a verb and its argument in the question with the input text.", "labels": [], "entities": []}, {"text": "The first version (KW) simply returns the rest of the sentence on the other side of the verb, while the second (KW-SYN) uses syntactic information to extract the subject or the object of the verb.", "labels": [], "entities": []}, {"text": "Other baselines are based on state-of-the-art relation extraction systems.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7237265557050705}]}, {"text": "When the extracted relation and one of the arguments match those in a given).", "labels": [], "entities": []}, {"text": "The EX-ACT versions of the methods return answers when they match the question argument exactly, and the SUB versions produce answers containing the question argument as a substring.", "labels": [], "entities": []}, {"text": "Similarly to the MLN system (USP-MLN), we generate answers as follows.", "labels": [], "entities": [{"text": "USP-MLN", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.8805752992630005}]}, {"text": "We use our trained model to parse a question, i.e. recursively decompose it into lexical items and assign them to semantic classes induced at training.", "labels": [], "entities": []}, {"text": "Using this semantic representation, we look for the type of an argument missing in the question, which, if found, is reported as an answer.", "labels": [], "entities": []}, {"text": "It is clear that overly coarse clusters of argument fillers or clustering of semantically related but not equivalent relations can hurt precision for this evaluation method.", "labels": [], "entities": [{"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.999016523361206}]}, {"text": "Each system is evaluated by counting the answers it generates, and computing the accuracy of those answers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9992067217826843}]}, {"text": "First, both USP models significantly outperform all other baselines: even though the accuracy of KW-SYN and TR-EXACT are comparable with our accuracy, the number of correct answers returned by USPBayes is 4 and 11 times smaller than those of KW-SYN and TR-EXACT, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.999291181564331}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9984902143478394}, {"text": "USPBayes", "start_pos": 193, "end_pos": 201, "type": "DATASET", "confidence": 0.956826388835907}]}, {"text": "While we are not beating the MLN baseline, the difference is not significant.", "labels": [], "entities": [{"text": "MLN baseline", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.8339529931545258}]}, {"text": "The effective number of questions is relatively small (less than 80 different questions are answered by any of the models).", "labels": [], "entities": []}, {"text": "More than 50% of USP-BAYES mistakes were due to wrong interpretation of only 5 different questions.", "labels": [], "entities": []}, {"text": "From another point of view, most of the mistakes are explained Question: What does cyclosporin A suppress?", "labels": [], "entities": []}, {"text": "Answer: expression of EGR-2 Sentence: As with EGR-3 , expression of EGR-2 was blocked by cyclosporin A . Question: What inhibits tnf-alpha?", "labels": [], "entities": []}, {"text": "Answer: IL -10 Sentence: Our previous studies inhuman monocytes have demonstrated that interleukin ( IL ) -10 inhibits lipopolysaccharide ( LPS ) -stimulated production of inflammatory cytokines , IL-1 beta , IL-6 , IL-8 , and tumor necrosis factor ( TNF ) -alpha by blocking gene transcription . by overly coarse clustering corresponding to just 3 classes, namely, 30%, 25% and 20% of errors are due to the clusters 6, 8 and 12, respectively.", "labels": [], "entities": [{"text": "IL -10 Sentence", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.5956173539161682}]}, {"text": "Though all these clusters have clear semantic interpretation (white blood cells, predicates corresponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our experiments.", "labels": [], "entities": []}, {"text": "Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of the induced semantic classes.", "labels": [], "entities": []}, {"text": " Table 2: Performance on the QA task.", "labels": [], "entities": [{"text": "QA task", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.7394644618034363}]}]}