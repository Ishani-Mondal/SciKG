{"title": [{"text": "Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction", "labels": [], "entities": [{"text": "Combining Morpheme-based Machine Translation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.5977089032530785}]}], "abstractContent": [{"text": "This paper extends the training and tuning regime for phrase-based statistical machine translation to obtain fluent translations into morphologically complex languages (we build an English to Finnish translation system).", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.5676413998007774}]}, {"text": "Our methods use unsupervised morphology induction.", "labels": [], "entities": [{"text": "morphology induction", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7569486200809479}]}, {"text": "Unlike previous work we focus on morphologically productive phrase pairs-our decoder can combine morphemes across phrase boundaries.", "labels": [], "entities": []}, {"text": "Morphemes in the target language may not have a corresponding morpheme or word in the source language.", "labels": [], "entities": []}, {"text": "Therefore, we propose a novel combination of post-processing morphology prediction with morpheme-based translation.", "labels": [], "entities": [{"text": "post-processing morphology prediction", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.642738143603007}]}, {"text": "We show, using both automatic evaluation scores and linguistically motivated analyses of the output, that our methods out-perform previously proposed ones and provide the best known results on the English-Finnish Europarl translation task.", "labels": [], "entities": [{"text": "Europarl translation task", "start_pos": 213, "end_pos": 238, "type": "TASK", "confidence": 0.7160100142161051}]}, {"text": "Our methods are mostly language independent, so they should improve translation into other target languages with complex morphology.", "labels": [], "entities": []}, {"text": "1 Translation and Morphology Languages with rich morphological systems present significant hurdles for statistical machine translation (SMT), most notably data sparsity, source-target asymmetry, and problems with automatic evaluation.", "labels": [], "entities": [{"text": "Translation and Morphology", "start_pos": 2, "end_pos": 28, "type": "TASK", "confidence": 0.8383934299151102}, {"text": "statistical machine translation (SMT)", "start_pos": 103, "end_pos": 140, "type": "TASK", "confidence": 0.8133984208106995}]}, {"text": "In this work, we propose to address the problem of morphological complexity in an English-to-Finnish MT task within a phrase-based translation framework.", "labels": [], "entities": [{"text": "MT task", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.8963499069213867}, {"text": "phrase-based translation", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.662388026714325}]}, {"text": "We focus on unsupervised segmentation methods to derive the morphological information supplied to the MT model in order to provide coverage on very large data-sets and for languages with few hand-annotated resources.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9488598704338074}]}, {"text": "In fact, in our experiments, unsuper-vised morphology always outperforms the use of a hand-built morphological analyzer.", "labels": [], "entities": []}, {"text": "Rather than focusing on a few linguistically motivated aspects of Finnish morphological behaviour, we develop techniques for handling morphological complexity in general.", "labels": [], "entities": [{"text": "Finnish morphological behaviour", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.678567369778951}]}, {"text": "We chose Finnish as our target language for this work, because it exemplifies many of the problems morphologically complex languages present for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9926022887229919}]}, {"text": "Among all the languages in the Europarl data-set, Finnish is the most difficult language to translate from and into, as was demonstrated in the MT Summit shared task (Koehn, 2005).", "labels": [], "entities": [{"text": "Europarl data-set", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9881405532360077}, {"text": "MT Summit shared task (Koehn, 2005)", "start_pos": 144, "end_pos": 179, "type": "TASK", "confidence": 0.7518742879231771}]}, {"text": "Another reason is the current lack of knowledge about how to apply SMT successfully to agglutinative languages like Turkish or Finnish.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9950326681137085}]}, {"text": "Our main contributions are: 1) the introduction of the notion of segmented translation where we explicitly allow phrase pairs that can end with a dangling morpheme, which can connect with other morphemes as part of the translation process, and 2) the use of a fully segmented translation model in combination with a post-processing morpheme prediction system, using unsupervised morphology induction.", "labels": [], "entities": [{"text": "segmented translation", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.7687161862850189}]}, {"text": "Both of these approaches beat the state of the art on the English-Finnish translation task.", "labels": [], "entities": [{"text": "English-Finnish translation task", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.70886958638827}]}, {"text": "Morphology can express both content and function categories, and our experiments show that it is important to use morphology both within the translation model (for morphology with content) and outside it (for morphology contributing to fluency).", "labels": [], "entities": []}, {"text": "Automatic evaluation measures for MT, BLEU (Papineni et al., 2002), WER (Word Error Rate) and PER (Position Independent Word Error Rate) use the word as the basic unit rather than morphemes.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9591522216796875}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.998171329498291}, {"text": "WER (Word Error Rate)", "start_pos": 68, "end_pos": 89, "type": "METRIC", "confidence": 0.8663388888041178}, {"text": "PER (Position Independent Word Error Rate", "start_pos": 94, "end_pos": 135, "type": "METRIC", "confidence": 0.8186121582984924}]}], "introductionContent": [], "datasetContent": [{"text": "For all of the models builtin this paper, we used the Europarl version 3 corpus () English-Finnish training data set, as well as the standard development and test data sets.", "labels": [], "entities": [{"text": "Europarl version 3 corpus () English-Finnish training data set", "start_pos": 54, "end_pos": 116, "type": "DATASET", "confidence": 0.8583594494395785}]}, {"text": "Our parallel training data consists of \u223c1 million sentences of 40 words or less, while the development and test sets were each 2,000 sentences long.", "labels": [], "entities": []}, {"text": "In all the experiments conducted in this paper, we used the Moses 5 phrase-based translation system ( ), 2008 version.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6372673809528351}]}, {"text": "We trained all of the Moses systems herein using the standard features: language model, reordering model, translation model, and word penalty; in addition to these, the factored experiments called for additional translation and generation features for the added factors as noted above.", "labels": [], "entities": [{"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9459242820739746}]}, {"text": "We used in all experiments the following settings: a hypothesis stack size 100, distortion limit 6, phrase translations limit 20, and maximum phrase length 20.", "labels": [], "entities": []}, {"text": "For the language models, we used SRILM 5-gram language models) for all factors.", "labels": [], "entities": []}, {"text": "For our word-based Baseline system, we trained a word-based model using the same Moses system with identical settings.", "labels": [], "entities": []}, {"text": "For evaluation against segmented translation systems in segmented forms before word reconstruction, we also segmented the baseline system's word-based output.", "labels": [], "entities": [{"text": "word reconstruction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7616079747676849}]}, {"text": "All the BLEU scores reported are for lowercase evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9988790154457092}]}, {"text": "We did an initial evaluation of the segmented output translation for each system using the no-  tion of m-BLEU score ( where the BLEU score is computed by comparing the segmented output with a segmented reference translation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9756056666374207}]}, {"text": "shows the m-BLEU scores for various systems.", "labels": [], "entities": []}, {"text": "We also show the m-BLEU score without unigrams, since over-segmentation could lead to artificially high m-BLEU scores.", "labels": [], "entities": []}, {"text": "In fact, if we compare the relative improvement of our m-BLEU scores for the Unsup L-match system we see a relative improvement of 39.75% over the baseline.", "labels": [], "entities": []}, {"text": "report an m-BLEU score of 55.64% but obtain a relative improvement of 0.6% over their baseline m-BLEU score.", "labels": [], "entities": []}, {"text": "We find that when using a good segmentation model, segmentation of the morphologically complex target language improves model performance over an unsegmented baseline (the confidence scores come from bootstrap resampling).", "labels": [], "entities": []}, {"text": "shows the evaluation scores for all the baselines and the methods introduced in this paper using standard  better than (, the previous best score for this task.", "labels": [], "entities": []}, {"text": "We also show a better relative improvement over our baseline when compared to (): a relative improvement of 4.86% for Unsup L-match compared to our baseline word-based model, compared to their 1.65% improvement over their baseline word-based model.", "labels": [], "entities": []}, {"text": "Our best performing method used unsupervised morphology with L-match (see Section 2.2) and the improvement is significant: bootstrap resampling provides a confidence margin of \u00b10.77 and a t-test) showed significance with p = 0.001.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Morpheme occurences in the phrase table  and in translation.", "labels": [], "entities": []}, {"text": " Table 3: Test Scores: lowercase BLEU, WER and  TER. The  *  indicates a statistically significant im- provement of BLEU score over the Baseline model.  The boldface scores are the best performing scores  per evaluation measure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9966886639595032}, {"text": "WER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9914221167564392}, {"text": "TER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9901781678199768}, {"text": "BLEU score", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9464483857154846}]}, {"text": " Table 4: Model Accuracy: Morphological Constructions. Freq. refers to the construction's average number  of occurrences per sentence, also averaged over the various translations. P, R and F stand for precision,  recall and F-score. The constructions are listed in descending order of their frequency in the texts. The  highlighted value in each column is the most accurate with respect to the reference value.", "labels": [], "entities": [{"text": "Freq", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9776942729949951}, {"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9994300007820129}, {"text": "recall", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.9988186955451965}, {"text": "F-score", "start_pos": 224, "end_pos": 231, "type": "METRIC", "confidence": 0.9827547669410706}]}]}