{"title": [{"text": "Jointly Learning to Extract and Compress", "labels": [], "entities": []}], "abstractContent": [{"text": "We learn a joint model of sentence extraction and compression for multi-document summa-rization.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7437798082828522}]}, {"text": "Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used.", "labels": [], "entities": []}, {"text": "We train the model using a margin-based objective whose loss captures end summary quality.", "labels": [], "entities": []}, {"text": "Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently.", "labels": [], "entities": []}, {"text": "Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance.", "labels": [], "entities": []}, {"text": "Our jointly extracted and compressed summaries outper-form both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.6053865551948547}]}, {"text": "We achieve the highest published ROUGE results to date on the TAC 2008 data set.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9902387857437134}, {"text": "TAC 2008 data set", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.983313724398613}]}], "introductionContent": [{"text": "Applications of machine learning to automatic summarization have met with limited success, and, as a result, many top-performing systems remain largely ad-hoc.", "labels": [], "entities": [{"text": "summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.7286624908447266}]}, {"text": "One reason learning may have provided limited gains is that typical models do not learn to optimize end summary quality directly, but rather learn intermediate quantities in isolation.", "labels": [], "entities": []}, {"text": "For example, many models learn to score each input sentence independently, and then assemble extractive summaries from the top-ranked sentences in away not incorporated into the learning process.", "labels": [], "entities": []}, {"text": "This extraction is often done in the presence of a heuristic that limits redundancy.", "labels": [], "entities": []}, {"text": "As another example, learn predictors of individual words' appearance in the references, but in isolation from the sentence selection procedure.", "labels": [], "entities": []}, {"text": "Exceptions are who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy.", "labels": [], "entities": []}, {"text": "One main contribution of the current paper is the direct optimization of summary quality in a single model; we find that our learned systems substantially outperform unlearned counterparts on both automatic and manual metrics.", "labels": [], "entities": []}, {"text": "While pure extraction is certainly simple and does guarantee some minimal readability, showed that sentence compression) has the potential to improve the resulting summaries.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7591538429260254}]}, {"text": "However, attempts to incorporate compression into a summarization system have largely failed to realize large gains.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9672396183013916}]}, {"text": "For example,) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems.", "labels": [], "entities": []}, {"text": "Similarly,, though not learning weights, do a limited form of compression jointly with extraction.", "labels": [], "entities": []}, {"text": "They report a marginal increase in the automatic wordoverlap metric ROUGE), but a decline in manual Pyramid ().", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.4984227120876312}, {"text": "manual Pyramid", "start_pos": 93, "end_pos": 107, "type": "METRIC", "confidence": 0.8396597504615784}]}, {"text": "A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.8656965494155884}]}, {"text": "Both and build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers.", "labels": [], "entities": []}, {"text": "Daum\u00e9 III learns parameters for compression and extraction jointly using an approximate training procedure, but his results are not competitive with state-of-the-art extractive systems, and he does not report improvements on manual content or quality metrics.", "labels": [], "entities": []}, {"text": "In our approach, we define a linear model that scores candidate summaries according to features that factor over the n-gram types that appear in the summary and the structural compressions used to create the sentences in the summary.", "labels": [], "entities": []}, {"text": "We train these parameters jointly using a margin-based objective whose loss captures end summary quality through the ROUGE metric.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9838676452636719}]}, {"text": "Because of the exponentially large set of candidate summaries, we use a cutting plane algorithm to incrementally detect and add active constraints efficiently.", "labels": [], "entities": []}, {"text": "To make joint learning possible we introduce anew, manually-annotated data set of extracted, compressed sentences.", "labels": [], "entities": []}, {"text": "Inference in our model can be cast as an integer linear program (ILP) and solved in reasonable time using a generic ILP solver; we also introduce a fast approximation scheme which achieves similar performance.", "labels": [], "entities": []}, {"text": "Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality.", "labels": [], "entities": []}, {"text": "We achieve the highest published comparable results (ROUGE) to date on our test set.", "labels": [], "entities": [{"text": "comparable results (ROUGE)", "start_pos": 33, "end_pos": 59, "type": "METRIC", "confidence": 0.7579744338989258}]}], "datasetContent": [{"text": "We set aside the TAC 2008 data set (48 problems) for testing and use the TAC 2009 data set (44 problems) for training, with hyper-parameters set to maximize six-fold cross-validation bigram recall on the training set.", "labels": [], "entities": [{"text": "TAC 2008 data set", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.976501852273941}, {"text": "TAC 2009 data set", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.9796903878450394}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9127225279808044}]}, {"text": "We run the factored SMO algorithm until convergence, and run the cutting-plane algorithm until convergence for = 10 \u22124 . We used GLPK to solve all ILPs.", "labels": [], "entities": [{"text": "GLPK", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.7506421208381653}]}, {"text": "We solved extractive ILPs exactly, and joint extractive and compressive ILPs approximately using an intermediate extraction size of 1000.", "labels": [], "entities": []}, {"text": "Constituency parses were produced using the Berkeley parser.", "labels": [], "entities": [{"text": "Constituency parses", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6942621767520905}]}, {"text": "We show results for three systems, EXTRACTIVE BASE-LINE, LEARNED EXTRACTIVE, LEARNED COM-PRESSIVE, and the standard baseline that extracts the first 100 words in the the most recent document, LAST DOCUMENT.", "labels": [], "entities": [{"text": "EXTRACTIVE", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.6669557690620422}, {"text": "BASE-LINE", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.7341826558113098}, {"text": "LEARNED", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9479236006736755}, {"text": "LEARNED COM-PRESSIVE", "start_pos": 77, "end_pos": 97, "type": "METRIC", "confidence": 0.7674539387226105}, {"text": "LAST DOCUMENT", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.43371452391147614}]}], "tableCaptions": [{"text": " Table 3: Bigram Recall (BR), ROUGE (R-2 and R-SU4)  and Pyramid (Pyr) scores are multiplied by 100; Linguis- tic Quality (LQ) is scored on a 1 (very poor) to 10 (very  good) scale.", "labels": [], "entities": [{"text": "Bigram Recall (BR)", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8723097324371338}, {"text": "ROUGE", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9879665970802307}, {"text": "Pyramid (Pyr) scores", "start_pos": 57, "end_pos": 77, "type": "METRIC", "confidence": 0.8048669457435608}, {"text": "Linguis- tic Quality (LQ)", "start_pos": 101, "end_pos": 126, "type": "METRIC", "confidence": 0.6737706022603172}]}, {"text": " Table 4: Summary statistics for the summaries gener- ated by each system: Average number of sentences per  summary, average number of words per summary sen- tence, and average number of non-stopword word types  per summary.", "labels": [], "entities": []}]}