{"title": [{"text": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a novel approach for lever-aging automatically extracted textual knowledge to improve the performance of control applications such as games.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to enrich a stochastic player with high-level guidance expressed in text.", "labels": [], "entities": []}, {"text": "Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text.", "labels": [], "entities": []}, {"text": "Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback.", "labels": [], "entities": []}, {"text": "We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide.", "labels": [], "entities": []}, {"text": "Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built-in AI of Civilization II.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we study the task of grounding linguistic analysis in control applications such as computer games.", "labels": [], "entities": [{"text": "grounding linguistic analysis", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.6894454658031464}]}, {"text": "In these applications, an agent attempts to optimize a utility function (e.g., game score) by learning to select situation-appropriate actions.", "labels": [], "entities": []}, {"text": "In complex domains, finding a winning strategy is challenging even for humans.", "labels": [], "entities": []}, {"text": "Therefore, human players typically rely on manuals and guides that describe promising tactics and provide general advice about the underlying task.", "labels": [], "entities": []}, {"text": "Surprisingly, such textual information has never been utilized in control algorithms despite its potential to greatly improve performance.", "labels": [], "entities": []}, {"text": "The code, data and complete experimental setup for this work are available at http://groups.csail.mit.edu/rbg/code/civ.", "labels": [], "entities": []}, {"text": "The natural resources available where a population settles affects its ability to produce food and goods.", "labels": [], "entities": []}, {"text": "Build your city on a plains or grassland square with a river running through it if possible.", "labels": [], "entities": []}, {"text": "Consider for instance the text shown in.", "labels": [], "entities": []}, {"text": "This is an excerpt from the user manual of the game Civilization II.", "labels": [], "entities": []}, {"text": "This text describes game locations where the action \"build-city\" can be effectively applied.", "labels": [], "entities": []}, {"text": "A stochastic player that does not have access to this text would have to gain this knowledge the hard way: it would repeatedly attempt this action in a myriad of states, thereby learning the characterization of promising state-action pairs based on the observed game outcomes.", "labels": [], "entities": []}, {"text": "In games with large state spaces, long planning horizons, and high-branching factors, this approach can be prohibitively slow and ineffective.", "labels": [], "entities": []}, {"text": "An algorithm with access to the text, however, could learn correlations between words in the text and game attributes -e.g., the word \"river\" and places with rivers in the game -thus leveraging strategies described in text to better select actions.", "labels": [], "entities": []}, {"text": "The key technical challenge in leveraging textual knowledge is to automatically extract relevant information from text and incorporate it effectively into a control algorithm.", "labels": [], "entities": []}, {"text": "Approaching this task in a supervised framework, as is common in traditional information extraction, is inherently difficult.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7477948665618896}]}, {"text": "Since the game's state space is extremely large, and the states that will be encountered during game play cannot be known a priori, it is impractical to manually annotate the information that would be relevant to those states.", "labels": [], "entities": []}, {"text": "Instead, we propose to learn text analysis based on a feedback signal inherent to the control application, such as game score.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7427254319190979}]}, {"text": "Our general setup consists of a game in a stochastic environment, where the goal of the player is to maximize a given utility function R(s) at state s.", "labels": [], "entities": []}, {"text": "We follow a common formulation that has been the basis of several successful applications of machine learning to games.", "labels": [], "entities": []}, {"text": "The player's behavior is determined by an action-value function Q(s, a) that assesses the goodness of an action a in a given state s based on the features of sand a.", "labels": [], "entities": []}, {"text": "This function is learned based solely on the utility R(s) collected via simulated game-play in a Monte-Carlo framework.", "labels": [], "entities": []}, {"text": "An obvious way to enrich the model with textual information is to augment the action-value function with word features in addition to state and action features.", "labels": [], "entities": []}, {"text": "However, adding all the words in the document is unlikely to help since only a small fraction of the text is relevant fora given state.", "labels": [], "entities": []}, {"text": "Moreover, even when the relevant sentence is known, the mapping between raw text and the action-state representation may not be apparent.", "labels": [], "entities": []}, {"text": "This representation gap can be bridged by inducing a predicate structure on the sentence-e.g., by identifying words that describe actions, and those that describe state attributes.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for learning an action-value function augmented with linguistic features, while simultaneously modeling sentence relevance and predicate structure.", "labels": [], "entities": []}, {"text": "We employ a multilayer neural network where the hidden layers represent sentence relevance and predicate parsing decisions.", "labels": [], "entities": [{"text": "sentence relevance and predicate parsing", "start_pos": 72, "end_pos": 112, "type": "TASK", "confidence": 0.6602338910102844}]}, {"text": "Despite the added complexity, all the parameters of this non-linear model can be effectively learned via Monte-Carlo simulations.", "labels": [], "entities": []}, {"text": "We test our method on the strategy game Civilization II, a notoriously challenging game with an immense action space.", "labels": [], "entities": []}, {"text": "3 As a source of knowledge for guiding our model, we use the official game manual.", "labels": [], "entities": [{"text": "official game manual", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.6441153089205424}]}, {"text": "As a baseline, we employ a similar MonteCarlo search based player which does not have access to textual information.", "labels": [], "entities": []}, {"text": "We demonstrate that the linguistically-informed player significantly outperforms the baseline in terms of number of games won.", "labels": [], "entities": []}, {"text": "Moreover, we show that modeling the deeper linguistic structure of sentences further improves performance.", "labels": [], "entities": []}, {"text": "In full-length games, our algorithm yields a 27% improvement over a language unaware base- line, and wins over 78% of games against the builtin, hand-crafted AI of Civilization II.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets We use the official game manual for Civilization II as our strategy guide.", "labels": [], "entities": []}, {"text": "This manual uses a large vocabulary of 3638 words, and is composed of 2083 sentences, each on average 16.9 words long.", "labels": [], "entities": []}, {"text": "Experimental Framework To apply our method to the Civilization II game, we use the game's open source implementation Freeciv.", "labels": [], "entities": [{"text": "Freeciv", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9276980757713318}]}, {"text": "We instrument the game to allow our method to programmatically measure the current state of the game and to execute game actions.", "labels": [], "entities": []}, {"text": "The Stanford parser) was used to generate the dependency parse information for sentences in the game manual.", "labels": [], "entities": []}, {"text": "Across all experiments, we start the game at the same initial state and run it for 100 steps.", "labels": [], "entities": []}, {"text": "At each step, we perform 500 Monte-Carlo roll-outs.", "labels": [], "entities": []}, {"text": "Each roll-out is run for 20 simulated game steps before halting the simulation and evaluating the outcome.", "labels": [], "entities": []}, {"text": "For our method, and for each of the baselines, we run 200 independent games in the above manner, with evaluations averaged across the 200 runs.", "labels": [], "entities": []}, {"text": "We use the same experimental settings across all methods, and all model parameters are initialized to zero.", "labels": [], "entities": []}, {"text": "The test environment consisted of typical PCs with single Intel Core i7 CPUs (4 hyper-threaded cores each), with the algorithms executing 8 simulation roll-outs in parallel.", "labels": [], "entities": []}, {"text": "In this setup, a single game of 100 steps runs in approximately 1.5 hours.", "labels": [], "entities": []}, {"text": "Evaluation Metrics We wish to evaluate two aspects of our method: how well it leverages textual information to improve game play, and the accuracy of the linguistic analysis it produces.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9989921450614929}]}, {"text": "We evaluate the first aspect by comparing our method against various baselines in terms of the percentage of games won against the built-in AI of Freeciv.", "labels": [], "entities": []}, {"text": "This AI is a fixed algorithm designed using extensive knowledge of the game, with the intention of challenging human players.", "labels": [], "entities": []}, {"text": "As such, it provides a good open-reference baseline.", "labels": [], "entities": []}, {"text": "Since full games can last for multiple days, we compute the percentage of games won within the first 100 game steps as our primary evaluation.", "labels": [], "entities": []}, {"text": "To confirm that performance under this evaluation is meaningful, we also compute the percentage of full games won over 50 independent runs, where each game is run to completion.: Win rate of our method and two baselines on 50 full length games played against the built-in AI.", "labels": [], "entities": [{"text": "Win rate", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9737879633903503}]}], "tableCaptions": [{"text": " Table 1: Win rate of our method and several baselines  within the first 100 game steps, while playing against the  built-in game AI. Games that are neither won nor lost are  still ongoing. Our model's win rate is statistically signif- icant against all baselines except sentence relevance. All  results are averaged across 200 independent game runs.  The standard errors shown are for percentage wins.", "labels": [], "entities": [{"text": "Win rate", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9557391107082367}]}]}