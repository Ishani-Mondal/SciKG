{"title": [{"text": "Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition", "labels": [], "entities": [{"text": "Robust Cross-Domain Named Entity Recognition", "start_pos": 36, "end_pos": 80, "type": "TASK", "confidence": 0.6254437267780304}]}], "abstractContent": [{"text": "We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries.", "labels": [], "entities": [{"text": "cross-domain language processing", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.6352279881636301}, {"text": "adaptation of named entity recognition (NER) from news text", "start_pos": 108, "end_pos": 167, "type": "TASK", "confidence": 0.768981318582188}]}, {"text": "The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token.", "labels": [], "entities": []}, {"text": "We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.", "labels": [], "entities": [{"text": "NER", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9453654885292053}]}], "introductionContent": [{"text": "As statistical Natural Language Processing (NLP) matures, NLP components are increasingly used in real-world applications.", "labels": [], "entities": [{"text": "statistical Natural Language Processing (NLP)", "start_pos": 3, "end_pos": 48, "type": "TASK", "confidence": 0.7354750122342791}]}, {"text": "In many cases, this means that some form of cross-domain adaptation is necessary because there are distributional differences between the labeled training set that is available and the real-world data in the application.", "labels": [], "entities": [{"text": "cross-domain adaptation", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7279796898365021}]}, {"text": "To address this problem, we propose anew type of features for NLP data, features extracted from search engine results.", "labels": [], "entities": []}, {"text": "Our motivation is that search engine results can be viewed as a substitute for the world knowledge that is required in NLP tasks, but that can only be extracted from a standard training set or precompiled resources to a limited extent.", "labels": [], "entities": []}, {"text": "For example, a named entity (NE) recognizer trained on news text may tag the NE London in an out-of-domain web query like London Klondike gold rush as a location.", "labels": [], "entities": [{"text": "London Klondike gold rush", "start_pos": 122, "end_pos": 147, "type": "DATASET", "confidence": 0.9456293880939484}]}, {"text": "But if we train the recognizer on features derived from search results for the sentence to be tagged, correct classification as person is possible.", "labels": [], "entities": []}, {"text": "This is because the search results for London Klondike gold rush contain snippets in which the first name Jack precedes London; this is a sure indicator of a last name and hence an NE of type person.", "labels": [], "entities": [{"text": "London Klondike gold rush", "start_pos": 39, "end_pos": 64, "type": "DATASET", "confidence": 0.9667870700359344}, {"text": "NE", "start_pos": 181, "end_pos": 183, "type": "METRIC", "confidence": 0.9769536852836609}]}, {"text": "We call our approach piggyback and search resultderived features piggyback features because we piggyback on a search engine like Google for solving a difficult NLP task.", "labels": [], "entities": []}, {"text": "In this paper, we use piggyback features to address a particularly hard cross-domain problem, the application of an NER system trained on news to web queries.", "labels": [], "entities": []}, {"text": "This problem is hard for two reasons.", "labels": [], "entities": []}, {"text": "First, the most reliable cue for NEs in English, as in many languages, is capitalization.", "labels": [], "entities": [{"text": "NEs", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9544529914855957}]}, {"text": "But queries are generally lowercase and even if uppercase characters are used, they are not consistent enough to be reliable features.", "labels": [], "entities": []}, {"text": "Thus, applying NER systems trained on news to web queries requires a robust cross-domain approach.", "labels": [], "entities": []}, {"text": "News to queries adaptation is also hard because queries provide limited context for NEs.", "labels": [], "entities": [{"text": "News to queries adaptation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5659060850739479}]}, {"text": "In news text, the first mention of a word like Ford is often a fully qualified, unambiguous name like Ford Motor Corporation or Gerald Ford.", "labels": [], "entities": []}, {"text": "Ina short query like buy ford or ford pardon, there is much less context than in news.", "labels": [], "entities": [{"text": "ford pardon", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.5424130409955978}]}, {"text": "The lack of context and capitalization, and the noisiness of real-world web queries (tokenization irregularities and misspellings) all make NER hard.", "labels": [], "entities": [{"text": "NER", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9252583980560303}]}, {"text": "The low annotator agreement we found for queries (Section 5) also confirms this point.", "labels": [], "entities": []}, {"text": "The correct identification of NEs in web queries can be crucial for providing relevant pages and ads to users.", "labels": [], "entities": []}, {"text": "Other domains have characteristics similar to web queries, e.g., automatically transcribed speech, social communities like Twitter, and SMS.", "labels": [], "entities": []}, {"text": "Thus, NER for short, noisy text fragments, in the absence of capitalization, is of general importance.", "labels": [], "entities": [{"text": "NER", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.5563901662826538}]}, {"text": "NER performance is to a large extent determined by the quality of the feature representation.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9084302186965942}]}, {"text": "Lexical, part-of-speech (PoS), shape and gazetteer features are standard.", "labels": [], "entities": []}, {"text": "While the impact of different types of features is well understood for standard NER, fundamentally different types of features can be used when leveraging search engine results.", "labels": [], "entities": []}, {"text": "Returning to the NE London in the query London Klondike gold rush, the feature \"proportion of search engine results in which a first name precedes the token of interest\" is likely to be useful in NER.", "labels": [], "entities": [{"text": "NE London in the query London Klondike gold rush", "start_pos": 17, "end_pos": 65, "type": "DATASET", "confidence": 0.8063517842027876}, {"text": "NER", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.5689043402671814}]}, {"text": "Since using search engine results for cross-domain robustness is anew approach in NLP, the design of appropriate features is crucial to its success.", "labels": [], "entities": []}, {"text": "A significant part of this paper is devoted to feature design and evaluation.", "labels": [], "entities": [{"text": "feature design and evaluation", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.7712257951498032}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "We describe standard NER features in Section 3.", "labels": [], "entities": [{"text": "NER", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8504889607429504}]}, {"text": "One main contribution of this paper is the large array of piggyback features that we propose in Section 4.", "labels": [], "entities": []}, {"text": "We describe the data sets we use and our experimental setup in Sections 5-6.", "labels": [], "entities": []}, {"text": "The results in Section 7 show that piggyback features significantly increase NER performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9716211557388306}]}, {"text": "This is the second main contribution of the paper.", "labels": [], "entities": []}, {"text": "We discuss challenges of using piggyback features -due to the cost of querying search engines -and present our conclusions and future work in Section 8.", "labels": [], "entities": []}, {"text": "found that capitalization of NEs in web queries is inconsistent and not a reliable cue for NER.", "labels": [], "entities": []}, {"text": "exploit query logs for NER in queries.", "labels": [], "entities": []}, {"text": "This is also promising, but the context in search results is richer and potentially more informative than that of other queries in logs.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we train an NER classifier on an in-domain data set and test it on two different outof-domain data sets.", "labels": [], "entities": [{"text": "NER classifier", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8814075291156769}]}, {"text": "We describe these data sets in  this section and the NER classifier and the details of the training regime in the next section, Section 6.", "labels": [], "entities": [{"text": "NER classifier", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.6286889910697937}]}, {"text": "As training data for all models evaluated we used the CoNLL 2003 English NER dataset, a corpus of approximately 300,000 tokens of Reuters news from 1992 annotated with person, location, organization and miscellaneous NE labels).", "labels": [], "entities": [{"text": "CoNLL 2003 English NER dataset", "start_pos": 54, "end_pos": 84, "type": "DATASET", "confidence": 0.9459342122077942}]}, {"text": "As out-of-domain newswire evaluation data we use the development test data from the NIST 1999 IEER named entity corpus, a dataset of 50,000 tokens of New York Times (NYT) and Associated Press Weekly news.", "labels": [], "entities": [{"text": "NIST 1999 IEER named entity corpus, a dataset of 50,000 tokens of New York Times (NYT) and Associated Press Weekly news", "start_pos": 84, "end_pos": 203, "type": "DATASET", "confidence": 0.7981922601660093}]}, {"text": "This corpus is annotated with person, location, organization, cardinal, duration, measure, and date labels.", "labels": [], "entities": [{"text": "duration", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9683199524879456}]}, {"text": "CoNLL and IEER are professionally edited and, in particular, properly capitalized news corpora.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8336108922958374}, {"text": "IEER", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.511389434337616}]}, {"text": "As capitalization is absent from queries we lowercased both CoNLL and IEER.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.48932579159736633}, {"text": "IEER", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9734785556793213}]}, {"text": "We also reannotated the lowercased datasets with PoS categories using the retrained TnT PoS tagger) to avoid using non-plausible PoS information.", "labels": [], "entities": []}, {"text": "Notice that this step is necessary as otherwise virtually no NNP/NNPS categories would be predicted on the query data because the lowercase NEs of web queries never occur in properly capitalized news; this causes an NER tagger trained on standard PoS to underpredict NEs (1-3% positive rate).", "labels": [], "entities": []}, {"text": "The 2005 KDD Cup is a query topic categorization task based on 800,000 queries ().", "labels": [], "entities": [{"text": "KDD Cup", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.784745067358017}]}, {"text": "We use a random subset of 2000 queries as a source of web queries.", "labels": [], "entities": []}, {"text": "By means of simple regular expressions we excluded from sampling queries that looked like urls or emails (\u2248 15%) as they are easy to identify and do not provide a significant chal-lenge.", "labels": [], "entities": []}, {"text": "We also excluded queries shorter than 10 characters (4%) and longer than 50 characters (2%) to provide annotators with enough context, but not an overly complex task.", "labels": [], "entities": []}, {"text": "The annotation procedure was carried out using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.9799923499425253}]}, {"text": "We instructed workers to follow the CoNLL 2003 NER guidelines (augmented with several examples from queries that we annotated) and identify up to three NEs in a short text and copy and paste them into a box with associated multiple choice menu with the 4 CoNLL NE labels: LOC, MISC, ORG, and PER.", "labels": [], "entities": [{"text": "CoNLL 2003 NER guidelines", "start_pos": 36, "end_pos": 61, "type": "DATASET", "confidence": 0.912692204117775}, {"text": "LOC", "start_pos": 272, "end_pos": 275, "type": "METRIC", "confidence": 0.9324719309806824}, {"text": "MISC", "start_pos": 277, "end_pos": 281, "type": "METRIC", "confidence": 0.9275864958763123}, {"text": "ORG", "start_pos": 283, "end_pos": 286, "type": "METRIC", "confidence": 0.9850106835365295}, {"text": "PER", "start_pos": 292, "end_pos": 295, "type": "METRIC", "confidence": 0.9930041432380676}]}, {"text": "Five workers annotated each query.", "labels": [], "entities": []}, {"text": "Ina first round we produced 1000 queries later used for development.", "labels": [], "entities": []}, {"text": "We call this set KDD-D.", "labels": [], "entities": []}, {"text": "We then expanded the guidelines with a few uncertain cases.", "labels": [], "entities": []}, {"text": "Ina second round, we generated another 1000 queries.", "labels": [], "entities": []}, {"text": "This set will be referred to as KDD-T.", "labels": [], "entities": [{"text": "KDD-T", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8913845419883728}]}, {"text": "Because annotator agreement is low on a per-token basis (\u03ba = .30 for KDD-D, \u03ba = .34 for KDD-T), we remove queries with less than 50% agreement, averaged over the tokens in the query.", "labels": [], "entities": []}, {"text": "After this filtering, KDD-D and KDD-T contain 777 and 819 queries, respectively.", "labels": [], "entities": []}, {"text": "Most of the rater disagreement involves the MISC NE class.", "labels": [], "entities": [{"text": "MISC NE class", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.6401161750157675}]}, {"text": "This is not surprising as MISC is a sort of place-holder category that is difficult to define and identify in queries, especially by untrained AMT workers.", "labels": [], "entities": []}, {"text": "We thus replaced MISC with the null label O.", "labels": [], "entities": [{"text": "MISC", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.45697715878486633}]}, {"text": "With these two changes, \u03ba was .54 on KDD-D and .64 on KDD-T.", "labels": [], "entities": [{"text": "KDD-D", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.9212926030158997}]}, {"text": "This is sufficient for repeatable experiments.", "labels": [], "entities": []}, {"text": "6 shows the distribution of NE types in the 5 datasets.", "labels": [], "entities": []}, {"text": "IEER has fewer NEs than CoNLL, KDD has more.", "labels": [], "entities": [{"text": "IEER", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5179688930511475}, {"text": "NEs", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9819538593292236}, {"text": "CoNLL", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8739756345748901}, {"text": "KDD", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8255318999290466}]}, {"text": "PER is about as prevalent in KDD as in CoNLL, but LOC and ORG have higher percentages, reflecting the fact that people search frequently for locations and commercial organizations.", "labels": [], "entities": [{"text": "PER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9925283193588257}, {"text": "CoNLL", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8648160099983215}, {"text": "LOC", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.879128098487854}, {"text": "ORG", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9920428395271301}]}, {"text": "These differences between source domain (CoNLL) and target domains (IEER, KDD) add to the difficulty of cross-domain generalization in this case.", "labels": [], "entities": [{"text": "IEER", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6579229831695557}, {"text": "cross-domain generalization", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.7031940221786499}]}, {"text": "Recall that the input features fora token w 0 consist of standard NER features (BASE and GAZ) and features derived from the search result we obtain by running a search for w \u22121 w 0 w 1 (URL, LEX, BOW, and MISC).", "labels": [], "entities": [{"text": "BASE", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9955572485923767}, {"text": "BOW", "start_pos": 196, "end_pos": 199, "type": "METRIC", "confidence": 0.9171595573425293}]}, {"text": "Since the MISC NE class is not annotated in IEER and has low agreement on KDD in the experimental evaluation we focus on the fourclass (PER, LOC, ORG, O) NER problem on all datasets.", "labels": [], "entities": [{"text": "MISC NE class", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.6941403349240621}, {"text": "IEER", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.7663263082504272}, {"text": "KDD", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.97252357006073}]}, {"text": "We use BIO encoding as in the original CoNLL task: Evaluation results.", "labels": [], "entities": [{"text": "BIO encoding", "start_pos": 7, "end_pos": 19, "type": "METRIC", "confidence": 0.9558453857898712}]}, {"text": "l = text lowercased, c = original capitalization preserved.", "labels": [], "entities": []}, {"text": "ALL scores significantly different from the best results for the three datasets (lines c7, i8, k7) are marked * (see text).", "labels": [], "entities": [{"text": "ALL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7968428730964661}]}, {"text": "We use SuperSenseTagger () as our NER tagger.", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.5746848583221436}]}, {"text": "It is a first-order conditional HMM trained with the perceptron algo-7 sourceforge.net/projects/supersensetag rithm), a discriminative model with excellent efficiency-performance trade-off.", "labels": [], "entities": []}, {"text": "The model is regularized by averaging).", "labels": [], "entities": []}, {"text": "For all models we used an appropriate development set for choosing the only hyperparameter, T , the number of training iterations on the source data.", "labels": [], "entities": []}, {"text": "T must be tuned separately for each evaluation because different target domains have different overfitting patterns.", "labels": [], "entities": [{"text": "T", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.8912301063537598}]}, {"text": "We train our NER system on an 80% sample of the CoNLL data.", "labels": [], "entities": [{"text": "NER", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8685063719749451}, {"text": "CoNLL data", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9776611328125}]}, {"text": "For our in-domain evaluation, we tune T on a 10% development sample of the CoNLL data and test on the remaining 10%.", "labels": [], "entities": [{"text": "T", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9836417436599731}, {"text": "CoNLL data", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9466019570827484}]}, {"text": "For our out-ofdomain evaluation, we use the IEER and KDD test sets.", "labels": [], "entities": [{"text": "IEER", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9928541779518127}, {"text": "KDD test sets", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.876936117808024}]}, {"text": "Here T is tuned on the corresponding development sets.", "labels": [], "entities": []}, {"text": "Since we do not train on IEER and KDD, these two data sets do not have training set portions.", "labels": [], "entities": [{"text": "IEER", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.6079588532447815}, {"text": "KDD", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.6757448315620422}]}, {"text": "For each data set, we perform 63 runs, corresponding to the 2 6 \u22121 = 63 different non-empty combinations of the 6 feature groups.", "labels": [], "entities": []}, {"text": "We report average F 1 , generated by five-trial training and evaluation, with random permutations of the training data.", "labels": [], "entities": [{"text": "F 1", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9667547047138214}]}, {"text": "We compute the scores using the original CoNLL phrasebased metric.", "labels": [], "entities": [{"text": "CoNLL phrasebased metric", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.9213258624076843}]}, {"text": "As a benchmark we use the baseline model with gazetteer features (BASE and GAZ).", "labels": [], "entities": [{"text": "BASE", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.998103141784668}, {"text": "GAZ", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9165604710578918}]}, {"text": "The robustness of this simple approach is well documented; e.g., show that the baseline model (gazetteer features without unsupervised features) produces an F 1 of .778 against .788 of the best unsupervised word representation feature.", "labels": [], "entities": [{"text": "F 1", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9951423406600952}]}, {"text": "In each column, the best numbers within a dataset for the \"lowercased\" runs are bolded (see below for discussion of the \"capitalization\" runs on lines c9 and i9).", "labels": [], "entities": []}, {"text": "For all experiments, we selected a subset of the combinations of the feature groups.", "labels": [], "entities": []}, {"text": "This subset always includes the best results and a number of other combinations where feature groups are added to or removed from the optimal combination.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentages of NEs in CoNLL, IEER, and KDD.", "labels": [], "entities": [{"text": "Percentages", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.955033540725708}, {"text": "CoNLL", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8806482553482056}, {"text": "IEER", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.7713629603385925}, {"text": "KDD", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.7135467529296875}]}, {"text": " Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).", "labels": [], "entities": [{"text": "ALL", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9907089471817017}]}]}