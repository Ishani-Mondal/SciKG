{"title": [{"text": "Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.7385714451471964}]}], "abstractContent": [{"text": "In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 182, "end_pos": 213, "type": "TASK", "confidence": 0.6771253446737925}]}, {"text": "We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 42, "end_pos": 86, "type": "TASK", "confidence": 0.5802770927548409}]}, {"text": "Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.9567110538482666}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9987101554870605}]}], "introductionContent": [{"text": "Language model is one of the most important knowledge sources for statistical machine translation (SMT) ().", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.8117521206537882}]}, {"text": "The standard n-gram language model) assigns probabilities to hypotheses in the target language conditioning on a context history of the preceding n \u2212 1 words.", "labels": [], "entities": []}, {"text": "Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 222, "end_pos": 225, "type": "TASK", "confidence": 0.9934493899345398}]}, {"text": "We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented.", "labels": [], "entities": []}, {"text": "In the first direction, more data is better.", "labels": [], "entities": []}, {"text": "In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are builtin a distributed manner (.", "labels": [], "entities": []}, {"text": "Such language models yield better translation results but at the cost of huge storage and high computation.", "labels": [], "entities": [{"text": "translation", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9586508274078369}]}, {"text": "The second direction digs deeply into monolingual data to build linguistically-informed language models.", "labels": [], "entities": []}, {"text": "For example, present a syntax-based language model for machine translation which is trained on syntactic parse trees.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8169500529766083}]}, {"text": "Again, explore a dependency language model to improve translation quality.", "labels": [], "entities": []}, {"text": "To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies.", "labels": [], "entities": []}, {"text": "In this paper, we pursue the second direction without resorting to any linguistic resources such as a syntactic parser.", "labels": [], "entities": []}, {"text": "With a belief that a language model that embraces a larger context provides better prediction ability, we learn additional information from training data to enhance conventional n-gram language models and extend their ability to capture richer contexts and long-distance dependencies.", "labels": [], "entities": []}, {"text": "In particular, we integrate backward n-grams and mutual information (MI) triggers into language models in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9785820841789246}]}, {"text": "In conventional n-gram language models, we look at the preceding n \u2212 1 words when calculating the probability of the current word.", "labels": [], "entities": []}, {"text": "We henceforth call the previous n \u2212 1 words plus the current word as forward n-grams and a language model built on forward n-grams as forward n-gram language model.", "labels": [], "entities": []}, {"text": "Similarly, backward n-grams refer to the succeeding n \u2212 1 words plus the current word.", "labels": [], "entities": []}, {"text": "We train a backward n-gram language model on backward n-grams and integrate the forward and backward language models together into the decoder.", "labels": [], "entities": []}, {"text": "In doing so, we attempt to capture both the preceding and succeeding contexts of the current word.", "labels": [], "entities": []}, {"text": "Different from the backward n-gram language model, the MI trigger model still looks at previous contexts, which however go beyond the scope of forward n-grams.", "labels": [], "entities": []}, {"text": "If the current word is indexed as w i , the farthest word that the forward n-gram includes is w i\u2212n+1 . However, the MI triggers are capable of detecting dependencies between w i and words from w 1 tow i\u2212n . By these triggers ({w k \u2192 w i }, 1 \u2264 k \u2264 i \u2212 n), we can capture long-distance dependencies that are outside the scope of forward n-grams.", "labels": [], "entities": []}, {"text": "We integrate the proposed backward language model and the MI trigger model into a state-ofthe-art phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9213871359825134}]}, {"text": "We evaluate the effectiveness of both models on Chinese-toEnglish translation tasks with large-scale training data.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation tasks", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.6617654164632162}]}, {"text": "Compared with the baseline which only uses the forward language model, our experimental results show that the additional backward language model is able to gain about 0.5 BLEU points, while the MI trigger model gains about 0.4 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.997260570526123}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9959220886230469}]}, {"text": "When both models are integrated into the decoder, they collectively improve the performance by up to 1 BLEU point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9992296695709229}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we will briefly introduce related work and show how our models differ from previous work.", "labels": [], "entities": []}, {"text": "Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder.", "labels": [], "entities": []}, {"text": "Section 5 will empirically evaluate the effectiveness of these two models.", "labels": [], "entities": []}, {"text": "Section 6 will conduct an indepth analysis.", "labels": [], "entities": []}, {"text": "In the end, we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conduct large-scale experiments on NIST Chinese-to-English translation tasks to evaluate the effectiveness of the proposed backward language model and MI trigger model in SMT.", "labels": [], "entities": [{"text": "NIST Chinese-to-English translation tasks", "start_pos": 55, "end_pos": 96, "type": "TASK", "confidence": 0.7800299972295761}, {"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.9905374646186829}]}, {"text": "Our experiments focus on the following two issues: 1.", "labels": [], "entities": []}, {"text": "How much improvements can we achieve by separately integrating the backward language model and the MI trigger model into our phrase-based SMT system?", "labels": [], "entities": [{"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.868539035320282}]}, {"text": "2. Can we obtain a further improvement if we jointly apply both models?", "labels": [], "entities": []}, {"text": "Our training corpora 7 consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs.", "labels": [], "entities": []}, {"text": "We used all corpora to train our translation model and smaller corpora without the United Nations corpus to build a maximum entropy based reordering model ().", "labels": [], "entities": [{"text": "United Nations corpus", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.9409355521202087}]}, {"text": "To train our language models and MI trigger model, we used the Xinhua section of the English Gigaword corpus (306 million words).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.8437677224477133}]}, {"text": "Firstly, we built a forward 5-gram language model using the SRILM toolkit) with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8947766125202179}]}, {"text": "Then we trained a backward 5-gram language model on the same monolingual corpus in the way described in Section 3.1.", "labels": [], "entities": []}, {"text": "Finally, we trained our MI trigger model still on this corpus according to the method in Section 4.1.", "labels": [], "entities": []}, {"text": "The trained MI trigger model consists of 2.88M trigger pairs.", "labels": [], "entities": []}, {"text": "We used the NIST MT03 evaluation test data as the development set, and the NIST MT04, MT05 as the test sets.", "labels": [], "entities": [{"text": "NIST MT03 evaluation test data", "start_pos": 12, "end_pos": 42, "type": "DATASET", "confidence": 0.8875340104103089}, {"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.9878042936325073}, {"text": "MT04", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.5330452919006348}, {"text": "MT05", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9249312877655029}]}, {"text": "We adopted the case-insensitive BLEU-4 () as the evaluation metric, which uses the shortest reference sentence length for the brevity penalty.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.981223464012146}]}, {"text": "Statistical significance in BLEU differences is tested by paired bootstrap re-sampling).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9938250780105591}]}, {"text": "The experimental results on the two NIST test sets are shown in: BLEU-4 scores (%) on the two test sets for different language models and their combinations.", "labels": [], "entities": [{"text": "NIST test sets", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9656507968902588}, {"text": "BLEU-4", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9997301697731018}]}, {"text": "+: better than the baseline (p < 0.01).", "labels": [], "entities": []}, {"text": "model, we obtain 0.49 and 0.56 BLEU points over the baseline on the MT-04 and MT-05 test set respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9992738366127014}, {"text": "MT-04", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.911999523639679}, {"text": "MT-05 test set", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9180723230044047}]}, {"text": "Both improvements are statistically significant (p < 0.01).", "labels": [], "entities": []}, {"text": "The MI trigger model also achieves statistically significant improvements of 0.33 and 0.44 BLEU points over the baseline on the MT-04 and MT-05 respectively.", "labels": [], "entities": [{"text": "MI trigger", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.7953844368457794}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9987647533416748}, {"text": "MT-04", "start_pos": 128, "end_pos": 133, "type": "DATASET", "confidence": 0.9565921425819397}, {"text": "MT-05", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.8251215219497681}]}, {"text": "When we integrate both the backward language model and the MI trigger model into our system, we obtain improvements of 1.09 and 0.71 BLEU points over the single forward language model on the MT-04 and MT-05 respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9992105960845947}, {"text": "MT-04", "start_pos": 191, "end_pos": 196, "type": "DATASET", "confidence": 0.9519997835159302}, {"text": "MT-05", "start_pos": 201, "end_pos": 206, "type": "DATASET", "confidence": 0.8702727556228638}]}, {"text": "These improvements are larger than those achieved by using only one model (the backward language model or the MI trigger model).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. When we combine the back- ward language model with the forward language Model  MT-04 MT-05  Forward (Baseline)  35.67  34.41  Forward+Backward  36.16+ 34.97+  Forward+MI  36.00+ 34.85+  Forward+Backward+MI 36.76+ 35.12+", "labels": [], "entities": [{"text": "MT-04", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.7721022963523865}, {"text": "MT-05", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.5819492936134338}, {"text": "Forward", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.5806601643562317}]}]}