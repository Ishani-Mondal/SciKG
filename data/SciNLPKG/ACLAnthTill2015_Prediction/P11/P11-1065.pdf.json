{"title": [{"text": "Learning Hierarchical Translation Structure with Linguistic Annotations", "labels": [], "entities": [{"text": "Hierarchical Translation Structure", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.7489241162935892}]}], "abstractContent": [{"text": "While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task.", "labels": [], "entities": []}, {"text": "The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations.", "labels": [], "entities": []}, {"text": "These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents.", "labels": [], "entities": []}, {"text": "We propose a novel flexible modelling approach to introduce linguistic information of varying gran-ularity from the source side.", "labels": [], "entities": []}, {"text": "Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data.", "labels": [], "entities": []}, {"text": "We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9983850717544556}]}], "introductionContent": [{"text": "Recent advances in Statistical Machine Translation (SMT) are widely centred around two concepts: (a) hierarchical translation processes, frequently employing Synchronous Context Free Grammars (SCFGs) and (b) transduction or synchronous rewrite processes over a linguistic syntactic tree.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.8605966369311014}, {"text": "hierarchical translation", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.6026909947395325}]}, {"text": "SCFGs in the form of the Inversion-Transduction Grammar (ITG) were first introduced by as a formalism to recursively describe the translation process.", "labels": [], "entities": []}, {"text": "The Hiero system utilised an ITG-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with 'gaps', offering competitive performance particularly for language pairs with extensive reordering.", "labels": [], "entities": []}, {"text": "As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level.", "labels": [], "entities": []}, {"text": "Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.7510961890220642}]}, {"text": "Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (;), target side () or both (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9951501488685608}]}, {"text": "Hierarchical translation was combined with target side linguistic annotation in ().", "labels": [], "entities": [{"text": "Hierarchical translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7840488255023956}]}, {"text": "Interestingly, early on () exemplified the difficulties of integrating linguistic information in translation systems.", "labels": [], "entities": []}, {"text": "Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation.", "labels": [], "entities": [{"text": "Syntaxbased MT", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6217119991779327}]}, {"text": "Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (, or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (, obscuring the impact of higher level syntactic processes.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 119, "end_pos": 131, "type": "TASK", "confidence": 0.8774301409721375}]}, {"text": "While it is assumed that linguistic structure does correlate with some translation phenomena, in this 642 work we do not employ it as the backbone of translation.", "labels": [], "entities": []}, {"text": "In place of linguistically constrained translation imposing syntactic parse structure, we opt for linguistically motivated translation.", "labels": [], "entities": []}, {"text": "We learn latent hierarchical structure, taking advantage of linguistic annotations but shaped and trained for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.9655060172080994}]}, {"text": "We start by labelling each phrase-pair span in the word-aligned training data with multiple linguistically motivated categories, offering multi-grained abstractions from its lexical content.", "labels": [], "entities": []}, {"text": "These phrasepair label charts are the input of our learning algorithm, which extracts the linguistically motivated rules and estimates the probabilities fora stochastic SCFG, without arbitrary constraints such as phrase or span sizes.", "labels": [], "entities": []}, {"text": "Estimating such grammars under a Maximum Likelihood criterion is known to be plagued by strong overfitting leading to degenerate estimates).", "labels": [], "entities": []}, {"text": "In contrast, our learning objective not only avoids overfitting the training data but, most importantly, learns joint stochastic synchronous grammars which directly aim at generalisation towards yet unseen instances.", "labels": [], "entities": []}, {"text": "By advancing from structures which mimic linguistic syntax, to learning linguistically aware latent recursive structures targeting translation, we achieve significant improvements in translation quality for 4 different language pairs in comparison with a strong hierarchical translation baseline.", "labels": [], "entities": []}, {"text": "Our key contributions are presented in the following sections.", "labels": [], "entities": []}, {"text": "Section 2 discusses the weak independence assumptions of SCFGs and introduces a joint translation model which addresses these issues and separates hierarchical translation structure from phrase-pair emission.", "labels": [], "entities": []}, {"text": "In section 3 we consider a chart over phrase-pair spans filled with sourcelanguage linguistically motivated labels.", "labels": [], "entities": []}, {"text": "We show how we can employ this crucial input to extract and train a hierarchical translation structure model with millions of rules.", "labels": [], "entities": []}, {"text": "Section 4 demonstrates decoding with the model by constraining derivations to linguistic hints of the source sentence and presents our empirical results.", "labels": [], "entities": []}, {"text": "We close with a discussion of related work and our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on four different language pairs with English as the source language and French, German, Dutch and Chinese as target.", "labels": [], "entities": []}, {"text": "The data for the first three language pairs are derived from parliament proceedings sourced from the Europarl corpus (, with WMT-07 development and test data for French and German.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.9946878254413605}]}, {"text": "The data for the English to Chinese task is composed of parliament proceedings and news articles.", "labels": [], "entities": []}, {"text": "For all language pairs we employ 200K and 400K sentence pairs for training, 2K for development and 2K for testing (single reference per source sentence).", "labels": [], "entities": []}, {"text": "Both the baseline and our method decode 647 with a 3-gram language model smoothed with modified Knesser-Ney discounting, trained on around 1M sentences per target language.", "labels": [], "entities": []}, {"text": "The parses of the source sentences employed by our system during training and decoding are created with the Charniak parser.", "labels": [], "entities": []}, {"text": "We compare against a state-of-the-art hierarchical translation baseline, based on the Joshua translation system under the default training and decoding settings (josh-base).", "labels": [], "entities": []}, {"text": "Apart of evaluating against a state-of-the-art system, especially on the English-Chinese language pair, the comparison has an added interesting aspect.", "labels": [], "entities": []}, {"text": "The heuristically trained baseline takes advantage of 'gap rules' to reorder based on lexical context cues, but makes very limited use of the hierarchical structure above the lexical surface.", "labels": [], "entities": []}, {"text": "In contrast, our method induces a grammar with no such rules, relying on lexical content and the strength of a higher level translation structure instead.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score im- provements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two.", "labels": [], "entities": []}, {"text": " Table 2: Additional experiments for English to Chi- nese translation examining (a) the impact of the linguis- tic annotations in the LTS system (lts), when com- pared with an instance not employing such annotations  (lts-nolabels) and (b) decoding with a 4th-order  language model (-lm4). BLEU scores for 200K and  400K training sentence pairs.", "labels": [], "entities": [{"text": "English to Chi- nese translation", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.6134508401155472}, {"text": "BLEU", "start_pos": 290, "end_pos": 294, "type": "METRIC", "confidence": 0.9976703524589539}]}]}