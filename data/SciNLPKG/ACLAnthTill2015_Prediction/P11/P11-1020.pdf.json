{"title": [{"text": "Collecting Highly Parallel Data for Paraphrase Evaluation", "labels": [], "entities": [{"text": "Paraphrase Evaluation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.8758048713207245}]}], "abstractContent": [{"text": "A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7229190766811371}]}, {"text": "We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.", "labels": [], "entities": []}, {"text": "The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates.", "labels": [], "entities": []}, {"text": "In addition to being simple and efficient to compute , experiments show that these metrics correlate highly with human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine paraphrasing has many applications for natural language processing tasks, including machine translation (MT), MT evaluation, summary evaluation, question answering, and natural language generation.", "labels": [], "entities": [{"text": "Machine paraphrasing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7731881439685822}, {"text": "machine translation (MT)", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8400694489479065}, {"text": "MT evaluation", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.9778923988342285}, {"text": "summary evaluation", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.8263299465179443}, {"text": "question answering", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.896213948726654}, {"text": "natural language generation", "start_pos": 177, "end_pos": 204, "type": "TASK", "confidence": 0.6693570613861084}]}, {"text": "However, alack of standard datasets and automatic evaluation metrics has impeded progress in the field.", "labels": [], "entities": []}, {"text": "Without these resources, researchers have resorted to developing their own small, ad hoc datasets (;), and have often relied on human judgments to evaluate their results ().", "labels": [], "entities": []}, {"text": "Consequently, it is difficult to compare different systems and assess the progress of the field as a whole.", "labels": [], "entities": []}, {"text": "Despite the similarities between paraphrasing and translation, several major differences have prevented researchers from simply following standards that have been established for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7250115722417831}]}, {"text": "Professional translators produce large volumes of bilingual data according to a more or less consistent specification, indirectly fueling work on machine translation algorithms.", "labels": [], "entities": [{"text": "machine translation algorithms", "start_pos": 146, "end_pos": 176, "type": "TASK", "confidence": 0.8495823939641317}]}, {"text": "In contrast, there are no \"professional paraphrasers\", with the result that there are no readily available large corpora and no consistent standards for what constitutes a high-quality paraphrase.", "labels": [], "entities": []}, {"text": "In addition to the lack of standard datasets for training and testing, there are also no standard metrics like BLEU () for evaluating paraphrase systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9982407093048096}]}, {"text": "Paraphrase evaluation is inherently difficult because the range of potential paraphrases fora given input is both large and unpredictable; in addition to being meaning-preserving, an ideal paraphrase must also diverge as sharply as possible inform from the original while still sounding natural and fluent.", "labels": [], "entities": [{"text": "Paraphrase evaluation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9549532234668732}]}, {"text": "Our work introduces two novel contributions which combine to address the challenges posed by paraphrase evaluation.", "labels": [], "entities": [{"text": "paraphrase evaluation", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.9335365891456604}]}, {"text": "First, we describe a framework for easily and inexpensively crowdsourcing arbitrarily large training and test sets of independent, redundant linguistic descriptions of the same semantic content.", "labels": [], "entities": []}, {"text": "Second, we define anew evaluation metric, PINC, that relies on simple BLEU-like n-gram comparisons to measure the degree of novelty of automatically generated paraphrases.", "labels": [], "entities": [{"text": "PINC", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8191820979118347}, {"text": "BLEU-like", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9961728453636169}]}, {"text": "We believe that this metric, along with the sentence-level paraphrases provided by our data collection approach, will make it possi-ble for researchers working on paraphrasing to compare system performance and exploit the kind of automated, rapid training-test cycle that has driven work on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 291, "end_pos": 322, "type": "TASK", "confidence": 0.8292748332023621}]}, {"text": "In addition to describing a mechanism for collecting large-scale sentence-level paraphrases, we are also making available to the research community 85K parallel English sentences as part of the Microsoft Research Video Description Corpus . The rest of the paper is organized as follows.", "labels": [], "entities": [{"text": "Microsoft Research Video Description Corpus", "start_pos": 194, "end_pos": 237, "type": "DATASET", "confidence": 0.8129317045211792}]}, {"text": "We first review relevant work in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 then describes our data collection framework and the resulting data.", "labels": [], "entities": []}, {"text": "Section 4 discusses automatic evaluations of paraphrases and introduces the novel metric PINC.", "labels": [], "entities": []}, {"text": "Section 5 presents experimental results establishing a correlation between our automatic metric and human judgments.", "labels": [], "entities": []}, {"text": "Sections 6 and 7 discuss possible directions for future research and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the limitations to the development of machine paraphrasing is the lack of standard metrics like BLEU, which has played a crucial role in driving progress in MT.", "labels": [], "entities": [{"text": "machine paraphrasing", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7617321610450745}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9982013702392578}, {"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.9929240942001343}]}, {"text": "Part of the issue is that a good paraphrase has the additional constraint that it should be lexically dissimilar to the source sentence while preserving the meaning.", "labels": [], "entities": []}, {"text": "These can become competing goals when using n-gram overlaps to establish semantic equivalence.", "labels": [], "entities": []}, {"text": "Thus, researchers have been unable to rely on BLEU or some derivative: the optimal paraphrasing engine under these terms would be one that simply returns the input.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9952882528305054}]}, {"text": "To combat such problems, have proposed PEM, which uses a second language as pivot to establish semantic equivalence.", "labels": [], "entities": []}, {"text": "Thus, no n-gram overlaps are required to determine the semantic adequacy of the paraphrase candidates.", "labels": [], "entities": []}, {"text": "PEM also separately measures lexical dissimilarity and fluency.", "labels": [], "entities": [{"text": "PEM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5110315084457397}]}, {"text": "Finally, all three scores are combined using a support vector machine (SVM) trained on human ratings of paraphrase pairs.", "labels": [], "entities": []}, {"text": "While PEM was shown to correlate well with human judgments, it has some limitations.", "labels": [], "entities": [{"text": "PEM", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.8844365477561951}]}, {"text": "It only models paraphrasing at the phrase level and not at the sentence level.", "labels": [], "entities": []}, {"text": "Further, while it does not need reference sentences for the evaluation dataset, PEM does require suitable bilingual data to train the metric.", "labels": [], "entities": [{"text": "PEM", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.8728197813034058}]}, {"text": "The result is that training a successful PEM becomes almost as challenging as the original paraphrasing problem, since paraphrases need to be learned from bilingual data.", "labels": [], "entities": [{"text": "PEM", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9252534508705139}]}, {"text": "The highly parallel nature of our data suggests a simpler solution to this problem.", "labels": [], "entities": []}, {"text": "To measure semantic equivalence, we simply use BLEU with multiple references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.995677649974823}]}, {"text": "The large number of reference paraphrases capture a wide space of sentences with equivalent meanings.", "labels": [], "entities": []}, {"text": "While the set of reference sentences can of course never be exhaustive, our data collection method provides a natural distribution of common phrases that might be used to describe an action or event.", "labels": [], "entities": []}, {"text": "A tight cluster with many similar parallel descriptions suggests there are only few common ways to express that concept.", "labels": [], "entities": []}, {"text": "In addition to measuring semantic adequacy and fluency using BLEU, we also need to measure lexical dissimilarity with the source sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.996394693851471}]}, {"text": "We introduce anew scoring metric PINC that measures how many n-grams differ between the two sentences.", "labels": [], "entities": []}, {"text": "In essence, it is the inverse of BLEU since we want to minimize the number of n-gram overlaps between the two sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9988124370574951}]}, {"text": "Specifically, for source sentence sand candidate sentence c: where N is the maximum n-gram considered and ngram sand n-gram care the lists of n-grams in the source and candidate sentences, respectively.", "labels": [], "entities": []}, {"text": "We use N = 4 in our evaluations.", "labels": [], "entities": []}, {"text": "The PINC score computes the percentage of ngrams that appear in the candidate sentence but not in the source sentence.", "labels": [], "entities": [{"text": "PINC score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.7148979008197784}]}, {"text": "This score is similar to the Jaccard distance, except that it excludes n-grams that only appear in the source sentence and not in the candidate sentence.", "labels": [], "entities": []}, {"text": "In other words, it rewards candi-dates for introducing new n-grams but not for omitting n-grams from the original sentence.", "labels": [], "entities": []}, {"text": "The results for each n are averaged arithmetically.", "labels": [], "entities": []}, {"text": "PINC evaluates single sentences instead of entire documents because we can reliably measure lexical dissimilarity at the sentence level.", "labels": [], "entities": []}, {"text": "Also notice that we do not put additional constraints on sentence length: while extremely short and extremely long sentences are likely to score high on PINC, they still must maintain semantic adequacy as measured by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9963723421096802}]}, {"text": "We use BLEU and PINC together as a 2-dimensional scoring metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9983115196228027}]}, {"text": "A good paraphrase, according to our evaluation metric, has few n-gram overlaps with the source sentence but many n-gram overlaps with the reference sentences.", "labels": [], "entities": []}, {"text": "This is consistent with our requirement that a good paraphrase should be lexically dissimilar from the source sentence while preserving its semantics.", "labels": [], "entities": []}, {"text": "Unlike, we treat these two criteria separately, since different applications might have different preferences for each.", "labels": [], "entities": []}, {"text": "For example, a paraphrase suggestion tool fora word processing software might be more concerned with semantic adequacy, since presenting a paraphrase that does not preserve the meaning would likely result in a negative user experience.", "labels": [], "entities": []}, {"text": "On the other hand, a query expansion algorithm might be less concerned with preserving the precise meaning so long as additional relevant terms are added to improve search recall.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7584108114242554}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9670412540435791}]}, {"text": "To verify the usefulness of our paraphrase corpus and the BLEU/PINC metric, we built and evaluated several paraphrase systems and compared the automatic scores to human ratings of the generated paraphrases.", "labels": [], "entities": [{"text": "BLEU/PINC metric", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.7929024994373322}]}, {"text": "We also investigated the pros and cons of collecting paraphrases using video annotation rather than directly eliciting them.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the two video description tasks", "labels": [], "entities": [{"text": "video description", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.754756897687912}]}, {"text": " Table 3: Average human ratings of the systems trained on  single parallel sentences and on all parallel sentences.", "labels": [], "entities": []}, {"text": " Table 4: Correlation between the human judges as well  as between the automatic metrics and the human judges.", "labels": [], "entities": []}]}