{"title": [{"text": "Event Discovery in Social Media Feeds", "labels": [], "entities": [{"text": "Event Discovery", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7056470960378647}]}], "abstractContent": [{"text": "We present a novel method for record extraction from social streams such as Twitter.", "labels": [], "entities": [{"text": "record extraction from social streams", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.8344034552574158}]}, {"text": "Unlike typical extraction setups, these environments are characterized by short, one sentence messages with heavily colloquial speech.", "labels": [], "entities": []}, {"text": "To further complicate matters, individual messages may not express the full relation to be uncovered, as is often assumed in extraction tasks.", "labels": [], "entities": []}, {"text": "We develop a graphical model that addresses these problems by learning a latent set of records and a record-message alignment simultaneously ; the output of our model is a set of canonical records, the values of which are consistent with aligned messages.", "labels": [], "entities": []}, {"text": "We demonstrate that our approach is able to accurately induce event records from Twitter messages , evaluated against events from a local city guide.", "labels": [], "entities": []}, {"text": "Our method achieves significant error reduction over baseline methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "We propose a method for discovering event records from social media feeds such as Twitter.", "labels": [], "entities": []}, {"text": "The task of extracting event properties has been well studied in the context of formal media (e.g., newswire), but data sources such as Twitter pose new challenges.", "labels": [], "entities": []}, {"text": "Social media messages are often short, make heavy use of colloquial language, and require situational context for interpretation (see examples in).", "labels": [], "entities": []}, {"text": "Not all properties of an event maybe expressed in a single message, and the mapping between messages and canonical event records is not obvious.", "labels": [], "entities": []}, {"text": "as location of the event and artist name.", "labels": [], "entities": []}, {"text": "We bias local decisions made by the CRF to be consistent with canonical record values, thereby facilitating consistency within an event cluster.", "labels": [], "entities": []}, {"text": "We employ a factorgraph model to capture the interaction between each of these decisions.", "labels": [], "entities": []}, {"text": "Variational inference techniques allow us to effectively and efficiently make predictions on a large body of messages.", "labels": [], "entities": []}, {"text": "A seed set of example records constitutes our only source of supervision; we do not observe alignment between these seed records and individual messages, nor any message-level field annotation.", "labels": [], "entities": []}, {"text": "The output of our model consists of an event-based clustering of messages, where each cluster is represented by a single multi-field record with a canonical value chosen for each field.", "labels": [], "entities": []}, {"text": "We apply our technique to construct entertainment event records for the city calendar section of NYC.com using a stream of Twitter messages.", "labels": [], "entities": []}, {"text": "Our method yields up to a 63% recall against the city table and up to 85% precision evaluated manually, significantly outperforming several baselines.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9985798597335815}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9995749592781067}]}], "datasetContent": [{"text": "Data We apply our approach to construct a database of concerts in New York City.", "labels": [], "entities": []}, {"text": "We used Twitter's public API to collect roughly 4.7 Million tweets across three weekends that we subsequently filter down to 5,800 messages.", "labels": [], "entities": []}, {"text": "The messages have an average length of 18 tokens, and the corpus vocabulary comprises 468,000 unique words 6 . We obtain labeled gold records using data scraped from the NYC.com music event guide; totaling 110 extracted records.", "labels": [], "entities": [{"text": "NYC.com music event guide", "start_pos": 170, "end_pos": 195, "type": "DATASET", "confidence": 0.9353027939796448}]}, {"text": "Each gold record had two fields of interest: ARTIST and VENUE.", "labels": [], "entities": [{"text": "ARTIST", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9652162194252014}, {"text": "VENUE", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9469366669654846}]}, {"text": "The first weekend of data (messages and events) was used for training and the second two weekends were used for testing.", "labels": [], "entities": []}, {"text": "Preprocessing Only a small fraction of Twitter messages are relevant to the target extraction task.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8597549796104431}, {"text": "target extraction task", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7999477585156759}]}, {"text": "Directly processing the raw unfiltered stream would prohibitively increase computational costs and make learning more difficult due to the noise inherent in the data.", "labels": [], "entities": []}, {"text": "To focus our efforts on the promising portion of the stream, we perform two types of filter- Only considering English tweets and not counting user names ing.", "labels": [], "entities": []}, {"text": "First, we only retain tweets whose authors list some variant of New York as their location in their profile.", "labels": [], "entities": []}, {"text": "Second, we employ a MIRA-based binary classifier () to predict whether a message mentions a concert event.", "labels": [], "entities": [{"text": "MIRA-based", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.7755811810493469}]}, {"text": "After training on 2,000 hand-annotated tweets, this classifier achieves an F 1 of 46.9 (precision of 35.0 and recall of 71.0) when tested on 300 messages.", "labels": [], "entities": [{"text": "F 1", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9921908378601074}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.998731791973114}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9992184638977051}]}, {"text": "While the two-stage filtering does not fully eliminate noise in the input stream, it greatly reduces the presence of irrelevant messages to a manageable 5,800 messages without filtering too many 'signal' tweets.", "labels": [], "entities": []}, {"text": "We also filter our gold record set to include only records in which each field value occurs at least once somewhere in the corpus, as these are the records which are possible to learn given the input.", "labels": [], "entities": [{"text": "gold record set", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.7566008667151133}]}, {"text": "This yields 11 training and 31 testing records.", "labels": [], "entities": []}, {"text": "Training The first weekend of data (2,184 messages and 11 records after preprocessing) is used for training.", "labels": [], "entities": []}, {"text": "As mentioned in Section 4, the only learned parameters in our model are those associated with the sequence labeling factor \u03c6 SEQ . While it is possible to train these parameters via direct annotation of messages with label sequences, we opted instead to use a simple approach where message tokens from the training weekend are labeled via their intersection with gold records, often called \"distant supervision\").", "labels": [], "entities": [{"text": "sequence labeling factor \u03c6 SEQ", "start_pos": 98, "end_pos": 128, "type": "METRIC", "confidence": 0.5411357343196869}]}, {"text": "Concretely, we automatically label message tokens in the training corpus with either the ARTIST or VENUE label if they belonged to a sequence that matched a gold record field, and with NONE otherwise.", "labels": [], "entities": [{"text": "ARTIST", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9935663342475891}, {"text": "VENUE", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9202759861946106}, {"text": "NONE", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9922457337379456}]}, {"text": "This is the only use that is made of the gold records throughout training.", "labels": [], "entities": []}, {"text": "\u03b8 SEQ parameters are trained using this labeling with a standard conditional likelihood objective.", "labels": [], "entities": []}, {"text": "Testing The two weekends of data used for testing totaled 3,662 tweets after preprocessing and 31 gold records for evaluation.", "labels": [], "entities": []}, {"text": "The two weekends were tested separately and their results were aggregated across weekends.", "labels": [], "entities": []}, {"text": "Our model assumes a fixed number of records K = 130.", "labels": [], "entities": []}, {"text": "We rank these records according to a heuristic ranking function that favors the uniqueness of a record's field values across the set and the number of messages in the testing corpus that have Chosen based on the training set Baseline We compare our system against three baselines that employ a voting methodology similar to.", "labels": [], "entities": []}, {"text": "The baselines label each message and then extract one record for each combination of labeled phrases.", "labels": [], "entities": []}, {"text": "Each extraction is considered a vote for that record's existence, and these votes are aggregated across all messages.", "labels": [], "entities": []}, {"text": "Our List Baseline labels messages by finding string overlaps against a list of musical artists and venues scraped from web data (the same lists used as features in our CRF component).", "labels": [], "entities": []}, {"text": "The CRF Baseline is most similar to's CRF Voting method and uses the maximum likelihood CRF labeling of each message.", "labels": [], "entities": []}, {"text": "The Low Threshold Baseline generates all possible records from labelings with a token-level likelihood greater than \u03bb = 0.1.", "labels": [], "entities": []}, {"text": "The output of these baselines is a set of records ranked by the number of votes cast for each, and we perform our evaluation against the top k of these records.", "labels": [], "entities": []}, {"text": "The evaluation of record construction is challenging because many induced music events discussed in Twitter messages are not in our gold data set; our gold records are precise but incomplete.", "labels": [], "entities": []}, {"text": "Because of this, we evaluate recall and precision separately.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9994503855705261}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9980783462524414}]}, {"text": "Both evaluations are performed using hard zero-one loss at record level.", "labels": [], "entities": []}, {"text": "This is a harsh evaluation criterion, but it is realistic for real-world use.", "labels": [], "entities": []}, {"text": "Recall We evaluate recall, shown in, against the gold event records for each weekend.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9993693232536316}]}, {"text": "This shows how well our model could do at replacing the a city event guide, providing Twitter users chat about events taking place.", "labels": [], "entities": []}, {"text": "We perform our evaluation by taking the top k records induced, performing a stable marriage matching against the gold records, and then evaluating the resulting matched pairs.", "labels": [], "entities": []}, {"text": "Stable marriage matching is a widely used approach that finds a bipartite matching between two groups such that no pairing exists in which both participants would prefer some other pairing (.", "labels": [], "entities": [{"text": "Stable marriage matching", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7578782439231873}]}, {"text": "With our hard loss function and no duplicate gold records, this amounts to the standard recall calculation.", "labels": [], "entities": [{"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9989344477653503}]}, {"text": "We choose this bipartite matching technique because it generalizes nicely to allow for other forms of loss calculation (such as token-level loss).", "labels": [], "entities": []}, {"text": "Precision To evaluate precision we assembled a list of the distinct records produced by all models and then manually determined if each record was correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9330133199691772}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9994320273399353}]}, {"text": "This determination was made blind to which model produced the record.", "labels": [], "entities": []}, {"text": "We then used this aggregate list of correct records to measure precision for each individual model, shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9995090961456299}]}, {"text": "By construction, our baselines incorporate a hard constraint that each relation learned must be expressed in entirety in at least one message.", "labels": [], "entities": []}, {"text": "Our model only incorporates a soft version of this constraint via the \u03c6 CON factor, but this constraint clearly has the ability to boost precision.", "labels": [], "entities": [{"text": "\u03c6 CON factor", "start_pos": 70, "end_pos": 82, "type": "METRIC", "confidence": 0.8981784184773763}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9984215497970581}]}, {"text": "To show it's effect, we additionally evaluate our model, labeled Our Work + Con, with this constraint applied in hard form as an output filter.", "labels": [], "entities": []}, {"text": "The downward trend in precision that can be seen in is the effect of our ranking algorithm, which attempts to push garbage collection records towards the bottom of the record list.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9992367029190063}]}, {"text": "As we incorporate these records, precision drops.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995118379592896}]}, {"text": "These lines trend up for two of the baselines because the rank-396 ing heuristic is not as effective for them.", "labels": [], "entities": []}, {"text": "These graphs confirm our hypothesis that we gain significant benefit by intertwining constraints on extraction consistency in the learning process, rather than only using this constraint to filter output.", "labels": [], "entities": []}], "tableCaptions": []}