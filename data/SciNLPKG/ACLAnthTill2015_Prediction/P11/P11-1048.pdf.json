{"title": [{"text": "A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing", "labels": [], "entities": [{"text": "Integrated CCG Supertagging", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.5161190430323283}, {"text": "Parsing", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.5785386562347412}]}], "abstractContent": [{"text": "Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the su-pertagger also prunes many bad parses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.998672604560852}]}, {"text": "Inspired by this analysis, we design a single model with both supertagging and parsing features , rather than separating them into distinct models chained together in a pipeline.", "labels": [], "entities": []}, {"text": "To overcome the resulting increase in complexity , we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.736912801861763}]}, {"text": "On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9773092865943909}, {"text": "F-measure", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.5337622165679932}]}], "introductionContent": [{"text": "Accurate and efficient parsing of Combinatorial Categorial Grammar) is a longstanding problem in computational linguistics, due to the complexities associated its mild context sensitivity.", "labels": [], "entities": [{"text": "parsing of Combinatorial Categorial Grammar", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.715373158454895}]}, {"text": "Even for practical CCG that are strongly context-free, parsing is much harder than with Penn Treebank-style contextfree grammars, with vast numbers of nonterminal categories leading to increased grammar constants.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9685617685317993}, {"text": "Penn Treebank-style contextfree grammars", "start_pos": 88, "end_pos": 128, "type": "DATASET", "confidence": 0.9178154170513153}]}, {"text": "Where atypical Penn Treebank grammar may have fewer than 100 nonterminals), we found that a CCG grammar derived from CCGbank contained over 1500.", "labels": [], "entities": [{"text": "Penn Treebank grammar", "start_pos": 15, "end_pos": 36, "type": "DATASET", "confidence": 0.9825876156489054}]}, {"text": "The same grammar assigns an average of 22 lexical categories per word, resulting in an enormous space of possible derivations.", "labels": [], "entities": []}, {"text": "The most successful approach to CCG parsing is based on a pipeline strategy ( \u00a72).", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8989192843437195}]}, {"text": "First, we tag (or multitag) each word of the sentence with a lexical category using a supertagger, a sequence model over these categories ().", "labels": [], "entities": []}, {"text": "Second, we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger.", "labels": [], "entities": []}, {"text": "Variations on this approach drive the widely-used, broad coverage C&C parser.", "labels": [], "entities": [{"text": "broad coverage C&C parser", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.5315174907445908}]}, {"text": "However, it fails when the supertagger makes errors.", "labels": [], "entities": []}, {"text": "We show experimentally that this pipeline significantly lowers the upper bound on parsing accuracy ( \u00a73).", "labels": [], "entities": [{"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.9357231259346008}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9000684022903442}]}, {"text": "The same experiment shows that the supertagger prunes many bad parses.", "labels": [], "entities": []}, {"text": "So, while we want to avoid the error propagation inherent to a pipeline, ideally we still want to benefit from the key insight of supertagging: that a sequence model over lexical categories can be quite accurate.", "labels": [], "entities": []}, {"text": "Our solution is to combine the features of both the supertagger and the parser into a single, less aggressively pruned model.", "labels": [], "entities": []}, {"text": "The challenge with this model is its prohibitive complexity, which we address with approximate methods: dual decomposition and belief propagation ( \u00a74).", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7920295894145966}, {"text": "belief propagation", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7953397333621979}]}, {"text": "We present the first side-by-side comparison of these algorithms on an NLP task of this complexity, measuring accuracy, convergence behavior, and runtime.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9993768334388733}]}, {"text": "In both cases our model significantly outperforms the pipeline approach, leading to the best published results in CCG parsing ( \u00a75).", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.7498912215232849}]}], "datasetContent": [{"text": "We use the C&C parser and its supertagger).", "labels": [], "entities": []}, {"text": "Our baseline is the hybrid model of; our integrated model simply adds the supertagger features to this model.", "labels": [], "entities": []}, {"text": "The parser relies solely on the supertagger for pruning, using CKY for search over the pruned space.", "labels": [], "entities": []}, {"text": "Training requires repeated calculation of feature expectations over packed charts of derivations.", "labels": [], "entities": []}, {"text": "For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million.", "labels": [], "entities": []}, {"text": "We also used a more permissive training supertagger beam) than in previous work.", "labels": [], "entities": []}, {"text": "Models were trained with the parser's L-BFGS trainer.", "labels": [], "entities": []}, {"text": "We evaluated on CCGbank, a right-most normalform CCG version of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9957896769046783}]}, {"text": "We use sections 02-21 (39603 sentences) for training, section 00 (1913 sentences) for development and section 23 (2407 sentences) for testing.", "labels": [], "entities": []}, {"text": "We supply gold-standard part-of-speech tags to the parsers.", "labels": [], "entities": []}, {"text": "Evaluation is based on labelled and unlabelled predicate argument structure recovery and supertag accuracy.", "labels": [], "entities": [{"text": "predicate argument structure recovery", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.5985128656029701}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9218073487281799}]}, {"text": "We only evaluate on sentences for which an analysis was returned; the coverage for all parsers is 99.22% on section 00, and 99.63% on section 23.", "labels": [], "entities": [{"text": "coverage", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9840714931488037}]}, {"text": "We combine the parser and the supertagger over the search space defined by the set of supertags within the supertagger beam (see Table 1); this avoids having to perform inference over the prohibitively large set of parses spanned by all supertags.", "labels": [], "entities": []}, {"text": "Hence at each beam setting, the model operates over the same search space as the baseline; the difference is that we search with our integrated model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter \u03b2 is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.", "labels": [], "entities": [{"text": "AST", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9078413844108582}, {"text": "Parameter \u03b2", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.9705276489257812}]}, {"text": " Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).", "labels": [], "entities": [{"text": "Reverse", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9501667618751526}, {"text": "CCGbank Section 00", "start_pos": 130, "end_pos": 148, "type": "DATASET", "confidence": 0.985207736492157}, {"text": "F-score (LF)", "start_pos": 179, "end_pos": 191, "type": "METRIC", "confidence": 0.9525365680456161}, {"text": "precision (LP)", "start_pos": 193, "end_pos": 207, "type": "METRIC", "confidence": 0.9522448480129242}, {"text": "recall (LR)", "start_pos": 212, "end_pos": 223, "type": "METRIC", "confidence": 0.9624846130609512}]}, {"text": " Table 3: Beam step function used for training (cf. Table 1).", "labels": [], "entities": []}, {"text": " Table 4: Results for individually-trained submodels combined using dual decomposition (DD) or belief propagation  (BP) for k iterations, evaluated by labelled and unlabelled F-score (LF/UF) and supertag accuracy (ST). We compare  against the previous best result of Clark and Curran", "labels": [], "entities": [{"text": "belief propagation  (BP)", "start_pos": 95, "end_pos": 119, "type": "METRIC", "confidence": 0.6935183942317963}, {"text": "F-score (LF/UF)", "start_pos": 175, "end_pos": 190, "type": "METRIC", "confidence": 0.9205747544765472}, {"text": "accuracy (ST)", "start_pos": 204, "end_pos": 217, "type": "METRIC", "confidence": 0.9276865571737289}]}, {"text": " Table 5: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn  (2010); we evaluate on sentences for which all parsers returned an analysis (2323 sentences for section 23 and 1834  sentences for section 00).", "labels": [], "entities": []}, {"text": " Table 6: Parsing time in seconds per sentence (vs. F- measure) on section 00.", "labels": [], "entities": [{"text": "F- measure", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.979979674021403}]}, {"text": " Table 7: Results of training with SGD on approximate  gradients from LPB on section 00. We test LBP in both  inference and training (train) as well as in inference only  (inf); a maximum number of 10 iterations is used.", "labels": [], "entities": [{"text": "LPB", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9403560757637024}, {"text": "LBP", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9281190037727356}]}]}