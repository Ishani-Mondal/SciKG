{"title": [{"text": "A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Joint Chinese Word Segmentation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.5914936661720276}, {"text": "Part-of-Speech Tagging", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.6862767785787582}]}], "abstractContent": [{"text": "The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.6929411143064499}, {"text": "Part-of-Speech (POS) tagging", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.68217453956604}]}, {"text": "As a result, effective high order features representing rich contexts are inconvenient to use.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel stacked sub-word model for this task, concerning both efficiency and effectiveness.", "labels": [], "entities": []}, {"text": "Our solution is a two step process.", "labels": [], "entities": []}, {"text": "First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information.", "labels": [], "entities": []}, {"text": "Second, the outputs of the three pre-dictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tag-ger.", "labels": [], "entities": []}, {"text": "The coarse-to-fine search scheme is efficient , while in the sub-word tagging step rich contextual features can be approximately derived.", "labels": [], "entities": []}, {"text": "Evaluation on the Penn Chinese Tree-bank shows that our model yields improvements over the best system reported in the literature .", "labels": [], "entities": [{"text": "Penn Chinese Tree-bank", "start_pos": 18, "end_pos": 40, "type": "DATASET", "confidence": 0.9810146888097128}]}], "introductionContent": [{"text": "Word segmentation and part-of-speech (POS) tagging are necessary initial steps for more advanced Chinese language processing tasks, such as parsing and semantic role labeling.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6742459684610367}, {"text": "part-of-speech (POS) tagging", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6790876269340516}, {"text": "parsing", "start_pos": 140, "end_pos": 147, "type": "TASK", "confidence": 0.9672471880912781}, {"text": "semantic role labeling", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.5934471984704336}]}, {"text": "Joint approaches that resolve the two tasks simultaneously have received much attention in recent research.", "labels": [], "entities": []}, {"text": "Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.998007595539093}, {"text": "segmentation error propagation", "start_pos": 109, "end_pos": 139, "type": "TASK", "confidence": 0.8888739148775736}]}, {"text": "A challenge for joint approaches is the large combined search space, which makes efficient decoding and structured learning of parameters very hard.", "labels": [], "entities": []}, {"text": "Moreover, the representation ability of models is limited since using rich contextual word features makes the search intractable.", "labels": [], "entities": []}, {"text": "To overcome such efficiency and effectiveness limitations, the approximate inference and reranking techniques have been explored in previous work.", "labels": [], "entities": []}, {"text": "In this paper, we present an effective and efficient solution for joint Chinese word segmentation and POS tagging.", "labels": [], "entities": [{"text": "joint Chinese word segmentation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.5209835022687912}, {"text": "POS tagging", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.8417593836784363}]}, {"text": "Our work is motivated by several characteristics of this problem.", "labels": [], "entities": []}, {"text": "First of all, a majority of words are easy to identify in the segmentation problem.", "labels": [], "entities": []}, {"text": "For example, a simple maximum matching segmenter can achieve an f-score of about 90.", "labels": [], "entities": [{"text": "f-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9875548481941223}]}, {"text": "We will show that it is possible to improve the efficiency and accuracy by using different strategies for different words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9972302317619324}]}, {"text": "Second, segmenters designed with different views have complementary strength.", "labels": [], "entities": [{"text": "segmenters", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9624325037002563}]}, {"text": "We argue that the agreements and disagreements of different solvers can be used to construct an intermediate sub-word structure for joint segmentation and tagging.", "labels": [], "entities": [{"text": "joint segmentation", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.6364901065826416}]}, {"text": "Since the sub-words are large enough in practice, the decoding for POS tagging over subwords is efficient.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.855272650718689}]}, {"text": "Finally, the Chinese language is characterized by the lack of morphology that often provides important clues for POS tagging, and the POS tags contain much syntactic information, which need context information within a large window for disambiguation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.8133020102977753}]}, {"text": "For example, showed the effectiveness of utilizing syntactic information to rerank POS tagging results.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.6790923774242401}]}, {"text": "As a result, the capability to represent rich contextual features is crucial to a POS tagger.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 82, "end_pos": 92, "type": "TASK", "confidence": 0.6015652418136597}]}, {"text": "In this work, we use a representation-efficiency tradeoff through stacked learning, away of approximating rich non-local fea-1385 tures.", "labels": [], "entities": []}, {"text": "This paper describes a novel stacked sub-word model.", "labels": [], "entities": []}, {"text": "Given multiple word segmentations of one sentence, we formally define a sub-word structure that maximizes the agreement of non-word-break positions.", "labels": [], "entities": []}, {"text": "Based on the sub-word structure, joint word segmentation and POS tagging is addressed as a two step process.", "labels": [], "entities": [{"text": "joint word segmentation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6679135660330454}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7691198289394379}]}, {"text": "In the first step, one word-based segmenter, one character-based segmenter and one local character classifier are used to produce coarse segmentation and POS information.", "labels": [], "entities": []}, {"text": "The results of the three predictors are then merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger.", "labels": [], "entities": []}, {"text": "If a string is consistently segmented as a word by the three segmenters, it will be a correct word prediction with a very high probability.", "labels": [], "entities": []}, {"text": "In the sub-word tagging phase, the fine-grained tagger mainly considers its POS tag prediction problem.", "labels": [], "entities": [{"text": "POS tag prediction", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7565491398175558}]}, {"text": "For the words that are not consistently predicted, the fine-grained tagger will also consider their bracketing problem.", "labels": [], "entities": []}, {"text": "The coarse-to-fine scheme significantly improves the efficiency of decoding.", "labels": [], "entities": []}, {"text": "Furthermore, in the sub-word tagging step, word features in a large window can be approximately derived from the coarse segmentation and tagging results.", "labels": [], "entities": []}, {"text": "To train a good sub-word tagger, we use the stacked learning technique, which can effectively correct the training/test mismatch problem.", "labels": [], "entities": [{"text": "sub-word tagger", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.8125457167625427}]}, {"text": "We conduct our experiments on the Penn Chinese Treebank and compare our system with the stateof-the-art systems.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.9809941649436951}]}, {"text": "Our system achieves an f-score of 98.17 for the word segmentation task and an f-score of 94.02 for the whole task, resulting in relative error reductions of 14.1% and 5.5% respectively over the best system reported in the literature.", "labels": [], "entities": [{"text": "f-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.992939829826355}, {"text": "word segmentation task", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.8238871494928995}, {"text": "f-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9949722290039062}, {"text": "error", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.8166571259498596}]}, {"text": "The remaining part of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief introduction to the problem and reviews the relevant previous research.", "labels": [], "entities": []}, {"text": "Section 3 describes the details of our method.", "labels": [], "entities": []}, {"text": "Section 4 presents experimental results and empirical analyses.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Training, development and test data on CTB 5.0", "labels": [], "entities": [{"text": "CTB 5.0", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9077600538730621}]}, {"text": " Table 3: Performance of the coarse-grained solvers on the  development data.", "labels": [], "entities": [{"text": "solvers", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.8421763777732849}]}, {"text": " Table 5: Performance of the stacked sub-word model  (K = 5) with features in different window sizes.", "labels": [], "entities": []}, {"text": " Table 6: Performance on the development data. No stack- ing and different folds of cross-validation are separately  applied.", "labels": [], "entities": [{"text": "stack- ing", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.795242170492808}]}, {"text": " Table 7: F-score performance on the test data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9988094568252563}]}]}