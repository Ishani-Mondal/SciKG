{"title": [{"text": "Semi-Supervised Frame-Semantic Parsing for Unknown Predicates", "labels": [], "entities": [{"text": "Frame-Semantic Parsing", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8260384500026703}]}], "abstractContent": [{"text": "We describe anew approach to disambiguat-ing semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data.", "labels": [], "entities": []}, {"text": "Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework.", "labels": [], "entities": []}, {"text": "We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones.", "labels": [], "entities": []}, {"text": "The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F 1 score on a blind test set, over a state-of-the-art supervised baseline.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 139, "end_pos": 159, "type": "TASK", "confidence": 0.682482972741127}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.8988938927650452}, {"text": "frame-semantic parsing F 1 score", "start_pos": 211, "end_pos": 243, "type": "METRIC", "confidence": 0.7208173394203186}]}], "introductionContent": [{"text": "Frame-semantic parsing aims to extract a shallow semantic structure from text, as shown in.", "labels": [], "entities": [{"text": "Frame-semantic parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8670870065689087}]}, {"text": "The FrameNet lexicon () is a rich linguistic resource containing expert knowledge about lexical and predicate-argument semantics.", "labels": [], "entities": []}, {"text": "The lexicon suggests an analysis based on the theory of frame semantics.", "labels": [], "entities": []}, {"text": "Recent approaches to frame-semantic parsing have broadly focused on the use of two statistical classifiers corresponding to the aforementioned subtasks: the first one to identify the most suitable semantic frame fora marked lexical predicate (target, henceforth) in a sentence, and the second for performing semantic role labeling (SRL) given the frame.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.716378316283226}, {"text": "semantic role labeling (SRL)", "start_pos": 308, "end_pos": 336, "type": "TASK", "confidence": 0.774466281135877}]}, {"text": "The FrameNet lexicon, its exemplar sentences containing instantiations of semantic frames, and full-text annotations provide supervision for learning frame-semantic parsers.", "labels": [], "entities": [{"text": "FrameNet lexicon", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8671532273292542}]}, {"text": "Yet these annotations lack coverage, including only 9,300 annotated target types.", "labels": [], "entities": [{"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9805746078491211}]}, {"text": "Recent papers have tried to address the coverage problem.", "labels": [], "entities": [{"text": "coverage", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.5676699280738831}]}, {"text": "used WordNet to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.9742226004600525}]}, {"text": "In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets ().", "labels": [], "entities": []}, {"text": "Relatedly, for the argument identification subtask, proposed a technique for generalization of semantic roles to overcome data sparseness.", "labels": [], "entities": [{"text": "argument identification subtask", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7818483312924703}]}, {"text": "Unseen targets continue to present a major obstacle to domain-general semantic analysis.", "labels": [], "entities": [{"text": "domain-general semantic analysis", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.6904679238796234}]}, {"text": "In this paper, we address the problem of idenfifying the semantic frames for targets unseen either in FrameNet (including the exemplar sentences) or the collection of full-text annotations released along with the lexicon.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.8887302279472351}]}, {"text": "Using a standard model for the argument identification stage (), our proposed method improves overall frame-semantic parsing, especially for unseen targets.", "labels": [], "entities": [{"text": "argument identification stage", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.7792482872804006}, {"text": "frame-semantic parsing", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.6772210150957108}]}, {"text": "To better handle these unseen targets, we adopt a graph-based semi-supervised learning stategy ( \u00a74).", "labels": [], "entities": []}, {"text": "We construct a large graph over potential targets, most of which: An example sentence from the PropBank section of the full-text annotations released as part of FrameNet 1.5.", "labels": [], "entities": []}, {"text": "Each row under the sentence correponds to a semantic frame and its set of corresponding arguments.", "labels": [], "entities": []}, {"text": "Thick lines indicate targets that evoke frames; thin solid/dotted lines with labels indicate arguments.", "labels": [], "entities": []}, {"text": "N m under \"bells\" is short for the Noise maker role of the NOISE MAKERS frame. are drawn from unannotated data, and a fraction of which come from seen FrameNet annotations.", "labels": [], "entities": [{"text": "NOISE MAKERS frame.", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.7314260800679525}]}, {"text": "Next, we perform label propagation on the graph, which is initialized by frame distributions over the seen targets.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7948038578033447}]}, {"text": "The resulting smoothed graph consists of posterior distributions over semantic frames for each target in the graph, thus increasing coverage.", "labels": [], "entities": []}, {"text": "These distributions are then evaluated within a frame-semantic parser ( \u00a75).", "labels": [], "entities": []}, {"text": "Considering unseen targets in test data (although few because the test data is also drawn from the training domain), significant absolute improvements of 15.7% and 13.7% are observed for frame identification and full framesemantic parsing, respectively, indicating improved coverage for hitherto unobserved predicates ( \u00a76).", "labels": [], "entities": [{"text": "frame identification", "start_pos": 187, "end_pos": 207, "type": "TASK", "confidence": 0.7581668496131897}, {"text": "full framesemantic parsing", "start_pos": 212, "end_pos": 238, "type": "TASK", "confidence": 0.6051705280939738}]}], "datasetContent": [{"text": "Before presenting our experiments and results, we will describe the datasets used in our experiments, and the various baseline models considered.", "labels": [], "entities": []}, {"text": "We used five-fold cross-validation to tune the hyperparameters \u03b1, K, \u00b5, and Min our model.", "labels": [], "entities": []}, {"text": "The  uniform regularization hyperparameter \u03bd for graph construction was set to 10 \u22126 and not tuned.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7359052300453186}]}, {"text": "For each cross-validation split, four folds were used to train a frame identification model, construct a graph, run label propagation and then the model was tested on the fifth fold.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.6883159875869751}, {"text": "label propagation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7021204829216003}]}, {"text": "This was done for all hyperparameter settings, which were \u03b1 \u2208 {0.2, 0.5, 0.8}, K \u2208 {5, 10, 15, 20}, \u00b5 \u2208 {0.01, 0.1, 0.3, 0.5, 1.0} and M \u2208 {2, 3, 5, 10}.", "labels": [], "entities": []}, {"text": "The joint setting which performed the best across five-folds was \u03b1 = 0.2, K = 10, \u00b5 = 1.0, M = 2.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.963815450668335}, {"text": "\u00b5", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9871290922164917}, {"text": "M", "start_pos": 91, "end_pos": 92, "type": "METRIC", "confidence": 0.9873517751693726}]}, {"text": "Similar tuning was also done for the baseline LinGraph, where \u03b1 was set to 0, and rest of the hyperparameters were tuned (the selected hyperparameters were K = 10, \u00b5 = 0.1 and M = 2).", "labels": [], "entities": [{"text": "LinGraph", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.960555374622345}]}, {"text": "With the chosen set of hyperparameters, the test set was used to measure final performance.", "labels": [], "entities": []}, {"text": "The standard evaluation script from the SemEval'07 task calculates precision, recall, and F 1 -score for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one in the FrameNet lexicon.", "labels": [], "entities": [{"text": "SemEval'07 task", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.8486095070838928}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9995083808898926}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9989675283432007}, {"text": "F 1 -score", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9909017831087112}, {"text": "FrameNet lexicon", "start_pos": 238, "end_pos": 254, "type": "DATASET", "confidence": 0.8969655632972717}]}, {"text": "We present precision, recall, and F 1 -measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.999419093132019}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995051622390747}, {"text": "F 1 -measure microaveraged", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.979502511024475}]}, {"text": "Statistical significance is measured using a reimplementation of Dan Bikel's parsing evaluation comparator.", "labels": [], "entities": []}, {"text": "present results for frame identification and full frame-semantic parsing respectively.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8211072385311127}, {"text": "full frame-semantic parsing", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6517651379108429}]}, {"text": "They also separately tabulate the results achieved for unknown targets.", "labels": [], "entities": []}, {"text": "Our full model, denoted by \"FullGraph,\" outperforms all the baselines for both tasks.", "labels": [], "entities": []}, {"text": "Note that the Self-training model even falls short of the supervised baseline SEMAFOR, unlike what was observed by Bejan (2009) for the frame identification task.", "labels": [], "entities": [{"text": "SEMAFOR", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.8200221657752991}, {"text": "frame identification task", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.8344479004542033}]}, {"text": "The model using a graph constructed solely from the thesaurus (LinGraph) outperforms both the supervised and the self-training baselines for all tasks, but falls short of the graph constructed using the similarity metric that is a linear combination of distributional similarity and supervised frame similarity.", "labels": [], "entities": []}, {"text": "This indicates that a graph constructed with some knowledge of the supervised data is more powerful.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frame identification results in percentage accu- racy on 4,458 test targets. Bold scores indicate significant  improvements relative to SEMAFOR and (  *  ) denotes sig- nificant improvements over LinGraph (p < 0.05).", "labels": [], "entities": [{"text": "Frame identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7454121708869934}, {"text": "accu- racy", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.8800641298294067}, {"text": "SEMAFOR", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.5501266121864319}, {"text": "LinGraph", "start_pos": 206, "end_pos": 214, "type": "DATASET", "confidence": 0.8642722964286804}]}, {"text": " Table 2: Full frame-semantic parsing precision, recall and F 1 score on 2,420 test sentences. Bold scores indicate  significant improvements relative to SEMAFOR and (  *  ) denotes significant improvements over LinGraph (p < 0.05).", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9855849146842957}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9997250437736511}, {"text": "F 1 score", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.989228347937266}, {"text": "SEMAFOR", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.5867050290107727}, {"text": "LinGraph", "start_pos": 212, "end_pos": 220, "type": "DATASET", "confidence": 0.9001401662826538}]}]}