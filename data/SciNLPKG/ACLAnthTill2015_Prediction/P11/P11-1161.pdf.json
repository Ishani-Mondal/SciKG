{"title": [{"text": "Ranking Class Labels Using Query Sessions", "labels": [], "entities": []}], "abstractContent": [{"text": "The role of search queries, as available within query sessions or in isolation from one another , in examined in the context of ranking the class labels (e.g., brazilian cities, business centers, hilly sites) extracted from Web documents for various instances (e.g., rio de janeiro).", "labels": [], "entities": []}, {"text": "The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance.", "labels": [], "entities": []}, {"text": "Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using document-based counts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Motivation: The offline acquisition of instances (rio de janeiro, porsche cayman) and their corresponding class labels (brazilian cities, locations, vehicles, sports cars) from text has been an active area of research.", "labels": [], "entities": []}, {"text": "In order to extract fine-grained classes of instances, existing methods often apply manuallycreated ( or automatically-learned () extraction patterns to text within large document collections.", "labels": [], "entities": []}, {"text": "In Web search, the relative ranking of documents returned in response to a query directly affects the outcome of the search.", "labels": [], "entities": []}, {"text": "Similarly, the quality of the relative ranking among class labels extracted fora given instance influences any applications (e.g., query refinements or structured extraction) using the extracted data.", "labels": [], "entities": []}, {"text": "But due to noise in Web data and limitations of extraction techniques, class labels acquired fora given instance (e.g., oil shale) may fail to properly capture the semantic classes to which the instance may belong (.", "labels": [], "entities": []}, {"text": "Inevitably, some of the extracted class labels will be less useful (e.g., sources, mutual concerns) or incorrect (e.g., plants for the instance oil shale).", "labels": [], "entities": []}, {"text": "In previous work, the relative ranking of class labels for an instance is determined mostly based on features derived from the source Web documents from which the data has been extracted, such as variations of the frequency of co-occurrence or diversity of extraction patterns producing a given pair ().", "labels": [], "entities": []}, {"text": "Contributions: This paper explores the role of Web search queries, rather than Web documents, in inducing superior ranking among class labels extracted automatically from documents for various instances.", "labels": [], "entities": []}, {"text": "It compares two sources of indirect ranking evidence available within anonymized query logs: a) co-occurrence of an instance and its class label in the same query; and b) co-occurrence of an instance and its class label, as separate queries within the same query session.", "labels": [], "entities": []}, {"text": "The former source is a noisy attempt to capture queries that narrow the search results to a particular class of the instance (e.g., jaguar car maker).", "labels": [], "entities": []}, {"text": "In comparison, the latter source noisily identifies searches that specialize from a class (e.g., car maker) to an instance (e.g., jaguar) or, conversely, generalize from an instance to a class.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first study comparing inherently-noisy queries and query sessions for the purpose of ranking of open-domain, labeled class instances.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces intuitions behind an approach using queries for ranking class labels of various instances, and describes associated ranking functions.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 describe the experimental setting and evaluation results over evaluation sets of instances associated with Web search queries.", "labels": [], "entities": []}, {"text": "The results illustrate the higher quality of the querybased, re-ranked lists of class labels, relative to alternative ranking methods using only document-based counts.", "labels": [], "entities": []}], "datasetContent": [{"text": "Textual Data Sources: The acquisition of the IsA repository relies on unstructured text available within Web documents and search queries.", "labels": [], "entities": []}, {"text": "The queries are fully-anonymized queries in English submitted to Google by Web users in 2009, and are available in two collections.", "labels": [], "entities": []}, {"text": "The first collection is a random sample of 50 million unique queries that are independent from one another.", "labels": [], "entities": []}, {"text": "The second collection is a random sample of 5 million query sessions.", "labels": [], "entities": []}, {"text": "Each session has an initial query and a series of subsequent queries.", "labels": [], "entities": []}, {"text": "A subsequent query is a query that has been submitted by the same Web user within no longer than a few minutes after the initial query.", "labels": [], "entities": []}, {"text": "Each subsequent query is accompanied by its frequency of occurrence in the session, with the corresponding initial query.", "labels": [], "entities": []}, {"text": "The document collection consists of a sample of 100 million documents in English.", "labels": [], "entities": []}, {"text": "Experimental Runs: The experimental runs correspond to different methods for extracting and ranking pairs of an instance and a class: \u2022 from the repository extracted here, with class labels of an instance ranked based on the frequency and the number of extraction patterns (Score H1 from Equation (1) in Section 2), in run Rd ; \u2022 from the repository extracted here, with class labels of an instance ranked via the rank-based merging of: Score H1+H2 from Section 2, in run R p , which corresponds to re-ranking using cooccurrence of an instance and its class label in the same query; Score H1+H3 from Section 2, in run R s , which corresponds to re-ranking using cooccurrence of an instance and its class label, as separate queries within the same query session; and Score H1+H2+H3 from Section 2, in run Ru , which corresponds to re-ranking using both types of cooccurrences in queries.", "labels": [], "entities": []}, {"text": "Evaluation Procedure: The manual evaluation of open-domain information extraction output is time consuming.", "labels": [], "entities": [{"text": "open-domain information extraction output", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.7004291787743568}]}, {"text": "A more practical alternative is an automatic evaluation procedure for ranked lists of class labels, based on existing resources and systems.", "labels": [], "entities": []}, {"text": "Assume that there is a gold standard, containing gold class labels that are each associated with a gold set of their instances.", "labels": [], "entities": []}, {"text": "The creation of such gold standards is discussed later.", "labels": [], "entities": []}, {"text": "Based on the gold standard, the ranked lists of class labels available within an IsA repository can be automatically evaluated as follows.", "labels": [], "entities": []}, {"text": "First, for each gold label, the ranked lists of class labels of individual gold instances are retrieved from the IsA repository.", "labels": [], "entities": [{"text": "IsA repository", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.7850519716739655}]}, {"text": "Second, the individual retrieved lists are merged into a ranked list of class labels, associated with the gold label.", "labels": [], "entities": []}, {"text": "The merged list can be computed, e.g., using an extension of the Score H1+H2+H3 formula (Equation (4)) described earlier in Section 2.", "labels": [], "entities": []}, {"text": "Third, the merged list is compared against the gold label, to estimate the accuracy of the merged list.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9994045495986938}]}, {"text": "Intuitively, a ranked list of class labels is a better approximation of a gold label, if class labels situated at better ranks in the list are closer in meaning to the gold label.", "labels": [], "entities": []}, {"text": "Evaluation Metric: Given a gold label and a list of class labels, if any, derived from the IsA repository, the rank of the highest class label that matches the gold label determines the score assigned to the gold label, in the form of the reciprocal rank of the match.", "labels": [], "entities": [{"text": "IsA repository", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.7505142092704773}]}, {"text": "Thus, if the gold label matches a class label at rank 1, 2 or 3 in the computed list, the gold label receives a score of 1, 0.5 or 0.33 respectively.", "labels": [], "entities": []}, {"text": "The score is 0 if the gold label does not match any of the top 20 class labels.", "labels": [], "entities": []}, {"text": "The overall score over the entire set of gold labels is the mean reciprocal rank (MRR) score overall gold labels from the set.", "labels": [], "entities": [{"text": "reciprocal rank (MRR) score", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.9109353721141815}]}, {"text": "Two types of MRR scores are automatically computed: \u2022 MRR f considers a gold label and a class label to match, if they are identical; \u2022 MRR p considers a gold label and a class label to match, if one or more of their tokens that are not stop words are identical.", "labels": [], "entities": [{"text": "MRR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9738965034484863}]}, {"text": "During matching, all string comparisons are caseinsensitive, and all tokens are first converted to their singular form (e.g., european countries to european country) using WordNet: Size and composition of evaluation sets of queries associated with non-filtered (Q e ) or manuallyfiltered (Q m ) instances sidered to not match in MRR f scores, but match in MRR p scores.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 172, "end_pos": 179, "type": "DATASET", "confidence": 0.9619984030723572}, {"text": "MRR f scores", "start_pos": 329, "end_pos": 341, "type": "METRIC", "confidence": 0.7506399552027384}, {"text": "MRR p scores", "start_pos": 356, "end_pos": 368, "type": "METRIC", "confidence": 0.8190988103548685}]}, {"text": "On the other hand, MRR p scores may give credit to less relevant class labels, such as insurance policies for the gold label insurance carriers.", "labels": [], "entities": [{"text": "MRR p scores", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.8011470635732015}]}, {"text": "Therefore, MRR p is an optimistic, and MRR f is a pessimistic estimate of the actual usefulness of the computed ranked lists of class labels as approximations of the gold labels.", "labels": [], "entities": [{"text": "MRR p", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9527194797992706}, {"text": "MRR f", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9792632758617401}]}, {"text": "IsA Repository: The IsA repository, extracted from the document collection, covers a total of 4.04 million instances associated with 7.65 million class labels.", "labels": [], "entities": []}, {"text": "The number of class labels available per instance and vice-versa follows a long-tail distribution, indicating that 2.12 million of the instances each have two or more class labels (with an average of 19.72 class labels per instance).", "labels": [], "entities": []}, {"text": "Evaluation Sets of Queries:  is obtained from a random sample of anonymized, class-seeking queries submitted by Web users to Google Squared.", "labels": [], "entities": []}, {"text": "The set contains 807 queries, each associated with a ranked list of between 10 and 100 gold instances automatically extracted by Google Squared.", "labels": [], "entities": []}, {"text": "Since the gold instances available as input for each query as part of Q e are automatically extracted, they mayor may not be true instances of the respective queries.", "labels": [], "entities": []}, {"text": "As described in, the second evaluation set Q m is a subset of 40 queries from Q e , such that the gold instances available for each query in Q mare found to be correct after manual inspection.", "labels": [], "entities": []}, {"text": "The 40 queries from Q mare associated with between 8 and 33 human-validated instances.", "labels": [], "entities": []}, {"text": "As shown in the upper part of, the queries from Q e are up to 8 tokens in length, with an average of 2 tokens per query.", "labels": [], "entities": []}, {"text": "Queries from Q mare comparatively shorter, both in maximum (3 tokens) and average (1.4 tokens) length.", "labels": [], "entities": [{"text": "average (1.4 tokens) length", "start_pos": 74, "end_pos": 101, "type": "METRIC", "confidence": 0.8514745434125265}]}, {"text": "The lower part of Table 2 shows the number of gold instances available as input, which average around 70 and 17 per query, for queries from Q e and Q m respectively.", "labels": [], "entities": []}, {"text": "To provide another view on the distribution of the queries from evaluation sets, lists tokens that are not stop words, which occur inmost queries from Q e . Comparatively, few query tokens occur in more than one query in Q m . Evaluation Procedure: Following the general evaluation procedure, each query from the sets Q e and Q m acts as a gold class label associated with the corresponding set of instances.", "labels": [], "entities": []}, {"text": "Given a query and its instances I from the evaluation sets Q e or Q m , a merged, ranked lists of class labels is computed out of the ranked lists of class labels available in the summarizes results from comparative experiments, quantifying a) horizontally, the impact of alternative parameter settings on the computed lists of class labels; and b) vertically, the comparative accuracy of the experimental runs over the query sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 377, "end_pos": 385, "type": "METRIC", "confidence": 0.9708979725837708}]}, {"text": "The experimental parameters are the number of input instances from the evaluation sets that are used for retrieving class labels, I-per-Q, set to 3, 5, 10; and the number of class labels retrieved per input instance, C-per-I, set to 5, 10, 20.", "labels": [], "entities": []}, {"text": "Four conclusions can be derived from the results.", "labels": [], "entities": []}, {"text": "First, the scores over Q mare higher than those over Q e , confirming the intuition that the higher-quality: Accuracy of instance set labeling, as full-match (MRR f ) or partial-match (MRR p ) scores over the evaluation sets of queries associated with non-filtered instances (Q e ) or manually-filtered instances (Q m ), for various experimental runs (I-per-Q=number of gold instances available in the input evaluation sets that are used for retrieving class labels; C-per-I=number of class labels retrieved from IsA repository per input instance) input set of instances available in Q m relative to Q e should lead to higher-quality class labels for the corresponding queries.", "labels": [], "entities": [{"text": "full-match (MRR f ) or partial-match (MRR p )", "start_pos": 147, "end_pos": 192, "type": "METRIC", "confidence": 0.6952701265161688}]}, {"text": "Second, when I-per-Q is fixed, increasing C-per-I leads to small, if any, score improvements.", "labels": [], "entities": [{"text": "I-per-Q", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9900418519973755}]}, {"text": "Third, when C-per-I is fixed, even small values of I-per-Q, such as 3 (that is, very small sets of instances provided as input) produce scores that are competitive with those obtained with a higher value like 10.", "labels": [], "entities": []}, {"text": "This suggests that useful class labels can be generated even in extreme scenarios, where the number of instances available as input is as small as 3 or 5.", "labels": [], "entities": []}, {"text": "Fourth and most importantly, for most combinations of parameter settings and on both query sets, the runs that take advantage of query logs (R p , R s , Ru ) produce the highest scores.", "labels": [], "entities": []}, {"text": "In particular, when I-per-Q is set to 10 and C-per-I to 20, run Ru identifies the original query as an exact match among the top three to four class labels returned (score 0.278); and as a partial match among the top one to two class labels returned (score 0.636), as an average over the Q e set.", "labels": [], "entities": []}, {"text": "The corresponding MRR f score of 0.278 over the Q e set obtained with run Ru is 27% higher than with run Rd . In all experiments, the higher scores of R p , R sand Ru can be attributed to higher-quality lists of class labels, relative to Rd . Among combinations of parameter settings described in, values around 10 for I-per-Q and 20 for C-per-I give the highest scores over both Q e and Q m . Among the query-based runs R p , R sand Ru , the highest scores in are obtained mostly for run R s . Thus, between the presence of a class label and an instance either in the same query, or as separate queries within the same query session, it is the latter that provides a more useful signal during the reranking of class labels of each instance.", "labels": [], "entities": [{"text": "MRR f score", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9603835344314575}]}, {"text": "illustrates the top class labels from the ranked lists generated in run R s for various queries from both Q e and Q m . The table suggests that the computed class labels are relatively resistant to noise and variation within the input set of gold instances.", "labels": [], "entities": []}, {"text": "For example, the top elements of the lists of class la-: Examples of gold instances available in the input, and actual ranked lists of class labels produced by run R s for various queries from the evaluation sets of queries associated with non-filtered gold instances (Q e ) or manually-filtered gold instances (Q m ) bels generated for computer languages are relevant and also quite similar for Q e vs. Q m , although the list of gold instances in Q e may contain incorrect items (e.g., acm transactions on mathematical software).", "labels": [], "entities": []}, {"text": "Similarly, the class labels computed for european countries are almost the same for Q e vs. Q m , although the overlap of the respective lists of 10 gold instances used as input is not large.", "labels": [], "entities": []}, {"text": "The table shows at least one query (park slope restaurants) for which the output is less than optimal, either because the class labels (e.g., businesses) are quite distant semantically from the query (for Q e ), or because no output is produced at all, due to no class labels being found in the IsA repository for any of the 10 input gold instances (for Q m ).", "labels": [], "entities": []}, {"text": "For many queries, however, the computed class labels arguably capture the meaning of the original query, although not necessarily in the exact same lexical form, and sometimes only partially.", "labels": [], "entities": []}, {"text": "For example, for the query endangered animals, only the fourth class label from Q m identifies the query exactly.", "labels": [], "entities": []}, {"text": "However, class labels preceding endangered animals already capture the notion of animals or species (first and third labels), or that they are endangered (second label).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of gold instances (upper part) and num- ber of query tokens (lower part) available per query, over  the evaluation sets of queries associated with non-filtered  gold instances (Q e ) or manually-filtered gold instances  (Q m )", "labels": [], "entities": []}, {"text": " Table 3: Query tokens occurring most frequently in  queries from the Q e evaluation set, along with the number  (Cnt) and examples of queries containing the tokens", "labels": [], "entities": []}, {"text": " Table 4: Accuracy of instance set labeling, as full-match (MRR f ) or partial-match (MRR p ) scores over the evaluation  sets of queries associated with non-filtered instances (Q e ) or manually-filtered instances (Q m ), for various experi- mental runs (I-per-Q=number of gold instances available in the input evaluation sets that are used for retrieving class  labels; C-per-I=number of class labels retrieved from IsA repository per input instance)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9655181169509888}, {"text": "partial-match (MRR p )", "start_pos": 71, "end_pos": 93, "type": "METRIC", "confidence": 0.7184536457061768}]}, {"text": " Table 5: Examples of gold instances available in the input, and actual ranked lists of class labels produced by run R s for  various queries from the evaluation sets of queries associated with non-filtered gold instances (Q e ) or manually-filtered  gold instances (Q m )", "labels": [], "entities": []}]}