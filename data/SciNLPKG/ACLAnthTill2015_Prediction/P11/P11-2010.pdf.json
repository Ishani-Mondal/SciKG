{"title": [{"text": "Latent Class Transliteration based on Source Language Origin", "labels": [], "entities": []}], "abstractContent": [{"text": "Transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic-or spelling-based models.", "labels": [], "entities": []}, {"text": "However , a single model cannot deal with different words from different language origins, e.g., \"get\" in \"piaget\" and \"target.\"", "labels": [], "entities": []}, {"text": "(2007) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly.", "labels": [], "entities": []}, {"text": "This model, however, requires an explicitly tagged training set with language origins.", "labels": [], "entities": []}, {"text": "We propose a novel method which models language origins as latent classes.", "labels": [], "entities": []}, {"text": "The parameters are learned from a set of translit-erated word pairs via the EM algorithm.", "labels": [], "entities": []}, {"text": "The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes.", "labels": [], "entities": [{"text": "transliteration task of Western names to Japanese", "start_pos": 32, "end_pos": 81, "type": "TASK", "confidence": 0.8514345373426165}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9986467957496643}]}], "introductionContent": [{"text": "Transliteration (e.g., \" baraku obama / Barak Obama\") is phonetic translation between languages with different writing systems.", "labels": [], "entities": []}, {"text": "Words are often transliterated when imported into differet languages, which is a major cause of spelling variations of proper nouns in Japanese and many other languages.", "labels": [], "entities": []}, {"text": "Accurate transliteration is also the key to robust machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7112969756126404}]}, {"text": "Phonetic-based rewriting models) and spelling-based supervised models) have been proposed for recognizing word-to-word transliteration correspondence.", "labels": [], "entities": [{"text": "recognizing word-to-word transliteration correspondence", "start_pos": 94, "end_pos": 149, "type": "TASK", "confidence": 0.8528744131326675}]}, {"text": "These methods usually learn a single model given a training set.", "labels": [], "entities": []}, {"text": "However, single models cannot deal with words from multiple language origins.", "labels": [], "entities": []}, {"text": "For example, the \"get\" parts in \"piaget / piaje\" (French origin) and \"target / t\u00af agetto\" (English origin) may differ in how they are transliterated depending on their origins.", "labels": [], "entities": []}, {"text": "tackled this issue by proposing a class transliteration model, which explicitly models and classifies origins such as language and genders, and switches corresponding transliteration model.", "labels": [], "entities": []}, {"text": "This method requires training sets of transliterated word pairs with language origin.", "labels": [], "entities": []}, {"text": "However, it is difficult to obtain such tagged data, especially for proper nouns, a rich source of transliterated words.", "labels": [], "entities": []}, {"text": "In addition, the explicitly tagged language origins are not necessarily helpful for loanwords.", "labels": [], "entities": []}, {"text": "For example, the word \"spaghetti\" (Italian origin) can also be found in an English dictionary, but applying an English model can lead to unwanted results.", "labels": [], "entities": []}, {"text": "In this paper, we propose a latent class transliteration model, which models the source language origin as unobservable latent classes and applies appropriate transliteration models to given transliteration pairs.", "labels": [], "entities": []}, {"text": "The model parameters are learned via the EM algorithm from training sets of transliterated pairs.", "labels": [], "entities": []}, {"text": "We expect that, for example, a latent class which is mostly occupied by Italian words would be assigned to \"spaghetti / supageti\" and the pair will be correctly recognized.", "labels": [], "entities": []}, {"text": "In the evaluation experiments, we evaluated the accuracy in estimating a corresponding Japanese transliteration given an unknown foreign word, 53 using lists of Western names with mixed languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993312358856201}]}, {"text": "The results showed that the proposed model achieves higher accuracy than conventional models without latent classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9984682202339172}]}, {"text": "Related researches include, where it is shown that source language origins may improve the pronunciation of proper nouns in text-to-speech systems.", "labels": [], "entities": []}, {"text": "Another one by estimates character-based error probabilities from query logs via the EM algorithm.", "labels": [], "entities": []}, {"text": "This model is less general than ours because it only deals with character-based error probability.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we evaluate the performance of the transliteration models as an information retrieval task, where the model ranks target t fora given source s , based on the model P (t |s ).", "labels": [], "entities": [{"text": "information retrieval task", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.794624129931132}]}, {"text": "We used all the tn in the test set X test = {(s n , tn )|1 \u2264 n \u2264 M } as target candidates and s n for queries.", "labels": [], "entities": []}, {"text": "Five-fold cross validation was adopted when learning the models, that is, the datasets described in the next subsections are equally splitted into five folds, of which four were used for training and one for testing.", "labels": [], "entities": []}, {"text": "The mean reciprocal rank (MRR) of top 10 ranked candidates was used as a performance measure.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.8276834338903427}]}, {"text": "Dataset 1: Western Person Name List This dataset contains 6,717 Western person names and their Katakana readings taken from an European name website 1 , consisting of German (de), English (en), and French (fr) person name pairs.", "labels": [], "entities": [{"text": "Western Person Name List", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.8681257963180542}]}, {"text": "The numbers of pairs for these languages are 2,470, 2,492, and 1,747, respectively.", "labels": [], "entities": []}, {"text": "Accent marks for non-English languages were left untouched.", "labels": [], "entities": [{"text": "Accent", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9896508455276489}]}, {"text": "Uppercase was normalized to lowercase.", "labels": [], "entities": []}, {"text": "Linked English and Japanese titles are extracted, unless the Japanese title contains any other characters than Katakana, hyphen, or middle dot.", "labels": [], "entities": []}, {"text": "The language origin of titles were detected whether appropriate country names are included in the first sentence of Japanese articles.", "labels": [], "entities": []}, {"text": "If they contain \" (of Germany),\" \" (of France),\" \" (of Italy),\" they are marked as German, French, and Italian origin, respectively.", "labels": [], "entities": []}, {"text": "If the sentence contains any of Spain, Argentina, Mexico, Peru, or Chile plus \"\"(of), it is marked as Spanish origin.", "labels": [], "entities": []}, {"text": "If they contain any of America, England, Australia or Canada plus \"\"(of), it is marked as English origin.", "labels": [], "entities": []}, {"text": "The latter parts of Japanese/foreign titles starting from \",\" or \"(\" were removed.", "labels": [], "entities": []}, {"text": "Japanese and foreign titles were split into chunks by middle dots and \" \", respectively, and resulting chunks were aligned.", "labels": [], "entities": []}, {"text": "Titles pairs with different numbers of chunks, or ones with foreign character length less than 3 were excluded.", "labels": [], "entities": []}, {"text": "All accent marks were normalized (German \"\u00df\" was converted to \"ss\").", "labels": [], "entities": []}, {"text": "Implementation Details P (c|s) of the class transliteration model was calculated by a character 3-gram language model with Witten-Bell discounting.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9030597805976868}]}, {"text": "Japanese Katakanas were all converted to Hepburn-style Roman characters, with minor changes so as to incorporate foreign pronunciations such as \"wi / \" and \"we / .\" The hyphens \"\" were replaced by the previous vowels (e.g., \" \" is converted to \"supagettii.\")", "labels": [], "entities": []}, {"text": "The maximum length of substitution pairs W described in Section 2 was set W = 2.", "labels": [], "entities": []}, {"text": "The EM algorithm parameters P (\u03b1 \u2192 \u03b2|z) were initialized to the probability P (\u03b1 \u2192 \u03b2) of the alpha-beta model plus Gaussian noise, and \u03c0 k were uniformly initialized to 1/K.", "labels": [], "entities": []}, {"text": "Based on the preliminary results, we repeated EM iterations for 40 times.", "labels": [], "entities": []}, {"text": "transliteration model P (c|s) and Equation.2, 5.2).", "labels": [], "entities": [{"text": "Equation.2", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9971157312393188}]}, {"text": "The overall precision is relatively lower than, e.g.,, which is attributed to the fact that European names can be quite ambiguous (e.g., \"Charles\" can read \" ch\u00af aruzu\" or \" sharuru\") The precision of Dataset 2 is even worse because it has more classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993834495544434}, {"text": "precision", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.998777449131012}]}, {"text": "We can also use the result of the latent class transliteration for clustering by regarding k * = arg max k \u03b3 nk as the class of the pair.", "labels": [], "entities": []}, {"text": "The resulting cluster purity way was 0.74.", "labels": [], "entities": [{"text": "cluster purity", "start_pos": 14, "end_pos": 28, "type": "METRIC", "confidence": 0.6410410106182098}]}], "tableCaptions": [{"text": " Table 1: Language Class Detection Result (Dataset 1)", "labels": [], "entities": [{"text": "Language Class Detection Result", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7144674435257912}]}, {"text": " Table 2: Language Class Detection Result (Dataset 2)", "labels": [], "entities": [{"text": "Language Class Detection Result", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7079867050051689}]}, {"text": " Table 3: Model Performance Comparison (MRR; %)", "labels": [], "entities": [{"text": "Model Performance Comparison (MRR; %)", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.6056730364050184}]}]}