{"title": [{"text": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7045874893665314}]}], "abstractContent": [{"text": "This paper introduces anew training set condensation technique designed for mixtures of labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "It finds a condensed set of labeled and unlabeled data points, typically smaller than what is obtained using condensed nearest neighbor on the labeled data only, and improves classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9336675405502319}]}, {"text": "We evaluate the algorithm on semi-supervised part-of-speech tagging and present the best published result on the Wall Street Journal data set.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6958463788032532}, {"text": "Wall Street Journal data set", "start_pos": 113, "end_pos": 141, "type": "DATASET", "confidence": 0.9832097291946411}]}], "introductionContent": [{"text": "Labeled data for natural language processing tasks such as part-of-speech tagging is often in short supply.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7080485224723816}]}, {"text": "Semi-supervised learning algorithms are designed to learn from a mixture of labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "Many different semi-supervised algorithms have been applied to natural language processing tasks, but the simplest algorithm, namely self-training, is the one that has attracted most attention, together with expectation maximization.", "labels": [], "entities": []}, {"text": "The idea behind self-training is simply to let a model trained on the labeled data label the unlabeled data points and then to retrain the model on the mixture of the original labeled data and the newly labeled data.", "labels": [], "entities": []}, {"text": "The nearest neighbor algorithm) is a memory-based or so-called lazy learning algorithm.", "labels": [], "entities": []}, {"text": "It is one of the most extensively used nonparametric classification algorithms, simple to implement yet powerful, owing to its theoretical properties guaranteeing that for all distributions, its probability of error is bound by twice the Bayes probability of error.", "labels": [], "entities": [{"text": "nonparametric classification", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.6870750188827515}]}, {"text": "Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (), dependency parsing and word sense disambiguation.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7044973820447922}, {"text": "dependency parsing", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.8425806760787964}, {"text": "word sense disambiguation", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.6987176934878031}]}, {"text": "Memorybased learning algorithms are said to be lazy because no model is learned from the labeled data points.", "labels": [], "entities": []}, {"text": "The labeled data points are the model.", "labels": [], "entities": []}, {"text": "Consequently, classification time is proportional to the number of labeled data points.", "labels": [], "entities": [{"text": "classification", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9734659194946289}]}, {"text": "This is of course impractical.", "labels": [], "entities": []}, {"text": "Many algorithms have been proposed to make memory-based learning more efficient.", "labels": [], "entities": []}, {"text": "The intuition behind many of them is that the set of labeled data points can be reduced or condensed, since many labeled data points are more or less redundant.", "labels": [], "entities": []}, {"text": "The algorithms try to extract a subset of the overall training set that correctly classifies all the discarded data points through the nearest neighbor rule.", "labels": [], "entities": []}, {"text": "Intuitively, the model finds good representatives of clusters in the data or discards the data points that are far from the decision boundaries.", "labels": [], "entities": []}, {"text": "Such algorithms are called training set condensation algorithms.", "labels": [], "entities": []}, {"text": "The need for training set condensation is particularly important in semi-supervised learning where we rely on a mixture of labeled and unlabeled data points.", "labels": [], "entities": []}, {"text": "While the number of labeled data points is typically limited, the number of unlabeled data points is typically high.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew semi-supervised learning algorithm that combines self-training and condensation to produce small subsets of labeled and unlabeled data points that are highly relevant for determining good deci-sion boundaries.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}