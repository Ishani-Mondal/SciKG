{"title": [{"text": "Towards a Framework for Abstractive Summarization of Multimodal Documents", "labels": [], "entities": [{"text": "Abstractive Summarization of Multimodal Documents", "start_pos": 24, "end_pos": 73, "type": "TASK", "confidence": 0.7952702462673187}]}], "abstractContent": [{"text": "We propose a framework for generating an ab-stractive summary from a semantic model of a multimodal document.", "labels": [], "entities": []}, {"text": "We discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary.", "labels": [], "entities": []}, {"text": "To this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic summarization of text is a prominent task in the field of natural language processing (NLP).", "labels": [], "entities": [{"text": "summarization of text", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.8463691473007202}, {"text": "natural language processing (NLP)", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.8218415677547455}]}, {"text": "While significant achievements have been made using statistical analysis and sentence extraction, \"true abstractive summarization remains a researcher's dream\" ().", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7669504880905151}, {"text": "true abstractive summarization", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.5675787130991617}]}, {"text": "Although existing systems produce high-quality summaries of relatively simple articles, there are limitations as to the types of documents these systems can handle.", "labels": [], "entities": []}, {"text": "One such limitation is the summarization of multimodal documents: no existing system is able to incorporate the non-text portions of a document (e.g., information graphics, images) into the overall summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9850046634674072}]}, {"text": "showed that the content of information graphics is often not repeated in the article's text, meaning important information maybe overlooked if the graphical content is not included in the summary.", "labels": [], "entities": []}, {"text": "Systems that perform statistical analysis of text and extract sentences from the original article to assemble a summary cannot access the information contained in non-text components, let alone seamlessly combine that information with the extracted text.", "labels": [], "entities": []}, {"text": "The problem is that information from the text and graphical components can only be integrated at the conceptual level, necessitating a semantic understanding of the underlying concepts.", "labels": [], "entities": []}, {"text": "Our proposed framework enables the generation of abstractive summaries from unified semantic models, regardless of the original format of the information sources.", "labels": [], "entities": []}, {"text": "We contend that this framework is more akin to the human process of conceptual integration and regeneration in writing an abstract, as compared to the traditional NLP techniques of rating and extracting sentences to form a summary.", "labels": [], "entities": []}, {"text": "Furthermore, this approach enables us to generate summary sentences about the information collected from graphical formats, for which there are no sentences available for extraction, and helps avoid the issues of coherence and ambiguity that tend to affect extraction-based summaries).", "labels": [], "entities": []}], "datasetContent": [{"text": "As an intermediate evaluation, we will rate the concepts stored in a model built only from text and use this rating to select sentences containing these concepts from the original document.", "labels": [], "entities": []}, {"text": "These sentences will be compared to another set chosen by traditional extraction methods.", "labels": [], "entities": []}, {"text": "Human judges will be asked to determine which set of sentences best captures the most important concepts in the document.", "labels": [], "entities": []}, {"text": "This \"checkpoint\" will allow us to assess how well our system identifies the most salient concepts in a text.", "labels": [], "entities": []}, {"text": "The summaries ultimately generated as final output by our prototype system will be evaluated against summaries written by human authors, as well as summaries created by extraction-based systems and a baseline of selecting the first few sentences.", "labels": [], "entities": []}, {"text": "For each comparison, participants will be asked to indicate a preference for one summary over another.", "labels": [], "entities": []}, {"text": "We propose to use preference-strength judgment experiments testing multiple dimensions of preference (e.g., accuracy, clarity, completeness).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9991040825843811}, {"text": "clarity", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9874140024185181}]}, {"text": "Compared to traditional rating scales, this alternative paradigm has been shown to result in better evaluator self-consistency and high inter-evaluator agreement.", "labels": [], "entities": []}, {"text": "This allows a larger proportion of observed variations to be accounted for by the characteristics of systems undergoing evaluation, and can result in a greater number of significant differences being discovered.", "labels": [], "entities": []}, {"text": "Automatic evaluation, though desirable, is likely unfeasible.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5914381146430969}]}, {"text": "As human-written summaries have only about 60% agreement (), there is no \"gold standard\" to compare our output against.", "labels": [], "entities": []}], "tableCaptions": []}