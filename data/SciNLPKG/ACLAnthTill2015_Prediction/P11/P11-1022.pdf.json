{"title": [{"text": "Goodness: A Method for Measuring Machine Translation Confidence", "labels": [], "entities": [{"text": "Measuring Machine Translation Confidence", "start_pos": 23, "end_pos": 63, "type": "TASK", "confidence": 0.7648511454463005}]}], "abstractContent": [{"text": "State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.8078688035408655}]}, {"text": "However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.988905668258667}]}, {"text": "We propose a novel framework to predict word-level and sentence-level MT errors with a large number of novel features.", "labels": [], "entities": [{"text": "MT errors", "start_pos": 70, "end_pos": 79, "type": "TASK", "confidence": 0.7903842926025391}]}, {"text": "Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score.", "labels": [], "entities": [{"text": "MT error prediction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8045114676157633}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9509363770484924}, {"text": "F-score", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9904725551605225}]}, {"text": "The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9672921001911163}, {"text": "human-targeted translation edit rate (HTER)", "start_pos": 72, "end_pos": 115, "type": "METRIC", "confidence": 0.7347804265362876}]}, {"text": "Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure.", "labels": [], "entities": [{"text": "TER reduction", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9754346013069153}]}, {"text": "Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9879889488220215}]}], "introductionContent": [{"text": "State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8809756755828857}]}, {"text": "In particular, statistical machine translation systems () have advanced to a state that the translation quality for certain language pairs (e.g. SpanishEnglish, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6309155523777008}]}, {"text": "However, a remaining open question is how to predict confidence scores for machine translated words and sentences.", "labels": [], "entities": []}, {"text": "An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whole sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9679837226867676}]}, {"text": "Such information is vital * Work done during an internship at IBM T.J.", "labels": [], "entities": []}, {"text": "Watson Research Center to realize the utility of machine translation in many areas.", "labels": [], "entities": [{"text": "Watson Research Center", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.9763467709223429}, {"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7803592085838318}]}, {"text": "For example, a post-editor would like to quickly identify which sentences might be incorrectly translated and in need of correction.", "labels": [], "entities": []}, {"text": "Other areas, such as cross-lingual question-answering, information extraction and retrieval, can also benefit from the confidence scores of MT output.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8520388603210449}, {"text": "MT", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9870225787162781}]}, {"text": "Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9723097085952759}, {"text": "phrase table and rule filtering", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.5549086809158326}]}, {"text": "Numerous attempts have been made to tackle the confidence estimation problem.", "labels": [], "entities": [{"text": "confidence estimation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6920725554227829}]}, {"text": "The work of is perhaps the best known study of sentence and word level features and their impact on translation error prediction.", "labels": [], "entities": [{"text": "translation error prediction", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.9251656532287598}]}, {"text": "Along this line of research, improvements can be obtained by incorporating more features as shown in.", "labels": [], "entities": []}, {"text": "Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9803545773029327}]}, {"text": "Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier.", "labels": [], "entities": []}, {"text": "introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking.", "labels": [], "entities": [{"text": "word posterior probabilities (WPP)", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.4973689466714859}]}, {"text": "From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9979608058929443}, {"text": "MT output", "start_pos": 107, "end_pos": 116, "type": "TASK", "confidence": 0.8673661351203918}]}, {"text": "Literally, it translates backward the MT output into the source language to see whether the output of backward translation matches the original source sentence.", "labels": [], "entities": [{"text": "MT output", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.8361072242259979}]}, {"text": "However, previous studies had a few shortcomings.", "labels": [], "entities": []}, {"text": "First, source-side features were not extensively investigated.", "labels": [], "entities": []}, {"text": "only investigated source ngram frequency statistics and source language model features, while other work mainly focused on target side features.", "labels": [], "entities": []}, {"text": "Second, previous work attempted to incorporate more features but faced scalability issues, i.e., to train many features we need many training examples and to train discriminatively we need to search through all possible translations of each training example.", "labels": [], "entities": []}, {"text": "Another issue of previous work was that they are all trained with BLEU/TER score computing against the translation references which is different from predicting the human-targeted translation edit rate (HTER) which is crucial in post-editing applications).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9988180994987488}, {"text": "TER score computing", "start_pos": 71, "end_pos": 90, "type": "METRIC", "confidence": 0.8754837115605673}, {"text": "human-targeted translation edit rate (HTER)", "start_pos": 165, "end_pos": 208, "type": "METRIC", "confidence": 0.7204560339450836}]}, {"text": "Finally, the backtranslation approach faces a serious issue when forward and backward translation models are symmetric.", "labels": [], "entities": []}, {"text": "In this case, back-translation will not be very informative to indicate forward translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we predict error types of each word in the MT output with a confidence score, extend it to the sentence level, then apply it to n-best list reranking task to improve MT quality, and finally design a visualization prototype.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9717792272567749}, {"text": "MT", "start_pos": 181, "end_pos": 183, "type": "TASK", "confidence": 0.9877959489822388}]}, {"text": "We try to answer the following questions: \u2022 Can we use a rich feature set such as sourceside information, alignment context, and dependency structures to improve error prediction performance?", "labels": [], "entities": [{"text": "error prediction", "start_pos": 162, "end_pos": 178, "type": "TASK", "confidence": 0.6218460351228714}]}, {"text": "\u2022 Can we predict more translation error types i.e substitution, insertion, deletion and shift?", "labels": [], "entities": []}, {"text": "\u2022 How good do our prediction methods correlate with human correction?", "labels": [], "entities": [{"text": "human correction", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.725830003619194}]}, {"text": "\u2022 Do confidence measures help the MT system to select a better translation?", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9760984778404236}]}, {"text": "\u2022 How confidence score can be presented to improve end-user perception?", "labels": [], "entities": []}, {"text": "In Section 2, we describe the models and training method for the classifier.", "labels": [], "entities": []}, {"text": "We describe novel features including source-side, alignment context, and dependency structures in Section 3.", "labels": [], "entities": []}, {"text": "Experimental results and analysis are reported in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 and 6 present applications of confidence scores.", "labels": [], "entities": [{"text": "confidence scores", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9453871846199036}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Contribution of different feature sets measure  in F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9854391813278198}]}, {"text": " Table 3: Reranking performance with goodness score.", "labels": [], "entities": [{"text": "Reranking", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9836722016334534}, {"text": "goodness score", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.9801378846168518}]}]}