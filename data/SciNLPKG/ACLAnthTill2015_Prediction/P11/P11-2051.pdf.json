{"title": [{"text": "Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.8506837685902914}]}], "abstractContent": [{"text": "We present an approach of expanding parallel corpora for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7821425497531891}]}, {"text": "By utilizing Semantic role labeling (SRL) on one side of the language pair, we extract SRL substitution rules from existing parallel corpus.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.7588813006877899}, {"text": "SRL substitution", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.9403321743011475}]}, {"text": "The rules are then used for generating new sentence pairs.", "labels": [], "entities": []}, {"text": "An SVM classifier is built to filter the generated sentence pairs.", "labels": [], "entities": []}, {"text": "The filtered corpus is used for training phrase-based translation models, which can be used directly in translation tasks or combined with base-line models.", "labels": [], "entities": []}, {"text": "Experimental results on Chinese-English machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7694423894087473}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9985387325286865}, {"text": "TER", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9855319857597351}, {"text": "NIST test sets", "start_pos": 146, "end_pos": 160, "type": "DATASET", "confidence": 0.9386387864748637}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) relies on parallel corpus.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8184829304615656}]}, {"text": "Aside from collecting parallel corpus, we have seen interesting research on automatically generating corpus from existing resources.", "labels": [], "entities": []}, {"text": "Typical examples are paraphrasing using bilingual) or monolingual () data.", "labels": [], "entities": []}, {"text": "In this paper, we propose a different methodology of generating additional parallel corpus.", "labels": [], "entities": []}, {"text": "The basic idea of paraphrasing is to find alternative ways that convey the same information.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9710615873336792}]}, {"text": "In contrast, we propose to build new parallel sentences that convey different information, yet retain correct grammatical and semantic structures.", "labels": [], "entities": []}, {"text": "The basic idea of the proposed method is to substitute source and target phrase pairs in a sentence pair with phrase pairs from other sentences.", "labels": [], "entities": []}, {"text": "The problem is how to identify where a substitution should happen and which phrase pairs are valid candidates for the substitution.", "labels": [], "entities": []}, {"text": "While syntactical constraints have been proven to helpful in identifying good paraphrases, it is insufficient in our task because it cannot properly filter the candidates for the replacement.", "labels": [], "entities": []}, {"text": "If we allow all the NPs to be replaced with other NPs, each sentence pair can generate huge number of new sentences.", "labels": [], "entities": []}, {"text": "Instead, we resort to Semantic Role Labeling () to provide more lexicalized and semantic constraints to select the candidates.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7204952637354533}]}, {"text": "The method only requires running SRL labeling on either side of the language pair, and that enables applications on low resource languages.", "labels": [], "entities": [{"text": "SRL labeling", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9364942014217377}]}, {"text": "Even with the SRL constraints, the generated corpus may still be large and noisy.", "labels": [], "entities": []}, {"text": "Hence, we apply an additional filtering stage on the generated corpus.", "labels": [], "entities": []}, {"text": "We used an SVM classifier with features derived from standard phrase based translation models and bilingual language models to identify high quality sentence pairs, and use these sentence pairs in the SMT training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.9942493438720703}]}, {"text": "In the remaining part of the paper, we introduce the approach and present experimental results on Chineseto-English translation tasks, which showed improvements across 5 NIST test sets.", "labels": [], "entities": [{"text": "Chineseto-English translation tasks", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.7744857966899872}, {"text": "NIST test sets", "start_pos": 170, "end_pos": 184, "type": "DATASET", "confidence": 0.9518447717030843}]}], "datasetContent": [{"text": "We performed experiments on Chinese to English MT tasks with the proposed approach.", "labels": [], "entities": [{"text": "Chinese to English MT tasks", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.5631843090057373}]}, {"text": "The baseline system is trained on the FBIS corpus, the statistics of the corpus is shown in.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.950415700674057}]}, {"text": "We adopted the ASSERT English SRL labeler (), which was trained on PropBank data using SVM classifier.", "labels": [], "entities": [{"text": "ASSERT English SRL labeler", "start_pos": 15, "end_pos": 41, "type": "DATASET", "confidence": 0.6272776499390602}, {"text": "PropBank data", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9626427590847015}]}, {"text": "The labeler reports 81.87% precision and 73.21% recall rate on CoNLL-2005 shared task on SRL.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9994958639144897}, {"text": "recall rate", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9796399474143982}, {"text": "CoNLL-2005 shared task on SRL", "start_pos": 63, "end_pos": 92, "type": "DATASET", "confidence": 0.7142349004745483}]}, {"text": "We aligned the parallel sentences with MGIZA(, and performed experiments with the Moses toolkit ().", "labels": [], "entities": [{"text": "MGIZA", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.5798437595367432}]}, {"text": "The rule extraction algorithm produces: Experiment results on Chinese-English translation tasks, the abbreviations for systems are as follows: BL: Baseline system, GS: System trained with only generated sentence pairs, IT: Interpolated phrase table with GS and BL,.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7325609028339386}, {"text": "Chinese-English translation tasks", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7401901880900065}, {"text": "BL", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9870431423187256}]}, {"text": "GA and IA are GS and IT systems trained with baseline word alignment models accordingly.", "labels": [], "entities": [{"text": "IA", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.48643723130226135}, {"text": "word alignment", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7249839901924133}]}, {"text": "LS is the GALE system with 8.7M sentence pairs.", "labels": [], "entities": [{"text": "LS", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6584211587905884}]}, {"text": "As we can observe in, we generated 29.6 million sentences from the 387K sentence pairs, and by using the SVM-based classifier, we filter the corpus down to 7.2 million.", "labels": [], "entities": []}, {"text": "We also observed that the average sentence length increases by 15% in the generated corpus.", "labels": [], "entities": []}, {"text": "That is because longer sentences have more slots for substitution.", "labels": [], "entities": []}, {"text": "Therefore, they have more occurrences in the generated corpus.", "labels": [], "entities": []}, {"text": "We used the NIST MT06 test set for tuning, and experimented with 5 test sets, including MT02, 03, 04, 05, 08.", "labels": [], "entities": [{"text": "NIST MT06 test set", "start_pos": 12, "end_pos": 30, "type": "DATASET", "confidence": 0.8709092736244202}, {"text": "MT02", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.9170474410057068}]}, {"text": "shows the BLEU and TER scores of the experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993011951446533}, {"text": "TER scores", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9700432121753693}]}, {"text": "As we can see in the results, by using only the generated sentence pairs, the performance of the system drops.", "labels": [], "entities": []}, {"text": "However the interpolated phrase tables outperform the baseline.", "labels": [], "entities": []}, {"text": "On average, the improvements on all the 5 test sets are 0.45 on BLEU score and -1.22 on TER when using the interpolated phrase table.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9844436943531036}, {"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9988504648208618}]}, {"text": "We do observe MT08 drops on BLEU scores; however, the TER scores are consistently improved across all the test sets.", "labels": [], "entities": [{"text": "MT08", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.7635651230812073}, {"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9983525276184082}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9991450309753418}]}, {"text": "When using baseline alignment model, we observe a quite different phenomenon.", "labels": [], "entities": []}, {"text": "In this case, interpolating the phrase tables no longer show improvements.", "labels": [], "entities": []}, {"text": "However, using the generated corpus alone achieves: Statistics of phrase tables and translation outputs, including the phrase tables (PT) size, the coverage of the BL phrase table entries (C.P.), the number of source phrases (D.S.), the number of new source phrases comparing to BL system (N.S.), the average number of alternative translations of each source phrase (T/S) and the average source phrase length in the output (A.L.) -1.80 on average TER.", "labels": [], "entities": [{"text": "A.L.)", "start_pos": 424, "end_pos": 429, "type": "METRIC", "confidence": 0.9012289345264435}, {"text": "TER", "start_pos": 447, "end_pos": 450, "type": "METRIC", "confidence": 0.9918723702430725}]}, {"text": "An explanation is that using identical alignment model makes the phrases extracted from the baseline and generated corpus similar, which undermines the idea of interpolating two phrase tables.", "labels": [], "entities": []}, {"text": "As shown in, it generates less new source phrases and 10% more phrase pairs that overlaps with the baseline phrase table.", "labels": [], "entities": []}, {"text": "For comparison, we also provide scores from a system that uses the training data for GALE project, which has 8.7M sentence pairs 4 . In we observe that the large GALE system yields better BLEU results while the IT or GA systems have even better TER scores than the GALE system.", "labels": [], "entities": [{"text": "GALE project", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.6440856158733368}, {"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9991111159324646}, {"text": "TER", "start_pos": 245, "end_pos": 248, "type": "METRIC", "confidence": 0.9989727735519409}]}, {"text": "The expanded corpus performs almost as well as the GALE system even though the large system has a phrase table that is four time larger.", "labels": [], "entities": []}, {"text": "The statistics of the phrase tables and translation outputs are listed in.", "labels": [], "entities": []}, {"text": "As we can see, the generated sentence introduces a large number of new source phrases and the average lengths of matching source phrases of all the systems are longer than the baseline, which could bean evidence for our claim that the proposed approach can generate more high quality sentences and phrase pairs that have not been observed in the original corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of generated corpus.", "labels": [], "entities": []}, {"text": " Table 2: Experiment results on Chinese-English transla- tion tasks, the abbreviations for systems are as follows:  BL: Baseline system, GS: System trained with only gen- erated sentence pairs, IT: Interpolated phrase table with  GS and BL,. GA and IA are GS and IT systems trained  with baseline word alignment models accordingly. LS is  the GALE system with 8.7M sentence pairs.", "labels": [], "entities": [{"text": "Chinese-English transla- tion tasks", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.6415383458137512}, {"text": "BL", "start_pos": 116, "end_pos": 118, "type": "METRIC", "confidence": 0.9809077382087708}, {"text": "word alignment", "start_pos": 297, "end_pos": 311, "type": "TASK", "confidence": 0.7423815131187439}]}, {"text": " Table 3: Statistics of phrase tables and translation out- puts, including the phrase tables (PT) size, the coverage  of the BL phrase table entries (C.P.), the number of source  phrases (D.S.), the number of new source phrases com- paring to BL system (N.S.), the average number of alter- native translations of each source phrase (T/S) and the  average source phrase length in the output (A.L.)", "labels": [], "entities": []}]}