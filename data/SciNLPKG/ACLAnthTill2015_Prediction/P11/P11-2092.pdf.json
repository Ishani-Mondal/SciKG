{"title": [{"text": "Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes", "labels": [], "entities": [{"text": "Improved Modeling of Out-Of-Vocabulary Words", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.9033613562583923}]}], "abstractContent": [{"text": "We present a class-based language model that clusters rare words of similar morphology together.", "labels": [], "entities": []}, {"text": "The model improves the prediction of words after histories containing out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "The morphological features used are obtained without the use of labeled data.", "labels": [], "entities": []}, {"text": "The perplexity improvement compared to a state of the art Kneser-Ney model is 4% overall and 81% on unknown histories.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the challenges in statistical language modeling are words that appear in the recognition task at hand, but not in the training set, so called outof-vocabulary (OOV) words.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.769842247168223}]}, {"text": "Especially for productive language it is often necessary to at least reduce the number of OOVs.", "labels": [], "entities": []}, {"text": "We present a novel approach based on morphological classes to handling OOV words in language modeling for English.", "labels": [], "entities": []}, {"text": "Previous work on morphological classes in English has not been able to show noticeable improvements in perplexity.", "labels": [], "entities": []}, {"text": "In this article class-based language models as proposed by are used to tackle the problem.", "labels": [], "entities": []}, {"text": "Our model improves perplexity of a Kneser-Ney (KN) model for English by 4%, the largest improvement of a state-of-the-art model for English due to morphological modeling that we are aware of.", "labels": [], "entities": []}, {"text": "A class-based language model groups words into classes and replaces the word transition probability by a class transition probability and a word emission probability: P (w 3 |w 1 w 2 ) = P (c 3 |c 1 c 2 ) \u00b7 P (w 3 |c 3 ).", "labels": [], "entities": []}, {"text": "Brown et al. and many other authors primarily use context information for clustering.", "labels": [], "entities": []}, {"text": "showed that context clustering works better than clusters based on part-of-speech tags.", "labels": [], "entities": [{"text": "context clustering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7493734657764435}]}, {"text": "However, since the context of an OOV word is unknown and it therefore cannot be assigned to a cluster, OOV words are as much a problem to a context-based class model as to a word model.", "labels": [], "entities": []}, {"text": "That is why we use non-distributional features -features like morphological suffixes that only depend on the shape of the word itself -to design anew class-based model that can naturally integrate unknown words.", "labels": [], "entities": []}, {"text": "In related work, factored language models were proposed to make use of morphological information in highly inflecting languages such as Finnish (,) and Arabic () or compounding languages like German (.", "labels": [], "entities": []}, {"text": "The main idea is to replace words by sequences of factors or features and to apply statistical language modeling to the resulting factor sequences.", "labels": [], "entities": []}, {"text": "If, for example, words were segmented into morphemes, an unknown word would be split into an unseen sequence, which could be recognized using discounting techniques.", "labels": [], "entities": []}, {"text": "However, if one morpheme, e.g. the stem, is unknown to the system, the fundamental problem remains unsolved.", "labels": [], "entities": []}, {"text": "Our class-based model uses a number of features that have not been used in factored models (e.g., shape and length features) and achieves -in contrast to factored models -good perplexity gains for English.", "labels": [], "entities": []}, {"text": "524 is capital(w) first character of w is an uppercase letter is all capital(w) \u2200 c \u2208 w : c is an uppercase letter capital character(w) \u2203 c \u2208 w : c is an uppercase letter appears in lowercase(w)", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the performance of the described model with a Kneser-Ney model and an interpolated model based on part-of-speech (POS) tags.", "labels": [], "entities": []}, {"text": "The relation between words and POS tags is many-to-many, but we transform it to a many-to-one relation by labeling every word -independent of its context -with its most frequent tag.", "labels": [], "entities": []}, {"text": "OOV words are treated equally even though their POS classes would not be known in areal application.", "labels": [], "entities": []}, {"text": "Treetagger was used to tag the entire corpus.", "labels": [], "entities": []}, {"text": "The experiments are carried out on a Wall Street Journal (WSJ) corpus of 50 million words that is split into training set (80%), valdev (5%), valtst (5%), and test set (10%).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 37, "end_pos": 69, "type": "DATASET", "confidence": 0.9387168458529881}]}, {"text": "The number of distinct feature vectors in training set, valdev and validation set (valdev+valtst) are 632, 466, and 512, respectively.", "labels": [], "entities": []}, {"text": "As mentioned above, the training set is used to learn suffixes and the maximum likelihood n-gram estimates.", "labels": [], "entities": []}, {"text": "The unknown word rate of the validation set is \ud97b\udf59 \u2248 0.028.", "labels": [], "entities": [{"text": "word rate", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.8040586113929749}]}, {"text": "We use two setups to evaluate our methods.", "labels": [], "entities": []}, {"text": "The first uses valdev for parameter estimation and valtst for testing and the second the entire validation set for parameter estimation and the test set for testing.", "labels": [], "entities": []}, {"text": "All models with a threshold greater or equal to the frequency of the most frequent word type are identical.", "labels": [], "entities": []}, {"text": "We use \u221e as the threshold to refer to these models.", "labels": [], "entities": []}, {"text": "Ina similar manner, the cluster count \u221e denotes a clustering where two words are in the same cluster if and only if their features are identical.", "labels": [], "entities": []}, {"text": "This is the finest possible clustering of the feature vectors.", "labels": [], "entities": []}, {"text": "shows the results of our experiments.", "labels": [], "entities": []}, {"text": "The KN model yields a perplexity of 88.06 on valtst (top row).", "labels": [], "entities": [{"text": "valtst", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9433615803718567}]}, {"text": "For small frequency thresholds overfitting effects cause that the interpolated models are worse than the KN model.", "labels": [], "entities": []}, {"text": "We can see that a clustering of the feature vectors is not necessary as the differences between all cluster models are small and c \u221e is the overall best model.", "labels": [], "entities": []}, {"text": "Surprisingly, morphological clustering and POS classes are close even though 526: Perplexities for different frequency thresholds \u03b8 and cluster models.", "labels": [], "entities": []}, {"text": "In the left table, perplexity is calculated overall events P (w 3 |w 1 w 2 ) of the valtst set.", "labels": [], "entities": []}, {"text": "On the right side, the subset of events where w 1 or w 2 are unknown is taken into account.", "labels": [], "entities": []}, {"text": "The overall best results for class models and POS models are highlighted in bold.", "labels": [], "entities": []}, {"text": "the POS class model uses oracle information to assign the right POS to an unknown word.", "labels": [], "entities": []}, {"text": "The optimal threshold is \u03b8 = 10 3 -the bolded perplexity values 84.43 and 84.56; that means that only 1.35% of the word types were excluded from the morphological clustering (86% of the tokens).", "labels": [], "entities": []}, {"text": "The improvement over the KN model is 4%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Proportion of dominant POS for types with train- ing set frequencies f \u2208 {0, 1} and for tokens. V* consists  of all verb POS tags.", "labels": [], "entities": []}, {"text": " Table 3: Perplexities for different frequency thresholds \u03b8 and cluster models. In the left table, perplexity is calculated  over all events P (w 3 |w 1 w 2 ) of the valtst set. On the right side, the subset of events where w 1 or w 2 are unknown is  taken into account. The overall best results for class models and POS models are highlighted in bold.", "labels": [], "entities": []}]}