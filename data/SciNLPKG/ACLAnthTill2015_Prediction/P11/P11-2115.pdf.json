{"title": [{"text": "Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection.", "labels": [], "entities": [{"text": "Surface realisation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7750167846679688}, {"text": "language generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.720989003777504}]}, {"text": "We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL).", "labels": [], "entities": [{"text": "content selection", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7486387491226196}, {"text": "surface realisation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.773424357175827}]}, {"text": "To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8051134049892426}]}, {"text": "It is based on a generation space in the form of a Hidden Markov Model (HMM).", "labels": [], "entities": []}, {"text": "Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain.", "labels": [], "entities": [{"text": "Surface realisation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7592601776123047}, {"text": "Natural Language Generation (NLG)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.8112131257851919}]}, {"text": "However, there are other linguistic phenomena, such as alignment (), consistency (, and variation, which influence people's assessment of discourse and generated output).", "labels": [], "entities": [{"text": "consistency", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9881551265716553}]}, {"text": "Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user's information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history.", "labels": [], "entities": []}, {"text": "In such cases, it is important to optimise surface realisation in a unified fashion with content selection.", "labels": [], "entities": []}, {"text": "We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this.", "labels": [], "entities": []}, {"text": "Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions given incomplete knowledge of the environment or best strategy to follow (.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8498566269874572}]}, {"text": "HRL has the additional advantage of scaling to large and complex problems.", "labels": [], "entities": [{"text": "HRL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.796918511390686}]}, {"text": "Since an HRL agent will ultimately learn the behaviour it is rewarded for, the reward function is arguably the agent's most crucial component.", "labels": [], "entities": []}, {"text": "Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework ().", "labels": [], "entities": [{"text": "PARADISE framework", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.6596235781908035}]}, {"text": "Since PARADISE-based reward functions typically rely on objective metrics, they are not ideally suited for surface realisation, which is more dependend on linguistic phenomena, e.g. frequency, consistency, and variation.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7341006100177765}, {"text": "consistency", "start_pos": 193, "end_pos": 204, "type": "METRIC", "confidence": 0.9143603444099426}]}, {"text": "However, linguistic and psychological studies (cited above) show that such phenomena are indeed worth modelling in an NLG system.", "labels": [], "entities": []}, {"text": "The contribution of this paper is therefore to induce a reward function from human data, specifically suited for surface generation.", "labels": [], "entities": [{"text": "surface generation", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7205030918121338}]}, {"text": "To this end, we train HMMs on a corpus of grammatical word sequences and use them to inform the agent's learning process.", "labels": [], "entities": []}, {"text": "In addition, we suggest to optimise surface realisation and content selection decisions in a joint, rather than isolated, fashion.", "labels": [], "entities": []}, {"text": "Results show that our combined approach generates more successful and human-like utterances than a greedy or random baseline.", "labels": [], "entities": []}, {"text": "This is related to, who also address interdependent decision making, but do not use an optmisation framework.", "labels": [], "entities": []}, {"text": "Since language models in our approach can be obtained for any domain for which corpus data is available, it generalises to new domains with limited effort and reduced development time.", "labels": [], "entities": []}, {"text": "For related work on using graphical models for language generation, see e.g.,, who use lattices, or, who use dynamic Bayesian networks.", "labels": [], "entities": [{"text": "language generation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7412373423576355}]}], "datasetContent": [{"text": "We test our approach using the (hand-crafted) hierarchy of generation subtasks in.", "labels": [], "entities": []}, {"text": "It consists of a root agent (M 0 0 ), and subtasks for low-level (M 2 0 ) and high-level (M 2 1 ) navigation strategies (M 1 1 ), and for instruction types 'orientation' (M 3 0 ), 'straight' (M 3 1 ), 'direction' (M 3 2 ), 'path' (M 3 3 ) and destination' (M 3 4 ).", "labels": [], "entities": []}, {"text": "Models M 3 0...4 are responsible for surface generation.", "labels": [], "entities": [{"text": "surface generation", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8072895109653473}]}, {"text": "They will be trained using HRL with an HMM-based reward function induced from human data.", "labels": [], "entities": []}, {"text": "All other agents use hand-crafted rewards.", "labels": [], "entities": []}, {"text": "Finally, subtask M 1 0 can repair a previous system utterance.", "labels": [], "entities": []}, {"text": "The states of the agent contain all situational and linguistic information relevant to its decision making, e.g., the spatial environment, discourse history, and status of grounding.", "labels": [], "entities": []}, {"text": "Due to space constraints, please see for the full state-action space.", "labels": [], "entities": []}, {"text": "We distinguish primitive actions (corresponding to single generation decisions) and composite actions (corresponding to generation subtasks).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of generation behaviours with  Precision-Recall and KL-divergence.", "labels": [], "entities": [{"text": "Precision-Recall", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.9889772534370422}, {"text": "KL-divergence", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.774855375289917}]}]}