{"title": [], "abstractContent": [{"text": "One of the major problems of K-means is that one must use dense vectors for its cen-troids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional.", "labels": [], "entities": []}, {"text": "We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors.", "labels": [], "entities": []}, {"text": "Our analysis gives theoretical motivation and justification for applying feature hashing to K-means, by showing how much will the objective of K-means be (additively) distorted.", "labels": [], "entities": []}, {"text": "Furthermore , to empirically verify our method, we experimented on a document clustering task.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7281809747219086}]}], "introductionContent": [{"text": "In natural language processing (NLP) and text mining, clustering methods are crucial for various tasks such as document clustering.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7879756987094879}, {"text": "text mining", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.7376279830932617}, {"text": "document clustering", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7914583384990692}]}, {"text": "Among them, Kmeans is \"the most important flat clustering algorithm\" () both for its simplicity and performance.", "labels": [], "entities": []}, {"text": "One of the major problems of K-means is that it has K centroids which are dense vectors where K is the number of clusters.", "labels": [], "entities": []}, {"text": "Thus, it is infeasible to store them in memory and slow to compute if the dimension of inputs is huge, as is often the case with NLP and text mining tasks.", "labels": [], "entities": [{"text": "text mining tasks", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.8114183147748312}]}, {"text": "A well-known heuristic is truncating after the most significant features (, but it is difficult to analyze its effect and to determine which features are significant.", "labels": [], "entities": []}, {"text": "Recently, introduced feature hashing, a simple yet effective and analyzable dimension-reduction technique for large-scale multitask learning.", "labels": [], "entities": [{"text": "feature hashing", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.6761391013860703}]}, {"text": "The idea is to combine features which have the same hash value.", "labels": [], "entities": []}, {"text": "For example, given a hash function hand a vector x, if h(1012) = h(41234) = 42, we make anew vector y by setting y 42 = x 1012 + x 41234 (or equally possibly This trick greatly reduces the size of dense vectors, since the maximum index value becomes equivalent to the maximum hash value of h.", "labels": [], "entities": []}, {"text": "Furthermore, unlike random projection, feature hashing retains sparsity of sparse input vectors.", "labels": [], "entities": [{"text": "feature hashing", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7085185348987579}]}, {"text": "An additional useful trait for NLP tasks is that it can save much memory by eliminating an alphabet storage (see the preliminaries for detail).", "labels": [], "entities": []}, {"text": "The authors also justified their method by showing that with feature hashing, dotproduct is unbiased, and the length of each vector is well-preserved with high probability under some conditions.", "labels": [], "entities": []}, {"text": "Plausibly this technique is useful also for clustering methods such as K-means.", "labels": [], "entities": []}, {"text": "In this paper, to motivate applying feature hashing to K-means, we show the residual sum of squares, the objective of K-means, is well-preserved under feature hashing.", "labels": [], "entities": []}, {"text": "We also demonstrate an experiment on document clustering and seethe feature size can be shrunk into 3.5% of the original in this case.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7072295248508453}]}], "datasetContent": [{"text": "To empirically verify our method, from 20 Newsgroups, a dataset for document classification or clustering 3 , we chose 6 classes and randomly drew 100 documents for each class.", "labels": [], "entities": [{"text": "document classification", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.7054992467164993}]}, {"text": "We used unigrams and bigrams as features and ran our method for various hash sizes m).", "labels": [], "entities": []}, {"text": "The number of unigrams is 33,017 and bigrams 109,395, so the feature size in the original space is 142,412.", "labels": [], "entities": []}, {"text": "To measure performance, we used the F 5 measure (.", "labels": [], "entities": [{"text": "F 5 measure", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9794064164161682}]}, {"text": "The scheme counts correctness pairwisely.", "labels": [], "entities": []}, {"text": "For example, if a document pair in an output cluster is actually in the same class, it is counted as true positive.", "labels": [], "entities": []}, {"text": "In contrast, if it is actually in the different class, it is counted as false positive.", "labels": [], "entities": []}, {"text": "Following this manner, a contingency where the precision P = T P/(T P + F P ) and the recall R = T P/(T P + F N ).", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9979534149169922}, {"text": "recall R", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9792729318141937}]}, {"text": "In short, F 5 measure strongly favors precision to recall.", "labels": [], "entities": [{"text": "F 5 measure", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9832178950309753}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9991318583488464}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9928566217422485}]}, {"text": "stated that in some cases separating similar documents is more unfavorable than putting dissimilar documents together, and in such cases the F \u03b2 measure (where \u03b2 > 1) is a good evaluation criterion.", "labels": [], "entities": [{"text": "F \u03b2 measure", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.9859179655710856}]}, {"text": "At the first look, it seems odd that performance can be higher than the original where m is low.", "labels": [], "entities": []}, {"text": "A possible hypothesis is that since K-means only locally minimizes RSS but in general there are many local minima which are far from the global optimal point, therefore distortion can be sometimes useful to escape from a bad local minimum and reach a better one.", "labels": [], "entities": []}, {"text": "As a rule, however, large distortion kills clustering performance as shown in the figure.", "labels": [], "entities": []}, {"text": "Although clustering is heavily case-dependent, in this experiment, the resulting clusters are still reliable where the hash size is 3.5% of the original feature space size (around 5,000).", "labels": [], "entities": []}], "tableCaptions": []}