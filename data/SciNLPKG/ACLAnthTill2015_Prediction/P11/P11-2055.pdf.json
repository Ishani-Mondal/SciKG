{"title": [{"text": "ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6719733675320944}]}], "abstractContent": [{"text": "This paper describes a set of exploratory experiments fora multilingual classification-based approach to Word Sense Disambigua-tion.", "labels": [], "entities": [{"text": "Word Sense Disambigua-tion", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.6096945305665334}]}, {"text": "Instead of using a predefined monolin-gual sense-inventory such as WordNet, we use a language-independent framework where the word senses are derived automatically from word alignments on a parallel corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9695329070091248}]}, {"text": "We built five classifiers with English as an input language and translations in the five supported languages (viz.", "labels": [], "entities": []}, {"text": "French, Dutch, Italian, Span-ish and German) as classification output.", "labels": [], "entities": []}, {"text": "The feature vectors incorporate both the more traditional local context features, as well as binary bag-of-words features that are extracted from the aligned translations.", "labels": [], "entities": []}, {"text": "Our results show that the ParaSense multilingual WSD system shows very competitive results compared to the best systems that were evaluated on the SemEval-2010 Cross-Lingual Word Sense Disambiguation task for all five target languages.", "labels": [], "entities": [{"text": "ParaSense multilingual WSD", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.4108293453852336}, {"text": "SemEval-2010 Cross-Lingual Word Sense Disambiguation task", "start_pos": 147, "end_pos": 204, "type": "TASK", "confidence": 0.670135090748469}]}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is the NLP task that consists in selecting the correct sense of a polysemous word in a given context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7745626270771027}]}, {"text": "Most stateof-the-art WSD systems are supervised classifiers that are trained on manually sense-tagged corpora, which are very time-consuming and expensive to build) . In order to overcome this acquisition bottleneck (sense-tagged corpora are scarce for languages other than English), we decided to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (.", "labels": [], "entities": [{"text": "WSD", "start_pos": 330, "end_pos": 333, "type": "TASK", "confidence": 0.9433122277259827}, {"text": "Europarl parallel corpus", "start_pos": 390, "end_pos": 414, "type": "DATASET", "confidence": 0.9566301703453064}]}, {"text": "Using translations from a parallel corpus implicitly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the target translations.", "labels": [], "entities": []}, {"text": "It also facilitates the integration of WSD in multilingual applications such as multilingual Information Retrieval (IR) or Machine Translation (MT).", "labels": [], "entities": [{"text": "WSD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9463406801223755}, {"text": "multilingual Information Retrieval (IR)", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.7757870753606161}, {"text": "Machine Translation (MT)", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.8448016166687011}]}, {"text": "Significant improvements in terms of general MT quality were for the first time reported by and.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9749082922935486}]}, {"text": "Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.661824107170105}, {"text": "MT evaluation", "start_pos": 197, "end_pos": 210, "type": "TASK", "confidence": 0.8721476197242737}]}, {"text": "Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (), for bilingual WSD modules (e.g. () and for WSD systems that use a combination of existing WordNets with multilingual evidence).", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7169569432735443}]}, {"text": "The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages.", "labels": [], "entities": [{"text": "WSD", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9832608103752136}]}, {"text": "To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidence from multiple languages into the feature vector will be more informative than a more restricted set of monolingual or bilingual features.", "labels": [], "entities": []}, {"text": "Furthermore, our WSD system does not use any information from external lexical resources such as WordNet or EuroWordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9723368883132935}, {"text": "EuroWordNet", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.8941137194633484}]}], "datasetContent": [{"text": "Starting point of the experiments was the six-lingual sentence-aligned Europarl corpus that was used in the SemEval-2010 \"Cross-Lingual Word Sense Disambiguation\" (CLWSD) task.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9822707772254944}, {"text": "SemEval-2010 \"Cross-Lingual Word Sense Disambiguation\" (CLWSD) task", "start_pos": 108, "end_pos": 175, "type": "TASK", "confidence": 0.7285376326604323}]}, {"text": "The task is a lexical sample task for twenty English ambiguous nouns that consists in assigning a correct translation in the five supported target languages (viz.", "labels": [], "entities": []}, {"text": "French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context.", "labels": [], "entities": []}, {"text": "In order to detect the relevant translations for each of the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings for all focus words.", "labels": [], "entities": []}, {"text": "This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that is used to train the Dutch classifier).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.6671112030744553}]}, {"text": "By considering this word alignment output as oracle information, we redefined the CLWSD task as a classification task.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.6900847405195236}]}, {"text": "To train our five classifiers (English as input language and French, German, Dutch, Italian and Spanish as focus languages), we used the memory-based learning (MBL) algorithm implemented in TIMBL ( , which has successfully been deployed in previous WSD classification tasks ).", "labels": [], "entities": [{"text": "WSD classification tasks", "start_pos": 249, "end_pos": 273, "type": "TASK", "confidence": 0.9621894558270773}]}, {"text": "We performed heuristic experiments to define the parameter settings for the classifier, leading to the selection of the Jeffrey Divergence distance metric, Gain Ratio feature weighting and k = 7 as number of nearest neighbours.", "labels": [], "entities": [{"text": "Jeffrey Divergence distance metric", "start_pos": 120, "end_pos": 154, "type": "METRIC", "confidence": 0.7173108831048012}, {"text": "Gain Ratio feature weighting", "start_pos": 156, "end_pos": 184, "type": "METRIC", "confidence": 0.9406644552946091}]}, {"text": "In future work, we plan to use an optimized word-expert approach in which a genetic algorithm performs joint feature selection and parameter optimization per ambiguous word (.", "labels": [], "entities": []}, {"text": "For our feature vector creation, we combined a set of English local context features and a set of binary bag-of-words features that were extracted from the aligned translations.", "labels": [], "entities": []}, {"text": "To evaluate our five classifiers, we used the sense inventory and test set of the SemEval \"Cross-Lingual Word Sense Disambiguation\" task.", "labels": [], "entities": [{"text": "Cross-Lingual Word Sense Disambiguation\" task", "start_pos": 91, "end_pos": 136, "type": "TASK", "confidence": 0.6998162021239599}]}, {"text": "The sense inventory was built upon the basis of the Europarl corpus: all retrieved translations of a polysemous word were manually grouped into clusters, which constitute different senses of that given word.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9881672263145447}]}, {"text": "The test instances were selected from the JRC-ACQUIS Multilingual Parallel Corpus 2 and BNC 3 . To label the test data, native speakers provided their top three translations from the predefined clusters of Europarl translations, in order to assign frequency weights to the set of gold standard translations.", "labels": [], "entities": [{"text": "JRC-ACQUIS Multilingual Parallel Corpus 2", "start_pos": 42, "end_pos": 83, "type": "DATASET", "confidence": 0.9003800988197327}, {"text": "BNC", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.7497574687004089}, {"text": "Europarl", "start_pos": 206, "end_pos": 214, "type": "DATASET", "confidence": 0.9747189879417419}]}, {"text": "A more detailed description of the construction of the data set can be found in.", "labels": [], "entities": []}, {"text": "As evaluation metrics, we used both the SemEval BEST precision metric from the CLWSD task as well as a straightforward accuracy measure.", "labels": [], "entities": [{"text": "BEST precision metric", "start_pos": 48, "end_pos": 69, "type": "METRIC", "confidence": 0.8190287550290426}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9986885190010071}]}, {"text": "The SemEval metric takes into account the frequency weights of the gold standard translations: translations that were picked by different annotators get a higher weight.", "labels": [], "entities": []}, {"text": "For the BEST evaluation, systems can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses.", "labels": [], "entities": [{"text": "BEST", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.6744778156280518}]}, {"text": "In this way, systems that output a lot of guesses are not favoured.", "labels": [], "entities": []}, {"text": "For a more detailed description of the SemEval scoring scheme, we refer to.", "labels": [], "entities": [{"text": "SemEval scoring", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7776919305324554}]}, {"text": "Following variables are used for the SemEval precision formula.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9032106995582581}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.5796223878860474}]}, {"text": "Let H be the set of annotators, T the set of test items and hi the set of responses for an item i \u2208 T for annotator h \u2208 H.", "labels": [], "entities": []}, {"text": "Let Abe the set of items from T where the system provides at least one answer and a i : i \u2208 A the set of guesses from the system for item i.", "labels": [], "entities": []}, {"text": "For each i, we calculate the multiset union (H i ) for all hi for all h \u2208 H and for each unique type (res) in H i that has an associated frequency (f req res ).", "labels": [], "entities": []}, {"text": "P rec = a i :i\u2208A The second metric we use is a straightforward accuracy measure, that divides the number of correct answers by the total amount of test instances.", "labels": [], "entities": [{"text": "accuracy measure", "start_pos": 63, "end_pos": 79, "type": "METRIC", "confidence": 0.9851985275745392}]}, {"text": "As a baseline, we selected the most frequent lemmatized translation that resulted from the automated word alignment (GIZA++).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.6543265134096146}]}, {"text": "We also compare our results with the two winning SemEval-2 systems for the Cross-Lingual Word Sense Disambiguation task, UvT-WSD (that only participated for Dutch and Spanish) and T3-COLEUR.", "labels": [], "entities": [{"text": "Cross-Lingual Word Sense Disambiguation task", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.7908097684383393}]}, {"text": "The UvT-WSD system, that also uses a k-nearest neighbor classifier and a variety of local and global context features, obtained the best scores for Spanish and Dutch in the SemEval CLWSD competition.", "labels": [], "entities": [{"text": "SemEval CLWSD competition", "start_pos": 173, "end_pos": 198, "type": "TASK", "confidence": 0.7503761847813925}]}, {"text": "Although we also use a memory-based learner, our method is different from this system in the way the feature vectors are constructed.", "labels": [], "entities": []}, {"text": "Next to the incorporation of similar local context features, we also include evidence from multiple languages in our feature vector.", "labels": [], "entities": []}, {"text": "For French, Italian and German however, the T3-COLEUR system outperformed the other systems in the SemEval competition.", "labels": [], "entities": []}, {"text": "This system adopts a different approach: during the training phase a monolingual WSD system processes the English input sentence and a word alignment module is used to extract the aligned translation.", "labels": [], "entities": []}, {"text": "The English senses together with their aligned translations (and probability scores) are then stored in a word sense translation table, in which look-ups are performed during the testing phase.", "labels": [], "entities": []}, {"text": "This system also differs from the Uvt-WSD and ParaSense systems in the sense that the word senses are derived from WordNet, whereas the other systems do not use any external resources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.9382293224334717}]}, {"text": "The results for all five classifiers are listed in two tables.", "labels": [], "entities": []}, {"text": "gives an overview of the SemEval-2010 weighted precision scores, whereas shows the more straightforward accuracy figures.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.8985234498977661}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9974132180213928}]}, {"text": "Both tables list the scores averaged overall twenty test words for the baseline (most frequent word alignment), the best SemEval system (for a given language) and the two ParaSense setups: one that exclusively uses automatically generated word alignments, and one that uses the verified word alignment labels.", "labels": [], "entities": []}, {"text": "For both setups we trained three flavors of the ParaSense system (1: local context + translation features, 2: translation features and 3: local context features).", "labels": [], "entities": []}, {"text": "The classification results show that for both setups all three flavors of the ParaSense system easily beat the baseline.", "labels": [], "entities": []}, {"text": "Moreover, the ParaSense system clearly outperforms the winning SemEval systems, except for Spanish where the scores are similar.", "labels": [], "entities": []}, {"text": "As all systems, viz.", "labels": [], "entities": []}, {"text": "the two SemEval systems as well as the three flavors of the ParaSense system, were trained on the same Europarl data, the scores illustrate the potential advantages of using a multilingual approach.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.991981029510498}]}, {"text": "Although we applied a very basic strategy for the selection of our bag-of-words translation features (we did not perform any filtering on the translations except for Part-of-Speech information), we observe that for three languages the full feature vector outperforms the classifier that uses the more traditional WSD local context features.", "labels": [], "entities": []}, {"text": "For Dutch, the classifier that merely uses translation features even outperforms the classifier that uses the local context features.", "labels": [], "entities": []}, {"text": "In previous research (Lefever and Hoste, 2011), we showed that the classifier using evidence from all different languages was constantly better than the ones using lessor no multilingual evidence.", "labels": [], "entities": []}, {"text": "In addition, the scores also degraded relatively to the number of translation features that was used.", "labels": [], "entities": []}, {"text": "As we used a different set of translation features for the latter pilot experiments (we only used the translations of the ambiguous words instead of the full bag-ofwords features we used for the current setup), we need to confirm this trend with more experiments using the current feature sets.", "labels": [], "entities": []}, {"text": "Another important observation is that the classification scores degrade when using the automatically generated word alignments, but only to a minor extent.", "labels": [], "entities": []}, {"text": "This clearly shows the viability of our setup.", "labels": [], "entities": []}, {"text": "Further experiments with different word alignment settings and symmetrisation methods should allow us to further improve the results with the automatically generated word alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7380270957946777}]}, {"text": "Using the nonvalidated labels makes the system very flexible and language-independent, as all steps in the feature vector creation can be run automatically.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SemEval precision scores averaged over all twenty test words", "labels": [], "entities": [{"text": "SemEval", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9557199478149414}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8422080874443054}]}, {"text": " Table 2: Accuracy percentages averaged over all twenty test words", "labels": [], "entities": [{"text": "Accuracy percentages", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9857230186462402}]}]}