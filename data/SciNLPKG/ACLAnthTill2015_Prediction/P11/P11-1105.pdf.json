{"title": [{"text": "A Joint Sequence Translation Model with Integrated Reordering", "labels": [], "entities": [{"text": "Sequence Translation", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.7873434126377106}]}], "abstractContent": [{"text": "We present a novel machine translation model which models translation by a linear sequence of operations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7090793699026108}]}, {"text": "In contrast to the \"N-gram\" model, this sequence includes not only translation but also reordering operations.", "labels": [], "entities": []}, {"text": "Key ideas of our model are (i) anew reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance re-orderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT.", "labels": [], "entities": []}, {"text": "We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results fora French-to-English task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9995519518852234}]}], "introductionContent": [{"text": "We present a novel generative model that explains the translation process as a linear sequence of operations which generate a source and target sentence in parallel.", "labels": [], "entities": []}, {"text": "Possible operations are (i) generation of a sequence of source and target words (ii) insertion of gaps as explicit target positions for reordering operations, and (iii) forward and backward jump operations which do the actual reordering.", "labels": [], "entities": []}, {"text": "The probability of a sequence of operations is defined according to an N-gram model, i.e., the probability of an operation depends on then \u2212 1 preceding operations.", "labels": [], "entities": []}, {"text": "Since the translation (generation) and reordering operations are coupled in a single generative story, the reordering decisions may depend on preceding translation decisions and translation decisions may depend on preceding reordering decisions.", "labels": [], "entities": []}, {"text": "This provides a natural reordering mechanism which is able to deal with local and long-distance reorderings in a consistent way.", "labels": [], "entities": []}, {"text": "Our approach can be viewed as an extension of the N-gram SMT approach) but our model does reordering as an integral part of a generative model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8689351677894592}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the relation of our work to phrase-based and the N-gram SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.664412796497345}]}, {"text": "Section 3 describes our generative story.", "labels": [], "entities": [{"text": "generative", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.9849639534950256}]}, {"text": "Section 4 defines the probability model, which is first presented as a generative model, and then shifted to a discriminative framework.", "labels": [], "entities": []}, {"text": "Section 5 provides details on the search strategy.", "labels": [], "entities": []}, {"text": "Section 6 explains the training process.", "labels": [], "entities": []}, {"text": "Section 7 describes the experimental setup and results.", "labels": [], "entities": []}, {"text": "Section 8 gives a few examples illustrating different aspects of our model and Section 9 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: This Work(Tw) vs Moses (Bl), no-rl = No Re- ordering Limit, rl-6 = Reordering limit 6", "labels": [], "entities": [{"text": "Reordering", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9469098448753357}]}, {"text": " Table 4: Our Systems with Gappy Units, asg = All Gappy  Units, hsg = Heuristic for pruning Gappy Units", "labels": [], "entities": []}, {"text": " Table 5: 10-best Translation Options With & Without  Gaps and using our Heuristic", "labels": [], "entities": [{"text": "Translation Options", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9675549566745758}]}]}