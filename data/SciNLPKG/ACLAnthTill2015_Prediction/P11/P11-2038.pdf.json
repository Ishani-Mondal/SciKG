{"title": [{"text": "Judging Grammaticality with Tree Substitution Grammar Derivations", "labels": [], "entities": [{"text": "Judging Grammaticality", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8675557971000671}]}], "abstractContent": [{"text": "In this paper, we show that local features computed from the derivations of tree substitution grammars-such as the identify of particular fragments, and a count of large and small fragments-are useful in binary grammatical classification tasks.", "labels": [], "entities": [{"text": "binary grammatical classification", "start_pos": 204, "end_pos": 237, "type": "TASK", "confidence": 0.6484723289807638}]}, {"text": "Such features outperform n-gram features and various model scores by a wide margin.", "labels": [], "entities": []}, {"text": "Although they fall short of the performance of the hand-crafted feature set of Charniak and Johnson (2005) developed for parse tree reranking, they do so with an order of magnitude fewer features.", "labels": [], "entities": [{"text": "parse tree reranking", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.7574988206227621}]}, {"text": "Furthermore , since the TSGs employed are learned in a Bayesian setting, the use of their derivations can be viewed as the automatic discovery of tree patterns useful for classification.", "labels": [], "entities": []}, {"text": "On the BLLIP dataset, we achieve an accuracy of 89.9% in discriminating between grammatical text and samples from an n-gram language model.", "labels": [], "entities": [{"text": "BLLIP dataset", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8563869595527649}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995594620704651}]}], "introductionContent": [{"text": "The task of a language model is to provide a measure of the grammaticality of a sentence.", "labels": [], "entities": []}, {"text": "Language models are useful in a variety of settings, for both human and machine output; for example, in the automatic grading of essays, or in guiding search in a machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7021703720092773}]}, {"text": "Language modeling has proved to be quite difficult.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7555181384086609}]}, {"text": "The simplest models, n-grams, are self-evidently poor models of language, unable to (easily) capture or enforce long-distance linguistic phenomena.", "labels": [], "entities": []}, {"text": "However, they are easy to train, are long-studied and well understood, and can be efficiently incorporated into search procedures, such as for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7814528048038483}]}, {"text": "As a result, the output of such text generation systems is often very poor grammatically, even if it is understandable.", "labels": [], "entities": []}, {"text": "Since grammaticality judgments area matter of the syntax of a language, the obvious approach for modeling grammaticality is to start with the extensive work produced over the past two decades in the field of parsing.", "labels": [], "entities": []}, {"text": "This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations.", "labels": [], "entities": []}, {"text": "Following, we conduct experiments in a classification setting, where the task is to distinguish between real text and \"pseudo-negative\" text obtained by sampling from a trigram language model.", "labels": [], "entities": []}, {"text": "Our primary points of comparison are the latent SVM training of, mentioned above, and the extensive set of local and nonlocal feature templates developed by for parse tree reranking.", "labels": [], "entities": []}, {"text": "In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the identity of the fragments used in the derivation, plus simple statistics computed over them.", "labels": [], "entities": []}, {"text": "Since these fragments are in turn learned automatically from a Treebank with a Bayesian model, their usefulness here suggests a greater potential for adapting to other languages and datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with a binary classification task, defined as follows: given a sequence of words, determine whether it is grammatical or not.", "labels": [], "entities": []}, {"text": "We use two datasets: the Wall Street Journal portion of the Penn Treebank (, and the BLLIP '99 dataset, 1 a collection of automatically-parsed sentences from three years of articles from the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal portion", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.9512224048376083}, {"text": "Penn Treebank", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8087681233882904}, {"text": "BLLIP '99 dataset", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.7862769588828087}, {"text": "Wall Street Journal", "start_pos": 191, "end_pos": 210, "type": "DATASET", "confidence": 0.8783175547917684}]}, {"text": "For both datasets, positive examples are obtained from the leaves of the parse trees, retaining their tokenization.", "labels": [], "entities": []}, {"text": "Negative examples were produced from a trigram language model by randomly generating sentences of length no more than 100 so as to match the size of the positive data.", "labels": [], "entities": []}, {"text": "The language model was built with SRILM) using interpolated Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.6203060746192932}]}, {"text": "The average sentence lengths for the positive and negative data were 23.", "labels": [], "entities": []}, {"text": "Each dataset is divided into training, development, and test sets.", "labels": [], "entities": []}, {"text": "For the Treebank, we trained the n-gram language model on sections 2 -21.", "labels": [], "entities": [{"text": "Treebank", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9343899488449097}]}, {"text": "The classifier then used sections 0, 24, and 22 for training, development, and testing, respectively.", "labels": [], "entities": []}, {"text": "For the BLLIP dataset, we followed Cherry and Quirk (2008): we randomly selected 450K sentences to train the n-gram language model, and 50K, 3K, and 3K sentences for classifier training, development, and testing, respectively.", "labels": [], "entities": [{"text": "BLLIP dataset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.8831178247928619}]}, {"text": "All sentences have 100 or fewer words.", "labels": [], "entities": []}, {"text": "contains statistics of the datasets used in our experiments.", "labels": [], "entities": []}, {"text": "To build the classifier, we used liblinear.", "labels": [], "entities": []}, {"text": "A bias of 1 was added to each feature vector.", "labels": [], "entities": []}, {"text": "We varied a cost or regularization parameter between 1e \u2212 5 and 100 in orders of magnitude; at each step, we built a model, evaluating it on the development set.", "labels": [], "entities": []}, {"text": "The model with the highest score was then used to produce the result on the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of sentences (first line) and words  (second line) using for training, development, and test- ing of the classifier. Each set of sentences is evenly split  between positive and negative examples.", "labels": [], "entities": []}]}