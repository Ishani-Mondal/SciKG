{"title": [{"text": "Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation", "labels": [], "entities": [{"text": "Discriminative Feature-Tied Mixture Modeling", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.613600604236126}, {"text": "Statistical Machine Translation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.8540864586830139}]}], "abstractContent": [{"text": "In this paper we present a novel discrimi-native mixture model for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.8313975433508555}]}, {"text": "We model the feature space with a log-linear combination of multiple mixture components.", "labels": [], "entities": []}, {"text": "Each component contains a large set of features trained in a maximum-entropy framework.", "labels": [], "entities": []}, {"text": "All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance.", "labels": [], "entities": []}, {"text": "This approach aims at bridging the gap between the maximum-likelihood training and the discrim-inative training for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9942903518676758}]}, {"text": "It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.700247123837471}]}, {"text": "The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.974926769733429}, {"text": "MT task", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.8845699429512024}]}], "introductionContent": [{"text": "Significant progress has been made in statistical machine translation (SMT) in recent years.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.8537666996320089}]}, {"text": "Among all the proposed approaches, the phrasebased method () has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9935071468353271}]}, {"text": "There exists significant amount of work focused on the improvement of translation performance with better features.", "labels": [], "entities": [{"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.976462185382843}]}, {"text": "The feature set could be either small (at the order of 10), or large (up to millions).", "labels": [], "entities": []}, {"text": "For example, the system described in) is a widely known one using small number of features in a maximum-entropy (log-linear) model).", "labels": [], "entities": []}, {"text": "The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.8158505360285441}]}, {"text": "The feature weights are usually optimized with minimum error rate training (MERT) as in.", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 47, "end_pos": 81, "type": "METRIC", "confidence": 0.8492773686136518}]}, {"text": "Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (.", "labels": [], "entities": [{"text": "MERT-based feature weight optimization", "start_pos": 12, "end_pos": 50, "type": "TASK", "confidence": 0.7161085307598114}, {"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.995408833026886}]}, {"text": "However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive.", "labels": [], "entities": []}, {"text": "In, there are 11K syntactic features proposed fora hierarchical phrase-based system.", "labels": [], "entities": []}, {"text": "The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set.", "labels": [], "entities": [{"text": "Margin Infused Relaxed Algorithm (MIRA)", "start_pos": 41, "end_pos": 80, "type": "METRIC", "confidence": 0.6897424118859428}]}, {"text": "Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to apply the same approach to millions of features due to the data sparseness issue, since the development set is usually small.", "labels": [], "entities": []}, {"text": "In, a maximum entropy (ME) model is proposed, which utilizes millions of features.", "labels": [], "entities": []}, {"text": "All the feature weights are trained with a maximum-likelihood (ML) approach on the full training corpus.", "labels": [], "entities": []}, {"text": "It achieves significantly better performance than a normal phrase-based system.", "labels": [], "entities": []}, {"text": "However, the estimation of feature weights has no direct connection with the final translation perfor-424 mance.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hybrid framework, a discriminative mixture model, to bridge the gap between the ML training and the discriminative training for SMT.", "labels": [], "entities": [{"text": "ML", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.9349388480186462}, {"text": "SMT", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9931434392929077}]}, {"text": "In Section 2, we briefly review the ME baseline of this work.", "labels": [], "entities": [{"text": "ME baseline", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.7336785793304443}]}, {"text": "In Section 3, we introduce the discriminative mixture model that combines various types of features.", "labels": [], "entities": []}, {"text": "In Section 4, we present experimental results on a large-scale Arabic-English MT task with focuses on feature combination, alignment combination, and domain adaptation, respectively.", "labels": [], "entities": [{"text": "MT task", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.8837083280086517}, {"text": "alignment combination", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.8888614773750305}, {"text": "domain adaptation", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.706000566482544}]}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: MT results with individual mixture component  (F1 to F8), baseline, or mixture model.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9673945307731628}, {"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9655805230140686}]}, {"text": " Table 2: MT results with different alignments, baseline,  or mixture model.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9770984649658203}]}]}