{"title": [{"text": "Latent Semantic Word Sense Induction and Disambiguation", "labels": [], "entities": [{"text": "Latent Semantic Word Sense Induction and Disambiguation", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.6147642689091819}]}], "abstractContent": [{"text": "In this paper, we present a unified model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory.", "labels": [], "entities": []}, {"text": "The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space.", "labels": [], "entities": []}, {"text": "The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions; in a similar vein, a particular instance of a word can be dis-ambiguated by determining its most important topical dimensions.", "labels": [], "entities": []}, {"text": "The model is evaluated on the SEMEVAL-2010 word sense induction and disambiguation task, on which it reaches state-of-the-art results.", "labels": [], "entities": [{"text": "SEMEVAL-2010 word sense induction and disambiguation", "start_pos": 30, "end_pos": 82, "type": "TASK", "confidence": 0.6875448922316233}]}], "introductionContent": [{"text": "Word sense induction (WSI) is the task of automatically identifying the senses of words in texts, without the need for handcrafted resources or manually annotated data.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.818281908830007}]}, {"text": "The manual construction of a sense inventory is a tedious and time-consuming job, and the result is highly dependent on the annotators and the domain at hand.", "labels": [], "entities": []}, {"text": "By applying an automatic procedure, we are able to only extract the senses that are objectively present in a particular corpus, and it allows for the sense inventory to be straightforwardly adapted to anew domain.", "labels": [], "entities": []}, {"text": "Word sense disambiguation (WSD), on the other hand, is the closely related task of assigning a sense label to a particular instance of a word in context, using an existing sense inventory.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8098139812548956}]}, {"text": "The bulk of WSD algorithms up till now use pre-defined sense inventories (such as WordNet) that often contain finegrained sense distinctions, which poses serious problems for computational semantic processing.", "labels": [], "entities": [{"text": "WSD", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9626947045326233}, {"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9415408372879028}, {"text": "computational semantic processing", "start_pos": 175, "end_pos": 208, "type": "TASK", "confidence": 0.6820630530516306}]}, {"text": "Moreover, most WSD algorithms take a supervised approach, which requires a significant amount of manually annotated training data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9771212935447693}]}, {"text": "The model presented here induces the senses of words in a fully unsupervised way, and subsequently uses the induced sense inventory for the unsupervised disambiguation of particular occurrences of words.", "labels": [], "entities": []}, {"text": "The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space.", "labels": [], "entities": []}, {"text": "The key idea is that the model combines tight, synonymlike similarity (based on dependency relations) with broad, topical similarity (based on a large 'bag of words' context window).", "labels": [], "entities": []}, {"text": "The intuition in this is that the dependency features can be disambiguated by the topical dimensions identified by the broad contextual features; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions (based on the instance's context words).", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents some previous research on distributional similarity and word sense induction.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.8304166793823242}]}, {"text": "Section 3 gives an overview of our method for word sense induction and disambiguation.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7801081538200378}]}, {"text": "Section 4 provides a quantitative evaluation and comparison to other algorithms in the framework of the word sense induction and disambiguation (WSI/WSD) task.", "labels": [], "entities": [{"text": "word sense induction and disambiguation (WSI/WSD) task", "start_pos": 104, "end_pos": 158, "type": "TASK", "confidence": 0.8299703191627156}]}, {"text": "The last section draws conclusions, and lays out a number of future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task ().", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7035653789838155}, {"text": "SEMEVAL-2010 WSI/WSD task", "start_pos": 94, "end_pos": 119, "type": "DATASET", "confidence": 0.5617514312267303}]}, {"text": "The SEMEVAL-2010 WSI/WSD task is based on a dataset of 100 target words, 50 nouns and 50 verbs.", "labels": [], "entities": [{"text": "SEMEVAL-2010 WSI/WSD task", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7106958866119385}]}, {"text": "For each target word, a training set is provided from which the senses of the word have to be induced without using any other resources.", "labels": [], "entities": []}, {"text": "The training set fora target word consists of a set of target word instances in context (sentences or paragraphs).", "labels": [], "entities": []}, {"text": "The complete training set contains 879,807 instances, viz.", "labels": [], "entities": []}, {"text": "716,945 noun and 162,862 verb instances.", "labels": [], "entities": []}, {"text": "The senses induced during training are used for disambiguation in the testing phase.", "labels": [], "entities": []}, {"text": "In this phase, the system is provided with a test set that consists of unseen instances of the target words.", "labels": [], "entities": []}, {"text": "The test set contains 8,915 instances in total, of which 5,285 nouns and 3,630 verbs.", "labels": [], "entities": []}, {"text": "The instances in the test set are tagged with OntoNotes senses ().", "labels": [], "entities": []}, {"text": "The system needs to disambiguate these instances using the senses acquired during training.", "labels": [], "entities": []}, {"text": "The results of the systems participating in the SEMEVAL-2010 WSI/WSD task are evaluated both in a supervised and in an unsupervised manner.", "labels": [], "entities": [{"text": "SEMEVAL-2010 WSI/WSD task", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.5971873641014099}]}, {"text": "The supervised evaluation in the SEMEVAL-2010 WSI/WSD task follows the scheme of the SEMEVAL-2007 WSI task, with some modifications.", "labels": [], "entities": [{"text": "SEMEVAL-2010 WSI/WSD task", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.5298315167427063}, {"text": "SEMEVAL-2007 WSI task", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.5140231748421987}]}, {"text": "One part of the test set is used as a mapping corpus, which maps the automatically induced clusters to gold standard senses; the other part acts as an evaluation corpus.", "labels": [], "entities": []}, {"text": "The mapping between clusters and gold standard senses is used to tag the evaluation corpus with gold standard tags.", "labels": [], "entities": []}, {"text": "The systems are then evaluated as in a standard WSD task, using recall.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 48, "end_pos": 56, "type": "TASK", "confidence": 0.8088605999946594}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9982157945632935}]}, {"text": "In the unsupervised evaluation, the induced senses are evaluated as clusters of instances which are compared to the sets of instances tagged with the gold standard senses (corresponding to classes).", "labels": [], "entities": []}, {"text": "Two partitions are thus created over the test set of a target word: a set of automatically generated clusters and a set of gold standard classes.", "labels": [], "entities": []}, {"text": "A number of these instances will be members of both one gold standard class and one cluster.", "labels": [], "entities": []}, {"text": "Consequently, the quality of the proposed clustering solution is evaluated by comparing the two groupings and measuring their similarity.", "labels": [], "entities": []}, {"text": "Two evaluation metrics are used during the unsupervised evaluation in order to estimate the quality of the clustering solutions, the V-Measure and the paired F-Score (.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 158, "end_pos": 165, "type": "METRIC", "confidence": 0.9380412101745605}]}, {"text": "V-Measure assesses the quality of a clustering by measuring its homogeneity (h) and its completeness (c).", "labels": [], "entities": []}, {"text": "Homogeneity refers to the degree that each cluster consists of data points primarily belonging to a single gold standard class, while completeness refers to the degree that each gold standard class consists of data points primarily assigned to a single cluster.", "labels": [], "entities": []}, {"text": "V-Measure is the harmonic mean of hand c.", "labels": [], "entities": []}, {"text": "In the paired F-Score (Artiles et al., 2009) evaluation, the clustering problem is transformed into a classification problem (.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.8716880679130554}]}, {"text": "A set of instance pairs is generated from the automatically induced clusters, which comprises pairs of the instances found in each cluster.", "labels": [], "entities": []}, {"text": "Similarly, a set of instance pairs is created from the gold standard classes, containing pairs of the instances found in each class.", "labels": [], "entities": []}, {"text": "Precision is then defined as the number of common instance pairs between the two sets to the total number of pairs in the clustering solution (cf. formula 8).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9815665483474731}]}, {"text": "Recall is defined as the number of common instance pairs between the two sets to the total number of pairs in the gold standard (cf. formula 9).", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9813032746315002}]}, {"text": "Precision and recall are finally combined to produce the harmonic mean (cf. formula 10).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9915048480033875}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9990556836128235}]}, {"text": "The obtained results are also compared to two baselines.", "labels": [], "entities": []}, {"text": "The most frequent sense (MFS) baseline groups all testing instances of a target word into one cluster.", "labels": [], "entities": []}, {"text": "The Random baseline randomly assigns an instance to one of the clusters.", "labels": [], "entities": []}, {"text": "This baseline is executed five times and the results are averaged.", "labels": [], "entities": []}, {"text": "In table 1, we present the performance of a number of algorithms on the V-measure.", "labels": [], "entities": []}, {"text": "We compare our V-measure scores with the scores of the best-ranked systems in the SEMEVAL 2010 WSI/WSD task, both for the complete data set and for nouns and verbs separately.", "labels": [], "entities": [{"text": "SEMEVAL 2010 WSI/WSD task", "start_pos": 82, "end_pos": 107, "type": "TASK", "confidence": 0.4948633362849553}]}, {"text": "The fourth column shows the average number of clusters induced in the test set by each algorithm.", "labels": [], "entities": []}, {"text": "The MFS baseline has a V-Measure equal to 0, since by definition its completeness is 1 and its homogeneity is 0.", "labels": [], "entities": [{"text": "MFS baseline", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.711420327425003}]}, {"text": "NMF con -our model that takes a conservative approach in the induction of candidate senses -does not beat the random baseline.", "labels": [], "entities": [{"text": "NMF con", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7024649679660797}]}, {"text": "NMF lib -our model that is more liberal in inducing senses -reaches better results.", "labels": [], "entities": []}, {"text": "With 11.8%, it scores similar to other algorithms that induce a similar average number of clusters, such as Duluth-WSI.", "labels": [], "entities": []}, {"text": "Pedersen has shown that the V-Measure tends to favour systems producing a higher number of clusters than the number of gold standard senses.", "labels": [], "entities": []}, {"text": "This is reflected in the scores of our models as well.", "labels": [], "entities": []}, {"text": "In the supervised evaluation, the automatically induced clusters are mapped to gold standard senses, using the mapping corpus (i.e. one part of the test set).", "labels": [], "entities": []}, {"text": "The obtained mapping is used to tag the evaluation corpus (i.e. the other part of the test set) with gold standard tags, which means that the methods are evaluated in a standard WSD task.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 178, "end_pos": 186, "type": "TASK", "confidence": 0.8182232081890106}]}, {"text": "NMF lib gets 62.6%, which makes it the best scoring algorithm on the supervised evaluation.", "labels": [], "entities": [{"text": "NMF lib", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7659554779529572}]}, {"text": "NMF con reaches 60.3%, which again indicates that it is in the same ballpark as other algorithms that induce a similar average number of senses.", "labels": [], "entities": [{"text": "NMF con", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6152949631214142}]}, {"text": "Some doubts have been cast on the representativeness of the supervised recall results as well.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9733012318611145}]}, {"text": "According to, the supervised learning algorithm that underlies this evaluation method tends to converge to the Most Frequent Sense (MFS) baseline, because the number of senses that the classifier assigns to the test instances is rather low.", "labels": [], "entities": []}, {"text": "We think these shortcomings indicate the need for the development of new evaluation metrics, capable of providing a more accurate evaluation of the performance of WSI systems.", "labels": [], "entities": [{"text": "WSI", "start_pos": 163, "end_pos": 166, "type": "TASK", "confidence": 0.9175924062728882}]}, {"text": "Nevertheless, these metrics still constitute a useful testbed for comparing the performance of different systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unsupervised V-measure evaluation on SE- MEVAL test set", "labels": [], "entities": [{"text": "SE- MEVAL test set", "start_pos": 47, "end_pos": 65, "type": "DATASET", "confidence": 0.7959301710128784}]}, {"text": " Table 2: Unsupervised paired F-score evaluation on SE- MEVAL testset", "labels": [], "entities": [{"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9876325130462646}, {"text": "SE- MEVAL testset", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.697436511516571}]}]}