{"title": [{"text": "Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment", "labels": [], "entities": [{"text": "ITG-based Word Alignment", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6964683334032694}]}], "abstractContent": [{"text": "Word alignment has an exponentially large search space, which often makes exact inference infeasible.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6994745284318924}]}, {"text": "Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.7939125001430511}]}, {"text": "However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9960450530052185}]}, {"text": "In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning.", "labels": [], "entities": [{"text": "parsing", "start_pos": 98, "end_pos": 105, "type": "TASK", "confidence": 0.9688624739646912}]}, {"text": "We also propose a variant of the grammar which eliminates those ambiguities.", "labels": [], "entities": []}, {"text": "Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.", "labels": [], "entities": []}], "introductionContent": [{"text": "In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.640999307235082}, {"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7777643799781799}]}, {"text": "The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.7343619763851166}]}, {"text": "Recently, inversion transduction grammars (, namely ITG, have been used to constrain the search space for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.78656005859375}]}, {"text": "ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence.", "labels": [], "entities": [{"text": "ITG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.824187159538269}]}, {"text": "The most general case of the ITG family is the bracketing transduction grammar [AA] denotes a monotone concatenation and AA denotes an inverted concatenation.", "labels": [], "entities": [{"text": "AA", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9852392673492432}]}, {"text": "(BTG,), which has only one nonterminal symbol.", "labels": [], "entities": [{"text": "BTG", "start_pos": 1, "end_pos": 4, "type": "DATASET", "confidence": 0.6877095103263855}]}, {"text": "Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment.", "labels": [], "entities": []}, {"text": "This is often referred to as the spurious ambiguity problem.", "labels": [], "entities": []}, {"text": "Calculating and saving those derivations will slowdown the parsing speed significantly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.9788244962692261}]}, {"text": "Furthermore, spurious derivations may fill up the n-best list and supersede potentially good results, making it harder to find the best alignment.", "labels": [], "entities": []}, {"text": "Besides, over-counting those spurious derivations will also affect the likelihood estimation.", "labels": [], "entities": [{"text": "likelihood estimation", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.953761637210846}]}, {"text": "In order to reduce spurious derivations, Wu (1997),, propose different variations of the grammar.", "labels": [], "entities": []}, {"text": "These grammars have different behaviors in parsing efficiency and accuracy, but so far no detailed comparison between them has been done.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9703558683395386}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9988085031509399}]}, {"text": "In this paper, we formally analyze alignments under ITG constraints and the different causes of spurious ambiguity for those alignments.", "labels": [], "entities": []}, {"text": "We do an empirical study of the influence of spurious ambiguity on parsing and discriminative learning by comparing different grammars in both synthetic and realdata experiments.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first in-depth analysis on this specific issue.", "labels": [], "entities": []}, {"text": "A new variant of the grammar is proposed, which efficiently removes all spurious ambiguities.", "labels": [], "entities": []}, {"text": "Our grammar shows advantages over previous ones in both experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We automatically generated 1000 fully aligned ITG alignments of length 20 by generating random permutations first and checking ITG constraints using a linear time algorithm).", "labels": [], "entities": []}, {"text": "Sparser alignments were generated by random removal of alignment links according to a given null-aligned word ratio.", "labels": [], "entities": []}, {"text": "Four grammars were used to parse these alignments, namely LG (  the 10-best alignments for sentence pairs that have 10% of words unaligned, the top 109 HaG derivations should be generated, while the top 10 LiuG or LGFN derivations are already enough.", "labels": [], "entities": []}, {"text": "shows the total parsing time using each grammar.", "labels": [], "entities": []}, {"text": "LG and HaG showed better performances when most of the words were aligned because their grammars are simpler and less constrained.", "labels": [], "entities": [{"text": "HaG", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.9171009063720703}]}, {"text": "However, when the number of null-aligned words increased, the parsing times for LG and HaG became much longer, caused by the calculation of the large number of spurious derivations.", "labels": [], "entities": [{"text": "HaG", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9068577289581299}]}, {"text": "Parsings using LG for 10 and 15 percent of null-aligned words took around 15 and 80 minutes, respectively, which cannot be plotted in the same scale with other grammars.", "labels": [], "entities": []}, {"text": "The parsing times of LGFN and LiuG also slowly increased, but parsing LGFN consistently took less time than LiuG.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9768331050872803}, {"text": "LiuG", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9425485134124756}, {"text": "parsing LGFN", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.8158832788467407}, {"text": "LiuG", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.9790883660316467}]}, {"text": "It should be noticed that the above results came from parsing according to some given alignment.", "labels": [], "entities": [{"text": "parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.9741095900535583}]}, {"text": "When searching without knowing the correct alignment, it is possible for every word to stay unaligned, which makes spurious ambiguity a much more serious issue.", "labels": [], "entities": []}, {"text": "To further study how spurious ambiguity affects the discriminative learning, we implemented a framework following.", "labels": [], "entities": []}, {"text": "We used a log-linear model, with features like IBM model1 probabilities (collected from FBIS data), relative distances, matchings of high frequency words, matchings of pos-tags, etc.", "labels": [], "entities": [{"text": "FBIS data", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.8198466897010803}]}, {"text": "Online training was performed using the margin infused relaxed algorithm (), MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9124341011047363}]}, {"text": "For each sentence pair (e, f ), we optimized with alignment results generated from the nbest parsing results.", "labels": [], "entities": []}, {"text": "Alignment error rate, AER, was used as the loss function.", "labels": [], "entities": [{"text": "Alignment error rate", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9352774024009705}, {"text": "AER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9907296299934387}]}, {"text": "We ran MIRA training for 20 iterations and evaluated the alignments of the best-scored derivations on the test set using the average weights.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.6856346130371094}]}, {"text": "We used the manually aligned Chinese-English corpus in NIST MT02 evaluation.", "labels": [], "entities": [{"text": "NIST MT02 evaluation", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.8490640918413798}]}, {"text": "The first 200 sentence pairs were used for training, and the last 150 for testing.", "labels": [], "entities": []}, {"text": "There are, on average, 10.3% words stay null-aligned in each sentence, but if restricted to sure links the average ratio increases to 22.6%.", "labels": [], "entities": []}, {"text": "We compared training using LGFN with 1-best, 20-best and HaG with 20-best (.", "labels": [], "entities": [{"text": "HaG", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8175129890441895}]}, {"text": "Training with HaG only obtained similar results with 1-best trained LGFN, which demonstrated that spurious ambiguity highly affected the nbest list here, resulting in a less accurate training.", "labels": [], "entities": [{"text": "HaG", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9301859736442566}]}, {"text": "Actually, the 20-best parsing using HaG only generated 4.53 different alignments on average.", "labels": [], "entities": [{"text": "HaG", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9476288557052612}]}, {"text": "20-best training using LGFN converged quickly after the first few iterations and obtained an AER score (17.23) better than other systems, which is also lower than the refined IBM Model 4 result (19.07).", "labels": [], "entities": [{"text": "AER score", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.989096075296402}]}, {"text": "We also trained a similar discriminative model but extended the lexical rule of LGFN to accept at maximum 3 consecutive words.", "labels": [], "entities": []}, {"text": "The model was used to align FBIS data for machine translation experiments.", "labels": [], "entities": [{"text": "FBIS data", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.8024039268493652}, {"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7815301418304443}]}, {"text": "Without initializing by phrases extracted from existing alignments or using complicated block features ( 2009), we further reduced AER on the test set to 12.25.", "labels": [], "entities": [{"text": "AER", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9995668530464172}]}, {"text": "An average improvement of 0.52 BLEU) score and 2.05 TER () score over 5 test sets fora typical phrase-based translation system, Moses (, validated the effectiveness of our experiments.", "labels": [], "entities": [{"text": "BLEU) score", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9717383980751038}, {"text": "TER", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9974835515022278}, {"text": "phrase-based translation", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.703412264585495}]}], "tableCaptions": [{"text": " Table 1: Average #derivations per alignment for LG and  HaG v.s. Percentage of unaligned words. (+ marked  parses have reached the beam size limit of 10000.)", "labels": [], "entities": []}]}