{"title": [{"text": "Faster and Smaller N -Gram Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "N-gram language models area major resource bottleneck in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8071436882019043}]}, {"text": "In this paper , we present several language model implementations that are both highly compact and fast to query.", "labels": [], "entities": []}, {"text": "Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.7595396041870117}]}, {"text": "Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques.", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 94, "end_pos": 114, "type": "DATASET", "confidence": 0.8109615047772726}]}, {"text": "We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.", "labels": [], "entities": []}], "introductionContent": [{"text": "For modern statistical machine translation systems, language models must be both fast and compact.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6428565680980682}]}, {"text": "The largest language models (LMs) can contain as many as several hundred billion n-grams (, so storage is a challenge.", "labels": [], "entities": []}, {"text": "At the same time, decoding a single sentence can trigger hundreds of thousands of queries to the language model, so speed is also critical.", "labels": [], "entities": [{"text": "speed", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.9904940724372864}]}, {"text": "As always, trade-offs exist between time, space, and accuracy, with many recent papers considering smallbut-approximate noisy LMs ( or small-but-slow compressed LMs (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9991841912269592}]}, {"text": "In this paper, we present several lossless methods for compactly but efficiently storing large LMs in memory.", "labels": [], "entities": []}, {"text": "As in much previous work, our methods are conceptually based on tabular trie encodings wherein each n-gram key is stored as the concatenation of one word (here, the last) and an offset encoding the remaining words (here, the context).", "labels": [], "entities": []}, {"text": "After presenting a bit-conscious basic system that typifies such approaches, we improve on it in several ways.", "labels": [], "entities": []}, {"text": "First, we show how the last word of each entry can be implicitly encoded, almost entirely eliminating its storage requirements.", "labels": [], "entities": []}, {"text": "Second, we show that the deltas between adjacent entries can be efficiently encoded with simple variable-length encodings.", "labels": [], "entities": []}, {"text": "Third, we investigate block-based schemes that minimize the amount of compressed-stream scanning during lookup.", "labels": [], "entities": []}, {"text": "To speedup our language models, we present two approaches.", "labels": [], "entities": []}, {"text": "The first is a front-end cache.", "labels": [], "entities": []}, {"text": "Caching itself is certainly not new to language modeling, but because well-tuned LMs are essentially lookup tables to begin with, naive cache designs only speedup slower systems.", "labels": [], "entities": []}, {"text": "We present a direct-addressing cache with a fast key identity check that speeds up our systems (or existing fast systems like the widelyused, speed-focused SRILM) by up to 300%.", "labels": [], "entities": []}, {"text": "Our second speed-up comes from a more fundamental change to the language modeling interface.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6875928640365601}]}, {"text": "Where classic LMs take word tuples and produce counts or probabilities, we propose an LM that takes a word-and-context encoding (so the context need not be re-looked up) and returns both the probability and also the context encoding for the suffix of the original query.", "labels": [], "entities": []}, {"text": "This setup substantially accelerates the scrolling queries issued by decoders, and also exploits language model state equivalence.", "labels": [], "entities": []}, {"text": "Overall, we are able to store the 4 billion n-grams of the Google Web1T () cor-pus, with associated counts, in 10 GB of memory, which is smaller than state-of-the-art lossy language model implementations, and significantly smaller than the best published lossless implementation (.", "labels": [], "entities": []}, {"text": "We are also able to simultaneously outperform SRILM in both total size and speed.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.3818906247615814}]}, {"text": "Our LM toolkit, which is implemented in Java and compatible with the standard ARPA file formats, is available on the web.", "labels": [], "entities": [{"text": "ARPA file formats", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9088854988416036}]}], "datasetContent": [{"text": "We tested our three implementations (HASH, SORTED, and COMPRESSED) on the WMT2010 language model.", "labels": [], "entities": [{"text": "HASH", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.912854790687561}, {"text": "COMPRESSED", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9500354528427124}, {"text": "WMT2010 language model", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.8734568556149801}]}, {"text": "For this language model, there are about 80 million unique probability/back-off pairs, so v \u2248 36.", "labels": [], "entities": []}, {"text": "Note that here v includes both the cost per key of storing the value rank as well as the (amortized) cost of storing two 32 bit floating point numbers (probability and back-off) for each unique value.", "labels": [], "entities": []}, {"text": "The results are shown in  We compare against three baselines.", "labels": [], "entities": []}, {"text": "The first two, SRILM-H and SRILM-S, refer to the hash tableand sorted array-based trie implementations provided by SRILM.", "labels": [], "entities": [{"text": "SRILM-H", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.8288581371307373}, {"text": "SRILM-S", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.8041468262672424}, {"text": "SRILM", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.8975852131843567}]}, {"text": "The third baseline is the TightlyPacked Trie (TPT) implementation of.", "labels": [], "entities": []}, {"text": "Because this implementation is not freely available, we use their published memory usage in bytes per n-gram on a language model of similar size and project total usage.", "labels": [], "entities": []}, {"text": "The memory usage of all of our models is considerably smaller than SRILM -our HASH implementation is about 25% the size of SRILM-H, and our SORTED implementation is about 25% the size of SRILM-S.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.617536187171936}, {"text": "SRILM-H", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.8893360495567322}, {"text": "SRILM-S", "start_pos": 187, "end_pos": 194, "type": "DATASET", "confidence": 0.9394928812980652}]}, {"text": "Our COMPRESSED implementation is also smaller than the state-of-the-art compressed TPT implementation.", "labels": [], "entities": []}, {"text": "In, we show the results of our COM-PRESSED implementation on WEB1T and against two baselines.", "labels": [], "entities": [{"text": "WEB1T", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.964153528213501}]}, {"text": "The first is compression of the ASCII text count files using gzip, and the second is the Tiered Minimal Perfect Hash (T-MPHR) of.", "labels": [], "entities": [{"text": "Tiered Minimal Perfect Hash (T-MPHR)", "start_pos": 89, "end_pos": 125, "type": "METRIC", "confidence": 0.6891042121819088}]}, {"text": "The latter is a lossy compression technique based on Bloomier filters) and additional variable-length encoding that achieves the best published compression of WEB1T to date.", "labels": [], "entities": [{"text": "WEB1T", "start_pos": 159, "end_pos": 164, "type": "DATASET", "confidence": 0.9204366207122803}]}, {"text": "Our COMPRESSED implementation is even smaller than T-MPHR, despite using a lossless compression technique.", "labels": [], "entities": []}, {"text": "Note that since T-MPHR uses a lossy encoding, it is possible to reduce the storage requirements arbitrarily at the cost of additional errors in the model.", "labels": [], "entities": []}, {"text": "We quote here the storage required when keys 7 are encoded using 12-bit hash codes, which gives a false positive rate of about 2 \u221212 =0.02%.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 98, "end_pos": 117, "type": "METRIC", "confidence": 0.7269623676935831}]}, {"text": "We first measured pure query speed by logging all LM queries issued by a decoder and measuring the time required to query those n-grams in isolation.", "labels": [], "entities": []}, {"text": "We used the the Joshua decoder 8 with the WMT2010 model to generate queries for the first 100 sentences of the French 2008 News test set.", "labels": [], "entities": [{"text": "Joshua decoder 8", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.8744641542434692}, {"text": "WMT2010", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9015975594520569}, {"text": "French 2008 News test set", "start_pos": 111, "end_pos": 136, "type": "DATASET", "confidence": 0.9778082370758057}]}, {"text": "This produced about 30 million queries.", "labels": [], "entities": []}, {"text": "We measured the time 9 required to perform each query in order with and without our direct-mapped caching, not including anytime spent on file I/O.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "As expected, HASH is the fastest of our implementations, and comparable 10 in speed to SRILM-H, but using sig- We used a grammar trained on all French-English data provided for WMT 2010 using the make scripts provided at http://sourceforge.net/projects/joshua/files /joshua/1.3/wmt2010-experiment.tgz/download All experiments were performed on an Amazon EC2 HighMemory Quadruple Extra Large instance, with an Intel Xeon X5550 CPU running at 2.67GHz and 8 MB of cache.", "labels": [], "entities": [{"text": "French-English data provided for WMT 2010", "start_pos": 144, "end_pos": 185, "type": "DATASET", "confidence": 0.6686866879463196}]}, {"text": "Because we implemented our LMs in Java, we issued queries to SRILM via Java Native Interface (JNI) calls, which introduces a performance overhead.", "labels": [], "entities": []}, {"text": "When called natively, we found that SRILM was about 200 ns/query faster.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.6073647737503052}]}, {"text": "SORTED is slower but of course more memory efficient, and COMPRESSED is the slowest but also the most compact representation.", "labels": [], "entities": []}, {"text": "In HASH+SCROLL, we issued queries to the language model using the context encoding, which speeds up queries substantially.", "labels": [], "entities": [{"text": "SCROLL", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.41353365778923035}]}, {"text": "Finally, we note that our direct-mapped cache is very effective.", "labels": [], "entities": []}, {"text": "The query speed of all models is boosted substantially.", "labels": [], "entities": []}, {"text": "In particular, our COMPRESSED implementation with caching is nearly as fast as SRILM-H without caching, and even the already fast HASH implementation is 300% faster in raw query speed with caching enabled.", "labels": [], "entities": [{"text": "SRILM-H", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.7271026968955994}]}, {"text": "We also measured the effect of LM performance on overall decoder performance.", "labels": [], "entities": []}, {"text": "We modified Joshua to optionally use our LM implementations during decoding, and measured the time required to decode all 2051 sentences of the 2008 News test set.", "labels": [], "entities": [{"text": "2008 News test set", "start_pos": 144, "end_pos": 162, "type": "DATASET", "confidence": 0.8738507926464081}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Without caching, SRILM-H and HASH were comparable in speed, while COMPRESSED introduces a performance penalty.", "labels": [], "entities": [{"text": "SRILM-H", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.661834716796875}, {"text": "HASH", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.5677850842475891}, {"text": "speed", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9845879077911377}, {"text": "COMPRESSED", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.5139931440353394}]}, {"text": "With caching enabled, overall decoder speed is improved for both HASH and SRILM-H, while the COMPRESSED implementation is only about 50% slower that the others.", "labels": [], "entities": [{"text": "HASH", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.7970989346504211}, {"text": "SRILM-H", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.7711413502693176}]}], "tableCaptions": [{"text": " Table 1: Sizes of the two language models used in our  experiments.", "labels": [], "entities": []}, {"text": " Table 2: Memory usages of several language model im- plementations on the WMT2010 language model. A   *  *  indicates that the storage in bytes per n-gram is re- ported for a different language model of comparable size,  and the total size is thus a rough projection.", "labels": [], "entities": [{"text": "WMT2010 language model", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.9456371068954468}, {"text": "A", "start_pos": 99, "end_pos": 100, "type": "METRIC", "confidence": 0.9611514806747437}]}, {"text": " Table 3: Memory usages of several language model im- plementations on the WEB1T. A  \u2020 indicates lossy com- pression.", "labels": [], "entities": [{"text": "WEB1T", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9839545488357544}]}]}