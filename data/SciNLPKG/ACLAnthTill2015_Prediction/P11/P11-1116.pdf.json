{"title": [{"text": "Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining", "labels": [], "entities": [{"text": "Nonlinear Evidence Fusion", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6335796018441519}, {"text": "Hyponymy Relation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8039931952953339}]}], "abstractContent": [{"text": "This paper focuses on mining the hypon-ymy (or is-a) relation from large-scale, open-domain web documents.", "labels": [], "entities": []}, {"text": "A nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results.", "labels": [], "entities": []}, {"text": "Based on the model, we design a set of evidence combination and propagation algorithms.", "labels": [], "entities": []}, {"text": "These significantly improve the result quality of existing approaches.", "labels": [], "entities": []}, {"text": "Experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision.", "labels": [], "entities": [{"text": "P@5", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9298035303751627}, {"text": "MAP", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.9222286939620972}]}], "introductionContent": [], "datasetContent": [{"text": "Corpus We adopt a publicly available dataset in our experiments: ClueWeb09 . This is a very large dataset collected by Carnegie Mellon University in early 2009 and has been used by several tracks of the Text Retrieval Conference (TREC) . The whole dataset consists of 1.04 billion web pages in ten languages while only those in English, about 500 million pages, are used in our experiments.", "labels": [], "entities": [{"text": "Text Retrieval Conference (TREC)", "start_pos": 203, "end_pos": 235, "type": "TASK", "confidence": 0.8334741691748301}]}, {"text": "The reason for selecting such a dataset is twofold: First, it is a corpus large enough for conducting webscale experiments and getting meaningful results.", "labels": [], "entities": []}, {"text": "Second, since it is publicly available, it is possible for other researchers to reproduce the experiments in this paper.", "labels": [], "entities": []}, {"text": "Term sets Approaches are evaluated by using two sets of selected terms: Wiki200, and Ext100.", "labels": [], "entities": [{"text": "Wiki200", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.8869328498840332}, {"text": "Ext100", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9089220762252808}]}, {"text": "For every term in the term sets, each approach generates a list of hypernym labels, which are manually judged by human annotators.", "labels": [], "entities": []}, {"text": "Wiki200 is constructed by first randomly selecting 400 Wikipedia 6 titles as our candidate terms, with the probability of a title T being selected to be ( ( )), where F(T) is the frequency of T in our data corpus.", "labels": [], "entities": [{"text": "Wiki200", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8311555981636047}, {"text": "F(T)", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9375523775815964}]}, {"text": "The reason of adopting such a probability formula is to balance popular terms and rare ones in our term set.", "labels": [], "entities": []}, {"text": "Then 200 terms are manually selected from the 400 candidate terms, with the principle of maximizing the diversity of terms in terms of length (i.e., number of words) and type (person, location, organization, software, movie, song, animal, plant, etc.).", "labels": [], "entities": []}, {"text": "Wiki200 is further divided into two subsets: Wiki100H and Wiki100L, containing respectively the 100 high-frequency and lowfrequency terms.", "labels": [], "entities": [{"text": "Wiki200", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8461077809333801}, {"text": "Wiki100H", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.8787177801132202}, {"text": "Wiki100L", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.8957918882369995}]}, {"text": "Ext100 is built by first selecting 200 non-Wikipedia-title terms at random from the term-label graph generated by the baseline approach (Formula 3.1), then manually selecting 100 terms.", "labels": [], "entities": []}, {"text": "Some sample terms in the term sets are listed in..", "labels": [], "entities": []}, {"text": "Sample terms in our term sets Annotation For each term in the term set, the top-5 results (i.e., hypernym labels) of various methods are mixed and judged by human annotators.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9701970815658569}]}, {"text": "Each annotator assigns each result item a judgment of \"Good\", \"Fair\" or \"Bad\".", "labels": [], "entities": []}, {"text": "The annotators do not know the method by which a result item is generated.", "labels": [], "entities": []}, {"text": "Six annotators participated in the labeling with a rough speed of 15 minutes per term.", "labels": [], "entities": [{"text": "labeling", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.9839514493942261}]}, {"text": "We also encourage the annotators to add new good results which are not discovered by any method.", "labels": [], "entities": []}, {"text": "The term sets and their corresponding user annotations are available for download at the following links (dataset ID=data.queryset.semcat01): http://research.microsoft.com/en-us/projects/needleseek/ http://needleseek.msra.cn/datasets/ Evaluation We adopt the following metrics to evaluate the hypernym list of a term generated by each method.", "labels": [], "entities": []}, {"text": "The evaluation score on a term set is the average overall the terms.", "labels": [], "entities": []}, {"text": "Precision@k: The percentage of relevant (good or fair) labels in the top-k results (labels judged as \"Fair\" are counted as 0.5) Recall@k: The ratio of relevant labels in the topk results to the total number of relevant labels R-Precision: Precision@R where R is the total number of labels judged as \"Good\" Mean average precision (MAP): The average of precision values at the positions of all good or fair results Before annotation and evaluation, the hypernym list generated by each method for each term is preprocessed to remove duplicate items.", "labels": [], "entities": [{"text": "Recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.990852415561676}, {"text": "Mean average precision (MAP)", "start_pos": 306, "end_pos": 334, "type": "METRIC", "confidence": 0.9555311898390452}, {"text": "precision", "start_pos": 351, "end_pos": 360, "type": "METRIC", "confidence": 0.9728931784629822}]}, {"text": "Two hypernyms are called duplicate items if they share the same headword (e.g., \"military conflict\" and \"conflict\").", "labels": [], "entities": []}, {"text": "For duplicate hypernyms, only the first (i.e., the highest ranked one) in the list is kept.", "labels": [], "entities": []}, {"text": "The goal with such a preprocessing step is to partially consider results diversity in evaluation and to make a more meaningful comparison among different methods.", "labels": [], "entities": []}, {"text": "Consider two hypernym lists for \"subway\": List-1: restaurant; chain restaurant; worldwide chain restaurant; franchise; restaurant franchise\u2026 List-2: restaurant; franchise; transportation; company; fast food\u2026 There are more detailed hypernyms in the first list about \"subway\" as a restaurant or a franchise; while the second list covers a broader range of meanings for the term.", "labels": [], "entities": []}, {"text": "It is hard to say which is better (without considering the upper-layer applications).", "labels": [], "entities": []}, {"text": "With this preprocessing step, we keep our focus on short hypernyms rather than detailed ones.", "labels": [], "entities": []}, {"text": "We first compare the evaluation results of different evidence fusion methods mentioned in Section 4.1.", "labels": [], "entities": [{"text": "evidence fusion", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7283411026000977}]}, {"text": "In, Linear means that Formula 3.1 is used to calculate label scores, whereas Log and PNorm represent our nonlinear approach with Formulas 4.11 and 4.12 being utilized.", "labels": [], "entities": []}, {"text": "The performance improvement numbers shown in the table are based on the linear version; and the upward pointing arrows indicate relative percentage improvement over the baseline.", "labels": [], "entities": []}, {"text": "From the table, we can see that the nonlinear methods outperform the linear ones on the Wiki200 term set.", "labels": [], "entities": [{"text": "Wiki200 term set", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.9398901263872782}]}, {"text": "It is interesting to note that the performance improvement is more significant on Wiki100H, the set of high frequency terms.", "labels": [], "entities": [{"text": "Wiki100H", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.907886803150177}]}, {"text": "By examining the labels and supporting sentences for the terms in each term set, we find that for many low-frequency terms (in Wiki100L), there are only a few supporting sentences (corresponding to one or two patterns).", "labels": [], "entities": []}, {"text": "So the scores computed by various fusion algorithms tend to be similar.", "labels": [], "entities": []}, {"text": "In contrast, more supporting sentences can be discovered for high-frequency terms.", "labels": [], "entities": []}, {"text": "Much information is contained in the sentences about the hypernyms of the high-frequency terms, but the linear function of Formula 3.1 fails to make effective use of it.", "labels": [], "entities": []}, {"text": "The two nonlinear methods achieve better performance by appropriately modeling the dependency between supporting sentences and computing the log-probability gain in a better way.", "labels": [], "entities": []}, {"text": "The comparison of the linear and nonlinear methods on the Ext100 term set is shown in.", "labels": [], "entities": [{"text": "Ext100 term set", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.961555540561676}]}, {"text": "Please note that the terms in Ext100 do not appear in Wikipedia titles.", "labels": [], "entities": [{"text": "Ext100", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9249842166900635}]}, {"text": "Thanks to the scale of the data corpus we are using, even the baseline approach achieves reasonably good performance.", "labels": [], "entities": []}, {"text": "Please note that the terms (refer to) we are using are \"harder\" than those adopted for evaluation in many existing papers.", "labels": [], "entities": []}, {"text": "Again, the results quality is improved with the nonlinear methods, although the performance improvement is not big due to the reason that most terms in Ext100 are rare.", "labels": [], "entities": [{"text": "Ext100", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.911338746547699}]}, {"text": "Please note that the recall (R@1, R@5) in this paper is pseudo-recall, i.e., we treat the number of known relevant (Good or Fair) results as the total number of relevant ones.", "labels": [], "entities": [{"text": "recall (R@1", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.8019709944725036}]}, {"text": "The parameter pin the PNorm method is related to the degree of correlations among supporting sentences.", "labels": [], "entities": []}, {"text": "The linear method of Formula 3.1 corresponds to the special case of p=1; while p= represents the case that other supporting sentences are fully correlated to the supporting sentence with the maximal log-probability gain.", "labels": [], "entities": []}, {"text": "shows that, for most of the term sets, the best performance is obtained for  The experimental results of evidence propagation are shown in.", "labels": [], "entities": [{"text": "evidence propagation", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.8489831387996674}]}, {"text": "The methods for comparison are, Base: The linear function without propagation.", "labels": [], "entities": []}, {"text": "NL: Nonlinear evidence fusion (PNorm with p=2) without propagation.", "labels": [], "entities": [{"text": "Nonlinear evidence fusion", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6583855251471201}]}, {"text": "LP: Linear propagation, i.e., the linear function is used to combine the evidence of pseudo supporting sentences.", "labels": [], "entities": [{"text": "Linear propagation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6908615976572037}]}], "tableCaptions": [{"text": " Table 2. Evidence dependency estimation for intra- pattern and inter-pattern supporting sentences", "labels": [], "entities": []}, {"text": " Table 4. Performance comparison among various  evidence fusion methods (Term sets: Wiki200 and  Wiki100H; p=2 for PNorm)", "labels": [], "entities": [{"text": "evidence fusion", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7717193961143494}, {"text": "Wiki200", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9252070188522339}, {"text": "Wiki100H", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.8797074556350708}]}, {"text": " Table 5. Performance comparison among various  evidence fusion methods (Term set: Ext100; p=2  for PNorm)", "labels": [], "entities": [{"text": "evidence fusion", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7634949088096619}, {"text": "Term", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9749048948287964}, {"text": "Ext100", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.6699437499046326}]}, {"text": " Table 6. Evidence propagation results (Term set:  Wiki200; Similarity graph: PB; Nonlinear formula:  PNorm)", "labels": [], "entities": [{"text": "Evidence propagation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7759172916412354}]}, {"text": " Table 6.  First, no performance improvement can be ob- tained with the linear propagation method (LP),  while the nonlinear propagation algorithm (NLP)  works quite well in improving both precision and  recall. The results demonstrate the high correlation  between pseudo supporting sentences and the great  potential of using term similarity to improve hy- pernymy extraction. The second observation is that  the NL+NLP approach achieves a much larger per- formance improvement than NL and NLP. Similar  results (omitted due to space limitation) can be  observed on the Ext100 term set.", "labels": [], "entities": [{"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9992990493774414}, {"text": "recall", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9968320727348328}, {"text": "hy- pernymy extraction", "start_pos": 355, "end_pos": 377, "type": "TASK", "confidence": 0.6769334003329277}, {"text": "Ext100 term set", "start_pos": 572, "end_pos": 587, "type": "DATASET", "confidence": 0.9823422829310099}]}, {"text": " Table 8. Combination of PB and DS graphs for  evidence propagation (Term set: Wiki100L)", "labels": [], "entities": [{"text": "evidence propagation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8535651564598083}, {"text": "Term set: Wiki100L", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.7509698867797852}]}, {"text": " Table 9. Combination of PB and DS graphs for  evidence propagation (Term set: Ext100)", "labels": [], "entities": [{"text": "evidence propagation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8708412647247314}]}]}