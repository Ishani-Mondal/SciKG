{"title": [{"text": "Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech", "labels": [], "entities": [{"text": "Automated Scoring of Spontaneous Non-Native Speech", "start_pos": 59, "end_pos": 109, "type": "TASK", "confidence": 0.8039581676324209}]}], "abstractContent": [{"text": "This paper focuses on identifying, extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence.", "labels": [], "entities": []}, {"text": "Our goal is to find effective features, selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses, and to build automatic scoring models based on the most promising features by using machine learning methods.", "labels": [], "entities": []}, {"text": "On human transcriptions with manually annotated clause and sentence boundaries, our best scoring model achieves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 123, "end_pos": 142, "type": "METRIC", "confidence": 0.9834802150726318}]}], "introductionContent": [{"text": "Past efforts directed at automated scoring of speech have used mainly features related to fluency (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g.,;.", "labels": [], "entities": []}, {"text": "While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system.", "labels": [], "entities": []}, {"text": "Examples of such aspects of speech, which are considered part of the construct 1 of \"communicative competence), include grammatical accuracy, syntactic complexity, vocabulary diversity, and aspects of spoken discourse structure, e.g., coherence and cohesion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9151086807250977}]}, {"text": "These different aspects of speaking proficiency are often highly correlated in a non-native speaker (, and so scoring models built solely on features of fluency and pronunciation may achieve reasonably high correlations with holistic human rater scores.", "labels": [], "entities": []}, {"text": "However, it is important to point out that such systems would still be unable to assess many important aspects of the speaking construct and therefore cannot be seen as ideal from a validity point of view.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to address one of these important aspects of spoken language in more detail, namely syntactic complexity.", "labels": [], "entities": []}, {"text": "This paper can be seen as a first step toward including features related to this part of the speaking construct into an already existing automated speech scoring system for spontaneous speech which so far mostly uses features related to fluency and pronunciation (.", "labels": [], "entities": []}, {"text": "We use data from the speaking section of the TOEFL\u00ae Practice Online (TPO) test, which is a low stakes practice test for non-native speakers where they are asked to provide six spontaneous speech samples of about one minute in length each in response to a variety of prompts.", "labels": [], "entities": [{"text": "TOEFL\u00ae Practice Online (TPO)", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.5519474744796753}]}, {"text": "Some prompts maybe simple questions, and others may involve reading or listening to passages first and then answering related questions.", "labels": [], "entities": []}, {"text": "All responses were scored holistically by human raters according to pre-defined scoring rubrics (i.e., specific scoring guidelines) on a scale of 1 to 4, 4 being the highest proficiency level.", "labels": [], "entities": []}, {"text": "In our automated scoring system, the first component is an ASR system that decodes the digitized speech sample, generating a time-annotated hypothesis for every response.", "labels": [], "entities": [{"text": "ASR", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.5630725622177124}]}, {"text": "Next, fluency and pronunciation features are computed based on the ASR output hypotheses, and finally a multiple regression scoring model, trained on human rater scores, computes the score fora given spoken response (see for more details).", "labels": [], "entities": [{"text": "ASR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.839626133441925}]}, {"text": "We conducted the study in three steps: (1) finding important measures of syntactic complexity from second language acquisition (SLA) and English language learning (ELL) literature, and extending this feature set based on our observations of the TPO data in analogous ways; (2) computing features based on transcribed speech responses and selecting features with highest correlations to human rater scores, also considering their comparative values for native speakers taking the same test; and (3) building scoring models for the selected sub-set of the features to generate a proficiency score for each speaker, using all six responses of that speaker.", "labels": [], "entities": [{"text": "TPO data", "start_pos": 245, "end_pos": 253, "type": "DATASET", "confidence": 0.6905538737773895}]}, {"text": "In the remainder of the paper, we will address related work in syntactic complexity (Section 2), introduce the speech data sets of our study (Section 3), describe the methods we used for feature extraction (Section 4), provide the experiment design and results, analyze and discuss the results in Section 6, before concluding the paper (Section 7).", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.7455779910087585}]}], "datasetContent": [{"text": "In the previous section, we identified 17 syntactic features that show promising correlations with human rater speaking proficiency scores.", "labels": [], "entities": []}, {"text": "These features as well as the human-rated scores will be used to build scoring models by using machine learning methods.", "labels": [], "entities": []}, {"text": "As introduced in Section 3, we have one training set (N=137 speakers with all of their responses combined) for model building and five testing sets (N=52 for each of them) for evaluation.", "labels": [], "entities": [{"text": "model building", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.8501203060150146}]}, {"text": "The publicly available machine learning package Weka was used in our experiments ().", "labels": [], "entities": []}, {"text": "We experimented with two algorithms in Weka: multiple regression (called \"LinearRegression\" in Weka) and decision tree (called \"M5P\"in Weka).", "labels": [], "entities": []}, {"text": "The score values to be predicted are real numbers (i.e., non-integer), because we have to compute the average score of one speaker's responses.", "labels": [], "entities": []}, {"text": "Our initial runs showed that decision tree models were consistently outperformed by multiple regression (MR) models and thus decided to only focus on MR models henceforth.", "labels": [], "entities": []}, {"text": "We set the \"AttributeSelectionMethod\" parameter in Weka's LinearRegression algorithm to all 3 of its possible values in turn: (Model-1) M5 method; (Model-2) no attribute selection; and (Model-3) greedy method.", "labels": [], "entities": []}, {"text": "The resulting three multiple regression models were then tested against the five testing sets.", "labels": [], "entities": []}, {"text": "Overall, correlations for all models for the NN-test-1-Hum set were between 0.45 and 0.49, correlations for sets NN-test-2-CB and NN-test-3-SB (human transcript based, and using automated boundaries) around 0.2, and for sets NNtest-4-ASR-CB and NN-test-5-ASR-SB (ASR hypotheses, and using automated boundaries), the correlations were not significant.", "labels": [], "entities": []}, {"text": "Model-2 (using all 17 features) had the highest correlation on NNtest-1-Hum and we provide correlation results of this model in.", "labels": [], "entities": [{"text": "NNtest-1-Hum", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9055210947990417}]}], "tableCaptions": [{"text": " Table 1. Overview of non-native data sets.", "labels": [], "entities": []}, {"text": " Table 2. List of syntactic complexity features selected to be included in building the scoring models.", "labels": [], "entities": []}]}