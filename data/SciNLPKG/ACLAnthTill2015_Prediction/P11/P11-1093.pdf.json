{"title": [{"text": "Algorithm Selection and Model Adaptation for ESL Correction Tasks", "labels": [], "entities": [{"text": "Algorithm Selection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7624298930168152}]}], "abstractContent": [{"text": "We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essential to making progress in ESL error correction-algorithm selection and model adaptation to the first language of the ESL learner.", "labels": [], "entities": [{"text": "correcting errors made by English as a Second Language (ESL) writers", "start_pos": 27, "end_pos": 95, "type": "TASK", "confidence": 0.8704980749350327}, {"text": "ESL error correction-algorithm selection", "start_pos": 160, "end_pos": 200, "type": "TASK", "confidence": 0.862676665186882}, {"text": "model adaptation", "start_pos": 205, "end_pos": 221, "type": "TASK", "confidence": 0.6929458826780319}]}, {"text": "A variety of learning algorithms have been applied to correct ESL mistakes, but often comparisons were made between incompara-ble data sets.", "labels": [], "entities": []}, {"text": "We conduct an extensive, fair comparison of four popular learning methods for the task, reversing conclusions from earlier evaluations.", "labels": [], "entities": []}, {"text": "Our results hold for different training sets, genres, and feature sets.", "labels": [], "entities": []}, {"text": "A second key issue in ESL error correction is the adaptation of a model to the first language of the writer.", "labels": [], "entities": [{"text": "ESL error correction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8959545294443766}]}, {"text": "Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the non-native writers.", "labels": [], "entities": []}, {"text": "We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been a lot of recent work on correcting writing mistakes made by English as a Second Language (ESL) learners ().", "labels": [], "entities": [{"text": "correcting writing mistakes made by English as a Second Language (ESL) learners", "start_pos": 39, "end_pos": 118, "type": "TASK", "confidence": 0.8235962774072375}]}, {"text": "Most of this work has focused on correcting mistakes in article and preposition usage, which are some of the most common error types among nonnative writers of English.", "labels": [], "entities": [{"text": "correcting mistakes in article and preposition usage", "start_pos": 33, "end_pos": 85, "type": "TASK", "confidence": 0.7582252110753741}]}, {"text": "Examples below illustrate some of these errors: 1.", "labels": [], "entities": []}, {"text": "\"They listen to None*/the lecture carefully.\"", "labels": [], "entities": []}, {"text": "2. \"He is an engineer with a passion to*/for what he does.\"", "labels": [], "entities": []}, {"text": "In (1) the definite article is incorrectly omitted.", "labels": [], "entities": []}, {"text": "In (2), the writer uses an incorrect preposition.", "labels": [], "entities": []}, {"text": "Approaches to correcting preposition and article mistakes have adopted the methods of the contextsensitive spelling correction task, which addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there.", "labels": [], "entities": [{"text": "correcting preposition and article mistakes", "start_pos": 14, "end_pos": 57, "type": "TASK", "confidence": 0.826006805896759}, {"text": "contextsensitive spelling correction task", "start_pos": 90, "end_pos": 131, "type": "TASK", "confidence": 0.7284562885761261}]}, {"text": "A candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there}.", "labels": [], "entities": []}, {"text": "Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target, e.g., words and part-of-speech tags.", "labels": [], "entities": []}, {"text": "A classifier is trained on text assumed to be error-free.", "labels": [], "entities": []}, {"text": "At decision time, for each word in text, e.g. there, the classifier predicts the most likely candidate from the corresponding confusion set {their, there}.", "labels": [], "entities": []}, {"text": "Models for correcting article and preposition errors are similarly trained on error-free native English text, where the confusion set includes all articles or prepositions (.", "labels": [], "entities": [{"text": "correcting article and preposition errors", "start_pos": 11, "end_pos": 52, "type": "TASK", "confidence": 0.8069144248962402}]}, {"text": "Although the choice of a particular learning algorithm differs, with the exception of decision trees (, all algorithms used are linear learning algorithms, some discriminative (, some probabilistic (, or \"counting\" (.", "labels": [], "entities": []}, {"text": "While model comparison has not been the goal of the earlier studies, it is quite common to compare systems, even when they are trained on different data sets and use different features.", "labels": [], "entities": [{"text": "model comparison", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7183127105236053}]}, {"text": "Furthermore, since there is no shared ESL data set, systems are also evaluated on data from different ESL sources or even on native data.", "labels": [], "entities": [{"text": "ESL data set", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.7786500553290049}]}, {"text": "Several conclusions have been made when comparing systems developed for ESL correction tasks.", "labels": [], "entities": [{"text": "ESL correction tasks", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.9548578461011251}]}, {"text": "A language model was found to outperform a maximum entropy classifier.", "labels": [], "entities": []}, {"text": "However, the language model was trained on the Gigaword corpus, 17 \u00b7 10 9 words (Linguistic Data Consortium, 2003), a corpus several orders of magnitude larger than the corpus used to train the classifier.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9522817134857178}, {"text": "Linguistic Data Consortium, 2003)", "start_pos": 81, "end_pos": 114, "type": "DATASET", "confidence": 0.8736936151981354}]}, {"text": "Similarly, web-based models built on Google Web1T 5-gram Corpus () achieve better results when compared to a maximum entropy model that uses a corpus 10, 000 times smaller ( . In this work, we compare four popular learning methods applied to the problem of correcting preposition and article errors and evaluate on a common ESL data set.", "labels": [], "entities": [{"text": "Google Web1T 5-gram Corpus", "start_pos": 37, "end_pos": 63, "type": "DATASET", "confidence": 0.7468537986278534}, {"text": "correcting preposition and article errors", "start_pos": 257, "end_pos": 298, "type": "TASK", "confidence": 0.8263252973556519}, {"text": "ESL data set", "start_pos": 324, "end_pos": 336, "type": "DATASET", "confidence": 0.8706440925598145}]}, {"text": "We compare two probabilistic approaches -Na\u00a8\u0131veNa\u00a8\u0131ve Bayes and language modeling; a discriminative algorithm Averaged Perceptron; and a count-based method SumLM (, which, as we show, is very similar to Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, but with a different free coefficient.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7504488825798035}]}, {"text": "We train our models on data from several sources, varying training sizes and feature sets, and show that there are significant differences in the performance of these algorithms.", "labels": [], "entities": []}, {"text": "Contrary to previous results (, we find that when trained on the same data with the same features, Averaged Perceptron achieves the best performance, followed by Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, then the language model, and finally the count-based approach.", "labels": [], "entities": []}, {"text": "Our results hold for 1 These two models also use different features.", "labels": [], "entities": []}, {"text": "training sets of different sizes, genres, and feature sets.", "labels": [], "entities": []}, {"text": "We also explain the performance differences from the perspective of each algorithm.", "labels": [], "entities": []}, {"text": "The second important question that we address is that of adapting the decision to the source language of the writer.", "labels": [], "entities": []}, {"text": "Errors made by non-native speakers exhibit certain regularities.", "labels": [], "entities": []}, {"text": "Adapting a model so that it takes into consideration the specific error patterns of the non-native writers was shown to be extremely helpful in the context of discriminative classifiers ().", "labels": [], "entities": []}, {"text": "However, this method requires generating new training data and training a separate classifier for each source language.", "labels": [], "entities": []}, {"text": "Our key contribution here is a novel, simple, and elegant adaptation method within the framework of the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes algorithm, which yields even greater performance gains.", "labels": [], "entities": []}, {"text": "Specifically, we show how the error patterns of the non-native writers can be viewed as a different distribution on candidate priors in the confusion set.", "labels": [], "entities": []}, {"text": "Following this observation, we train Na\u00a8\u0131veNa\u00a8\u0131ve Bayes in a traditional way, regardless of the source language of the writer, and then, only at decision time, change the prior probabilities of the model from the ones observed in the native training data to the ones corresponding to error patterns in the non-native writer's source language (Section 4).", "labels": [], "entities": []}, {"text": "A related idea has been applied in Word Sense Disambiguation to adjust the model priors to anew domain with different sense distributions).", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7405175169308981}]}, {"text": "The paper has two main contributions.", "labels": [], "entities": []}, {"text": "First, we conduct a fair comparison of four learning algorithms and show that the discriminative approach Averaged Perceptron is the best performing model (Sec. 3).", "labels": [], "entities": []}, {"text": "Our results do not support earlier conclusions with respect to the performance of count-based models () and language models.", "labels": [], "entities": []}, {"text": "In fact, we show that SumLM is comparable to Averaged Perceptron trained with a 10 times smaller corpus, and language model is comparable to Averaged Perceptron trained with a 2 times smaller corpus.", "labels": [], "entities": [{"text": "SumLM", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.5378468632698059}]}, {"text": "The second, and most significant, of our contributions is a novel way to adapt a model to the source language of the writer, without re-training the model (Sec. 4).", "labels": [], "entities": []}, {"text": "As we show, adapting to the source language of the writer provides significant performance improvement, and our new method also performs better than previous, more complicated methods.", "labels": [], "entities": []}, {"text": "Section 2 presents the theoretical component of the linear learning framework.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the experiments, which compare the four learning models.", "labels": [], "entities": []}, {"text": "Section 4 presents the key result of this work, a novel method of adapting the model to the source language of the learner.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Statistics on prepositions and articles in the  ESL data. Column Incorrect denotes the number of  cases judged to be incorrect by the annotator.", "labels": [], "entities": [{"text": "ESL data", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9501630663871765}, {"text": "Incorrect", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.6494364142417908}]}, {"text": " Table 4. The table shows that AP trained on 5 \u00b7 10 6  preposition contexts performs as well as NB trained  on 10 7 (i.e., with twice as much data; the perfor- mance of LM trained on 10 7 contexts is better than  that of AP trained with 10 times less data (10 6 ), but  not as good as that of AP trained with half as much  data (5\u00b710 6 ); AP outperforms SumLM, when the lat- ter uses 10 times more data.", "labels": [], "entities": []}, {"text": " Table 5: Performance Comparison of the four algo- rithms for different training data, training sizes, and win- dow sizes. Each row shows results for training data of the  same size. The last row shows performance on the article  correction task. All other results are for prepositions.", "labels": [], "entities": [{"text": "article  correction task", "start_pos": 221, "end_pos": 245, "type": "TASK", "confidence": 0.8420512477556864}]}, {"text": " Table 6: Effect of Window Size in terms of AAU C. Per- formance improves, as the window increases.", "labels": [], "entities": [{"text": "Window Size", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.6898555606603622}, {"text": "AAU C", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.7072612643241882}]}, {"text": " Table 7: Feature coverage for ESL and native data.  Percentage of test n-gram features that occurred in train- ing. Native refers to data from Wikipedia and NYT. B09  refers to statistics from Bergsma et al. (2009).", "labels": [], "entities": [{"text": "NYT", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.9565405249595642}, {"text": "B09", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9735854864120483}]}, {"text": " Table 8: Examples of adapted candidate priors for  two author's choices -on and at -based on the er- rors made by Chinese learners. Global prior denotes  the probability of the candidate in the standard model  and is based on the relative frequency of the candidate  in native training data. Adapted priors are dependent on  the author's preposition and the author's first language.  Adapted priors for the author's choice are very high.  Other candidates are given higher priors if they often ap- pear as corrections for the author's choice.", "labels": [], "entities": []}, {"text": " Table 9: Adapting to writer's source language. Re- sults are reported in terms of AAU C. NB-adapted is the  model with adapted priors. Results for NB-adapted are  based on 10-fold CV.", "labels": [], "entities": [{"text": "Re- sults", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9497117797533671}, {"text": "AAU C", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.6384305655956268}]}]}