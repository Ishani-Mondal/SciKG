{"title": [{"text": "An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques", "labels": [], "entities": [{"text": "Paraphrase Generation", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7714410424232483}]}], "abstractContent": [{"text": "Paraphrase generation is an important task that has received a great deal of interest recently.", "labels": [], "entities": [{"text": "Paraphrase generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9832685589790344}]}, {"text": "Proposed data-driven solutions to the problem have ranged from simple approaches that make minimal use of NLP tools to more complex approaches that rely on numerous language-dependent resources.", "labels": [], "entities": []}, {"text": "Despite all of the attention, there have been very few direct empirical evaluations comparing the merits of the different approaches.", "labels": [], "entities": []}, {"text": "This paper empirically examines the tradeoffs between simple and sophisticated paraphrase harvesting approaches to help shed light on their strengths and weaknesses.", "labels": [], "entities": [{"text": "paraphrase harvesting", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7251480668783188}]}, {"text": "Our evaluation reveals that very simple approaches fare surprisingly well and have a number of distinct advantages, including strong precision, good coverage, and low redundancy.", "labels": [], "entities": [{"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9988184571266174}, {"text": "coverage", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9725065231323242}]}], "introductionContent": [{"text": "A popular idiom states that \"variety is the spice of life\".", "labels": [], "entities": []}, {"text": "As with life, variety also adds spice and appeal to language.", "labels": [], "entities": []}, {"text": "Paraphrases make it possible to express the same meaning in an almost unbounded number of ways.", "labels": [], "entities": []}, {"text": "While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning.", "labels": [], "entities": []}, {"text": "In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase-and sentence-level paraphrases Many data-driven approaches to the paraphrase problem have been proposed.", "labels": [], "entities": []}, {"text": "The approaches vastly differ in their complexity and the amount of NLP resources that they rely on.", "labels": [], "entities": []}, {"text": "At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools.", "labels": [], "entities": []}, {"text": "Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics.", "labels": [], "entities": []}, {"text": "At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora and may also rely on part-of-speech (POS) taggers, chunkers, parsers, and statistical machine translation tools.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 185, "end_pos": 216, "type": "TASK", "confidence": 0.6512164274851481}]}, {"text": "Constructing large comparable and bilingual corpora is expensive and, in some cases, impossible.", "labels": [], "entities": []}, {"text": "Despite all of the previous research, there have not been any evaluations comparing the quality of simple and sophisticated data-driven approaches for generating paraphrases.", "labels": [], "entities": []}, {"text": "Evaluation is not only important from a practical perspective, but also from a methodological standpoint, as well, since it is often more fruitful to devote attention to building upon the current state-of-the-art as opposed to improving upon less effective approaches.", "labels": [], "entities": []}, {"text": "Although the more sophisticated approaches have garnered considerably more attention from researchers, from a practical perspective, simplicity, quality, and flexibility are the most important properties.", "labels": [], "entities": [{"text": "flexibility", "start_pos": 158, "end_pos": 169, "type": "METRIC", "confidence": 0.9651416540145874}]}, {"text": "But are simple methods adequate enough for the task?", "labels": [], "entities": []}, {"text": "The primary goal of this paper is to take a small step towards addressing the lack of comparative evaluations.", "labels": [], "entities": []}, {"text": "To achieve this goal, we empirically evaluate three previously proposed paraphrase generation techniques, which range from very simple approaches that make use of little-to-no NLP or language-dependent resources to more sophisticated ones that heavily rely on such resources.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.8690449893474579}]}, {"text": "Our evaluation helps develop a better understanding of the strengths and weaknesses of each type of approach.", "labels": [], "entities": []}, {"text": "The evaluation also brings to light additional properties, including the number of redundant paraphrases generated, that future approaches and evaluations may want to consider more carefully.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experimental evaluation is to analyze the effectiveness of a variety of paraphrase generation techniques, ranging from simple to sophisticated.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.9106799066066742}]}, {"text": "Our evaluation focuses on generating paraphrases for verb phrases, which tend to exhibit more variation than other types of phrases.", "labels": [], "entities": []}, {"text": "Furthermore, our interest in paraphrase generation was initially inspired by challenges encountered during research related to machine reading (.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.9770306050777435}, {"text": "machine reading", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7690930664539337}]}, {"text": "Information extraction systems, which are key component of machine reading systems, can use paraphrase technology to automatically expand seed sets of relation triggers, which are commonly verb phrases.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7772287428379059}]}, {"text": "We randomly sampled 50 verb phrases from 1000 news articles about terrorism and another 50 verb phrases from 500 news articles about American football.", "labels": [], "entities": []}, {"text": "Individual occurrences of verb phrases were sampled, which means that more common verb phrases were more likely to be selected and that a given phrase could be selected multiple times.", "labels": [], "entities": []}, {"text": "This sampling strategy was used to evaluate the systems across a realistic sample of phrases.", "labels": [], "entities": []}, {"text": "To obtain a richer class of phrases beyond basic verb groups, we defined verb phrases to be contiguous sequences of tokens that matched the following POS tag pattern: Following the methodology used in previous paraphrase evaluations, we presented annotators with two sentences.", "labels": [], "entities": []}, {"text": "The first sentence was randomly selected from amongst all of the sentences in the evaluation corpus that contain the original phrase.", "labels": [], "entities": []}, {"text": "The second sentence was the same as the first, except the original phrase is replaced with the system generated paraphrase.", "labels": [], "entities": []}, {"text": "Annotators were given the following options, which were adopted from those described by, for each sentence pair: 0) Different meaning; 1) Same meaning; revised is 1 Available at http://www.cs.jhu.edu/ \u02dc ccb/.", "labels": [], "entities": []}, {"text": "grammatically incorrect; and 2) Same meaning; revised is grammatically correct.", "labels": [], "entities": []}, {"text": "shows three example sentence pairs and their corresponding annotations according to the guidelines just described.", "labels": [], "entities": []}, {"text": "Amazon's Mechanical Turk service was used to collect crowdsourced annotations.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk service", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.882722282409668}]}, {"text": "For each paraphrase system, we retrieve (up to) 10 paraphrases for each phrase in the evaluation set.", "labels": [], "entities": []}, {"text": "This yields a total of 6,465 unique (phrase, paraphrase) pairs after pooling results from all systems.", "labels": [], "entities": []}, {"text": "Each Mechanical Turk HIT consisted of 12 sentence pairs.", "labels": [], "entities": [{"text": "Mechanical Turk HIT", "start_pos": 5, "end_pos": 24, "type": "DATASET", "confidence": 0.609945684671402}]}, {"text": "To ensure high quality annotations and help identify spammers, 2 of the 12 sentence pairs per HIT were actually \"hidden tests\" for which the correct answer was known by us.", "labels": [], "entities": []}, {"text": "We automatically rejected any HITs where the worker failed either of these hidden tests.", "labels": [], "entities": []}, {"text": "We also rejected all work from annotators who failed at least 25% of their hidden tests.", "labels": [], "entities": []}, {"text": "We collected a total of 51,680 annotations.", "labels": [], "entities": []}, {"text": "We rejected 65% of the annotations based on the hidden test filtering just described, leaving 18,150 annotations for our evaluation.", "labels": [], "entities": []}, {"text": "Each sentence pair received a minimum of 1, a median of 3, and maximum of 6 annotations.", "labels": [], "entities": []}, {"text": "The raw agreement of the annotators (after filtering) was 77% and the Fleiss' Kappa was 0.43, which signifies moderate agreement.", "labels": [], "entities": [{"text": "Fleiss' Kappa", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.7030066847801208}]}, {"text": "The systems were evaluated in terms of coverage and expected precision at k.", "labels": [], "entities": [{"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9937866926193237}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9961532950401306}]}, {"text": "Coverage is defined as the percentage of phrases for which the system returned at least one paraphrase.", "labels": [], "entities": []}, {"text": "Expected precision at k is the expected number of correct paraphrases amongst the top k returned, and is computed as: where pi is the proportion of positive annotations for item i.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9577786922454834}]}, {"text": "When computing the mean expected precision over a set of input phrases, only those phrases that generate one or more paraphrases is considered in the mean.", "labels": [], "entities": [{"text": "mean expected precision", "start_pos": 19, "end_pos": 42, "type": "METRIC", "confidence": 0.6238557497660319}]}, {"text": "Hence, if precision were to be averaged overall 100 phrases, then systems with poor coverage would perform significantly worse.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988697171211243}]}, {"text": "Thus, one should take a holistic view of the results, rather than focus on coverage or precision in isolation, but consider them, and their respective tradeoffs, together.", "labels": [], "entities": [{"text": "coverage", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9365050196647644}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9627298712730408}]}], "tableCaptions": [{"text": " Table 2: Coverage (C) and expected precision at k (Pk)  under lenient and strict evaluation criteria.", "labels": [], "entities": [{"text": "Coverage (C)", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8644470423460007}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9133702516555786}]}, {"text": " Table 3: Expected precision at k (Pk) when considering  redundancy under lenient and strict evaluation criteria.", "labels": [], "entities": [{"text": "Expected", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9647302031517029}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8975207805633545}]}]}