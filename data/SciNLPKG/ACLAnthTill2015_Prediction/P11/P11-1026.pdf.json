{"title": [], "abstractContent": [{"text": "Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data.", "labels": [], "entities": [{"text": "corpus exploration", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7583754360675812}]}, {"text": "However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.746216893196106}]}, {"text": "In this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic.", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 118, "end_pos": 151, "type": "TASK", "confidence": 0.6952762554089228}]}, {"text": "We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models, as exemplified by probabilistic latent semantic indexing and latent Dirichlet allocation (LDA) ( are unsupervised statistical techniques to discover the thematic topics that permeate a large corpus of text documents.", "labels": [], "entities": []}, {"text": "Topic models have had considerable application beyond natural language processing in computer vision (), biology (, and psychology () in addition to their canonical application to text.", "labels": [], "entities": []}, {"text": "For text, one of the few real-world applications of topic models is corpus exploration.", "labels": [], "entities": [{"text": "corpus exploration", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8088708817958832}]}, {"text": "Unannotated, noisy, and ever-growing corpora are the norm rather than the exception, and topic models offer away to quickly get the gist a large corpus.", "labels": [], "entities": []}, {"text": "1 For examples, see Rexa http://rexa.info/, JSTOR Contrary to the impression given by the tables shown in topic modeling papers, topics discovered by topic modeling don't always make sense to ostensible end users.", "labels": [], "entities": []}, {"text": "Part of the problem is that the objective function of topic models doesn't always correlate with human judgements ().", "labels": [], "entities": []}, {"text": "Another issue is that topic models -with their bagof-words vision of the world -simply lack the necessary information to create the topics as end-users expect.", "labels": [], "entities": []}, {"text": "There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (), syntax (, or authorship.", "labels": [], "entities": []}, {"text": "Similarly, there has been an effort to inject human knowledge into topic models).", "labels": [], "entities": []}, {"text": "However, these area priori fixes.", "labels": [], "entities": []}, {"text": "They don't help a frustrated consumer of topic models staring at a collection of topics that don't make sense.", "labels": [], "entities": []}, {"text": "In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models.", "labels": [], "entities": [{"text": "topic modeling (ITM)", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.8026766121387482}]}, {"text": "In Section 2, we review prior work on creating probabilistic models that incorporate human knowledge, which we extend in Section 3 to apply to ITM sessions.", "labels": [], "entities": []}, {"text": "Section 4 discusses the implementation of this process during the inference process.", "labels": [], "entities": []}, {"text": "Via a motivating example in Section 5, simulated ITM sessions in Section 6, and areal interactive test in Section 7, we demonstrate that our approach is able to focus a user's desires in a topic model, better capture the key properties of a corpus, and capture diverse interests from users on the web.", "labels": [], "entities": []}], "datasetContent": [{"text": "Next, we consider a process for evaluating our ITM using automatically derived constraints.", "labels": [], "entities": []}, {"text": "These constraints are meant to simulate a user with a predefined list of categories (e.g. reviewers for journal submissions, e-mail folders, etc.).", "labels": [], "entities": []}, {"text": "The categories grow more and more specific during the session as the simulated users add more constraint words.", "labels": [], "entities": []}, {"text": "To test the ability of ITM to discover relevant subdivisions in a corpus, we use a dataset with predefined, intrinsic labels and assess how well the discovered latent topic structure can reproduce the corpus's inherent structure.", "labels": [], "entities": []}, {"text": "Specifically, fora corpus with M classes, we use the per-document topic distribution as a feature vector in a supervised classi-252: Five topics from a 20 topic topic model on the editorials from the New York times before adding a constraint (left) and after (right).", "labels": [], "entities": [{"text": "New York times", "start_pos": 200, "end_pos": 214, "type": "DATASET", "confidence": 0.6952068905035654}]}, {"text": "After the constraint was added, which encouraged Russian and Soviet terms to be in the same topic, non-Russian terms gained increased prominence in Topic 1, and \"Moscow\" (which was not part of the constraint) appeared in Topic 20.", "labels": [], "entities": []}, {"text": "The lower the classification error rate, the better the model has captured the structure of the corpus.", "labels": [], "entities": [{"text": "classification error rate", "start_pos": 14, "end_pos": 39, "type": "METRIC", "confidence": 0.8329574267069498}]}], "tableCaptions": []}