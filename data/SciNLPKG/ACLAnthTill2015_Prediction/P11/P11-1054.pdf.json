{"title": [{"text": "In-domain Relation Discovery with Meta-constraints via Posterior Regularization", "labels": [], "entities": [{"text": "Relation Discovery", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8793042600154877}, {"text": "Regularization", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.4634988009929657}]}], "abstractContent": [{"text": "We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain.", "labels": [], "entities": []}, {"text": "Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain.", "labels": [], "entities": []}, {"text": "These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context.", "labels": [], "entities": []}, {"text": "We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference.", "labels": [], "entities": []}, {"text": "Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "Furthermore, we find that a small set of constraints is applicable across the domains , and that using domain-specific constraints can further improve performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we introduce a novel approach for the unsupervised learning of relations and their instantiations from a set of in-domain documents.", "labels": [], "entities": []}, {"text": "Given a collection of news articles about earthquakes, for example, our method discovers relations such as the earthquake's location and resulting damage, and extracts phrases representing the relations' instantiations.", "labels": [], "entities": []}, {"text": "Clusters of similar in-domain documents are increasingly available in forms such as Wikipedia article categories, financial reports, and biographies.", "labels": [], "entities": []}, {"text": "In contrast to previous work, our approach learns from domain-independent meta-constraints on relation expression, rather than supervision specific to particular relations and their instances.", "labels": [], "entities": []}, {"text": "In particular, we leverage the linguistic intuition that documents in a single domain exhibit regularities in how they express their relations.", "labels": [], "entities": []}, {"text": "These regularities occur both in the relations' lexical and syntactic realizations as well as at the level of document structure.", "labels": [], "entities": []}, {"text": "For instance, consider the damage relation excerpted from earthquake articles in.", "labels": [], "entities": []}, {"text": "Lexically, we observe similar words in the instances and their contexts, such as \"destroying\" and \"houses.\"", "labels": [], "entities": []}, {"text": "Syntactically, in two instances the relation instantiation is the dependency child of the word \"destroying.\"", "labels": [], "entities": []}, {"text": "On the discourse level, these instances appear toward the beginning of their respective documents.", "labels": [], "entities": []}, {"text": "In general, valid relations in many domains are characterized by these coherence properties.", "labels": [], "entities": []}, {"text": "We capture these regularities using a Bayesian model where the underlying relations are repre-sented as latent variables.", "labels": [], "entities": []}, {"text": "The model takes as input a constituent-parsed corpus and explains how the constituents arise from the latent variables.", "labels": [], "entities": []}, {"text": "Each relation instantiation is encoded by the variables as a relation-evoking indicator word (e.g., \"destroying\") and corresponding argument constituent (e.g., \"some homes\").", "labels": [], "entities": []}, {"text": "Our approach capitalizes on relation regularity in two ways.", "labels": [], "entities": [{"text": "relation regularity", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7394691407680511}]}, {"text": "First, the model's generative process encourages coherence in the local features and placement of relation instances.", "labels": [], "entities": []}, {"text": "Second, we apply posterior regularization () during inference to enforce higher-level declarative constraints, such as requiring indicators and arguments to be syntactically linked.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two domains previously studied for high-level document structure analysis, news articles about earthquakes and financial markets.", "labels": [], "entities": [{"text": "high-level document structure analysis", "start_pos": 63, "end_pos": 101, "type": "TASK", "confidence": 0.715662881731987}]}, {"text": "Our results demonstrate that we can successfully identify domain-relevant relations.", "labels": [], "entities": []}, {"text": "We also study the importance and effectiveness of the declaratively-specified constraints.", "labels": [], "entities": []}, {"text": "In particular, we find that a small set of declarative constraints are effective across domains, while additional domainspecific constraints yield further benefits.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets and Metrics We evaluate on two datasets, financial market reports and newswire articles about earthquakes, previously used in work on high-level content analysis ().", "labels": [], "entities": []}, {"text": "Finance articles chronicle daily market movements of currencies and stock indexes, and earthquake articles document specific earthquakes.", "labels": [], "entities": []}, {"text": "Constituent parses are obtained automatically using the Stanford parser ( and then converted to dependency parses using the PennConvertor tool In our task, annotation conventions for desired output relations can greatly impact token-level performance, and the model cannot learn to fit a particular convention by looking at example data.", "labels": [], "entities": [{"text": "PennConvertor", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.9658547639846802}]}, {"text": "For example, earthquakes times are frequently reported in both local and GMT, and either maybe arbitrarily chosen as correct.", "labels": [], "entities": []}, {"text": "Moreover, the baseline we  compare against produces lambda calculus formulas rather than spans of text as output, so a token-level comparison requires transforming its output.", "labels": [], "entities": []}, {"text": "For these reasons, we evaluate on both sentencelevel and token-level precision, recall, and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9503235816955566}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9996558427810669}, {"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9984700083732605}]}, {"text": "Precision is measured by mapping every induced relation cluster to its closest gold relation and computing the proportion of predicted sentences or words that are correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9834616184234619}]}, {"text": "Conversely, for recall we map every gold relation to its closest predicted relation and find the proportion of gold sentences or words that are predicted.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.8642194271087646}]}, {"text": "This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7214685380458832}]}, {"text": "Note that sentence-level scores are always at least as high as token-level scores, since it is possible to select a sentence correctly but none of its true relation tokens while the opposite is not possible.", "labels": [], "entities": []}, {"text": "Domain-specific Constraints On top of the crossdomain constraints from Section 5, we study whether imposing basic domain-specific constraints can be beneficial.", "labels": [], "entities": []}, {"text": "The finance dataset is heavily quantitative, so we consider applying a single domain-specific constraint stating that most relation arguments should include a number.", "labels": [], "entities": [{"text": "finance dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7062814384698868}]}, {"text": "Likewise, earthquake articles are typically written with a majority of the relevant information toward the beginning of the document, so its domain-specific constraint is that most relations should occur in the first two sentences of a document.", "labels": [], "entities": []}, {"text": "Note that these domain-specific constraints are not specific to individual relations or instances, but rather encode a preference across all relation types.", "labels": [], "entities": []}, {"text": "In both cases, we again use an 80% threshold without tuning.", "labels": [], "entities": []}, {"text": "Features For indicators, we use the word, part of speech, and word stem.", "labels": [], "entities": []}, {"text": "For arguments, we use the word, syntactic constituent label, the headword of the parent constituent, and the dependency label of the argument to its parent.", "labels": [], "entities": []}, {"text": "Baselines We compare against three alternative unsupervised approaches.", "labels": [], "entities": []}, {"text": "Note that the first two only identify relation-bearing sentences, not the specific words that participate in the relation.", "labels": [], "entities": []}, {"text": "Clustering (CLUTO): A straightforward way of identifying sentences bearing the same relation is to simply cluster them.", "labels": [], "entities": []}, {"text": "We implement a clustering baseline using the CLUTO toolkit with word and part-of-speech features.", "labels": [], "entities": []}, {"text": "As with our model, we set the number of clusters K to the true number of relation types.", "labels": [], "entities": []}, {"text": "Mallows Topic Model (MTM): Another technique for grouping similar sentences is the Mallows-based topic model of.", "labels": [], "entities": []}, {"text": "The datasets we consider here exhibit high-level regularities in content organization, so we expect that a topic model with global constraints could identify plausible clusters of relation-bearing sentences.", "labels": [], "entities": []}, {"text": "Again, K is set to the true number of relation types.", "labels": [], "entities": []}, {"text": "Unsupervised Semantic Parsing (USP): Our final unsupervised comparison is to USP, an unsupervised deep semantic parser introduced by.", "labels": [], "entities": [{"text": "Unsupervised Semantic Parsing (USP)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6733269244432449}, {"text": "USP", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9295397996902466}]}, {"text": "USP induces a lambda calculus representation of an entire corpus and was shown to be competitive with open information extraction approaches (.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6900406380494436}]}, {"text": "We give USP the required Stanford dependency format as input (.", "labels": [], "entities": [{"text": "USP", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8875866532325745}]}, {"text": "We find that the results are sensitive to the cluster granularity prior, so we tune this parameter and report the best-performing runs.", "labels": [], "entities": []}, {"text": "We recognize that USP targets a different output representation than ours: a hierarchical semantic structure over the entirety of a dependency-parsed text.", "labels": [], "entities": []}, {"text": "In contrast, we focus on discovering a limited number K of domain-relevant relations expressed as constituent phrases.", "labels": [], "entities": []}, {"text": "Despite these differences, both methods ultimately aim to capture domain-specific relations expressed with varying verbalizations, and both operate over in-domain input corpora supplemented with syntactic information.", "labels": [], "entities": []}, {"text": "For these reasons, USP provides a clear and valuable point of comparison.", "labels": [], "entities": [{"text": "USP", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9297838807106018}]}, {"text": "For this comparison, we transform USP's lambda calculus formulas to relation spans as follows.", "labels": [], "entities": [{"text": "USP", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.9076371192932129}]}, {"text": "First, we group lambda forms by a combination of core form, argument form, and the parent's core form.", "labels": [], "entities": []}, {"text": "We then filter to the K relations that appear in the most documents.", "labels": [], "entities": []}, {"text": "For token-level evaluation we take the dependency tree fragment corresponding to the lambda form.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"a strong earthquake rocked the Philippines island of Mindoro early Tuesday,\" USP learns that the word \"Tuesday\" has a core form corresponding to words {Tuesday, Wednesday, Saturday}, a parent form corresponding to words {shook, rock, hit, jolt}, and an argument form of TMOD; all phrases with this same combination are grouped as a relation.", "labels": [], "entities": [{"text": "TMOD", "start_pos": 300, "end_pos": 304, "type": "METRIC", "confidence": 0.8485721945762634}]}, {"text": "Training Regimes and Hyperparameters For each run of our model we perform three random restarts to convergence and select the posterior with lowest final free energy.", "labels": [], "entities": []}, {"text": "We fix K to the true number of annotated relation types for both our model and USP and L (the number of document segments) to five.", "labels": [], "entities": [{"text": "USP", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8001582622528076}]}, {"text": "Dirichlet hyperparameters are set to 0.1.'s first two sections present the results of our main evaluation.", "labels": [], "entities": []}, {"text": "For earthquake, the far more difficult domain, our base model with only the domainindependent constraints strongly outperforms all three baselines across both metrics.", "labels": [], "entities": []}, {"text": "For finance, the CLUTO and USP baselines achieve performance comparable to or slightly better than our base model.", "labels": [], "entities": [{"text": "CLUTO", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8841874599456787}, {"text": "USP baselines", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8983336985111237}]}, {"text": "Our approach, however, has the advantage of providing a formalism for seamlessly incorporating additional arbitrary domain-specific constraints.", "labels": [], "entities": []}, {"text": "When we add such constraints (denoted as model+DSC), we achieve consistently higher performance than all baselines across both datasets and metrics, demonstrating that this approach provides a simple and effective framework for injecting domain knowledge into relation discovery.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 260, "end_pos": 278, "type": "TASK", "confidence": 0.8410601615905762}]}], "tableCaptions": [{"text": " Table 3: Top section: our model, with and without domain-specific constraints (DSC). Middle section: The three  baselines. Bottom section: ablation analysis of constraint sets for our model. For all scores, higher is better.", "labels": [], "entities": []}]}