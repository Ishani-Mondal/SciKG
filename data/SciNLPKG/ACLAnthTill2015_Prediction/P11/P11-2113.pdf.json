{"title": [{"text": "Probabilistic Document Modeling for Syntax Removal in Text Summarization", "labels": [], "entities": [{"text": "Probabilistic Document Modeling", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6343801915645599}, {"text": "Syntax Removal in Text Summarization", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.6831080973148346}]}], "abstractContent": [{"text": "Statistical approaches to automatic text sum-marization based on term frequency continue to perform on par with more complex sum-marization methods.", "labels": [], "entities": []}, {"text": "To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words.", "labels": [], "entities": []}, {"text": "The standard approach of using an a priori stopword list tends to result in both undercoverage, where syn-tactical words are seen as semantically relevant , and overcoverage, where words related to content are ignored.", "labels": [], "entities": []}, {"text": "We present a genera-tive probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.5616938918828964}]}, {"text": "This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.", "labels": [], "entities": [{"text": "coverage", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9932071566581726}, {"text": "DUC 2006 and TAC 2010 datasets", "start_pos": 128, "end_pos": 158, "type": "DATASET", "confidence": 0.9230363766352335}, {"text": "ROUGE", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.8934633135795593}]}], "introductionContent": [{"text": "While the dominant problem in Information Retrieval in the first part of the century was finding relevant information within a datastream that is exponentially growing, the problem has arguably transitioned from finding what we are looking for to sifting through it.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.8025977313518524}]}, {"text": "We can now be quite confident that search engines like Google will return several pages relevant to our queries, but rarely does one have time to go through the enormous amount of data that is supplied.", "labels": [], "entities": []}, {"text": "Therefore, automatic text summarization, which aims at providing a shorter representation of the salient parts of a large amount of information, has been steadily growing in both importance and popularity over the last several years.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.5820163488388062}]}, {"text": "The summarization tracks at the Document Understanding Conference (DUC), and its successor the Text Analysis Conference (TAC) 1 , have helped fuel this interest by hosting yearly competitions to promote the advancement of automatic text summarization methods.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9693925976753235}, {"text": "Document Understanding Conference (DUC)", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.745975598692894}, {"text": "Text Analysis Conference (TAC)", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.8450432320435842}, {"text": "text summarization", "start_pos": 232, "end_pos": 250, "type": "TASK", "confidence": 0.6269820183515549}]}, {"text": "The tasks at the DUC and TAC involve taking a set of documents as input and outputting a short summary (either 100 or 250 words, depending on the year) containing what the system deems to be the most important information contained in the original documents.", "labels": [], "entities": [{"text": "DUC", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.9231508374214172}, {"text": "TAC", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.5378553867340088}]}, {"text": "While a system matching human performance will likely require deep language understanding, most existing systems use an extractive, rather than abstractive, approach whereby the most salient sentences are extracted from the original documents and strung together to form an output summary.", "labels": [], "entities": []}, {"text": "In this paper, we present a summarization model based on () that integrates topics and syntax.", "labels": [], "entities": [{"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9632635116577148}]}, {"text": "We show that a simple model that separates syntax and content words and uses the content distribution as a representative model of the important words in a document set can achieve high performance in multi-document summarization, competitive with state-of-the-art summarization systems.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 201, "end_pos": 229, "type": "TASK", "confidence": 0.5930788815021515}]}], "datasetContent": [{"text": "Here we describe our experiments and give quantitative results using the ROUGE automatic text sum- See http://lingpipe.files.wordpress.com/ 2010/07/lda1.pdf for more information.", "labels": [], "entities": [{"text": "ROUGE automatic text sum", "start_pos": 73, "end_pos": 97, "type": "METRIC", "confidence": 0.8615243583917618}]}], "tableCaptions": [{"text": " Table 1: ROUGE Results on the DUC 2006 dataset. Re- sults statistically significantly higher than SumBasic (as  determined by a pairwise t-test with 99% confidence) are  displayed in bold.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9837965369224548}, {"text": "DUC 2006 dataset", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9857186675071716}, {"text": "Re- sults statistically", "start_pos": 49, "end_pos": 72, "type": "METRIC", "confidence": 0.9620263874530792}]}]}