{"title": [{"text": "Fine-Grained Class Label Markup of Search Queries", "labels": [], "entities": []}], "abstractContent": [{"text": "We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries.", "labels": [], "entities": [{"text": "semantic analysis of short text segments", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.8787140150864919}]}, {"text": "Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning.", "labels": [], "entities": [{"text": "Extracting meaning from short text segments", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.9047640860080719}]}, {"text": "Furthermore search queries lack explicit syntax often used to determine intent in question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7337891310453415}]}, {"text": "In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.9035215377807617}, {"text": "class-label extraction", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.756319135427475}]}, {"text": "This class-label correlation (CLC) model admits a robust parallel approximation , allowing it to scale to large amounts of query data.", "labels": [], "entities": []}, {"text": "We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.7554248571395874}]}], "introductionContent": [{"text": "Search queries are generally short and rarely contain much explicit syntax, making query understanding a purely semantic endeavor.", "labels": [], "entities": [{"text": "query understanding", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7419501841068268}]}, {"text": "Furthermore, as in nounphrase understanding, shallow lexical semantics is often irrelevant or misleading; e.g., the query [tropical breeze cleaners] has little to do with island vacations, nor are desert birds relevant to, which refers to a car model.", "labels": [], "entities": [{"text": "nounphrase understanding", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.9047702252864838}]}, {"text": "This paper introduces class-label correlation (CLC), a novel unsupervised approach to extract- * Contributions made during an internship at Google.", "labels": [], "entities": [{"text": "class-label correlation (CLC)", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6001148819923401}]}, {"text": "ing shallow semantic content that combines classbased semantic markup (e.g., roadrunner is a car model) with a latent variable model for capturing weakly compositional interactions between query constituents.", "labels": [], "entities": []}, {"text": "Constituents are tagged with IsA class labels from a large, automatically extracted lexicon, using a probabilistic context free grammar (PCFG).", "labels": [], "entities": []}, {"text": "Correlations between the resulting label\u2192term distributions are captured using a set of latent production rules specified by a hierarchical Dirichlet Process () with latent data groupings.", "labels": [], "entities": []}, {"text": "Concretely, the IsA tags capture the inventory of potential meanings (e.g., jaguar can be labeled as european car or large cat) and relevant constituent spans, while the latent variable model performs sense and theme disambiguation (e.g.,jaguar habitat] would lend evidence for the large cat label).", "labels": [], "entities": []}, {"text": "In addition to broad sense disambiguation, CLC can distinguish closely related usages, e.g., the use of dell in [dell motherboard replacement] and [dell stock price].", "labels": [], "entities": []}, {"text": "1 Furthermore, by employing IsA class labeling as a preliminary step, CLC can account for common non-compositional phrases, such as big apple unlike systems relying purely on lexical semantics.", "labels": [], "entities": [{"text": "IsA class labeling", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6673142910003662}]}, {"text": "Additional examples can be found later, in In addition to improving query understanding, potential applications of CLC include: (1) relation extraction, (2) query substitutions or broad matching (), and (3) classifying other short textual fragments such as SMS messages or tweets.", "labels": [], "entities": [{"text": "query understanding", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8327020704746246}, {"text": "relation extraction", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.8425311148166656}, {"text": "classifying other short textual fragments such as SMS messages or tweets", "start_pos": 207, "end_pos": 279, "type": "TASK", "confidence": 0.6065774478695609}]}, {"text": "We implement a parallel inference procedure for CLC and evaluate it on a sample of 500M search queries along two dimensions: (1) query constituent chunking precision (i.e., how accurate are the inferred spans breaks; cf.,;), and (2) class label assignment precision (i.e., given the query intent, how relevant are the inferred class labels), paying particular attention to cases where queries contain ambiguous constituents.", "labels": [], "entities": [{"text": "query constituent chunking precision", "start_pos": 129, "end_pos": 165, "type": "METRIC", "confidence": 0.4874442145228386}, {"text": "precision", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.5475972890853882}]}, {"text": "CLC compares favorably to several simpler submodels, with gains in performance stemming from coarse-graining related class labels and increasing the number of clusters used to capture between-label correlations.", "labels": [], "entities": []}, {"text": "(Paper organization): Section 2 discusses relevant background, Section 3 introduces the CLC model, Section 4 describes the experimental setup employed, Section 5 details results, Section 6 introduces areas for future work and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our dataset consists of a sample of 450M English queries submitted by anonymous Web users to Figure 3: Distribution in the query corpus, broken down by query length (red/solid=all queries; blue/dashed=queries with ambiguous spans); most queries contain between 2-6 tokens.", "labels": [], "entities": []}, {"text": "The queries have an average of 3.81 tokens per query (1.7B tokens).", "labels": [], "entities": []}, {"text": "Single token queries are removed as the model is incapable of using context to disambiguate their meaning.", "labels": [], "entities": []}, {"text": "shows the distribution of remaining queries.", "labels": [], "entities": []}, {"text": "During training, we include 10 copies of each query (4.5B queries total), allowing an estimate of the Bayes average posterior from a single Gibbs sample.", "labels": [], "entities": []}, {"text": "Query markup is evaluated for phrase-chunking precision (Section 5.1) and label precision (Section 5.2) by human raters across two different samples: (1) an unbiased sample from the original corpus, and (2) a biased sample of queries containing ambiguous spans.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.8318201303482056}, {"text": "label precision", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.8003788590431213}]}, {"text": "Two raters scored a total of 10K labels from 800 spans across 300 queries.", "labels": [], "entities": []}, {"text": "Span labels were marked as incorrect (0.0), badspan (0.0), ambiguous (0.5), or correct (1.0), with numeric scores for label precision as indicated.", "labels": [], "entities": [{"text": "correct (1.0)", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9166628271341324}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.8946823477745056}]}, {"text": "Chunking precision is measured as the percentage of labels not marked as badspan.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9174332618713379}]}, {"text": "We report two sets of precision scores depending on how null labels are handled: Strict evaluation treats null-labeled spans as incorrect, while Normal evaluation removes null-labeled spans from the precision calculation.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 22, "end_pos": 38, "type": "METRIC", "confidence": 0.9721590280532837}, {"text": "precision", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9769168496131897}]}, {"text": "Normal evaluation was included since the simpler models (e.g., CLC-BASE) tend to produce a significantly higher number of null assignments.", "labels": [], "entities": []}, {"text": "Model evaluations were broken down into maximum a posteriori (MAP) and Bayes average estimates.", "labels": [], "entities": [{"text": "maximum a posteriori (MAP)", "start_pos": 40, "end_pos": 66, "type": "METRIC", "confidence": 0.839116503794988}]}, {"text": "MAP estimates are calculated as the single most likely label/cluster assignment across all query copies; all assignments in the sample are averaged 1204", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Chunking and label precision across five models. Confidence intervals are standard error; sparklines  show distribution of precision scores (left is zero, right is one). Hist shows the distribution of human rating  response (log y scale): green/first is correct, blue/second is ambiguous, cyan/third is missing and red/fourth  is incorrect. Spearman's \u03c1 columns give label precision correlations with query length (weak negative corre- lation) and the number of applicable labels (weak to strong positive correlation); dots indicate significance.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.7237182855606079}, {"text": "precision scores", "start_pos": 133, "end_pos": 149, "type": "METRIC", "confidence": 0.9605103731155396}]}]}