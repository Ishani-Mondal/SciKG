{"title": [{"text": "Semi-supervised latent variable models for sentence-level sentiment analysis", "labels": [], "entities": [{"text": "sentence-level sentiment analysis", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8025664786497752}]}], "abstractContent": [{"text": "We derive two variants of a semi-supervised model for fine-grained sentiment analysis.", "labels": [], "entities": [{"text": "fine-grained sentiment analysis", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6836983760197958}]}, {"text": "Both models leverage abundant natural supervision in the form of review ratings, as well as a small amount of manually crafted sentence labels, to learn sentence-level sentiment clas-sifiers.", "labels": [], "entities": []}, {"text": "The proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart.", "labels": [], "entities": []}, {"text": "This allows for highly efficient estimation and inference algorithms with rich feature definitions.", "labels": [], "entities": []}, {"text": "We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence-level sentiment analysis compared to all baselines.", "labels": [], "entities": [{"text": "sentence-level sentiment analysis", "start_pos": 148, "end_pos": 181, "type": "TASK", "confidence": 0.786510556936264}]}], "introductionContent": [], "datasetContent": [{"text": "For the following experiments, we used the same data set and a comparable experimental setup to that of T\u00e4ckstr\u00f6m and McDonald (2011).", "labels": [], "entities": []}, {"text": "We compare the two proposed hybrid models to the fully supervised model of (FineToCoarse) as well as to the soft variant of the coarsely supervised model of T\u00e4ckstr\u00f6m and McDonald (2011) (Coarse).", "labels": [], "entities": [{"text": "FineToCoarse", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.9076462984085083}]}, {"text": "The learning rate was fixed to \u03b7 = 0.001, while we tuned the prior variances, \u03c3 2 , and the number of epochs for each model.", "labels": [], "entities": []}, {"text": "When sampling according to \u03bb during optimization of L I (\u03b8 I ), we cycle through D F and DC deterministically, but shuffle these sets between epochs.", "labels": [], "entities": []}, {"text": "Due to time constraints, we fixed the interpolation factor to \u03bb = 0.1, but tuning this could The annotated test data can be downloaded from http://www.sics.se/people/oscar/datasets.", "labels": [], "entities": []}, {"text": "potentially improve the results of the interpolated model.", "labels": [], "entities": []}, {"text": "For the same reason we allowed a maximum of 30 epochs, for all models, while report a maximum of 75 epochs.", "labels": [], "entities": []}, {"text": "To assess the impact of fully labeled versus coarsely labeled data, we took stratified samples without replacement, of sizes 60, 120, and 240 reviews, from the fully labeled folds and of sizes 15,000 and 143,580 reviews from the coarsely labeled data.", "labels": [], "entities": []}, {"text": "On average each review consists often sentences.", "labels": [], "entities": []}, {"text": "We performed 5-fold stratified cross-validation over the labeled data, while using stratified samples for the coarsely labeled data.", "labels": [], "entities": []}, {"text": "Statistical significance was assessed by a hierachical bootstrap of 95% confidence intervals, using the technique described by. lists sentence-level accuracy along with 95% confidence interval for all tested models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9097723960876465}]}, {"text": "We first note that the interpolated model dominates all other models in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9985033273696899}]}, {"text": "While the cascaded model requires both large amounts of fully labeled and coarsely labeled data, the interpolated model is able to take advantage of both types of data on its own and jointly.", "labels": [], "entities": []}, {"text": "Still, by comparing the fully supervised and the coarsely supervised models, the superior impact of fully labeled over coarsely labeled data is evident.", "labels": [], "entities": []}, {"text": "As can be seen in, when all data is used, the cascaded model outperforms the interpolated model for some recall values, and vice versa, while both models dominate the supervised approach for the full range of recall values.", "labels": [], "entities": [{"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9660399556159973}, {"text": "recall", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.9666215777397156}]}], "tableCaptions": [{"text": " Table 1: Sentence level results for varying numbers of fully labeled (D F ) and coarsely labeled (D C ) reviews. Bold:  significantly better than the FineToCoarse model according to a hierarchical bootstrapped confidence interval, p < 0.05.", "labels": [], "entities": []}, {"text": " Table 2: POS / NEG / NEU sentence-level F 1 -scores per  document category (|D C | = 143,580 and |D F | = 240).", "labels": [], "entities": [{"text": "POS / NEG / NEU sentence-level F 1 -scores", "start_pos": 10, "end_pos": 52, "type": "DATASET", "confidence": 0.6150254756212234}]}]}