{"title": [{"text": "Effective Measures of Domain Similarity for Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9392998218536377}]}], "abstractContent": [{"text": "It is well known that parsing accuracy suffers when a model is applied to out-of-domain data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9868667125701904}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9257863163948059}]}, {"text": "It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001).", "labels": [], "entities": []}, {"text": "Hence, an important task is to select appropriate domains.", "labels": [], "entities": []}, {"text": "However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7700158357620239}]}, {"text": "As more and more data becomes available, automatic ways to select data that is beneficial fora new (unknown) target domain are becoming attractive.", "labels": [], "entities": []}, {"text": "This paper evaluates various ways to automatically acquire related training data fora given test set.", "labels": [], "entities": []}, {"text": "The results show that an unsupervised technique based on topic models is effective-it outperforms random data selection on both languages examined , English and Dutch.", "labels": [], "entities": []}, {"text": "Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The parsing system used in this study is the MST parser (), a state-of-the-art data-driven graph-based dependency parser.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.7380341291427612}]}, {"text": "It is a system that can be trained on a variety of languages given training data in CoNLL format).", "labels": [], "entities": []}, {"text": "Additionally, the parser implements both projective and non-projective parsing algorithms.", "labels": [], "entities": []}, {"text": "The projective algorithm is used for the experiments on English, while the non-projective variant is used for Dutch.", "labels": [], "entities": []}, {"text": "We train the parser using default settings.", "labels": [], "entities": []}, {"text": "MST takes PoS-tagged data as input; we use gold-standard tags in the experiments.", "labels": [], "entities": [{"text": "MST", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6767926812171936}]}, {"text": "We estimate topic models using Latent Dirichlet Allocation ( implemented in the MALLET 4 toolkit.", "labels": [], "entities": [{"text": "MALLET 4 toolkit", "start_pos": 80, "end_pos": 96, "type": "DATASET", "confidence": 0.7209759553273519}]}, {"text": "Like Lippincott et al., we set the number of topics to 100, and otherwise use standard settings (no further optimization).", "labels": [], "entities": []}, {"text": "We experimented with the removal of stopwords, but found no deteriorating effect while keeping them.", "labels": [], "entities": []}, {"text": "Thus, all experiments are carried out on data where stopwords were not removed.", "labels": [], "entities": []}, {"text": "We implemented the similarity measures presented in Section 3.1.", "labels": [], "entities": [{"text": "similarity", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9300633668899536}]}, {"text": "For skew divergence, that requires parameter \u03b1, we set \u03b1 = .99 (close to KL divergence) since that has shown previously to work best).", "labels": [], "entities": [{"text": "skew divergence", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6844553649425507}]}, {"text": "Additionally, we evaluate the approach on English PoS tagging using two different taggers: MXPOST, the MaxEnt tagger of Ratnaparkhi and Citar, 6 a trigram HMM tagger.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.6757663488388062}, {"text": "MXPOST", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.5105708241462708}, {"text": "Ratnaparkhi", "start_pos": 120, "end_pos": 131, "type": "DATASET", "confidence": 0.6806014776229858}]}, {"text": "In all experiments, parsing performance is measured as Labeled Attachment Score (LAS), the percentage of tokens with correct dependency edge and label.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9705773591995239}, {"text": "Labeled Attachment Score (LAS)", "start_pos": 55, "end_pos": 85, "type": "METRIC", "confidence": 0.8807778656482697}]}, {"text": "To compute LAS, we use the CoNLL 2007 evaluation script 7 with punctuation tokens excluded from scoring (as was the default setting in CoNLL 2006).", "labels": [], "entities": [{"text": "LAS", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9386336207389832}, {"text": "CoNLL 2007 evaluation script 7", "start_pos": 27, "end_pos": 57, "type": "DATASET", "confidence": 0.9465920805931092}]}, {"text": "PoS tagging accuracy is measured as the percentage of correctly labeled words out of all words.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7149812579154968}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.8685756325721741}]}, {"text": "Statistical significance is determined by Approximate Randomization Test) with 10,000 iterations.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.5177212953567505}, {"text": "Approximate Randomization Test", "start_pos": 42, "end_pos": 72, "type": "METRIC", "confidence": 0.9056133230527242}]}, {"text": "In the first set of experiments, we focus on the WSJ and evaluate the similarity functions to gather related data fora given test article.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.6872056126594543}]}, {"text": "We have 22 WSJ articles as test set, sampled from sections 23 and 24.", "labels": [], "entities": [{"text": "WSJ articles", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.8612954318523407}]}, {"text": "Regarding feature representations, we examined three possibilities: relative frequencies of words, relative frequencies of character tetragrams (both unsmoothed) and document topic distributions.", "labels": [], "entities": []}, {"text": "In the following, we only discuss representations based on words or topic models as we found character tetragrams less stable; they performed sometimes like their word-based counterparts but other times, considerably worse.", "labels": [], "entities": []}, {"text": "compares the effect of the different ways to select related data in comparison to the random baseline for increasing amounts of training data.", "labels": [], "entities": []}, {"text": "The table gives the average over 22 test articles (rather than showing individual tables for the 22 articles).", "labels": [], "entities": []}, {"text": "We select articles up to various thresholds that specify the total number of sentences selected in each round (e.g. 0.3k, 1.2k, etc.).", "labels": [], "entities": []}, {"text": "11 In more detail, shows the result of applying various similarity functions (introduced in Section 3.1) over the two different feature representations (w: words; tm: topic model) for increasing amounts of data.", "labels": [], "entities": []}, {"text": "We additionally provide results of using the Renyi divergence.", "labels": [], "entities": []}, {"text": "Clearly, as more and more data is selected, the differences become smaller, because we are close to the data limit.", "labels": [], "entities": []}, {"text": "However, for all data points less than 38k (97%), selection by jensen-shannon, variational and cosine similarity outperform random data selection significantly for both types of feature representations (words and topic model).", "labels": [], "entities": [{"text": "variational", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9541149139404297}]}, {"text": "For selection by topic models, this additionally holds for the euclidean measure.", "labels": [], "entities": []}, {"text": "For Dutch, we evaluate the approach on a bigger and more varied dataset.", "labels": [], "entities": []}, {"text": "It contains in total over 50k articles and 20 million words (cf.).", "labels": [], "entities": []}, {"text": "In contrast to the English data, only a small portion of the dataset is manually annotated: 281 articles.", "labels": [], "entities": [{"text": "English data", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.7285302877426147}]}, {"text": "Since we want to evaluate the performance of different similarity measures, we want to keep the influence of noise as low as possible.", "labels": [], "entities": []}, {"text": "Therefore, we annotated the remaining articles with a parsing system that is more accurate, the Alpino parser).", "labels": [], "entities": []}, {"text": "Note that using a more accurate parsing system to train another parser has recently also been proposed by as uptraining.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9671034812927246}]}, {"text": "Alpino is a parser tailored to Dutch, that has been developed over the last ten years, and reaches an accuracy level of 90% on general newspaper text.", "labels": [], "entities": [{"text": "Alpino", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8381776213645935}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9991828799247742}]}, {"text": "It uses a conditional MaxEnt model as parse selection component.", "labels": [], "entities": []}, {"text": "Details of the parser are given in).", "labels": [], "entities": []}, {"text": "The results on Dutch are shown in.", "labels": [], "entities": [{"text": "Dutch", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.9736377596855164}]}, {"text": "Domain similarity measures clearly outperform random data selection also in this setting with another language and a considerably larger pool of data (20 million words; 51k articles).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the datasets for English and Dutch.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of similarity measures based  on words (w) and topic model (tm): parsing accu- racy for increasing amounts of training data as average  over 22 WSJ articles (js=jensen-shannon; cos=cosine;  skw=skew; var=variational; euc=euclidean; ryi=renyi).  Best score (per representation) underlined, best overall  score bold; indicates significantly better (p < 0.05)  than random.", "labels": [], "entities": [{"text": "WSJ articles", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9006877541542053}]}, {"text": " Table 3: Average overlap (in %) of similarity measure:  random selection (ran) vs. measures based on words (w)  and topic model (tm).", "labels": [], "entities": [{"text": "Average overlap", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7208688259124756}]}, {"text": " Table 4: Average overlap (in %) for different feature  representations x as tm/w, where tm=topic model and  w=words. Highest pair-wise overlap is underlined.", "labels": [], "entities": [{"text": "Average overlap", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.915456086397171}]}, {"text": " Table 5: Domain Adaptation Results on English (signifi- cantly better: than random; \u2666 than random and union).", "labels": [], "entities": []}]}