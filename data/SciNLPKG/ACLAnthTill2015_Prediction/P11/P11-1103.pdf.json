{"title": [], "abstractContent": [{"text": "One of the major challenges facing statistical machine translation is how to model differences in word order between languages.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6426338950792948}]}, {"text": "Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics.", "labels": [], "entities": []}, {"text": "Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated.", "labels": [], "entities": []}, {"text": "This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics.", "labels": [], "entities": []}, {"text": "We show that the metric is more consistent with human judgements than other metrics, including the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9681480526924133}]}, {"text": "We also show that the LRscore can successfully be used as the objective function when training translation model parameters.", "labels": [], "entities": []}, {"text": "Training with the LRscore leads to output which is preferred by humans.", "labels": [], "entities": []}, {"text": "Moreover, the translations incur no penalty in terms of BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9982397556304932}]}], "introductionContent": [{"text": "Research in machine translation has focused broadly on two main goals, improving word choice and improving word order in translation output.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7610970437526703}]}, {"text": "Current machine translation metrics rely upon indirect methods for measuring the quality of the word order, and their ability to capture the quality of word order is poor (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7481542229652405}]}, {"text": "There are currently two main approaches to evaluating reordering.", "labels": [], "entities": []}, {"text": "The first is exemplified by the BLEU score (), which counts the number of matching n-grams between the reference and the hypothesis.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.988391250371933}]}, {"text": "Word order is captured by the proportion of longer n-grams which match.", "labels": [], "entities": []}, {"text": "This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference.", "labels": [], "entities": []}, {"text": "Another approach is taken by two other commonly used metrics, ME-TEOR () and TER).", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9537197351455688}, {"text": "TER", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9966782331466675}]}, {"text": "They both search for an alignment between the translation and the reference, and from this they calculate a penalty based on the number of differences in order between the two sentences.", "labels": [], "entities": []}, {"text": "When block moves are allowed the search space is very large, and matching stems and synonyms introduces errors.", "labels": [], "entities": []}, {"text": "Importantly, none of these metrics capture the distance by which words are out of order.", "labels": [], "entities": []}, {"text": "Also, they conflate reordering performance with the quality of the lexical items in the translation, making it difficult to tease apart the impact of changes.", "labels": [], "entities": []}, {"text": "More sophisticated metrics, such as the RTE metric, use higher level syntactic or semantic analysis to determine the grammaticality of the output.", "labels": [], "entities": []}, {"text": "These approaches require annotation and can be very slow to run.", "labels": [], "entities": []}, {"text": "For most research, shallow metrics are more appropriate.", "labels": [], "entities": []}, {"text": "We introduce a novel shallow metric, the Lexical Reordering Score (LRscore), which explicitly measures the quality of word order in machine translations and interpolates it with a lexical metric.", "labels": [], "entities": [{"text": "Lexical Reordering Score (LRscore)", "start_pos": 41, "end_pos": 75, "type": "METRIC", "confidence": 0.7470503201087316}]}, {"text": "This results in a simple, decomposable metric which makes it easy for researchers to pinpoint the effect of their changes.", "labels": [], "entities": []}, {"text": "In this paper we show that the LRscore is more consistent with human judgements than other metrics for five out of eight different language pairs.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.7017855048179626}]}, {"text": "We also apply the LRscore during Minimum Error Rate Training (MERT) to see whether information on reordering allows the translation model to produce better reorderings.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9626486301422119}, {"text": "Minimum Error Rate Training (MERT)", "start_pos": 33, "end_pos": 67, "type": "METRIC", "confidence": 0.784830127443586}]}, {"text": "We show that humans prefer the output of systems trained with the LRscore 52.5% as compared to 43.9% when training with the BLEU score.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.8604289293289185}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9981006979942322}]}, {"text": "Furthermore, training with the LRscore does not result in lower BLEU scores.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.679801881313324}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9990886449813843}]}, {"text": "The rest of the paper proceeds as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the reordering and lexical metrics that are used and how they are combined.", "labels": [], "entities": []}, {"text": "Section 3 presents the experiments on consistency with human judgements and describes how to train the language independent parameter of the LRscore.", "labels": [], "entities": []}, {"text": "Section 4 reports the results of the experiments on MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.40531063079833984}]}, {"text": "Finally we discuss related work and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "We hypothesise that the LRscore is a good metric for training translation models.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8640379309654236}]}, {"text": "We test this by evaluating the output of the models, first with automatic metrics, and then by using human evaluation.", "labels": [], "entities": []}, {"text": "We choose to run the experiment with Chinese-English as this language pair has a large amount of medium and long distance reorderings.", "labels": [], "entities": []}, {"text": "Human judgements of translation quality are necessary to determine whether humans prefer sentences from models trained with the BLEU score or with the LRscore.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.960040420293808}]}, {"text": "There have been some recent studies which have used the online micro-market, Amazons Mechanical Turk, to collect human annotations (.", "labels": [], "entities": [{"text": "Amazons Mechanical Turk", "start_pos": 77, "end_pos": 100, "type": "DATASET", "confidence": 0.8844510316848755}]}, {"text": "While some of the data generated is very noisy, invalid responses are largely due to a small number of workers (.", "labels": [], "entities": []}, {"text": "We use Mechanical 1032 Turk and we improve annotation quality by collecting multiple judgements, and eliminating workers who do not achieve a certain level of performance on gold standard questions.", "labels": [], "entities": [{"text": "Mechanical 1032 Turk", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9005108674367269}]}, {"text": "We randomly selected a subset of sentences from the test set.", "labels": [], "entities": []}, {"text": "We use 60 sentences each for comparing training with BLEU to training with LR-HB4 and with LR-KB4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.989277720451355}]}, {"text": "These sentences were between 15 and 30 words long.", "labels": [], "entities": []}, {"text": "Shorter sentences tend to have uninteresting differences, and longer sentences may have many conflicting differences.", "labels": [], "entities": []}, {"text": "Workers were presented with a reference sentence and two translations which were randomly ordered.", "labels": [], "entities": []}, {"text": "They were told to compare the translations and select their preferred translation or \"Don't Know\".", "labels": [], "entities": []}, {"text": "Workers were screened to guarantee reasonable judgement quality.", "labels": [], "entities": []}, {"text": "20 sentence pairs were randomly selected from the 120 test units and annotated as gold standard questions.", "labels": [], "entities": []}, {"text": "Workers who got less than 60% of these gold questions correct were disqualified and their judgements discarded.", "labels": [], "entities": []}, {"text": "After disagreeing with a gold annotation, a worker is presented with the gold answer and an explanation.", "labels": [], "entities": []}, {"text": "This guides the worker on how to perform the task and motivates them to be more accurate.", "labels": [], "entities": []}, {"text": "We used the Crowdflower 2 interface to Mechanical Turk, which implements the gold functionality.", "labels": [], "entities": []}, {"text": "Even though experts can disagree on preference judgements, gold standard labels are necessary to weed out the poor standard workers.", "labels": [], "entities": []}, {"text": "There were 21 trusted workers who achieved an average accuracy of 91% on the gold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9821248054504395}]}, {"text": "There were 96 untrusted workers who averaged 29% accuracy on the gold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9995143413543701}]}, {"text": "Three judgements were collected from the trusted workers for each of the 120 test sentences.", "labels": [], "entities": []}, {"text": "In this experiment we demonstrate that the reordering metrics can be used as learning criterion in minimum error rate training to improve parameter estimation for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7976146638393402}]}, {"text": "isolation, and also as part of the LRscore together with the Hamming distance and Kendall's tau distance.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.8458445072174072}]}, {"text": "We test with these metrics, and we also report the TER and METEOR scores for comparison.", "labels": [], "entities": [{"text": "TER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9990079998970032}, {"text": "METEOR", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9867287278175354}]}, {"text": "The first thing we note in is that we would expect the highest scores when training with the same metric as that used for evaluation as MERT maximises the objective function on the development data set.", "labels": [], "entities": []}, {"text": "Here, however, when testing with BLEU, we see that training with BLEU and with LR-HB4 leads to equally high BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9950119256973267}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9960171580314636}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9989594221115112}]}, {"text": "The reordering component is more discerning than the BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.998397171497345}]}, {"text": "It reliably increases as the word order approaches that of the reference, whereas BLEU can reports the same score fora large number of different alternatives.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9974046349525452}]}, {"text": "This might make the reordering metric easier to optimise, leading to the joint best scores attest time.", "labels": [], "entities": []}, {"text": "This is an important result, as it shows that by training with the LRscore objective function, BLEU scores do not decrease, which is desirable as BLEU scores are usually reported in the field.", "labels": [], "entities": [{"text": "LRscore objective function", "start_pos": 67, "end_pos": 93, "type": "METRIC", "confidence": 0.7686506907145182}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9983739852905273}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9970388412475586}]}, {"text": "The LRscore also results in better scores when evaluated with itself and the other two baseline metrics, TER and METEOR.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.7484561204910278}, {"text": "TER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9977912902832031}, {"text": "METEOR", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9373814463615417}]}, {"text": "Reordering and the lexical metrics are orthogonal information sources, and this shows that combining them results in better performing systems.", "labels": [], "entities": []}, {"text": "BLEU has shown to be a strong baseline metric to use as an objective function, and so the LRscore performance in Table 5 is a good result.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9673436880111694}, {"text": "LRscore", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9746331572532654}]}, {"text": "Examining the weights that result from the different MERT runs, the only notable difference is that the weight of the distortion cost is considerably lower with the LRscore.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.7978158593177795}]}, {"text": "This shows more trust in the quality of reorderings.", "labels": [], "entities": []}, {"text": "Although it is interesting to look at the model weights, any final conclusion on the impact of the metrics on training must depend on human evaluation of translation quality.", "labels": [], "entities": []}, {"text": "1033 Type Sentence Reference silicon valley is still a rich area in the united states.", "labels": [], "entities": []}, {"text": "the average salary in the area was us $62,400 a year, which was 64% higher than the american average.", "labels": [], "entities": []}, {"text": "LR-KB4 silicon valley is still an affluent area of the united states, the regional labor with an average annual salary of 6.24 million us dollars, higher than the average level of 60 percent.", "labels": [], "entities": [{"text": "LR-KB4 silicon valley", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.9164665341377258}]}, {"text": "BLEU silicon valley is still in the united states in the region in an affluent area of the workforce, the average annual salary of 6.24 million us dollars, higher than the average level of 60 percent  We collect human preference judgements for output from systems trained using the BLEU score and the LRscore in order to determine whether training with the LRscore leads to genuine improvements in translation quality.", "labels": [], "entities": [{"text": "BLEU silicon valley", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8880270719528198}, {"text": "BLEU score", "start_pos": 282, "end_pos": 292, "type": "METRIC", "confidence": 0.961005300283432}]}, {"text": "shows the number of the times humans preferred the LRscore or the BLEU score output, or when they did not know.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.658688485622406}, {"text": "BLEU score output", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.978300929069519}]}, {"text": "We can see that humans have a greater preference for the output for systems trained with the LRscore, which is preferred 52.5% of the time, compared to the BLEU score, which was only preferred 43.9% of the time.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.6150668859481812}, {"text": "BLEU score", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9775342047214508}]}, {"text": "The sign test can be used to determine whether this difference is significant.", "labels": [], "entities": []}, {"text": "Our null hypothesis is that the probability of a human preferring the LRscore trained output is the same as that of preferring the BLEU trained output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9558137655258179}]}, {"text": "The one-tailed alternative hypothesis is that humans prefer the LRscore output.", "labels": [], "entities": []}, {"text": "If the null hypothesis is true, then there is only a probability of 0.048 that 189 out of 347 (189 + 158) people will select the LRscore output.", "labels": [], "entities": []}, {"text": "We therefore discard the null hypothesis and the human preference for the output of the LRscore trained system is significant to the 95% level.", "labels": [], "entities": []}, {"text": "In order to judge how reliable our judgements are we calculate the inter-annotator agreement.", "labels": [], "entities": []}, {"text": "This is given by the Kappa coefficient (K), which balances agreement with expected agreement.", "labels": [], "entities": [{"text": "Kappa coefficient (K)", "start_pos": 21, "end_pos": 42, "type": "METRIC", "confidence": 0.963195264339447}]}, {"text": "The Kappa coefficient is 0.464 which is considered to be a moderate level of agreement.", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9426305294036865}, {"text": "agreement", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9891897439956665}]}, {"text": "In analysis of the results, we found that output from the system trained with the LRscore tend to produce sentences with better structure.", "labels": [], "entities": []}, {"text": "In we see atypical example.", "labels": [], "entities": []}, {"text": "The word order of the sentence trained with BLEU is mangled, whereas the LR-KB4 model outputs a clear translation which more closely matches the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.996996283531189}]}, {"text": "It also garners higher reordering and BLEU scores.", "labels": [], "entities": [{"text": "reordering", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9953964352607727}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9995521903038025}]}, {"text": "We expect that more substantial gains can be made in the future by using models which have more powerful reordering capabilities.", "labels": [], "entities": []}, {"text": "A richer set of reordering features, and a model capable of longer distance reordering would better leverage metrics which reward good word orderings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. The percentage consistency between human judgements of rank and metrics. The LRscore variations (LR-*)  are optimised for average consistency across language pair (shown in right hand column). The bold numbers represent  the best consistency score per language pair.", "labels": [], "entities": [{"text": "LRscore variations (LR-*)", "start_pos": 87, "end_pos": 112, "type": "METRIC", "confidence": 0.8577864170074463}]}, {"text": " Table 4. The parameter setting representing the % impact  of the reordering component for the different versions of  the LRscore metric.", "labels": [], "entities": []}, {"text": " Table 7. A reference sentence is compared with output from models trained with BLEU and with the LR-KB4 lrscore.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9945800304412842}, {"text": "LR-KB4 lrscore", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.8580809533596039}]}, {"text": " Table 6. The number of the times human judges preferred  the output of systems trained either with the LRscore or  with the BLEU score, or were unable to choose.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.6719059944152832}, {"text": "BLEU score", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9760330319404602}]}]}