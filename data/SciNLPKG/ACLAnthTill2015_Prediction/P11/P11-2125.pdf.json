{"title": [{"text": "An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We combine multiple word representations based on semantic clusters extracted from the (Brown et al., 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al., 2006) in order to improve dis-criminative dependency parsing in the MST-Parser framework (McDonald et al., 2005).", "labels": [], "entities": [{"text": "dis-criminative dependency parsing", "start_pos": 217, "end_pos": 251, "type": "TASK", "confidence": 0.7270242770512899}]}, {"text": "We also provide an ensemble method for combining diverse cluster-based models.", "labels": [], "entities": []}, {"text": "The two contributions together significantly improves unlabeled dependency accuracy from 90.82% to 92.13%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9853776693344116}]}], "introductionContent": [{"text": "A simple method for using unlabeled data in discriminative dependency parsing was provided in () which involved clustering the labeled and unlabeled data and then each word in the dependency treebank was assigned a cluster identifier.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7113820463418961}]}, {"text": "These identifiers were used to augment the feature representation of the edge-factored or secondorder features, and this extended feature set was used to discriminatively train a dependency parser.", "labels": [], "entities": []}, {"text": "The use of clusters leads to the question of how to integrate various types of clusters (possibly from different clustering algorithms) in discriminative dependency parsing.", "labels": [], "entities": [{"text": "discriminative dependency parsing", "start_pos": 139, "end_pos": 172, "type": "TASK", "confidence": 0.5954243044058481}]}, {"text": "Clusters obtained from the () clustering algorithm are typically viewed as \"semantic\", e.g. one cluster might contain plan, letter, request, memo, . .", "labels": [], "entities": []}, {"text": "while another may contain people, customers, employees, students, . .", "labels": [], "entities": []}, {"text": ".. Another clustering view that is more \"syntactic\" in nature comes from the use of statesplitting in PCFGs.", "labels": [], "entities": []}, {"text": "For instance, we could extract a syntactic cluster loss, time, profit, earnings, performance, rating, . .", "labels": [], "entities": []}, {"text": ".: all head words of noun phrases corresponding to cluster of direct objects of verbs like improve.", "labels": [], "entities": []}, {"text": "In this paper, we obtain syntactic clusters from the Berkeley parser ().", "labels": [], "entities": []}, {"text": "This paper makes two contributions: 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework), and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in.", "labels": [], "entities": [{"text": "discriminative dependency parsing", "start_pos": 150, "end_pos": 183, "type": "TASK", "confidence": 0.7332899371782938}]}, {"text": "These two contributions combined significantly improves unlabeled dependency accuracy: 90.82% to 92.13% on Sec.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9889451861381531}]}, {"text": "23 of the Penn Treebank, and we see consistent improvements across all our test sets.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9910717606544495}]}], "datasetContent": [{"text": "The experiments were done on the English Penn Treebank, using standard head-percolation rules to convert the phrase structure into dependency trees.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.9239633282025655}]}, {"text": "We split the Treebank into a training set (Sections 2-21), a devel-  opment set (Section 22), and test sets (Sections 0, 1, 23, and 24).", "labels": [], "entities": []}, {"text": "All our experimental settings match previous work.", "labels": [], "entities": []}, {"text": "POS tags for the development and test data were assigned by MX-POST, where the tagger was trained on the entire training corpus.", "labels": [], "entities": [{"text": "MX-POST", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9083376526832581}]}, {"text": "To generate part of speech tags for the training data, we used 20-way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds.", "labels": [], "entities": []}, {"text": "We set model weights \u03b1 kin Eqn (2) to one for all experiments.", "labels": [], "entities": [{"text": "Eqn", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9511616230010986}]}, {"text": "Syntactic State-Splitting The sentence-specific word clusters are derived from the parse trees usingBerkeley parser 1 , which generates phrase-structure parse trees with split syntactic categories.", "labels": [], "entities": []}, {"text": "To generate parse trees for development and test data, the parser is trained on the entire training data to learn a PCFG with latent annotations using split-merge operations for 5 iterations.", "labels": [], "entities": []}, {"text": "To generate parse trees for the training data, we used 20-way jackknifing as with the tagger.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: For each test section and model, the number in the  first/second row is the unlabeled-accuracy/unlabeled-complete- correct. See the text for more explanation.", "labels": [], "entities": []}]}