{"title": [{"text": "Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation", "labels": [], "entities": [{"text": "Syntactic Translation", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.8183831572532654}]}], "abstractContent": [{"text": "We describe an exact decoding algorithm for syntax-based statistical translation.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.7135326266288757}]}, {"text": "The approach uses Lagrangian relaxation to decompose the decoding problem into tractable sub-problems, thereby avoiding exhaustive dynamic programming.", "labels": [], "entities": []}, {"text": "The method recovers exact solutions, with certificates of optimality, on over 97% of test examples; it has comparable speed to state-of-the-art decoders.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work has seen widespread use of synchronous probabilistic grammars in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.821269229054451}]}, {"text": "The decoding problem fora broad range of these systems (e.g.,) corresponds to the intersection of a (weighted) hypergraph with an n-gram language model.", "labels": [], "entities": []}, {"text": "The hypergraph represents a large set of possible translations, and is created by applying asynchronous grammar to the source language string.", "labels": [], "entities": []}, {"text": "The language model is then used to rescore the translations in the hypergraph.", "labels": [], "entities": []}, {"text": "Decoding with these models is challenging, largely because of the cost of integrating an n-gram language model into the search process.", "labels": [], "entities": []}, {"text": "Exact dynamic programming algorithms for the problem are well known (), but are too expensive to be used in practice.", "labels": [], "entities": []}, {"text": "Previous work on decoding for syntax-based SMT has therefore been focused primarily on approximate search methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8685042262077332}]}, {"text": "This paper describes an efficient algorithm for exact decoding of synchronous grammar models for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.9601215720176697}]}, {"text": "We avoid the construction of () by using Lagrangian relaxation to decompose the decoding problem into the following sub-problems: 1.", "labels": [], "entities": []}, {"text": "Dynamic programming over the weighted hypergraph.", "labels": [], "entities": []}, {"text": "This step does not require language model integration, and hence is highly efficient.", "labels": [], "entities": [{"text": "language model integration", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.6534401774406433}]}, {"text": "2. Application of an all-pairs shortest path algorithm to a directed graph derived from the weighted hypergraph.", "labels": [], "entities": []}, {"text": "The size of the derived directed graph is linear in the size of the hypergraph, hence this step is again efficient.", "labels": [], "entities": []}, {"text": "Informally, the first decoding algorithm incorporates the weights and hard constraints on translations from the synchronous grammar, while the second decoding algorithm is used to integrate language model scores.", "labels": [], "entities": []}, {"text": "Lagrange multipliers are used to enforce agreement between the structures produced by the two decoding algorithms.", "labels": [], "entities": []}, {"text": "In this paper we first give background on hypergraphs and the decoding problem.", "labels": [], "entities": []}, {"text": "We then describe our decoding algorithm.", "labels": [], "entities": []}, {"text": "The algorithm uses a subgradient method to minimize a dual function.", "labels": [], "entities": []}, {"text": "The dual corresponds to a particular linear programming (LP) relaxation of the original decoding problem.", "labels": [], "entities": []}, {"text": "The method will recover an exact solution, with a certificate of optimality, if the underlying LP relaxation has an integral solution.", "labels": [], "entities": []}, {"text": "In some cases, however, the underlying LP will have a fractional solution, in which case the method will not be exact.", "labels": [], "entities": []}, {"text": "The second technical contribution of this paper is to describe a method that iteratively tightens the underlying LP relaxation until an exact solution is produced.", "labels": [], "entities": []}, {"text": "We do this by gradually introducing constraints to step 1 (dynamic programming over the hypergraph), while still maintaining efficiency.", "labels": [], "entities": []}, {"text": "We report experiments using the tree-to-string model of.", "labels": [], "entities": []}, {"text": "Our method gives exact solutions on over 97% of test examples.", "labels": [], "entities": []}, {"text": "The method is comparable in speed to state-of-the-art decoding algorithms; for example, over 70% of the test examples are decoded in 2 seconds or less.", "labels": [], "entities": []}, {"text": "We compare our method to cube pruning, and find that our method gives improved model scores on a significant number of examples.", "labels": [], "entities": []}, {"text": "One consequence of our work is that we give accurate estimates of the number of search errors for cube pruning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experiments on translation from Chinese to English, using the tree-to-string model described In fact in our experiments we use the original hypergraph to compute admissible outside scores for an exact A* search algorithm for this problem.", "labels": [], "entities": [{"text": "translation from Chinese to English", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.8808756232261657}]}, {"text": "We have found the resulting search algorithm to be very efficient.", "labels": [], "entities": []}, {"text": "in We ran the full algorithm with the tightening method described in section 6.", "labels": [], "entities": []}, {"text": "We ran the method fora limit of 200 iterations, hence some examples may not terminate with an exact solution.", "labels": [], "entities": []}, {"text": "Our method gives exact solutions on 598/616 development set sentences (97.1%), and 675/691 test set sentences (97.7%).", "labels": [], "entities": []}, {"text": "In cases where the method does not converge within 200 iterations, we can return the best primal solution y t found by the algorithm during those iterations.", "labels": [], "entities": []}, {"text": "We can also get an upper bound on the difference f (y * ) \u2212 f (y t ) using mint L(u t ) as an upper bound on f (y * ).", "labels": [], "entities": []}, {"text": "Of the examples that did not converge, the worst example had abound that was 1.4% off (y t ) (more specifically, f (y t ) was -24.74, and the upper bound on f (y * ) \u2212 f (y t ) was 0.34).", "labels": [], "entities": []}, {"text": "gives information on decoding time for our method and two other exact decoding methods: integer linear programming (using constraints D0-D6), and exhaustive dynamic programming using the construction of (.", "labels": [], "entities": []}, {"text": "Our method is clearly the most efficient, and is comparable in speed to state-of-the-art decoding algorithms.", "labels": [], "entities": []}, {"text": "We also compare our method to cube pruning.", "labels": [], "entities": []}, {"text": "We reimplemented cube pruning in C++, to give a fair comparison to our method.", "labels": [], "entities": []}, {"text": "Cube pruning has a parameter, b, dictating the maximum number of items stored at each chart entry.", "labels": [], "entities": []}, {"text": "With b = 50, our decoder finds higher scoring solutions on 50.5% of all examples (349 examples), the cube-pruning method gets a strictly higher score on only 1 example (this was one of the examples that did not converge within 200 iterations).", "labels": [], "entities": []}, {"text": "With b = 500, our decoder finds better solutions on 18.5% of the examples (128 cases), cubepruning finds a better solution on 3 examples.", "labels": [], "entities": []}, {"text": "The median decoding time for our method is 0.79 seconds; the median times for cube pruning with b = 50 and b = 500 are 0.06 and 1.2 seconds respectively.", "labels": [], "entities": []}, {"text": "Our results give a very good estimate of the percentage of search errors for cube pruning.", "labels": [], "entities": []}, {"text": "A natural question is how large b must be before exact solutions are returned on almost all examples.", "labels": [], "entities": []}, {"text": "Even at b = 1000, we find that our method gives a better solution on 95 test examples (13.7%).", "labels": [], "entities": []}, {"text": "also gives a speed comparison of our method to a linear programming (LP) solver that solves the LP relaxation defined by constraints D0-D6.", "labels": [], "entities": []}, {"text": "We still see speed-ups, in spite of the fact that our method is solving a harder problem (it provides integral solutions).", "labels": [], "entities": []}, {"text": "The Lagrangian relaxation method, when run without the tightening method of section 6, is solving a dual of the problem being solved by the LP solver.", "labels": [], "entities": []}, {"text": "Hence we can measure how often the tightening procedure is absolutely necessary, by seeing how often the LP solver provides a fractional solution.", "labels": [], "entities": []}, {"text": "We find that this is the case on 54.0% of the test examples: the tightening procedure is clearly important.", "labels": [], "entities": []}, {"text": "Inspection of the tightening procedure shows that the number of partitions required (the parameter q) is generally quite small: 59% of examples that require tightening require q \u2264 6; 97.2% require q \u2264 10.", "labels": [], "entities": []}], "tableCaptions": []}