{"title": [{"text": "Unsupervised Semantic Role Induction via Split-Merge Clustering", "labels": [], "entities": [{"text": "Semantic Role Induction", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.64019047220548}]}], "abstractContent": [{"text": "In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers.", "labels": [], "entities": [{"text": "semantic role induction", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.729132870833079}]}, {"text": "We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality.", "labels": [], "entities": []}, {"text": "The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently.", "labels": [], "entities": []}, {"text": "By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system.", "labels": [], "entities": [{"text": "role induction", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7495282292366028}, {"text": "argument identification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7515577375888824}]}, {"text": "Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsuper-vised approaches by a wide margin.", "labels": [], "entities": [{"text": "CoNLL 2008 benchmark dataset", "start_pos": 18, "end_pos": 46, "type": "DATASET", "confidence": 0.9702817350625992}]}], "introductionContent": [{"text": "Recent years have seen increased interest in the shallow semantic analysis of natural language text.", "labels": [], "entities": [{"text": "shallow semantic analysis of natural language text", "start_pos": 49, "end_pos": 99, "type": "TASK", "confidence": 0.8247857902731214}]}, {"text": "The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents ().", "labels": [], "entities": [{"text": "identification and labeling of the semantic roles conveyed by sentential constituents", "start_pos": 57, "end_pos": 142, "type": "TASK", "confidence": 0.6888149163939736}]}, {"text": "Semantic roles describe the relations that hold between a predicate and its arguments, abstracting over surface syntactic configurations.", "labels": [], "entities": []}, {"text": "In the example sentences below.", "labels": [], "entities": []}, {"text": "window occupies different syntactic positions -it is the object of broke in sentences (1a,b), and the subject in (1c) -while bearing the same semantic role, i.e., the physical object affected by the breaking event.", "labels": [], "entities": []}, {"text": "Analogously, rock is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b).", "labels": [], "entities": []}, {"text": "[ The semantic roles in the examples are labeled in the style of), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations.", "labels": [], "entities": []}, {"text": "Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate 1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates.", "labels": [], "entities": []}, {"text": "This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8688400983810425}]}, {"text": "Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction () and question answering, to machine translation () and summarization ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7988472282886505}, {"text": "question answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.8971979022026062}, {"text": "machine translation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.8003602027893066}, {"text": "summarization", "start_pos": 216, "end_pos": 229, "type": "TASK", "confidence": 0.989330530166626}]}, {"text": "Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize se-mantic role labeling as a supervised learning problem.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.785106360912323}, {"text": "se-mantic role labeling", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.6961461106936137}]}, {"text": "Current approaches have high performancea system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see for details), however only on languages and domains for which large amounts of role-annotated training data are available.", "labels": [], "entities": []}, {"text": "For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9261775612831116}]}, {"text": "Unfortunately, the reliance on role-annotated data which is expensive and time-consuming to produce for every language and domain, presents a major bottleneck to the widespread application of semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.6002886394659678}]}, {"text": "Given the data requirements for supervised systems and the current paucity of such data, unsupervised methods offer a promising alternative.", "labels": [], "entities": []}, {"text": "They require no human effort for training thus leading to significant savings in time and resources required for annotating text.", "labels": [], "entities": []}, {"text": "And their output can be used in different ways, e.g., as a semantic preprocessing step for applications that require broad coverage understanding or as training material for supervised algorithms.", "labels": [], "entities": []}, {"text": "In this paper we present a simple approach to unsupervised semantic role labeling.", "labels": [], "entities": [{"text": "unsupervised semantic role labeling", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.6162021830677986}]}, {"text": "Following common practice, our system proceeds in two stages.", "labels": [], "entities": []}, {"text": "It first identifies the semantic arguments of a predicate and then assigns semantic roles to them.", "labels": [], "entities": []}, {"text": "Both stages operate over syntactically analyzed sentences without access to any data annotated with semantic roles.", "labels": [], "entities": []}, {"text": "Argument identification is carried out through a small set of linguistically-motivated rules, whereas role induction is treated as a clustering problem.", "labels": [], "entities": [{"text": "Argument identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7090040892362595}, {"text": "role induction", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.78947913646698}]}, {"text": "In this setting, the goal is to assign argument instances to clusters such that each cluster contains arguments corresponding to a specific semantic role and each role corresponds to exactly one cluster.", "labels": [], "entities": []}, {"text": "We formulate a clustering algorithm that executes a series of split and merge operations in order to transduce an initial clustering into a final clustering of better quality.", "labels": [], "entities": []}, {"text": "Split operations leverage syntactic cues so as to create \"pure\" clusters that contain arguments of the same role whereas merge operations bring together argument instances of a particular role located in different clusters.", "labels": [], "entities": []}, {"text": "We test the effectiveness of our induction method on the CoNLL 2008 benchmark dataset and demonstrate improvements over competitive unsupervised methods by a wide margin.", "labels": [], "entities": [{"text": "CoNLL 2008 benchmark dataset", "start_pos": 57, "end_pos": 85, "type": "DATASET", "confidence": 0.9790167659521103}]}], "datasetContent": [{"text": "In this section we describe how we assessed the performance of our system.", "labels": [], "entities": []}, {"text": "We discuss the dataset on which our experiments were carried out, explain how our system's output was evaluated and present the methods used for comparison with our approach.", "labels": [], "entities": []}, {"text": "Data For evaluation purposes, the system's output was compared against the CoNLL 2008 shared task dataset () which provides: Clustering results with our split-merge algorithm, the unsupervised model proposed in and a baseline that assigns arguments to clusters based on their syntactic function.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task dataset", "start_pos": 75, "end_pos": 105, "type": "DATASET", "confidence": 0.9271347403526307}]}, {"text": "The dataset was taken from the Wall Street Journal portion of the Penn Treebank corpus and converted into a dependency format (.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn Treebank corpus", "start_pos": 31, "end_pos": 86, "type": "DATASET", "confidence": 0.9507242772314284}]}, {"text": "In addition to gold standard dependency parses, the dataset also contains automatic parses obtained from the MaltParser (.", "labels": [], "entities": []}, {"text": "Although the dataset provides annotations for verbal and nominal predicate-argument constructions, we only considered the former, following previous work on semantic role labeling).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.6539900203545889}]}, {"text": "Evaluation Metrics For each verb, we determine the extent to which argument instances in a cluster share the same gold standard role (purity) and the extent to which a particular gold standard role is assigned to a single cluster (collocation).", "labels": [], "entities": []}, {"text": "More formally, for each group of verb-specific clusters we measure the purity of the clusters as the percentage of instances belonging to the majority gold class in their respective cluster.", "labels": [], "entities": []}, {"text": "Let N denote the total number of instances, G j the set of instances belonging to the j-th gold class and Ci the set of instances belonging to the i-th cluster.", "labels": [], "entities": []}, {"text": "Purity can then be written as: Collocation is defined as follows.", "labels": [], "entities": []}, {"text": "For each gold role, we determine the cluster with the largest number of instances for that role (the role's primary cluster) and then compute the percentage of instances that belong to the primary cluster for each gold role as: The per-verb scores are aggregated into an overall score by averaging overall verbs.", "labels": [], "entities": []}, {"text": "We use the microaverage obtained by weighting the scores for individual verbs proportionately to the number of instances for that verb.", "labels": [], "entities": []}, {"text": "Finally, we use the harmonic mean of purity and collocation as a single measure of clustering quality: Comparison Models We compared our splitmerge algorithm against two competitive approaches.", "labels": [], "entities": []}, {"text": "The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser.", "labels": [], "entities": []}, {"text": "This baseline has been previously used as point of comparison by other unsupervised semantic role labeling systems ( and shown difficult to outperform.", "labels": [], "entities": []}, {"text": "Our implementation allocates up to N = 21 clusters 2 for each verb, one for each of the 20 most frequent functions in the CoNLL dataset and a default cluster for all other functions.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.9533065855503082}]}, {"text": "The second comparison model is the one proposed in Lang and Lapata (2010) (see Section 2).", "labels": [], "entities": []}, {"text": "We used the same model settings (with 10 latent variables) and feature set proposed in that paper.", "labels": [], "entities": []}, {"text": "Our method's only parameter is the threshold \u03b1 which we heuristically set to 0.1.", "labels": [], "entities": []}, {"text": "On average our method induces 10 clusters per verb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Clustering results with our split-merge algorithm, the unsupervised model proposed in", "labels": [], "entities": []}, {"text": " Table 3: Clustering results for individual verbs with  our split-merge algorithm and the syntactic function  baseline.", "labels": [], "entities": []}, {"text": " Table 4: Clustering results for individual semantic  roles with our split-merge algorithm and the syntac- tic function baseline.", "labels": [], "entities": []}]}