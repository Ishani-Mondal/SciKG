{"title": [{"text": "Translating from Morphologically Complex Languages: A Paraphrase-Based Approach", "labels": [], "entities": [{"text": "Translating from Morphologically Complex Languages", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8152161240577698}]}], "abstractContent": [{"text": "We propose a novel approach to translating from a morphologically complex language.", "labels": [], "entities": [{"text": "translating", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9777463674545288}]}, {"text": "Unlike previous research, which has targeted word inflections and concatenations, we focus on the pairwise relationship between morphologically related words, which we treat as potential paraphrases and handle using paraphrasing techniques at the word, phrase, and sentence level.", "labels": [], "entities": []}, {"text": "An important advantage of this framework is that it can cope with deriva-tional morphology, which has so far remained largely beyond the capabilities of statistical machine translation systems.", "labels": [], "entities": [{"text": "deriva-tional morphology", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6796190142631531}, {"text": "statistical machine translation", "start_pos": 153, "end_pos": 184, "type": "TASK", "confidence": 0.6211599806944529}]}, {"text": "Our experiments translating from Malay, whose morphology is mostly derivational, into English show significant improvements over rivaling approaches based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million En-glish word tokens).", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.7832682828108469}]}, {"text": "This assumption can be traced back to the first word-based models of IBM (, which were initially proposed for two languages with limited morphology: French and English.", "labels": [], "entities": []}, {"text": "While several significantly improved models have been developed since then, including phrase-based ( , hierarchical (), treelet), and syntactic () models, they all preserved the assumption that words should be atomic.", "labels": [], "entities": []}, {"text": "Ignoring morphology was fine as long as the main research interest remained focused on languages with limited (e.g., English, French, Spanish) or minimal (e.g., Chinese) morphology.", "labels": [], "entities": [{"text": "Ignoring morphology", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9003279209136963}]}, {"text": "Since the attention shifted to languages like Arabic, however, the importance of morphology became obvious and several approaches to handle it have been proposed.", "labels": [], "entities": []}, {"text": "Depending on the particular language of interest, researchers have paid attention to word inflections and clitics, e.g., for Arabic, Finnish, and Turkish, or to noun compounds, e.g., for German.", "labels": [], "entities": []}, {"text": "However, derivational morphology has not been specifically targeted so far.", "labels": [], "entities": []}, {"text": "In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language.", "labels": [], "entities": [{"text": "translating from a morphologically complex language", "start_pos": 57, "end_pos": 108, "type": "TASK", "confidence": 0.7638378838698069}]}, {"text": "Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level.", "labels": [], "entities": []}, {"text": "An important advantage of this framework is that it can cope with various kinds of morphological wordforms, including derivational ones.", "labels": [], "entities": []}, {"text": "We demonstrate its potential on Malay, whose morphology is mostly derivational.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 gives an overview of Malay morphology, Section 3 introduces our paraphrase-based approach to translating from morphologically complex languages, Section 4 describes our dataset and our experimental setup, Section 5 presents and analyses the results, and Section 6 compares our work to previous research.", "labels": [], "entities": [{"text": "translating from morphologically complex languages", "start_pos": 155, "end_pos": 205, "type": "TASK", "confidence": 0.870134687423706}]}, {"text": "Finally, Section 7 concludes the paper and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we tokenized and lowercased all datasets: training, development, and testing.", "labels": [], "entities": []}, {"text": "We then built directed word-level alignments for the training bitext for English\u2192Malay and for Malay\u2192English using IBM model 4 (), which we symmetrized using the intersect+grow heuristic.", "labels": [], "entities": []}, {"text": "Next, we extracted phraselevel translation pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty.", "labels": [], "entities": []}, {"text": "We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9828872084617615}]}, {"text": "We set all weights on the development dataset by optimizing BLEU () using minimum error rate training, and we plugged them in abeam search decoder (  to translate the Malay test sentences to English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9982447624206543}]}, {"text": "Finally, we detokenized the output, and we evaluated it against the three reference translations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results. Shown are BLEU scores and improvements over the baseline (in %) for different numbers  of training sentences. Statistically significant improvements are in bold for p < 0.01 and in italic for p < 0.05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9988277554512024}]}, {"text": " Table 2: Detailed BLEU n-gram precision scores: in  %, for different numbers of training sentence pairs, for  baseline and lattice + sent-par + word-par + phrase-par.", "labels": [], "entities": [{"text": "BLEU n-gram precision scores", "start_pos": 19, "end_pos": 47, "type": "METRIC", "confidence": 0.8758499920368195}]}, {"text": " Table 3: Results for different evaluation measures: for  baseline and lattice + sent-par + word-par + phrase-par  (in % for all measures except for NIST).", "labels": [], "entities": [{"text": "NIST", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.861589252948761}]}]}