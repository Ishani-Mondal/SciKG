{"title": [{"text": "A Hierarchical Model of Web Summaries", "labels": [], "entities": [{"text": "Hierarchical Model of Web Summaries", "start_pos": 2, "end_pos": 37, "type": "TASK", "confidence": 0.6071705996990204}]}], "abstractContent": [{"text": "We investigate the relevance of hierarchical topic models to represent the content of Web gists.", "labels": [], "entities": []}, {"text": "We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories.", "labels": [], "entities": []}, {"text": "Our first approach, based on information-theoretic grounds, uses an algorithm similar to recur-sive feature selection.", "labels": [], "entities": []}, {"text": "Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA.", "labels": [], "entities": []}, {"text": "We evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The work presented in this paper is aimed at leveraging a manually created document ontology to model the content of an underlying document collection.", "labels": [], "entities": []}, {"text": "While the primary usage of ontologies is as a means of organizing and navigating document collections, they can also help in inferring a significant amount of information about the documents attached to them, including path-level, statistical, representations of content, and fine-grained views on the level of specificity of the language used in those documents.", "labels": [], "entities": []}, {"text": "Our study focuses on the ontology underlying DMOZ 1 , a popular Web directory.", "labels": [], "entities": []}, {"text": "We propose two methods for crystalizing a hierarchical topic model against its hierarchy and show that the resulting models outperform a flat unigram model in its predictive power over held-out data.", "labels": [], "entities": [{"text": "crystalizing a hierarchical topic", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.8227866441011429}]}, {"text": "1 http://www.dmoz.org To construct our hierarchical topic models, we adopt the mixed membership formalism, where a document is represented as a mixture over a set of word multinomials.", "labels": [], "entities": []}, {"text": "We consider the document hierarchy H (e.g. the DMOZ hierarchy) as a tree where internal nodes (category nodes) and leaf nodes (documents), as well as the edges connecting them, are known a priori.", "labels": [], "entities": []}, {"text": "Each node Ni in H is mapped to a multinomial word distribution M ult Ni , and each path c d to a leaf node Dis associated with a mixture over the multinonials (M ult C 0 . .", "labels": [], "entities": []}, {"text": "M ult C k , M ult D ) appearing along this path.", "labels": [], "entities": []}, {"text": "The mixture components are combined using a mixing proportion vector (\u03b8 C 0 . .", "labels": [], "entities": []}, {"text": "\u03b8 C k ), so that the likelihood of string w being produced bypath c d is: where: In the following, we propose two models that fit in this framework.", "labels": [], "entities": []}, {"text": "We describe how they allow the derivation of both p(w i |c d,j ) and \u03b8 and present early experimental results showing that explicit hierarchical information of content can indeed be used as a basis for content modeling purposes.", "labels": [], "entities": [{"text": "content modeling", "start_pos": 202, "end_pos": 218, "type": "TASK", "confidence": 0.7281109094619751}]}], "datasetContent": [{"text": "The perplexities obtained for the hierarchical and ngram models are reported in: Perplexity of the hierarchical models and the reference n-gram models over the entire DMOZ dataset (all), and the non-Regional portion of the dataset (reg).", "labels": [], "entities": [{"text": "DMOZ dataset", "start_pos": 167, "end_pos": 179, "type": "DATASET", "confidence": 0.9477033615112305}]}, {"text": "When taken on the entire hierarchy (all), the performance of the Bayesian and entropy-based models significantly exceeds that of the 1-gram model (significant under paired t-test, both with p-value < 2.2 \u00b7 10 \u221216 ) while remaining well below that of either the 2 or 3 gram models.", "labels": [], "entities": []}, {"text": "This suggests that, although the hierarchy plays a key role in the appearance of content in DMOZ gists, word context is also a key factor that needs to betaken into account: the two families of models we propose are based on the bag-of-word assumption and, by design, assume that words are drawn i.i.d. from an underlying distribution.", "labels": [], "entities": []}, {"text": "While it is not clear how one could extend the information-theoretic models to include such context, we are currently investigating enhancements to the hLLDA model along the lines of the approach proposed in.", "labels": [], "entities": []}, {"text": "A second area of analysis is to compare the performance of the various models on the entire hierarchy versus on the non-Regional portion of the tree (reg).", "labels": [], "entities": []}, {"text": "We can see that the perplexity of the proposed models decreases while that of the flat n-grams models increase.", "labels": [], "entities": []}, {"text": "Since the non-Regional portion of the DMOZ hierarchy is organized more consistently in a semantic fashion 6 , we believe this reflects the ability of the hierarchical models to take advantage of the corpus structure to represent the content of the summaries.", "labels": [], "entities": []}, {"text": "On the other hand, the Regional portion of the dataset seems to contribute a significant amount of noise to the hierarchy, leading to a loss in performance for those models.", "labels": [], "entities": []}, {"text": "We can observe that while hLLDA outperforms all information-theoretical models when applied to the entire DMOZ corpus, it falls behind the entropybased model when restricted to the non-regional section of the corpus.", "labels": [], "entities": [{"text": "DMOZ corpus", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.9251260161399841}]}, {"text": "Also if the reduction in perplexity remains limited for the entropy, \u03c7 2 and hLLDA models, the cross-entropy based model incurs a more significant boost in performance when applied to the more semantically-organized portion of the corpus.", "labels": [], "entities": []}, {"text": "The reason behind such disparity in behavior is not clear and we plan on investigating this issue as part of our future work.", "labels": [], "entities": []}, {"text": "Further analyzing the impact of the respective DMOZ sub-sections, we show in results for the hierarchical and 1-gram models when trained and tested over the 14 main sub-trees of the hierarchy.", "labels": [], "entities": []}, {"text": "Our intuition is that differences in the organization of those sub-trees might affect the predictive power of the various models.", "labels": [], "entities": []}, {"text": "Looking at sub-trees we can see that the trend is the same for most of them, with the best level of perplexity being achieved by the hierarchical Bayesian model, closely followed by the information-theoretical model using entropy as its selection criterion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity of the hierarchical models and the  reference n-gram models over the entire DMOZ dataset  (all), and the non-Regional portion of the dataset (reg).", "labels": [], "entities": [{"text": "DMOZ dataset", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9566397368907928}]}]}