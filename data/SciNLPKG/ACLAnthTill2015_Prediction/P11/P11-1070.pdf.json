{"title": [], "abstractContent": [{"text": "Counts from large corpora (like the web) can be powerful syntactic cues.", "labels": [], "entities": []}, {"text": "Past work has used web counts to help resolve isolated ambiguities , such as binary noun-verb PP attachments and noun compound bracketings.", "labels": [], "entities": [{"text": "noun-verb PP attachments", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.63808473944664}, {"text": "noun compound bracketings", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.6342759231726328}]}, {"text": "In this work, we first present a method for generating web count features that address the full range of syntactic attachments.", "labels": [], "entities": []}, {"text": "These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure.", "labels": [], "entities": []}, {"text": "We then integrate our features into full-scale dependency and constituent parsers.", "labels": [], "entities": []}, {"text": "We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al.", "labels": [], "entities": [{"text": "error", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.6256856918334961}]}, {"text": "(2006), and 3.4% over a non-local constituent reranker.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current state-of-the art syntactic parsers have achieved accuracies in the range of 90% F1 on the Penn Treebank, but a range of errors remain.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9927975535392761}, {"text": "F1", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9992080330848694}, {"text": "Penn Treebank", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9947023689746857}]}, {"text": "From a dependency viewpoint, structural errors can be cast as incorrect attachments, even for constituent (phrase-structure) parsers.", "labels": [], "entities": []}, {"text": "For example, in the Berkeley parser (), about 20% of the errors are prepositional phrase attachment errors as in, where a preposition-headed (IN) phrase was assigned an incorrect parent in the implied dependency tree.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.66279403368632}]}, {"text": "Here, the Berkeley parser (solid blue edges) incorrectly attaches from debt to the noun phrase $ 30 billion whereas the correct attachment (dashed gold edges) is to the verb raising.", "labels": [], "entities": []}, {"text": "However, there area range of error types, as shown in.", "labels": [], "entities": []}, {"text": "Here, (a) is a non-canonical PP attachment ambiguity whereby yesterday afternoon should attach to had already, (b) is an NP-internal ambiguity where half a should attach to dozen and not to newspapers, and (c) is an adverb attachment ambiguity, where just should modify fine and not the verb 's.", "labels": [], "entities": []}, {"text": "Resolving many of these errors requires information that is simply not present in the approximately 1M words on which the parser was trained.", "labels": [], "entities": []}, {"text": "One way to access more information is to exploit surface counts from large corpora like the web).", "labels": [], "entities": []}, {"text": "For example, the phrase raising from is much more frequent on the Web than $ x billion from.", "labels": [], "entities": [{"text": "raising from", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8659886121749878}]}, {"text": "While this 'affinity' is only a surface correlation, showed that comparing such counts can often correctly resolve tricky PP attachments.", "labels": [], "entities": []}, {"text": "This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments.", "labels": [], "entities": [{"text": "disambiguating isolated, binary PP attachments", "start_pos": 61, "end_pos": 107, "type": "TASK", "confidence": 0.6438404023647308}]}, {"text": "For example, showed that looking for paraphrase counts can further improve PP resolution.", "labels": [], "entities": [{"text": "PP resolution", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.8592205047607422}]}, {"text": "In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal at-693 tachment.", "labels": [], "entities": []}, {"text": "Still other work has exploited Web counts for other isolated ambiguities, such as NP coordination () and noun-sequence bracketing.", "labels": [], "entities": [{"text": "NP coordination", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7268232107162476}, {"text": "noun-sequence bracketing", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.7758961319923401}]}, {"text": "For example, in (b), half dozen is more frequent than half newspapers.", "labels": [], "entities": []}, {"text": "In this paper, we show how to apply these ideas to all attachments in full-scale parsing.", "labels": [], "entities": [{"text": "full-scale parsing", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.4979562610387802}]}, {"text": "Doing so requires three main issues to be addressed.", "labels": [], "entities": []}, {"text": "First, we show how features can be generated for arbitrary head-argument configurations.", "labels": [], "entities": []}, {"text": "Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex.", "labels": [], "entities": [{"text": "Affinity", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9152963757514954}]}, {"text": "Second, we integrate our features into full-scale parsing systems.", "labels": [], "entities": []}, {"text": "For dependency parsing, we augment the features in the second-order parser of.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8332261741161346}]}, {"text": "For constituent parsing, we rerank the output of the Berkeley parser).", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.5534772872924805}]}, {"text": "Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence.", "labels": [], "entities": []}, {"text": "Instead, we consider how to efficiently mine the Google n-grams corpus.", "labels": [], "entities": [{"text": "Google n-grams corpus", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.8794045646985372}]}, {"text": "Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction.", "labels": [], "entities": []}, {"text": "The most similar work is, which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features.", "labels": [], "entities": [{"text": "noun bracketing", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.6835249960422516}]}, {"text": "use Web counts to filter out certain 'semantically bad' parses from extraction candidate sets but are not concerned with distinguishing amongst top parses.", "labels": [], "entities": []}, {"text": "In an important contrast, smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in addition to POS tags (also see).", "labels": [], "entities": []}, {"text": "Their work also gives away to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases.", "labels": [], "entities": []}, {"text": "This work uses a large web-scale corpus (Google n-grams) to compute features for the full parsing task.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.812189519405365}]}, {"text": "To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers.", "labels": [], "entities": []}, {"text": "For the dependency case, we can integrate them into the dynamic programming of abase parser; we use the discriminativelytrained MST dependency parser).", "labels": [], "entities": []}, {"text": "Our first-order web-features give 7.0% relative error reduction over the second-order dependency baseline of.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 39, "end_pos": 63, "type": "METRIC", "confidence": 0.7859978477160136}]}, {"text": "For constituent parsing, we use a reranking framework) and show 9.2% relative error reduction over the Berkeley parser baseline.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.701635330915451}, {"text": "relative error reduction", "start_pos": 69, "end_pos": 93, "type": "METRIC", "confidence": 0.7592985431353251}]}, {"text": "In the same framework, we also achieve 3.4% error reduction over the non-local syntactic features used in.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.9647453129291534}]}, {"text": "Our webscale features reduce errors fora range of attachment types.", "labels": [], "entities": []}, {"text": "Finally, we present an analysis of influential features.", "labels": [], "entities": []}, {"text": "We not only reproduce features suggested in previous work but also discover a range of new ones.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our features are designed to be used in full-sentence parsing rather than for limited decisions about isolated ambiguities.", "labels": [], "entities": [{"text": "full-sentence parsing", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.6991612017154694}]}, {"text": "We first integrate our features into a dependency parser, where the integration is more natural and pushes all the way into the underlying dynamic program.", "labels": [], "entities": []}, {"text": "We then add them to a constituent parser in a reranking approach.", "labels": [], "entities": []}, {"text": "We also verify that our features contribute on top of standard reranking features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: UAS results for English WSJ dependency parsing. Dev  is WSJ section 22 (all sentences) and Test is WSJ section 23  (all sentences). The order 2 baseline represents McDonald and  Pereira (2006).", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7220288515090942}, {"text": "WSJ dependency parsing", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8234495719273885}, {"text": "Dev", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9021918773651123}, {"text": "WSJ section 22", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.900734027226766}, {"text": "WSJ", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.9286548495292664}]}, {"text": " Table 2: Oracle F1-scores for k-best lists output by Berkeley  parser for English WSJ parsing (Dev is section 22 and Test is  section 23, all lengths).", "labels": [], "entities": [{"text": "WSJ parsing", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.6905148029327393}]}, {"text": " Table 3: Parsing results for reranking 50-best lists of Berkeley  parser (Dev is WSJ section 22 and Test is WSJ section 23, all  lengths).", "labels": [], "entities": [{"text": "WSJ section 22", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.8839847644170126}, {"text": "WSJ", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.9507489204406738}]}, {"text": " Table 4: Error reduction for attachments of various child (argu- ment) categories. The columns depict the tag, its total attach- ments as argument, number of correct ones in baseline", "labels": [], "entities": [{"text": "Error reduction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.768702894449234}]}]}