{"title": [{"text": "Together We Can: Bilingual Bootstrapping for WSD", "labels": [], "entities": [{"text": "WSD", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.7255520224571228}]}], "abstractContent": [{"text": "Recent work on bilingual Word Sense Disam-biguation (WSD) has shown that a resource deprived language (L 1) can benefit from the annotation work done in a resource rich language (L 2) via parameter projection.", "labels": [], "entities": [{"text": "Word Sense Disam-biguation (WSD)", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7370592653751373}]}, {"text": "However , this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible.", "labels": [], "entities": []}, {"text": "Instead , we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data.", "labels": [], "entities": []}, {"text": "We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L 1 is used to annotate the untagged data of L 2 and vice versa using parameter projection.", "labels": [], "entities": []}, {"text": "The untagged instances of L 1 and L 2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated.", "labels": [], "entities": []}, {"text": "Our experiments show that such a bilingual boot-strapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L 1) and Marathi (L 2) as the language pair performs better than monolingual boot-strapping and significantly reduces annotation cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "The high cost of collecting sense annotated data for supervised approaches) has always remained a matter of concern for some of the resource deprived languages of the world.", "labels": [], "entities": []}, {"text": "The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages).", "labels": [], "entities": []}, {"text": "To circumvent this problem, unsupervised and knowledge based approaches) have been proposed as an alternative but they have failed to deliver good accuracies.", "labels": [], "entities": []}, {"text": "Semi-supervised approaches which use a small amount of annotated data and a large amount of untagged data have shown promise albeit fora limited set of target words.", "labels": [], "entities": []}, {"text": "The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD.", "labels": [], "entities": []}, {"text": "Recent work by in this direction has shown that it is possible to perform cost effective WSD in a target language (L 2 ) without compromising much on accuracy by leveraging on the annotation work done in another language ).", "labels": [], "entities": [{"text": "WSD", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9615967869758606}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9979233741760254}]}, {"text": "This is achieved with the help of a novel synsetaligned multilingual dictionary which facilitates the projection of parameters learned from the Wordnet and annotated corpus of L 1 to L 2 . This approach thus obviates the need for collecting large amounts of annotated corpora in multiple languages by relying on sufficient annotated corpus in one resource rich language.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.9824770092964172}]}, {"text": "However, in many situations such a pivot resource rich language itself may not be available.", "labels": [], "entities": []}, {"text": "Instead, we might have two or more languages having a small amount of annotated corpus and a large amount of untagged corpus.", "labels": [], "entities": []}, {"text": "Addressing such situations is the main focus of this work.", "labels": [], "entities": []}, {"text": "Specifically, we address the following question: In the absence of a pivot resource rich language is it possible for two resource deprived languages to mutually benefit from each other's annotated data?", "labels": [], "entities": []}, {"text": "While addressing the above question we assume that even though it is hard to obtain large amounts of annotated data in multiple languages, it should be fairly easy to obtain a large amount of untagged data in these languages.", "labels": [], "entities": []}, {"text": "We leverage on such untagged data by employing a bootstrapping strategy.", "labels": [], "entities": []}, {"text": "The idea is to train an initial model using a small amount of annotated data in both the languages and iteratively expand this seed data by including untagged instances which get tagged with a high confidence in successive iterations.", "labels": [], "entities": []}, {"text": "Instead of using monolingual bootstrapping, we use bilingual bootstrapping via parameter projection.", "labels": [], "entities": []}, {"text": "In other words, the parameters learned from the annotated data of L 1 (and L 2 respectively) are projected to L 2 (and L 1 respectively) and the projected model is used to tag the untagged instances of L 2 (and L 1 respectively).", "labels": [], "entities": []}, {"text": "Such a bilingual bootstrapping strategy when tested on two domains, viz., Tourism and Health using Hindi (L 1 ) and Marathi (L 2 ) as the language pair, consistently does better than a baseline strategy which uses only seed data for training without performing any bootstrapping.", "labels": [], "entities": []}, {"text": "Further, it consistently performs better than monolingual bootstrapping.", "labels": [], "entities": []}, {"text": "A simple and intuitive explanation for this is as follows.", "labels": [], "entities": []}, {"text": "In monolingual bootstrapping a language can benefit only from its own seed data and hence can tag only those instances with high confidence which it has already seen.", "labels": [], "entities": []}, {"text": "On the other hand, in bilingual bootstrapping a language can benefit from the seed data available in the other language which was not previously seen in its self corpus.", "labels": [], "entities": []}, {"text": "This is very similar to the process of co-training) wherein the annotated data in the two languages can be seen as two different views of the same data.", "labels": [], "entities": []}, {"text": "Hence, the classifier trained on one view can be improved by adding those untagged instances which are tagged with a high confidence by the classifier trained on the other view.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we present related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the Synset aligned multilingual dictionary which facilitates parameter projection.", "labels": [], "entities": [{"text": "parameter projection", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.7361738383769989}]}, {"text": "Section 4 discusses the work of on parameter projection.", "labels": [], "entities": []}, {"text": "In section 5 we discuss bilingual bootstrapping which is the main focus of our work followed by a brief discussion on monolingual bootstrapping.", "labels": [], "entities": []}, {"text": "Section 6 describes the experimental setup.", "labels": [], "entities": []}, {"text": "In section 7 we present the results followed by discussion in section 8.", "labels": [], "entities": []}, {"text": "Section 9 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the publicly available dataset 1 described in for all our experiments.", "labels": [], "entities": []}, {"text": "The data was collected from two domains, viz., Tourism and Health.", "labels": [], "entities": []}, {"text": "The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi.", "labels": [], "entities": [{"text": "Tourism domain", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9050920009613037}]}, {"text": "Similarly, English documents for Health domain were obtained from two doctors and were manually translated into Hindi and Marathi.", "labels": [], "entities": [{"text": "Health domain", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9401333928108215}]}, {"text": "The entire data was then manually annotated by three lexicographers adept in Hindi and Marathi.", "labels": [], "entities": []}, {"text": "The various statistics pertaining to the total number of words, number of words per POS category and average degree of polysemy are described in.", "labels": [], "entities": []}, {"text": "Although        ber of monosemous words, we would like to clearly state that we do not consider monosemous words while evaluating the performance of our algorithms (as monosemous words do not need any disambiguation).", "labels": [], "entities": []}, {"text": "We did a 4-fold cross validation of our algorithm using the above described corpora.", "labels": [], "entities": []}, {"text": "Note that even though the corpora were parallel we did not use this property in anyway in our experiments or algorithm.", "labels": [], "entities": []}, {"text": "In fact, the documents in the two languages were randomly split into 4 folds without ensuring that the parallel documents remain in the same folds for the two languages.", "labels": [], "entities": []}, {"text": "We experimented with different seed sizes varying from 0 to 5000 in steps of 250.", "labels": [], "entities": []}, {"text": "The seed annotated data and untagged instances for bootstrapping are extracted from 3 folds of the data and the final evaluation is done on the held-out data in the 4th fold.", "labels": [], "entities": []}, {"text": "We ran both the bootstrapping algorithms (i.e., monolingual bootstrapping and bilingual bootstrapping) for 10 iterations but, we observed that after 1-2 iterations the algorithms converge.", "labels": [], "entities": []}, {"text": "In each iteration only those words for which P (assigned sense|word) > 0.6 get moved to the labeled data.", "labels": [], "entities": []}, {"text": "Ideally, this threshold (0.6) should have been selected using a development set.", "labels": [], "entities": []}, {"text": "However, since our work focuses on resource scarce languages we did not want to incur the additional cost of using a development set.", "labels": [], "entities": []}, {"text": "Hence, we used a fixed threshold of 0.6 so that in each iteration only those words get moved to the labeled data for which the assigned sense is clearly a majority sense (P > 0.6).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Polysemous and Monosemous words per cate- gory in each domain for Hindi", "labels": [], "entities": []}, {"text": " Table 3: Polysemous and Monosemous words per cate- gory in each domain for Marathi", "labels": [], "entities": []}, {"text": " Table 4: Average degree of Wordnet polysemy per cate- gory in the 2 domains for Hindi", "labels": [], "entities": []}, {"text": " Table 5: Average degree of Wordnet polysemy per cate- gory in the 2 domains for Marathi", "labels": [], "entities": [{"text": "Marathi", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.46974918246269226}]}, {"text": " Table 6: Reduction in annotation cost achieved using Bilingual Bootstrapping", "labels": [], "entities": []}]}