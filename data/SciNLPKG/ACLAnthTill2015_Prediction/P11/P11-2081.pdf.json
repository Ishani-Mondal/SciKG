{"title": [{"text": "Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity", "labels": [], "entities": []}], "abstractContent": [{"text": "Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.8608768781026205}]}, {"text": "We demonstrate that, fora large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.9141006469726562}]}, {"text": "We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 213, "end_pos": 233, "type": "METRIC", "confidence": 0.8215527534484863}]}], "introductionContent": [{"text": "Statistical alignment models have become widely used in machine translation, question answering, textual entailment, and non-NLP application areas such as information retrieval) and object recognition ().", "labels": [], "entities": [{"text": "Statistical alignment", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7898570001125336}, {"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.785900890827179}, {"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8903480768203735}, {"text": "textual entailment", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7050872594118118}, {"text": "information retrieval", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.7484676241874695}, {"text": "object recognition", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.8118842244148254}]}, {"text": "The complexity of the probabilistic models needed to explain the hidden correspondence among words has necessitated the development of highly non-convex and difficult to optimize models, such as HMMs ( and IBM Models 3 and higher.", "labels": [], "entities": []}, {"text": "To reduce the impact of getting stuck in bad local optima the original IBM paper ( proposed the idea of training a sequence of models from simpler to complex, and using the simpler models to initialize the more complex ones.", "labels": [], "entities": []}, {"text": "IBM Model 1 was the first model in this sequence and was considered a reliable initializer due to its convexity.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9172250827153524}]}, {"text": "In this paper we show that although IBM Model 1 is convex, it is not strictly convex, and there is a large space of parameter values that achieve the same optimal value of the objective.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8584798375765482}]}, {"text": "We study the magnitude of this problem by formulating the space of optimal parameters as solutions to a set of linear equalities and seek maximally different parameter values that reach the same objective, using a linear programming approach.", "labels": [], "entities": []}, {"text": "This lets us quantify the percentage of model parameters that are not uniquely defined, as well as the number of word types that have uncertain translation probabilities.", "labels": [], "entities": []}, {"text": "We additionally study the achieved variance in parameters resulting from different random initialization in EM, and the impact of initialization on test set log-likelihood and alignment error rate.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 176, "end_pos": 196, "type": "METRIC", "confidence": 0.8888129194577535}]}, {"text": "These experiments suggest that initialization does matter in practice, contrary to what is suggested in.", "labels": [], "entities": [{"text": "initialization", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.981044352054596}]}], "datasetContent": [{"text": "In this section, we show that the solution space defined by the LP of Eq.", "labels": [], "entities": []}, {"text": "3-5 can be fairly large.", "labels": [], "entities": []}, {"text": "We demonstrate this with Bulgarian-English parallel data drawn from the JRC-AQUIS corpus).", "labels": [], "entities": [{"text": "JRC-AQUIS corpus", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.9668290913105011}]}, {"text": "Our training data consists of up to 10,000 sentence pairs, which is representative of the amount of data used to train SMT systems for language pairs that are relatively resource-poor.", "labels": [], "entities": [{"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9934515953063965}]}, {"text": "relies on two methods for determining to what extent the model t(f |e) can vary while remaining optimal.", "labels": [], "entities": []}, {"text": "The EM-LP-N method consists of applying the method described at the end of Section 4 with N training sentence pairs.", "labels": [], "entities": []}, {"text": "For EM-rand-N , we instead run EM 100 times (also on N sentence pairs) until convergence using different random starting points, and then use cosine similarity to compare the resulting models.", "labels": [], "entities": []}, {"text": "6 shows some surprising results: First, EM-LP-128 finds that, for about 68% of target token types, cosine similarity between contrastive models is equal to 0.", "labels": [], "entities": []}, {"text": "A cosine of zero essentially means that we can turn 1's into 0's without affecting log-likelihood, as in the short sentence example in Section 4.", "labels": [], "entities": []}, {"text": "Second, with a much larger training set, EM-rand-10K finds a cosine similarity lower or equal to 0.5 for 30% of word types, which is a large portion of the vocabulary.", "labels": [], "entities": []}, {"text": "While the first method is better at finding divergent optimal model parameters, it needs to construct large linear programs that do not scale with large training sets (linear systems quickly reach millions of entries, even with 128 sentence pairs).", "labels": [], "entities": []}, {"text": "We use EM-rand to assess the model space on larger training set, while we use EM-LP mainly to illustrate that divergence between optimal models can be much larger than suggested by EM-rand.: Results using 100 random initialization trials.", "labels": [], "entities": []}, {"text": "In we show additional statistics computed from the EM-rand-N experiments.", "labels": [], "entities": []}, {"text": "Every row represents statistics fora given training set size (in number of sent.", "labels": [], "entities": []}, {"text": "pairs, first column); the second column shows the percent of target word types that always co-occur with another word type (we term these words coupled); the third, fourth, and fifth columns show the percent of word types whose translation distributions were found to be non-unique, where we define the non-unique types to be ones where the minimum cosine between any two different optimal parameter vectors was less than .95.", "labels": [], "entities": []}, {"text": "The percent of non-unique types are reported overall, as well as only among coupled words (c.) and non-coupled words (non-c.).", "labels": [], "entities": []}, {"text": "The last two columns show the standard deviation in test set log-likelihood across different random trials, as well as the difference between the log-likelihood of the uniformly initialized model and the best model from the random trials.", "labels": [], "entities": []}, {"text": "We can see that as the training set size increases, the percentage of words that have non-unique translation probabilities goes down but is still very large.", "labels": [], "entities": []}, {"text": "The coupled words almost always end up having varying translation parameters at convergence (more than 99.5% of these words).", "labels": [], "entities": []}, {"text": "This also happens fora sizable portion of the non-coupled words, which suggests that there are additional patterns of cooccurrence that result in non-determinism.", "labels": [], "entities": []}, {"text": "We also computed the percent of word types that are coupled for two more-realistically sized data-sets: we found that in a 1.6 million sent pair English-Bulgarian corpus 15% of Bulgarian word types were coupled and in a 1.9 million English-German corpus from the WMT workshop), 13% of the German word types were coupled.", "labels": [], "entities": [{"text": "WMT workshop", "start_pos": 263, "end_pos": 275, "type": "DATASET", "confidence": 0.8968708515167236}]}, {"text": "The log-likelihood statistics show that although the standard deviation goes down with training set size, it is still large at reasonable data sizes.", "labels": [], "entities": []}, {"text": "Interestingly, the uniformly initialized model performs worse fora very small data size, but it catches up and surpasses the random models at data sizes greater than 100 sentence pairs.", "labels": [], "entities": []}, {"text": "To further evaluate the impact of initialization for IBM Model 1, we report on a set of experiments looking at alignment error rate achieved by different models.", "labels": [], "entities": [{"text": "initialization", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9713754653930664}, {"text": "alignment error rate", "start_pos": 111, "end_pos": 131, "type": "METRIC", "confidence": 0.7218290964762369}]}, {"text": "We report the performance of Model 1, as well as the performance of the more competitive HMM alignment model (, initialized from IBM-1 parameters.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.8594124913215637}]}, {"text": "The dataset for these experiments is English-French parallel data from Hansards.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.7470139861106873}]}, {"text": "The manually aligned data for evaluation consists of 137 sentences (a development set from).", "labels": [], "entities": []}, {"text": "We look at two different training set sizes, a small set consisting of 1000 sentence pairs, and a reasonably-sized dataset containing 100,000 sentence pairs.", "labels": [], "entities": []}, {"text": "In each data size condition, we report on the performance achieved by IBM-1, and the performance achieved by HMM initialized from the IBM-1 parameters.", "labels": [], "entities": [{"text": "IBM-1", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9397687315940857}]}, {"text": "For IBM Model 1 training, we either perform only 5 EM iterations (the standard setting in GIZA++), or run it to convergence.", "labels": [], "entities": []}, {"text": "For each of these two settings, we either start training from uniform t(f |e) parameters, or random parameters.", "labels": [], "entities": []}, {"text": "Table 2 details the results of these experiments.", "labels": [], "entities": []}, {"text": "Each row in the table represents an experimental condition, indicating the training data size (1K in the first four rows and 100K in the next four rows), the type of initialization (uniform versus random) and the number of iterations EM was run for Model 1 (5 iterations versus unlimited (to convergence, denoted \u221e)).", "labels": [], "entities": []}, {"text": "The numbers in the table are alignment error rates, achieved at the end of Model 1 training, and at 5 iterations of HMM.", "labels": [], "entities": [{"text": "alignment error rates", "start_pos": 29, "end_pos": 50, "type": "METRIC", "confidence": 0.7494358420372009}]}, {"text": "When random initialization is used, we run 20 random trials with different initialization, and report the min, max, and mean AER achieved in each setting.", "labels": [], "entities": [{"text": "max", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.8484196066856384}, {"text": "AER", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9233524203300476}]}, {"text": "From the: AER results for Model 1 and HMM using uniform and random initialization.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9988100528717041}]}, {"text": "We do not report mean and max for uniform, since they are identical to min. than the AER of the uniform-initialized models.", "labels": [], "entities": [{"text": "mean", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9834866523742676}, {"text": "AER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9993343949317932}]}, {"text": "In some cases, even the mean of the random trials was better than the corresponding uniform model.", "labels": [], "entities": []}, {"text": "Interestingly, the advantage of the randomly initialized models in AER does not seem to diminish with increased training data size like their advantage in test set perplexity.", "labels": [], "entities": [{"text": "AER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7655487060546875}]}], "tableCaptions": [{"text": " Table 1: Results using 100 random initialization trials.", "labels": [], "entities": []}]}