{"title": [{"text": "Local Histograms of Character N -grams for Authorship Attribution", "labels": [], "entities": [{"text": "Authorship Attribution", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6524864286184311}]}], "abstractContent": [{"text": "This paper proposes the use of local his-tograms (LH) over character n-grams for authorship attribution (AA).", "labels": [], "entities": [{"text": "authorship attribution (AA)", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6569826304912567}]}, {"text": "LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word his-tograms.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7530588507652283}]}, {"text": "In this work we explore the suitabil-ity of LHs over n-grams at the character-level for AA.", "labels": [], "entities": [{"text": "AA", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9035950899124146}]}, {"text": "We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors.", "labels": [], "entities": [{"text": "AA", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.986200749874115}]}, {"text": "We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches.", "labels": [], "entities": [{"text": "AA data sets", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8859708309173584}, {"text": "AA", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9842028617858887}]}, {"text": "We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets.", "labels": [], "entities": []}, {"text": "Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.", "labels": [], "entities": [{"text": "authorship verification", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.8005810379981995}, {"text": "plagiarism detection", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.7564438283443451}]}], "introductionContent": [{"text": "Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (.", "labels": [], "entities": [{"text": "Authorship attribution (AA)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8533567547798157}]}, {"text": "There is abroad field of application for AA methods, including spam filtering (de), fraud detection, computer forensics, cyber bullying ( and plagiarism detection.", "labels": [], "entities": [{"text": "AA", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9768383502960205}, {"text": "spam filtering (de)", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8346461415290832}, {"text": "fraud detection", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.8489197492599487}, {"text": "plagiarism detection", "start_pos": 142, "end_pos": 162, "type": "TASK", "confidence": 0.8475154340267181}]}, {"text": "Therefore, the development of automated AA techniques has received much attention recently).", "labels": [], "entities": [{"text": "AA", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.764990508556366}]}, {"text": "The AA problem can be naturally posed as one of single-label multiclass classification, with as many classes as candidate authors.", "labels": [], "entities": [{"text": "AA", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8615733981132507}]}, {"text": "However, unlike usual text categorization tasks, where the core problem is modeling the thematic content of documents), the goal in AA is modeling authors' writing style).", "labels": [], "entities": []}, {"text": "Hence, document representations that reveal information about writing style are required to achieve good accuracy in AA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9991680383682251}, {"text": "AA", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9297658801078796}]}, {"text": "Word and character based representations have been used in AA with some success so far.", "labels": [], "entities": [{"text": "AA", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9891033172607422}]}, {"text": "Such representations can capture style information through word or character usage, but they lack sequential information, which can reveal further stylistic information.", "labels": [], "entities": []}, {"text": "In this paper, we study the use of richer document representations for the AA task.", "labels": [], "entities": [{"text": "AA task", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.922397792339325}]}, {"text": "In particular, we consider local histograms over n-grams at the character-level obtained via the locally-weighted bag of words (LOWBOW) framework ( . Under LOWBOW, a document is represented by a set of local histograms, computed across the whole document but smoothed by kernels centered on different document locations.", "labels": [], "entities": []}, {"text": "In this way, document representations preserve both word/character usage and sequential information (i.e., information about the positions in which words or characters occur), which can be more helpful for modeling the writing style of authors.", "labels": [], "entities": []}, {"text": "We report experimental results in an AA data set used in previous studies under several conditions (.", "labels": [], "entities": [{"text": "AA data set", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9280772805213928}]}, {"text": "Results confirm that local histograms of character n-grams are more helpful for AA than the usual global histograms of words or character n-grams; our results are superior to those reported in related works.", "labels": [], "entities": [{"text": "AA", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9672839045524597}]}, {"text": "We also show that local histograms over character n-grams are more helpful than local histograms over words, as originally proposed by ( . Further, we performed experiments with imbalanced and small training sets (i.e., under a realistic AA setting) using the aforementioned representations.", "labels": [], "entities": []}, {"text": "We found that the LOWBOW-based representation resulted even more advantageous in these challenging conditions.", "labels": [], "entities": []}, {"text": "The contributions of this work are as follows: \u2022 We show that the LOWBOW framework can be helpful for AA, giving evidence that sequential information encoded in local histograms is useful for modeling the writing style of authors.", "labels": [], "entities": [{"text": "AA", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9914541244506836}]}, {"text": "\u2022 We propose the use of local histograms over character-level n-grams for AA.", "labels": [], "entities": [{"text": "AA", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9830372929573059}]}, {"text": "We show that character-level representations, which have proved to be very effective for AA, can be further improved by adopting a local histogram formulation.", "labels": [], "entities": [{"text": "AA", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9919959902763367}]}, {"text": "Also, we empirically show that local histograms at the character-level are more helpful than local histograms at the word-level for AA.", "labels": [], "entities": []}, {"text": "\u2022 We study several kernels fora support vector machine AA classifier under the local histograms formulation.", "labels": [], "entities": []}, {"text": "Our study confirms that the diffusion kernel (Lafferty and) is the most effective among those we tried, although competitive performance can be obtained with simpler kernels.", "labels": [], "entities": []}, {"text": "\u2022 We report experimental results that are superior to state of the art approaches (, with improvements ranging from 2%\u22126% in balanced data sets and from 14% \u2212 30% in imbalanced data sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we considered the data set used in ().", "labels": [], "entities": []}, {"text": "This corpus is a subset of the RCV1 collection () and comprises documents authored by 10 authors.", "labels": [], "entities": [{"text": "RCV1 collection", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.9718250632286072}]}, {"text": "All of the documents belong to the same topic.", "labels": [], "entities": []}, {"text": "Since this data set has predefined training and testing partitions, our results are comparable to those obtained by other researchers.", "labels": [], "entities": []}, {"text": "There are 50 documents per author for training and 50 documents per author for testing.", "labels": [], "entities": []}, {"text": "We performed experiments with LOWBOW 3 representations at word and character-level.", "labels": [], "entities": []}, {"text": "For the experiments with words, we took the top 2,500 most common words used across the training documents and obtained LOWBOW representations.", "labels": [], "entities": [{"text": "LOWBOW", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9719700813293457}]}, {"text": "We used this setting in agreement with previous work on AA ().", "labels": [], "entities": [{"text": "AA", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9297667145729065}]}, {"text": "For our character n-gram experiments, we obtained LOW-BOW representations for character 3-grams (only n-grams of size n = 3 were used) considering the 2, 500 most common n-grams.", "labels": [], "entities": [{"text": "LOW-BOW", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9754049181938171}]}, {"text": "Again, this setting was adopted in agreement with previous work on AA with character n-grams).", "labels": [], "entities": [{"text": "AA", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9610220789909363}]}, {"text": "All our experiments use the SVM implementation provided by.", "labels": [], "entities": []}, {"text": "In order to compare our methods to related works we adopted the following experimental setting.", "labels": [], "entities": []}, {"text": "We perform experiments using all of the training documents per author, that is, a balanced corpus (we call this setting BC).", "labels": [], "entities": []}, {"text": "Next we evaluate the performance of classifiers over reduced training sets.", "labels": [], "entities": []}, {"text": "We tried balanced reduced data sets with: 1, 3, 5 and 10 documents per author (we call this configuration RBC).", "labels": [], "entities": []}, {"text": "Also, we experimented with reducedimbalanced data sets using the same imbalance rates reported in (Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a): we tried settings 2 \u2212 10, 5 \u2212 10, and 10 \u2212 20, where, for example, setting 2-10 means that we use at least 2 and at most 10 documents per author (we call this setting IRBC).", "labels": [], "entities": [{"text": "IRBC", "start_pos": 329, "end_pos": 333, "type": "METRIC", "confidence": 0.8653246164321899}]}, {"text": "BC setting represents the AA problem under ideal conditions, whereas settings RBC and IRBC aim at emulating a more realistic scenario, where limited sample documents are available and the whole data set is highly imbalanced).", "labels": [], "entities": [{"text": "AA", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.7674752473831177}, {"text": "IRBC", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8428952097892761}]}, {"text": "We used LOWBOW code of G.", "labels": [], "entities": [{"text": "LOWBOW", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9412767887115479}, {"text": "G", "start_pos": 23, "end_pos": 24, "type": "METRIC", "confidence": 0.8359588384628296}]}, {"text": "Lebanon and Y. Mao available from http://www.cc.gatech.edu/\u223cymao8/lowbow.htm  We first compare the performance of the LOWBOW histogram representation to that of the traditional BOW representation.", "labels": [], "entities": []}, {"text": "shows the accuracy (i.e., percentage of documents in the test set that were associated to its correct author) for the BOW and LOWBOW histogram representations when using words and character n-grams information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994893074035645}, {"text": "BOW", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9371746182441711}]}, {"text": "For LOWBOW histograms, we report results with three different configurations for \u00b5.", "labels": [], "entities": []}, {"text": "As in ( , we consider uniformly distributed locations and we varied the number of locations that were included in each setting.", "labels": [], "entities": []}, {"text": "We denote with k the number of local histograms.", "labels": [], "entities": []}, {"text": "In preliminary experiments we tried several other values fork, although we found that representative results can be obtained with the values we considered here.", "labels": [], "entities": []}, {"text": "From we can see that the BOW representation is very effective, outperforming most of the LOWBOW histogram configurations.", "labels": [], "entities": [{"text": "BOW", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9706349968910217}]}, {"text": "Despite a small difference in performance, BOW is advantageous over LOWBOW histograms because it is simpler to compute and it does not rely on parameter selection.", "labels": [], "entities": [{"text": "BOW", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9922542572021484}]}, {"text": "Recall that the LOWBOW histogram representations are obtained by the combination of several local histograms calculated at different locations of the document, hence, it seems that the raw sum of local histograms results in a loss of useful information for representing documents.", "labels": [], "entities": [{"text": "LOWBOW", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.8845072984695435}]}, {"text": "The worse performance was obtained when k = 2 local histograms are considered (see row 3 in).", "labels": [], "entities": []}, {"text": "This result is somewhat expected since the larger the number of local histograms, the more LOWBOW histograms approach the BOW formulation ( . We now describe the AA performance obtained when using the BOLH formulation; these results are shown in, showing that bags of local histograms area better way to exploit the LOWBOW framework for AA.", "labels": [], "entities": [{"text": "BOW", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9645013809204102}, {"text": "BOLH", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.8801010847091675}, {"text": "AA", "start_pos": 337, "end_pos": 339, "type": "TASK", "confidence": 0.9717321395874023}]}, {"text": "As expected, different kernels yield different results.", "labels": [], "entities": []}, {"text": "However, the diffusion kernel outperformed most of the results obtained with other kernels; confirming the results obtained by other researchers (: Authorship attribution accuracy when using bags of local histograms and different kernels for word-based and character-based representations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.897557258605957}]}, {"text": "The BC data set is used.", "labels": [], "entities": [{"text": "BC data set", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8811618487040201}]}, {"text": "Settings 1, 2 and 3 correspond to k = 2, 5 and 20, respectively.", "labels": [], "entities": []}, {"text": "On average, the worse kernel was that based on the earth mover's distance (EMD), suggesting that the comparison of local histograms at different locations is not a fruitful approach (recall that this is the only kernel that compares local histograms at different locations).", "labels": [], "entities": [{"text": "earth mover's distance (EMD)", "start_pos": 51, "end_pos": 79, "type": "METRIC", "confidence": 0.6813147493771144}]}, {"text": "This result evidences that authors use similar word/character distributions at similar locations when writing different documents.", "labels": [], "entities": []}, {"text": "The best performance across settings and kernels was obtained with the diffusion kernel (in bold, column 3, row 9) (86.4%); that result is 8% higher than that obtained with the BOW representation and 9% better than the best configuration of LOWBOW histograms, see.", "labels": [], "entities": []}, {"text": "Furthermore, that result is more than 5% higher than the best reported result in related work (80.8% as reported in).", "labels": [], "entities": []}, {"text": "Therefore, the considered local histogram representations over character n-grams have proved to be very effective for AA.", "labels": [], "entities": [{"text": "AA", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.9925230145454407}]}, {"text": "One should note that, in general, better performance was obtained when using character-level rather than word-level information.", "labels": [], "entities": []}, {"text": "This confirms the results already reported by other researchers that have used character-level and word-level information for AA (.", "labels": [], "entities": [{"text": "AA", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.983116090297699}]}, {"text": "We believe this can be attributed to the fact that character n-grams provide a representation for the document at a finer granularity, which can be better exploited with local histogram representations.", "labels": [], "entities": []}, {"text": "Note that by considering 3-grams, words of length up to three are incorporated, and usually these words are function words (e.g., the, it, as, etc.), which are known to be indicative of writing style.", "labels": [], "entities": []}, {"text": "Also, n-gram information is more dense in documents than word-level information.", "labels": [], "entities": []}, {"text": "Hence, the local histograms are less sparse when using character-level information, which results in better AA performance.).", "labels": [], "entities": [{"text": "AA", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9208031892776489}]}, {"text": "Columns show the true author for test documents and rows show the authors predicted by the SVM.", "labels": [], "entities": []}, {"text": "shows the confusion matrix for the setting that reached the best results (i.e., column 3, last row in).", "labels": [], "entities": [{"text": "confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9894651174545288}]}, {"text": "From this table we can see that 8 out of the 10 authors were recognized with an accuracy higher or equal to 80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9995026588439941}]}, {"text": "For these authors sequential information seems to be particularly helpful.", "labels": [], "entities": []}, {"text": "However, low recognition performance was obtained for authors BL (B. K. Lim) and JM (J. MacArtney).", "labels": [], "entities": [{"text": "BL", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9584604501724243}]}, {"text": "The SVM with BOW representation of character ngrams achieved recognition rates of 40% and 50% for BL and JM respectively.", "labels": [], "entities": [{"text": "BOW", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9706512689590454}, {"text": "recognition", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9895580410957336}]}, {"text": "Thus, we can state that sequential information was indeed helpful for modeling BL writing style (improvement of 28%), although it is an author that resulted very difficult to model.", "labels": [], "entities": [{"text": "BL writing", "start_pos": 79, "end_pos": 89, "type": "TASK", "confidence": 0.8518268167972565}]}, {"text": "On the other hand, local histograms were not very useful for identifying documents written by JM (made it worse by \u22128%).", "labels": [], "entities": [{"text": "identifying documents written by JM", "start_pos": 61, "end_pos": 96, "type": "TASK", "confidence": 0.8710658311843872}]}, {"text": "The largest improvement (38%) of local histograms over the BOW formulation was obtained for author TN (T. Nissen).", "labels": [], "entities": [{"text": "BOW", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.5817492008209229}, {"text": "author TN (T. Nissen)", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.8219143052895864}]}, {"text": "This result gives evidence that TN uses a similar distribution of words in similar locations across the documents he writes.", "labels": [], "entities": []}, {"text": "These results are interesting, although we would like to perform a careful analysis of results in order to determine for what type of authors it would be beneficial to use local histograms, and what type of authors are better modeled with a standard BOW approach.", "labels": [], "entities": []}, {"text": "In this section we report results with RBC and IRBC data sets, which aim to evaluate the performance of our methods in a realistic setting.", "labels": [], "entities": [{"text": "IRBC data sets", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.8361138502756754}]}, {"text": "For these experiments we compare the performance of the BOW, LOWBOW histogram and BOLH representations; for the latter, we considered the best setting as reported in (i.e., an SVM with diffusion kernel and k = 20).", "labels": [], "entities": [{"text": "BOW", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9736658930778503}, {"text": "LOWBOW", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9578059315681458}, {"text": "BOLH", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.994596540927887}]}, {"text": "show the AA performances when using word and character information, respectively.", "labels": [], "entities": [{"text": "AA", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9635811448097229}]}, {"text": "We first analyze the results in the RBC data set (recall that for this data set we consider 1, 3, 5, 10, and 50, randomly selected documents per author).", "labels": [], "entities": [{"text": "RBC data set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9383344252904257}]}, {"text": "From we can see that BOW and LOWBOW histogram representations obtained similar performance to each other across the different training set sizes, which agree with results in for the BC data sets.", "labels": [], "entities": [{"text": "BOW", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9945000410079956}, {"text": "LOWBOW", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9576577544212341}, {"text": "BC data sets", "start_pos": 182, "end_pos": 194, "type": "DATASET", "confidence": 0.8791924516359965}]}, {"text": "The best performance across the different configurations of the RBC data set was obtained with the BOLH formulation (row 6 in Tables 5 and 6).", "labels": [], "entities": [{"text": "RBC data set", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.867368757724762}, {"text": "BOLH", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9973598122596741}]}, {"text": "The improvements of local histograms over the BOW formulation vary across different settings and when using information at word-level and character-level.", "labels": [], "entities": []}, {"text": "When using words (columns 2-6 in) the differences in performance are of 15.6%, 6.2%, 6.8%, 2.9%, 3.8% when using 1, 3, 5, 10 and 50 documents per author, respectively.", "labels": [], "entities": []}, {"text": "Thus, it is evident that local histograms are more beneficial when less documents are considered.", "labels": [], "entities": []}, {"text": "Here, the lack of information is compensated by the availability of several histograms per author.", "labels": [], "entities": []}, {"text": "When using character n-grams (columns 2-6 in) the corresponding differences in performance are of 5.4%, 6.4%, 6.4%, 6% and 11.4%, when using 1, 3, 5, 10, and 50 documents per author, respectively.", "labels": [], "entities": []}, {"text": "In this case, the larger improvement was obtained when 50 documents per author are available; nevertheless, one should note that results using character-level information are, in general, significantly better than those obtained with word-level information; hence, improvements are expected to be smaller.", "labels": [], "entities": []}, {"text": "When we compare the results of the BOLH formulation with the best reported results elsewhere (c.f. last row 6 in) (Plakias and Stamatatos, 2008b), we found that the improvements range from 14% to 30.2% when using character ngrams and from 1.2% to 26% when using words.", "labels": [], "entities": [{"text": "BOLH", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9662485718727112}]}, {"text": "The differences in performance are larger when less information is used (e.g., when 5 documents are used for training) and we believe the differences would be even larger if results for 1 and 3 documents were available.", "labels": [], "entities": []}, {"text": "These are very positive results; for example, we can obtain almost 71% of accuracy, using local histograms of character n-grams when a single document is available per author (recall that we have used all of the test samples for evaluating the performance of our methods).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995446801185608}]}, {"text": "We now analyze the performance of the different methods when using the IRBC data set (columns 7-9 in).", "labels": [], "entities": [{"text": "IRBC data set", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9789199034372965}]}, {"text": "The same pattern as before can be observed in experimental results for these data sets as well: BOW and LOWBOW histograms obtained comparable performance to each other and the BOLH formulation performed the best.", "labels": [], "entities": [{"text": "BOW", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9921616911888123}, {"text": "LOWBOW", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9590215086936951}, {"text": "BOLH", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9944685697555542}]}, {"text": "The BOLH formulation outperforms state of the art approaches by a considerable margin that ranges from 10% to 27%.", "labels": [], "entities": [{"text": "BOLH", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.994721531867981}]}, {"text": "Again, better results were obtained when using character n-grams for the local histograms.", "labels": [], "entities": []}, {"text": "With respect to RBC data sets, the BOLH at the character-level resulted very robust to the reduction of training set size and the highly imbalanced data.", "labels": [], "entities": [{"text": "RBC data sets", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.8288365006446838}, {"text": "BOLH", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9982494115829468}]}, {"text": "Summarizing, the results obtained in RBC and IRBC data sets show that the use of local histograms is advantageous under challenging conditions.", "labels": [], "entities": [{"text": "RBC", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.7338613867759705}, {"text": "IRBC data sets", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9079757332801819}]}, {"text": "An SVM under the BOLH representation is less sensitive to the number of training examples available and to the imbalance of data than an SVM using the BOW representation.", "labels": [], "entities": [{"text": "BOLH", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.8759183883666992}]}, {"text": "Our hypothesis for this behavior is that local histograms can bethought of as expanding training instances, because for each training instance in the BOW formulation we have k\u2212training instances under BOLH.", "labels": [], "entities": [{"text": "BOLH", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.9761812686920166}]}, {"text": "The benefits of such expansion become more notorious as the number of available documents per author decreases.: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms.", "labels": [], "entities": [{"text": "AA", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9995943903923035}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9600192904472351}, {"text": "IRBC (columns 7-9) data sets", "start_pos": 150, "end_pos": 178, "type": "DATASET", "confidence": 0.6889161552701678}]}, {"text": "We report results for the BOW, LOWBOW histogram and BOLH representations.", "labels": [], "entities": [{"text": "BOW", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9717830419540405}, {"text": "LOWBOW", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9620931148529053}, {"text": "BOLH", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9917003512382507}]}, {"text": "For reference (last row), we also include the best result reported in), when available, for each configuration.: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms.", "labels": [], "entities": [{"text": "AA", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9994945526123047}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9002918601036072}, {"text": "RBC", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.8319604396820068}, {"text": "IRBC data sets", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.8964979648590088}]}], "tableCaptions": [{"text": " Table 2: Authorship attribution accuracy for the BOW  representation and LOWBOW histograms. Column 2  shows the parameters we used for the LOWBOW his- tograms; columns 3 and 4 show results using words and  character n-grams, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8712034225463867}, {"text": "BOW", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.705528974533081}]}, {"text": " Table 3: Authorship attribution accuracy when using bags  of local histograms and different kernels for word-based  and character-based representations. The BC data set is  used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20,  respectively.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8323646783828735}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9391162395477295}, {"text": "BC data set", "start_pos": 158, "end_pos": 169, "type": "DATASET", "confidence": 0.8739646275838217}]}, {"text": " Table 4: Confusion matrix (in terms of percentages) for  the best result in the BC corpus (i.e., last row, column 3  in", "labels": [], "entities": [{"text": "Confusion matrix", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9764667451381683}, {"text": "BC corpus", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.9149678945541382}]}, {"text": " Table 5: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms. We report  results for the BOW, LOWBOW histogram and BOLH representations. For reference (last row), we also include the  best result reported in", "labels": [], "entities": [{"text": "AA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995118379592896}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9289887547492981}, {"text": "RBC", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.5803933143615723}, {"text": "IRBC", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.6186599731445312}, {"text": "BOW", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9308919310569763}, {"text": "BOLH", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9226580858230591}]}, {"text": " Table 6: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms.", "labels": [], "entities": [{"text": "AA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993730187416077}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9100179076194763}, {"text": "RBC", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.7829459309577942}, {"text": "IRBC data sets", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8705302874247233}]}]}