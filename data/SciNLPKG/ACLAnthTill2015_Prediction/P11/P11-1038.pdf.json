{"title": [{"text": "Lexical Normalisation of Short Text Messages: Makn Sens a #twitter", "labels": [], "entities": [{"text": "Lexical Normalisation of Short Text Messages", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8826124866803488}]}], "abstractContent": [{"text": "Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP.", "labels": [], "entities": []}, {"text": "In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words.", "labels": [], "entities": []}, {"text": "Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.", "labels": [], "entities": []}, {"text": "Both word similarity and context are then exploited to select the most probable correction candidate for the word.", "labels": [], "entities": []}, {"text": "The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter and other micro-blogging services are highly attractive for information extraction and text mining purposes, as they offer large volumes of real-time data, with around 65 millions tweets posted on Twitter per day in June 2010.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7729087471961975}, {"text": "text mining", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.7678618431091309}]}, {"text": "The quality of messages varies significantly, however, ranging from high quality newswire-like text to meaningless strings.", "labels": [], "entities": []}, {"text": "Typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures and emoticons abound in short text messages, causing grief for text processing tools.", "labels": [], "entities": []}, {"text": "For instance, presented with the input u must be talkin bout the paper but I was thinkin movies (\"You must be talking about the paper but I was thinking movies\"), the Stanford parser () analyses bout the paper and thinkin movies as a clause and noun phrase, respectively, rather than a prepositional phrase and verb phrase.", "labels": [], "entities": []}, {"text": "If there were someway of preprocessing the message to produce a more canonical lexical rendering, we would expect the quality of the parser to improve appreciably.", "labels": [], "entities": []}, {"text": "Our aim in this paper is this task of lexical normalisation of noisy English text, with a particular focus on Twitter and SMS messages.", "labels": [], "entities": [{"text": "lexical normalisation of noisy English text", "start_pos": 38, "end_pos": 81, "type": "TASK", "confidence": 0.8375144302845001}]}, {"text": "In this paper, we will collectively refer to individual instances of typos, ad hoc abbreviations, unconventional spellings, phonetic substitutions and other causes of lexical deviation as \"illformed words\".", "labels": [], "entities": []}, {"text": "The message normalisation task is challenging.", "labels": [], "entities": [{"text": "message normalisation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8813729882240295}]}, {"text": "It has similarities with spell checking, but differs in that ill-formedness in text messages is often intentional, whether due to the desire to save characters/keystrokes, for social identity, or due to convention in this text sub-genre.", "labels": [], "entities": [{"text": "spell checking", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7895164787769318}]}, {"text": "We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4 \"before\", which tend to be considered beyond the remit of spell checking ().", "labels": [], "entities": [{"text": "spell checkers", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7881967425346375}]}, {"text": "The free writing style of text messages makes the task even more complex, e.g. with word lengthening such as goooood being commonplace for emphasis.", "labels": [], "entities": []}, {"text": "In addition, the detection of ill-formed words is difficult due to noisy context.", "labels": [], "entities": [{"text": "detection of ill-formed words", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.8360969126224518}]}, {"text": "Our objective is to restore ill-formed words to their canonical lexical forms in standard English.", "labels": [], "entities": []}, {"text": "Through a pilot study, we compared OOV words in Twitter and SMS data with other domain corpora, 368 revealing their characteristics in OOV word distribution.", "labels": [], "entities": [{"text": "OOV word distribution", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.6770993868509928}]}, {"text": "We found Twitter data to have an unsurprisingly long tail of OOV words, suggesting that conventional supervised learning will not perform well due to data sparsity.", "labels": [], "entities": [{"text": "OOV", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9233884215354919}]}, {"text": "Additionally, many illformed words are ambiguous, and require context to disambiguate.", "labels": [], "entities": []}, {"text": "For example, Gooood may refer to Good or God depending on context.", "labels": [], "entities": []}, {"text": "This provides the motivation to develop a method which does not require annotated training data, but is able to leverage context for lexical normalisation.", "labels": [], "entities": []}, {"text": "Our approach first generates a list of candidate canonical lexical forms, based on morphological and phonetic variation.", "labels": [], "entities": []}, {"text": "Then, all candidates are ranked according to a list of features generated from noisy context and similarity between ill-formed words and candidates.", "labels": [], "entities": []}, {"text": "Our proposed cascaded method is shown to achieve state-of-the-art results on both SMS and Twitter data.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are as follows: (1) we conduct a pilot study on the OOV word distribution of Twitter and other text genres, and analyse different sources of non-standard orthography in Twitter; (2) we generate a text normalisation dataset based on Twitter data; (3) we propose a novel normalisation approach that exploits dictionary lookup, word similarity and word context, without requiring annotated data; and (4) we demonstrate that our method achieves state-of-the-art accuracy over both SMS and Twitter data.", "labels": [], "entities": [{"text": "OOV word distribution", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.6342265804608663}, {"text": "text normalisation", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.7026326507329941}, {"text": "accuracy", "start_pos": 490, "end_pos": 498, "type": "METRIC", "confidence": 0.997547447681427}]}], "datasetContent": [{"text": "The aim of our experiments is to compare the effectiveness of different methodologies over text messages, based on two datasets: (1) an SMS corpus; and (2) a novel Twitter dataset developed as part of this research, based on a random sampling of 549 English tweets.", "labels": [], "entities": []}, {"text": "The English tweets were annotated by three independent annotators.", "labels": [], "entities": []}, {"text": "All OOV words were pre-identified, and the annotators were requested to determine: (a) whether each OOV word was ill-formed or not; and (b) what the standard form was for ill-formed words, subject to the task definition outlined in Section 3.1.", "labels": [], "entities": []}, {"text": "The total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.", "labels": [], "entities": [{"text": "Twitter datasets", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.7031875997781754}]}, {"text": "The language filtering of Twitter to automatically identify English tweets was based on the language identification method of, using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and askew divergence nearest prototype classifier.", "labels": [], "entities": [{"text": "EuroGOV dataset", "start_pos": 137, "end_pos": 152, "type": "DATASET", "confidence": 0.9954856038093567}]}, {"text": "We reimplemented the state-of-art noisy channel model of and SMT approach of as benchmark methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9906856417655945}]}, {"text": "We implement the SMT approach in Moses (, with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9941520094871521}]}, {"text": "This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus.", "labels": [], "entities": [{"text": "SMS corpus", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.8517297208309174}]}, {"text": "The 10-fold cross-validated BLEU score () over this data is 0.81.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9759162366390228}]}, {"text": "In addition to comparing our method with competitor methods, we also study the contribution of different feature groups.", "labels": [], "entities": []}, {"text": "We separately compare dictionary lookup over our Internet slang dictionary, the contextual feature model, and the word similarity feature model, as well as combinations of these three.", "labels": [], "entities": []}, {"text": "The evaluation of lexical normalisation consists of two stages): (1) illformed word detection, and (2) candidate selection.", "labels": [], "entities": [{"text": "lexical normalisation", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7070411890745163}, {"text": "illformed word detection", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6016097764174143}]}, {"text": "In terms of detection, we want to make sense of how well the system can identify ill-formed words and leave correct OOV words untouched.", "labels": [], "entities": []}, {"text": "This step is crucial to further normalisation, because if correct OOV words are identified as ill-formed, the candidate selection step can never be correct.", "labels": [], "entities": [{"text": "normalisation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9788681268692017}]}, {"text": "Conversely, if an ill-formed word is predicted to be correct, the candidate selection will have no chance to normalise it.", "labels": [], "entities": []}, {"text": "We evaluate detection performance by token-level precision, recall and F-score (\u03b2 = 1).", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9554205536842346}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9997648596763611}, {"text": "F-score", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9994538426399231}]}, {"text": "Previous work over the SMS corpus has assumed perfect ill-formed word detection and focused only on the candidate selection step, so we evaluate ill-formed word detection for the Twitter data only.", "labels": [], "entities": [{"text": "SMS corpus", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.8779872953891754}, {"text": "word detection", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7744042873382568}, {"text": "word detection", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.7644787430763245}]}, {"text": "For candidate selection, we once again evaluate using token-level precision, recall and F-score.", "labels": [], "entities": [{"text": "candidate selection", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8200410306453705}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9183436036109924}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9996615648269653}, {"text": "F-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9977275729179382}]}, {"text": "Additionally, we evaluate using the BLEU score over the normalised form of each message, as the SMT method can lead to perturbations of the token stream, vexing standard precision, recall and F-score evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9727085530757904}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9833505749702454}, {"text": "precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.805686891078949}, {"text": "recall", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9995324611663818}, {"text": "F-score", "start_pos": 192, "end_pos": 199, "type": "METRIC", "confidence": 0.9972134232521057}]}, {"text": "Some conclusions can be drawn from the graphs.", "labels": [], "entities": []}, {"text": "First, higher detection threshold values (t d ) give better precision but lower recall.", "labels": [], "entities": [{"text": "detection threshold", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.9696272015571594}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9993116855621338}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.999091386795044}]}, {"text": "Generally, as t dis raised from 1 to 10, the precision improves slightly but recall drops dramatically, with the net effect that the F-score decreases monotonically.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9997199177742004}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9995262622833252}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9984013438224792}]}, {"text": "Thus, we use a 374 smaller threshold, i.e. t d = 1.", "labels": [], "entities": []}, {"text": "Second, there are differences between the two corpora, with dependencies from the Blog corpus producing slightly lower precision but higher recall, compared with the NYT corpus.", "labels": [], "entities": [{"text": "Blog corpus", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9496142268180847}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.999016284942627}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9992402791976929}, {"text": "NYT corpus", "start_pos": 166, "end_pos": 176, "type": "DATASET", "confidence": 0.9389200508594513}]}, {"text": "The lower precision for the Blog corpus appears to be due to the text not being as clean as NYT, introducing parser errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989871382713318}, {"text": "Blog corpus", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9588853418827057}, {"text": "NYT", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9341626763343811}]}, {"text": "Nevertheless, the difference in F-score between the two corpora is insignificant.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9995259046554565}]}, {"text": "Third, we obtain the best results, especially in terms of precision, for w d = 0.5, i.e. with expanded dependencies, but penalised relative to nonexpanded dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9994151592254639}]}], "tableCaptions": [{"text": " Table 1.  \"Letter\" refers to instances where letters are miss- ing or there are extraneous letters, but the lexi- cal correspondence to the target word form is triv- ially accessible (e.g. shuld \"should\"). \"Number  Substitution\" refers to instances of letter-number  substitution, where numbers have been substituted  for phonetically-similar sequences of letters (e.g. 4  \"for\"). \"Letter&Number\" refers to instances which  have both extra/missing letters and number substitu- tion (e.g. b4 \"before\"). \"Slang\" refers to instances", "labels": [], "entities": [{"text": "letter-number  substitution", "start_pos": 253, "end_pos": 280, "type": "TASK", "confidence": 0.7290609776973724}]}, {"text": " Table 2: Recall and average number of candidates for dif- ferent confusion set generation strategies", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9818472266197205}, {"text": "dif- ferent confusion set generation", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.627405529220899}]}]}