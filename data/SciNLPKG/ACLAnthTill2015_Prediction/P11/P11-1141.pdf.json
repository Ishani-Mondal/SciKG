{"title": [{"text": "Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Parsing the Internal Structure of Words", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8671730955441793}, {"text": "Chinese Word Segmentation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6227658092975616}]}], "abstractContent": [{"text": "Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes.", "labels": [], "entities": []}, {"text": "Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6387651463349661}]}, {"text": "In this paper we argue that this is unsatisfying in many ways, both practically and theoretically.", "labels": [], "entities": []}, {"text": "Instead, we propose that word structures should be recovered in morphological analysis.", "labels": [], "entities": []}, {"text": "An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction.", "labels": [], "entities": []}, {"text": "Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.9778582453727722}]}], "introductionContent": [], "datasetContent": [{"text": "For several reasons, it is a little tricky to evaluate the accuracy of our model for integrated morphological and syntactic parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9994314312934875}, {"text": "syntactic parsing", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.685070589184761}]}, {"text": "First and foremost, we currently know of no other same effort in parsing the structures of Chinese words, and we have to annotate word structures by ourselves.", "labels": [], "entities": [{"text": "parsing the structures of Chinese words", "start_pos": 65, "end_pos": 104, "type": "TASK", "confidence": 0.8649802903334299}]}, {"text": "Hence there is no baseline performance to compare with.", "labels": [], "entities": []}, {"text": "Secondly, simply reporting the accuracy of labeled precision and recall is not very informative because our parser takes raw sentences as input, and its output includes a lot of easy cases like word segmentation and partof-speech tagging results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9982481002807617}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.7448272705078125}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9972994923591614}, {"text": "word segmentation", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.728109821677208}, {"text": "partof-speech tagging", "start_pos": 216, "end_pos": 237, "type": "TASK", "confidence": 0.8265902996063232}]}, {"text": "Despite these difficulties, we note that higherlevel constituent parsing results are still somewhat comparable with previous performance in parsing Penn Chinese Treebank, because constituent parsing does not involve word structures directly.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.6963830888271332}, {"text": "Chinese Treebank", "start_pos": 153, "end_pos": 169, "type": "DATASET", "confidence": 0.9237731993198395}, {"text": "constituent parsing", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7348300814628601}]}, {"text": "Having said that, it must be pointed out that the comparison is meaningful only in a limited sense, as in previous literatures on Chinese parsing, the input is always word segmented or even part-of-speech tagged.", "labels": [], "entities": [{"text": "Chinese parsing", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.6548140645027161}]}, {"text": "That is, the bracketing in our case is around characters instead of words.", "labels": [], "entities": []}, {"text": "Another observation is we can still evaluate Chinese word segmentation and partof-speech tagging accuracy, by reading off the corresponding result from parse trees.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6001564264297485}, {"text": "partof-speech tagging", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7811582088470459}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9415236711502075}]}, {"text": "Again because we split the words with internal structures into their components, comparison with other systems should be viewed with that in mind.", "labels": [], "entities": []}, {"text": "Based on these discussions, we divide the labels of all constituents into three categories: Phrase labels are the labels in Peen Chinese Treebank for nonterminal phrase structures, including NP, VP, PP, etc.", "labels": [], "entities": [{"text": "Peen Chinese Treebank", "start_pos": 124, "end_pos": 145, "type": "DATASET", "confidence": 0.9044141173362732}]}, {"text": "POS labels represent part-of-speech tags such as NN, VV, DEG, etc.", "labels": [], "entities": []}, {"text": "Flat labels are generated in our annotation for words with no interesting structures.", "labels": [], "entities": []}, {"text": "Recall that they always end with an 'f' such as NNf, VVf and DEGf, etc.", "labels": [], "entities": [{"text": "DEGf", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9343902468681335}]}, {"text": "With this classification, we report our parser's accuracy for phrase labels, which is approximately the accuracy of constituent parsing of Penn Chinese Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9995212554931641}, {"text": "phrase labels", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.6964109241962433}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9992695450782776}, {"text": "Penn Chinese Treebank", "start_pos": 139, "end_pos": 160, "type": "DATASET", "confidence": 0.984691321849823}]}, {"text": "We report our parser's word segmentation accuracy based on the flat labels.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.686128169298172}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9396837949752808}]}, {"text": "This accuracy is in fact the joint accuracy of segmentation and part-of-speech tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9991707801818848}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9981729984283447}, {"text": "part-of-speech tagging", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6934675425291061}]}, {"text": "Most importantly, we can report our parser's accuracy in recovering word structures based on POS labels and flat labels, since word structures may contain only these two kinds of labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9995378255844116}]}, {"text": "With the standard split of CTB 5.0 data into training, development and test sets), the result are summarized in.", "labels": [], "entities": [{"text": "CTB 5.0 data", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9167414903640747}]}, {"text": "For all label categories, the PARSEEVAL measures (  Though not directly comparable, we can make some remarks to the accuracy of our model.", "labels": [], "entities": [{"text": "PARSEEVAL", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.993906557559967}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9993182420730591}]}, {"text": "For constituent parsing, the best result on CTB 5.0 is reported to be 78% F 1 measure for unlimited sentences with automatically assigned POS tags.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8234857022762299}, {"text": "CTB 5.0", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.937312126159668}, {"text": "F 1 measure", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.988801121711731}]}, {"text": "Our result for phrase labels is close to this accuracy.", "labels": [], "entities": [{"text": "phrase labels", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.7324019074440002}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.999479353427887}]}, {"text": "Besides, the result for flat labels compares favorably with the state of the art accuracy of about 93% F 1 for joint word segmentation and part-of-speech tagging ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.998950719833374}, {"text": "F 1", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9726364314556122}, {"text": "joint word segmentation", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.6080625454584757}, {"text": "part-of-speech tagging", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.6959225386381149}]}, {"text": "For ordinary word segmentation, the best result is reported to be around 97% F 1 on CTB 5.0 (), while our parser performs at 97.3%, though we should remember that the result concerns flat words only.", "labels": [], "entities": [{"text": "ordinary word segmentation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6154431104660034}, {"text": "F 1", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9921345412731171}, {"text": "CTB 5.0", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9295081198215485}]}, {"text": "Finally, we seethe performance of word structure recovery is almost as good as the recognition of flat words.", "labels": [], "entities": [{"text": "word structure recovery", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7805331150690714}, {"text": "recognition of flat words", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.8082660734653473}]}, {"text": "This means that parsing word structures accurately is possible with a generative model.", "labels": [], "entities": [{"text": "parsing word structures", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8747912049293518}]}, {"text": "It is interesting to see how well the parser does in recognizing the structure of words that were not seen during training.", "labels": [], "entities": []}, {"text": "For this, we sampled 100 such words including those with prefixes or suffixes and personal names.", "labels": [], "entities": []}, {"text": "We found that for 82 of these words, our parser can correctly recognize their structures.", "labels": [], "entities": []}, {"text": "This means our model has learnt something that generalizes well to unseen words.", "labels": [], "entities": []}, {"text": "In error analysis, we found that the parser tends to overgeneralize for prefix and suffix characters.", "labels": [], "entities": []}, {"text": "For example, \u398c\u658a\u455b 'great writer' is a noun phrase consisting of an adjective \u398c 'great' and a noun \u658a\u455b 'writer', as shown in is because the character \u455b 'expert' is a very productive suffix, as in \u447a\u60c6\u455b 'chemist' and \u5809\u4602\u455b 'diplomat'.", "labels": [], "entities": []}, {"text": "This observation is illuminating because most errors of our parser follow this pattern.", "labels": [], "entities": []}, {"text": "Currently we don't have any non-ad hoc way of preventing such kind of over generalization.", "labels": [], "entities": [{"text": "over generalization", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.6212086975574493}]}], "tableCaptions": [{"text": " Table 1. For all  label categories, the PARSEEVAL measures (", "labels": [], "entities": [{"text": "PARSEEVAL", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9855713844299316}]}, {"text": " Table 1: Labeled precision and recall for the three types  of labels. The line labeled 'Flat*' is for unlabeled met- rics of flat words, which is effectively the ordinary word  segmentation accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9890064001083374}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9994896650314331}, {"text": "word  segmentation", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.725449413061142}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.5096731781959534}]}]}