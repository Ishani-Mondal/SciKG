{"title": [{"text": "Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments", "labels": [], "entities": [{"text": "Grade Short Answer Questions", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6346597895026207}, {"text": "Semantic Similarity Measures", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.7339259584744772}, {"text": "Dependency Graph Alignments", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6246237456798553}]}], "abstractContent": [{"text": "In this work we address the task of computer-assisted assessment of short student answers.", "labels": [], "entities": [{"text": "assessment of short student answers", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.7376482486724854}]}, {"text": "We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation.", "labels": [], "entities": []}, {"text": "We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most important aspects of the learning process is the assessment of the knowledge acquired by the learner.", "labels": [], "entities": []}, {"text": "Ina typical classroom assessment (e.g., an exam, assignment or quiz), an instructor or a grader provides students with feedback on their answers to questions related to the subject matter.", "labels": [], "entities": []}, {"text": "However, in certain scenarios, such as a number of sites worldwide with limited teacher availability, online learning environments, and individual or group study sessions done outside of class, an instructor may not be readily available.", "labels": [], "entities": []}, {"text": "In these instances, students still need some assessment of their knowledge of the subject, and so, we must turn to computerassisted assessment (CAA).", "labels": [], "entities": []}, {"text": "While some forms of CAA do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers made up of free text that may require textual analysis.", "labels": [], "entities": [{"text": "CAA", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9522366523742676}]}, {"text": "Research to date has concentrated on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (, and the assessment of short student answers, which is the focus of this work.", "labels": [], "entities": []}, {"text": "An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers.", "labels": [], "entities": []}, {"text": "Note that this is different from the related tasks of paraphrase detection and textual entailment, since a common requirement in student answer grading is to provide a grade on a certain scale rather than make a simple yes/no decision.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.9658207595348358}, {"text": "textual entailment", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.693385049700737}]}, {"text": "In this paper, we explore the possibility of improving upon existing bag-of-words (BOW) approaches to short answer grading by utilizing machine learning techniques.", "labels": [], "entities": [{"text": "short answer grading", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.5904894173145294}]}, {"text": "Furthermore, in an attempt to mirror the ability of humans to understand structural (e.g. syntactic) differences between sentences, we employ a rudimentary dependency-graph alignment module, similar to those more commonly used in the textual entailment community.", "labels": [], "entities": [{"text": "dependency-graph alignment", "start_pos": 156, "end_pos": 182, "type": "TASK", "confidence": 0.705760657787323}]}, {"text": "Specifically, we seek answers to the following questions.", "labels": [], "entities": []}, {"text": "First, to what extent can machine learning be leveraged to improve upon existing approaches to short answer grading.", "labels": [], "entities": [{"text": "short answer grading", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.5812874436378479}]}, {"text": "Second, does the dependency parse structure of a text provide clues that can be exploited to improve upon existing BOW methodologies?", "labels": [], "entities": [{"text": "dependency parse structure of a text", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.8135572920242945}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: BOW Features with Question Demoting (QD).  Pearson's correlation, root mean square error (RMSE),  and median RMSE for all individual questions.", "labels": [], "entities": [{"text": "BOW", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9865404963493347}, {"text": "Pearson's correlation", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.9013885060946146}, {"text": "root mean square error (RMSE)", "start_pos": 76, "end_pos": 105, "type": "METRIC", "confidence": 0.8824462805475507}, {"text": "RMSE", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.5249813795089722}]}, {"text": " Table 6: Alignment Feature/Grade Correlations using  Pearson's \u03c1. Results are also reported when inverse doc- ument frequency weighting (IDF) and question demoting  (QD) are used.", "labels": [], "entities": [{"text": "inverse doc- ument frequency weighting (IDF)", "start_pos": 98, "end_pos": 142, "type": "METRIC", "confidence": 0.7489562001493242}, {"text": "question demoting  (QD", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.5975807458162308}]}, {"text": " Table 7: The results of the SVM models trained on the full suite of BOW measures, the alignment scores, and the  hybrid model. The terms \"normalized\", \"unnormalized\", and \"both\" indicate which subset of the 8 alignment features  were used to train the SVM model. For ease of comparison, we include in both sections the scores for the IAA, the  \"Average grade\" baseline, and two of the top performing BOW metrics -both with question demoting.", "labels": [], "entities": [{"text": "IAA", "start_pos": 335, "end_pos": 338, "type": "DATASET", "confidence": 0.7061290740966797}, {"text": "Average grade\" baseline", "start_pos": 346, "end_pos": 369, "type": "METRIC", "confidence": 0.9112843126058578}]}]}