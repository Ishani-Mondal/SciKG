{"title": [{"text": "Word Alignment via Submodular Maximization over Matroids", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7059719562530518}, {"text": "Submodular Maximization", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7956468164920807}]}], "abstractContent": [{"text": "We cast the word alignment problem as maximizing a submodular function under matroid constraints.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7816876769065857}]}, {"text": "Our framework is able to express complex interactions between alignment components while remaining computationally efficient , thanks to the power and generality of submodular functions.", "labels": [], "entities": []}, {"text": "We show that submod-ularity naturally arises when modeling word fertility.", "labels": [], "entities": []}, {"text": "Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches .", "labels": [], "entities": [{"text": "Hansards alignment task", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6653351783752441}, {"text": "alignment error", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.606564998626709}]}], "introductionContent": [{"text": "Word alignment is a key component inmost statistical machine translation systems.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7425341904163361}, {"text": "statistical machine translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6612057685852051}]}, {"text": "While classical approaches for word alignment are based on generative models (e.g., IBM models () and HMM (), word alignment can also be viewed as a matching problem, where each word pair is associated with a score reflecting the desirability of aligning that pair, and the alignment is then the highest scored matching under some constraints.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.8099215030670166}, {"text": "word alignment", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.8324394226074219}]}, {"text": "Several matching-based approaches have been proposed in the past.", "labels": [], "entities": []}, {"text": "introduces the competitive linking algorithm which greedily constructs matchings under the one-to-one mapping assumption.", "labels": [], "entities": []}, {"text": "In (), matchings are found using an algorithm for constructing a maximum weighted bipartite graph matching, where word pair scores come from alignment posteriors of generative models.", "labels": [], "entities": []}, {"text": "Similarly, cast word alignment as a maximum weighted matching problem and propose a framework for learning word pair scores as a function of arbitrary features of that pair.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7457430958747864}]}, {"text": "These approaches, however, have two potentially substantial limitations: words have fertility of at most one, and interactions between alignment decisions are not representable.", "labels": [], "entities": []}, {"text": "address this issue by formulating the alignment problem as a quadratic assignment problem, and off-the-shelf integer linear programming (ILP) solvers are used to solve to optimization problem.", "labels": [], "entities": []}, {"text": "While efficient for some median scale problems, ILP-based approaches are limited since when modeling more sophisticated interactions, the number of variables (and/or constraints) required grows polynomially, or even exponentially, making the resultant optimization impractical to solve.", "labels": [], "entities": []}, {"text": "In this paper, we treat the word alignment problem as maximizing a submodular function subject to matroid constraints (to be defined in Section 2).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7932794690132141}]}, {"text": "Submodular objective functions can represent complex interactions among alignment decisions, and essentially extend the modular (linear) objectives used in the aforementioned approaches.", "labels": [], "entities": []}, {"text": "While our extensions add expressive power, they do not result in a heavy computational burden.", "labels": [], "entities": []}, {"text": "This is because maximizing a monotone submodular function under a matroid constraint can be solved efficiently using a simple greedy algorithm.", "labels": [], "entities": []}, {"text": "The greedy algorithm, moreover, is a constant factor approximation algorithm that guarantees a near-optimal solution.", "labels": [], "entities": []}, {"text": "In this paper, we moreover show that submodularity naturally arises in word alignment problems when modeling word fertility (see Section 4).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7860020697116852}]}, {"text": "Experiment results on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to the maximum weighted matching approach, while being at least 50 times 170 faster than an ILP-based approach.", "labels": [], "entities": [{"text": "Hansards alignment task", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.5450751384099325}, {"text": "alignment error rates", "start_pos": 103, "end_pos": 124, "type": "METRIC", "confidence": 0.8604506055514017}]}], "datasetContent": [{"text": "We evaluated our approaches using the EnglishFrench Hansards data from the 2003 NAACL shared task (.", "labels": [], "entities": [{"text": "EnglishFrench Hansards data from the 2003 NAACL shared task", "start_pos": 38, "end_pos": 97, "type": "DATASET", "confidence": 0.9409364991717868}]}, {"text": "This corpus consists of 1.1M automatically aligned sentences, and comes with a test set of 447 sentences, which have been hand-aligned and are marked with both \"sure\" and \"possible\" alignments (.", "labels": [], "entities": []}, {"text": "Using these alignments, alignment error rate (AER) is calculated as: where S is the set of sure gold pairs, and P is the set of possible gold pairs.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.9177875022093455}]}, {"text": "We followed the work in () and split the original test set into 347 test examples, and 100 training examples for parameters tuning.", "labels": [], "entities": []}, {"text": "In general, the score of aligning i to j can be modeled as a function of arbitrary features.", "labels": [], "entities": []}, {"text": "Although parameter learning in our framework would be another interesting topic to study, we focus herein on the inference problem.", "labels": [], "entities": [{"text": "parameter learning", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7374792695045471}]}, {"text": "Therefore, only one feature (Eq. 5) was used in our experiments in order for no feature weight learning to be required.", "labels": [], "entities": []}, {"text": "In particular, we estimated the score of aligning i to j as where the translation probability p(f j |e i ) and alignment probability p(i|j, I) were obtained from IBM model 2 trained on the 1.1M sentences.", "labels": [], "entities": [{"text": "alignment probability p", "start_pos": 111, "end_pos": 134, "type": "METRIC", "confidence": 0.9680537184079488}]}, {"text": "The IBM 2 models gives an AER of 21.0% with French as the target, inline with the numbers reported in and.", "labels": [], "entities": [{"text": "AER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9997186064720154}]}, {"text": "We tested two types of partition matroid constraints.", "labels": [], "entities": []}, {"text": "The first is a global matroid constraint: which restricts fertility of all words on F side to beat most b.", "labels": [], "entities": []}, {"text": "This constraint is denoted as Fert F (A) \u2264 bin for simplicity.", "labels": [], "entities": [{"text": "Fert F (A) \u2264 bin", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.930929924760546}]}, {"text": "The second type, denoted as Fert F (A) \u2264 k j , is word-dependent: where the fertility of word on j is restricted to beat most k j . Here k j = max{b : , where \u03b8 is a threshold and p b (f ) is the probability that French word f was aligned to at most b English words based on the IBM 2 alignment.", "labels": [], "entities": [{"text": "Fert F (A)", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9246081590652466}]}, {"text": "As mentioned in Section 3, matroid constraints generalize the matching constraint.", "labels": [], "entities": []}, {"text": "In particular, when using two matroid constraints, Fert E (A) \u2264 1 and Fert F (A) \u2264 1, we have the matching constraint where fertility for both English and French words are restricted to beat most one.", "labels": [], "entities": [{"text": "Fert E (A)", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9417118191719055}, {"text": "Fert F (A)", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9511350870132447}]}, {"text": "Our setup 1 (see Table 1) uses these two constraints along with a modular objective function, which is equivalent to the maximum weighted bipartite matching problem.", "labels": [], "entities": []}, {"text": "Using 173 greedy algorithm to solve this problem, we get AER 21.0% (setup 1 in) -no significant difference compared to the AER (20.9%) achieved by the exact solution (maximum weighted bipartite matching approach), illustrating that greedy solutions are nearoptimal.", "labels": [], "entities": [{"text": "AER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9993079900741577}, {"text": "AER", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.999327540397644}]}, {"text": "Note that the bipartite matching approach does not improve performance over IBM 2 model, presumably because only one feature was used here.", "labels": [], "entities": []}, {"text": "When allowing fertility of English words to be more than one, we see a significant AER reduction using a submodular objective (setup 4 and 5) instead of a modular objective (setup 2 and 3), which verifies our claim that submodularity lends itself to modeling the marginal benefit of growing fertility.", "labels": [], "entities": [{"text": "AER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9938883185386658}]}, {"text": "In setup 2 and 4, while allowing larger fertility for English words, we restrict the fertility of French words to be most one.", "labels": [], "entities": []}, {"text": "To allow higher fertility for French words, one possible approach is to use constraint Fert F (A) \u2264 2, in which all French words are allowed to have fertility up to 2.", "labels": [], "entities": [{"text": "Fert F (A) \u2264 2", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.9257379003933498}]}, {"text": "This approach, however, results in a significant increase of false positive alignments since all French words tend to collect as many matches as permitted.", "labels": [], "entities": []}, {"text": "This issue could be alleviated by introducing asymmetric version of the objective function in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 94, "end_pos": 96, "type": "DATASET", "confidence": 0.9065880179405212}]}, {"text": "3 such that marginal benefit of higher fertility of French words are also compressed.", "labels": [], "entities": []}, {"text": "Alternatively, we use the second type of matroid constraint in which fertility upper bounds of French words are word-dependent instead of global.", "labels": [], "entities": []}, {"text": "With \u03b8 = .8, about 10 percent of the French words have k j equal to 2 or greater.", "labels": [], "entities": []}, {"text": "By using the word-dependent matroid constraint (setup 3 and 5), AERs are reduced compared to those using global matroid constraints.", "labels": [], "entities": [{"text": "AERs", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9977243542671204}]}, {"text": "In particular, 18.6% AER is achieved by setup 5, which significantly outperforms the maximum weighted bipartite matching approach.", "labels": [], "entities": [{"text": "AER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.999589741230011}]}, {"text": "We also compare our method with model of which also allows fertility larger than one by penalizing different levels of fertility.", "labels": [], "entities": []}, {"text": "We used s i,j as an edge feature and p b (f ) as anode feature together with two additional features: a bias feature and the bucketed frequency of the word type.", "labels": [], "entities": []}, {"text": "The same procedures for training and decoding as in) were performed where MOSEK was used as the ILP solver.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 96, "end_pos": 106, "type": "TASK", "confidence": 0.6360332816839218}]}, {"text": "As shown in, performance of setup 5 outperforms this model and moreover, our approach is at least 50 times faster: it took our approach only about half a second to align all the 347 test set sentence pairs whereas using the ILP-based approach took about 40 seconds.", "labels": [], "entities": []}], "tableCaptions": []}