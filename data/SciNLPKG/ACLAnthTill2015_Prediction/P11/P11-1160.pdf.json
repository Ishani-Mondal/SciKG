{"title": [], "abstractContent": [{"text": "Recent work has shown how a parallel corpus can be leveraged to build syntactic parser fora target language by projecting automatic source parse onto the target sentence using word alignments.", "labels": [], "entities": []}, {"text": "The projected target dependency parses are not always fully connected to be useful for training traditional dependency parsers.", "labels": [], "entities": []}, {"text": "In this paper, we present a greedy non-directional parsing algorithm which doesn't need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them.", "labels": [], "entities": []}, {"text": "Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi.", "labels": [], "entities": []}, {"text": "It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (), syntactic parsing ( and machine translation).", "labels": [], "entities": [{"text": "Part-Of-Speech (POS) tagging", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.6444824814796448}, {"text": "word sense disambiguation", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.6773391167322794}, {"text": "syntactic parsing", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.7683437764644623}, {"text": "machine translation", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.7867444157600403}]}, {"text": "Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora.", "labels": [], "entities": []}, {"text": "Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank fora target language.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7349321097135544}]}, {"text": "The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool.", "labels": [], "entities": []}, {"text": "Due to the usage of automatic source parses, automatic word alignments and differences in the annotation schemes of source and target languages, the projected parses are not always fully connected and can have edges missing.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7085040509700775}]}, {"text": "Nonliteral translations and divergences in the syntax of the two languages also lead to incomplete projected parse trees.", "labels": [], "entities": []}, {"text": "shows an English-Hindi parallel sentence with correct source parse, alignments and target dependency parse.", "labels": [], "entities": []}, {"text": "For the same sentence, is a sample partial dependency parse projected using an automatic source parser on aligned text.", "labels": [], "entities": []}, {"text": "This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out all our experiments on parallel corpora belonging to English-Hindi, EnglishBulgarian and English-Spanish language pairs.", "labels": [], "entities": []}, {"text": "While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (  were used in our work (7 rules dataset for Bulgarian and 3 rules dataset for Spanish).", "labels": [], "entities": [{"text": "Hindi projected treebank", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.7120078802108765}]}, {"text": "The Hindi, Bulgarian and Spanish projected dependency treebanks have 44760, 39516 and 76958 sentences respectively.", "labels": [], "entities": []}, {"text": "Since we don't have confidence scores for the projections on the sentences, we picked 10,000 sentences randomly in each of the three datasets for training the parsers 2 . Other methods of choosing the 10K sentences such as those with the max. no. of relations, those with least no. of unconnected words, those with max. no. of contiguous partial trees that can be learned by GNPPA parser etc. were tried out.", "labels": [], "entities": []}, {"text": "Among all these, random selection was consistent and yielded the best results.", "labels": [], "entities": []}, {"text": "The errors introduced in the projected parses by errors in word alignment, source parser and projection are not consistent enough to be exploited to select the better parses from the entire projected data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7015945911407471}]}, {"text": "gives an account of the randomly chosen 10k sentences in terms of the number of words, words without parents etc.", "labels": [], "entities": []}, {"text": "Around 40% of the words spread over 88% of sentences in Bulgarian and 97% of sentences in Spanish have no parents.", "labels": [], "entities": []}, {"text": "Traditional dependency parsers which only train from fully connected trees would not be able to learn from these sentences.", "labels": [], "entities": []}, {"text": "P(GNPPA) is the percentage of relations in the data that are learned by the GNPPA parser satisfying the contiguous partial tree constraint and P(E-GNPPA) is the per-: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained on 10k parses selected randomly.", "labels": [], "entities": [{"text": "P(GNPPA)", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8837961405515671}]}, {"text": "Punct indicates evaluation with punctuation whereas NoPunct indicates without punctuation.", "labels": [], "entities": [{"text": "Punct", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9865180850028992}]}, {"text": "* next to an accuracy denotes statistically significant (McNemar's and p < 0.05) improvement over the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9707399010658264}, {"text": "McNemar's and p < 0.05)", "start_pos": 57, "end_pos": 80, "type": "METRIC", "confidence": 0.9155500275748116}]}, {"text": "\u2020 denotes significance over GNPPA centage that satisfies the partially contiguous constraint.", "labels": [], "entities": [{"text": "GNPPA centage", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8254080414772034}]}, {"text": "E-GNPPA parser learns around 2-5% more no. of relations than GNPPA due to the relaxation in the constraints.", "labels": [], "entities": []}, {"text": "The Hindi test data that was released as part of the was used for evaluation.", "labels": [], "entities": [{"text": "Hindi test data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.787472685178121}]}, {"text": "For Bulgarian and Spanish, we used the same test data that was used in the work of.", "labels": [], "entities": []}, {"text": "These test datasets had sentences from the training section of the CoNLL Shared Task () that had lengths less than or equal to 10.", "labels": [], "entities": [{"text": "CoNLL Shared Task", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.8318796157836914}]}, {"text": "All the test datasets have gold POS tags.", "labels": [], "entities": []}, {"text": "A baseline parser was built to compare learning from partial parses with learning from fully connected parses.", "labels": [], "entities": []}, {"text": "Full parses are constructed from partial parses in the projected data by randomly assigning parents to unconnected parents, similar to the work in ().", "labels": [], "entities": []}, {"text": "The unconnected words in the parse are selected randomly one by one and are assigned parents randomly to complete the parse.", "labels": [], "entities": []}, {"text": "This process is repeated for all the sentences in the three language datasets.", "labels": [], "entities": []}, {"text": "The parser is then trained with the GNPPA algorithm on these fully connected parses to be used as the baseline.", "labels": [], "entities": []}, {"text": "lists the accuracies of the baseline, GNPPA and E-GNPPA parsers.", "labels": [], "entities": []}, {"text": "The accuracies are unlabeled attachment scores (UAS): the percentage of words with the correct head.", "labels": [], "entities": [{"text": "unlabeled attachment scores (UAS)", "start_pos": 19, "end_pos": 52, "type": "METRIC", "confidence": 0.813901831706365}]}, {"text": "compares our accuracies with those reported in () for Bulgarian and Spanish.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9973709583282471}]}], "tableCaptions": [{"text": " Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  \u2020 denotes significance over GNPPA", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7470956444740295}, {"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.9630723595619202}, {"text": "GNPPA", "start_pos": 368, "end_pos": 373, "type": "DATASET", "confidence": 0.6607487201690674}]}, {"text": " Table 4: Comparison of baseline, GNPPA and E- GNPPA with baseline and discriminative model  from (", "labels": [], "entities": []}]}