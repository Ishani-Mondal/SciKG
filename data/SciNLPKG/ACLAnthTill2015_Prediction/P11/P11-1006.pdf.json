{"title": [{"text": "A Fast and Accurate Method for Approximate String Search", "labels": [], "entities": [{"text": "Accurate", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9908456206321716}, {"text": "Approximate String Search", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6987262864907583}]}], "abstractContent": [{"text": "This paper proposes anew method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows.", "labels": [], "entities": [{"text": "approximate string search", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.8437481721242269}, {"text": "candidate generation", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7581247687339783}, {"text": "spelling error correction", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.7336388826370239}]}, {"text": "Given a misspelled word, the system finds words in a dictionary, which are most \"similar\" to the misspelled word.", "labels": [], "entities": []}, {"text": "The paper proposes a probabilistic approach to the task, which is both accurate and efficient.", "labels": [], "entities": [{"text": "accurate", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9718962907791138}]}, {"text": "The approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates.", "labels": [], "entities": []}, {"text": "The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word.", "labels": [], "entities": []}, {"text": "The learning method employs the criterion in candidate generation as loss function.", "labels": [], "entities": []}, {"text": "The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates.", "labels": [], "entities": []}, {"text": "Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9987493753433228}]}], "introductionContent": [{"text": "This paper addresses the following problem, referred to as approximate string search.", "labels": [], "entities": [{"text": "approximate string search", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6300098995367686}]}, {"text": "Given a query string, a dictionary of strings (vocabulary), and a set of operators, the system returns the top k strings in the dictionary that can be transformed from the query string by applying several operators in the operator set.", "labels": [], "entities": []}, {"text": "Here each operator is a rule that can replace a substring in the query string with another substring.", "labels": [], "entities": []}, {"text": "The top k results are defined in * Contribution during internship at Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.8958180546760559}]}, {"text": "terms of an evaluation measure employed in a specific application.", "labels": [], "entities": []}, {"text": "The requirement is that the task must be conducted very efficiently.", "labels": [], "entities": []}, {"text": "Approximate string search is useful in many applications including spelling error correction, similar terminology retrieval, duplicate detection, etc.", "labels": [], "entities": [{"text": "Approximate string search", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6563177605470022}, {"text": "spelling error correction", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.8380616903305054}, {"text": "terminology retrieval", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.6732253879308701}, {"text": "duplicate detection", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.828078031539917}]}, {"text": "Although certain progress has been made for addressing the problem, further investigation on the task is still necessary, particularly from the viewpoint of enhancing both accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9992013573646545}]}, {"text": "Without loss of generality, in this paper we address candidate generation in spelling error correction.", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.7997251152992249}]}, {"text": "Candidate generation is to find the most possible corrections of a misspelled word.", "labels": [], "entities": [{"text": "Candidate generation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7446108758449554}]}, {"text": "In such a problem, strings are words, and the operators represent insertion, deletion, and substitution of characters with or without surrounding characters, for example, \"a\"\u2192\"e\" and \"lly\"\u2192\"ly\".", "labels": [], "entities": []}, {"text": "Note that candidate generation is concerned with a single word; after candidate generation, the words surrounding it in the text can be further leveraged to make the final candidate selection, e.g.,,.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8219568729400635}]}, {"text": "In spelling error correction, proposed employing a generative model for candidate generation and a hierarchy of trie structures for fast candidate retrieval.", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.8703250288963318}, {"text": "candidate generation", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.866569846868515}]}, {"text": "Our approach is a discriminative approach and is aimed at improving Brill and Moore's method.", "labels": [], "entities": []}, {"text": "proposed using a logistic regression model for approximate dictionary matching.", "labels": [], "entities": [{"text": "approximate dictionary matching", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.5954847633838654}]}, {"text": "Their method is also a discriminative approach, but it is largely different from our approach in the following points.", "labels": [], "entities": []}, {"text": "It formalizes the problem as binary classification and assumes that there is only one rule applicable each time in candidate generation.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7215493023395538}]}, {"text": "Efficiency is also not a major concern for them, because it is for offline text mining.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8835024237632751}, {"text": "text mining", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.720696821808815}]}, {"text": "There are two fundamental problems in research on approximate string search: (1) how to build a model that can archive both high accuracy and efficiency, and (2) how to develop a data structure and algorithm that can facilitate efficient retrieval of the top k candidates.", "labels": [], "entities": [{"text": "approximate string search", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7341790994008383}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9889628291130066}]}, {"text": "In this paper, we propose a probabilistic approach to the task.", "labels": [], "entities": []}, {"text": "Our approach is novel and unique in the following aspects.", "labels": [], "entities": []}, {"text": "It employs (a) a log-linear (discriminative) model for candidate generation, (b) an effective algorithm for model learning, and (c) an efficient algorithm for candidate retrieval.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.8542180359363556}, {"text": "model learning", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.7044807970523834}, {"text": "candidate retrieval", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7893211245536804}]}, {"text": "The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction given the misspelled word.", "labels": [], "entities": []}, {"text": "The learning method employs, in the training process, a criterion that represents the goal of making both accurate and efficient prediction (candidate generation).", "labels": [], "entities": []}, {"text": "As a result, the model is optimally trained toward its objective.", "labels": [], "entities": []}, {"text": "The retrieval algorithm uses special data structures and efficiently performs the top k candidates finding.", "labels": [], "entities": []}, {"text": "It is guaranteed to find the best k candidates without enumerating all the possible ones.", "labels": [], "entities": []}, {"text": "We empirically evaluated the proposed method in spelling error correction of web search queries.", "labels": [], "entities": [{"text": "spelling error correction of web search queries", "start_pos": 48, "end_pos": 95, "type": "TASK", "confidence": 0.8319394077573504}]}, {"text": "The experimental results have verified that the accuracy of the top candidates given by our method is significantly higher than those given by the baseline methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9992952346801758}]}, {"text": "Our method is more accurate than the baseline methods in different settings such as large rule sets and large vocabulary sizes.", "labels": [], "entities": []}, {"text": "The efficiency of our method is also very high in different experimental settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimentally evaluated our approach in spelling error correction of queries in web search.", "labels": [], "entities": [{"text": "spelling error correction of queries", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.753429627418518}]}, {"text": "The problem is more challenging than usual due to the following reasons.", "labels": [], "entities": []}, {"text": "The vocabulary of queries in web search is extremely large due to the scale, diversity, and dynamics of the Internet.", "labels": [], "entities": []}, {"text": "Efficiency is critically important, because the response time of top k candidate retrieval for web search must be kept very low.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.966799259185791}]}, {"text": "Our approach for candidate generation is in fact motivated by the application.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8507677912712097}]}, {"text": "Two representative methods were used as baselines: the generative model proposed by) referred to as generative and the logistic regression model proposed by Misspelled Correct Misspelled: Examples of Word Pairs referred to as logistic.", "labels": [], "entities": []}, {"text": "Note that's model is not particularly for spelling error correction, but it can be employed in the task.", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.8046281337738037}]}, {"text": "When using their method for ranking, we used outputs of the logistic regression model as rank scores.", "labels": [], "entities": []}, {"text": "We compared our method with the two baselines in terms of top k accuracy, which is ratio of the true corrections among the top k candidates generated by a method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9631916880607605}]}, {"text": "All the methods shared the same settings: 973,902 words in the vocabulary, 10,597 rules for correction, and up to two rules used in one transformation.", "labels": [], "entities": []}, {"text": "We made use of 100,000 word pairs mined from query sessions for training, and 10,000 word pairs for testing.", "labels": [], "entities": []}, {"text": "The experimental results are shown in.", "labels": [], "entities": []}, {"text": "We can see that our method always performs the best when compared with the baselines and the improvements are statistically significant (p < 0.01).", "labels": [], "entities": []}, {"text": "The logistic method works better than generative, when k is small, but its performance becomes saturated, when k is large.", "labels": [], "entities": [{"text": "generative", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.9750428795814514}]}, {"text": "Usually a discriminative model works better than a generative model, and that seems to be what happens with small k's.", "labels": [], "entities": []}, {"text": "However, logistic cannot work so well for large k's, because it only allows the use of one rule each time.", "labels": [], "entities": []}, {"text": "We observe that there are many word pairs in the data that need to be transformed with multiple rules.", "labels": [], "entities": []}, {"text": "Next, we conducted experiments to investigate how the top k accuracy changes with different sizes of vocabularies, maximum numbers of applicable rules and sizes of rule set for the three methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.8975930213928223}]}, {"text": "The experimental results are shown in and.", "labels": [], "entities": []}, {"text": "For the experiment in, we enlarged the vocabulary size from 973,902 (smallVocab) to 2,206,948 (largeVocab) and kept the other settings the same as in the previous experiment.", "labels": [], "entities": []}, {"text": "Because more candidates can be generated with a larger vocabulary, the performances of all the methods de- cline.", "labels": [], "entities": []}, {"text": "However, the drop of accuracy by our method is much smaller than that by generative, which means our method is more powerful when the vocabulary is large, e.g., for web search.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9988974332809448}]}, {"text": "For the experiment in, we changed the maximum number of rules that can be applied to a transformation from 2 to 3.", "labels": [], "entities": []}, {"text": "Because logistic can only use one rule at a time, it is not included in this experiment.", "labels": [], "entities": []}, {"text": "When there are more applicable rules, more candidates can be generated and thus ranking of them becomes more challenging.", "labels": [], "entities": []}, {"text": "The accuracies of both methods drop, but our method is constantly better than generative.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.991640567779541}]}, {"text": "Moreover, the decrease inaccuracy by our method is clearly less than that by generative.", "labels": [], "entities": []}, {"text": "For the experiment in, we enlarged the number of rules from 10,497 (smallRuleNum) to 24,054 (largeRuleNum).", "labels": [], "entities": []}, {"text": "The performance of our method and those of the two baselines did not change so much, and our method still visibly outperform the baselines when more rules are exploited.", "labels": [], "entities": []}, {"text": "We have also experimentally evaluated the efficiency of our approach.", "labels": [], "entities": []}, {"text": "Because most existing work uses a predefined ranking function, it is not fair to make a comparison with them.", "labels": [], "entities": []}, {"text": "Moreover, Okazaki et al.'", "labels": [], "entities": []}, {"text": "method does not consider efficiency, and Brill and Moore's method is based a complicated retrieve algorithm which is very hard to implement.", "labels": [], "entities": []}, {"text": "Instead of making comparison with the existing methods in terms of efficiency, we evaluated the efficiency of our method by looking at how efficient it becomes with its data structure and pruning technique.", "labels": [], "entities": []}, {"text": "Next, since the running time of our method is proportional to the number of visited nodes on the vocabulary trie, we evaluated the efficiency of our method in terms of number of visited nodes.", "labels": [], "entities": []}, {"text": "The result reported here is that when k is 10.", "labels": [], "entities": []}, {"text": "Specifically, we tested how the number of visited nodes changes according to three factors: maximum number of applicable rules in a transformation, vocabulary size and rule set size.", "labels": [], "entities": []}, {"text": "The experimental results are shown in and respectively.", "labels": [], "entities": []}, {"text": "From, with increasing maximum number of applicable rules in a transformation, number of visited nodes increases first and then stabilizes, especially when the words are long.", "labels": [], "entities": []}, {"text": "Note that pruning becomes even more effective because number of visited nodes without pruning grows much faster.", "labels": [], "entities": []}, {"text": "It demonstrates that our method is very efficient when compared to the non-pruning method.", "labels": [], "entities": []}, {"text": "Admittedly, the efficiency of our method also deteriorates somewhat.", "labels": [], "entities": []}, {"text": "This would not cause a noticeable issue in real applications, however.", "labels": [], "entities": []}, {"text": "In the previous section, we have seen that using up to two rules in a transformation can bring a very high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9983909130096436}]}, {"text": "From and, we can conclude that the numbers of visited nodes are stable and thus the efficiency of our method keeps high with larger vocabulary size and number of rules.", "labels": [], "entities": []}, {"text": "It indicates that our pruning strategy is very effective.", "labels": [], "entities": []}, {"text": "From all the figures, we can see that our method is always efficient especially when the words are relatively short.", "labels": [], "entities": []}, {"text": "In Section 3.1, we introduce the non-positive constraints on the parameters, i.e., \u2200\u03bb r \u2264 0, to enable the pruning technique for efficient top k retrieval.", "labels": [], "entities": []}, {"text": "We experimentally verified the impact of the constraints to both the accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9992735981941223}]}, {"text": "For ease of reference, we name the model with the non-positive constraints as bounded, and the origi-.", "labels": [], "entities": []}, {"text": "All the experiments were conducted based on the typical setting of our experiments: 973,902 words in the vocabulary, 10,597 rules, and up to two rules in one transformation.", "labels": [], "entities": []}, {"text": "In, we can see that the difference between bounded and unbounded in terms of accuracy is negligible, and we can draw a conclusion that adding the constraints does not hurt the accuracy., it is easy to note that bounded is much faster than unbounded because our pruning strategy can be applied to bounded.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9992417097091675}, {"text": "accuracy.", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.999498724937439}]}], "tableCaptions": []}