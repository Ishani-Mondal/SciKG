{"title": [], "abstractContent": [{"text": "We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation , which converts a source tree to a target string.", "labels": [], "entities": []}, {"text": "Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7368434816598892}]}, {"text": "As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included.", "labels": [], "entities": []}, {"text": "Less than 2 times slower, the adjoining tree-to-string system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST Chinese-English test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9990473389625549}, {"text": "NIST Chinese-English test sets", "start_pos": 168, "end_pos": 198, "type": "DATASET", "confidence": 0.917425274848938}]}], "introductionContent": [{"text": "Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years.", "labels": [], "entities": [{"text": "Syntax-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6647568643093109}, {"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7569805979728699}]}, {"text": "So far, most of them have been based on synchronous context-free grammars (CFG), tree substitution grammars (TSG);, and inversion transduction grammars (ITG)).", "labels": [], "entities": []}, {"text": "Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax.", "labels": [], "entities": []}, {"text": "For example, points out that the translation of languages that can stack an unbounded number of clauses in an \"inside-out\" way ( provably goes beyond the expressive power of synchronous CFG and TSG.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7764919698238373}]}, {"text": "Synchronous tree adjoining grammars (TAG)) area good candidate.", "labels": [], "entities": []}, {"text": "As a formal tree rewriting system, TAG provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks.", "labels": [], "entities": [{"text": "TAG", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8357322216033936}, {"text": "CFG", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9160571098327637}]}, {"text": "As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages.", "labels": [], "entities": [{"text": "TAG", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7602625489234924}]}, {"text": "Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs.", "labels": [], "entities": [{"text": "TAG", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8581972718238831}]}, {"text": "The idea of using synchronous TAG in machine translation has been pursued by several researchers (), but only recently in its probabilistic form.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7346512675285339}]}, {"text": "argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system.", "labels": [], "entities": []}, {"text": "However, one major challenge for applying synchronous TAG to machine translation is computational complexity.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7857646048069}]}, {"text": "While TAG requires O(n 6 ) time for monolingual parsing, synchronous TAG requires O(n 12 ) for bilingual parsing.", "labels": [], "entities": [{"text": "TAG", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.37622305750846863}, {"text": "O(n 6 ) time", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.8811033666133881}, {"text": "O", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9653494954109192}, {"text": "bilingual parsing", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.617531031370163}]}, {"text": "One solution is to use tree insertion grammars (TIG) introduced by.", "labels": [], "entities": [{"text": "tree insertion grammars (TIG)", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.8176945249239603}]}, {"text": "As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n 3 ) time for monolingual parsing.", "labels": [], "entities": [{"text": "TIG", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.5283820629119873}]}, {"text": "firstly demonstrate 1278 Figure 1: Initial and auxiliary tree pairs.", "labels": [], "entities": []}, {"text": "The source side (Chinese) is a Treebank-style linguistic tree.", "labels": [], "entities": [{"text": "Treebank-style linguistic tree", "start_pos": 31, "end_pos": 61, "type": "DATASET", "confidence": 0.9218398332595825}]}, {"text": "The target side (English) is a purely structural tree using a single non-terminal (X).", "labels": [], "entities": []}, {"text": "By convention, substitution and foot nodes are marked with a down arrow (\u2193) and an asterisk ( * ), respectively.", "labels": [], "entities": []}, {"text": "The dashed lines link substitution sites (e.g., NP \u2193 and X \u2193 in \u03b2 1 ) and adjoining sites (e.g., NP and X in \u03b1 2 ) in tree pairs.", "labels": [], "entities": []}, {"text": "Substituting the initial tree pair \u03b1 1 at the NP \u2193 -X \u2193 node pair in the auxiliary tree pair \u03b2 1 yields a derived tree pair \u03b2 2 , which can be adjoined at NN-X in \u03b1 2 to generate \u03b1 3 . the use of synchronous TIG for machine translation and report promising results.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.7453500032424927}]}, {"text": "prove that adjoining can improve translation quality significantly over a state-of-the-art stringto-tree system () that uses synchronous TSG with tractable computational complexity.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9543692469596863}]}, {"text": "In this paper, we introduce synchronous TAG into tree-to-string translation (), which is the simplest and fastest among syntax-based approaches (Section 2).", "labels": [], "entities": [{"text": "tree-to-string translation", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.7776529490947723}]}, {"text": "We propose anew rule extraction algorithm based on GHKM () that directly induces asynchronous TAG from an aligned and parsed bilingual corpus without converting Treebank-style trees to TAG derivations explicitly (Section 3).", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7744434177875519}, {"text": "GHKM", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8398652076721191}]}, {"text": "As tree-tostring translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem: reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining.", "labels": [], "entities": []}, {"text": "We describe how to convert TAG derivations to translation forest (Section 4).", "labels": [], "entities": []}, {"text": "We evaluated the new tree-to-string system on NIST Chinese-English tests and obtained consistent improvements (+0.7 BLEU) over the STSGbased baseline system without significant loss in efficiency (1.6 times slower) (Section 5).", "labels": [], "entities": [{"text": "NIST Chinese-English tests", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.9447694619496664}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9985131621360779}]}], "datasetContent": [{"text": "We evaluated our adjoining tree-to-string translation system on Chinese-English translation.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6478458046913147}]}, {"text": "The bilingual corpus consists of 1.5M sentences with 42.1M Chinese words and 48.3M English words.", "labels": [], "entities": []}, {"text": "The Chinese sentences in the bilingual corpus were parsed by an in-house parser.", "labels": [], "entities": []}, {"text": "To maintain a reasonable grammar size, we follow  to restrict that the height of a rule tree is no greater than 3 and the surface string's length is no greater than 7.", "labels": [], "entities": []}, {"text": "After running GIZA++ to obtain word alignment, our rule extraction algorithm extracted 23.0M initial rules without adjoining sites, 6.6M initial rules with adjoining sites, and 5.3M auxiliary rules.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7586138248443604}, {"text": "rule extraction", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7204892486333847}]}, {"text": "We used the SRILM toolkit) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus, which contains 238M English words.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.8121115267276764}, {"text": "Xinhua portion of the GIGAWORD corpus", "start_pos": 67, "end_pos": 104, "type": "DATASET", "confidence": 0.747102697690328}]}, {"text": "We used the 2002 NIST MT Chinese-English test set as the development set and the NIST test sets as the test sets.", "labels": [], "entities": [{"text": "NIST MT Chinese-English test set", "start_pos": 17, "end_pos": 49, "type": "DATASET", "confidence": 0.9097367882728576}, {"text": "NIST test sets", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.991684357325236}]}, {"text": "We evaluated translation quality using the BLEU metric, as calculated by mteval-v11b.pl with case-insensitive matching of n-grams.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9589911103248596}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9947144389152527}]}, {"text": "shows top-10 phrase categories of foot nodes and their average occurrences in training corpus.", "labels": [], "entities": []}, {"text": "We find that VP (verb phrase) is most likely to be the label of afoot node in an auxiliary rule.", "labels": [], "entities": []}, {"text": "On average, there are 12.4 nodes labeled with VP are identical to one of its ancestors per tree.", "labels": [], "entities": []}, {"text": "NP and IP are also found to be foot node labels frequently.", "labels": [], "entities": []}, {"text": "shows the average occurrences of foot node labels VP, NP, and IP over various distances.", "labels": [], "entities": []}, {"text": "A distance is the difference of levels between afoot node   and the root node.", "labels": [], "entities": []}, {"text": "For example, in, the distance between NP 0,1 and NP 0,3 is 2 and the distance between VP 6,8 and VP 3,8 is 1.", "labels": [], "entities": []}, {"text": "As most foot nodes are usually very close to the root nodes, we restrict that afoot node must be the direct descendant of the root node in our experiments.", "labels": [], "entities": []}, {"text": "shows the BLEU scores on the NIST Chinese-English test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987930059432983}, {"text": "NIST Chinese-English test sets", "start_pos": 29, "end_pos": 59, "type": "DATASET", "confidence": 0.9754172712564468}]}, {"text": "Our baseline system is the tree-to-string system using STSG ().", "labels": [], "entities": []}, {"text": "The STAG system outperforms the STSG system significantly on the MT04 and MT05 test sets at pl.01 level.", "labels": [], "entities": [{"text": "STAG", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.848912239074707}, {"text": "MT04", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9528114199638367}, {"text": "MT05 test sets", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.933627982934316}]}, {"text": "also gives the results of Moses ( and an in-house hierarchical phrase-based system).", "labels": [], "entities": []}, {"text": "Our STAG system achieves comparable performance with the hierarchical system.", "labels": [], "entities": [{"text": "STAG", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.6658942103385925}]}, {"text": "The absolute improvement of +0.7 BLEU over STSG is close to the finding of on string-to-tree translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9988141059875488}, {"text": "string-to-tree translation", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.7067567408084869}]}, {"text": "We feel that one major obstacle for achieving further improvement is that composed rules generated on the fly during decoding (e.g., r 1 + r 3 + r 5 in) usually have too many non-terminals, making cube pruning in the in-1285  tersection phase suffering from severe search errors (only a tiny fraction of the search space can be explored).", "labels": [], "entities": []}, {"text": "To produce the 1-best translations on the MT05 test set that contains 1,082 sentences, while the STSG system used 40,169 initial rules without adjoining sites, the STAG system used 28,046 initial rules without adjoining sites, 1,057 initial rules with adjoining sites, and 1,527 auxiliary rules.", "labels": [], "entities": [{"text": "MT05 test set", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9369338353474935}]}, {"text": "shows the average decoding time on the MT05 test set.", "labels": [], "entities": [{"text": "MT05 test set", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9363483985265096}]}, {"text": "While rule matching for STSG needs 0.086 second per sentence, the matching time for STAG only increases to 0.109 second.", "labels": [], "entities": [{"text": "rule matching", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.7457173466682434}, {"text": "STAG", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.41221049427986145}]}, {"text": "For STAG, the conversion of derivation forests to translation forests takes 0.562 second when we restrict that at most 200 rules can be generated on the fly for each node.", "labels": [], "entities": [{"text": "STAG", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9104150533676147}]}, {"text": "As we use cube pruning, although the translation forest of STAG is bigger than that of STSG, the intersection time barely increases.", "labels": [], "entities": [{"text": "STAG", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.5059215426445007}]}, {"text": "In total, the STAG system runs in 1.763 seconds per sentence, only 1.6 times slower than the baseline system.", "labels": [], "entities": [{"text": "STAG", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.7168140411376953}]}], "tableCaptions": [{"text": " Table 3: BLEU scores on NIST Chinese-English test sets.  Scores marked in bold are significantly better that those  of STSG at pl.01 level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990732669830322}, {"text": "NIST Chinese-English test sets", "start_pos": 25, "end_pos": 55, "type": "DATASET", "confidence": 0.9596701264381409}]}, {"text": " Table 4: Comparison of average decoding time.", "labels": [], "entities": []}]}