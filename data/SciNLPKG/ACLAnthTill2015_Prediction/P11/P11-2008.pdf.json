{"title": [{"text": "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7810623347759247}]}], "abstractContent": [{"text": "We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7445805966854095}]}, {"text": "We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9979687333106995}]}, {"text": "The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.", "labels": [], "entities": [{"text": "richer text analysis", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.6559978822867075}]}], "introductionContent": [{"text": "The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form.", "labels": [], "entities": []}, {"text": "The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data ().", "labels": [], "entities": []}, {"text": "However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (.", "labels": [], "entities": []}, {"text": "One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6649955987930298}, {"text": "syntactic analysis", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7392390966415405}]}, {"text": "Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB;.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.8968448340892792}, {"text": "Wall Street Journal corpus of the Penn Treebank (PTB;", "start_pos": 80, "end_pos": 133, "type": "DATASET", "confidence": 0.9377078359777277}]}, {"text": "Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140-character limit of each message (\"tweet\").", "labels": [], "entities": []}, {"text": "shows three tweets which illustrate these challenges.", "labels": [], "entities": []}, {"text": "In this paper, we produce an English POS tagger that is designed especially for Twitter data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.5642076879739761}]}, {"text": "Our contributions are as follows: \u2022 we developed a POS tagset for Twitter, \u2022 we manually tagged 1,827 tweets, \u2022 we developed features for Twitter POS tagging and conducted experiments to evaluate them, and \u2022 we provide our annotated corpus and trained POS tagger to the research community.", "labels": [], "entities": []}, {"text": "Beyond these specific contributions, we see this work as a case study in how to rapidly engineer a core NLP system fora new and idiosyncratic dataset.", "labels": [], "entities": []}, {"text": "This project was accomplished in 200 person-hours spread across 17 people and two months.", "labels": [], "entities": []}, {"text": "This was made possible by two things: (1) an annotation scheme that fits the unique characteristics of our data and provides an appropriate level of linguistic detail, and (2) a feature set that captures Twitter-specific properties and exploits existing resources such as tag dictionaries and phonetic normalization.", "labels": [], "entities": [{"text": "phonetic normalization", "start_pos": 293, "end_pos": 315, "type": "TASK", "confidence": 0.7185109555721283}]}, {"text": "The success of this approach demonstrates that with careful design, supervised machine learning can be applied to rapidly produce effective language technology in new domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation was designed to test the efficacy of this feature set for part-of-speech tagging given limited training data.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8187948167324066}]}, {"text": "We randomly divided the set of 1,827 annotated tweets into a training set of 1,000 (14,542 tokens), a development set of 327 (4,770 tokens), and a test set of 500 (7,124 tokens).", "labels": [], "entities": []}, {"text": "We compare our system against the Stanford tagger.", "labels": [], "entities": []}, {"text": "Due to the different tagsets, we could not apply the pretrained Stanford tagger to our data.", "labels": [], "entities": [{"text": "Stanford tagger", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9129485785961151}]}, {"text": "Instead, we retrained it on our labeled data, using a standard set of features: words within a 5-word window, word shapes in a 3-word window, and up to length-3 prefixes, length-3 suffixes, and prefix/suffix pairs.", "labels": [], "entities": []}, {"text": "10 The Stanford system was regularized using a Gaussian prior of \u03c3 2 = 0.5 and our system with a Gaussian prior of \u03c3 2 = 5.0, tuned on development data.", "labels": [], "entities": [{"text": "Stanford system", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9508457481861115}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Our tagger with the full feature set achieves a relative error reduction of 25% compared to the Stanford tagger.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.9859258830547333}]}, {"text": "We also show feature ablation experiments, each of which corresponds to removing one category of features from the full set.", "labels": [], "entities": []}, {"text": "In    are incorrect in a specific ablation, but are corrected in the full system (i.e. when the feature is added).", "labels": [], "entities": []}, {"text": "The \u2212TAGDICT ablation gets elects, Governor, and next wrong in tweet (a).", "labels": [], "entities": [{"text": "TAGDICT ablation", "start_pos": 5, "end_pos": 21, "type": "METRIC", "confidence": 0.9347778558731079}]}, {"text": "These words appear in the PTB tag dictionary with the correct tags, and thus are fixed by that feature.", "labels": [], "entities": [{"text": "PTB tag dictionary", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.907370944817861}]}, {"text": "In (b), withhh is initially misclassified an interjection (likely caused by interjections with the same suffix, like ohhh), but is corrected by METAPH, because it is normalized to the same equivalence class as with.", "labels": [], "entities": [{"text": "METAPH", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.741320013999939}]}, {"text": "Finally, s/o in tweet (c) means \"shoutout\", which appears only once in the training data; adding DISTSIM causes it to be correctly identified as a verb.", "labels": [], "entities": [{"text": "DISTSIM", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9507325291633606}]}, {"text": "Substantial challenges remain; for example, despite the NAMES feature, the system struggles to identify proper nouns with nonstandard capitalization.", "labels": [], "entities": []}, {"text": "This can be observed from, which shows the recall of each tag type: the recall of proper nouns (\u02c6) is only 71%.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9981386661529541}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9966468214988708}]}, {"text": "The system also struggles with the miscellaneous category (G), which covers many rare tokens, including obscure symbols and artifacts of tokenization errors.", "labels": [], "entities": [{"text": "miscellaneous category (G)", "start_pos": 35, "end_pos": 61, "type": "METRIC", "confidence": 0.6841479480266571}]}, {"text": "Nonetheless, we are encouraged by the success of our system on the whole, leveraging out-of-domain lexical resources (TAGDICT), in-domain lexical resources (DISTSIM), and sublexical analysis (METAPH).", "labels": [], "entities": []}, {"text": "Finally, we note that, even though 1,000 training examples may seem small, the test set accuracy when training on only 500 tweets drops to 87.66%, a decrease of only 1.7% absolute.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9764299392700195}]}], "tableCaptions": [{"text": " Table 1: The set of tags used to annotate tweets. The", "labels": [], "entities": []}, {"text": " Table 2: Tagging accuracies on development and test", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9805882573127747}]}]}