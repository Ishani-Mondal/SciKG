{"title": [{"text": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations", "labels": [], "entities": [{"text": "Knowledge-Based Weak Supervision", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6019230782985687}, {"text": "Information Extraction of Overlapping Relations", "start_pos": 37, "end_pos": 84, "type": "TASK", "confidence": 0.8515692830085755}]}], "abstractContent": [{"text": "Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text.", "labels": [], "entities": [{"text": "Information extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8874352693557739}]}, {"text": "Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.69503153860569}]}, {"text": "Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint-for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple).", "labels": [], "entities": []}, {"text": "This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts.", "labels": [], "entities": [{"text": "sentence-level extraction", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.7133317589759827}]}, {"text": "We apply our model to learn extractors for NY Times text using weak supervision from Free-base.", "labels": [], "entities": [{"text": "NY Times text", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8799025217692057}, {"text": "Free-base", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.9803109169006348}]}, {"text": "Experiments show that the approach runs quickly and yields surprising gains inaccuracy, at both the aggregate and sentence level.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information-extraction (IE), the process of generating relational data from natural-language text, continues to gain attention.", "labels": [], "entities": [{"text": "Information-extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6902529373764992}]}, {"text": "Many researchers dream of creating a large repository of high-quality extracted tuples, arguing that such a knowledge base could benefit many important tasks such as question answering and summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.90562903881073}, {"text": "summarization", "start_pos": 189, "end_pos": 202, "type": "TASK", "confidence": 0.970784604549408}]}, {"text": "Most approaches to IE use supervised learning of relation-specific examples, which can achieve high precision and recall.", "labels": [], "entities": [{"text": "IE", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9947243332862854}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9981670379638672}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9972590208053589}]}, {"text": "Unfortunately, however, fully supervised methods are limited by the availability of training data and are unlikely to scale to the thousands of relations found on the Web.", "labels": [], "entities": []}, {"text": "A more promising approach, often called \"weak\" or \"distant\" supervision, creates its own training data by heuristically matching the contents of a database to corresponding text).", "labels": [], "entities": []}, {"text": "For example, suppose that r(e 1 , e 2 ) = Founded(Jobs, Apple) is aground tuple in the database and s =\"Steve Jobs founded Apple, Inc.\" is a sentence containing synonyms for both e 1 = Jobs and e 2 = Apple, then s maybe a natural language expression of the fact that r(e 1 , e 2 ) holds and could be a useful training example.", "labels": [], "entities": []}, {"text": "While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (),  observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles).", "labels": [], "entities": [{"text": "Freebase records to NY Times articles", "start_pos": 301, "end_pos": 338, "type": "DATASET", "confidence": 0.741827110449473}]}, {"text": "To fix this problem they cast weak supervision as a form of multi-instance learning, assuming only that at least one of the sentences containing e 1 and e 2 are expressing r(e 1 , e 2 ), and their method yields a substantial improvement in extraction performance.", "labels": [], "entities": []}, {"text": "However,s model (like that of previous systems) assumes that relations do not overlap -there cannot exist two facts r(e 1 , e 2 ) and q(e 1 , e 2 ) that are both true for any pair of entities, e 1 and e 2 . Unfortunately, this assumption is often violated; 541 for example both Founded(Jobs, Apple) and CEO-of(Jobs, Apple) are clearly true.", "labels": [], "entities": []}, {"text": "Indeed, 18.3% of the weak supervision facts in Freebase that match sentences in the NY Times 2007 corpus have overlapping relations.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9629837274551392}, {"text": "NY Times 2007 corpus", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.9290092885494232}]}, {"text": "This paper presents MULTIR, a novel model of weak supervision that makes the following contributions: \u2022 MULTIR introduces a probabilistic, graphical model of multi-instance learning which handles overlapping relations.", "labels": [], "entities": []}, {"text": "\u2022 MULTIR also produces accurate sentence-level predictions, decoding individual sentences as well as making corpus-level extractions.", "labels": [], "entities": [{"text": "corpus-level extractions", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7089539468288422}]}, {"text": "\u2022 MULTIR is computationally tractable.", "labels": [], "entities": [{"text": "MULTIR", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.4903867542743683}]}, {"text": "Inference reduces to weighted set cover, for which it uses a greedy approximation with worst case running time O(|R| \u00b7 |S|) where R is the set of possible relations and S is largest set of sentences for any entity pair.", "labels": [], "entities": [{"text": "Inference", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9777373671531677}]}, {"text": "In practice, MULTIR runs very quickly.", "labels": [], "entities": [{"text": "MULTIR", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.4284118413925171}]}, {"text": "\u2022 We present experiments showing that MULTIR outperforms a reimplementation of 's approach on both aggregate (corpus as a whole) and sentential extractions.", "labels": [], "entities": [{"text": "sentential extractions", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.6835261434316635}]}, {"text": "Additional experiments characterize aspects of MULTIR's performance.", "labels": [], "entities": [{"text": "MULTIR", "start_pos": 47, "end_pos": 53, "type": "TASK", "confidence": 0.6995379328727722}]}], "datasetContent": [{"text": "We follow the approach of  for generating weak supervision data, computing features, and evaluating aggregate extraction.", "labels": [], "entities": [{"text": "aggregate extraction", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7725032866001129}]}, {"text": "We also introduce new metrics for measuring sentential extraction performance, both relation-independent and relation-specific.", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8432064056396484}]}, {"text": "Evaluation is challenging, since only a small percentage (approximately 3%) of sentences match facts in Freebase, and the number of matches is highly unbalanced across relations, as we will see in more detail later.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.9658817052841187}]}, {"text": "We use the following metrics.", "labels": [], "entities": []}, {"text": "Aggregate Extraction Let \u2206 e be the set of extracted relations for any of the systems; we compute aggregate precision and recall by comparing \u2206 e with \u2206.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9620083570480347}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9995008707046509}]}, {"text": "This metric is easily computed but underestimates extraction accuracy because Freebase is incomplete and some true relations in \u2206 e will be marked wrong.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.968052327632904}, {"text": "Freebase", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9795864224433899}]}, {"text": "Sentential Extraction Let Se be the sentences where some system extracted a relation and SF be the sentences that match the arguments of a fact in \u2206.", "labels": [], "entities": [{"text": "Sentential Extraction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9409832656383514}]}, {"text": "We manually compute sentential extraction accuracy by sampling a set of 1000 sentences from Se \u222a SF and manually labeling the correct extraction decision, either a relation r \u2208 R or none.", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.8077081143856049}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.91281658411026}]}, {"text": "We then report precision and recall for each system on this set of sampled sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996869564056396}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9995429515838623}]}, {"text": "These results provide a good approximation to the true precision but can overestimate the actual recall, since we did not manually check the much larger set of sentences where no approach predicted extractions.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9840468764305115}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9983972907066345}]}, {"text": "To evaluate our algorithm, we first compare it to an existing approach for using multi-instance learning with weak supervision ( ), using the same data and features.", "labels": [], "entities": []}, {"text": "We report both aggregate extraction and sentential extraction results.", "labels": [], "entities": [{"text": "aggregate extraction", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7317624390125275}, {"text": "sentential extraction", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.711704209446907}]}, {"text": "We then investigate relation-specific performance of our system.", "labels": [], "entities": []}, {"text": "Finally, we report running time comparisons.", "labels": [], "entities": []}, {"text": "shows approximate precision / recall curves for three systems computed with aggregate metrics (Section 6.3) that test how closely the extractions match the facts in Freebase.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9947776794433594}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9364373087882996}, {"text": "Freebase", "start_pos": 165, "end_pos": 173, "type": "DATASET", "confidence": 0.9678691625595093}]}, {"text": "The systems include the original results reported by  as well as our new model (MULTIR).", "labels": [], "entities": [{"text": "MULTIR", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.7409034371376038}]}, {"text": "We also compare with SOLOR, a reimplementation of their algorithm, which we builtin Factorie (), and will use later to evaluate sentential extraction.", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.824724555015564}]}, {"text": "MULTIR achieves competitive or higher precision overall ranges of recall, with the exception of the very low recall range of approximately 0-1%.", "labels": [], "entities": [{"text": "MULTIR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6275610327720642}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.998685896396637}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9969674944877625}, {"text": "recall range", "start_pos": 109, "end_pos": 121, "type": "METRIC", "confidence": 0.9871217608451843}]}, {"text": "It also significantly extends the highest recall achieved, from 20% to 25%, with little loss in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9946696162223816}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9989157915115356}]}, {"text": "To investigate the low precision in the 0-1% recall range, we manually checked the ten highest con-546 fidence extractions produced by MULTIR that were marked wrong.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9985894560813904}, {"text": "recall range", "start_pos": 45, "end_pos": 57, "type": "METRIC", "confidence": 0.9700959622859955}, {"text": "MULTIR", "start_pos": 135, "end_pos": 141, "type": "DATASET", "confidence": 0.8714687824249268}]}, {"text": "We found that all ten were true facts that were simply missing from Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9839676022529602}]}, {"text": "A manual evaluation, as we perform next for sentential extraction, would remove this dip.", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8753177225589752}]}], "tableCaptions": [{"text": " Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy  (% true) of matches between sentences and facts in Freebase.", "labels": [], "entities": [{"text": "Estimated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9863331913948059}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.919564962387085}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9982307553291321}, {"text": "accuracy  (% true)", "start_pos": 110, "end_pos": 128, "type": "METRIC", "confidence": 0.8642153590917587}, {"text": "Freebase", "start_pos": 171, "end_pos": 179, "type": "DATASET", "confidence": 0.9335213303565979}]}]}