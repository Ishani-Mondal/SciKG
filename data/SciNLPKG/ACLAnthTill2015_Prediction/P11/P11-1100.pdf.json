{"title": [{"text": "Automatically Evaluating Text Coherence Using Discourse Relations", "labels": [], "entities": [{"text": "Automatically Evaluating Text Coherence", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8188874572515488}]}], "abstractContent": [{"text": "We present a novel model to represent and assess the discourse coherence of text.", "labels": [], "entities": []}, {"text": "Our model assumes that coherent text implicitly favors certain types of discourse relation transitions.", "labels": [], "entities": []}, {"text": "We implement this model and apply it towards the text ordering ranking task, which aims to discern an original text from a per-muted ordering of its sentences.", "labels": [], "entities": [{"text": "text ordering ranking task", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8267641663551331}]}, {"text": "The experimental results demonstrate that our model is able to significantly outperform the state-of-the-art coherence model by Barzilay and Lap-ata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds.", "labels": [], "entities": [{"text": "error rate", "start_pos": 170, "end_pos": 180, "type": "METRIC", "confidence": 0.9750386476516724}]}, {"text": "We further show that our model is synergistic with the previous approach, demonstrating an error reduction of 73% when the features from both models are combined for the task.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 91, "end_pos": 106, "type": "METRIC", "confidence": 0.9813158512115479}]}], "introductionContent": [{"text": "The coherence of a text is usually reflected by its discourse structure and relations.", "labels": [], "entities": []}, {"text": "In Rhetorical Structure Theory (RST), observed that certain RST relations tend to favor one of two possible canonical orderings.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8287001003821691}]}, {"text": "Some relations (e.g., Concessive and Conditional) favor arranging their satellite span before the nucleus span.", "labels": [], "entities": []}, {"text": "In contrast, other relations (e.g., Elaboration and Evidence) usually order their nucleus before the satellite.", "labels": [], "entities": []}, {"text": "If a text that uses non-canonical relation orderings is rewritten to use canonical orderings, it often improves text quality and coherence.", "labels": [], "entities": []}, {"text": "This notion of preferential ordering of discourse relations is observed in natural language in general, and generalizes to other discourse frameworks aside from RST.", "labels": [], "entities": []}, {"text": "The following example shows a Contrast relation between the two sentences.", "labels": [], "entities": [{"text": "Contrast", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.8811483383178711}]}, {"text": "(1) [ Everyone agrees that most of the nation's old bridges need to be repaired or replaced. ] [ But there's disagreement over how to do it.", "labels": [], "entities": []}, {"text": "] S2 Here the second sentence provides contrasting information to the first.", "labels": [], "entities": []}, {"text": "If this order is violated without rewording (i.e., if the two sentences are swapped), it produces an incoherent text.", "labels": [], "entities": []}, {"text": "In addition to the intra-relation ordering, such preferences also extend to inter-relation ordering: The second sentence above provides a contrast to the previous sentence and an explanation for the next one.", "labels": [], "entities": []}, {"text": "This pattern of Contrast-followed-by-Cause is rather common in text (.", "labels": [], "entities": []}, {"text": "Ordering the three sentences differently results in incoherent, cryptic text.", "labels": [], "entities": []}, {"text": "Thus coherent text exhibits measurable preferences for specific intra-and inter-discourse relation ordering.", "labels": [], "entities": []}, {"text": "Our key idea is to use the converse of this phenomenon to assess the coherence of a text.", "labels": [], "entities": []}, {"text": "In this paper, we detail our model to capture the coherence of a text based on the statistical distribution of the discourse structure and relations.", "labels": [], "entities": []}, {"text": "Our method specifically focuses on the discourse relation transitions between adjacent sentences, modeling them in a discourse role matrix.", "labels": [], "entities": []}, {"text": "Our study makes additional contributions.", "labels": [], "entities": []}, {"text": "We implement and validate our model on three data sets, which show robust improvements over the current state-of-the-art for coherence assessment.", "labels": [], "entities": []}, {"text": "We also provide the first assessment of the upper-bound of human performance on the standard task of distinguishing coherent from incoherent orderings.", "labels": [], "entities": []}, {"text": "To the best our knowledge, this is also the first study in which we show output from an automatic discourse parser helps in coherence modeling.", "labels": [], "entities": [{"text": "coherence modeling", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7499408423900604}]}], "datasetContent": [{"text": "We evaluate our coherence model on the task of text ordering ranking, a standard coherence evaluation task used in both ( ) and (.", "labels": [], "entities": [{"text": "text ordering ranking", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.8111295104026794}]}, {"text": "In this task, the system is asked to decide which of two texts is more coherent.", "labels": [], "entities": []}, {"text": "The pair of texts consists of a source text and one of its permutations (i.e., the text's sentence order is randomized).", "labels": [], "entities": []}, {"text": "Assuming that the original text is always more discourse-coherent than its permutation, an ideal system will prefer the original to the permuted text.", "labels": [], "entities": []}, {"text": "A system's accuracy is thus the number of times the system correctly chooses the original divided by the total number of test pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9991533756256104}]}, {"text": "In order to acquire a large data set for training and testing, we follow the approach in () to create a collection of synthetic data from Wall Street Journal (WSJ) articles in the Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) articles in the Penn Treebank", "start_pos": 138, "end_pos": 193, "type": "DATASET", "confidence": 0.9000271396203474}]}, {"text": "All of the WSJ articles are randomly split into a training and a testing set; 40 articles are held out from the training set for development.", "labels": [], "entities": []}, {"text": "For each article, its sentences are permuted up to 20 times to create a set of permutations . Each permutation is paired with its source text to form a pair.", "labels": [], "entities": []}, {"text": "We also evaluate on two other data collections (cf.  American News Corpus, and narratives from the National Transportation Safety Board.", "labels": [], "entities": [{"text": "American News Corpus", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.9714001615842184}, {"text": "National Transportation Safety Board", "start_pos": 99, "end_pos": 135, "type": "DATASET", "confidence": 0.8956746757030487}]}, {"text": "These collections are much smaller than the WSJ data, as each training/testing set contains only up to 100 source articles.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9524551033973694}]}, {"text": "Similar to the WSJ data, we construct pairs by permuting each source article up to 20 times.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.9550721645355225}]}, {"text": "Our model has two parameters: (1) the term frequency (TF) that is used as a threshold to identify salient terms, and (2) the lengths of the subsequences that are extracted as features.", "labels": [], "entities": [{"text": "term frequency (TF)", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.942483103275299}]}, {"text": "These parameters are tuned on the development set, and the best ones that produce the optimal accuracy are TF >= 2 and lengths of the sub-sequences <= 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9993416666984558}, {"text": "TF", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9983695149421692}]}, {"text": "We must also be careful in using the automatic discourse parser.", "labels": [], "entities": []}, {"text": "We note that the discourse parser of comes trained on the PDTB, which provides annotations on top of the whole WSJ data.", "labels": [], "entities": [{"text": "discourse parser", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.6902530193328857}, {"text": "PDTB", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9354575276374817}, {"text": "WSJ data", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.9536857306957245}]}, {"text": "As we also use the WSJ data for evaluation, we must avoid parsing an article that has already been used in training the parser to prevent training on the test data.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9179318249225616}]}, {"text": "We re-train the parser with 24 WSJ sections and use the trained parser to parse the sentences in our WSJ collection from the remaining section.", "labels": [], "entities": [{"text": "WSJ collection", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9037760198116302}]}, {"text": "We repeat this re-training/parsing process for all 25 sections.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.8730616569519043}]}, {"text": "Because the Earthquakes and Accidents data do not overlap with the WSJ training data, we use the parser as distributed to parse these two data sets.", "labels": [], "entities": [{"text": "Earthquakes and Accidents data", "start_pos": 12, "end_pos": 42, "type": "DATASET", "confidence": 0.7699588686227798}, {"text": "WSJ training data", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9250957369804382}]}, {"text": "Since the discourse parser utilizes paragraph boundaries but a permuted text does not have such boundaries, we ignore paragraph boundaries and treat the source text as if it has only one paragraph.", "labels": [], "entities": []}, {"text": "This is to make sure that we do not give the system extra information because of this difference between the source and permuted text.", "labels": [], "entities": []}, {"text": "While the text ordering ranking task has been used in previous studies, two key questions about this task have remained unaddressed in the previous work: to what extent is the assumption that the source text is more coherent than its permutation correct? and (2) how well do humans perform on this task?", "labels": [], "entities": [{"text": "text ordering ranking", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8613571524620056}]}, {"text": "The answer to the first is needed to validate the correctness of this synthetic task, while the second aims to obtain the upper bound for evaluation.", "labels": [], "entities": []}, {"text": "We conduct a human evaluation to answer these questions.", "labels": [], "entities": []}, {"text": "We randomly select 50 source text/permutation pairs from each of the WSJ, Earthquakes, and Accidents training sets.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.5736537575721741}, {"text": "Earthquakes, and Accidents training sets", "start_pos": 74, "end_pos": 114, "type": "DATASET", "confidence": 0.7148386587699255}]}, {"text": "We observe that some of the source texts have formulaic structures in their initial sentences that giveaway the correct ordering.", "labels": [], "entities": []}, {"text": "Sources from the Earthquakes data always begin with a headline sentence and a location-newswire sentence, and many sources from the Accidents data start with two sentences of \"This is preliminary . .", "labels": [], "entities": [{"text": "Earthquakes data", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.970582902431488}, {"text": "Accidents data", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.8524415194988251}]}, {"text": "We remove these sentences from the source and permuted texts, to avoid the subjects judging based on these clues instead of textual coherence.", "labels": [], "entities": []}, {"text": "For each set of 50 pairs, we assigned two human subjects (who are not authors of this paper) to perform the ranking.", "labels": [], "entities": []}, {"text": "The subjects are told to identify the source text from the pair.", "labels": [], "entities": []}, {"text": "When both subjects rank a source text higher than its permutation, we interpret it as the subjects agreeing that the source text is more coherent than the permutation.", "labels": [], "entities": []}, {"text": "shows the inter-subject agreements.: Inter-subject agreements on the three data sets.", "labels": [], "entities": []}, {"text": "While our study is limited and only indicative, we conclude from these results that the task is tractable.", "labels": [], "entities": []}, {"text": "Also, since our subjects' judgments correlate highly with the gold standard, the assumption that the original text is always more coherent than the permuted text is supported.", "labels": [], "entities": []}, {"text": "Importantly though, human performance is not perfect, suggesting fair upper bound limits on system performance.", "labels": [], "entities": []}, {"text": "We note that the Accidents data set is relatively easier to rank, as it has a higher upper bound than the other two.", "labels": [], "entities": [{"text": "Accidents data set", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.8593781391779581}]}], "tableCaptions": [{"text": " Table 3: Inter-subject agreements on the three data sets.", "labels": [], "entities": []}, {"text": " Table 4: Test set ranking accuracy. The first row shows  the baseline performance, the next four show our model  with different settings, and the last row is a combined  model. Double (**) and single (*) asterisks indicate that  the respective model significantly outperforms the base- line at p < 0.01 and p < 0.05, respectively. We follow", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9951637983322144}]}]}