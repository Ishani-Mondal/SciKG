{"title": [{"text": "Joint Training of Dependency Parsing Filters through Latent Support Vector Machines", "labels": [], "entities": []}], "abstractContent": [{"text": "Graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins.", "labels": [], "entities": [{"text": "Graph-based dependency parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7102410793304443}]}, {"text": "State-of-the-art methods for arc filtering use separate classifiers to make point-wise decisions about the tree; they label tokens with roles such as root, leaf, or attaches-to-the-left, and then filter arcs accordingly.", "labels": [], "entities": [{"text": "arc filtering", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.8681396842002869}]}, {"text": "Because these classifiers overlap substantially in their filtering consequences, we propose to train them jointly, so that each classifier can focus on the gaps of the others.", "labels": [], "entities": []}, {"text": "We integrate the various pointwise decisions as latent variables in a single arc-level SVM classifier.", "labels": [], "entities": []}, {"text": "This novel framework allows us to combine nine pointwise filters, and adjust their sensitivity using a shared threshold based on arc length.", "labels": [], "entities": []}, {"text": "Our system filters 32% more arcs than the independently-trained classifiers, without reducing filtering speed.", "labels": [], "entities": []}, {"text": "This leads to faster parsing with no reduction inaccuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9728823304176331}]}], "introductionContent": [{"text": "A dependency tree represents syntactic relationships between words using directed arcs.", "labels": [], "entities": []}, {"text": "Each token in the sentence is anode in the tree, and each arc connects ahead to its modifier.", "labels": [], "entities": []}, {"text": "There are two dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7838543355464935}]}, {"text": "In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time.", "labels": [], "entities": []}, {"text": "Much of the time for parsing is spent scoring each potential arc in the complete dependency graph, one for each ordered word-pair in the sentence.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9825851917266846}]}, {"text": "Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy).", "labels": [], "entities": [{"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.953004777431488}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8516088128089905}]}, {"text": "The vast majority of these arcs are bad; in an n-word sentence, only n of then 2 potential arcs are correct.", "labels": [], "entities": []}, {"text": "If many arcs can be filtered before parsing begins, then the entire process can be sped up substantially.", "labels": [], "entities": []}, {"text": "Previously, we proposed a cascade of filters to prune potential arcs.", "labels": [], "entities": []}, {"text": "One stage of this cascade operates one token at a time, labeling each token t according to various roles in the tree: \u2022 Not-a-head (NaH ): t is not the head of any arc \u2022 Head-to-left (HtL{1/5/*}): t's head is to its left within 1, 5 or any number of words \u2022 Head-to-right (HtR{1/5/*}): as head-to-left \u2022 Root (Root): t is the root node, which eliminates arcs according to projectivity Similar to, each role has a corresponding binary classifier.", "labels": [], "entities": []}, {"text": "These tokenrole classifiers were shown to be more effective than vine parsing), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short).", "labels": [], "entities": [{"text": "vine parsing", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.7684349715709686}]}, {"text": "In this work, we propose a novel filtering framework that integrates all the information used in token-role classification and vine parsing, but offers a number of advantages.", "labels": [], "entities": [{"text": "token-role classification", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.8498367369174957}, {"text": "vine parsing", "start_pos": 127, "end_pos": 139, "type": "TASK", "confidence": 0.6712094843387604}]}, {"text": "In our previous work, classifier decisions would often overlap: different token-role classifiers would agree to filter the same arc.", "labels": [], "entities": []}, {"text": "Based on this observation, we propose a joint training framework where only the most confident: The dotted arc can be filtered by labeling any of the boxed roles as True; i.e., predicting that the head the3 is not the head of any arc, or that the modifier his6 attaches elsewhere.", "labels": [], "entities": []}, {"text": "Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses.", "labels": [], "entities": []}, {"text": "classifier is given credit for eliminating an arc.", "labels": [], "entities": []}, {"text": "The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation.", "labels": [], "entities": []}, {"text": "Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures) or sentence labels ().", "labels": [], "entities": []}, {"text": "In our framework, each classifier learns to focus on the cases where the other classifiers are less confident.", "labels": [], "entities": []}, {"text": "Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9584721922874451}]}, {"text": "We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9920197129249573}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9735756516456604}]}, {"text": "Ultimately, the biggest gains in filter quality are achieved when we jointly train the token-role classifiers together with a dynamic threshold that is based on arc length and shared across all classifiers.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extract dependency structures from the Penn Treebank using the head rules of.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9966207146644592}]}, {"text": "We divide the Treebank into train (sections 2-21), development (22) and test.", "labels": [], "entities": [{"text": "Treebank", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9501386880874634}]}, {"text": "We part-of-speech tag our data using a perceptron tagger similar to the one described by.", "labels": [], "entities": []}, {"text": "The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds.", "labels": [], "entities": []}, {"text": "Development and test sets are tagged using the entire training set.", "labels": [], "entities": []}, {"text": "We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (.", "labels": [], "entities": []}, {"text": "LSVM training was stopped after 4 iterations, as determined during development.", "labels": [], "entities": [{"text": "LSVM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5209521651268005}]}, {"text": "For the token-role classifiers, we re-implement the feature set, initializing \u00af w with high-precision subclassifiers trained independently for each token-role.", "labels": [], "entities": []}, {"text": "Vine and None subclassifiers are initialized with a zero vector.", "labels": [], "entities": [{"text": "Vine", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9393616914749146}]}, {"text": "At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of.", "labels": [], "entities": []}, {"text": "Parsing experiments are carried out using the MST parser), which we have modified to filter arcs before carrying out feature extraction.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9242525696754456}, {"text": "feature extraction", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7064756006002426}]}, {"text": "It is trained using 5-best MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9531866908073425}]}, {"text": "Following, we measure intrinsic filter quality with reduction, the proportion of total arcs removed, and coverage, the proportion of true arcs retained.", "labels": [], "entities": [{"text": "coverage", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9987776875495911}]}, {"text": "For parsing results, we present dependency accuracy, the percentage of tokens that are assigned the correct head.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.983423113822937}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.5930485129356384}]}], "tableCaptions": [{"text": " Table 1: Ablation analysis of intrinsic filter quality.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9379441142082214}]}, {"text": " Table 2: Parsing with jointly-trained filters outperforms independently-trained filters (R+L), as well as a more complex  cascade (R+L+Q). *Accounts for total time spent parsing and applying filters, averaged over five runs.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9788448214530945}, {"text": "Accounts", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9984771609306335}]}]}