{"title": [{"text": "Jigs and Lures: Associating Web Queries with Structured Entities", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose methods for estimating the probability that an entity from an entity database is associated with a web search query.", "labels": [], "entities": []}, {"text": "Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs.", "labels": [], "entities": [{"text": "Association", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.943732738494873}]}, {"text": "Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model.", "labels": [], "entities": []}, {"text": "A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error.", "labels": [], "entities": []}, {"text": "The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query-product associations through web search session analysis.", "labels": [], "entities": []}, {"text": "Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall , that our top-performing model affects 9% of general web queries with 94% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9988215565681458}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9952693581581116}]}], "introductionContent": [{"text": "Commercial search engines use query associations in a variety of ways, including the recommendation of related queries in Bing, 'something different' in Google, and 'also try' and related concepts in Yahoo.", "labels": [], "entities": []}, {"text": "Mining techniques to extract such query associations generally fall into four categories: (a) clustering queries by their co-clicked url patterns); (b) leveraging co-occurrences of sequential queries in web search query sessions (; (c) pattern-based extraction over lexicosyntactic structures of individual queries; and (d) distributional similarity techniques over news or web corpora ().", "labels": [], "entities": []}, {"text": "These techniques operate at the surface level, associating one surface context (e.g., queries) to another.", "labels": [], "entities": []}, {"text": "In this paper, we focus instead on associating surface contexts with entities that refer to a particular entry in a knowledge base such as Freebase, IMDB, Amazon's product catalog, or The Library of Congress.", "labels": [], "entities": [{"text": "The Library of Congress", "start_pos": 184, "end_pos": 207, "type": "DATASET", "confidence": 0.9068463295698166}]}, {"text": "Whereas the former models might associate the string \"Ronaldinho\" with the strings \"AC Milan\" or \"Lionel Messi\", our goal is to associate \"Ronaldinho\" with, for example, the Wikipedia entity page \"wiki/AC Milan\" or the Freebase entity \"en/lionel mess\".", "labels": [], "entities": [{"text": "Freebase entity \"en/lionel mess", "start_pos": 219, "end_pos": 250, "type": "DATASET", "confidence": 0.8546472702707563}]}, {"text": "Or for the query string \"ice fishing\", we aim to recommend products in a commercial catalog, such as jigs or lures.", "labels": [], "entities": [{"text": "ice fishing\"", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7722472250461578}]}, {"text": "The benefits and potential applications are large.", "labels": [], "entities": []}, {"text": "By knowing the entity identifiers associated with a query (instead of strings), one can greatly improve both the presentation of search results as well as the click-through experience.", "labels": [], "entities": []}, {"text": "For example, consider when the associated entity is a product.", "labels": [], "entities": []}, {"text": "Not only can we present the product name to the web user, but we can also display the image, price, and reviews associated with the entity identifier.", "labels": [], "entities": []}, {"text": "Once the entity is clicked, instead of issuing a simple web search query, we can now directly show a product page for the exact product; or we can even perform actions directly on the entity, such as buying the entity on Amazon.com, retrieving the product's oper-83 ating manual, or even polling your social network for friends that own the product.", "labels": [], "entities": []}, {"text": "This is a big step towards a richer semantic search experience.", "labels": [], "entities": []}, {"text": "In this paper, we define the association between a query string q and an entity id e as the probability that e is relevant given the query q, P (e|q).", "labels": [], "entities": []}, {"text": "Following, we model relevance as the likelihood that a user would click one given q, events which can be observed in large query-click graphs.", "labels": [], "entities": []}, {"text": "Due to the extreme sparsity of query click graphs (), we propose several smoothing models that extend the click graph with query synonyms and then use the synonym click probabilities as a background model.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our smoothing models, via a large-scale empirical study over realworld data, which significantly reduce model errors.", "labels": [], "entities": []}, {"text": "We further apply our models to the task of queryproduct recommendation.", "labels": [], "entities": []}, {"text": "Queries in session logs are annotated using our association probabilities and recommendations are obtained by modeling sessionlevel query-product co-occurrences in the annotated sessions.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate that our models affect 9% of general web queries with 94% recommendation precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9038505554199219}]}], "datasetContent": [{"text": "We instantiate our models from Sections 3 and 4 using search query logs and a large catalog of products from a commercial search engine.", "labels": [], "entities": []}, {"text": "We form our QEC graphs by first collecting in Ce aggregate query-click-entity counts observed over two years in a commerce vertical search engine.", "labels": [], "entities": []}, {"text": "Similarly, Cu is formed by collecting aggregate query-click-url counts observed over six months in a web search engine, where each query must have frequency at least 10.", "labels": [], "entities": []}, {"text": "Three final QEC graphs are sampled by taking various snapshots of the above graph as follows: a) TRAIN consists of 50% of the graph; b) TEST consists of 25% of the graph; c) DEV consists of 25% of the graph.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9912530779838562}]}, {"text": "We instantiate our recommendation algorithm from Section 4.2 using session co-occurrence frequencies  from a one-month snapshot of user query sessions at a Web search engine, where session boundaries occur when 60 seconds elapse in between user queries.", "labels": [], "entities": []}, {"text": "We experiment with the recommendation parameters defined at the end of Section 4.2 as follows: k = 10, f ranging from 10 to 100, and p ranging from 3 to 10.", "labels": [], "entities": []}, {"text": "For each configuration, we report coverage as the total number of queries in the output (i.e., the queries for which there is some recommendation) divided by the total number of queries in the log.", "labels": [], "entities": [{"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9843466281890869}]}, {"text": "For our performance metrics, we sampled two sets of queries: (a) Query Set Sample: uniform random sample of 100 queries from the unique queries in the one-month log; and (b) Query Bag Sample: weighted random sample, by query frequency, of 100 queries from the query instances in the onemonth log.", "labels": [], "entities": []}, {"text": "For each sample query, we pooled together and randomly shuffled all recommendations by our algorithm using both\u02c6Pboth\u02c6 both\u02c6P mle and\u02c6Pand\u02c6 and\u02c6P intp on each parameter configuration.", "labels": [], "entities": []}, {"text": "We then manually annotated each {query, product} pair as relevant, mildly relevant or non-relevant.", "labels": [], "entities": []}, {"text": "In total, 1127 pairs were annotated.", "labels": [], "entities": []}, {"text": "Interannotator agreement between two judges on this task yielded a Cohen's Kappa of 0.56.", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.8305054505666097}]}, {"text": "We therefore collapsed the mildly relevant and non-relevant classes yielding two final classes: relevant and non-relevant.", "labels": [], "entities": []}, {"text": "Cohen's Kappa on this binary classification is 0.71.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9366276860237122}]}, {"text": "Let C M be the number of relevant (i.e., correct) suggestions recommended by a configuration M and let |M | be the number of recommendations returned by M . Then we define the (micro-) precision of M as: PM = C MC . We define relative recall () between two configurations M 1 and  markable is the {f = 10, p = 10} configuration where th\u00ea P intp model affected 9.4% of all query instances posed by the millions of users of a major search engine, with a precision of 94%.", "labels": [], "entities": [{"text": "precision", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.9132104516029358}, {"text": "recall", "start_pos": 235, "end_pos": 241, "type": "METRIC", "confidence": 0.9101536870002747}, {"text": "precision", "start_pos": 452, "end_pos": 461, "type": "METRIC", "confidence": 0.997088611125946}]}, {"text": "Although this model covers 0.8% of the unique queries, the fact that it covers many head queries such as walmart and iphone accounts for the large query instance coverage.", "labels": [], "entities": []}, {"text": "Also since there maybe many general web queries for which there is no appropriate product in the database, a coverage of 100% is not attainable (nor desirable); in fact the upper bound for the coverage is likely to be much lower.", "labels": [], "entities": [{"text": "coverage", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9796502590179443}, {"text": "coverage", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9725403785705566}]}], "tableCaptions": [{"text": " Table 2: Model analysis: M SE and M SE W with vari- ance and error reduction relative t\u00f4  P mle .  \u2020 indicates sta- tistical significance over\u02c6Pover\u02c6 over\u02c6P mle with 95% confidence.", "labels": [], "entities": [{"text": "error reduction relative t\u00f4  P mle", "start_pos": 62, "end_pos": 96, "type": "METRIC", "confidence": 0.8133990863958994}]}, {"text": " Table 3: Example query-product association scores for a  random sample of five products. Bold queries resulted  from the expansion algorithm in Section 3.2.", "labels": [], "entities": []}, {"text": " Table 4: Experimental results for product recommenda- tions. All configurations are for k = 10.", "labels": [], "entities": []}, {"text": " Table 5: Sample product recommendations.", "labels": [], "entities": [{"text": "Sample product", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8682293593883514}]}]}