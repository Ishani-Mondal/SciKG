{"title": [{"text": "The impact of language models and loss functions on repair disfluency detection", "labels": [], "entities": []}], "abstractContent": [{"text": "Unrehearsed spoken language often contains disfluencies.", "labels": [], "entities": []}, {"text": "In order to correctly interpret a spoken utterance, any such disfluen-cies must be identified and removed or otherwise dealt with.", "labels": [], "entities": []}, {"text": "Operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisy-channel model.", "labels": [], "entities": []}, {"text": "We show that language models trained on large amounts of non-speech data improve performance more than a language model trained on a more modest amount of speech data, and that optimising f-score rather than log loss improves disfluency detection performance.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 226, "end_pos": 246, "type": "TASK", "confidence": 0.7555335164070129}]}, {"text": "Our approach uses a log-linear reranker, operating on the top n analyses of a noisy channel model.", "labels": [], "entities": []}, {"text": "We use large language models, introduce new features into this reranker and examine different optimisation strategies.", "labels": [], "entities": []}, {"text": "We obtain a disfluency detection f-scores of 0.838 which improves upon the current state-of-the-art.", "labels": [], "entities": [{"text": "disfluency detection f-scores", "start_pos": 12, "end_pos": 41, "type": "METRIC", "confidence": 0.6655767659346262}]}], "introductionContent": [{"text": "Most spontaneous speech contains disfluencies such as partial words, filled pauses (e.g., \"uh\", \"um\", \"huh\"), explicit editing terms (e.g., \"I mean\"), parenthetical asides and repairs.", "labels": [], "entities": []}, {"text": "Of these, repairs pose particularly difficult problems for parsing and related Natural Language Processing (NLP) tasks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.9834781289100647}]}, {"text": "This paper presents a model of disfluency detection based on the noisy channel framework, which specifically targets the repair disfluencies.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.908769965171814}]}, {"text": "By combining language models and using an appropriate loss function in a log-linear reranker we are able to achieve f-scores which are higher than previously reported.", "labels": [], "entities": []}, {"text": "Often in natural language processing algorithms, more data is more important than better algorithms).", "labels": [], "entities": []}, {"text": "It is this insight that drives the first part of the work described in this paper.", "labels": [], "entities": []}, {"text": "This paper investigates how we can use language models trained on large corpora to increase repair detection accuracy performance.", "labels": [], "entities": [{"text": "repair detection", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.8419305682182312}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.6981878876686096}]}, {"text": "There are three main innovations in this paper.", "labels": [], "entities": []}, {"text": "First, we investigate the use of a variety of language models trained from text or speech corpora of various genres and sizes.", "labels": [], "entities": []}, {"text": "The largest available language models are based on written text: we investigate the effect of written text language models as opposed to language models based on speech transcripts.", "labels": [], "entities": []}, {"text": "Second, we develop anew set of reranker features explicitly designed to capture important properties of speech repairs.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.6954778283834457}]}, {"text": "Many of these features are lexically grounded and provide a large performance increase.", "labels": [], "entities": []}, {"text": "Third, we utilise a loss function, approximate expected f-score, that explicitly targets the asymmetric evaluation metrics used in the disfluency detection task.", "labels": [], "entities": [{"text": "disfluency detection task", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.8836267391840616}]}, {"text": "We explain how to optimise this loss function, and show that this leads to a marked improvement in disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.5462077260017395}]}, {"text": "This is consistent with and, who observed similar improvements when using approximate f-score loss for other problems.", "labels": [], "entities": []}, {"text": "Similarly we introduce a loss function based on the edit-f-score in our domain.", "labels": [], "entities": []}, {"text": "703 Together, these three improvements are enough to boost detection performance to a higher f-score than previously reported in literature.", "labels": [], "entities": [{"text": "f-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9949690699577332}]}, {"text": "investigate the use of 'ultra large feature spaces' as an aid for disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7880086302757263}]}, {"text": "Using over 19 million features, they report a final f-score in this task of 0.820.", "labels": [], "entities": [{"text": "f-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.8367099761962891}]}, {"text": "Operating on the same body of text (Switchboard), our work leads to an f-score of 0.838, this is a 9% relative improvement in residual f-score.", "labels": [], "entities": [{"text": "f-score", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.983354389667511}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "First in Section 2 we describe related work.", "labels": [], "entities": []}, {"text": "Then in Section 3 we present some background on disfluencies and their structure.", "labels": [], "entities": []}, {"text": "Section 4 describes appropriate evaluation techniques.", "labels": [], "entities": []}, {"text": "In Section 5 we describe the noisy channel model we are using.", "labels": [], "entities": []}, {"text": "The next three sections describe the new additions: Section 6 describe the corpora used for language models, Section 7 describes features used in the loglinear model employed by the reranker and Section 8 describes appropriate loss functions which are critical for our approach.", "labels": [], "entities": []}, {"text": "We evaluate the new model in Section 9.", "labels": [], "entities": []}, {"text": "Section 10 draws up a conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Disfluency detection systems like the one described here identify a subset of the word tokens in each transcribed utterance as \"edited\" or disfluent.", "labels": [], "entities": [{"text": "Disfluency detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8283444046974182}]}, {"text": "Perhaps the simplest way to evaluate such systems is to calculate the accuracy of labelling they produce, i.e., the fraction of words that are correctly labelled (i.e., either \"edited\" or \"not edited\").", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9991971850395203}]}, {"text": "However, as observe, because only 5.9% of words in the Switchboard corpus are \"edited\", the trivial baseline classifier which assigns all words the \"not edited\" label achieves a labelling accuracy of 94.1%.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.8801369369029999}, {"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.906023383140564}]}, {"text": "Because the labelling accuracy of the trivial baseline classifier is so high, it is standard to use a different evaluation metric that focuses more on the detection of \"edited\" words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.921718418598175}]}, {"text": "We follow and report the f-score of our disfluency detection system.", "labels": [], "entities": [{"text": "f-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9245770573616028}, {"text": "disfluency detection", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7319964468479156}]}, {"text": "The f-score f is: where g is the number of \"edited\" words in the gold test corpus, e is the number of \"edited\" words proposed by the system on that corpus, and c is the number of the \"edited\" words proposed by the system that are in fact correct.", "labels": [], "entities": [{"text": "gold test corpus", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.8709112604459127}]}, {"text": "A perfect classifier which correctly labels every word achieves an f-score of 1, while the trivial baseline classifiers which label every word as \"edited\" or \"not edited\" respectively achieve a very low f-score.", "labels": [], "entities": [{"text": "f-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9820240139961243}, {"text": "f-score", "start_pos": 203, "end_pos": 210, "type": "METRIC", "confidence": 0.9574689865112305}]}, {"text": "Informally, the f-score metric focuses more on the \"edited\" words than it does on the \"not edited\" words.", "labels": [], "entities": []}, {"text": "As we will see in section 8, this has implications for the choice of loss function used to train the classifier.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Edited word detection f-score on held-out data for a variety of language models and loss functions", "labels": [], "entities": [{"text": "word detection f-score", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6320013105869293}]}]}