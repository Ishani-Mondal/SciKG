{"title": [], "abstractContent": [{"text": "Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.9010871052742004}]}, {"text": "This supervision bottleneck is one of the major difficulties in scaling up semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7561168074607849}]}, {"text": "We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm.", "labels": [], "entities": []}, {"text": "The algorithm takes a self training approach driven by confidence estimation.", "labels": [], "entities": []}, {"text": "Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart , demonstrating the promise of unsuper-vised approaches for this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9994795918464661}]}], "introductionContent": [{"text": "Semantic parsing, the ability to transform Natural Language (NL) input into a formal Meaning Representation (MR), is one of the longest standing goals of natural language processing.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8553951978683472}, {"text": "formal Meaning Representation (MR)", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7203239103158315}, {"text": "natural language processing", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.6434286634127299}]}, {"text": "The importance of the problem stems from both theoretical and practical reasons, as the ability to convert NL into a formal MR has countless applications.", "labels": [], "entities": []}, {"text": "The term semantic parsing has been used ambiguously to refer to several semantic tasks (e.g., semantic role labeling).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7342098206281662}, {"text": "semantic role labeling", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.6213782330354055}]}, {"text": "We follow the most common definition of this task: finding a mapping between NL input and its interpretation expressed in a welldefined formal MR language.", "labels": [], "entities": []}, {"text": "Unlike shallow semantic analysis tasks, the output of a semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system.", "labels": [], "entities": []}, {"text": "Current approaches for this task take a data driven approach, in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser -a set of parameterized rules mapping lexical items and syntactic patterns to their MR.", "labels": [], "entities": []}, {"text": "Given a sentence, these rules are applied recursively to derive the most probable interpretation.", "labels": [], "entities": []}, {"text": "Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.8615150451660156}]}, {"text": "Unfortunately annotating sentences with their MR is a time consuming task which requires specialized domain knowledge and therefore minimizing the supervision effort is one of the key challenges in scaling semantic parsers.", "labels": [], "entities": [{"text": "scaling semantic parsers", "start_pos": 198, "end_pos": 222, "type": "TASK", "confidence": 0.7515477140744528}]}, {"text": "In this work we present the first unsupervised approach for this task.", "labels": [], "entities": []}, {"text": "Our model compensates for the lack of training data by employing a self training protocol based on identifying high confidence self labeled examples and using them to retrain the model.", "labels": [], "entities": []}, {"text": "We base our approach on a simple observation: semantic parsing is a difficult structured prediction task, which requires learning a complex model, however identifying good predictions can be done with afar simpler model capturing repeating patterns in the predicted data.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7926933765411377}]}, {"text": "We present several simple, yet highly effective confidence measures capturing such patterns, and show how to use them to train a semantic parser without manually annotated sentences.", "labels": [], "entities": []}, {"text": "Our basic premise, that predictions with high confidence score are of high quality, is further used to improve the performance of the unsupervised train-1486 ing procedure.", "labels": [], "entities": []}, {"text": "Our learning algorithm takes an EMlike iterative approach, in which the predictions of the previous stage are used to bias the model.", "labels": [], "entities": []}, {"text": "While this basic scheme was successfully applied to many unsupervised tasks, it is known to converge to a sub optimal point.", "labels": [], "entities": []}, {"text": "We show that by using confidence estimation as a proxy for the model's prediction quality, the learning algorithm can identify a better model compared to the default convergence criterion.", "labels": [], "entities": []}, {"text": "We evaluate our learning approach and model on the well studied Geoquery domain), consisting of natural language questions and their prolog interpretations used to query a database consisting of U.S. geographical information.", "labels": [], "entities": [{"text": "Geoquery domain", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9411022365093231}]}, {"text": "Our experimental results show that using our approach we are able to train a good semantic parser without annotated data, and that using a confidence score to identify good models results in a significant performance improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experimental evaluation.", "labels": [], "entities": []}, {"text": "We compare several confidence measures and analyze their properties.", "labels": [], "entities": []}, {"text": "1 defines the naming conventions used throughout this section to refer to the different models we evaluated.", "labels": [], "entities": []}, {"text": "We begin by describing our experimental setup and then proceed to describe the experiments and their results.", "labels": [], "entities": []}, {"text": "For the sake of clarity we focus on the best performing models (COMBINED using BIGRAM and PROPORTION) first and discuss other models later in the section.", "labels": [], "entities": [{"text": "COMBINED", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9502344727516174}, {"text": "BIGRAM", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.993994414806366}]}, {"text": "In all our experiments we used the Geoquery dataset (, consisting of U.S. geography NL questions and their corresponding Prolog logical MR.", "labels": [], "entities": [{"text": "Geoquery dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9555153250694275}, {"text": "U.S. geography NL questions", "start_pos": 69, "end_pos": 96, "type": "DATASET", "confidence": 0.8217728734016418}, {"text": "Prolog logical MR", "start_pos": 121, "end_pos": 138, "type": "DATASET", "confidence": 0.8881088495254517}]}, {"text": "We used the data split described in (), consisting of 250 queries for evaluation purposes.", "labels": [], "entities": []}, {"text": "We compared our system to several supervised models, which were trained using a disjoint set of queries.", "labels": [], "entities": []}, {"text": "Our learning system had access only to the NL questions, and the logical forms were only used to evaluate the system's performance.", "labels": [], "entities": []}, {"text": "We report the proportion of correct structures (accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9994277358055115}]}, {"text": "Note that this evaluation cor-1491 responds to the 0/1 loss over the predicted structures.", "labels": [], "entities": []}, {"text": "Initialization Our learning framework requires an initial weight vector as input.", "labels": [], "entities": [{"text": "Initialization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9706043004989624}]}, {"text": "We use a straightforward heuristic and provide uniform positive weights to three features.", "labels": [], "entities": []}, {"text": "This approach is similar in spirit to previous.", "labels": [], "entities": []}, {"text": "We refer to this system as INI-TIAL MODEL throughout this section.", "labels": [], "entities": [{"text": "INI-TIAL MODEL", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.7257720530033112}]}, {"text": "Competing Systems We compared our system to several other systems: (1) PRED.", "labels": [], "entities": [{"text": "PRED", "start_pos": 71, "end_pos": 75, "type": "TASK", "confidence": 0.5220203995704651}]}, {"text": "SCORE: An unsupervised framework using the model's internal prediction score (w T \u03a6(x, y, z)) for confidence estimation.", "labels": [], "entities": [{"text": "SCORE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7111442685127258}, {"text": "internal prediction score (w T \u03a6", "start_pos": 51, "end_pos": 83, "type": "METRIC", "confidence": 0.7951164415904454}]}, {"text": "(2) ALL EXAMPLES: Treating all predicted structures as correct, i.e., at each iteration the model is trained overall the predictions it made.", "labels": [], "entities": [{"text": "ALL EXAMPLES", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.7827155888080597}]}, {"text": "The reported score was obtained by selecting the model at the training iteration with the highest overall confidence score (see line 12 in Alg. 1).", "labels": [], "entities": []}, {"text": "(3) RESPONSE BASED: A natural upper bound to our framework is the approach used in (.", "labels": [], "entities": [{"text": "RESPONSE", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9690144658088684}, {"text": "BASED", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.7601792216300964}]}, {"text": "While our approach is based on assessing the correctness os the model's predictions according to unsupervised confidence estimation, their framework is provided with external supervision for these decisions, indicating if the predicted structures are correct.", "labels": [], "entities": []}, {"text": "(4) SUPERVISED: A fully supervised framework trained over 250 (x, z) pairs using structured SVM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparing our Self-trained systems with  Response-based and supervised models. Results show  that our COMBINED approach outperforms all other un- supervised models.", "labels": [], "entities": []}, {"text": " Table 4: Using confidence to approximate model perfor- mance. We compare the best result obtained in any of the  learning algorithm iterations (Best), the result obtained  by approximating the best result using the averaged pre- diction confidence (Conf. estim.) and the result of us- ing the default convergence criterion (Default). Results  in parentheses are the result of using the UNIGRAM con- fidence to approximate the model's performance.", "labels": [], "entities": []}]}