{"title": [{"text": "Clause Restructuring For SMT Not Absolutely Helpful", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9953358769416809}]}], "abstractContent": [{"text": "There area number of systems that use a syntax-based reordering step prior to phrase-based statistical MT.", "labels": [], "entities": [{"text": "phrase-based statistical MT", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.4988046884536743}]}, {"text": "An early work proposing this idea showed improved translation performance , but subsequent work has had mixed results.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9800921678543091}]}, {"text": "Speculations as to cause have suggested the parser, the data, or other factors.", "labels": [], "entities": []}, {"text": "We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?", "labels": [], "entities": [{"text": "PSMT", "start_pos": 141, "end_pos": 145, "type": "TASK", "confidence": 0.9600326418876648}]}], "introductionContent": [{"text": "Phrase-based statistical machine translation (PSMT) translates documents from one human language to another by dividing text into contiguous sequences of words (phrases), translating each, and finally reordering them according to a distortion model.", "labels": [], "entities": [{"text": "Phrase-based statistical machine translation (PSMT) translates documents from one human language", "start_pos": 0, "end_pos": 96, "type": "TASK", "confidence": 0.8094123785312359}]}, {"text": "The PSMT distortion model typically does not consider linguistic information, and as such encounters difficulty in language pairs that require specific long-distance reorderings, such as German-English.", "labels": [], "entities": [{"text": "PSMT distortion", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9254492521286011}]}, {"text": "address this problem by reordering German sentences to more closely parallel English word order, prior to translation by a PSMT system.", "labels": [], "entities": []}, {"text": "They find that this reordering-aspreprocessing approach results in a significant improvement in translation performance over the baseline.", "labels": [], "entities": []}, {"text": "However, there have been several other systems using the reordering-as-preprocessing approach, and they have met with mixed success.", "labels": [], "entities": []}, {"text": "We systematically explore possible explanations for these contradictory results, and conclude that, while reordering is helpful for some sentences, potential improvement can be eroded by many aspects of the PSMT system, independent of the reordering.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a number of experiments with the HD system to attempt to replicate the CKK and HD findings.", "labels": [], "entities": [{"text": "CKK", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9035957455635071}]}, {"text": "All parts of the system are available online.", "labels": [], "entities": []}, {"text": "1 Each experiment is paired: the reordered system reuses the recasing and language models of its corresponding baseline system, to eliminate one source of possible variation.", "labels": [], "entities": []}, {"text": "Training the parser with less data affects only the reordered systems; for experiments using these models, the corresponding baselines (and thus the shared models) are not retrained.", "labels": [], "entities": []}, {"text": "For each system pair, we also run the HD oracle.", "labels": [], "entities": []}, {"text": "All systems are evaluated using case-insensitive BLEU (  from the Moses multi-reference BLEU script (multibleu), using one reference translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9438687562942505}]}, {"text": "Comparing the scripts, we found that the NIST scores are always lower than multi-bleu's on test2008, but higher on newstest2009, with differences at most 0.23.", "labels": [], "entities": [{"text": "NIST", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.7405396103858948}]}, {"text": "This partially indicates the noise level in the scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision and recall for the parsers mentioned in   \u00a73. The numbers are collated for reference only and are  not directly comparable; see the text for details.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9973101615905762}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9986095428466797}]}, {"text": " Table 2: Corpora used, and # of sentence pairs in each.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores for each experiment on Europarl  test set. Columns give: language model order, distortion  model (distance, lexicalised), tuning data (none (-), Eu- roparl, News), baseline BLEU score, reordered system  BLEU score, performance increase, oracle BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991769194602966}, {"text": "Europarl  test set", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.9949494997660319}, {"text": "BLEU score", "start_pos": 195, "end_pos": 205, "type": "METRIC", "confidence": 0.9178951978683472}, {"text": "BLEU score", "start_pos": 225, "end_pos": 235, "type": "METRIC", "confidence": 0.9413368999958038}, {"text": "BLEU score", "start_pos": 266, "end_pos": 276, "type": "METRIC", "confidence": 0.9771145582199097}]}, {"text": " Table 5: Results on news test set. Columns as for Table 4.", "labels": [], "entities": [{"text": "news test set", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9360103805859884}]}, {"text": " Table 6: Results using the smaller parsing models.  Columns are as for Table 4 except LM removed (all are  3-gram), and parser data percentage (%) added.", "labels": [], "entities": []}]}