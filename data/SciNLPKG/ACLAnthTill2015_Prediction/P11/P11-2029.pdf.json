{"title": [{"text": "How Much Can We Gain from Supervised Word Alignment?", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.6800895184278488}]}], "abstractContent": [{"text": "Word alignment is a central problem in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7264422625303268}, {"text": "statistical machine translation (SMT)", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.8069427212079366}]}, {"text": "In recent years, supervised alignment algorithms , which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9078804850578308}]}, {"text": "The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm.", "labels": [], "entities": [{"text": "SMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9937006235122681}]}, {"text": "Our experiments used a manually aligned Chinese-English corpus with 280K words recently released by the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 104, "end_pos": 136, "type": "DATASET", "confidence": 0.8293492496013641}]}, {"text": "We treated the human alignment as the oracle of supervised alignment.", "labels": [], "entities": []}, {"text": "The result is surprising: the gain of human alignment over a state of the art unsuper-vised method (GIZA++) is less than 1 point in BLEU.", "labels": [], "entities": [{"text": "human alignment", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.5809738636016846}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9866765737533569}]}, {"text": "Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word alignment is a central problem in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7264422625303268}, {"text": "statistical machine translation (SMT)", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.8069427212079366}]}, {"text": "A recent trend in this area of research is to exploit supervised learning to improve alignment accuracy by mimicking human alignment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.90465247631073}, {"text": "mimicking human alignment", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.7578500509262085}]}, {"text": "Studies in this line of work include, just to name a few.", "labels": [], "entities": []}, {"text": "The objective of this work is to explore the performance limit of supervised word alignment.", "labels": [], "entities": [{"text": "supervised word alignment", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.6042094528675079}]}, {"text": "More specifically, we would like to know what magnitude of gain in MT performance we can expect from supervised alignment over the state of the art unsupervised alignment if we have access to a large amount of parallel data.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9929312467575073}]}, {"text": "Since alignment errors have been assumed to be a major hindrance to good MT, an answer to such a question might help us find new directions in MT research.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9948392510414124}, {"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.997327446937561}]}, {"text": "Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (, a state of the art unsupervised aligner.", "labels": [], "entities": []}, {"text": "Our study was based on a manually aligned ChineseEnglish corpus) with 280K word tokens.", "labels": [], "entities": [{"text": "ChineseEnglish corpus", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.9527486562728882}]}, {"text": "Such a study has been previously impossible due to the lack of a hand-aligned corpus of sufficient size.", "labels": [], "entities": []}, {"text": "To our surprise, the gain in MT performance using human alignment is very small, less than 1 point in BLEU.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.994999885559082}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9986032843589783}]}, {"text": "Furthermore, our diagnostic experiments indicate that the result is not an artifact of small training size since alignment errors are less harmful with more data.", "labels": [], "entities": []}, {"text": "We would like to stress that our result does not mean we should discontinue research in improving word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.7858129143714905}]}, {"text": "Rather it shows that current translation models, of which the string-to-tree model used in this work is an example, cannot fully utilize super-accurate word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 152, "end_pos": 166, "type": "TASK", "confidence": 0.7134084105491638}]}, {"text": "In order to significantly improve MT quality we need to improve both word alignment and the translation model.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.996826708316803}, {"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7903623580932617}]}, {"text": "In fact, we found that some of the information in the LDC hand-aligned corpus that might be useful for resolving certain translation ambiguities (e.g. verb tense, pronoun coreferences and modifier-head relations) is even harmful to the system used in this work.", "labels": [], "entities": [{"text": "LDC hand-aligned corpus", "start_pos": 54, "end_pos": 77, "type": "DATASET", "confidence": 0.6860591272513071}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2, giza-strong vs. giza-weak).", "labels": [], "entities": []}, {"text": " Table 2: MT results (lower case) on small corpus", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9638486504554749}]}, {"text": " Table 3: MT results (lower case) on large corpus", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9763957262039185}]}]}