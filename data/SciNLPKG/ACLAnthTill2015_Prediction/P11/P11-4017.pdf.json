{"title": [{"text": "Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia's Edit History", "labels": [], "entities": [{"text": "Wikipedia Revision Toolkit", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7159962256749471}]}], "abstractContent": [{"text": "We present an open-source toolkit which allows (i) to reconstruct past states of Wikipedia, and (ii) to efficiently access the edit history of Wikipedia articles.", "labels": [], "entities": []}, {"text": "Reconstructing past states of Wikipedia is a prerequisite for reproducing previous experimental work based on Wikipedia.", "labels": [], "entities": [{"text": "Reconstructing past states of Wikipedia", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.773212468624115}]}, {"text": "Beyond that, the edit history of Wikipedia articles has been shown to be a valuable knowledge source for NLP, but access is severely impeded by the lack of efficient tools for managing the huge amount of provided data.", "labels": [], "entities": []}, {"text": "By using a dedicated storage format, our toolkit massively decreases the data volume to less than 2% of the original size, and at the same time provides an easy-to-use interface to access the revision data.", "labels": [], "entities": []}, {"text": "The language-independent design allows to process any language represented in Wikipedia.", "labels": [], "entities": []}, {"text": "We expect this work to consolidate NLP research using Wikipedia in general, and to foster research making use of the knowledge encoded in Wikipedia's edit history.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last decade, the free encyclopedia Wikipedia has become one of the most valuable and comprehensive knowledge sources in Natural Language Processing.", "labels": [], "entities": []}, {"text": "It has been used for numerous NLP tasks, e.g. word sense disambiguation, semantic relatedness measures, or text categorization.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7277642488479614}, {"text": "text categorization", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7548125684261322}]}, {"text": "A detailed survey on usages of Wikipedia in NLP can be found in (.", "labels": [], "entities": []}, {"text": "The majority of Wikipedia-based NLP algorithms works on single snapshots of Wikipedia, which are published by the Wikimedia Foundation as XML dumps at irregular intervals.", "labels": [], "entities": []}, {"text": "Such a snapshot only represents the state of Wikipedia at a certain fixed point in time, while Wikipedia actually is a dynamic resource that is constantly changed by its millions of editors.", "labels": [], "entities": []}, {"text": "This rapid change is bound to have an influence on the performance of NLP algorithms using Wikipedia data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.8546977639198303}]}, {"text": "However, the exact consequences are largely unknown, as only very few papers have systematically analyzed this influence.", "labels": [], "entities": []}, {"text": "This is mainly due to older snapshots becoming unavailable, as there is no official backup server.", "labels": [], "entities": []}, {"text": "As a consequence, older experimental results cannot be reproduced anymore.", "labels": [], "entities": []}, {"text": "In this paper, we present a toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation inform of a database dump.", "labels": [], "entities": []}, {"text": "Section 3 gives a more detailed overview of the reconstruction process.", "labels": [], "entities": []}, {"text": "Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms.", "labels": [], "entities": []}, {"text": "The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection), text summarization, sentence compression, unsupervised extraction of lexical simplifications, the expansion of textual entailment corpora (, or assesing the trustworthiness of Wikipedia articles ().", "labels": [], "entities": [{"text": "vandalism detection", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7437363713979721}, {"text": "text summarization", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7545022070407867}, {"text": "sentence compression", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.7743989825248718}, {"text": "unsupervised extraction of lexical simplifications", "start_pos": 163, "end_pos": 213, "type": "TASK", "confidence": 0.7464203715324402}]}, {"text": "However, efficient access to this new resource has been limited by the immense size of the data.", "labels": [], "entities": []}, {"text": "The revisions for all articles in the current English Wikipedia sum up to over 5 terabytes of text.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.8774505853652954}]}, {"text": "Consequently, most of the above mentioned previous work only regarded small samples of the available data.", "labels": [], "entities": []}, {"text": "However, using more data usually leads to better results, or how put it \"more data are better data\".", "labels": [], "entities": []}, {"text": "Thus, in Section 4, we present a tool to efficiently access Wikipedia's edit history.", "labels": [], "entities": [{"text": "Wikipedia's edit history", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.7835784703493118}]}, {"text": "It provides an easy-to-use API for programmatically accessing the revision data and reduces the required storage space to less than 2% of its original size.", "labels": [], "entities": []}, {"text": "Both tools are publicly available on Google Code (http://jwpl.googlecode. com) as open source software under the LGPL v3.", "labels": [], "entities": [{"text": "Google Code", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9255124926567078}]}], "datasetContent": [], "tableCaptions": []}