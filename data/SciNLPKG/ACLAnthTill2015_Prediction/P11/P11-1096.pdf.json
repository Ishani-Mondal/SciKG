{"title": [{"text": "Simple Supervised Document Geolocation with Geodesic Grids", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents.", "labels": [], "entities": []}, {"text": "Geolocation can bean effective means of summarizing large document collections and it is an important component of geographic information retrieval.", "labels": [], "entities": [{"text": "summarizing large document collections", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.9192972928285599}, {"text": "geographic information retrieval", "start_pos": 115, "end_pos": 147, "type": "TASK", "confidence": 0.659762034813563}]}, {"text": "We describe several simple supervised methods for document geolocation using only the document's raw text as evidence.", "labels": [], "entities": [{"text": "document geolocation", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6914129853248596}]}, {"text": "All of our methods predict locations in the context of geodesic grids of varying degrees of resolution.", "labels": [], "entities": []}, {"text": "We evaluate the methods on geotagged Wikipedia articles and Twitter feeds.", "labels": [], "entities": []}, {"text": "For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers.", "labels": [], "entities": [{"text": "median prediction error", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.8563669125239054}]}, {"text": "Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.", "labels": [], "entities": [{"text": "median error", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.8981455862522125}]}], "introductionContent": [{"text": "There area variety of applications that arise from connecting linguistic content-be it a word, phrase, document, or entire corpus-to geography.", "labels": [], "entities": []}, {"text": "provides a systematic overview of geography-based language applications over the previous decade, with a special focus on the problem of toponym resolution-identifying and disambiguating the references to locations in texts.", "labels": [], "entities": [{"text": "toponym resolution-identifying and disambiguating the references to locations in texts", "start_pos": 137, "end_pos": 223, "type": "TASK", "confidence": 0.8752311825752258}]}, {"text": "Perhaps the most obvious and far-reaching application is geographic information retrieval (, with applications like MetaCarta's geographic text search () and NewsStand (; these allow users to browse and search for content through a geo-centric interface.", "labels": [], "entities": [{"text": "geographic information retrieval", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.6358307103315989}]}, {"text": "The Perseus project performs automatic toponym resolution on historical texts in order to display a map with each text showing the locations that are mentioned); Google Books also does this for some books, though the toponyms are identified and resolved quite crudely.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7745411396026611}]}, {"text": "use a location-based topic model to summarize travelogues, enrich them with automatically chosen images, and provide travel recommendations.", "labels": [], "entities": []}, {"text": "Eisenstein et al investigate questions of dialectal differences and variation in regional interests in Twitter users using a collection of geotagged tweets.", "labels": [], "entities": []}, {"text": "An intuitive and effective strategy for summarizing geographically-based data is identification of the location-a specific latitude and longitude-that forms the primary focus of each document.", "labels": [], "entities": [{"text": "summarizing geographically-based", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.8931924402713776}]}, {"text": "Determining a single location of a document is only a well-posed problem for certain documents, generally of fairly small size, but there area number of natural situations in which such collections arise.", "labels": [], "entities": [{"text": "Determining a single location of a document", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8848206996917725}]}, {"text": "For example, a great number of articles in Wikipedia have been manually geotagged; this allows those articles to appear in their geographic locations while geobrowsing in an application like Google Earth.", "labels": [], "entities": []}, {"text": "investigates the use of Wikipedia as a source of data for article geolocation, in addition to article classification by category (location, person, etc.) and toponym resolution.", "labels": [], "entities": [{"text": "article geolocation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.754290759563446}, {"text": "article classification", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.6785759627819061}, {"text": "toponym resolution", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.6435025930404663}]}, {"text": "Overell's main goal is toponym resolution, for which geolocation serves as an input feature.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8030973076820374}]}, {"text": "For document geolocation, Overell uses a simple model that makes use only of the metadata available (article title, incoming and outgoing links, etc.)-the actual article text 955 is not used at all.", "labels": [], "entities": []}, {"text": "However, for many document collections, such metadata is unavailable, especially in the case of recently digitized historical documents.", "labels": [], "entities": []}, {"text": "evaluate their geographic topic model by geolocating USA-based Twitter users based on their tweet content.", "labels": [], "entities": []}, {"text": "This is essentially a document geolocation task, where each document is a concatenation of all the tweets fora single user.", "labels": [], "entities": []}, {"text": "Their geographic topic model receives supervision from many documents/users and predicts locations for unseen documents/users.", "labels": [], "entities": []}, {"text": "In this paper, we tackle document geolocation using several simple supervised methods on the textual content of documents and a geodesic grid as a discrete representation of the earth's surface.", "labels": [], "entities": []}, {"text": "Our approach is similar to that of, who geolocate Flickr images using their associated textual tags.", "labels": [], "entities": []}, {"text": "1 Essentially, the task is cast similarly to language modeling approaches in information retrieval ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.7914145588874817}]}, {"text": "Discrete cells representing areas on the earth's surface correspond to documents (with each cell-document being a concatenation of all actual documents that are located in that cell); new documents are then geolocated to the most similar cell according to standard measures such as Kullback-Leibler divergence).", "labels": [], "entities": []}, {"text": "Performance is measured both on geotagged Wikipedia articles) and tweets (.", "labels": [], "entities": []}, {"text": "We obtain high accuracy on Wikipedia using KL divergence, with a median error of just 11.8 kilometers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9991505146026611}, {"text": "KL divergence", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.5631004422903061}]}, {"text": "For the Twitter data set, we obtain a median error of 479 km, which improves on the 494 km error of Eisenstein et al.", "labels": [], "entities": [{"text": "Twitter data set", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9014443953831991}, {"text": "median error", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.9612240493297577}]}, {"text": "An advantage of our approach is that it is far simpler, is easy to implement, and scales straightforwardly to large datasets like Wikipedia.", "labels": [], "entities": []}], "datasetContent": [{"text": "The approaches described in the previous section are evaluated on both the geotagged Wikipedia and Twitter datasets.", "labels": [], "entities": [{"text": "Wikipedia and Twitter datasets", "start_pos": 85, "end_pos": 115, "type": "DATASET", "confidence": 0.6391351297497749}]}, {"text": "Given a predicted cel\u00ee c fora document, the prediction error is the great-circle distance between the true location and the center of\u02c6cof\u02c6 of\u02c6c, as described in section 3.", "labels": [], "entities": [{"text": "prediction error", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9449474513530731}]}], "tableCaptions": [{"text": " Table 1: Mean prediction error (km) on the Twitter dev  set for various combinations of vocabulary threshold (in  feeds) and grid size, using the KL divergence strategy.", "labels": [], "entities": [{"text": "Mean prediction error (km)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9381459752718607}, {"text": "Twitter dev  set", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.9086166024208069}]}, {"text": " Table 2: Prediction error (km) on the Wikipedia and Twitter test sets for each of the strategies using the optimal grid  resolution and (for Twitter) the optimal threshold, as determined by performance on the corresponding development  sets. Eisenstein et al. (2010) used a fixed Twitter threshold of 40. Threshold makes no difference for cell prior  maximum.", "labels": [], "entities": [{"text": "Prediction error (km)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9565938711166382}, {"text": "Wikipedia and Twitter test sets", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.7454327642917633}, {"text": "Threshold", "start_pos": 306, "end_pos": 315, "type": "METRIC", "confidence": 0.9986286163330078}]}]}