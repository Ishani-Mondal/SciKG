{"title": [{"text": "Extracting Opinion Expressions and Their Polarities -Exploration of Pipelines and Joint Models", "labels": [], "entities": [{"text": "Extracting Opinion Expressions", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8777307073275248}]}], "abstractContent": [{"text": "We investigate systems that identify opinion expressions and assigns polarities to the extracted expressions.", "labels": [], "entities": []}, {"text": "In particular, we demonstrate the benefit of integrating opinion extraction and polarity classification into a joint model using features reflecting the global polarity structure.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7464701533317566}, {"text": "polarity classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7136615067720413}]}, {"text": "The model is trained using large-margin structured prediction methods.", "labels": [], "entities": []}, {"text": "The system is evaluated on the MPQA opinion corpus, where we compare it to the only previously published end-to-end system for opinion expression extraction and polarity classification.", "labels": [], "entities": [{"text": "MPQA opinion corpus", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.9600609938303629}, {"text": "opinion expression extraction", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.6568185389041901}, {"text": "polarity classification", "start_pos": 161, "end_pos": 184, "type": "TASK", "confidence": 0.668832540512085}]}, {"text": "The results show an improvement of between 10 and 15 absolute points in F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9691435694694519}]}], "introductionContent": [{"text": "Automatic systems for the analysis of opinions expressed in text on the web have been studied extensively.", "labels": [], "entities": [{"text": "analysis of opinions expressed in text on the web", "start_pos": 26, "end_pos": 75, "type": "TASK", "confidence": 0.8841213451491462}]}, {"text": "Initially, this was formulated as a coarsegrained task -locating opinionated documentsand tackled using methods derived from standard retrieval or categorization.", "labels": [], "entities": []}, {"text": "However, in recent years there has been a shift towards a more detailed task: not only finding the text expressing the opinion, but also analysing it: who holds the opinion and to what is addressed; it is positive or negative (polarity); what its intensity is.", "labels": [], "entities": []}, {"text": "This more complex formulation leads us deep into NLP territory; the methods employed here have been inspired by information extraction and semantic role labeling, combinatorial optimization and structured machine learning.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.8119730949401855}, {"text": "semantic role labeling", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.6158824662367502}]}, {"text": "A crucial step in the automatic analysis of opinion is to markup the opinion expressions: the pieces of text allowing us to infer that someone has a particular feeling about some topic.", "labels": [], "entities": []}, {"text": "Then, opinions can be assigned a polarity describing whether the feeling is positive, neutral or negative.", "labels": [], "entities": []}, {"text": "These two tasks have generally been tackled in isolation.", "labels": [], "entities": []}, {"text": "introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in); later we also added holder extraction.", "labels": [], "entities": [{"text": "holder extraction", "start_pos": 210, "end_pos": 227, "type": "TASK", "confidence": 0.8497942388057709}]}, {"text": "For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features ().", "labels": [], "entities": []}, {"text": "While the tasks of expression detection and polarity classification have mostly been studied in isolation, developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities.", "labels": [], "entities": [{"text": "expression detection", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.782141923904419}, {"text": "polarity classification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7287129908800125}]}, {"text": "This is so far the only published result on joint opinion segmentation and polarity classification.", "labels": [], "entities": [{"text": "joint opinion segmentation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6283675829569498}, {"text": "polarity classification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.8818131983280182}]}, {"text": "However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier.", "labels": [], "entities": []}, {"text": "In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a sequence labeler, which, by construction, is restricted to use simple local features.", "labels": [], "entities": [{"text": "expression extraction", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7279438376426697}]}, {"text": "In contrast, in (Johansson and Moschitti, 2010b), we showed that global structure matters: opinions interact to a large extent, and we can learn about their interactions on the opinion level by means of their interactions on the syntactic and semantic levels.", "labels": [], "entities": []}, {"text": "It is intuitive that this should also be valid when polarities enter the 101 picture -this was also noted by.", "labels": [], "entities": []}, {"text": "Evaluative adjectives referring to the same evaluee may cluster together in the same clause or be dominated by a verb of categorization; opinions with opposite polarities maybe conjoined through a contrastive discourse connective such as but.", "labels": [], "entities": []}, {"text": "In this paper, we first implement two strong baselines consisting of pipelines of opinion expression segmentation and polarity labeling and compare them to the joint opinion extractor and polarity classifier by.", "labels": [], "entities": [{"text": "opinion expression segmentation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6393576363722483}]}, {"text": "Secondly, we extend the global structure approach and add features reflecting the polarity structure of the sentence.", "labels": [], "entities": []}, {"text": "Our systems were superior by between 8 and 14 absolute F-measure points.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9466190934181213}]}], "datasetContent": [{"text": "Opinion expression boundaries are hard to define rigorously ( ), so evaluations of their quality typically use soft metrics.", "labels": [], "entities": []}, {"text": "The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard.", "labels": [], "entities": [{"text": "MPQA annotators", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9558352530002594}, {"text": "overlap metric", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.9611238837242126}]}, {"text": "This has also been used to evaluate opinion extractors (.", "labels": [], "entities": [{"text": "opinion extractors", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8153564929962158}]}, {"text": "However, this metric has a number of problems: 1) it is possible to \"fool\" the metric by creating expressions that cover the whole sentence; 2) it does not give higher credit to output that is \"almost perfect\" rather than \"almost incorrect\".", "labels": [], "entities": []}, {"text": "Therefore, in, we measured the intersection between the system output and the gold standard: every compared segment is assigned a score between 0 and 1, as opposed to strict or overlap scoring that only assigns 0 or 1.", "labels": [], "entities": []}, {"text": "For compatibility we present results in both metrics.", "labels": [], "entities": []}, {"text": "We first compared the two baselines to the new integrated segmentation/polarity system.", "labels": [], "entities": []}, {"text": "shows the performance according to the intersection metric.", "labels": [], "entities": []}, {"text": "Our first baseline consists of an expression segmenter and a polarity classifier (ES+PC), while in the second baseline we also add the expression reranker (ER) as we did in).", "labels": [], "entities": [{"text": "reranker (ER)", "start_pos": 146, "end_pos": 159, "type": "METRIC", "confidence": 0.738809198141098}]}, {"text": "The new reranker described in this paper is referred to as the expression/polarity reranker (EPR).", "labels": [], "entities": [{"text": "expression/polarity reranker (EPR)", "start_pos": 63, "end_pos": 97, "type": "METRIC", "confidence": 0.6924024309430804}]}, {"text": "We carried out the evaluation using the same partition of the MPQA dataset as in our previous work, with 541 documents in the training set and 150 in the test set.", "labels": [], "entities": [{"text": "MPQA dataset", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.9806272983551025}]}, {"text": "The result shows that the reranking-based models give us significant boosts in recall, following our previous results in, which also mainly improved the recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9996777772903442}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9991036057472229}]}, {"text": "The precision shows a slight drop but much lower than the recall improvement.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9998764991760254}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9996510744094849}]}], "tableCaptions": [{"text": " Table 1: Results with intersection metric.", "labels": [], "entities": []}, {"text": " Table 2: Results with overlap metric.", "labels": [], "entities": [{"text": "overlap", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9622424244880676}]}]}