{"title": [{"text": "BLAST: A Tool for Error Analysis of Machine Translation Output", "labels": [], "entities": [{"text": "BLAST", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5875569581985474}, {"text": "Error Analysis of Machine Translation Output", "start_pos": 18, "end_pos": 62, "type": "TASK", "confidence": 0.8111719886461893}]}], "abstractContent": [{"text": "We present BLAST, an open source tool for error analysis of machine translation (MT) output.", "labels": [], "entities": [{"text": "BLAST", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9851443767547607}, {"text": "error analysis of machine translation (MT) output", "start_pos": 42, "end_pos": 91, "type": "TASK", "confidence": 0.8141433629724715}]}, {"text": "We believe that error analysis, i.e., to identify and classify MT errors, should bean integral part of MT development, since it gives a qualitative view, which is not obtained by standard evaluation methods.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.646502360701561}, {"text": "MT errors", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.7726464867591858}, {"text": "MT development", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.9726344347000122}]}, {"text": "BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface.", "labels": [], "entities": [{"text": "BLAST", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7685580849647522}, {"text": "MT researchers", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9152975678443909}]}, {"text": "It is designed to be flexible, and can be used with any MT system, language pair, and error typology.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9351771473884583}]}, {"text": "The annotation task can be aided by highlighting similarities with a reference translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation evaluation is a difficult task, since there is not only one correct translation of a sentence, but many equally good translation options.", "labels": [], "entities": [{"text": "Machine translation evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8910210529963175}]}, {"text": "Often, machine translation (MT) systems are only evaluated quantitatively, e.g. by the use of automatic metrics, which is fast and cheap, but does not give any indication of the specific problems of a MT system.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8595759868621826}, {"text": "MT", "start_pos": 201, "end_pos": 203, "type": "TASK", "confidence": 0.9754458665847778}]}, {"text": "Thus, we advocate human error analysis of MT output, where humans identify and classify the problems in machine translated sentences.", "labels": [], "entities": [{"text": "human error analysis", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.6077147225538889}, {"text": "MT output", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.856791079044342}]}, {"text": "In this paper we present BLAST, 1 a graphical tool for performing human error analysis, from any MT system and for any language pair.", "labels": [], "entities": [{"text": "BLAST", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9701762795448303}, {"text": "human error analysis", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.6098511616388956}]}, {"text": "BLAST has a graphical user interface, and is designed to be easy The BiLingual Annotation/Annotator/Analysis Support Tool, available for download at http://www.ida.liu.", "labels": [], "entities": [{"text": "BLAST", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5479415059089661}]}, {"text": "se/ \u223c sarst/blast/ and intuitive to work with.", "labels": [], "entities": []}, {"text": "It can aid the user by highlighting similarities with a reference sentence.", "labels": [], "entities": []}, {"text": "BLAST is flexible in that it can be used with output from any MT system, and with any hierarchical error typology.", "labels": [], "entities": [{"text": "BLAST", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7334802150726318}]}, {"text": "It has a modular design, allowing easy extension with new modules.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no other publicly available tool for MT error annotation.", "labels": [], "entities": [{"text": "MT error annotation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.9045136769612631}]}, {"text": "Since we believe that error analysis is a vital complement to MT evaluation, we think that BLAST can be useful for many other MT researchers and developers.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9655029475688934}, {"text": "BLAST", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9669767022132874}, {"text": "MT", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.9812039732933044}]}, {"text": "discussed the complexity of MT evaluation, and stressed the importance of adjusting evaluation to the purpose and context of the translation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9784907102584839}]}, {"text": "However, MT is very often only evaluated quantitatively using a single metric, especially in research papers.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9909080266952515}]}, {"text": "Quantitative evaluations can be automatic, using metrics such as Bleu () or Meteor (, where the MT output is compared to one or more human reference translations.", "labels": [], "entities": [{"text": "Quantitative evaluations", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8783282935619354}, {"text": "Bleu", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9476767182350159}, {"text": "Meteor", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8849056363105774}]}, {"text": "Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system.", "labels": [], "entities": []}, {"text": "Comparing scores from different metrics can give a very rough indication of some major problems, especially in combination with a part-ofspeech analysis).", "labels": [], "entities": []}], "datasetContent": [{"text": "Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g.).", "labels": [], "entities": []}, {"text": "A combination of human and automatic metrics is human-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed).", "labels": [], "entities": [{"text": "TER", "start_pos": 201, "end_pos": 204, "type": "METRIC", "confidence": 0.9921092391014099}]}, {"text": "While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system.", "labels": [], "entities": []}, {"text": "Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.7459619045257568}, {"text": "identification and classification of MT errors", "start_pos": 100, "end_pos": 146, "type": "TASK", "confidence": 0.8041194975376129}]}, {"text": "There have been several suggestions for general MT error typologies, targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems.", "labels": [], "entities": [{"text": "MT error typologies", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8880506753921509}]}, {"text": "It is also possible to focus error analysis at a specific problem, such as verb form errors ().", "labels": [], "entities": []}, {"text": "We have not been able to find any other freely available tool for error analysis of MT.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7245974838733673}, {"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9142796397209167}]}, {"text": "mentioned in a footnote that \"a tool for highlighting the differences [between the MT system and a correct translation] also proved to be quite useful\" for error analysis.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.8323067426681519}, {"text": "error analysis", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.6194580644369125}]}, {"text": "They do not describe this tool any further, and do not discuss if it was also used to mark and store the error annotations themselves.", "labels": [], "entities": []}, {"text": "Some tools for post-editing of MT output, a related activity to error analysis, have been described in the literature.", "labels": [], "entities": [{"text": "MT output", "start_pos": 31, "end_pos": 40, "type": "TASK", "confidence": 0.8969046771526337}, {"text": "error analysis", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.691020056605339}]}, {"text": "Font presented an online tool for eliciting information from the user when post-editing sentences, in order to improve a rule-based translation system.", "labels": [], "entities": [{"text": "Font", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9430304169654846}]}, {"text": "The post-edit operations were labeled with error categories, making it a type of error analysis.", "labels": [], "entities": []}, {"text": "This tool was highly connected to their translation system, and it required users to post-edit sentences by modifying word alignments, something that many users found difficult.", "labels": [], "entities": []}, {"text": "described a postediting tool used for HTER calculation, which has been used in large evaluation campaigns.", "labels": [], "entities": [{"text": "HTER calculation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.9019646644592285}]}, {"text": "The tool is a pure post-editing tool and the edits are not classified.", "labels": [], "entities": []}, {"text": "Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and 2 Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison, and word alignment ().", "labels": [], "entities": [{"text": "MT-related tasks", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.925916314125061}, {"text": "MT evaluation", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.9227003455162048}, {"text": "2", "start_pos": 143, "end_pos": 144, "type": "METRIC", "confidence": 0.8625545501708984}, {"text": "information system comparison", "start_pos": 232, "end_pos": 261, "type": "TASK", "confidence": 0.587953507900238}, {"text": "word alignment", "start_pos": 267, "end_pos": 281, "type": "TASK", "confidence": 0.7746825814247131}]}], "tableCaptions": []}