{"title": [{"text": "Semi-supervised Relation Extraction with Large-scale Word Clustering", "labels": [], "entities": [{"text": "Semi-supervised Relation Extraction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6751168370246887}]}], "abstractContent": [{"text": "We present a simple semi-supervised relation extraction system with large-scale word clustering.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7151010185480118}, {"text": "word clustering", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7360257506370544}]}, {"text": "We focus on systematically exploring the effectiveness of different cluster-based features.", "labels": [], "entities": []}, {"text": "We also propose several statistical methods for selecting clusters at an appropriate level of granularity.", "labels": [], "entities": []}, {"text": "When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction is an important information extraction task in natural language processing (NLP), with many practical applications.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9628352522850037}, {"text": "information extraction", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8185951113700867}, {"text": "natural language processing (NLP)", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.7854014039039612}]}, {"text": "The goal of relation extraction is to detect and characterize semantic relations between pairs of entities in text.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7993994057178497}, {"text": "characterize semantic relations between pairs of entities in text", "start_pos": 49, "end_pos": 114, "type": "TASK", "confidence": 0.8189312352074517}]}, {"text": "For example, a relation extraction system needs to be able to extract an Employment relation between the entities US soldier and US in the phrase US soldier.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7871188223361969}]}, {"text": "Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based.", "labels": [], "entities": []}, {"text": "Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.8320357799530029}]}, {"text": "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (.", "labels": [], "entities": []}, {"text": "In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (;.", "labels": [], "entities": []}, {"text": "Both lines of work depend on effective features, either explicitly or implicitly.", "labels": [], "entities": []}, {"text": "The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features.", "labels": [], "entities": [{"text": "supervised relation extraction", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6786764164765676}]}, {"text": "For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not.", "labels": [], "entities": []}, {"text": "Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are crucial for extracting the relation.", "labels": [], "entities": []}, {"text": "This motivates our work to use word clusters as additional features for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8937372267246246}]}, {"text": "The assumption is that even if the word soldier may never have been seen in the annotated Employment relation instances, other words which share the same cluster membership with soldier such as president and ambassador may have been observed in the Employment instances.", "labels": [], "entities": []}, {"text": "The absence of lexical features can be compensated by the cluster features.", "labels": [], "entities": []}, {"text": "Moreover, word clusters may implicitly correspond to different relation classes.", "labels": [], "entities": []}, {"text": "For example, the cluster of president maybe related to the Employment relation as in US president while the cluster of businessman maybe related to the Affiliation relation as in US businessman.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: we explore the cluster-based features in a systematic way and propose several statistical methods for selecting effective clusters.", "labels": [], "entities": []}, {"text": "We study the impact of the size of training data on cluster features and analyze the performance improvements through an extensive experimental study.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 presents related work and Section 3 provides the background of the relation extraction task and the word clustering algorithm.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.8882033824920654}, {"text": "word clustering", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.8070220947265625}]}, {"text": "Section 4 describes in detail a state-of-the-art supervised baseline system.", "labels": [], "entities": []}, {"text": "Section 5 describes the clusterbased features and the cluster selection methods.", "labels": [], "entities": []}, {"text": "We present experimental results in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first present details of our unsupervised word clusters, the relation extraction data set and its preprocessing.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7467844486236572}]}, {"text": "We then present a series of experiments coupled with result analyses.", "labels": [], "entities": []}, {"text": "We used the English portion of the TDT5 corpora (LDC2006T18) as our unlabeled data for inducing word clusters.", "labels": [], "entities": [{"text": "TDT5 corpora (LDC2006T18)", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.9276123166084289}]}, {"text": "It contains roughly 83 million words in 3.4 million sentences with a vocabulary size of 450K.", "labels": [], "entities": []}, {"text": "We left case intact in the corpora.", "labels": [], "entities": []}, {"text": "Following previous work, we used Liang's implementation of the Brown clustering algorithm ().", "labels": [], "entities": []}, {"text": "We induced 1,000 word clusters for words that appeared at least twice in the corpora.", "labels": [], "entities": []}, {"text": "The reduced vocabulary contains 255K unique words.", "labels": [], "entities": []}, {"text": "The clusters are available at http://www.cs.nyu.edu/~asun/data/TDT5_BrownW C.tar.gz.", "labels": [], "entities": [{"text": "TDT5_BrownW C.tar.gz", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.8563915342092514}]}, {"text": "For relation extraction, we used the benchmark ACE 2004 training data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9793603718280792}, {"text": "ACE 2004 training data", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.9620282202959061}]}, {"text": "Following most of the previous research, we used in experiments the nwire (newswire) and bnews (broadcast news) genres of the data containing 348 documents and 4374 relation instances.", "labels": [], "entities": []}, {"text": "We extracted an instance for every pair of mentions in the same sentence which were separated by no more than two other mentions.", "labels": [], "entities": []}, {"text": "The non-relation instances generated were about 8 times more than the relation instances.", "labels": [], "entities": []}, {"text": "Preprocessing of the ACE documents: We used the Stanford parser 6 for syntactic and dependency parsing.", "labels": [], "entities": [{"text": "ACE documents", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.8816937208175659}, {"text": "dependency parsing", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7846409976482391}]}, {"text": "We used chunklink 7 to derive chunking information from the Stanford parsing.", "labels": [], "entities": []}, {"text": "Because some bnews documents are in lowercase, we recover the case for the head of a mention if its type is NAM by making the first character into its uppercase.", "labels": [], "entities": []}, {"text": "This is for better matching between the words in ACE and the words in the unsupervised word clusters.", "labels": [], "entities": []}, {"text": "We used the OpenNLP 8 maximum entropy (maxent) package as our machine learning tool.", "labels": [], "entities": []}, {"text": "We choose to work with maxent because the training is fast and it has a good support for multiclass classification.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7602715492248535}]}], "tableCaptions": [{"text": " Table 1: ACE relation types and examples from the  annotation guideline 2 . The heads of the two entity  mentions are marked. Types are listed in decreasing  order of frequency of occurrence in the ACE corpus.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 199, "end_pos": 209, "type": "DATASET", "confidence": 0.8676500022411346}]}, {"text": " Table 2. The lengths  of the bit strings also vary among different words. Bit string  Examples  111011011100  US \u2026  1110110111011  U.S. \u2026  1110110110000  American \u2026  1110110111110110 Cuban, Pakistani, Russian \u2026  11111110010111 Germany, Poland, Greece \u2026  110111110100  businessman, journalist, reporter  1101111101111  president, governor, premier\u2026  1101111101100  senator, soldier, ambassador \u2026  11011101110  spokesman, spokeswoman, \u2026  11001100  people, persons, miners, Haitians  110110111011111 base, compound, camps, camp \u2026  110010111  helicopters, tanks, Marines \u2026", "labels": [], "entities": [{"text": "US \u2026  1110110111011  U.S. \u2026  1110110110000  American \u2026  1110110111110110 Cuban", "start_pos": 111, "end_pos": 189, "type": "DATASET", "confidence": 0.9234232783317566}]}, {"text": " Table 5: Performance comparison on the ACE 2004  data over the 7 relation types.", "labels": [], "entities": [{"text": "ACE 2004  data", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9702580769856771}]}, {"text": " Table 6: The tradeoff between performance and training  time of each method in selecting clusters. PC3 means  using 3 prefixes with the PC method. \u25b3 in this paper  means the difference between a system and the baseline.", "labels": [], "entities": []}, {"text": " Table 7: Performance 12 of the baseline and using  different cluster features with PC4 over the 7 types.", "labels": [], "entities": []}, {"text": " Table 8: Performance over the 7 relation types with different sizes of training data. Prefix10 uses the single prefix  length 10 to generate word clusters as used by Chan and Roth (2010).", "labels": [], "entities": []}, {"text": " Table 9: Performance of each individual relation type based on 5-fold cross-validation.", "labels": [], "entities": []}]}