{"title": [{"text": "Crowdsourcing Translation: Professional Quality from Non-Professionals", "labels": [], "entities": [{"text": "Crowdsourcing Translation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7907071113586426}]}], "abstractContent": [{"text": "Naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised.", "labels": [], "entities": [{"text": "collecting translations", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7256591022014618}]}, {"text": "We demonstrate a variety of mechanisms that increase the translation quality to near professional levels.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9571003913879395}]}, {"text": "Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them.", "labels": [], "entities": []}, {"text": "We propose a set of features that model both the translations and the translators, such as country of residence , LM perplexity of the translation, edit rate from the other translations, and (option-ally) calibration against professional translators.", "labels": [], "entities": [{"text": "LM", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.860114336013794}, {"text": "edit rate", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9466415345668793}]}, {"text": "Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations.", "labels": [], "entities": []}, {"text": "We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators.", "labels": [], "entities": [{"text": "NIST 2009 Urdu-to-English evaluation set", "start_pos": 16, "end_pos": 56, "type": "DATASET", "confidence": 0.9389807224273682}]}, {"text": "The total cost is more than an order of magnitude lower than professional translation.", "labels": [], "entities": [{"text": "professional translation", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6878345906734467}]}], "introductionContent": [{"text": "In natural language processing research, translations are most often used in statistical machine translation (SMT), where systems are trained using bilingual sentence-aligned parallel corpora.", "labels": [], "entities": [{"text": "natural language processing research", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.7039390504360199}, {"text": "statistical machine translation (SMT)", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.7817350476980209}]}, {"text": "SMT owes its existence to data like the Canadian Hansards (which bylaw must be published in both French and English).", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9741386771202087}, {"text": "Canadian Hansards", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.8465465903282166}]}, {"text": "SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like Arabic-English, where there is ample data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9804674386978149}]}, {"text": "However, large bilingual parallel corpora exist for relatively few languages pairs.", "labels": [], "entities": []}, {"text": "There are various options for creating new training resources for new language pairs.", "labels": [], "entities": []}, {"text": "These include harvesting the web for translations or comparable corpora, improving SMT models so that they are better suited to the low resource setting), or designing models that are capable of learning translations from monolingual corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9884781241416931}]}, {"text": "Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive.", "labels": [], "entities": []}, {"text": "For example, estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word.", "labels": [], "entities": [{"text": "TamilEnglish corpus", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.8317035734653473}]}, {"text": "At that rate, translating enough data to build even a small parallel corpus like the LDC's 1.5 million word Urdu-English corpus would exceed half a million dollars.", "labels": [], "entities": [{"text": "LDC's 1.5 million word Urdu-English corpus", "start_pos": 85, "end_pos": 127, "type": "DATASET", "confidence": 0.8059573514120919}]}, {"text": "In this paper we examine the idea of creating low cost translations via crowdscouring.", "labels": [], "entities": []}, {"text": "We use Amazon's Mechanical Turk to hire a large group of nonprofessional translators, and have them recreate an Urdu-English evaluation set at a fraction of the cost of professional translators.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 7, "end_pos": 31, "type": "DATASET", "confidence": 0.9394776821136475}]}, {"text": "The original dataset already has professionally-produced reference translations, which allows us to objectively and quantitatively compare the quality of professional and nonprofessional translations.", "labels": [], "entities": []}, {"text": "Although many of the individual non-expert translators produce low-quality, disfluent translations, we show that it is possible toSigns of human livings have been found in many caves in Attapure.", "labels": [], "entities": [{"text": "Attapure", "start_pos": 186, "end_pos": 194, "type": "DATASET", "confidence": 0.9822378754615784}]}, {"text": "In 1994, the remains of pre-historic man, which are believed to be 800,000 years old were discovered and they were named`Homenamed`Home Antecessor' meaning`Themeaning`The Founding Man'.", "labels": [], "entities": []}, {"text": "Prior to that 6 lac years old humans, named as Homogenisens in scientific terms,were believed to be the oldest dwellers of this area.", "labels": [], "entities": []}, {"text": "Archaeological experts say that evidence is found that proves that the inhabitants of this area used molded tools.", "labels": [], "entities": []}, {"text": "The ground where these digs took place has been claimed to be the oldest known European discovery of civilization, as announced by the French News Agency.", "labels": [], "entities": [{"text": "French News Agency", "start_pos": 135, "end_pos": 153, "type": "DATASET", "confidence": 0.9277550578117371}]}, {"text": "!\"#$\"% &' ()*\"+*, &-,./%, 0#1 234 5, 0#1 1994 67\"89: ;2< &=\"> &*\"1 &*,?@ A\"B C'D 8 E \"F G8?H= )> 'I\"+*, &*\"%' &JK8 ?+#B &LJ8, )1)< 0#MJ> 0#NO &' P\"#O \"8: Q\"* \"' &+J-\"B 0#MJ> I\"+*, 2*,?@ C'D 6 RG$ 2B 5, 5, ;2< \"=\"> \"M' S+J#>?GTU#< )1)< 0#1 VW3X, P2Y= 2=\"> 2*\"1 &Z-\"<9 [8?= \\8.$ 2' 234 2+8, 0#M*, ]' 2< \"JM' \"' [8?<\"1 2' ]^8.$ _ 9\"`a 2' 234 5, ]' 2< \"/bc ]/@ 2B [> 0#< 2b1 .<,)d P2Y= 2=?'", "labels": [], "entities": []}, {"text": "A\"^K/B, &Y% 9,ef, 2-)< 2#' &-Wgh i)T Signs of human life of ancient people have been discovered in several caves of Atapuerca.", "labels": [], "entities": []}, {"text": "In 1994, several homo antecessor fossils i.e. pioneer human were uncovered in this region, which are supposed to be 800,000 years old.", "labels": [], "entities": []}, {"text": "Previously, 600,000 years old ancestors, called homo hudlabar in scientific term, were supposed to be the most ancient inhabitants of the region.Archeologists are of the view that they have gathered evidence that the people of this region had also been using fabricated tools.", "labels": [], "entities": []}, {"text": "On the basis of the level at which this excavation was carried out, the French news agency has termed it the oldest European discovery.", "labels": [], "entities": [{"text": "French news agency", "start_pos": 72, "end_pos": 90, "type": "DATASET", "confidence": 0.8628549774487814}]}], "datasetContent": [{"text": "We translated the Urdu side of the Urdu-English test set of the 2009 NIST MT Evaluation Workshop.", "labels": [], "entities": [{"text": "Urdu side of the Urdu-English test set of the 2009 NIST MT Evaluation Workshop", "start_pos": 18, "end_pos": 96, "type": "DATASET", "confidence": 0.7863790052277702}]}, {"text": "The set consists of 1,792 Urdu sentences from a variety of news and online sources.", "labels": [], "entities": []}, {"text": "The set includes four different reference translations for each source sentence, produced by professional translation agencies.", "labels": [], "entities": []}, {"text": "NIST contracted the LDC to oversee the translation process and perform quality control.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9694154858589172}, {"text": "translation process", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.9124862253665924}]}, {"text": "This particular dataset, with its multiple reference translations, is very useful because we can measure the quality range for professional translators, which gives us an idea of whether or not the crowdsourced translations approach the quality of a professional translator.", "labels": [], "entities": []}, {"text": "To measure the quality of the translations, we make use of the existing professional translations.", "labels": [], "entities": []}, {"text": "Since we have four professional translation sets, we can calculate the BLEU score () for one professional translator P 1 using the other three P 2,3,4 as a reference set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9877620935440063}]}, {"text": "We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation.", "labels": [], "entities": []}, {"text": "We can see how a translation set T (chosen by our model) compares to this range by calculating T 's BLEU scores against the same four sets of three reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9950830936431885}]}, {"text": "We will evaluate different strategies for selecting such a set T , and see how much each improves on the BLEU score, compared to randomly picking from among the Turker translations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9742442667484283}]}, {"text": "We also evaluate Turker translation quality by using them as reference sets to score various submissions to the NIST MT evaluation.", "labels": [], "entities": [{"text": "Turker translation", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8248357474803925}, {"text": "NIST MT evaluation", "start_pos": 112, "end_pos": 130, "type": "DATASET", "confidence": 0.7262053291002909}]}, {"text": "Specifically, we measure the correlation (using Pearson's r) between BLEU scores of MT systems measured against nonprofessional translations, and BLEU scores measured against professional translations.", "labels": [], "entities": [{"text": "Pearson's r)", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.7673902437090874}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9960376024246216}, {"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9497914910316467}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.99873286485672}]}, {"text": "Since the main purpose of the NIST dataset was to compare MT systems against each other, this is a more direct fitness-for-task measure.", "labels": [], "entities": [{"text": "NIST dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9645536839962006}, {"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9779256582260132}]}, {"text": "We chose the middle 6 systems (in terms of performance) submitted to the NIST evaluation, out of 12, as those systems were fairly close to each other, with less than 2 BLEU points separating them.", "labels": [], "entities": [{"text": "NIST evaluation", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9497730731964111}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9990559220314026}]}, {"text": "We establish the performance of professional translators, calculate oracle upper bounds on Turker translation quality, and carryout a set of experiments that demonstrate the effectiveness of our model and that determine which features are most helpful.", "labels": [], "entities": [{"text": "Turker translation", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.5991717875003815}]}, {"text": "Each number reported in this section is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets.", "labels": [], "entities": []}, {"text": "Furthermore, each of those 4 numbers is itself based on a five-fold cross validation, where 80% of the data is used to compute feature values, and 20% used for evaluation.", "labels": [], "entities": []}, {"text": "The 80% portion is used to compute the aggregate worker-level features.", "labels": [], "entities": []}, {"text": "For the worker calibration feature, we utilize the references for 10% of the data (which is within the 80% portion).", "labels": [], "entities": []}], "tableCaptions": []}