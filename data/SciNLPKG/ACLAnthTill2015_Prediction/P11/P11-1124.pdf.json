{"title": [{"text": "Consistent Translation using Discriminative Learning: A Translation Memory-inspired Approach *", "labels": [], "entities": [{"text": "Consistent Translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8684347569942474}]}], "abstractContent": [{"text": "We present a discriminative learning method to improve the consistency of translations in phrase-based Statistical Machine Translation (SMT) systems.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 103, "end_pos": 140, "type": "TASK", "confidence": 0.7796857208013535}]}, {"text": "Our method is inspired by Translation Memory (TM) systems which are widely used by human translators in industrial settings.", "labels": [], "entities": [{"text": "Translation Memory (TM)", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.9176151990890503}]}, {"text": "We constrain the translation of an input sentence using the most similar 'transla-tion example' retrieved from the TM.", "labels": [], "entities": []}, {"text": "Differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance.", "labels": [], "entities": []}, {"text": "We observe that using this method can benefit the SMT system by not only producing consistent translations, but also improved translation outputs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9938161373138428}]}, {"text": "We report a 0.9 point improvement in terms of BLEU score on English-Chinese technical documents.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9794919788837433}]}], "introductionContent": [{"text": "Translation consistency is an important factor for large-scale translation, especially for domainspecific translations in an industrial environment.", "labels": [], "entities": [{"text": "Translation consistency", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7401272356510162}]}, {"text": "For example, in the translation of technical documents, lexical as well as structural consistency is essential to produce a fluent target-language sentence.", "labels": [], "entities": [{"text": "translation of technical documents", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.8366961926221848}]}, {"text": "Moreover, even in the case of translation errors, consistency in the errors (e.g. repetitive error patterns) are easier to diagnose and subsequently correct by translators.", "labels": [], "entities": [{"text": "translation errors", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9224039614200592}, {"text": "consistency", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9752786755561829}]}, {"text": "In phrase-based SMT, translation models and language models are automatically learned and/or generalised from the training data, and a translation is produced by maximising a weighted combination of these models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.6796540021896362}]}, {"text": "Given that global contextual information is not normally incorporated, and that training data is usually noisy in nature, there is no guarantee that an SMT system can produce translations in a consistent manner.", "labels": [], "entities": [{"text": "SMT", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9887334108352661}]}, {"text": "On the other hand, TM systems -widely used by translators in industrial environments for enterprise localisation by translators -can shed some light on mitigating this limitation.", "labels": [], "entities": []}, {"text": "TM systems can assist translators by retrieving and displaying previously translated similar 'example' sentences (displayed as source-target pairs, widely called 'fuzzy matches' in the localisation industry).", "labels": [], "entities": []}, {"text": "In TM systems, fuzzy matches are retrieved by calculating the similarity or the so-called 'fuzzy match score' (ranging from 0 to 1 with 0 indicating no matches and 1 indicating a full match) between the input sentence and sentences in the source side of the translation memory.", "labels": [], "entities": [{"text": "similarity", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9651281237602234}]}, {"text": "When presented with fuzzy matches, translators can then avail of useful chunks in previous translations while composing the translation of anew sentence.", "labels": [], "entities": []}, {"text": "Most translators only consider a few sentences that are most similar to the current input sentence; this process can inherently improve the consistency of translation, given that the new translations produced by translators are likely to be similar to the target side of the fuzzy match they have consulted.", "labels": [], "entities": [{"text": "consistency", "start_pos": 140, "end_pos": 151, "type": "METRIC", "confidence": 0.9778803586959839}]}, {"text": "Previous research as discussed in detail in Sec-tion 2 has focused on using fuzzy match score as a threshold when using the target side of the fuzzy matches to constrain the translation of the input sentence.", "labels": [], "entities": [{"text": "translation of the input sentence", "start_pos": 174, "end_pos": 207, "type": "TASK", "confidence": 0.8093296647071838}]}, {"text": "In our approach, we use a more finegrained discriminative learning method to determine whether the target side of the fuzzy matches should be used as a constraint in translating the input sentence.", "labels": [], "entities": [{"text": "translating the input sentence", "start_pos": 166, "end_pos": 196, "type": "TASK", "confidence": 0.8210513442754745}]}, {"text": "We demonstrate that our method can consistently improve translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9732747077941895}]}, {"text": "The rest of the paper is organized as follows: we begin by briefly introducing related research in Section 2.", "labels": [], "entities": []}, {"text": "We present our discriminative learning method for consistent translation in Section 3 and our feature design in Section 4.", "labels": [], "entities": [{"text": "consistent translation", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.5509399473667145}]}, {"text": "We report the experimental results in Section 5 and conclude the paper and point out avenues for future research in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our data set is an English-Chinese translation memory with technical translation from Symantec, consisting of 87K sentence pairs.", "labels": [], "entities": []}, {"text": "The average sentence length of the English training set is 13.3 words and the size of the training set is comparable to the larger TMs used in the industry.", "labels": [], "entities": []}, {"text": "Detailed corpus statistics about the training, development and test sets for the SMT system are shown in.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9943062663078308}]}, {"text": "The composition of test subsets based on fuzzy match scores is shown in.", "labels": [], "entities": []}, {"text": "We can see that sentences in the test sets are longer than those in the training data, implying a relatively difficult translation task.", "labels": [], "entities": []}, {"text": "We train the SVM classifier using the libSVM) toolkit.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.572058230638504}]}, {"text": "The SVM-  training and validation is on the same training sentences 1 as the SMT system with 5-fold cross validation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9703727960586548}]}, {"text": "The SVM hyper-parameters are tuned using the training data of the first fold in the 5-fold cross validation via a brute force grid search.", "labels": [], "entities": []}, {"text": "More specifically, for parameter C in (1), we search in the range [2 \u22125 , 2 15 ], while for parameter \u03b3 (2) we search in the range [2 \u221215 , 2 3 ].", "labels": [], "entities": []}, {"text": "The step size is 2 on the exponent.", "labels": [], "entities": []}, {"text": "We conducted experiments using a standard loglinear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (, the refinement and phrase-extraction heuristics described in (, minimum-errorrate training, a 5-gram language model with Kneser-Ney smoothing trained with SRILM) on the Chinese side of the training data, and Moses ( which is capable of handling user-specified translations for some portions of the input during decoding.", "labels": [], "entities": [{"text": "IBM word alignment", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.5859854618708292}]}, {"text": "The maximum phrase length is set to 7.", "labels": [], "entities": []}, {"text": "The performance of the phrase-based SMT system is measured by BLEU score () and TER ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8547638654708862}, {"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9888128936290741}, {"text": "TER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9980217218399048}]}, {"text": "Significance test-ing is carried out using approximate randomisation) with a 95% confidence level.", "labels": [], "entities": []}, {"text": "We also measure the quality of the classification by precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.999686598777771}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9990687966346741}]}, {"text": "Let Abe the set of predicted markup input sentences, and B be the set of input sentences where the markup version has a lower TER score than the plain version.", "labels": [], "entities": [{"text": "TER score", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9861984252929688}]}, {"text": "We standardly define precision P and recall R as in:", "labels": [], "entities": [{"text": "precision P", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9441657364368439}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9953799247741699}, {"text": "R", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.5168555378913879}]}], "tableCaptions": [{"text": " Table 2: Composition of test subsets based on fuzzy  match scores", "labels": [], "entities": []}, {"text": " Table 3: Performance of Discriminative Learning (%)", "labels": [], "entities": []}, {"text": " Table 4: The impact of classification confidence thresholding", "labels": [], "entities": [{"text": "classification confidence thresholding", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.8692909677823385}]}, {"text": " Table 5. From this ta-", "labels": [], "entities": []}, {"text": " Table 5: Performance using fuzzy match score for classi- fication", "labels": [], "entities": []}, {"text": " Table 6: Percentage of training sentences with markup  vs without markup grouped by fuzzy match (FM) score  ranges", "labels": [], "entities": [{"text": "fuzzy match (FM) score", "start_pos": 85, "end_pos": 107, "type": "METRIC", "confidence": 0.9115022023518881}]}, {"text": " Table 7: Contribution of Features (%)", "labels": [], "entities": []}]}