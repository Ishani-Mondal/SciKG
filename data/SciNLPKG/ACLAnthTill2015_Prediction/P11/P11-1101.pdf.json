{"title": [{"text": "Underspecifying and Predicting Voice for Surface Realisation Ranking", "labels": [], "entities": [{"text": "Surface Realisation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7408421039581299}]}], "abstractContent": [{"text": "This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German.", "labels": [], "entities": []}, {"text": "We investigate the relationship between the surface realisa-tion performance and the character of the input to generation, i.e. its degree of underspec-ification.", "labels": [], "entities": []}, {"text": "We extend a syntactic surface reali-sation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants.", "labels": [], "entities": []}, {"text": "This allows us to study the interaction of voice and word order alternations in realistic Ger-man corpus data.", "labels": [], "entities": [{"text": "Ger-man corpus data", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.7858302593231201}]}, {"text": "We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9984902143478394}]}], "introductionContent": [{"text": "This paper 1 presents work on modelling the usage of voice and word order alternations in a free word order language.", "labels": [], "entities": []}, {"text": "Given a set of meaning-equivalent candidate sentences, such as in the simplified English Example (1), our model makes predictions about which candidate sentence is most appropriate or natural given the context. a. It wasn't until June that the Parliament approved it. b. It wasn't until June that it was approved by the Parliament. c. It wasn't until June that it was approved.", "labels": [], "entities": []}, {"text": "We address the problem of predicting the usage of linguistic alternations in the framework of a surface realisation ranking system.", "labels": [], "entities": []}, {"text": "Such ranking systems are practically relevant for the real-world application of grammar-based generators that usually generate several grammatical surface sentences from a given abstract input, e.g. ().", "labels": [], "entities": []}, {"text": "Moreover, this framework allows for detailed experimental studies of the interaction of specific linguistic features.", "labels": [], "entities": []}, {"text": "Thus it has been demonstrated that for free word order languages like German, word order prediction quality can be improved with carefully designed, linguistically informed models capturing information-structural strategies.", "labels": [], "entities": [{"text": "word order prediction", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.7009705305099487}]}, {"text": "This paper is situated in the same framework, using rich linguistic representations over corpus data for machine learning of realisation ranking.", "labels": [], "entities": []}, {"text": "However, we go beyond the task of finding the correct ordering for an almost fixed set of word forms.", "labels": [], "entities": []}, {"text": "Quite obviously, word order is only one of the means at a speaker's disposal for expressing some content in a contextually appropriate form; we add systematic alternations like the voice alternation (active vs. passive) to the picture.", "labels": [], "entities": []}, {"text": "As an alternative way of promoting or demoting the prominence of a syntactic argument, its interaction with word ordering strategies in real corpus data is of high theoretical interest).", "labels": [], "entities": [{"text": "word ordering", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.7043853253126144}]}, {"text": "Our main goals are (i) to establish a corpus-based surface realisation framework for empirically investigating interactions of voice and word order in German, (ii) to design an input representation for generation capturing voice alternations in a variety of contexts, (iii) to better understand the relationship between the performance of a generation ranking model and the type of realisation candidates available in its input.", "labels": [], "entities": []}, {"text": "In working towards these goals, this paper addresses the question of evaluation.", "labels": [], "entities": []}, {"text": "We conduct a pilot human evaluation on the voice al-ternation data and relate our findings to our results established in the automatic ranking experiments.", "labels": [], "entities": []}, {"text": "Addressing interactions among a range of grammatical and discourse phenomena on realistic corpus data turns out to be a major methodological challenge for data-driven surface realisation.", "labels": [], "entities": [{"text": "data-driven surface realisation", "start_pos": 155, "end_pos": 186, "type": "TASK", "confidence": 0.7065005302429199}]}, {"text": "The set of candidate realisations available for ranking will influence the findings, and here, existing surface realisers vary considerably.", "labels": [], "entities": []}, {"text": "point out the differences across approaches in the type of syntactic and semantic information present and absent in the input representation; and it is the type of underspecification that determines the number (and character) of available candidate realisations and, hence, the complexity of the realisation task.", "labels": [], "entities": []}, {"text": "We study the effect of varying degrees of underspecification explicitly, extending a syntactic generation system by a semantic component capturing voice alternations.", "labels": [], "entities": []}, {"text": "In regeneration studies involving underspecified underlying representations, corpusoriented work reveals an additional methodological challenge.", "labels": [], "entities": []}, {"text": "When using standard semantic representations, as common in broad-coverage work in semantic parsing (i.e., from the point of view of analysis), alternative variants for sentence realisation will often receive slightly different representations: In the context of (1), the continuation (1-c) is presumably more natural than (1-b), but with a standard sentence-bounded semantic analysis, only (1-a) and (1-b) would receive equivalent representations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7534875273704529}, {"text": "sentence realisation", "start_pos": 168, "end_pos": 188, "type": "TASK", "confidence": 0.7276198863983154}]}, {"text": "Rather than waiting for the availability of robust and reliable techniques for detecting the reference of implicit arguments in analysis (or for contextually aware reasoning components), we adopt a relatively simple heuristic approach (see Section 3.1) that approximates the desired equivalences by augmented representations for examples like.", "labels": [], "entities": []}, {"text": "This way we can overcome an extremely skewed distribution in the naturally occurring meaning-equivalent active vs. passive sentences, a factor which we believe justifies taking the risk of occasional overgeneration.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: Section 2 situates our methodology with respect to other work on surface realisation and briefly summarises the relevant theoretical linguistic background.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.71912482380867}]}, {"text": "In Section 3, we present our generation architecture and the design of the input representation.", "labels": [], "entities": []}, {"text": "Section 4 describes the setup for the experiments in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6, we present the results from the human evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three best sentences rather than only the top-ranked sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9993938207626343}]}, {"text": "This means that the model sometimes predicts almost equal naturalness for different voice realisations.", "labels": [], "entities": []}, {"text": "Moreover, in the case of word order, we know from previous evaluation studies, that humans sometimes prefer different realisations than the original corpus sentences.", "labels": [], "entities": [{"text": "word order", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.7072103917598724}]}, {"text": "This Section investigates agreement inhuman judgements of voice realisation.", "labels": [], "entities": [{"text": "voice realisation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7570124268531799}]}, {"text": "Whereas previous studies in generation mainly used human evaluation to compare different systems, or to correlate human and automatic evaluations, our primary interest is the agreement or correlation between human rankings.", "labels": [], "entities": []}, {"text": "In particular, we explore the hypothesis that this agreement is higher in certain contexts than in others.", "labels": [], "entities": []}, {"text": "In order to select these contexts, we use the predictions made by our ranking model.", "labels": [], "entities": []}, {"text": "The questionnaire for our experiment comprised 24 items falling into 3 classes: a) items where the 3 best sentences predicted by the model have the same voice as the original sentence (\"Correct\"), b) items where the 3 top-ranked sentences realise different voices (\"Mixed\"), c) items where the model predicted the incorrect voice in all 3 top sentences (\"False\").", "labels": [], "entities": [{"text": "False", "start_pos": 355, "end_pos": 360, "type": "METRIC", "confidence": 0.9763094782829285}]}, {"text": "Each item is composed of the original sentence, the 3 top-ranked sentences (if not identical to the corpus sentence) and 2 further sentences such that each item contains different voices.", "labels": [], "entities": []}, {"text": "For each item, we presented the previous context sentence.", "labels": [], "entities": []}, {"text": "The experiment was completed by 8 participants, all native speakers of German, 5 had a linguistic background.", "labels": [], "entities": []}, {"text": "The participants were asked to rank each sentence on a scale from 1-6 according to its naturalness and plausibility in the given context.", "labels": [], "entities": []}, {"text": "The participants were explicitly allowed to use the same rank for sentences they find equally natural.", "labels": [], "entities": []}, {"text": "The participants made heavy use of this option: out of the 192 annotated items, only 8 are ranked such that no two sentences have the same rank.", "labels": [], "entities": []}, {"text": "We compare the human judgements by correlat-ing them with Spearman's \u03c1.", "labels": [], "entities": []}, {"text": "This measure is considered appropriate for graded annotation tasks in general, and has also been used for analysing human realisation rankings.", "labels": [], "entities": []}, {"text": "We normalise the ranks according to the procedure in.", "labels": [], "entities": []}, {"text": "In, we report the correlations obtained from averaging overall pairwise correlations between the participants and the correlations restricted to the item and sentence classes.", "labels": [], "entities": []}, {"text": "We used bootstrap re-sampling on the pairwise correlations to test that the correlations on the different item classes significantly differ from each other.", "labels": [], "entities": []}, {"text": "The correlations in suggest that the agreement between annotators is highest on the false items, and lowest on the mixed items.", "labels": [], "entities": [{"text": "agreement", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9758182764053345}]}, {"text": "Humans tended to give the best rank to the original sentence more often on the false items (91%) than on the others.", "labels": [], "entities": []}, {"text": "Moreover, the agreement is generally higher on the sentences realising the correct voice.", "labels": [], "entities": [{"text": "agreement", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9951727986335754}]}, {"text": "These results seem to confirm our hypothesis that the general level of agreement between humans differs depending on the context.", "labels": [], "entities": []}, {"text": "However, one has to be careful in relating the effects in our data solely to voice preferences.", "labels": [], "entities": []}, {"text": "Since the sentences were chosen automatically, some examples contain very unnatural word orders that probably guided the annotators' decisions more than the voice.", "labels": [], "entities": []}, {"text": "This is illustrated by Example (6) showing two passive sentences from our questionnaire which differ only in the position of the adverb besser \"better\".", "labels": [], "entities": []}, {"text": "Sentence (6-a) is completely implausible fora native speaker of German, whereas Sentence (6-b) sounds very natural.", "labels": [], "entities": []}, {"text": "This observation brings us back to our initial point that the surface realisation task is especially challenging due to the interaction of a range of semantic and discourse phenomena.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7280678898096085}]}, {"text": "Obviously, this interaction makes it difficult to single out preferences fora specific alternation type.", "labels": [], "entities": []}, {"text": "Future work will have to establish how this problem should be dealt within Items All Correct Mixed False \"All\" sent.", "labels": [], "entities": []}, {"text": "0.58 0.6 0.54 0.62 \"Correct\" sent.", "labels": [], "entities": [{"text": "Correct", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9938604831695557}]}, {"text": "0.64 0.63 0.56 0.72 \"False\" sent.", "labels": [], "entities": [{"text": "False", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9860888123512268}]}, {"text": "0.47 0.57 0.48 0.44 Top-ranked corpus sent.", "labels": [], "entities": []}, {"text": "84% 78% 83% 91%: Human Evaluation the design of human evaluation experiments.", "labels": [], "entities": [{"text": "Human Evaluation", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.6794758290052414}]}], "tableCaptions": [{"text": " Table 2: Evaluation of Experiment 1", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of Voice Prediction by Ling. Model in  Experiment 1", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9745966792106628}]}, {"text": " Table 4: Evaluation of Experiment 2", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9508841633796692}]}, {"text": " Table 5: Evaluation of Experiment 3", "labels": [], "entities": []}]}