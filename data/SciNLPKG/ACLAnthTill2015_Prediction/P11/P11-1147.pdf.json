{"title": [{"text": "Unsupervised Discovery of Domain-Specific Knowledge from Text", "labels": [], "entities": [{"text": "Unsupervised Discovery of Domain-Specific Knowledge from Text", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8109911978244781}]}], "abstractContent": [{"text": "Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input.", "labels": [], "entities": [{"text": "Learning by Reading (LbR)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6226540853579839}]}, {"text": "This requires knowledge about the domain structure (such as entities , classes, and actions) in order to do inference.", "labels": [], "entities": []}, {"text": "We present a method to infer this implicit knowledge from unlabeled text.", "labels": [], "entities": []}, {"text": "Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling.", "labels": [], "entities": []}, {"text": "From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicate-argument structures like \"quarterbacks throw passes to receivers\".", "labels": [], "entities": []}, {"text": "Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches.", "labels": [], "entities": []}, {"text": "Human subjects judged up to 96.6% of the resulting propositions to be sensible.", "labels": [], "entities": []}, {"text": "The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of Learning by Reading (LbR) is to enable a computer to learn about anew domain and then to reason about it in order to perform such tasks as question answering, threat assessment, and explanation (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.7961889803409576}, {"text": "threat assessment", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.8331882655620575}]}, {"text": "This requires joint efforts from Information Extraction, Knowledge Representation, and logical inference.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7705041170120239}, {"text": "Knowledge Representation", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.7772776484489441}]}, {"text": "All these steps depend on the system having access to basic, often unstated, foundational knowledge about the domain.", "labels": [], "entities": []}, {"text": "Most documents, however, do not explicitly mention this information in the text, but assume basic background knowledge about the domain, such as positions (\"quarterback\"), titles (\"winner\"), or actions (\"throw\") for sports game reports.", "labels": [], "entities": []}, {"text": "Without this knowledge, the text will not make sense to the reader, despite being well-formed English.", "labels": [], "entities": []}, {"text": "Luckily, the information is often implicitly contained in the document or can be inferred from similar texts.", "labels": [], "entities": []}, {"text": "Our system automatically acquires domainspecific knowledge (classes and actions) from large amounts of unlabeled data, and trains a probabilistic model to determine and apply the most informative classes (quarterback, etc.) at appropriate levels of generality for unseen data.", "labels": [], "entities": []}, {"text": "E.g., from sentences such as \"Steve Young threw a pass to Michael Holt\", \"Quarterback Steve Young finished strong\", and \"Michael Holt, the receiver, left early\" we can learn the classes quarterback and receiver, and the proposition \"quarterbacks throw passes to receivers\".", "labels": [], "entities": []}, {"text": "We will thus assume that the implicit knowledge comes in two forms: actions in the form of predicate-argument structures, and classes as part of the source data.", "labels": [], "entities": []}, {"text": "Our task is to identify and extract both.", "labels": [], "entities": []}, {"text": "Since LbR systems must quickly adapt and scale well to new domains, we need to be able to work with large amounts of data and minimal supervision.", "labels": [], "entities": []}, {"text": "Our approach produces simple propositions about the domain (see for examples of actual propositions learned by our system).", "labels": [], "entities": []}, {"text": "American football was the first official evaluation domain in the DARPA-sponsored Machine Reading program, and provides the background fora number of LbR systems.", "labels": [], "entities": [{"text": "DARPA-sponsored Machine Reading", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6233541667461395}]}, {"text": "Sports is particularly amenable, since it usually follows a finite, explicit set of rules.", "labels": [], "entities": []}, {"text": "Due to its popularity, results are easy to evaluate with lay subjects, and game reports, databases, etc.", "labels": [], "entities": []}, {"text": "provide a large amount of data.", "labels": [], "entities": []}, {"text": "The same need for basic knowledge appears in all domains, though.", "labels": [], "entities": []}, {"text": "In music, musicians play instruments, in electronics, components constitute circuits, circuits use electricity, etc.", "labels": [], "entities": []}, {"text": "Our approach differs from verb-argument identification or Named Entity (NE) tagging in several respects.", "labels": [], "entities": [{"text": "verb-argument identification", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7218878269195557}, {"text": "Named Entity (NE) tagging", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6486799071232477}]}, {"text": "While previous work on verb-argument selection () uses fixed sets of classes, we cannot know a priori how many and which classes we will encounter.", "labels": [], "entities": [{"text": "verb-argument selection", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7043385654687881}]}, {"text": "We therefore provide away to derive the appropriate classes automatically and include a probability distribution for each of them.", "labels": [], "entities": []}, {"text": "Our approach is thus less restricted and can learn context-dependent, finegrained, domain-specific propositions.", "labels": [], "entities": []}, {"text": "While a NEtagged corpus could produce a general proposition like \"PERSON throws to PERSON\", our method enables us to distinguish the arguments and learn \"quarterback throws to receiver\" for American football and \"outfielder throws to third base\" for baseball.", "labels": [], "entities": []}, {"text": "While in NE tagging each word has only one correct tag in a given context, we have hierarchical classes: an entity can be correctly labeled as a player or a quarterback (and possibly many more classes), depending on the context.", "labels": [], "entities": [{"text": "NE tagging", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.9473473429679871}]}, {"text": "By taking context into account, we are also able to label each sentence individually and account for unseen entities without using external resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "We want to evaluate how well our model predicts the data, and how sensible the resulting propositions are.", "labels": [], "entities": []}, {"text": "We define a good model as one that generalizes well and produces semantically useful propositions.", "labels": [], "entities": []}, {"text": "First, since we derive the classes in a data-driven way, we have no gold standard data available for comparison.", "labels": [], "entities": []}, {"text": "Second, there is no accepted evaluation measure for this kind of task.", "labels": [], "entities": []}, {"text": "Ultimately, we would like to evaluate our model externally, such as measuring its impact on performance of a LbR system.", "labels": [], "entities": []}, {"text": "In the absence thereof, we resort to several complementary measures, as well as performing an annotation task.", "labels": [], "entities": []}, {"text": "We derive evaluation criteria as follows.", "labels": [], "entities": []}, {"text": "A model generalizes well if it can cover ('explain') all the sentences in the corpus with a few propositions.", "labels": [], "entities": []}, {"text": "This requires a measure of generality.", "labels": [], "entities": []}, {"text": "However, while a proposition such as \"PERSON does THING\", has excellent generality, it possesses no discriminating power.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9523677825927734}, {"text": "THING", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9200626611709595}]}, {"text": "We also need the propositions to partition the sentences into clusters of semantic similarity, to support effective inference.", "labels": [], "entities": []}, {"text": "This requires a measure of distribution.", "labels": [], "entities": []}, {"text": "Maximal distribution, achieved by assigning every sentence to a different proposition, however, is not useful either.", "labels": [], "entities": [{"text": "Maximal distribution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7317851483821869}]}, {"text": "We need to find an appropriate level of generality within which the sentences are clustered into propositions for the best overall groupings to support inference.", "labels": [], "entities": []}, {"text": "To assess the learned model, we apply the measures of generalization, entropy, and perplexity (see Sections 3.2, 3.3, and 3.4).", "labels": [], "entities": []}, {"text": "These measures can be used to compare different systems.", "labels": [], "entities": []}, {"text": "We do not attempt to weight or combine the different measures, but present each in its own right.", "labels": [], "entities": []}, {"text": "Further, to assess label accuracy, we use Amazon's Mechanical Turk annotators to judge the sensibility of the propositions produced by each system (Section 3.5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9696429371833801}, {"text": "Amazon's Mechanical Turk annotators", "start_pos": 42, "end_pos": 77, "type": "DATASET", "confidence": 0.9303117394447327}]}, {"text": "We reason that if our system learned to infer the correct classes, then the resulting propositions should constitute true, general statements about that domain, and thus be judged as sensible.", "labels": [], "entities": []}, {"text": "This approach allows the effective annotation of sufficient amounts of data for an evaluation (first described for NLP in).", "labels": [], "entities": []}, {"text": "With the trained model, we use Viterbi decoding to extract the best class sequence for each example in the data.", "labels": [], "entities": []}, {"text": "This translates the original corpus sentences into propositions.", "labels": [], "entities": []}, {"text": "See steps 2 and 3 in.", "labels": [], "entities": []}, {"text": "We create two baseline systems from the same corpus, one which uses the most frequent class (MFC) for each entity, and another one which uses a class picked at random from the applicable classes of each entity.", "labels": [], "entities": []}, {"text": "Ultimately, we are interested in labeling unseen data from the same domain with the correct class, so we evaluate separately on the full corpus and the subset of sentences that contain unknown entities (i.e., entities for which no class information was available in the corpus, cf. Section 2.2).", "labels": [], "entities": []}, {"text": "For the latter case, we select all examples containing at least one unknown entity (labeled UNK), resulting in a subset of 41, 897 sentences, and repeat the evaluation steps described above.", "labels": [], "entities": []}, {"text": "Here, we have to consider a much larger set of possible classes per entity (the 20 overall most frequent classes).", "labels": [], "entities": []}, {"text": "The MFC baseline for these cases is the most frequent of the 20 classes for UNK tokens, while the random baseline chooses randomly from that set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage of propositions derived from labeling the full data set that were judged sensible", "labels": [], "entities": []}, {"text": " Table 2: Percentage of propositions derived from labeling unknown entities that were judged sensible", "labels": [], "entities": []}]}