{"title": [{"text": "Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel computational formulation of speaker authority in discourse.", "labels": [], "entities": []}, {"text": "This notion, which focuses on how speakers position themselves relative to each other in discourse , is first developed into a reliable coding scheme (0.71 agreement between human annotators).", "labels": [], "entities": []}, {"text": "We also provide a computational model for automatically annotating text using this coding scheme, using supervised learning enhanced by constraints implemented with Integer Linear Programming.", "labels": [], "entities": []}, {"text": "We show that this constrained model's analyses of speaker authority correlates very strongly with expert human judgments (r 2 coefficient of 0.947).", "labels": [], "entities": [{"text": "r 2 coefficient", "start_pos": 122, "end_pos": 137, "type": "METRIC", "confidence": 0.9517914851506551}]}], "introductionContent": [{"text": "In this work, we seek to formalize the ways speakers position themselves in discourse.", "labels": [], "entities": []}, {"text": "We do this in away that maintains a notion of discourse structure, and which can be aggregated to evaluate a speaker's overall stance in a dialogue.", "labels": [], "entities": []}, {"text": "We define the body of work in positioning to include any attempt to formalize the processes by which speakers attempt to influence or give evidence of their relations to each other.", "labels": [], "entities": []}, {"text": "Constructs such as Initiative and Control, which attempt to operationalize the authority over a discourse's structure, fall under the umbrella of positioning.", "labels": [], "entities": []}, {"text": "As we construe positioning, it also includes work on detecting certainty and confusion in speech (), which models a speaker's understanding of the information in their statements.", "labels": [], "entities": [{"text": "certainty", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.8411257266998291}]}, {"text": "Work in dialogue act tagging is also relevant, as it seeks to describe the actions and moves with which speakers display these types of positioning ().", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.6470464368661245}]}, {"text": "To complement these bodies of work, we choose to focus on the question of how speakers position themselves as authoritative in a discourse.", "labels": [], "entities": []}, {"text": "This means that we must describe the way speakers introduce new topics or discussions into the discourse; the way they position themselves relative to that topic; and how these functions interact with each other.", "labels": [], "entities": []}, {"text": "While all of the tasks mentioned above focus on specific problems in the larger rhetorical question of speaker positioning, none explicitly address this framing of authority.", "labels": [], "entities": [{"text": "speaker positioning", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.6790413707494736}]}, {"text": "Each does have valuable ties to the work that we would like to do, and in section 2, we describe prior work in each of those areas, and elaborate on how each relates to our questions.", "labels": [], "entities": []}, {"text": "We measure this as an authoritativeness ratio.", "labels": [], "entities": []}, {"text": "Of the contentful dialogue moves made by a speaker, in what fraction of those moves is the speaker positioned as the primary authority on that topic?", "labels": [], "entities": []}, {"text": "To measure this quantitatively, we introduce the Negotiation framework, a construct from the field of systemic functional linguistics (SFL), which addresses specifically the concepts that we are interested in.", "labels": [], "entities": [{"text": "systemic functional linguistics (SFL)", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.7954444090525309}]}, {"text": "We present a reproducible formulation of this sociolinguistics research in section 3, along with our preliminary findings on reliability between human coders, where we observe inter-rater agreement of 0.71.", "labels": [], "entities": [{"text": "reliability", "start_pos": 125, "end_pos": 136, "type": "METRIC", "confidence": 0.9695674180984497}, {"text": "agreement", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.49710357189178467}]}, {"text": "Applying this coding scheme to data, we see strong correlations with important motivational constructs such as Self-Efficacy () as well as learning gains.", "labels": [], "entities": []}, {"text": "Next, we address automatic coding of the Negotiation framework, which we treat as a two-1018 dimensional classification task.", "labels": [], "entities": []}, {"text": "One dimension is a set of codes describing the authoritative status of a contribution . The other dimension is a segmentation task.", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8917516469955444}]}, {"text": "We impose constraints on both of these models based on the structure observed in the work of SFL.", "labels": [], "entities": [{"text": "SFL", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.639843225479126}]}, {"text": "These constraints are formulated as boolean statements describing what a correct label sequence looks like, and are imposed on our model using an Integer Linear Programming formulation ().", "labels": [], "entities": []}, {"text": "In section 5, this model is evaluated on a subset of the MapTask corpus ( and shows a high correlation with human judgements of authoritativeness (r 2 = 0.947).", "labels": [], "entities": [{"text": "MapTask corpus", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9265239238739014}]}, {"text": "After a detailed error analysis, we will conclude the paper in section 6 with a discussion of our future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This coding scheme was evaluated for reliability on two corpora using Cohen's kappa.", "labels": [], "entities": []}, {"text": "Within the social sciences community, a kappa above 0.7 is considered acceptable.", "labels": [], "entities": []}, {"text": "Two conversations were each coded by hand by two trained annotators.", "labels": [], "entities": []}, {"text": "The first conversation was between three students in a collaborative learning task; inter-rater reliability kappa for Negotiation labels was 0.78.", "labels": [], "entities": [{"text": "inter-rater reliability kappa", "start_pos": 84, "end_pos": 113, "type": "METRIC", "confidence": 0.9240073959032694}]}, {"text": "The second conversation was from the MapTask corpus, and kappa was 0.71.", "labels": [], "entities": [{"text": "MapTask corpus", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.9065207242965698}, {"text": "kappa", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9972931742668152}]}, {"text": "Further data was labelled by hand by one trained annotator.", "labels": [], "entities": []}, {"text": "In our work, we label conversations using the coding scheme above.", "labels": [], "entities": []}, {"text": "To determine how well these codes correlate with other interesting factors, we choose to assign a quantitative measure of authoritativeness to each speaker.", "labels": [], "entities": []}, {"text": "This measure can then be compared to other features of a speaker.", "labels": [], "entities": []}, {"text": "To do this, we use the coded labels to assign an Authoritativeness Ratio to each speaker.", "labels": [], "entities": []}, {"text": "First, we define a function A(S, c, L) fora speaker, a contribution, and a set of labels L \u2286 {K1, K2, A1, A2, o, ch} as: We then define the Authoritativeness ratio Auth(S) fora speaker S in a dialogue consisting of contributions c 1 ...c n as: The intuition behind this ratio is that we are only interested in the four main label types in our analysis -at least for an initial description of authority, we do not consider the non-contentful o moves.", "labels": [], "entities": []}, {"text": "Within these four main labels, there are clearly two that appear \"dominant\" -statements of factor opinion, and commands or instructions -and two that appear less dominant -questions or requests for information, and narration of an action.", "labels": [], "entities": []}, {"text": "We sum these together to reach a single numeric value for each speaker's projection of authority in the dialogue.", "labels": [], "entities": []}, {"text": "The full details of our external validations of this approach are available in.", "labels": [], "entities": []}, {"text": "To summarize, we considered two data sets involving student collaborative learning.", "labels": [], "entities": []}, {"text": "The first data set consisted of pairs of students interacting over two days, and was annotated for aggressive behavior, to assess warning factors in social interactions.", "labels": [], "entities": []}, {"text": "Our analysis 1021 showed that aggressive behavior correlated with authoritativeness ratio (p < .05), and that less aggressive students became less authoritative in the second day (p < .05, effect size .15\u03c3).", "labels": [], "entities": []}, {"text": "The second data set was analyzed for Self-Efficacy -the confidence of each student in their own ability (Bandura, 1997) -as well as actual learning gains based on pre-and post-test scores.", "labels": [], "entities": []}, {"text": "We found that the Authoritativeness ratio was a significant predictor of learning gains (r 2 = .41, p < .04).", "labels": [], "entities": [{"text": "Authoritativeness ratio", "start_pos": 18, "end_pos": 41, "type": "METRIC", "confidence": 0.913254588842392}]}, {"text": "Furthermore, in a multiple regression, we determined that the Authoritativeness ratio of both students in a group predict the average Self-Efficacy of the pair (r 2 = .12, p < .01).", "labels": [], "entities": [{"text": "Authoritativeness", "start_pos": 62, "end_pos": 79, "type": "METRIC", "confidence": 0.953202486038208}]}, {"text": "We test our models on a twenty conversation subset of the MapTask corpus detailed in.", "labels": [], "entities": [{"text": "MapTask corpus", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8948584198951721}]}, {"text": "We compare the use of four models in our results.", "labels": [], "entities": []}, {"text": "\u2022 Baseline: This model uses a bag-of-words feature space as input to an SVM classifier.", "labels": [], "entities": []}, {"text": "No segmentation model is used and no ILP constraints are enforced.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.9633821845054626}]}, {"text": "\u2022 Baseline+ILP: This model uses the baseline feature space as input to both classification and segmentation models.", "labels": [], "entities": []}, {"text": "ILP constraints are enforced between these models.", "labels": [], "entities": []}, {"text": "\u2022 Contextual: This model uses our enhanced feature space from section 4.2, with no segmentation model and no ILP constraints enforced.", "labels": [], "entities": []}, {"text": "\u2022 Contextual+ILP: This model uses the enhanced feature spaces for both Negotiation labels and segment boundaries from section 4.2 to enforce ILP constraints.", "labels": [], "entities": []}, {"text": "For segmentation, we evaluate our models using exact-match accuracy.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9898659586906433}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.5196213722229004}]}, {"text": "We use multiple evaluation metrics to judge classification.", "labels": [], "entities": []}, {"text": "The first and most basic is accuracy -the percentage of accurately chosen Negotiation labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995404481887817}]}, {"text": "Secondly, we use Cohen's Kappa to judge improvement inaccuracy over chance.", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.8319305777549744}]}, {"text": "The final evaluation is the r 2 coefficient computed between predicted and actual Authoritativeness ratios per speaker.", "labels": [], "entities": [{"text": "r 2 coefficient", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.8756972948710123}]}, {"text": "This represents how much variance in authoritativeness is accounted for in the predicted ratios.", "labels": [], "entities": []}, {"text": "This final metric is the most important for measuring reproducibility of human analyses of speaker authority in conversation.", "labels": [], "entities": []}, {"text": "We use SIDE for feature extraction  (Joachims, 1999), and Learning-Based Java for ILP inference ().", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7062729299068451}]}, {"text": "Performance is evaluated by 20-fold cross-validation, where each fold is trained on 19 conversations and tested on the remaining one.", "labels": [], "entities": []}, {"text": "Statistical significance was calculated using a student's paired t-test.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.753917783498764}]}, {"text": "For accuracy and kappa, n = 20 (one data point per conversation) and for r 2 , n = 40 (one data point per speaker).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9988151788711548}]}], "tableCaptions": [{"text": " Table 1: The six codes in our coding scheme, along with  their frequency in our corpus of twenty conversations.", "labels": [], "entities": []}, {"text": " Table 2: Performance evaluation for our models. Each  line is significantly improved in both accuracy and r 2 er- ror from the previous line (p < .01).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9996064305305481}, {"text": "r 2 er- ror", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.7605940818786621}]}]}