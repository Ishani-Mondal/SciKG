{"title": [{"text": "Coherent Citation-Based Summarization of Scientific Papers", "labels": [], "entities": [{"text": "Coherent Citation-Based Summarization of Scientific Papers", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7330263803402582}]}], "abstractContent": [{"text": "In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper.", "labels": [], "entities": [{"text": "citation-based summarization", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.5752926468849182}]}, {"text": "Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper).", "labels": [], "entities": []}, {"text": "Meanwhile, the fluency of the produced summaries has been mostly ignored.", "labels": [], "entities": []}, {"text": "For example , diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered.", "labels": [], "entities": []}, {"text": "This resulted in noisy and confusing summaries.", "labels": [], "entities": []}, {"text": "In this work, we present an approach for producing readable and cohesive citation-based summaries.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Scientific research is a cumulative activity.", "labels": [], "entities": []}, {"text": "The work of downstream researchers depends on access to upstream discoveries.", "labels": [], "entities": []}, {"text": "The footnotes, end notes, or reference lists within research articles make this accumulation possible.", "labels": [], "entities": []}, {"text": "When a reference appears in a scientific paper, it is often accompanied by a span of text describing the work being cited.", "labels": [], "entities": []}, {"text": "We name the sentence that contains an explicit reference to another paper citation sentence.", "labels": [], "entities": []}, {"text": "Citation sentences usually highlight the most important aspects of the cited paper such as the research problem it addresses, the method it proposes, the good results it reports, and even its drawbacks and limitations.", "labels": [], "entities": []}, {"text": "By aggregating all the citation sentences that cite a paper, we have a rich source of information about it.", "labels": [], "entities": []}, {"text": "This information is valuable because human experts have put their efforts to read the paper and summarize its important contributions.", "labels": [], "entities": [{"text": "summarize", "start_pos": 96, "end_pos": 105, "type": "TASK", "confidence": 0.9603555798530579}]}, {"text": "One way to make use of these sentences is creating a summary of the target paper.", "labels": [], "entities": []}, {"text": "This summary is different from the abstract or a summary generated from the paper itself.", "labels": [], "entities": []}, {"text": "While the abstract represents the author's point of view, the citation summary is the summation of multiple scholars' viewpoints.", "labels": [], "entities": []}, {"text": "The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization.", "labels": [], "entities": [{"text": "summarizing a scientific paper", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.8827272653579712}, {"text": "citationbased summarization", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.578098475933075}]}, {"text": "There has been previous work done on citationbased summarization (.", "labels": [], "entities": [{"text": "citationbased summarization", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.7090669572353363}]}, {"text": "Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper.", "labels": [], "entities": []}, {"text": "The cohesion and the readability of the produced summaries have been mostly ignored.", "labels": [], "entities": []}, {"text": "This resulted in noisy and confusing summaries.", "labels": [], "entities": []}, {"text": "In this work, we focus on the coherence and readability aspects of the problem.", "labels": [], "entities": []}, {"text": "Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing.", "labels": [], "entities": []}, {"text": "Our experiments show that our approach produces better summaries than several baseline summarization systems.", "labels": [], "entities": [{"text": "summaries", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.9858576059341431}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "After we examine previous work in Section 2, we outline the motivation of our approach in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the three stages of our summarization system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9758034944534302}]}, {"text": "The evaluation and the results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We provide three levels of evaluation.", "labels": [], "entities": []}, {"text": "First, we evaluate each of the components in our system separately.", "labels": [], "entities": []}, {"text": "Then we evaluate the summaries that our system generate in terms of extraction quality.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the coherence and readability of the summaries.", "labels": [], "entities": []}, {"text": "Reference Tagging and Reference Scope Identification Evaluation: We ran our reference tagging and scope identification components on the 2,284 sentences in dataset1.", "labels": [], "entities": [{"text": "Reference Tagging", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7573172152042389}, {"text": "Reference Scope Identification Evaluation", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.6937056109309196}, {"text": "scope identification", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.6851155310869217}]}, {"text": "Then, we went through the tagged sentences and the extracted scopes, and counted the number of correctly/incorrectly tagged (extracted)/missed references (scopes).", "labels": [], "entities": []}, {"text": "Our tagging  Our scope identification component extracted the scope of target references with good precision (86.4%) but low recall (35.2%).", "labels": [], "entities": [{"text": "scope identification", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8503570854663849}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9985150694847107}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9994438290596008}]}, {"text": "In fact, extracting a useful scope fora reference requires more than just finding a grammatical substring.", "labels": [], "entities": []}, {"text": "In future work, we plan to employ text regeneration techniques to improve the recall by generating grammatical sentences from ungrammatical fragments.", "labels": [], "entities": [{"text": "text regeneration", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7459240853786469}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.8917392492294312}]}, {"text": "Sentence Filtering Evaluation: We used Support Vector Machines (SVM) with linear kernel as our classifier.", "labels": [], "entities": [{"text": "Sentence Filtering Evaluation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9433392882347107}]}, {"text": "We performed 10-fold cross validation on the labeled sentences (unsuitable vs all other categories) in dataset1.", "labels": [], "entities": []}, {"text": "Our classifier achieved 80.3% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9957016110420227}]}, {"text": "Sentence Classification Evaluation: We used SVM in this step as well.", "labels": [], "entities": [{"text": "Sentence Classification Evaluation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.9179136753082275}]}, {"text": "We also performed 10-fold cross validation on the labeled sentences (the five functional categories).", "labels": [], "entities": []}, {"text": "This classifier achieved 70.1% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9979793429374695}]}, {"text": "The precision and recall for each category are given in Author Name Replacement Evaluation: The classifier used in this task is also SVM.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994655251502991}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9993127584457397}, {"text": "Author Name Replacement Evaluation", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.7225008904933929}]}, {"text": "We performed 10-fold cross validation on the labeled sentences of dataset1.", "labels": [], "entities": []}, {"text": "Our classifier achieved 77.41% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9871088266372681}]}, {"text": "Produced using our system There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g. constraint-based techniques and transformation-based techniques.", "labels": [], "entities": [{"text": "tagging and morphological disambiguation", "start_pos": 70, "end_pos": 110, "type": "TASK", "confidence": 0.6378057673573494}]}, {"text": "A thorough removal of ambiguity requires a syntactic process.", "labels": [], "entities": []}, {"text": "A rule-based tagger described in was equipped with a set of guessing rules that had been hand-crafted using knowledge of English morphology and intuitions.", "labels": [], "entities": []}, {"text": "The precision of rule-based taggers may exceed that of the probabilistic ones.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999483585357666}, {"text": "rule-based taggers", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.581228569149971}]}, {"text": "The construction of a linguistic rule-based tagger, however, has been considered a difficult and time-consuming task.", "labels": [], "entities": []}, {"text": "Produced using Another approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint Grammar work (, where a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses fora given word in a given context.", "labels": [], "entities": [{"text": "Constraint Grammar", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.6762909293174744}]}, {"text": "Some systems even perform the POS tagging as part of a syntactic analysis process.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.7718633115291595}]}, {"text": "A rule-based tagger described in is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition.", "labels": [], "entities": []}, {"text": "Older versions of EngCG (using about 1,150 constraints) are reported to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remait unresolved.", "labels": [], "entities": [{"text": "correct analysis", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.9419892430305481}]}, {"text": "We evaluate the resulting disambiguated text by a number of metrics defined as follows).", "labels": [], "entities": []}, {"text": "To evaluate the extraction quality, we use dataset2 (that has never been used for training or tuning any of the system components).", "labels": [], "entities": []}, {"text": "We use our system to generate summaries for each of the 30 papers in dataset2.", "labels": [], "entities": []}, {"text": "We also generate summaries for the papers using a number of baseline systems (described in Section 5.3.1).", "labels": [], "entities": []}, {"text": "All the generated summaries were 5 sentences long.", "labels": [], "entities": []}, {"text": "We use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) based on the longest common substrings (ROUGE-L) as our evaluation metric.", "labels": [], "entities": [{"text": "Recall-Oriented Understudy", "start_pos": 11, "end_pos": 37, "type": "METRIC", "confidence": 0.8881264328956604}]}, {"text": "We asked human judges (not including the authors) to rate the coherence and readability of a number of summaries for each of dataset2 papers.", "labels": [], "entities": []}, {"text": "For each paper we evaluated 3 summaries.", "labels": [], "entities": []}, {"text": "The sum-: Coherence Evaluation mary that our system produced, the human summary, and a summary produced by summarizer (the best baseline -after our system and its variations -in terms of extraction quality as shown in the previous subsection.)", "labels": [], "entities": []}, {"text": "The summaries were randomized and given to the judges without telling them how each summary was produced.", "labels": [], "entities": []}, {"text": "The judges were not given access to the source text.", "labels": [], "entities": []}, {"text": "They were asked to use a five pointscale to rate how coherent and readable the summaries are, where 1 means that the summary is totally incoherent and needs significant modifications to improve its readability, and 5 means that the summary is coherent and no modifications are needed to improve its readability.", "labels": [], "entities": []}, {"text": "We gave each summary to 5 different judges and took the average of their ratings for each summary.", "labels": [], "entities": []}, {"text": "We used Weighted Kappa with linear weights to measure the interrater agreement.", "labels": [], "entities": []}, {"text": "The Weighted Kappa measure between the five groups of ratings was 0.72.", "labels": [], "entities": [{"text": "Weighted Kappa measure", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.9424636363983154}]}, {"text": "shows the number of summaries in each rating range.", "labels": [], "entities": []}, {"text": "The results show that our approach significantly improves the coherence of citation-based summarization.", "labels": [], "entities": []}, {"text": "shows two sample summaries (each 5 sentences long) for the Voutilainen (1995) paper.", "labels": [], "entities": []}, {"text": "One summary was produced using our system and the other was produced using Qazvinian and Radev (2008) system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Precision and recall results achieved by our cita- tion sentence classifier", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9746065139770508}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9977492690086365}]}]}