{"title": [{"text": "A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction", "labels": [], "entities": [{"text": "Hierarchical Pitman-Yor Process HMM", "start_pos": 2, "end_pos": 37, "type": "TASK", "confidence": 0.5276540070772171}, {"text": "Speech Induction", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.6866949051618576}]}], "abstractContent": [{"text": "In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7079847306013107}]}, {"text": "We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics.", "labels": [], "entities": []}, {"text": "Central to our approach is anew type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts.", "labels": [], "entities": []}, {"text": "In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications inhuman language learning and for developing portable language processing systems.", "labels": [], "entities": [{"text": "Unsupervised part-of-speech (PoS) induction", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.614767387509346}]}, {"text": "Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early approach.", "labels": [], "entities": [{"text": "PoS induction", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.8871715068817139}]}, {"text": "One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions.", "labels": [], "entities": []}, {"text": "In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages.", "labels": [], "entities": [{"text": "PoS induction", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9154867827892303}]}, {"text": "Moreover, the performance of our unsupervised model approaches that of many existing semi-supervised systems, despite our method not receiving any human input.", "labels": [], "entities": []}, {"text": "In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging fora sequence of words.", "labels": [], "entities": []}, {"text": "HMMs have been popular for unsupervised PoS induction from its very beginnings, and justifiably so, as the most discriminating feature for deciding a word's PoS is its local syntactic context.", "labels": [], "entities": [{"text": "PoS induction", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9040956795215607}]}, {"text": "Our work brings together several strands of research including Bayesian non-parametric HMMs, Pitman-Yor language models), tagging constraints over word types) and the incorporation of morphological features.", "labels": [], "entities": []}, {"text": "The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties.", "labels": [], "entities": []}, {"text": "Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions.", "labels": [], "entities": []}, {"text": "This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation.", "labels": [], "entities": []}, {"text": "Unlike many existing approaches, our model is a principled generative model and does not include any hand tuned language specific features.", "labels": [], "entities": []}, {"text": "Inspired by previous successful approaches (, we develop anew typelevel inference procedure in the form of an MCMC sampler with an approximate method for incorporating the complex dependencies that arise between jointly sampled events.", "labels": [], "entities": [{"text": "MCMC sampler", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.9037013649940491}]}, {"text": "Our experimental evaluation demonstrates that our model, particularly when restricted to a single tag per type, produces 865 state-of-the-art results across a range of corpora and languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains.", "labels": [], "entities": []}, {"text": "For our core English experiments we report results on the entire Penn., while for other languages we use the corpora made available for the CoNLL-X Shared Task ().", "labels": [], "entities": [{"text": "Penn.", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.9914496541023254}]}, {"text": "We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of.", "labels": [], "entities": [{"text": "v-measure (VM) metrics", "start_pos": 49, "end_pos": 71, "type": "METRIC", "confidence": 0.8490755200386048}]}, {"text": "M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9990301132202148}, {"text": "VM", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.7286162376403809}, {"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9985570311546326}, {"text": "recall", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9947285056114197}]}, {"text": "The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred.", "labels": [], "entities": [{"text": "HMM sampler", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.764115035533905}]}, {"text": "The 1HMM sampler converges more quickly so we use two hundred samples for these models.", "labels": [], "entities": []}, {"text": "All reported results are the mean of three sampling runs.", "labels": [], "entities": []}, {"text": "An important detail for any unsupervised learning algorithm is its initialisation.", "labels": [], "entities": []}, {"text": "We used slightly different initialisation for each of our inference strategies.", "labels": [], "entities": []}, {"text": "For the unrestricted HMM we randomly assigned each word token to a class.", "labels": [], "entities": []}, {"text": "For the restricted 1HMM we use a similar initialiser to 870.", "labels": [], "entities": [{"text": "1HMM", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.9102445244789124}]}, {"text": "Starred entries denote results reported in CGS10., assigning each of the k most frequent word types to its own class, and then randomly dividing the rest of the types between the classes.", "labels": [], "entities": [{"text": "CGS10.", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.969383180141449}]}, {"text": "As a baseline we report the performance of mkcls (Och, 1999) on all test corpora.", "labels": [], "entities": []}, {"text": "This model seems not to have been evaluated in prior work on unsupervised PoS tagging, which is surprising given its consistently good performance.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.8742625415325165}]}, {"text": "First we present our results on the most frequently reported evaluation, the WSJ sections of the Penn.", "labels": [], "entities": [{"text": "WSJ sections of the Penn.", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.9420876204967499}]}, {"text": "Treebank, along with a number of state-of-the-art results previously reported ().", "labels": [], "entities": [{"text": "Treebank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9187911152839661}]}, {"text": "All of these models are allowed 45 tags, the same number of tags as in the gold-standard.", "labels": [], "entities": []}, {"text": "The performance of our models is strong, particularly the 1HMM.", "labels": [], "entities": [{"text": "1HMM", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9139256477355957}]}, {"text": "We also see that incorporating a character language model (1HMM-LM) leads to further gains in performance, improving over the best reported scores under both M-1 and VM.", "labels": [], "entities": [{"text": "VM", "start_pos": 166, "end_pos": 168, "type": "DATASET", "confidence": 0.7551826238632202}]}, {"text": "We have omitted the results for the HMM-LM as experimentation showed that the local Gibbs sampler became hopelessly stuck, failing to mix due to the model's deep structure (its peak performance was \u2248 55%).", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of the PYP prior we include results using a Dirichlet Process prior (DP).", "labels": [], "entities": []}, {"text": "We see that for all models the use of the PYP provides some gain for the HMM, but diminishes for the 1HMM.", "labels": [], "entities": [{"text": "PYP", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.8618417978286743}]}, {"text": "This is perhaps a consequence of the expected table count approximation for the typesampled PYP-1HMM: the DP relies lesson the table counts than the PYP.", "labels": [], "entities": []}, {"text": "If we restrict the model to bigrams we see a considerable drop in performance.", "labels": [], "entities": []}, {"text": "Note that the bigram PYP-HMM outperforms the closely related BHMM (the main difference being that we smooth tag bigrams with unigrams).", "labels": [], "entities": [{"text": "BHMM", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.661721408367157}]}, {"text": "It is also interesting to compare the bigram PYP-1HMM to the closely related model of.", "labels": [], "entities": []}, {"text": "That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9995947480201721}]}, {"text": "Figures 4 and 5 provide insight into the behavior of the sampling algorithms.", "labels": [], "entities": []}, {"text": "The former shows that both our models and mkcls induce a more uniform distribution over tags than specified by the treebank.", "labels": [], "entities": []}, {"text": "It is unclear whether it is desirable for models to exhibit behavior closer to the treebank, which dedicates separate tags to very infrequent phenomena while lumping the large range of noun types into a single category.", "labels": [], "entities": []}, {"text": "The graph in shows that the type-based 1HMM sampler finds a good tagging extremely quickly and then sticks with it, 871   save for the occasional step change demonstrated by the 1HMM-LM line.", "labels": [], "entities": []}, {"text": "The locally sampled model is far slower to converge, rising slowly and plateauing well below the other models.", "labels": [], "entities": []}, {"text": "In we compare the distributions over WSJ tags for mkcls and the PYP-1HMM-LM.", "labels": [], "entities": []}, {"text": "On the macro scale we can see that our model induces a sparser distribution.", "labels": [], "entities": []}, {"text": "With closer inspection we can identify particular improvements our model makes.", "labels": [], "entities": []}, {"text": "In the first column for mkcls and the third column for our model we can see similar classes with significant counts for DTs and PRPs, indicating a class that the models maybe using to represent the start of sentences (informed by start transitions or capitalisation).", "labels": [], "entities": []}, {"text": "This column exemplifies the sparsity of the PYP model's posterior.", "labels": [], "entities": [{"text": "sparsity", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9635791778564453}]}, {"text": "We continue our evaluation on the CoNLL multilingual corpora.", "labels": [], "entities": [{"text": "CoNLL multilingual corpora", "start_pos": 34, "end_pos": 60, "type": "DATASET", "confidence": 0.7876120408376058}]}, {"text": "These results show a highly consistent story of performance for our models across diverse corpora.", "labels": [], "entities": []}, {"text": "In all cases the PYP-1HMM outperforms the PYP-HMM, which are both outperformed by the PYP-1HMM-LM.", "labels": [], "entities": []}, {"text": "The character language model provides large gains in performance on a number of corpora, in particular those with rich morphology (Arabic +5%, Portuguese +5%, Spanish +4%).", "labels": [], "entities": []}, {"text": "We again note the strong performance of the mkcls model, significantly beating recently published state-of-theart results for both Dutch and Swedish.", "labels": [], "entities": []}, {"text": "Overall our best model (PYP-1HMM-LM) outperforms both the state-of-the-art, where previous work exists, as well as mkcls consistently across all languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  \u2020 Lee et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9873764514923096}]}]}