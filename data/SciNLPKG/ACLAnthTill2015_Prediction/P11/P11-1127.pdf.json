{"title": [], "abstractContent": [{"text": "We present minimum Bayes-risk system combination , a method that integrates consensus decoding and system combination into a unified multi-system minimum Bayes-risk (MBR) technique.", "labels": [], "entities": []}, {"text": "Unlike other MBR methods that re-rank translations of a single SMT system, MBR system combination uses the MBR decision rule and a linear combination of the component systems' probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9740768671035767}]}, {"text": "We introduce expected BLEU, an approximation to the BLEU score that allows to efficiently apply MBR in these conditions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9969898462295532}, {"text": "BLEU score", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.975692093372345}, {"text": "MBR", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.7923519611358643}]}, {"text": "MBR system combination is a general method that is independent of specific SMT models, enabling us to combine systems with heterogeneous structure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9741486310958862}]}, {"text": "Experiments show that our approach bring significant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.8142390549182892}]}], "introductionContent": [{"text": "Once statistical models are trained, a decoding approach determines what translations are finally selected.", "labels": [], "entities": []}, {"text": "Two parallel lines of research have shown consistent improvements over the max-derivation decoding objective, which selects the highest probability derivation.", "labels": [], "entities": []}, {"text": "Consensus decoding procedures select translations fora single system with a minimum Bayes risk (MBR) (.", "labels": [], "entities": [{"text": "Bayes risk (MBR)", "start_pos": 84, "end_pos": 100, "type": "METRIC", "confidence": 0.9695788979530334}]}, {"text": "System combination procedures, on the other hand, generate translations from the output of multiple component systems by combining the best fragments of these outputs.", "labels": [], "entities": []}, {"text": "In this paper, we present minimum Bayes risk system combination, a technique that unifies these two approaches by learning a consensus translation over multiple underlying component systems.", "labels": [], "entities": []}, {"text": "MBR system combination operates directly on the outputs of the component models.", "labels": [], "entities": [{"text": "MBR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.88459312915802}]}, {"text": "We perform an MBR decoding using a linear combination of the component models' probability distributions.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.7723743915557861}]}, {"text": "Instead of re-ranking the translations provided by the component systems, we search for the hypothesis with the minimum expected translation error among all the possible finite-length strings in the target language.", "labels": [], "entities": []}, {"text": "By using a loss function based on BLEU), we avoid the hypothesis alignment problem that is central to standard system combination approaches (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.997757613658905}, {"text": "hypothesis alignment", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7363920658826828}]}, {"text": "MBR system combination assumes only that each translation model can produce expectations of n-gram counts; the latent derivation structures of the component systems can differ arbitrary.", "labels": [], "entities": []}, {"text": "This flexibility allows us to combine a great variety of SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9914060831069946}]}, {"text": "The key contributions of this paper are three: the usage of a linear combination of distributions within the MBR decoding, which allows multiple SMT models to be involved in, and makes the computation of n-grams statistics to be more accurate; the decoding in an extended search space, which allows to find better hypotheses than the evidences provided by the component models; and the use of an expected BLEU score instead of the sentence-wise BLEU, which allows to efficiently apply MBR decoding in the huge search space under consideration.", "labels": [], "entities": [{"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9761328101158142}, {"text": "BLEU", "start_pos": 405, "end_pos": 409, "type": "METRIC", "confidence": 0.9980505704879761}, {"text": "BLEU", "start_pos": 445, "end_pos": 449, "type": "METRIC", "confidence": 0.945496678352356}]}, {"text": "We evaluate in a multi-source translation task obtaining improvements of up to +2.0 BLEU abs.", "labels": [], "entities": [{"text": "BLEU abs", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9780415296554565}]}, {"text": "over the best single system max-derivation, and state-ofthe-art performance in the system combination task of the ACL 2010 workshop on SMT.", "labels": [], "entities": [{"text": "ACL 2010 workshop", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.8475276231765747}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.5440935492515564}]}], "datasetContent": [{"text": "We report results on a multi-source translation task.", "labels": [], "entities": [{"text": "multi-source translation task", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7137179573376974}]}, {"text": "From the Europarl corpus released for the ACL 2006 workshop on MT (WMT2006), we select those sentence pairs from the German-English (de-en), Spanish-English (es-en) and FrenchEnglish (fr-en) sub-corpora that share the same English translation.", "labels": [], "entities": [{"text": "Europarl corpus released for the ACL 2006 workshop on MT (WMT2006)", "start_pos": 9, "end_pos": 75, "type": "DATASET", "confidence": 0.8949197805844821}]}, {"text": "We obtain a multi-source corpus with German, Spanish and French as source languages and English as target language.", "labels": [], "entities": []}, {"text": "All the experiments were carried outwith the lowercased and tokenized version of this corpus.", "labels": [], "entities": [{"text": "tokenized version of this corpus", "start_pos": 60, "end_pos": 92, "type": "DATASET", "confidence": 0.6847369313240051}]}, {"text": "We report results using BLEU () and translation edit rate (: Performance from best single system maxderivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9988227486610413}]}, {"text": "95% confidence intervals computed using paired bootstrap re-sampling ().", "labels": [], "entities": []}, {"text": "In) systems without statistically significant differences are marked with the same superscript.", "labels": [], "entities": []}, {"text": "We first apply MBR-SC to the best system (MBR-SC-Expected).", "labels": [], "entities": [{"text": "MBR-SC", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.6079406142234802}]}, {"text": "Best MBR and MBR-SC-Expected differ only in the gain function: MBR uses sentence level BLEU while MBR-SC-Expected uses the expected BLEU gain described in Section 5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.787521243095398}, {"text": "BLEU gain", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9698257744312286}]}, {"text": "MBR-SC-Expected performance is comparable to MBR decoding on the 1000-best list from the single best system.", "labels": [], "entities": []}, {"text": "The expected BLEU approximation performs as well as sentence-level BLEU and additionally requires less total computation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9918172955513}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.909980297088623}]}, {"text": "We now extend the evidences space to the conjoined 1000-best lists (MBR-SC-E/Conjoin).", "labels": [], "entities": [{"text": "MBR-SC-E", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.6755276322364807}]}, {"text": "MBR-SC-E/Conjoin is much better than the best MBR on a single system.", "labels": [], "entities": [{"text": "MBR-SC-E", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8120471835136414}]}, {"text": "This implies that either the expected BLEU statistics computed in the conjoined evidences space are stronger or the larger conjoined evidences spaces introduce better hypotheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9969814419746399}]}, {"text": "When we restrict the BLEU statistics to be computed from only the best system's evidences space (MBR-SC-E/C/evidences-best), BLEU scores dramatically decrease relative to MBR-SC-E/Conjoin.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9914751052856445}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9991305470466614}, {"text": "MBR-SC-E", "start_pos": 171, "end_pos": 179, "type": "DATASET", "confidence": 0.846010148525238}]}, {"text": "This implies that the expected BLEU statistics computed over the conjoined 1000-best lists are stronger than the corresponding statistics from the single best system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9982703924179077}]}, {"text": "On the other hand, if we restrict the search space to only the 1000-best list of the best system (MBR-SC-E/C/hypotheses-best), BLEU scores also decrease relative to MBR-SC-E/Conjoin.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9996352195739746}, {"text": "MBR-SC-E", "start_pos": 165, "end_pos": 173, "type": "DATASET", "confidence": 0.8682652711868286}]}, {"text": "This implies that the conjoined search space also contains better hypotheses than the single best system's search space.", "labels": [], "entities": []}, {"text": "These results validate our approach.", "labels": [], "entities": []}, {"text": "The linear combination of the probability distributions in the conjoined evidences spaces allows to compute much stronger statistics for the expected BLEU gain and also contains some better hypotheses than the single best system's search space does.", "labels": [], "entities": [{"text": "BLEU gain", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9755929708480835}]}, {"text": "We next expand the conjoined evidences spaces using the decoding algorithm described in Section 4.2 (MBR-SC-E/C/Extended).", "labels": [], "entities": [{"text": "MBR-SC-E/C/Extended)", "start_pos": 101, "end_pos": 121, "type": "DATASET", "confidence": 0.900858054558436}]}, {"text": "In this case, the expected BLEU statistics are computed from the conjoined 1000-best lists of the three systems, but the hypotheses space where we perform the decoding is expanded to the set of all possible finitelength hypotheses over the vocabulary of the evidences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9975892305374146}]}, {"text": "We take the output of MBR-SC-E/Conjoin as the initial hypotheses of the decoding (see Algorithm 1).", "labels": [], "entities": [{"text": "MBR-SC-E", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8562982678413391}]}, {"text": "MBR-SC-E/C/Extended improves BLEU score of MBR-SC-E/Conjoin but obtains a slightly worse TER score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9786379635334015}, {"text": "TER score", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9859445691108704}]}, {"text": "Since these two systems are identical in their expected BLEU statistics, the improvements in BLEU imply that the extended search space has introduced better hypotheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.995898425579071}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9974439144134521}]}, {"text": "The degradation in TER performance can be explained by the use of a BLEU-based gain function in the decoding process.", "labels": [], "entities": [{"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7101768255233765}, {"text": "BLEU-based", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9985564351081848}]}, {"text": "We finally compute the optimum values for the scaling factors of the different system using MERT (MBR-SC-E/C/Ex/MERT).", "labels": [], "entities": [{"text": "MERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9696551561355591}]}, {"text": "MBR-SC-E/C/Ex/MERT slightly improves BLEU score of MBR-SC-E/C/Extended.", "labels": [], "entities": [{"text": "MERT", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.8618164658546448}, {"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9820011854171753}]}, {"text": "This implies that the optimal values of the scaling factors do not deviate much from 1.0; a similar result was reported in).", "labels": [], "entities": []}, {"text": "We hypothesize that this is because the three component systems share the same SMT model, pre-process and decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.986241340637207}]}, {"text": "We expect to obtain larger improvements when combining systems implementing different MT paradigms.", "labels": [], "entities": [{"text": "MT paradigms", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.9202247858047485}]}, {"text": "MBR-SC-E/C/Ex/MERT is the standard setup for MBR system combination and, from now, on we will refer to it as MBR-SC.", "labels": [], "entities": [{"text": "MBR-SC-E", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9078980684280396}, {"text": "MERT", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.7515791058540344}, {"text": "MBR-SC", "start_pos": 109, "end_pos": 115, "type": "DATASET", "confidence": 0.9156636595726013}]}, {"text": "We next evaluate performance of MBR system combination on N -best lists of increasing sizes, and compare it to MBR-SC-E/C/Extended and MBR-SC-E/Conjoin in the same N -best lists.", "labels": [], "entities": []}, {"text": "We list the results of the Best MAX system for comparison.", "labels": [], "entities": []}, {"text": "Results in confirm the conclusions extracted from results displayed in.", "labels": [], "entities": []}, {"text": "MBR-SCConjoin is consistently better than the Best MAX system, and differences in BLEU increase with the size of the evidences space.", "labels": [], "entities": [{"text": "MBR-SCConjoin", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8539171814918518}, {"text": "Best MAX system", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.8914302190144857}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.999235987663269}]}, {"text": "This implies that the linear combination of posterior probabilities allow to compute stronger statistics for the expected BLEU gain, and, in addition, the larger the evidences space is, the stronger the computed statistics are.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9985790252685547}]}, {"text": "MBR-SC-C/Extended is also consistently better than MBR-SC-Conjoin with an almost constant improvement of +0.4 BLEU points.", "labels": [], "entities": [{"text": "MBR-SC-C/Extended", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8308481375376383}, {"text": "MBR-SC-Conjoin", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.7789608836174011}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9985132813453674}]}, {"text": "This result show that the extended search space always contains better hypotheses than the conjoined evidences spaces; also confirms the soundness of Algorithm 1 that allows to reach them.", "labels": [], "entities": []}, {"text": "Finally, MBR-SC also slightly improves MBR-SC-C/Extended.", "labels": [], "entities": [{"text": "MBR-SC", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.6151182651519775}, {"text": "MBR-SC-C", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.711537778377533}]}, {"text": "The optimization of the scaling factors allows only small improvements in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9943791031837463}]}, {"text": "display the MBR system combination translation and compare it to the max-derivation translations of the three component systems.", "labels": [], "entities": [{"text": "MBR", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.9085495471954346}]}, {"text": "Reference translation is also listed for comparison.", "labels": [], "entities": []}, {"text": "MBR-MAX de\u2192en i will return later . MAX es\u2192en i shall comeback to that later . MAX fr\u2192en i will return to this later . MBR-SC i will return to this point later . Reference i will return to this point later . SC adds word \"point\" to create anew translation equal to the reference.", "labels": [], "entities": []}, {"text": "MBR-SC is able to detect that this is valuable word even though it does not appear in the max-derivation hypotheses.", "labels": [], "entities": [{"text": "MBR-SC", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9246313571929932}]}], "tableCaptions": [{"text": " Table 1: Performance of base systems.", "labels": [], "entities": []}, {"text": " Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.", "labels": [], "entities": []}, {"text": " Table 3: Results on the test set for different setups of  minimum Bayes risk system combination.", "labels": [], "entities": []}]}