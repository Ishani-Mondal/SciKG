{"title": [], "abstractContent": [{"text": "We propose a principled and efficient phrase-to-phrase alignment model, useful in machine translation as well as other related natural language processing problems.", "labels": [], "entities": [{"text": "phrase-to-phrase alignment", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7354360371828079}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8025879859924316}]}, {"text": "Ina hidden semi-Markov model, word-to-phrase and phrase-to-word translations are modeled directly by the system.", "labels": [], "entities": []}, {"text": "Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsu-pervised training with multi-word units.", "labels": [], "entities": []}, {"text": "Expanding the state space to include \"gappy phrases\" (such as French ne pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments.", "labels": [], "entities": []}, {"text": "The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptot-ically equivalent runtime.", "labels": [], "entities": [{"text": "translation", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.9474725127220154}]}], "introductionContent": [{"text": "Word alignment is an important part of statistical machine translation (MT) pipelines.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7524701058864594}, {"text": "statistical machine translation (MT) pipelines", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.8221952574593681}]}, {"text": "Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems ().", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 125, "end_pos": 169, "type": "TASK", "confidence": 0.5562179386615753}]}, {"text": "Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (), which in turn are derived from bilingual word alignments and syntactic * Author was a summer intern at Microsoft Research during this project.", "labels": [], "entities": [{"text": "syntactic machine translation", "start_pos": 5, "end_pos": 34, "type": "TASK", "confidence": 0.6431470314661661}]}, {"text": "would like, where indicates a gap).", "labels": [], "entities": []}, {"text": "Much work has addressed this problem: generative models for direct phrasal alignment (), heuristic word-alignment combinations (, models with pseudoword collocations, synchronous grammar based approaches, etc.", "labels": [], "entities": []}, {"text": "Most have a large state-space, using constraints and approximations for efficient inference.", "labels": [], "entities": []}, {"text": "We present anew phrasal alignment model based on the hidden Markov framework (.", "labels": [], "entities": []}, {"text": "Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments.", "labels": [], "entities": []}, {"text": "We also augment the state space to include contiguous sequences.", "labels": [], "entities": []}, {"text": "This corresponds to phrase-to-word and phrase-to-phrase alignments.", "labels": [], "entities": []}, {"text": "We generalize alignment by agreement () to this space, and find that agreement discourages EM from overfitting.", "labels": [], "entities": []}, {"text": "Finally, we make the alignment space more symmetric by including gappy (or non-contiguous) phrases.", "labels": [], "entities": []}, {"text": "This allows agreement to reinforce non-contiguous align- Figure 2: The model of E given F can represent the phrasal alignment {e1, e2} \u223c {f1}.", "labels": [], "entities": []}, {"text": "However, the model of F given E cannot: the probability mass is distributed between {e1} \u223c {f1} and {e2} \u223c {f1}.", "labels": [], "entities": []}, {"text": "Agreement of the forward and backward HMM alignments tends to place less mass on phrasal links and greater mass on word-to-word links.", "labels": [], "entities": []}, {"text": "ments, such English not to French ne pas.", "labels": [], "entities": []}, {"text": "Pruning the set of allowed phrases preserves the time complexity of the word-to-word HMM alignment model.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.8948338627815247}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F1 scores of automatic word alignments, evaluated on the test set of the hand-aligned sentence pairs.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993817806243896}, {"text": "word alignments", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7057472616434097}]}]}