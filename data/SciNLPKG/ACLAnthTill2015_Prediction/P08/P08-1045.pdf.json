{"title": [{"text": "Name Translation in Statistical Machine Translation Learning When to Transliterate", "labels": [], "entities": [{"text": "Name Translation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8850606679916382}, {"text": "Statistical Machine Translation Learning", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.7782279029488564}]}], "abstractContent": [{"text": "We present a method to transliterate names in the framework of end-to-end statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.6099053919315338}]}, {"text": "The system is trained to learn when to transliterate.", "labels": [], "entities": []}, {"text": "For Arabic to English MT, we developed and trained a transliterator on a bitext of 7 million sentences and Google's English terabyte ngrams and achieved better name translation accuracy than 3 out of 4 professional translators.", "labels": [], "entities": [{"text": "Arabic to English MT", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.5309558212757111}, {"text": "name translation", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.5736065059900284}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.8919471502304077}]}, {"text": "The paper also includes a discussion of challenges in name translation evaluation.", "labels": [], "entities": [{"text": "name translation evaluation", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.9423221349716187}]}], "introductionContent": [{"text": "State-of-the-art statistical machine translation (SMT) is bad at translating names that are not very common, particularly across languages with different character sets and sound systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.7793090691169103}]}, {"text": "For example, consider the following automatic translation: 1 The SMT system drops most names in this example.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9869022369384766}]}, {"text": "\"Name dropping\" and mis-translation happens when the system encounters an unknown word, mistakes a name fora common noun, or trains on noisy parallel data.", "labels": [], "entities": [{"text": "Name dropping\"", "start_pos": 1, "end_pos": 15, "type": "TASK", "confidence": 0.8761246005694071}]}, {"text": "The state-of-the-art is poor for taken from NIST02-05 corpora two reasons.", "labels": [], "entities": [{"text": "NIST02-05 corpora", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9650290012359619}]}, {"text": "First, although names are important to human readers, automatic MT scoring metrics (such as BLEU) do not encourage researchers to improve name translation in the context of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9857103228569031}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9975759387016296}, {"text": "name translation", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.7287080436944962}, {"text": "MT", "start_pos": 173, "end_pos": 175, "type": "TASK", "confidence": 0.9658026099205017}]}, {"text": "Names are vastly outnumbered by prepositions, articles, adjectives, common nouns, etc.", "labels": [], "entities": []}, {"text": "Second, name translation is a hard problem -even professional human translators have trouble with names.", "labels": [], "entities": [{"text": "name translation", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.9296694099903107}]}, {"text": "Here are four reference translations taken from the same corpus, with mistakes underlined: and phrases.", "labels": [], "entities": []}, {"text": "Most of this work has been disconnected from end-to-end MT, a problem which we address head-on in this paper.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9649989008903503}]}, {"text": "The simplest way to integrate name handling into SMT is: (1) run a named-entity identification system on the source sentence, (2) transliterate identified entities with a special-purpose transliteration component, and (3) run the SMT system on the source sentence, as usual, but when looking up phrasal translations for the words identified in step 1, instead use the transliterations from step 2.", "labels": [], "entities": [{"text": "name handling", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.820237010717392}, {"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9878117442131042}, {"text": "SMT", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.9791069030761719}]}, {"text": "Many researchers have attempted this, and it does notwork.", "labels": [], "entities": []}, {"text": "Typically, translation quality is degraded rather than improved, for the following reasons: \ud97b\udf59 Automatic named-entity identification makes errors.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9581762552261353}, {"text": "named-entity identification", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.6647733002901077}]}, {"text": "Some words and phrases that should not be transliterated are nonetheless sent to the transliteration component, which returns a bad translation.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 Not all named entities should be transliterated.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 Transliteration components make errors.", "labels": [], "entities": []}, {"text": "The base SMT system may translate a commonlyoccurring name just fine, due to the bitext it was trained on, while the transliteration component can easily supply a worse answer.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9766755700111389}]}, {"text": "\ud97b\udf59 Integration hobbles SMT's use of longer phrases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9870308041572571}]}, {"text": "Even if the named-entity identification and transliteration components operate perfectly, adopting their translations means that the SMT system may no longer have access to longer phrases that include the name.", "labels": [], "entities": [{"text": "SMT", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.9852999448776245}]}, {"text": "For example, our base SMT system translates QJ \ud97b\udf59 \ud97b\udf59 KP \ud97b\udf59 J K . \ud97b\udf59 \ud97b\udf59 Z @ PP P \ud97b\udf59 @ (as a whole phrase) to \"Premier Li Peng\", based on its bitext knowledge.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9836814403533936}, {"text": "QJ \ud97b\udf59 \ud97b\udf59 KP \ud97b\udf59 J K . \ud97b\udf59 \ud97b\udf59 Z", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.7728945098140023}, {"text": "Premier Li Peng\"", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.8757492303848267}]}, {"text": "However, if we force \ud97b\udf59 J K . \ud97b\udf59 \ud97b\udf59 to translate as a separate phrase to \"Li Peng\", then the term Z @ PP P @ Q J \ud97b\udf59 \ud97b\udf59 KP becomes ambiguous (with translations including \"Prime Minister\", \"Premier\", etc.), and we observe incorrect choices being subsequently made.", "labels": [], "entities": []}, {"text": "To spur better work in name handling, an ACE entity-translation pilot evaluation was recently.", "labels": [], "entities": [{"text": "name handling", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9395173788070679}]}, {"text": "This evaluation involves a mixture of entity identification and translation concerns-for example, the scoring system asks for coreference determination, which mayor may not be of interest for improving machine translation output.", "labels": [], "entities": [{"text": "entity identification", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7182098925113678}, {"text": "coreference determination", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.9596202671527863}, {"text": "machine translation output", "start_pos": 202, "end_pos": 228, "type": "TASK", "confidence": 0.7906017700831095}]}, {"text": "In this paper, we adopt a simpler metric.", "labels": [], "entities": []}, {"text": "We ask: what percentage of source-language named entities are translated correctly?", "labels": [], "entities": []}, {"text": "This is a precision metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984458088874817}]}, {"text": "We can readily apply it to any base SMT system, and to human translations as well.", "labels": [], "entities": [{"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9822170734405518}]}, {"text": "Our goal in augmenting abase SMT system is to increase this percentage.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9926969408988953}]}, {"text": "A secondary goal is to make sure that our overall translation quality (as measured by BLEU) does not degrade as a result of the name-handling techniques we introduce.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9981860518455505}]}, {"text": "We make all our measurements on an Arabic/English newswire translation task.", "labels": [], "entities": [{"text": "Arabic/English newswire translation task", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.6269388248523077}]}, {"text": "Our overall technical approach is summarized here, along with references to sections of this paper: \ud97b\udf59 We build a component for transliterating between Arabic and English (Section 3).", "labels": [], "entities": [{"text": "transliterating between Arabic and English", "start_pos": 127, "end_pos": 169, "type": "TASK", "confidence": 0.8412438273429871}]}, {"text": "\ud97b\udf59 We automatically learn to tag those words and phrases in Arabic text, which we believe the transliteration component will translate correctly (Section 4).", "labels": [], "entities": []}, {"text": "\ud97b\udf59 We integrate suggested transliterations into the base SMT search space, with their use controlled by a feature function (Section 5).", "labels": [], "entities": [{"text": "SMT search", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9283918738365173}]}, {"text": "\ud97b\udf59 We evaluate both the base SMT system and the augmented system in terms of entity translation accuracy and BLEU (Sections 2 and 6).", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9860219955444336}, {"text": "entity translation", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6468875259160995}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.8756265044212341}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9996727705001831}]}], "datasetContent": [{"text": "In this section we present the evaluation method that we use to measure our system and also discuss challenges in name transliteration evaluation.", "labels": [], "entities": [{"text": "name transliteration evaluation", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.7542757987976074}]}, {"text": "General MT metrics such as BLEU, TER, METEOR are not suitable for evaluating named entity translation and transliteration, because they are not focused on named entities (NEs).", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9707115888595581}, {"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9985039234161377}, {"text": "TER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9925770163536072}, {"text": "METEOR", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9726429581642151}, {"text": "named entity translation and transliteration", "start_pos": 77, "end_pos": 121, "type": "TASK", "confidence": 0.6675532162189484}]}, {"text": "Dropping a comma or a the is penalized as much as dropping a name.", "labels": [], "entities": []}, {"text": "We therefore use another metric, jointly developed with BBN and LanguageWeaver.", "labels": [], "entities": [{"text": "BBN", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9500068426132202}, {"text": "LanguageWeaver", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9102997779846191}]}, {"text": "The general idea of the Named Entity Weak Accuracy (NEWA) metric is to \ud97b\udf59 Count number of NEs in source text: N \ud97b\udf59 Count number of correctly translated NEs: C \ud97b\udf59 Divide C/N to get an accuracy figure In NEWA, an NE is counted as correctly translated if the target reference NE is found in the MT output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9984369874000549}]}, {"text": "The metric has the advantage that it is easy to compute, has no special requirements on an MT system (such as depending on source-target word alignment) and is tokenization independent.", "labels": [], "entities": []}, {"text": "In the result section of this paper, we will use the NEWA metric to measure and compare the accuracy of NE translations in our end-to-end SMT translations and four human reference translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9988850951194763}, {"text": "SMT translations", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.9380025565624237}]}], "tableCaptions": [{"text": " Table 1: Accuracy of \"transliterate-me\" tagger", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9943822622299194}]}, {"text": " Table 2: Name translation accuracy with respect to BBN  and re-annotated Gold Standard on 1730 named entities  in 637 sentences.", "labels": [], "entities": [{"text": "Name translation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8752664029598236}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9840006828308105}, {"text": "BBN", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8134849667549133}, {"text": "Gold Standard", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.6456738263368607}]}, {"text": " Table 3: Name translation accuracy in end-to-end statistical machine translation (SMT) system for different named  entity (NE) types: Person (PER), Geopolitical Entity, which includes countries, provinces and towns (GPE), Organi- zation (ORG), Facility (FAC), Nominal Person, e.g. Swede (PER.Nom), other location (LOC).", "labels": [], "entities": [{"text": "Name translation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8433962464332581}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.942169725894928}, {"text": "statistical machine translation (SMT)", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.7962022423744202}]}]}