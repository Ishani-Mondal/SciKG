{"title": [{"text": "Unsupervised Learning of Narrative Event Chains", "labels": [], "entities": [{"text": "Narrative Event Chains", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7206611831982931}]}], "abstractContent": [{"text": "Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge.", "labels": [], "entities": []}, {"text": "We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text.", "labels": [], "entities": []}, {"text": "A narrative event chain is a partially ordered set of events related by a common protagonist.", "labels": [], "entities": []}, {"text": "We describe a three step process to learning narrative event chains.", "labels": [], "entities": [{"text": "learning narrative event chains", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6007067486643791}]}, {"text": "The first uses unsu-pervised distributional methods to learn narrative relations between events sharing corefer-ring arguments.", "labels": [], "entities": []}, {"text": "The second applies a temporal classifier to partially order the connected events.", "labels": [], "entities": []}, {"text": "Finally, the third prunes and clusters self-contained chains from the space of events.", "labels": [], "entities": []}, {"text": "We introduce two evaluations: the narrative cloze to evaluate event relatedness, and an order coherence task to evaluate narrative order.", "labels": [], "entities": []}, {"text": "We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence.", "labels": [], "entities": [{"text": "narrative prediction", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8319072127342224}]}], "introductionContent": [{"text": "This paper induces anew representation of structured knowledge called narrative event chains (or narrative chains).", "labels": [], "entities": []}, {"text": "Narrative chains are partially ordered sets of events centered around a common protagonist.", "labels": [], "entities": [{"text": "Narrative chains", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.86550372838974}]}, {"text": "They are related to structured sequences of participants and events that have been called scripts ( or Fillmorean frames.", "labels": [], "entities": [{"text": "Fillmorean", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.928646445274353}]}, {"text": "These participants and events can be filled in and instantiated in a particular text situation to draw inferences.", "labels": [], "entities": []}, {"text": "Chains focus on a single actor to facilitate learning, and thus this paper addresses the three tasks of chain induction: narrative event induction, temporal ordering of events and structured selection (pruning the event space into discrete sets).", "labels": [], "entities": [{"text": "chain induction", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.715100958943367}, {"text": "narrative event induction", "start_pos": 121, "end_pos": 146, "type": "TASK", "confidence": 0.6282975574334463}]}, {"text": "Learning these prototypical schematic sequences of events is important for rich understanding of text.", "labels": [], "entities": []}, {"text": "Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization, coreference resolution and question answering.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.6372751494248708}, {"text": "summarization", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9870221018791199}, {"text": "coreference resolution", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.9433739185333252}, {"text": "question answering", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.8972915410995483}]}, {"text": "For example, proposed that understanding text about restaurants required knowledge about the Restaurant Script, including the participants (Customer,), the events constituting the script (entering, sitting down, asking for menus, etc.), and the various preconditions, ordering, and results of each of the constituent actions.", "labels": [], "entities": []}, {"text": "Consider these two distinct narrative chains.", "labels": [], "entities": []}, {"text": "accused X W joined X claimed W served X argued W oversaw dismissed X W resigned It would be useful for question answering or textual entailment to know that 'X denied ' is also a likely event in the left chain, while ' replaces W' temporally follows the right.", "labels": [], "entities": [{"text": "question answering", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8444414734840393}, {"text": "textual entailment", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.6491871178150177}]}, {"text": "Narrative chains (such as Firing of Employee or Executive Resigns) offer the structure and power to directly infer these new subevents by providing critical background knowledge.", "labels": [], "entities": [{"text": "Firing of Employee or Executive Resigns", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.7670141955216726}]}, {"text": "In part due to its complexity, automatic induction has not been addressed since the early nonstatistical work of.", "labels": [], "entities": [{"text": "automatic induction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.6931920200586319}]}, {"text": "The first step to narrative induction uses an entitybased model for learning narrative relations by fol-lowing a protagonist.", "labels": [], "entities": [{"text": "narrative induction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8402087390422821}]}, {"text": "As a narrative progresses through a series of events, each event is characterized by the grammatical role played by the protagonist, and by the protagonist's shared connection to surrounding events.", "labels": [], "entities": []}, {"text": "Our algorithm is an unsupervised distributional learning approach that uses coreferring arguments as evidence of a narrative relation.", "labels": [], "entities": []}, {"text": "We show, using anew evaluation task called narrative cloze, that our protagonist-based method leads to better induction than a verb-only approach.", "labels": [], "entities": []}, {"text": "The next step is to order events in the same narrative chain.", "labels": [], "entities": []}, {"text": "We apply work in the area of temporal classification to create partial orders of our learned events.", "labels": [], "entities": [{"text": "temporal classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7631637454032898}]}, {"text": "We show, using a coherence-based evaluation of temporal ordering, that our partial orders lead to better coherence judgements of real narrative instances extracted from documents.", "labels": [], "entities": []}, {"text": "Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains.", "labels": [], "entities": []}], "datasetContent": [{"text": "The cloze task is used to evaluate a system (or human) for language proficiency by removing a random word from a sentence and having the system attempt to fill in the blank (e.g. I forgot to the waitress for the good service).", "labels": [], "entities": []}, {"text": "Depending on the type of word removed, the test can evaluate syntactic knowledge as well as semantic.", "labels": [], "entities": []}, {"text": "proposed an extended task, discourse cloze, to evaluate discourse knowledge (removing phrases that are recoverable from knowledge of discourse relations like contrast and consequence).", "labels": [], "entities": []}, {"text": "We present anew cloze task that requires narrative knowledge to solve, the narrative cloze.", "labels": [], "entities": []}, {"text": "The narrative cloze is a sequence of narrative events in a document from which one event has been removed.", "labels": [], "entities": []}, {"text": "The task is to predict the missing verb and typed dependency.", "labels": [], "entities": []}, {"text": "Take this example text about American football with McCann as the protagonist: These clauses are represented in the narrative model as five events: (threw subject), (pulled object), (told object), (start subject), (completed subject).", "labels": [], "entities": []}, {"text": "These verb/dependency events makeup a narrative cloze model.", "labels": [], "entities": []}, {"text": "We could remove (threw subject) and use the remaining four events to rank this missing event.", "labels": [], "entities": []}, {"text": "Removing a single such pair to be filled in automatically allows us to evaluate a system's knowledge of narrative relations and coherence.", "labels": [], "entities": []}, {"text": "We do not claim this cloze task to be solvable even by humans, New York Times Editorial occupied subj brought subj rejecting subj projects subj met subj appeared subj offered subj voted pp for offer subj thinks subj but rather assert it as a comparative measure to evaluate narrative knowledge.", "labels": [], "entities": []}, {"text": "We use years 1994-2004 (1,007,227 documents) of the Gigaword Corpus () for training 2 . We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006) 3 , recording all verbs with subject, object, or prepositional typed dependencies.", "labels": [], "entities": [{"text": "Gigaword Corpus", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.8961575925350189}, {"text": "Stanford Parser (de Marneffe et al., 2006)", "start_pos": 144, "end_pos": 186, "type": "DATASET", "confidence": 0.8662068843841553}]}, {"text": "We use the OpenNLP 4 coreference engine to resolve the entity mentions.", "labels": [], "entities": [{"text": "OpenNLP 4 coreference engine", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.8419204503297806}]}, {"text": "For each document, the verb pairs that share coreferring entities are recorded with their dependency types.", "labels": [], "entities": []}, {"text": "Particles are included with the verb.", "labels": [], "entities": []}, {"text": "We used 10 news stories from the 1994 section of the corpus for development.", "labels": [], "entities": [{"text": "1994 section of the corpus", "start_pos": 33, "end_pos": 59, "type": "DATASET", "confidence": 0.6499939084053039}]}, {"text": "The stories were hand chosen to represent a range of topics such as business, sports, politics, and obituaries.", "labels": [], "entities": []}, {"text": "We used 69 news stories from the 2001 (year selected randomly) section of the corpus for testing (also removed from training).", "labels": [], "entities": [{"text": "2001 (year selected randomly) section of the corpus", "start_pos": 33, "end_pos": 84, "type": "DATASET", "confidence": 0.7912894785404205}]}, {"text": "The test set documents were randomly chosen and not preselected fora range of topics.", "labels": [], "entities": []}, {"text": "From each document, the entity involved in the most events was selected as the protagonist.", "labels": [], "entities": []}, {"text": "For this evaluation, we only look at verbs.", "labels": [], "entities": []}, {"text": "All verb clauses involving the protagonist are manually extracted and translated into the narrative events (verb,dependency).", "labels": [], "entities": []}, {"text": "Exceptions that are not included are verbs in headlines, quotations (typically not part of a narrative), \"be\" properties (e.g. john is happy), modifying verbs (e.g. hurried to leave, only leave is used), and multiple instances of one event.", "labels": [], "entities": []}, {"text": "The original test set included 100 documents, but those without a narrative chain at least five events in length were removed, leaving 69 documents.", "labels": [], "entities": []}, {"text": "Most of the removed documents were not stories, but genres such as interviews and cooking recipes.", "labels": [], "entities": []}, {"text": "An example of an extracted chain is shown in.", "labels": [], "entities": []}, {"text": "We evalute with Narrative Cloze using leave-oneout cross validation, removing one event and using the rest to generate a ranked list of guesses.", "labels": [], "entities": []}, {"text": "The test dataset produces 740 cloze tests (69 narratives with 740 events).", "labels": [], "entities": []}, {"text": "After generating our ranked guesses, the position of the correct event is averaged overall 740 tests for the final score.", "labels": [], "entities": []}, {"text": "We penalize unseen events by setting their ranked position to the length of the guess list (ranging from 2k to 15k). is an example of a ranked guess list fora short chain of three events.", "labels": [], "entities": []}, {"text": "If the original document contained (fired obj), this cloze test would score 3.", "labels": [], "entities": []}, {"text": "We want to evaluate temporal order at the narrative level, across all events within a chain.", "labels": [], "entities": []}, {"text": "We envision narrative chains being used for tasks of coherence, among other things, and so it is desired to evaluate temporal decisions within a coherence framework.", "labels": [], "entities": []}, {"text": "Along these lines, our test set uses actual narrative chains from documents, hand labeled fora partial ordering.", "labels": [], "entities": []}, {"text": "We evaluate coherence of these true chains against a random ordering.", "labels": [], "entities": []}, {"text": "The task is thus deciding which of the two chains is most coherent, the original or the random (baseline 50%)?", "labels": [], "entities": []}, {"text": "We generated up to 300 random orderings for each test document, averaging the accuracy across all.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992750287055969}]}, {"text": "Our evaluation data is the same 69 documents used in the test set for learning narrative relations.", "labels": [], "entities": []}, {"text": "The chain from each document is hand identified and labeled fora partial ordering using only the before relation.", "labels": [], "entities": []}, {"text": "Ordering was done by the authors and all attempts were made to include every before relation that exists in the document, or that could be deduced through transitivity rules.", "labels": [], "entities": []}, {"text": "shows an example and its full reversal, although the evaluation uses random orderings.", "labels": [], "entities": []}, {"text": "Each edge is a distinct before relation and is used in the judgement score.", "labels": [], "entities": []}, {"text": "The coherence score fora partially ordered narrative chain is the sum of all the relations that our classified corpus agrees with, weighted by how certain we are.", "labels": [], "entities": []}, {"text": "If the gigaword classifications disagree, a weighted negative score is given.", "labels": [], "entities": []}, {"text": "Confidence is based on a logarithm scale of the difference between the counts of before and after classifications.", "labels": [], "entities": [{"text": "Confidence", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9303095936775208}]}, {"text": "Formally, the score is calculated as the following: where E is the set of all event pairs, B(i, j) is how many times we classified events i and j as before in Gigaword, and D(i, j) = |B(i, j) \u2212 B(j, i)|.", "labels": [], "entities": [{"text": "B", "start_pos": 91, "end_pos": 92, "type": "METRIC", "confidence": 0.9803950190544128}, {"text": "Gigaword", "start_pos": 159, "end_pos": 167, "type": "DATASET", "confidence": 0.902805507183075}]}, {"text": "The relation i\u03b2j indicates that i is temporally before j.", "labels": [], "entities": []}], "tableCaptions": []}