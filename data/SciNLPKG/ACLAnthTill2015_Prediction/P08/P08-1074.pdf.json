{"title": [{"text": "Phrase Chunking using Entropy Guided Transformation Learning", "labels": [], "entities": [{"text": "Phrase Chunking", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9460372924804688}]}], "abstractContent": [{"text": "Entropy Guided Transformation Learning (ETL) is anew machine learning strategy that combines the advantages of decision trees (DT) and Transformation Based Learning (TBL).", "labels": [], "entities": [{"text": "Entropy Guided Transformation Learning (ETL)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7467909838472094}]}, {"text": "In this work, we apply the ETL framework to four phrase chunking tasks: Por-tuguese noun phrase chunking, English base noun phrase chunking, English text chunking and Hindi text chunking.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.703402191400528}, {"text": "Por-tuguese noun phrase chunking", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.505730889737606}, {"text": "English base noun phrase chunking", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.5674810349941254}, {"text": "English text chunking", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.5952342947324117}, {"text": "Hindi text chunking", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.6912766297658285}]}, {"text": "In all four tasks, ETL shows better results than Decision Trees and also than TBL with hand-crafted templates.", "labels": [], "entities": [{"text": "ETL", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.6763220429420471}]}, {"text": "ETL provides anew training strategy that accelerates transformation learning.", "labels": [], "entities": [{"text": "ETL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9527341723442078}]}, {"text": "For the English text chunking task this corresponds to a factor of five speedup.", "labels": [], "entities": [{"text": "English text chunking task", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.710325300693512}]}, {"text": "For Por-tuguese noun phrase chunking, ETL shows the best reported results for the task.", "labels": [], "entities": [{"text": "Por-tuguese noun phrase chunking", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.5457433387637138}, {"text": "ETL", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.978797435760498}]}, {"text": "For the other three linguistic tasks, ETL shows state-of-the-art competitive results and maintains the advantages of using a rule based system.", "labels": [], "entities": [{"text": "ETL", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.363962858915329}]}], "introductionContent": [{"text": "Phrase Chunking is a Natural Language Processing (NLP) task that consists in dividing a text into syntactically correlated parts of words.", "labels": [], "entities": [{"text": "Phrase Chunking", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9095039367675781}]}, {"text": "Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk).", "labels": [], "entities": []}, {"text": "It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction.", "labels": [], "entities": [{"text": "parsing", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.9575004577636719}, {"text": "information extraction", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.8027735352516174}]}, {"text": "Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (), Winnow (), votedperceptrons (, Transformation-Based Learning (TBL)) and Hidden Markov Model (HMM) (), Memory-based.", "labels": [], "entities": []}, {"text": "State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques ().", "labels": [], "entities": [{"text": "English base noun phrase chunking", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.5437902867794037}, {"text": "text chunking", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.706825390458107}]}, {"text": "TBL is one of the most accurate rule-based techniques for phrase chunking tasks).", "labels": [], "entities": [{"text": "TBL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.45301076769828796}, {"text": "phrase chunking tasks", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.8600524465243021}]}, {"text": "On the other hand, TBL rules must follow patterns, called templates, that are meant to capture the relevant feature combinations.", "labels": [], "entities": []}, {"text": "The process of generating good templates is highly expensive.", "labels": [], "entities": []}, {"text": "It strongly depends on the problem expert skills to build them.", "labels": [], "entities": []}, {"text": "Even when a template set is available fora given task, it may not be effective when we change from a language to another . In this work, we apply Entropy Guided Transformation Learning (ETL) for phrase chunking.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 195, "end_pos": 210, "type": "TASK", "confidence": 0.7801699936389923}]}, {"text": "ETL is anew machine learning strategy that combines the advantages of Decision Trees (DT) and TBL (dos).", "labels": [], "entities": [{"text": "ETL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7967674732208252}, {"text": "TBL (dos", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9166207710901896}]}, {"text": "The ETL key idea is to use decision tree induction to obtain feature combinations (templates) and then use the TBL algorithm to generate transformation rules.", "labels": [], "entities": []}, {"text": "ETL produces transformation rules that are more effective than decision trees and also eliminates the need of a problem domain expert to build TBL templates.", "labels": [], "entities": []}, {"text": "We evaluate the performance of ETL over four phrase chunking tasks: (1) English Base Noun Phrase (NP) chunking; (2) Portuguese NP chunking; (3) English Text Chunking; and (4) Hindi Text Chunking.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.715671718120575}, {"text": "English Base Noun Phrase (NP) chunking", "start_pos": 72, "end_pos": 110, "type": "TASK", "confidence": 0.5283352322876453}, {"text": "NP chunking", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.6718389987945557}, {"text": "Hindi Text Chunking", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.6027719378471375}]}, {"text": "Base NP chunking consists in recognizing non-overlapping text segments that contain NPs.", "labels": [], "entities": [{"text": "Base NP chunking", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5572837094465891}]}, {"text": "Text chunking consists in dividing a text into syntactically correlated parts of words.", "labels": [], "entities": [{"text": "Text chunking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7249700874090195}]}, {"text": "For these four tasks, ETL shows state-of-the-art competitive results and maintains the advantages of using a rule based system.", "labels": [], "entities": [{"text": "ETL", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.34551793336868286}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, the ETL strategy is described.", "labels": [], "entities": [{"text": "ETL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.7709437608718872}]}, {"text": "In section 3, the experimental design and the corresponding results are reported.", "labels": [], "entities": []}, {"text": "Finally, in section 4, we present our concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the experimental setup and results of the application of ETL to four phrase chunking tasks.", "labels": [], "entities": [{"text": "phrase chunking tasks", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7805187106132507}]}, {"text": "ETL results are compared with the results of DT and TBL using hand-crafted templates.", "labels": [], "entities": [{"text": "ETL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5965341329574585}]}, {"text": "In the TBL step, for each one of the four chunking tasks, the initial classifier assigns to each word the chunk tag that was most frequently associated with the part-of-speech of that word in the training set.", "labels": [], "entities": [{"text": "TBL step", "start_pos": 7, "end_pos": 15, "type": "TASK", "confidence": 0.8124049305915833}]}, {"text": "The DT learning works as a feature selector and is not affected by irrelevant features.", "labels": [], "entities": []}, {"text": "We have tried several context window sizes when training the classifiers.", "labels": [], "entities": []}, {"text": "Some of the tested window sizes would be very hard to be explored by a domain expert using TBL alone.", "labels": [], "entities": []}, {"text": "The corresponding huge number of possible templates would be very difficult to be managed by a template designer.", "labels": [], "entities": []}, {"text": "For the four tasks, the following experimental setup provided us our best results.", "labels": [], "entities": []}, {"text": "ETL in the ETL learning, we use the features word, POS and chunk.", "labels": [], "entities": [{"text": "ETL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7575305104255676}]}, {"text": "In order to overcome the sparsity problem, we only use the 200 most frequent words to induce the DT.", "labels": [], "entities": []}, {"text": "In the DT learning, the chunk tag of the word is the one applied by the initial classifier.", "labels": [], "entities": [{"text": "DT learning", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.8667162954807281}]}, {"text": "On the other hand, the chunk tag of neighbor words are the true ones.", "labels": [], "entities": []}, {"text": "We report results for ETL trained with all the templates at the same time as well as using template evolution.", "labels": [], "entities": []}, {"text": "TBL the results for the TBL approach refers to TBL trained with the set of templates proposed in ().", "labels": [], "entities": []}, {"text": "DT the best result for the DT classifier is shown.", "labels": [], "entities": [{"text": "DT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8350774645805359}]}, {"text": "The features word, POS and chunk are used to generate the DT classifier.", "labels": [], "entities": []}, {"text": "The chunk tag of a word and its neighbors are the ones guessed by the initial classifier.", "labels": [], "entities": []}, {"text": "Using only the 100 most frequent words gives our best results.", "labels": [], "entities": []}, {"text": "In all experiments, the term WS=X subscript means that a window of size X was used for the given model.", "labels": [], "entities": []}, {"text": "For instance, ETL W S=3 corresponds to ETL trained with window of size three, that is, the current token, the previous and the next one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Portuguese noun phrase chunking.", "labels": [], "entities": [{"text": "Portuguese noun phrase chunking", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.5291656777262688}]}, {"text": " Table 3: Portuguese noun phrase chunking using ETL  with template evolution.", "labels": [], "entities": [{"text": "Portuguese noun phrase chunking", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.5112213753163815}]}, {"text": " Table 4: Portuguese noun phrase chunking using six ad- ditional hand-crafted templates.", "labels": [], "entities": [{"text": "Portuguese noun phrase chunking", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.5092174112796783}]}, {"text": " Table 5: Committee with the classifiers ETL W S=5 ,  ETL W S=7 and ETL W S=9 , shown in Table 4.", "labels": [], "entities": []}, {"text": " Table 6: Base NP chunking.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.6514939218759537}]}, {"text": " Table 7: Base NP chunking using ETL with template evo- lution.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.7474677264690399}]}, {"text": " Table 8: Base NP chunking using a committee of eight  ETL classifiers.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.7046269774436951}]}, {"text": " Table 9: English text Chunking.", "labels": [], "entities": [{"text": "English text Chunking", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.724796712398529}]}, {"text": " Table 10: English text chunking using ETL with template  evolution.", "labels": [], "entities": [{"text": "English text chunking", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.608920673529307}]}, {"text": " Table 11: English text Chunking using a committee of  eight ETL classifiers.", "labels": [], "entities": [{"text": "Chunking", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.7133079767227173}]}, {"text": " Table 12: English text chunking results, broken down by  chunk type, for the ETL committee.", "labels": [], "entities": [{"text": "English text chunking", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.5827520589033762}, {"text": "ETL committee", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.728683352470398}]}, {"text": " Table 13: Hindi text Chunking.", "labels": [], "entities": []}, {"text": " Table 14: Comparison with best systems of SPSAL-2007", "labels": [], "entities": [{"text": "SPSAL-2007", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.6144710779190063}]}]}