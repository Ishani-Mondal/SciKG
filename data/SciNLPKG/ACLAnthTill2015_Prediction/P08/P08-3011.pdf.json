{"title": [{"text": "Adaptive Language Modeling for Word Prediction", "labels": [], "entities": [{"text": "Adaptive Language Modeling", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8046929240226746}, {"text": "Word Prediction", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7363778054714203}]}], "abstractContent": [{"text": "We present the development and tuning of a topic-adapted language model for word prediction , which improves keystroke savings over a comparable baseline.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.8320021033287048}]}, {"text": "We outline our plans to develop and integrate style adaptations , building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "People who use Augmentative and Alternative Communication (AAC) devices communicate slowly, often below 10 words per minute (wpm) compared to 150 wpm or higher for speech ().", "labels": [], "entities": [{"text": "Augmentative and Alternative Communication (AAC)", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.6790873663766044}]}, {"text": "AAC devices are highly specialized keyboards with speech synthesis, typically providing single-button input for common words or phrases, but requiring a user to type letter-by-letter for other words, called fringe vocabulary.", "labels": [], "entities": []}, {"text": "Many commercial systems (e.g., PRC's ECO) and researchers (; have leveraged word prediction to help speed AAC communication rate.", "labels": [], "entities": [{"text": "PRC's ECO", "start_pos": 31, "end_pos": 40, "type": "TASK", "confidence": 0.587821364402771}, {"text": "word prediction", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7146288603544235}, {"text": "AAC communication", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.9154432415962219}]}, {"text": "While the user is typing an utterance letter-by-letter, the system continuously provides potential completions of the current word to the user, which the user may select.", "labels": [], "entities": []}, {"text": "The list of predicted words is generated using a language model.", "labels": [], "entities": []}, {"text": "At best, modern devices utilize a trigram model and very basic recency promotion.", "labels": [], "entities": []}, {"text": "However, one of the lamented weaknesses of ngram models is their sensitivity to the training data.", "labels": [], "entities": []}, {"text": "They require substantial training data to be accurate, and increasingly more data as more of the context is utilized.", "labels": [], "entities": [{"text": "accurate", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9635065793991089}]}, {"text": "For example, demonstrate that bigram and trigram models for word prediction are not saturated even when trained on 3 million words, in contrast to a unigram model.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.807080864906311}]}, {"text": "In addition to the problem of needing substantial amounts of training text to build a reasonable model, ngrams are sensitive to the difference between training and testing/user texts.", "labels": [], "entities": []}, {"text": "An ngram model trained on text of a different topic and/or style may perform very poorly compared to a model trained and tested on similar text. and have demonstrated the domain sensitivity of ngram models for word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 210, "end_pos": 225, "type": "TASK", "confidence": 0.8029775619506836}]}, {"text": "The problem of utilizing ngram models for conversational AAC usage is that no substantial corpora of AAC text are available (much less conversational AAC text).", "labels": [], "entities": []}, {"text": "The most similar available corpora are spoken language, but are typically much smaller than written corpora.", "labels": [], "entities": []}, {"text": "The problem of corpora for AAC is that similarity and availability are inversely related, illustrated in.", "labels": [], "entities": [{"text": "AAC", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9359049797058105}]}, {"text": "At one extreme, a very large amount of formal written English is available, however, it is very dissimilar from conversational AAC text, making it less useful for word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 163, "end_pos": 178, "type": "TASK", "confidence": 0.8208017349243164}]}, {"text": "At the other extreme, logged text from the current conversation of the AAC user is the most highly related text, but it is extremely sparse.", "labels": [], "entities": []}, {"text": "While this trend is demonstrated with a variety of language modeling applications, the problem is more severe for AAC due to the extremely limited availability of AAC text.", "labels": [], "entities": [{"text": "AAC", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.8573704957962036}]}, {"text": "Even if we train our models on both a large number of general texts in addition to highly related in-domain texts to address the problem, we must focus the models on the most relevant texts.", "labels": [], "entities": []}, {"text": "We address the problem of balancing training size and similarity by dynamically adapting the language model to the most topically relevant portions of the training data.", "labels": [], "entities": []}, {"text": "We present the results of experimenting with different topic segmentations and relevance scores in order to tune existing methods to topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.7664632797241211}]}, {"text": "Our approach is designed to seamlessly degrade to the baseline model when no relevant topics are found, by interpolating frequencies as well as ensuring that all training documents contribute some non-zero probabilities to the model.", "labels": [], "entities": []}, {"text": "We also outline our plans to adapt ngram models to the style of discourse and then combine the topical and stylistic adaptations.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}