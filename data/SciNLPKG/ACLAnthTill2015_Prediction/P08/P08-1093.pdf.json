{"title": [{"text": "Generating Impact-Based Summaries for Scientific Literature", "labels": [], "entities": [{"text": "Generating Impact-Based Summaries", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6731836994489034}]}], "abstractContent": [{"text": "In this paper, we present a study of a novel summarization problem, i.e., summarizing the impact of a scientific publication.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9617153406143188}, {"text": "summarizing the impact of a scientific publication", "start_pos": 74, "end_pos": 124, "type": "TASK", "confidence": 0.6999689085142953}]}, {"text": "Given a paper and its citation context, we study how to extract sentences that can represent the most influential content of the paper.", "labels": [], "entities": []}, {"text": "We propose language modeling methods for solving this problem, and study how to incorporate features such as authority and proximity to accurately estimate the impact language model.", "labels": [], "entities": []}, {"text": "Experiment results on a SIGIR publication collection show that the proposed methods are effective for generating impact-based summaries .", "labels": [], "entities": [{"text": "SIGIR publication collection", "start_pos": 24, "end_pos": 52, "type": "DATASET", "confidence": 0.8045115868250529}]}], "introductionContent": [{"text": "The volume of scientific literature has been growing rapidly.", "labels": [], "entities": []}, {"text": "From recent statistics, each year 400,000 new citations are added to MEDLINE, the major biomedical literature database . This fast growth of literature makes it difficult for researchers, especially beginning researchers, to keep track of the research trends and find high impact papers on unfamiliar topics.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.924728512763977}]}, {"text": "Impact factors) are useful, but they are just numerical values, so they cannot tell researchers which aspects of a paper are influential.", "labels": [], "entities": []}, {"text": "On the other hand, a regular contentbased summary (e.g., the abstract or conclusion section of a paper or an automatically generated topical summary () can help a user know 1 http://www.nlm.nih.gov/bsd/history/tsld024.htm about the main content of a paper, but not necessarily the most influential content of the paper.", "labels": [], "entities": []}, {"text": "Indeed, the abstract of a paper mostly reflects the expected impact of the paper as perceived by the author(s), which could significantly deviate from the actual impact of the paper in the research community.", "labels": [], "entities": []}, {"text": "Moreover, the impact of a paper changes overtime due to the evolution and progress of research in afield.", "labels": [], "entities": []}, {"text": "For example, an algorithm published a decade ago maybe no longer the state of the art, but the problem definition in the same paper can be still well accepted.", "labels": [], "entities": []}, {"text": "Although much work has been done on text summarization (See Section 6 fora detailed survey), to the best of our knowledge, the problem of impact summarization has not been studied before.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7155250608921051}, {"text": "impact summarization", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.67227503657341}]}, {"text": "In this paper, we study this novel summarization problem and propose language modeling-based approaches to solving the problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9869468212127686}]}, {"text": "By definition, the impact of a paper has to be judged based on the consent of research community, especially by people who cited it.", "labels": [], "entities": []}, {"text": "Thus in order to generate an impact-based summary, we must use not only the original content, but also the descriptions of that paper provided in papers which cited it, making it a challenging task and different from a regular summarization setup such as news summarization.", "labels": [], "entities": [{"text": "news summarization", "start_pos": 255, "end_pos": 273, "type": "TASK", "confidence": 0.643428236246109}]}, {"text": "Indeed, unlike a regular summarization system which identifies and interprets the topic of a document, an impact summarization system should identify and interpret the impact of a paper.", "labels": [], "entities": []}, {"text": "We define the impact summarization problem in the framework of extraction-based text summarization, and cast the problem as an impact sentence retrieval problem.", "labels": [], "entities": [{"text": "impact summarization", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.6723870486021042}, {"text": "text summarization", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7065891474485397}]}, {"text": "We propose language models to exploit both the citation context and original content of a paper to generate an impact-based summary.", "labels": [], "entities": []}, {"text": "We study how to incorporate features such as authority and proximity into the estimation of language models.", "labels": [], "entities": []}, {"text": "We propose and evaluate several different strategies for estimating the impact language model, which is key to impact summarization.", "labels": [], "entities": [{"text": "impact summarization", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.6013800501823425}]}, {"text": "No existing test collection is available for evaluating impact summarization.", "labels": [], "entities": [{"text": "impact summarization", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6583971083164215}]}, {"text": "We construct a test collection using 28 years of ACM SIGIR papers to evaluate the proposed methods.", "labels": [], "entities": [{"text": "ACM SIGIR papers", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.7505895892779032}]}, {"text": "Experiment results on this collection show that the proposed approaches are effective for generating impact-based summaries.", "labels": [], "entities": []}, {"text": "The results also show that using both the original document content and the citation contexts is important and incorporating citation authority and proximity is beneficial.", "labels": [], "entities": []}, {"text": "An impact-based summary is not only useful for facilitating the exploration of literature, but also helpful for suggesting query terms for literature retrieval, understanding the evolution of research trends, and identifying the interactions of different research fields.", "labels": [], "entities": [{"text": "literature retrieval", "start_pos": 139, "end_pos": 159, "type": "TASK", "confidence": 0.6895743161439896}]}, {"text": "The proposed methods are also applicable to summarizing the impact of documents in other domains where citation context exists, such as emails and weblogs.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9743620157241821}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 and 3, we define the impact-based summarization problem and propose the general language modeling approach.", "labels": [], "entities": [{"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7359434366226196}, {"text": "general language modeling", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6724437375863394}]}, {"text": "In Section 4, we present different strategies and features for estimating an impact language model, a key challenge in impact summarization.", "labels": [], "entities": [{"text": "impact summarization", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.6897516250610352}]}, {"text": "We discuss our experiments and results in Section 5.", "labels": [], "entities": []}, {"text": "Finally, the related work and conclusions are discussed in Section 6 and Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the current practice in evaluating summarization, particularly DUC 3 , we use the ROUGE evaluation package ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9526653289794922}, {"text": "DUC 3", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.8201062679290771}, {"text": "ROUGE", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9437105059623718}]}, {"text": "Among ROUGE metrics, ROUGE-N (models n-gram cooccurrence, N = 1, 2) and ROUGE-L (models longest common sequence) generally perform well in evaluating both single-document summarization and multi-document summarization ().", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.8038628101348877}, {"text": "multi-document summarization", "start_pos": 189, "end_pos": 217, "type": "TASK", "confidence": 0.6327432543039322}]}, {"text": "Since they are general evaluation measures for summarization, they are also applicable to evaluating the MEAD-Doc+Cite baseline method to be described below.", "labels": [], "entities": [{"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9861305356025696}, {"text": "MEAD-Doc+Cite baseline", "start_pos": 105, "end_pos": 127, "type": "DATASET", "confidence": 0.7200356125831604}]}, {"text": "Thus although we evaluated our methods with all the metrics provided by ROUGE, we only report ROUGE-1 and ROUGE-L in this paper (other metrics give very similar results).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9905277490615845}, {"text": "ROUGE-L", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9518423080444336}]}], "tableCaptions": [{"text": " Table 1: Performance Comparison of Summarizers", "labels": [], "entities": [{"text": "Summarizers", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.6340356469154358}]}, {"text": " Table 3: Effectiveness of interpolation", "labels": [], "entities": [{"text": "interpolation", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.5437729358673096}]}, {"text": " Table 4: Authority (pg(s)) and proximity (pr(s))", "labels": [], "entities": [{"text": "Authority", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9394059777259827}, {"text": "proximity", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9755715131759644}]}]}