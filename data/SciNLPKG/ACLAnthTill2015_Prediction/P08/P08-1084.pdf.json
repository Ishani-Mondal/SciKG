{"title": [{"text": "Unsupervised Multilingual Learning for Morphological Segmentation", "labels": [], "entities": [{"text": "Morphological Segmentation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6999587565660477}]}], "abstractContent": [{"text": "For centuries, the deep connection between languages has brought about major discoveries about human communication.", "labels": [], "entities": []}, {"text": "In this paper we investigate how this powerful source of information can be exploited for unsuper-vised language learning.", "labels": [], "entities": []}, {"text": "In particular, we study the task of morphological segmentation of multiple languages.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7741954624652863}]}, {"text": "We present a non-parametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme patterns , or abstract morphemes.", "labels": [], "entities": []}, {"text": "We apply our model to three Semitic languages: Arabic, He-brew, Aramaic, as well as to English.", "labels": [], "entities": []}, {"text": "Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models.", "labels": [], "entities": [{"text": "error", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9971519708633423}]}, {"text": "Furthermore , we provide evidence that our joint model achieves better performance when applied to languages from the same family.", "labels": [], "entities": []}], "introductionContent": [{"text": "For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians.", "labels": [], "entities": []}, {"text": "The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals.", "labels": [], "entities": []}, {"text": "The connection between languages should be a powerful source of information for automatic linguistic analysis as well.", "labels": [], "entities": [{"text": "automatic linguistic analysis", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6076119045416514}]}, {"text": "In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning?", "labels": [], "entities": []}, {"text": "(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?", "labels": [], "entities": []}, {"text": "We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning).", "labels": [], "entities": [{"text": "unsupervised morphological segmentation", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.7402427792549133}]}, {"text": "For example, the English word misunderstanding would be segmented into misunderstand -ing.", "labels": [], "entities": []}, {"text": "This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented).", "labels": [], "entities": []}, {"text": "The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.", "labels": [], "entities": []}, {"text": "For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows: This example illustrates the potential benefits of unsupervised multilingual learning.", "labels": [], "entities": []}, {"text": "The three Semitic languages use cognates (words derived from a common ancestor) to represent the word land.", "labels": [], "entities": []}, {"text": "They also use an identical suffix (-y) to represent the first person possessive pronoun (my).", "labels": [], "entities": []}, {"text": "These similarities inform should guide the model by constraining the space of joint segmentations.", "labels": [], "entities": []}, {"text": "The corresponding English phrase lacks this resemblance to its Semitic counterparts.", "labels": [], "entities": []}, {"text": "However, in this as in many cases, no segmentation is required for English as all the morphemes are expressed as individual words.", "labels": [], "entities": []}, {"text": "For this reason, English should provide a strong source of disambiguation for highly inflected languages, such as Arabic and Hebrew.", "labels": [], "entities": []}, {"text": "In general, we pose the following question.", "labels": [], "entities": []}, {"text": "In which scenario will multilingual learning be most effective?", "labels": [], "entities": []}, {"text": "Will it be for related languages, which share a common core of linguistic features, or for distant languages, whose linguistic divergence can provide strong sources of disambiguation?", "labels": [], "entities": []}, {"text": "As a first step towards answering this question, we propose a model which can take advantage of both similarities and differences across languages.", "labels": [], "entities": []}, {"text": "This joint bilingual model identifies optimal morphemes for two languages and at the same time finds compact multilingual representations.", "labels": [], "entities": []}, {"text": "For each language in the pair, the model favors segmentations which yield high frequency morphemes.", "labels": [], "entities": []}, {"text": "Moreover, bilingual morpheme pairs which consistently share a common semantic or syntactic function are treated as abstract morphemes, generated by a single language-independent process.", "labels": [], "entities": []}, {"text": "These abstract morphemes are induced automatically by the model from recurring bilingual patterns.", "labels": [], "entities": []}, {"text": "For example, in the case above, the tuple (in, fy, b-, b-) would constitute one of three abstract morphemes in the phrase.", "labels": [], "entities": []}, {"text": "When a morpheme occurs in one language without a direct counterpart in the other language, our model can explain away the stray morpheme as arising through a language-specific process.", "labels": [], "entities": []}, {"text": "To achieve this effect in a probabilistic framework, we formulate a hierarchical Bayesian model with Dirichlet Process priors.", "labels": [], "entities": []}, {"text": "This framework allows us to define priors over the infinite set of possible morphemes in each language.", "labels": [], "entities": []}, {"text": "In addition, we define a prior over abstract morphemes.", "labels": [], "entities": []}, {"text": "This prior can incorporate knowledge of the phonetic relationship between the two alphabets, giving potential cognates greater prior likelihood.", "labels": [], "entities": []}, {"text": "The resulting posterior distributions concentrate their probability mass on a small group of recurring and stable patterns within and between languages.", "labels": [], "entities": []}, {"text": "We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations.", "labels": [], "entities": []}, {"text": "The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known fora highly productive morphology.", "labels": [], "entities": []}, {"text": "Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation.", "labels": [], "entities": []}, {"text": "When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%.", "labels": [], "entities": [{"text": "error", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.5215131640434265}]}, {"text": "Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning.", "labels": [], "entities": []}, {"text": "However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit.", "labels": [], "entities": []}], "datasetContent": [{"text": "Morpheme Definition For the purpose of these experiments, we define morphemes to include conjunctions, prepositional and pronominal affixes, plural and dual suffixes, particles, definite articles, and roots.", "labels": [], "entities": []}, {"text": "We do not model cases of infixed morpheme transformations, as those cannot be modeled by linear segmentation.", "labels": [], "entities": []}, {"text": "Dataset As a source of parallel data, we use the Hebrew Bible and translations.", "labels": [], "entities": []}, {"text": "For the Hebrew version, we use an edition distributed by Westminster Hebrew Institute ().", "labels": [], "entities": [{"text": "Westminster Hebrew Institute", "start_pos": 57, "end_pos": 85, "type": "DATASET", "confidence": 0.9827636281649271}]}, {"text": "This Bible edition is augmented by gold standard morphological analysis (including segmentation) performed by biblical scholars.", "labels": [], "entities": []}, {"text": "For the Arabic, Aramaic, and English versions, fied by augmenting the model with a pair of \"morphemeidentity\" variables deterministically drawn from each abstract morpheme.", "labels": [], "entities": []}, {"text": "Thus the identity of the drawn morphemes can be retained even while resampling their generation mechanism.", "labels": [], "entities": []}, {"text": "we use the Van Dyke Arabic translation, 4 Targum Onkelos, 5 and the Revised Standard Version, respectively.", "labels": [], "entities": [{"text": "Van Dyke Arabic translation", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.7364613711833954}, {"text": "Revised Standard Version", "start_pos": 68, "end_pos": 92, "type": "DATASET", "confidence": 0.9284525314966837}]}, {"text": "We obtained gold standard segmentations of the Arabic translation with a hand-crafted Arabic morphological analyzer which utilizes manually constructed word lists and compatibility rules and is further trained on a large corpus of hand-annotated Arabic data).", "labels": [], "entities": []}, {"text": "The accuracy of this analyzer is reported to be 94% for full morphological analyses, and 98%-99% when part-of-speech tag accuracy is not included.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996585845947266}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8963608145713806}]}, {"text": "We don't have gold standard segmentations for the English and Aramaic portions of the data, and thus restrict our evaluation to Hebrew and Arabic.", "labels": [], "entities": []}, {"text": "To obtain our corpus of short parallel phrases, we preprocessed each language pair using the Giza++ alignment toolkit.", "labels": [], "entities": []}, {"text": "6 Given word alignments for each language pair, we extract a list of phrase pairs that form independent sets in the bipartite alignment graph.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6972709149122238}]}, {"text": "This process allows us to group together phrases like fy s . bah . in Arabic and bbqr in Hebrew while being reasonably certain that all the relevant morphemes are contained in the short extracted phrases.", "labels": [], "entities": []}, {"text": "The number of words in such phrases ranges from one to four words in the Semitic languages and up to six words in English.", "labels": [], "entities": []}, {"text": "Before performing any experiments, a manual inspection of the generated parallel phrases revealed that many infrequent phrase pairs occurred merely as a result of noisy translation and alignment.", "labels": [], "entities": []}, {"text": "Therefore, we eliminated all parallel phrases that occur fewer than five times.", "labels": [], "entities": []}, {"text": "As a result of this process, we obtain 6,139 parallel short phrases in Arabic, Hebrew, Aramaic, and English.", "labels": [], "entities": []}, {"text": "The average number of morphemes per word in the Hebrew data is 1.8 and is 1.7 in Arabic.", "labels": [], "entities": []}, {"text": "For the bilingual models which employs probabilistic string-edit distance as a prior on abstract morphemes, we parameterize the string-edit model with the chart of Semitic consonant relationships listed on page xxiv of.", "labels": [], "entities": []}, {"text": "All pairs of corresponding letters are given equal substitution probability, while all other letter pairs are given substitution probability of zero.", "labels": [], "entities": []}, {"text": "Evaluation Methods Following previous work, we evaluate the performance of our automatic segmentation algorithm using F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9704365730285645}]}, {"text": "This measure is the harmonic mean of recall and precision, which are calculated on the basis of all possible segmentation points.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9990905523300171}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9994266033172607}]}, {"text": "The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase.", "labels": [], "entities": []}, {"text": "During testing, we do not allow the models to consider any multilingual evidence.", "labels": [], "entities": []}, {"text": "This restriction allows us to simulate future performance on purely monolingual data.", "labels": [], "entities": []}, {"text": "Baselines Our primary purpose is to compare the performance of our bilingual model with its fully monolingual counterpart.", "labels": [], "entities": []}, {"text": "However, to demonstrate the competitiveness of this baseline model, we also provide results using MORFESSOR), a state-of-the-art unsupervised system for morphological segmentation.", "labels": [], "entities": [{"text": "MORFESSOR", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.8637192249298096}, {"text": "morphological segmentation", "start_pos": 153, "end_pos": 179, "type": "TASK", "confidence": 0.743579089641571}]}, {"text": "While developed originally for Finnish, this system has been successfully applied to a range of languages including German, Turkish and English.", "labels": [], "entities": []}, {"text": "The probabilistic formulation of this model is close to our monolingual segmentation model, but it uses a greedy search specifically designed for the segmentation task.", "labels": [], "entities": []}, {"text": "We use the publicly available implementation of this system.", "labels": [], "entities": []}, {"text": "To provide some idea of the inherent difficulty of this segmentation task, we also provide results from a random baseline which makes segmentation decisions based on a coin weighted with the true segmentation frequency.", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9000050127506256}]}, {"text": "shows the performance of the various automatic segmentation methods.", "labels": [], "entities": []}, {"text": "The first three rows provide baselines, as mentioned in the previous section.", "labels": [], "entities": [{"text": "baselines", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9588464498519897}]}, {"text": "Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F . The next three rows shows the performance of various bilingual models that don't use character-tocharacter phonetic correspondences to capture cognate information.", "labels": [], "entities": [{"text": "MONOLINGUAL", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9804368615150452}]}, {"text": "We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLIN-GUAL.", "labels": [], "entities": [{"text": "HEBREW", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9880397915840149}, {"text": "ARAMAIC", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.7879719138145447}, {"text": "MONOLIN-GUAL", "start_pos": 112, "end_pos": 124, "type": "METRIC", "confidence": 0.736595094203949}]}, {"text": "We notice that in general, adding Englishwhich has comparatively little morphological ambiguity -is about as useful as adding a more closely related Semitic language.", "labels": [], "entities": []}, {"text": "However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONO-LINGUAL by 10% and 24% for the Hebrew/Arabic pair.", "labels": [], "entities": [{"text": "error", "start_pos": 206, "end_pos": 211, "type": "METRIC", "confidence": 0.529792308807373}]}], "tableCaptions": [{"text": " Table 1: Precision, recall and F-score evaluated on Arabic and Hebrew. The first three rows provide baselines (random  selection, an alternative state-of-the-art system, and the monolingual version of our model). The next three rows show  the result of our bilingual model when one of Arabic, Hebrew, Aramaic, or English is added. The final two rows  show the result of the bilingual model when character-to-character phonetic correspondences are used in the abstract  morpheme prior.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9974562525749207}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992654919624329}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9992495179176331}]}]}