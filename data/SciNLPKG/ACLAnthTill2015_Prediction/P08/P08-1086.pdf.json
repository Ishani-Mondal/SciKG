{"title": [{"text": "Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation", "labels": [], "entities": [{"text": "Large Scale Class-Based Language Modeling in Machine Translation", "start_pos": 32, "end_pos": 96, "type": "TASK", "confidence": 0.6994324512779713}]}], "abstractContent": [{"text": "In statistical language modeling, one technique to reduce the problematic effects of data spar-sity is to partition the vocabulary into equivalence classes.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.7715569337209066}]}, {"text": "In this paper we investigate the effects of applying such a technique to higher-order n-gram models trained on large corpora.", "labels": [], "entities": []}, {"text": "We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications for large vocabularies (>1 million words) using such large training corpora (>30 billion tokens).", "labels": [], "entities": [{"text": "exchange clustering", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7066737711429596}]}, {"text": "The resulting clusterings are then used in training partially class-based language models.", "labels": [], "entities": []}, {"text": "We show that combining them with word-based n-gram models in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.6415728231271108}, {"text": "BLEU score", "start_pos": 208, "end_pos": 218, "type": "METRIC", "confidence": 0.9558283090591431}]}], "introductionContent": [{"text": "A statistical language model assigns a probability P (w) to any given string of words w m 1 = w 1 , ..., w m . In the case of n-gram language models this is done by factoring the probability: and making a Markov assumption by approximating this by: Even after making the Markov assumption and thus treating all strings of preceding words as equal which * Parts of this research were conducted while the author studied at the Berlin Institute of Technology do not differ in the last n \u2212 1 words, one problem ngram language models suffer from is that the training data is too sparse to reliably estimate all conditional probabilities P (w i |w i\u22121 1 ).", "labels": [], "entities": []}, {"text": "Class-based n-gram models are intended to help overcome this data sparsity problem by grouping words into equivalence classes rather than treating them as distinct words and thus reducing the number of parameters of the model ().", "labels": [], "entities": []}, {"text": "They have often been shown to improve the performance of speech recognition systems when combined with word-based language models).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7396062910556793}]}, {"text": "However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.683560291926066}]}, {"text": "Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams, and even despite the increasing size and decreasing sparsity of language model training corpora (, class-based n-gram models might lead to improvements when increasing the n-gram order.", "labels": [], "entities": []}, {"text": "When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification.", "labels": [], "entities": [{"text": "word classification", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7091041058301926}]}, {"text": "Most often, variants of the exchange algorithm ( or the agglomerative clustering algorithm presented in () are used, both of which have prohibitive runtimes when clustering large vocabularies on the basis of large training corpora with a sufficiently high number of classes.", "labels": [], "entities": []}, {"text": "In this paper we introduce a modification of the exchange algorithm with improved efficiency and then present a distributed version of the modified algorithm, which makes it feasible to obtain word clas-sifications using billions of tokens of training data.", "labels": [], "entities": []}, {"text": "We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 169, "end_pos": 200, "type": "TASK", "confidence": 0.6480216185251871}]}], "datasetContent": [{"text": "We trained a number of predictive class-based language models on different Arabic and English corpora using clusterings trained on the complete data of the same corpus.", "labels": [], "entities": []}, {"text": "We use the distributed training and application infrastructure described in) with modifications to allow the training of predictive class-based models and their application in the decoder of the machine translation system.", "labels": [], "entities": []}, {"text": "For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff (.", "labels": [], "entities": []}, {"text": "Models with Stupid Backoff return scores rather than normalized probabilities, thus perplexities cannot be calculated for these models.", "labels": [], "entities": [{"text": "Stupid Backoff return scores", "start_pos": 12, "end_pos": 40, "type": "METRIC", "confidence": 0.8126617670059204}]}, {"text": "Instead we report BLEU scores () of the machine translation system using different combinations of word-and classbased models for translation tasks from English to Arabic and Arabic to English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9988937973976135}, {"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7392011880874634}]}], "tableCaptions": [{"text": " Table 1: BLEU scores of the Arabic English system using  models trained on the English en target data set", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992526173591614}, {"text": "English en target data set", "start_pos": 80, "end_pos": 106, "type": "DATASET", "confidence": 0.746266633272171}]}, {"text": " Table 2: BLEU scores of the Arabic English system using  models trained on various data sets", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992409944534302}]}, {"text": " Table 3: BLEU scores of the English Arabic system using  models trained on various data sets", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991864562034607}]}]}