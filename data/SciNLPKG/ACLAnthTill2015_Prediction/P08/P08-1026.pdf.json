{"title": [{"text": "Regular tree grammars as a formalism for scope underspecification", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose the use of regular tree grammars (RTGs) as a formalism for the underspecified processing of scope ambiguities.", "labels": [], "entities": []}, {"text": "By applying standard results on RTGs, we obtain a novel algorithm for eliminating equivalent readings and the first efficient algorithm for computing the best reading of a scope ambiguity.", "labels": [], "entities": []}, {"text": "We also show how to derive RTGs from more traditional underspecified descriptions.", "labels": [], "entities": [{"text": "RTGs", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9621188044548035}]}], "introductionContent": [{"text": "Underspecification) has become the standard approach to dealing with scope ambiguity in large-scale hand-written grammars (see e.g. Copestake and Flickinger).", "labels": [], "entities": []}, {"text": "The key idea behind underspecification is that the parser avoids computing all scope readings.", "labels": [], "entities": []}, {"text": "Instead, it computes a single compact underspecified description for each parse.", "labels": [], "entities": []}, {"text": "One can then strengthen the underspecified description to efficiently eliminate subsets of readings that were not intended in the given context); so when the individual readings are eventually computed, the number of remaining readings is much smaller and much closer to the actual perceived ambiguity of the sentence.", "labels": [], "entities": []}, {"text": "In the past few years, a \"standard model\" of scope underspecification has emerged: A range of formalisms from Underspecified DRT to dominance graphs ( have offered mechanisms to specify the \"semantic material\" of which the semantic representations are built up, plus dominance or outscoping relations between these building blocks.", "labels": [], "entities": []}, {"text": "This has been a very successful approach, but recent algorithms for eliminating subsets of readings have pushed the expressive power of these formalisms to their limits; for instance, speculate that further improvements over their (incomplete) redundancy elimination algorithm require a more expressive formalism than dominance graphs.", "labels": [], "entities": []}, {"text": "On the theoretical side, has shown that none of the major underspecification formalisms are expressively complete, i.e. supports the description of an arbitrary subset of readings.", "labels": [], "entities": []}, {"text": "Furthermore, the somewhat implicit nature of dominance-based descriptions makes it difficult to systematically associate readings with probabilities or costs and then compute a best reading.", "labels": [], "entities": []}, {"text": "In this paper, we address both of these shortcomings by proposing regular tree grammars (RTGs) as a novel underspecification formalism.", "labels": [], "entities": []}, {"text": "Regular tree grammars () area standard approach for specifying sets of trees in theoretical computer science, and are closely related to regular tree transducers as used e.g. in recent work on statistical MT) and grammar formalisms).", "labels": [], "entities": [{"text": "MT", "start_pos": 205, "end_pos": 207, "type": "TASK", "confidence": 0.6625307202339172}]}, {"text": "We show that the \"dominance charts\" proposed by can be naturally seen as regular tree grammars; using their algorithm, classical underspecified descriptions (dominance graphs) can be translated into RTGs that describe the same sets of readings.", "labels": [], "entities": []}, {"text": "However, RTGs are trivially expressively complete because every finite tree language is also regular.", "labels": [], "entities": []}, {"text": "We exploit this increase inexpressive power in presenting a novel redundancy elimination algorithm that is simpler and more powerful than the one by; in our algorithm, redundancy elimination amounts to intersection of regular tree languages.", "labels": [], "entities": [{"text": "redundancy elimination", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7818390727043152}, {"text": "redundancy elimination", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.7235142290592194}]}, {"text": "Furthermore, we show how to define a PCFG-style cost model on RTGs and compute best readings of deterministic RTGs efficiently, and illustrate this model on a machine learning based model of scope preferences.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first efficient algorithm for computing best readings of a scope ambiguity in the literature.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we will first sketch the existing standard approach to underspecification.", "labels": [], "entities": []}, {"text": "We will then define regular tree grammars and show how to see them as an underspecification formalism in Section 3.", "labels": [], "entities": []}, {"text": "We will present the new redundancy elimination algorithm, based on language intersection, in Section 4, and show how to equip RTGs with weights and compute best readings in Section 5.", "labels": [], "entities": [{"text": "redundancy elimination", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7912478744983673}, {"text": "language intersection", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.6967156231403351}]}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The algorithm presented here is not only more transparent than KT06, but also more powerful; for example, it will reduce the graph in of completely, whereas KT06 won't.", "labels": [], "entities": []}, {"text": "To measure the extent to which the new algorithm improves upon KT06, we compare both algorithms on the USRs in the Rondane treebank (version of January 2006).", "labels": [], "entities": [{"text": "KT06", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8518512845039368}, {"text": "USRs", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.9552794694900513}, {"text": "Rondane treebank (version of January 2006)", "start_pos": 115, "end_pos": 157, "type": "DATASET", "confidence": 0.9124142080545425}]}, {"text": "The Rondane treebank is a \"Redwoods style\" treebank () containing MRS-based underspecified representations for sentences from the tourism domain, and is distributed together with the English Resource Grammar (ERG)).", "labels": [], "entities": [{"text": "Rondane treebank", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8440554440021515}, {"text": "Redwoods style\" treebank", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.7419972717761993}, {"text": "English Resource Grammar (ERG))", "start_pos": 183, "end_pos": 214, "type": "DATASET", "confidence": 0.8288094600041708}]}, {"text": "The treebank contains 999 MRS-nets, which we translate automatically into dominance graphs and further into RTGs; the median number of scope readings per sentence is 56.", "labels": [], "entities": []}, {"text": "For our experiment, we consider all 950 MRS-nets with less than 650 000 configurations.", "labels": [], "entities": [{"text": "MRS-nets", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.811553955078125}]}, {"text": "We use a slightly weaker version of the rewrite system that used in their evaluation.", "labels": [], "entities": []}, {"text": "It turns out that the median number of equivalence classes, computed by pairwise comparison of all configurations, is 8.", "labels": [], "entities": []}, {"text": "The median number of configurations that remain after running our algorithm is also 8.", "labels": [], "entities": []}, {"text": "By contrast, the median number after running KT06 is 11.", "labels": [], "entities": [{"text": "KT06", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.758508026599884}]}, {"text": "For a more fine-grained comparison, shows the percentage of USRs for which the two algorithms achieve complete reduction, i.e. retain only one reading per equivalence class.", "labels": [], "entities": []}, {"text": "In the diagram, we have grouped USRs according to the natural logarithm of their numbers of configurations, and report the percentage of USRs in this group on which the algorithms were complete.", "labels": [], "entities": []}, {"text": "The new algorithm dramatically outperforms KT06: In total, it reduces 96% of all USRs completely, whereas KT06 was complete only for 40%.", "labels": [], "entities": []}, {"text": "This increase in completeness is partially due to the new algorithm's ability to use non-chart RTGs: For 28% of the sentences, it computes RTGs that are not dominance charts.", "labels": [], "entities": []}, {"text": "KT06 was only able to reduce 5 of these 263 graphs completely.", "labels": [], "entities": [{"text": "KT06", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9267361164093018}]}, {"text": "The algorithm needs 25 seconds to run for the entire corpus (old algorithm: 17 seconds), and it would take 50 (38) more seconds to run on the 49 large USRs that we exclude from the experiment.", "labels": [], "entities": []}, {"text": "By contrast, it takes about 7 hours to compute the equivalence classes by pairwise comparison, and it would take an estimated several billion years to compute the equivalence classes of the excluded USRs.", "labels": [], "entities": [{"text": "USRs", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.93485027551651}]}, {"text": "In short, the redundancy elimination algorithm presented here achieves nearly complete reduction at a tiny fraction of the runtime, and makes a useful task that was completely infeasible before possible.", "labels": [], "entities": [{"text": "redundancy elimination", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.78350830078125}]}], "tableCaptions": []}