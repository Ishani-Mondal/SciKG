{"title": [{"text": "Event Matching Using the Transitive Closure of Dependency Relations", "labels": [], "entities": [{"text": "Event Matching", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7200829386711121}]}], "abstractContent": [{"text": "This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations.", "labels": [], "entities": []}, {"text": "The method yields a model capable of matching events with an F-measure of 66.5%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.999457061290741}]}], "introductionContent": [{"text": "Question answering systems are evolving from their roots as factoid or definitional answering systems to systems capable of answering much more openended questions.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9032705426216125}, {"text": "factoid or definitional answering", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.6521844789385796}]}, {"text": "For example, it is one thing to ask for the birthplace of a person, but it is quite another to ask for all locations visited by a person over a specific period of time.", "labels": [], "entities": []}, {"text": "Queries may contain several types of arguments: person, organization, country, location, etc.", "labels": [], "entities": []}, {"text": "By far, however, the most challenging of the argument types are the event or topic arguments, where the argument text can be a noun phrase, a participial verb phrase or an entire indicative clause.", "labels": [], "entities": []}, {"text": "For example, the following are all possible event arguments: \u2022 the U.S. invasion of Iraq \u2022 Red Cross admitting Israeli and Palestinian groups \u2022 GM offers buyouts to union employees In this paper, we describe a method to match an event query argument to the sentences that mention that event.", "labels": [], "entities": []}, {"text": "That is, we seek to model p(s contains e | s, e), where e is a textual description of an event (such as an event argument fora GALE distillation query) and where sis an arbitrary sentence.", "labels": [], "entities": [{"text": "GALE distillation query", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.510808527469635}]}, {"text": "In the first example above, \"the U.S. invasion of Iraq\", such a model should produce a very high score for that event description and the sentence \"The U.S. invaded Iraq in 2003.\"", "labels": [], "entities": []}], "datasetContent": [{"text": "We created 159 queries to test this model framework.", "labels": [], "entities": []}, {"text": "We adapted a publicly-available search engine (citation omitted) to retrieve documents automatically from the GALE corpus likely to be relevant to the event queries, and then used a set of simple heuristics-a subset of the lowlevel features described in \u00a72-to retrieve sentences that were more likely than not to be relevant.", "labels": [], "entities": [{"text": "GALE corpus", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.8383707106113434}]}, {"text": "We then had our most experienced annotator annotate sentences with five possible tags: relevant, irrelevant, relevant-in-context, irrelevant-in-context and garbage (to deal with sentences that were unintelligible \"word salad\").", "labels": [], "entities": []}, {"text": "Crucially, the annotation guidelines for this task were that an event had to be explicitly mentioned in a sentence in order for that sentence to be tagged relevant.", "labels": [], "entities": []}, {"text": "We separated the data roughly into an 80/10/10 split for training, devtest and test.", "labels": [], "entities": []}, {"text": "We then trained our event-matching model solely on the examples marked relevant or irrelevant, of which there were 3546 instances.", "labels": [], "entities": []}, {"text": "For all the experiments reported, we tested on our development test set, which comprised 465 instances that had been marked relevant or irrelevant.", "labels": [], "entities": [{"text": "development test set", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.7485831578572592}]}, {"text": "We trained the kernel version of an averaged perceptron model), using a polynomial kernel with degree 4 and additive term 1.", "labels": [], "entities": []}, {"text": "As a baseline, we trained and tested a model using only the lexical-matching features.", "labels": [], "entities": []}, {"text": "We then trained and tested models using only the low-level features and all features.", "labels": [], "entities": []}, {"text": "shows the performance statistics of all three models, and shows the ROC curves of these models.", "labels": [], "entities": [{"text": "ROC", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9932818412780762}]}, {"text": "Clearly, the dependency features help; at our normal operating point of 0, F-measure rises from 62.2 to 66.5.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9963921904563904}]}, {"text": "Looking solely  In example features, x \u2208 {m, s}, depending on whether the dependency match was due to \"morph-or-mention\" matching or synonym matching.", "labels": [], "entities": []}, {"text": "at pairs of predictions, McNemar's test reveals differences (p 0.05) between the predictions of the baseline model and the other two models, but not between those of the low-level model and the model trained with all features.", "labels": [], "entities": []}], "tableCaptions": []}