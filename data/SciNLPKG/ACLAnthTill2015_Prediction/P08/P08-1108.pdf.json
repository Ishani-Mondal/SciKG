{"title": [{"text": "Integrating Graph-Based and Transition-Based Dependency Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7417438328266144}]}, {"text": "In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9829437136650085}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9293941855430603}]}, {"text": "By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9988522529602051}]}], "introductionContent": [{"text": "Syntactic dependency graphs have recently gained a wide interest in the natural language processing community and have been used for many problems ranging from machine translation () to ontology construction (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.8187562525272369}, {"text": "ontology construction", "start_pos": 186, "end_pos": 207, "type": "TASK", "confidence": 0.7979990839958191}]}, {"text": "A dependency graph fora sentence represents each word and its syntactic dependents through labeled directed arcs, as shown in.", "labels": [], "entities": []}, {"text": "One advantage of this representation is that it extends naturally to discontinuous constructions, which arise due to long distance dependencies or in languages where syntactic structure is encoded in morphology rather than in word order.", "labels": [], "entities": []}, {"text": "This is undoubtedly one of the reasons for the emergence of dependency parsers fora wide range of languages.", "labels": [], "entities": []}, {"text": "Many of these parsers are based on data-driven parsing models, which learn to produce dependency graphs for sentences solely from an annotated corpus and can be easily ported to any language or domain in which annotated resources exist.", "labels": [], "entities": []}, {"text": "Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased . In graph-based parsing, we learn a model for scoring possible dependency graphs fora given sentence, typically by factoring the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8366838097572327}, {"text": "parsing", "start_pos": 332, "end_pos": 339, "type": "TASK", "confidence": 0.9654432535171509}]}, {"text": "This type of model has been used by, among others, Eisner (1996),.", "labels": [], "entities": []}, {"text": "In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph.", "labels": [], "entities": []}, {"text": "This approach is represented, for example, by the models of,, and.", "labels": [], "entities": []}, {"text": "Theoretically, these approaches are very different.", "labels": [], "entities": []}, {"text": "The graph-based models are globally trained and use exact inference algorithms, but define features over a limited history of parsing decisions.", "labels": [], "entities": []}, {"text": "The transitionbased models are essentially the opposite.", "labels": [], "entities": []}, {"text": "They use local training and greedy inference algorithms, but define features over a rich history of parsing decisions.", "labels": [], "entities": []}, {"text": "This is a fundamental trade-off that is hard to overcome by tractable means.", "labels": [], "entities": []}, {"text": "Both models have been used to achieve state-of-the-art accuracy fora wide range of languages, as shown in the CoNLL shared tasks on dependency parsing, but  showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.996942937374115}, {"text": "dependency parsing", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.6723216325044632}]}, {"text": "In this paper, we consider a simple way of integrating graph-based and transition-based models in order to exploit their complementary strengths and thereby improve parsing accuracy beyond what is possible by either model in isolation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 165, "end_pos": 172, "type": "TASK", "confidence": 0.972356379032135}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9340877532958984}]}, {"text": "The method integrates the two models by allowing the output of one model to define features for the other.", "labels": [], "entities": []}, {"text": "This method is simple -requiring only the definition of new features -and robust by allowing a model to learn relative to the predictions of the other.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present an experimental evaluation of the two guided models based on data from the CoNLL-X shared task, followed by a comparative error analysis including both the base models and the guided models.", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.7767176429430643}]}, {"text": "The data for the experiments are training and test sets for all thirteen languages from the CoNLL-X shared task on multilingual dependency parsing with training sets ranging in size from from 29,000 tokens (Slovene) to 1,249,000 tokens (Czech).", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.5563468337059021}]}, {"text": "The test sets are all standardized to about 5,000 tokens each.", "labels": [], "entities": []}, {"text": "For more information on the data sets, see.", "labels": [], "entities": []}, {"text": "The guided models were trained according to the scheme explained in section 3, with two-fold crossvalidation when parsing the training data with the guide parsers.", "labels": [], "entities": []}, {"text": "Preliminary experiments suggested that cross-validation with more folds had a negligible impact on the results.", "labels": [], "entities": []}, {"text": "Models are evaluated by their labeled attachment score (LAS) on the test set, i.e., the percentage of tokens that are assigned both the correct head and the correct label, using the evaluation software from the CoNLL-X shared task with default settings.", "labels": [], "entities": [{"text": "labeled attachment score (LAS)", "start_pos": 30, "end_pos": 60, "type": "METRIC", "confidence": 0.85817551612854}]}, {"text": "German, Portuguese and Slovene.", "labels": [], "entities": []}, {"text": "Finally, given that the two base models had the previously best performance for these data sets, the guided models achieve a substantial improvement of the state of the art.", "labels": [], "entities": []}, {"text": "While there is no statistically significant difference between the two base models, they are both outperformed by Malt MST (p < 0.0001), which in turn has significantly lower accuracy than MST Malt (p < 0.0005).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9989922642707825}]}], "tableCaptions": [{"text": " Table 2: Labeled attachment scores for base parsers and  guided parsers (improvement in percentage points).", "labels": [], "entities": [{"text": "Labeled attachment scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7568080425262451}]}, {"text": " Table 3: Accuracy relative to dependent part of speech  (improvement in percentage points).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993081092834473}]}]}