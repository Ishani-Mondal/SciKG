{"title": [], "abstractContent": [{"text": "We present an automatic approach to determining whether a pronoun in text refers to a preceding noun phrase or is instead non-referential.", "labels": [], "entities": []}, {"text": "We extract the surrounding tex-tual context of the pronoun and gather, from a large corpus, the distribution of words that occur within that context.", "labels": [], "entities": []}, {"text": "We learn to reliably classify these distributions as representing either referential or non-referential pronoun instances.", "labels": [], "entities": []}, {"text": "Despite its simplicity, experimental results on classifying the English pronoun it show the system achieves the highest performance yet attained on this important task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of coreference resolution is to determine which noun phrases in a document refer to the same real-world entity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9743464887142181}]}, {"text": "As part of this task, coreference resolution systems must decide which pronouns refer to preceding noun phrases (called antecedents) and which do not.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9475025832653046}]}, {"text": "In particular, a long-standing challenge has been to correctly classify instances of the English pronoun it.", "labels": [], "entities": []}, {"text": "Consider the sentences: (1) You can make it in advance.", "labels": [], "entities": []}, {"text": "(2) You can make it in Hollywood.", "labels": [], "entities": []}, {"text": "In sentence (1), it is an anaphoric pronoun referring to some previous noun phrase, like \"the sauce\" or \"an appointment.\"", "labels": [], "entities": []}, {"text": "In sentence (2), it is part of the idiomatic expression \"make it\" meaning \"succeed.\"", "labels": [], "entities": []}, {"text": "A coreference resolution system should find an antecedent for the first it but not the second.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.9401607513427734}]}, {"text": "Pronouns that do not refer to preceding noun phrases are called non-anaphoric or non-referential pronouns.", "labels": [], "entities": []}, {"text": "The word it is one of the most frequent words in the English language, accounting for about 1% of tokens in text and over a quarter of all third-person pronouns.", "labels": [], "entities": []}, {"text": "Usually between a quarter and a half of it instances are non-referential (e.g. Section 4, Table 3).", "labels": [], "entities": []}, {"text": "As with other pronouns, the preceding discourse can affect it's interpretation.", "labels": [], "entities": []}, {"text": "For example, sentence (2) can be interpreted as referential if the preceding sentence is \"You want to make a movie?\"", "labels": [], "entities": []}, {"text": "We show, however, that we can reliably classify a pronoun as being referential or non-referential based solely on the local context surrounding the pronoun.", "labels": [], "entities": []}, {"text": "We do this by turning the context into patterns and enumerating all the words that can take the place of it in these patterns.", "labels": [], "entities": []}, {"text": "For sentence (1), we can extract the context pattern \"make * in advance\" and for sentence (2) \"make * in Hollywood,\" where \"*\" is a wildcard that can be filled by any token.", "labels": [], "entities": []}, {"text": "Nonreferential distributions tend to have the word it filling the wildcard position.", "labels": [], "entities": []}, {"text": "Referential distributions occur with many other noun phrase fillers.", "labels": [], "entities": [{"text": "noun phrase fillers", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6753223737080892}]}, {"text": "For example, in our n-gram collection (Section 3.4), \"make it in advance\" and \"make them in advance\" occur roughly the same number of times, indicating a referential pattern.", "labels": [], "entities": []}, {"text": "In contrast, \"make it in Hollywood\" occurs 3421 times while \"make them in Hollywood\" does not occur at all.", "labels": [], "entities": []}, {"text": "These simple counts strongly indicate whether another noun can replace the pronoun.", "labels": [], "entities": []}, {"text": "Thus we can computationally distinguish between a) pronouns that refer to nouns, and b) all other instances: including those that have no antecedent, like sentence (2), and those that refer to sentences, clauses, or implied topics of discourse.", "labels": [], "entities": []}, {"text": "Beyond the practical value of this distinction, Section 3 provides some theoretical justification for our binary classification.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7308700084686279}]}, {"text": "Section 3 also shows how to automatically extract and collect counts for context patterns, and how to combine the information using a machine learned classifier.", "labels": [], "entities": []}, {"text": "Section 4 describes our data for learning and evaluation, It-Bank: a set of over three thousand labelled instances of the pronoun it from a variety of text sources.", "labels": [], "entities": []}, {"text": "Section 4 also explains our comparison approaches and experimental methodology.", "labels": [], "entities": []}, {"text": "Section 5 presents our results, including an interesting comparison of our system to human classification given equivalent segments of context.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow's evaluation criteria.", "labels": [], "entities": []}, {"text": "Precision (P) is the proportion of instances that we label as non-referential that are indeed non-referential.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.968629002571106}]}, {"text": "Recall (R) is the proportion of true non-referentials that we detect, and is thus a measure of the coverage Our approach, on the other hand, would seem to be susceptible to such intervening material, if it pushes indicative context tokens out of the 5-token window.  of the system.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9689502567052841}]}, {"text": "F-Score (F) is the geometric average of precision and recall; it is the most common nonreferential detection metric.", "labels": [], "entities": [{"text": "F-Score (F)", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9632756859064102}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9989292025566101}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9975416660308838}]}, {"text": "Accuracy (Acc) is the percentage of instances labelled correctly.", "labels": [], "entities": [{"text": "Accuracy (Acc)", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9213094115257263}]}, {"text": "gives precision, recall, F-score, and accuracy on the Train/Test split.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9997765421867371}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9997527003288269}, {"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9994184970855713}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9997978806495667}, {"text": "Train/Test split", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.91408471763134}]}, {"text": "Note that while the LL system has high detection precision, it has very low recall, sharply reducing F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.8422305583953857}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9993473887443542}, {"text": "F-score", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.997880220413208}]}, {"text": "The MINIPL approach sacrifices some precision for much higher recall, but again has fairly low F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9994109869003296}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9996627569198608}, {"text": "F-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9989086389541626}]}, {"text": "To our knowledge, our COMBO system, with an F-Score of 77.1%, achieves the highest performance of any non-referential system yet implemented.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9993358254432678}]}, {"text": "Even more importantly, DISTRIB, which requires only minimal linguistic processing and no encoding of specific indicator patterns, achieves 75.8% F-Score.", "labels": [], "entities": [{"text": "DISTRIB", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.8526878356933594}, {"text": "F-Score", "start_pos": 145, "end_pos": 152, "type": "METRIC", "confidence": 0.9981517195701599}]}, {"text": "The difference between COMBO and DISTRIB is not statistically significant, while both are significantly better than the rule-based approaches.", "labels": [], "entities": [{"text": "COMBO", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.5746749043464661}]}, {"text": "8 This provides strong motivation fora \"light-weight\" approach to non-referential it detection -one that does not require parsing or hand-crafted rules and -is easily ported to new languages and text domains.", "labels": [], "entities": [{"text": "non-referential it detection", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7047869563102722}]}], "tableCaptions": [{"text": " Table 2: 5-gram context patterns and pattern-filler counts  for the Section 3.2 example.", "labels": [], "entities": []}, {"text": " Table 3: Data sets used in experiments.", "labels": [], "entities": []}, {"text": " Table 4: Train/Test-split performance (%).", "labels": [], "entities": []}, {"text": " Table 5: 10-fold cross validation F-Score (%).", "labels": [], "entities": [{"text": "F-Score", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9628472328186035}]}, {"text": " Table 6: Evaluation on Test-200 (%).", "labels": [], "entities": []}]}