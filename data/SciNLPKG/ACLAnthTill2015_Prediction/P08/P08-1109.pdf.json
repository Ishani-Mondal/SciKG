{"title": [{"text": "Efficient, Feature-based, Conditional Random Field Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by gen-erative methods.", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.7353350073099136}]}, {"text": "While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first general, feature-rich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data.", "labels": [], "entities": [{"text": "WSJ parsing data", "start_pos": 284, "end_pos": 300, "type": "DATASET", "confidence": 0.8421566883722941}]}, {"text": "Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering.", "labels": [], "entities": []}, {"text": "On WSJ15, we attain a state-of-the-art F-score of 90.9%, a 14% relative reduction in error over previous models, while being two orders of magnitude faster.", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8513471484184265}, {"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9995887875556946}, {"text": "error", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.6952465772628784}]}, {"text": "On sentences of length 40, our system achieves an F-score of 89.0%, a 36% relative reduction in error over a gener-ative baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9996084570884705}, {"text": "error", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.784797728061676}]}], "introductionContent": [{"text": "Over the past decade, feature-based discriminative models have become the tool of choice for many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.7014523297548294}]}, {"text": "Although they take much longer to train than generative models, they typically produce higher performing systems, in large part due to the ability to incorporate arbitrary, potentially overlapping features.", "labels": [], "entities": []}, {"text": "However, constituency parsing remains an area dominated by generative methods, due to the computational complexity of the problem.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.9408925473690033}, {"text": "generative", "start_pos": 59, "end_pos": 69, "type": "TASK", "confidence": 0.9674340486526489}]}, {"text": "Previous work on discriminative parsing falls under one of three approaches.", "labels": [], "entities": [{"text": "discriminative parsing", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6741869449615479}]}, {"text": "One approach does discriminative reranking of the n-best list of a generative parser, still usually depending highly on the generative parser score as a feature.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8890421092510223}]}, {"text": "A second group of papers does parsing by a sequence of independent, discriminative decisions, either greedily or with use of a small beam.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9836233258247375}]}, {"text": "This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse.", "labels": [], "entities": []}, {"text": "Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times.", "labels": [], "entities": []}, {"text": "One exception is the recent work of, who discriminatively train a grammar with latent variables and do not restrict themselves to short sentences.", "labels": [], "entities": []}, {"text": "However their model, like the discriminative parser of, makes no use of features, and effectively ignores the largest advantage of discriminative training.", "labels": [], "entities": []}, {"text": "It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering.", "labels": [], "entities": [{"text": "generative training", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.9223747849464417}]}, {"text": "For example, in (), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.6814587712287903}, {"text": "error", "start_pos": 199, "end_pos": 204, "type": "METRIC", "confidence": 0.9866489768028259}]}, {"text": "When they add in only a small set of orthographic features, their CRF error rate drops considerably more to 4.3%, and their out-of-vocabulary error rate drops by more than half.", "labels": [], "entities": [{"text": "CRF error rate", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.6912270585695902}, {"text": "out-of-vocabulary error rate", "start_pos": 124, "end_pos": 152, "type": "METRIC", "confidence": 0.6657214164733887}]}, {"text": "This is further supported by, who saw no parsing gains when switch-ing from generative to discriminative training, and by who saw only small gains of around 0.7% for their final model when switching training methods.", "labels": [], "entities": [{"text": "generative to discriminative training", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.860849529504776}]}, {"text": "In this work, we provide just such a framework for training a feature-rich discriminative parser.", "labels": [], "entities": []}, {"text": "Unlike previous work, we do not restrict ourselves to short sentences, but we do provide results both for training and testing on sentences of length \u2264 15 (WSJ15) and for training and testing on sentences of length \u2264 40, allowing previous WSJ15 results to be put in context with respect to most modern parsing literature.", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.692570686340332}]}, {"text": "Our model is a conditional random field based model.", "labels": [], "entities": []}, {"text": "For a rule application, we allow arbitrary features to be defined over the rule categories, span and split point indices, and the words of the sentence.", "labels": [], "entities": []}, {"text": "It is well known that constituent length influences parse probability, but PCFGs cannot easily take this information into account.", "labels": [], "entities": []}, {"text": "Another benefit of our feature based model is that it effortlessly allows smoothing over previously unseen rules.", "labels": [], "entities": []}, {"text": "While the rule maybe novel, it will likely contain features which are not.", "labels": [], "entities": []}, {"text": "Practicality comes from three sources.", "labels": [], "entities": [{"text": "Practicality", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9332336783409119}]}, {"text": "We made use of stochastic optimization methods which allow us to find optimal model parameters with very few passes through the data.", "labels": [], "entities": []}, {"text": "We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS.", "labels": [], "entities": []}, {"text": "We also used limited parallelization, and prefiltering of the chart to avoid scoring rules which cannot tile into complete parses of the sentence.", "labels": [], "entities": []}, {"text": "This speed-up does not come with a performance cost; we attain an F-score of 90.9%, a 14% relative reduction in errors over previous work on WSJ15.", "labels": [], "entities": [{"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9997021555900574}, {"text": "errors", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9891306161880493}, {"text": "WSJ15", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.9562326073646545}]}], "datasetContent": [{"text": "Note that this property is satisfied, without scaling, for objective functions that sum over the training data, as it is in our case, but any priors must be scaled down by a factor of b/ |D|.", "labels": [], "entities": []}, {"text": "The stochastic gradient, \u2207L (D (i) b ; \u03b8 ), is then simply the derivative of the stochastic function value.", "labels": [], "entities": []}, {"text": "For both WSJ15 and WSJ40, we trained a generative model; a discriminative model, which used lexicon features, but no grammar features other than the rules themselves; and a feature-based model which had access to all features.", "labels": [], "entities": [{"text": "WSJ15", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.9452431797981262}, {"text": "WSJ40", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.8721886873245239}]}, {"text": "For the length 15 data we also did experiments in which we relaxed the grammar.", "labels": [], "entities": []}, {"text": "By this we mean that we added (previously unseen) rules to the grammar, as a means of smoothing.", "labels": [], "entities": []}, {"text": "We chose which rules to add by taking existing rules and modifying the parent annotation on the parent of the rule.", "labels": [], "entities": []}, {"text": "We used stochastic gradient descent for: Lexicon and grammar features.", "labels": [], "entities": []}, {"text": "w is the word and t the tag.", "labels": [], "entities": []}, {"text": "r represents a particular rule along with span/split information; \u03c1 is the rule itself, r p is the parent of the rule; w b , w s , and we are the first, first after the split (for binary rules) and last word that a rule spans in a particular context.", "labels": [], "entities": []}, {"text": "All states, including the POS tags, are annotated with parent information; b(s) represents the base label fora state sand p(s) represents the parent annotation on state s. ds(w) represents the distributional similarity cluster, and lc(w) the lower cased version of the word, and unk(w) the unknown word class.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Development and test set results, training and testing on sentences of length \u2264 15 from the Penn treebank.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9959417879581451}]}, {"text": " Table 4: Test set results, training on sentences of length \u2264 40 from the Penn treebank. The generative-all results were  trained on all sentences regardless of length", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9964918494224548}]}]}