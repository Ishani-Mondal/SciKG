{"title": [{"text": "Combining Source and Target Language Information for Name Tagging of Machine Translation Output", "labels": [], "entities": [{"text": "Name Tagging of Machine Translation Output", "start_pos": 53, "end_pos": 95, "type": "TASK", "confidence": 0.7299693326155344}]}], "abstractContent": [{"text": "A Named Entity Recognizer (NER) generally has worse performance on machine translated text, because of the poor syntax of the MT output and other errors in the translation.", "labels": [], "entities": []}, {"text": "As some tagging distinctions are clearer in the source, and some in the target, we tried to integrate the tag information from both source and target to improve target language tagging performance, especially recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.9956091046333313}]}, {"text": "In our experiments with Chinese-to-English MT output, we first used a simple merge of the outputs from an ET (Entity Translation) system and an English NER system, getting an absolute gain of 7.15% in F-measure, from 73.53% to 80.68%.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9007406234741211}, {"text": "ET (Entity Translation)", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.7473336458206177}, {"text": "F-measure", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9907494187355042}]}, {"text": "We then trained an MEMM module to integrate them more discriminatively, and got a further average gain of 2.74% in F-measure, from 80.68% to 83.42%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9899885058403015}]}], "introductionContent": [{"text": "Because of the growing multilingual environment for NLP, there is an increasing need to be able to annotate and analyze the output of machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.827179753780365}]}, {"text": "But treating this task as one of processing \"ordinary text\" can lead to poor results.", "labels": [], "entities": []}, {"text": "We examine this problem with respect to the name tagging of English text.", "labels": [], "entities": [{"text": "name tagging of English text", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.8373980402946473}]}, {"text": "A Named Entity Recognizer (NER) trained on an English corpus does not have the same performance when applied to machine-translated text.", "labels": [], "entities": []}, {"text": "From our experiments on NIST 05 Chineseto-English MT evaluation data, when we used the same English NER to tag the reference translation and the MT output, the F-measure was 81.38% for the reference but only 73.53% for the MT output.", "labels": [], "entities": [{"text": "NIST 05 Chineseto-English MT evaluation data", "start_pos": 24, "end_pos": 68, "type": "DATASET", "confidence": 0.8607913255691528}, {"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.9139138460159302}, {"text": "F-measure", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9979106783866882}, {"text": "MT", "start_pos": 223, "end_pos": 225, "type": "TASK", "confidence": 0.903635561466217}]}, {"text": "There are two primary reasons for this.", "labels": [], "entities": []}, {"text": "First, the performance of current translation systems is not very good, and so the output is quite different from Standard English text.", "labels": [], "entities": []}, {"text": "The fluency of the translated text will be poor, and the context of a named entity maybe weird.", "labels": [], "entities": []}, {"text": "Second, the translated text has some foreign names which are hard for the English NER to recognize, even if they are well translated by the MT system, because such names appear very infrequently in the English training corpus.", "labels": [], "entities": []}, {"text": "Training an NER on MT output does not seem to bean attractive solution.", "labels": [], "entities": [{"text": "MT output", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.8757152259349823}]}, {"text": "It may take a lot of time to manually annotate a large amount of training data, and this labor may have to be repeated fora new MT system or even anew version of an existing MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.8636345267295837}]}, {"text": "Furthermore, the resulting system may still notwork well, in so far as the translation is not good and information is somehow distorted.", "labels": [], "entities": []}, {"text": "In fact, sometimes the meanings of the translated sentences are hard to decipher unless we check the source language or get a human translated document as reference.", "labels": [], "entities": []}, {"text": "As a result, we need source language information to aid the English NER.", "labels": [], "entities": [{"text": "English NER", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.6716741025447845}]}, {"text": "However, it is also not enough to rely entirely on the source language NE results and map them onto the translated English text.", "labels": [], "entities": []}, {"text": "First, the word alignment from source language to English generated by the MT system may not be accurate, leading to problems in mapping the Chinese name tags.", "labels": [], "entities": [{"text": "word alignment from source language", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.7960934042930603}, {"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.7850225567817688}]}, {"text": "Second, the translated text is not exactly same as the source language because there maybe information missed or added.", "labels": [], "entities": []}, {"text": "For example, the Chinese phrase \"\u9999\u6e2f\u5730\u94c1 \", which is not a name in Chinese, and should be literally translated as \"the subway in Hong Kong\", may end up being translated to \"mtrc\", the abbreviation of \"The Mass Transit Railway Corporation\", which is an organization in Hong Kong (and so should get a name tag in English).", "labels": [], "entities": []}, {"text": "If we can use the information from both the source language and the translated text, we cannot only find the named entities missed by the English NER, but also modify incorrect boundaries in the English results which are caused by the bad content.", "labels": [], "entities": []}, {"text": "However, using word alignment to map the source language information into the English text is problematic, for two reasons: First, the word alignment produced by machine translation is typically not very good, with a Chinese-English AER (alignment error rate) of about 40%.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7509791851043701}, {"text": "word alignment", "start_pos": 135, "end_pos": 149, "type": "TASK", "confidence": 0.679053008556366}, {"text": "AER (alignment error rate)", "start_pos": 233, "end_pos": 259, "type": "METRIC", "confidence": 0.9493822554747263}]}, {"text": "So just using word alignment to map the information would introduce a lot of noise.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7590496242046356}]}, {"text": "Second, in the case of function words in English which have no corresponding realization in Chinese, traditional word alignment would align the function word with another Chinese constituent, such as a name, which could lead to boundary errors in tagging English names.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.7635900676250458}]}, {"text": "We have therefore used an alternative method to fetch the source language information for information extraction, which is called Entity Translation and is described in Section 3.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8355377018451691}, {"text": "Entity Translation", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.7636229693889618}]}], "datasetContent": [{"text": "The experiment was carried out on the Chinese part of the NIST 05 machine translation evaluation (NIST05) and NIST 04 machine translation evaluation (NIST04) data, where NISTT05 contains 100 documents and NIST04 contains 200 documents.", "labels": [], "entities": [{"text": "NIST 05 machine translation evaluation", "start_pos": 58, "end_pos": 96, "type": "TASK", "confidence": 0.8308111667633057}, {"text": "NIST 04 machine translation evaluation", "start_pos": 110, "end_pos": 148, "type": "TASK", "confidence": 0.8041151285171508}, {"text": "NIST04) data", "start_pos": 150, "end_pos": 162, "type": "DATASET", "confidence": 0.7587146759033203}, {"text": "NISTT05", "start_pos": 170, "end_pos": 177, "type": "DATASET", "confidence": 0.9371856451034546}, {"text": "NIST04", "start_pos": 205, "end_pos": 211, "type": "DATASET", "confidence": 0.9368252754211426}]}, {"text": "We annotated all the data in NIST05 and 120 documents for NIST04 for our experiment.", "labels": [], "entities": [{"text": "NIST05", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9855980277061462}, {"text": "NIST04", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.971473217010498}]}, {"text": "The ET system used a Chinese HMM-based NER trained on 1,460,648 words; the English name tagger was also HMM-based and trained on 450,000 words.", "labels": [], "entities": []}, {"text": "First, we want to seethe result with very small training data, and so divided the NIST05 data into 5 subsets, each containing 20 documents.", "labels": [], "entities": [{"text": "NIST05 data", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9816000163555145}]}, {"text": "We ran across validation experiment on this small corpus, with 4 subsets as training data and 1 as testing data.", "labels": [], "entities": []}, {"text": "We refer to this configuration as Corpus1 . Second, to see whether increasing the training data would appreciably influence the result, we added the annotated NIST04 data into the training corpus, and we call this configuration Corpus2.", "labels": [], "entities": [{"text": "NIST04 data", "start_pos": 159, "end_pos": 170, "type": "DATASET", "confidence": 0.9714295566082001}]}, {"text": "We conducted some experiments with a small corpus in which we relied on the alignment information from the MT system, but the results were much worse than using the ET output.", "labels": [], "entities": [{"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.5971135497093201}]}, {"text": "Simple merge using alignment yielded a name tagger F score of 73.34% (1.42% worse than the baseline, 75.76%), while ET F score of 81.23%; MEMM with minimal features using alignment yielded an improvement of 1.7% (vs. 7.9% using ET).", "labels": [], "entities": [{"text": "F score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.8798421919345856}, {"text": "ET F score", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9601180752118429}]}], "tableCaptions": [{"text": " Table 1.  S  imple merge method on Corpus1 (100 documents)", "labels": [], "entities": [{"text": "imple", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9099175333976746}, {"text": "Corpus1", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9548848271369934}]}]}