{"title": [{"text": "Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking", "labels": [], "entities": [{"text": "Morphological Tagging", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.6309136748313904}, {"text": "Lexeme", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.9066026210784912}]}], "abstractContent": [{"text": "We investigate the tasks of general morphological tagging, diacritization, and lemmatiza-tion for Arabic.", "labels": [], "entities": [{"text": "general morphological tagging", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.7044309775034586}]}, {"text": "We show that for all tasks we consider, both modeling the lexeme explicitly, and retuning the weights of individual classi-fiers for the specific task, improve the performance .", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We have three main research hypotheses: (1) Using lexemic features helps in all tasks, but especially in the diacritization and lexeme choice tasks.", "labels": [], "entities": []}, {"text": "(2) Tuning the weights helps over using identical weights.", "labels": [], "entities": []}, {"text": "(3) Tuning to the task that is evaluated improves over tuning to other tasks.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9676607251167297}]}, {"text": "For each of the two feature sets, BASE-16 and FULL-19, we tune the weights using seven tuning metrics, producing seven sets of weights.", "labels": [], "entities": [{"text": "BASE-16", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9226474165916443}, {"text": "FULL-19", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9408460855484009}]}, {"text": "We then evaluate the seven automatically weighted systems using seven evaluation metrics.", "labels": [], "entities": []}, {"text": "The tuning metrics are identical to the evaluation metrics and they correspond to the seven tasks described in Section 2.", "labels": [], "entities": []}, {"text": "Instead of showing 98 results, we show in four results for each of the seven tasks: for both the BASE-16 and FULL-19 feature sets, we give the untuned performance, and then the best-performing tuned performance.", "labels": [], "entities": [{"text": "BASE-16", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.8699688911437988}, {"text": "FULL-19 feature sets", "start_pos": 109, "end_pos": 129, "type": "DATASET", "confidence": 0.7517054279645284}]}, {"text": "We indicate which tuning metric provided the best tun-) only, with no tuning (i.e., all 14 features have a weight of 1).", "labels": [], "entities": []}, {"text": "The untuned results were determined by also setting almost all feature weights to 1; the only exception is the Isdefault feature, which is given a weight of -(8/14) when included in untuned sets.", "labels": [], "entities": []}, {"text": "Since this feature is meant to penalize analyses, its value must be negative; we use this particular value so that our results can be readily compared to previous work.", "labels": [], "entities": []}, {"text": "All results are the best published results to date on these test sets; fora deeper discussion, seethe longer version of this paper which is available as a technical report.", "labels": [], "entities": []}, {"text": "We thus find our three hypotheses confirmed: (1) Using lexemic features reduces error for the morphological tagging tasks (measured on tuned data) by 3% to 11%, but by 36% to 71% for the diacritic and lexeme choice tasks.", "labels": [], "entities": [{"text": "error", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.995997428894043}, {"text": "morphological tagging tasks", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.7216904362042745}]}, {"text": "The highest error reduction is indeed for the lexical choice task.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.9400674998760223}]}, {"text": "(2) Tuning the weights helps over using identical weights.", "labels": [], "entities": []}, {"text": "With only morphological features, we obtain an error reduction of between 4% and 12%; with all features, the error reduction from tuning ranges between 8% and 20%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9788572490215302}, {"text": "error reduction", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.9517791271209717}]}, {"text": "(3) As for the correlation between tuning task and evaluation task, it turned out that when we use only morphological features, two tuning tasks worked best for all evaluation tasks, namely MorphAll and AllChoice, thus not confirming our hypothesis.", "labels": [], "entities": []}, {"text": "We speculate that in the absence of the lexical features, more features is better (these two tasks are the two hardest tasks for morphological features only).", "labels": [], "entities": []}, {"text": "If we add the lexemic features, we do find our hypothesis confirmed, with almost all evaluation tasks performing best when the weights are tuned for that task.", "labels": [], "entities": []}, {"text": "In the case of the three exceptions, the differences between the best performance and performance when tuned to the same task are very slight (< 0.06%).", "labels": [], "entities": []}], "tableCaptions": []}