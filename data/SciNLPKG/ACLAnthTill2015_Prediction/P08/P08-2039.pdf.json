{"title": [{"text": "Segmentation for English-to-Arabic Statistical Machine Translation", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9532127976417542}, {"text": "Statistical Machine Translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.8577135602633158}]}], "abstractContent": [{"text": "In this paper, we report on a set of initial results for English-to-Arabic Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7982021272182465}]}, {"text": "We show that morphological decomposition of the Arabic source is beneficial, especially for smaller-size corpora, and investigate different recombina-tion techniques.", "labels": [], "entities": [{"text": "morphological decomposition", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.7537294328212738}]}, {"text": "We also report on the use of Factored Translation Models for English-to-Arabic translation.", "labels": [], "entities": [{"text": "English-to-Arabic translation", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.6959935426712036}]}], "introductionContent": [{"text": "Arabic has a complex morphology compared to English.", "labels": [], "entities": []}, {"text": "Words are inflected for gender, number, and sometimes grammatical case, and various clitics can attach to word stems.", "labels": [], "entities": []}, {"text": "An Arabic corpus will therefore have more surface forms than an English corpus of the same size, and will also be more sparsely populated.", "labels": [], "entities": []}, {"text": "These factors adversely affect the performance of Arabic\u2194English Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Arabic\u2194English Statistical Machine Translation (SMT)", "start_pos": 50, "end_pos": 102, "type": "TASK", "confidence": 0.629647410578198}]}, {"text": "In prior work, it has been shown that morphological segmentation of the Arabic source benefits the performance of Arabic-to-English SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.80218505859375}]}, {"text": "The use of similar techniques for English-to-Arabic SMT requires recombination of the target side into valid surface forms, which is not a trivial task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8490434885025024}]}, {"text": "In this paper, we present an initial set of experiments on English-to-Arabic SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.678607165813446}]}, {"text": "We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus.", "labels": [], "entities": []}, {"text": "We show that segmenting the Arabic target in training and decoding improves performance.", "labels": [], "entities": []}, {"text": "We propose various schemes for recombining the segmented Arabic, and compare their effect on translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9660287499427795}]}, {"text": "We also report on applying Factored Translation Models ( for English-to-Arabic translation.", "labels": [], "entities": [{"text": "English-to-Arabic translation", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.743973046541214}]}], "datasetContent": [{"text": "The English source is aligned to the segmented Arabic target using GIZA++, and the decoding is done using the phrase-based SMT system MOSES.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.8064078688621521}]}, {"text": "We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic.", "labels": [], "entities": []}, {"text": "Tuning is done using Och's algorithm to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric ().", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9619078636169434}, {"text": "phrase translation", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8219723105430603}, {"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9926457405090332}]}, {"text": "For our baseline system the tuning reference was nonsegmented Arabic.", "labels": [], "entities": []}, {"text": "For the segmented Arabic experiments we experiment with 2 tuning schemes: T1 uses segmented Arabic for reference, and T2 tunes on non-segmented Arabic.", "labels": [], "entities": []}, {"text": "The Factored Translation Models experiments uses the MOSES system.", "labels": [], "entities": [{"text": "Factored Translation Models", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7134061654408773}, {"text": "MOSES", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.7801857590675354}]}], "tableCaptions": [{"text": " Table 1: Recombination Results. Percentage of sentences  with mis-combined words.", "labels": [], "entities": []}, {"text": " Table 2: BLEU (1-reference) scores for the News data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991568326950073}, {"text": "News data", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.9647107124328613}]}, {"text": " Table 3: BLEU (1-reference) scores for the IWSLT data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992321729660034}, {"text": "IWSLT data", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9428374171257019}]}]}