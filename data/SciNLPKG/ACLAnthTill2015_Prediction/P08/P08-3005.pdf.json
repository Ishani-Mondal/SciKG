{"title": [{"text": "A Re-examination on Features in Regression Based Approach to Auto- matic MT Evaluation", "labels": [], "entities": [{"text": "Re-examination", "start_pos": 2, "end_pos": 16, "type": "TASK", "confidence": 0.8757120966911316}, {"text": "Auto- matic MT", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.512363076210022}]}], "abstractContent": [{"text": "Machine learning methods have been extensively employed in developing MT evaluation metrics and several studies show that it can help to achieve a better correlation with human assessments.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9255791902542114}]}, {"text": "Adopting the regression SVM framework, this paper discusses the linguistic motivated feature formulation strategy.", "labels": [], "entities": [{"text": "linguistic motivated feature formulation", "start_pos": 64, "end_pos": 104, "type": "TASK", "confidence": 0.6431415006518364}]}, {"text": "We argue that \"blind\" combination of available features does not yield a general metrics with high correlation rate with human assessments.", "labels": [], "entities": []}, {"text": "Instead, certain simple intuitive features serve better in establishing the regression SVM evaluation model.", "labels": [], "entities": []}, {"text": "With six features selected, we show evidences to support our view through a few experiments in this paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic evaluation of machine translation (MT) system has become a hot research issue in MT circle.", "labels": [], "entities": [{"text": "automatic evaluation of machine translation (MT)", "start_pos": 4, "end_pos": 52, "type": "TASK", "confidence": 0.6729013025760651}, {"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.982746958732605}]}, {"text": "Compared with the huge amount of manpower cost and time cost of human evaluation, the automatic evaluations have lower cost and reusability.", "labels": [], "entities": []}, {"text": "Although the automatic evaluation metrics have succeeded in the system level, there are still on-going investigations to get reference translation better) or to deal with sub-document level evaluation ().", "labels": [], "entities": [{"text": "reference translation", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.6883773505687714}]}, {"text": "N-grams' co-occurrence based metrics such as BLEU and NIST can reach a fairly good correlation with human judgments, but due to their consideration for the capability of generalization across multiple languages, they discard the inherent linguistic knowledge of the sentence evaluated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9966183304786682}, {"text": "NIST", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.7952593564987183}]}, {"text": "Actually, fora certain target language, one could exploit this knowledge to help us developing a more \"human-like\" metric.", "labels": [], "entities": []}, {"text": "showed that compared with metrics limited in lexical dimension, metrics integrating deep linguistic information will be more reliable.", "labels": [], "entities": []}, {"text": "The introduction of machine learning methods aimed at the improvement of MT evaluation metrics' precision is a recent trend.", "labels": [], "entities": [{"text": "MT evaluation metrics'", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8881473143895467}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9003050923347473}]}, {"text": "treated the evaluation of MT outputs as classification problem between human translation and machine translation.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.8971154987812042}, {"text": "human translation and machine translation", "start_pos": 71, "end_pos": 112, "type": "TASK", "confidence": 0.7039227187633514}]}, {"text": "proposed a SVM classifier based on confidence score, which takes the distance between feature vector and the decision surface as the measure of the MT system's output.", "labels": [], "entities": [{"text": "MT", "start_pos": 148, "end_pos": 150, "type": "TASK", "confidence": 0.9679407477378845}]}, {"text": "Joshua S. adopted regression SVM to improve the evaluation metric.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we will first discuss some pitfalls of the n-gram based metrics such as BLEU and NIST, together with the intuition that factors from the linguist knowledge can be used to evaluate MT system's outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9940739274024963}, {"text": "NIST", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.8940041661262512}, {"text": "MT", "start_pos": 207, "end_pos": 209, "type": "TASK", "confidence": 0.9802199006080627}]}, {"text": "Then, we will propose a MT evaluation metric based on SVM regression using information from various linguistic levels (lexical level, phrase level, syntax level and sentence-level) as features.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9036640822887421}]}, {"text": "Finally, from empirical studies, we will show that this metric, with less simple linguistic motivated features, will result in a better correlation with human judgments than previous regression-based methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use SVM-Light (Joachims 1999) to train our learning models.", "labels": [], "entities": []}, {"text": "Our main dataset is NIST's 2003 Chinese MT evaluations.", "labels": [], "entities": [{"text": "NIST's 2003 Chinese MT evaluations", "start_pos": 20, "end_pos": 54, "type": "DATASET", "confidence": 0.8350964983304342}]}, {"text": "There are 6\u00d7919=5514 sentences generated by six systems together with human assessment data which contains a fluency score and adequacy score marked by two human judges.", "labels": [], "entities": []}, {"text": "Because there is bias in the distributions of the two judges' assessment, we normalize the scores following.", "labels": [], "entities": []}, {"text": "The normalized score is the average of the sum of the normalized fluency score and the normalized adequacy score.", "labels": [], "entities": []}, {"text": "To determine the quality of a metric, we use Spearman rank correlation coefficient which is distribution-independent between the score given to the evaluative data and human assessment data.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 45, "end_pos": 82, "type": "METRIC", "confidence": 0.7712298408150673}]}, {"text": "The Spearman coefficient is areal number ranging from -1 to +1, indicating perfect negative correlations or perfect positive correlations.", "labels": [], "entities": []}, {"text": "We take the correlation rates of the metrics reported in Albrecht and Hwa and a standard automatic metric BLEU as a baseline comparison.", "labels": [], "entities": [{"text": "correlation", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9716089963912964}, {"text": "Albrecht and Hwa", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.8062101006507874}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9672830104827881}]}, {"text": "Among the features described in section 3.2, we finally adopted 6 features: \u2022 Content words precision and recall after morphological reduction defined in Eq.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9947867393493652}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9965672492980957}]}, {"text": "4. \u2022 Noun-phrases' case insensitive precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9756162166595459}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9934060573577881}]}, {"text": "\u2022 P-norm (Eq. 7) function's output.", "labels": [], "entities": []}, {"text": "\u2022 Rescaled parser score defined in Eq.", "labels": [], "entities": [{"text": "Rescaled parser score", "start_pos": 2, "end_pos": 23, "type": "METRIC", "confidence": 0.8991960883140564}, {"text": "Eq", "start_pos": 35, "end_pos": 37, "type": "DATASET", "confidence": 0.9126203060150146}]}, {"text": "8. Our first experiment will compare the correlation rate between metric using rescaled parser score and that using parser score directly.", "labels": [], "entities": [{"text": "correlation rate", "start_pos": 41, "end_pos": 57, "type": "METRIC", "confidence": 0.9596865177154541}]}], "tableCaptions": [{"text": " Table 1. For comparison, the result from  BLEU is also included.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9951816201210022}]}, {"text": " Table 1: Spearman rank-correlation coefficients for re- gression based metrics using linear and Gaussian kernel,  and using rescaled parser score or directly the parser  score. Coefficient for BLEU is also involved.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.996601939201355}]}, {"text": " Table 3: Cross year experiment result. All the learning  based metrics are developed from NIST 2003.", "labels": [], "entities": [{"text": "NIST 2003", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.9714278876781464}]}]}