{"title": [{"text": "Semi-supervised Convex Training for Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.8027364015579224}]}], "abstractContent": [{"text": "We present a novel semi-supervised training algorithm for learning dependency parsers.", "labels": [], "entities": [{"text": "learning dependency parsers", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6679941217104594}]}, {"text": "By combining a supervised large margin loss with an unsupervised least squares loss, a dis-criminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems.", "labels": [], "entities": []}, {"text": "To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora.", "labels": [], "entities": []}, {"text": "Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outper-forms corresponding supervised methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (;.", "labels": [], "entities": []}, {"text": "However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain.", "labels": [], "entities": []}, {"text": "Perceiving the limitation of supervised learning-in particular, the heavy dependence on annotated corpora-many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input.", "labels": [], "entities": []}, {"text": "Following the common theme of \"more data is better data\" we also use both a limited labeled corpora and a plentiful unlabeled data resource.", "labels": [], "entities": []}, {"text": "Our goal is to obtain better performance than a purely supervised approach without unreasonable computational effort.", "labels": [], "entities": []}, {"text": "Unfortunately, although significant recent progress has been made in the area of semi-supervised learning, the performance of semi-supervised learning algorithms still fall far short of expectations, particularly in challenging real-world tasks such as natural language parsing or machine translation.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 253, "end_pos": 277, "type": "TASK", "confidence": 0.6585858364899954}, {"text": "machine translation", "start_pos": 281, "end_pos": 300, "type": "TASK", "confidence": 0.7000667601823807}]}, {"text": "A large number of distinct approaches to semisupervised training algorithms have been investigated in the literature.", "labels": [], "entities": []}, {"text": "Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (.", "labels": [], "entities": [{"text": "generative models", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.9134977757930756}]}, {"text": "Self-training is a commonly used technique for semi-supervised learning that has been ap-plied to several natural language processing tasks.", "labels": [], "entities": []}, {"text": "The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining.", "labels": [], "entities": []}, {"text": "Recently, successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9860158562660217}, {"text": "parser adaptation", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.916400820016861}]}, {"text": "More recently, have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms.", "labels": [], "entities": []}, {"text": "They also show connections between these algorithms and other related machine learning algorithms.", "labels": [], "entities": []}, {"text": "Another approach, generative probabilistic models, area well-studied framework that can be extremely effective.", "labels": [], "entities": [{"text": "generative probabilistic", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.9104371070861816}]}, {"text": "However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima.", "labels": [], "entities": []}, {"text": "Moreover, EM optimizes a marginal likelihood score that is not discriminative.", "labels": [], "entities": []}, {"text": "Consequently, most previous work that has attempted semi-supervised or unsupervised approaches to parsing have not produced results beyond the state of the art supervised results ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 98, "end_pos": 105, "type": "TASK", "confidence": 0.9786564111709595}]}, {"text": "Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by.", "labels": [], "entities": [{"text": "Contrastive Estimation (CE)", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.5606600344181061}]}, {"text": "Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance.", "labels": [], "entities": [{"text": "Contrastive Estimation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7096697092056274}]}, {"text": "It makes use of a set of examples (its neighborhood) that are similar in someway to an observed example, requiring the learner to move probability mass to a given example, taking only from the example's neighborhood.", "labels": [], "entities": []}, {"text": "Nevertheless, CE still suffers from shortcomings, including local minima.", "labels": [], "entities": [{"text": "CE", "start_pos": 14, "end_pos": 16, "type": "DATASET", "confidence": 0.7249135971069336}]}, {"text": "In recent years, SVMs have demonstrated state of the art results in many supervised learning tasks.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.9620101451873779}]}, {"text": "As a result, many researchers have put effort on developing algorithms for semi-supervised SVMs (S3VMs)).", "labels": [], "entities": []}, {"text": "However, the standard objective of an S3VM is non-convex on the unlabeled data, thus requiring sophisticated global optimization heuristics to obtain reasonable solutions.", "labels": [], "entities": []}, {"text": "A number of researchers have proposed several efficient approximation algorithms for S3VMs).", "labels": [], "entities": []}, {"text": "For example, propose an algorithm that smoothes the objective with a Gaussian function, and then performs a gradient descent search in the primal space to achieve a local solution.", "labels": [], "entities": []}, {"text": "An alternative approach is proposed by who formulate a semi-definite programming (SDP) approach.", "labels": [], "entities": []}, {"text": "In particular, they present an algorithm for multiclass unsupervised and semi-supervised SVM learning, which relaxes the original non-convex objective into a close convex approximation, thereby allowing a global solution to be obtained.", "labels": [], "entities": [{"text": "SVM learning", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.9149458706378937}]}, {"text": "However, the computational cost of SDP is still quite expensive.", "labels": [], "entities": [{"text": "SDP", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9531877636909485}]}, {"text": "Instead of devising various techniques for coping with non-convex loss functions, we approach the problem from a different perspective.", "labels": [], "entities": []}, {"text": "We simply replace the non-convex loss on unlabeled data with an alternative loss that is jointly convex with respect to both the model parameters and (the encoding of) the self-trained prediction targets.", "labels": [], "entities": []}, {"text": "More specifically, for the loss on the unlabeled data part, we substitute the original unsupervised structured SVM loss with a least squares loss, but keep constraints on the inferred prediction targets, which avoids trivialization.", "labels": [], "entities": []}, {"text": "Although using a least squares loss function for classification appears misguided, there is a precedent for just this approach in the early pattern recognition literature ().", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7895184457302094}]}, {"text": "This loss function has the advantage that the entire training objective on both the labeled and unlabeled data now becomes convex, since it consists of a convex structured large margin loss on labeled data and a convex least squares loss on unlabeled data.", "labels": [], "entities": []}, {"text": "As we will demonstrate below, this approach admits an efficient training procedure that can find a global minimum, and, perhaps surprisingly, can systematically improve the accuracy of supervised training approaches for learning dependency parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9988094568252563}, {"text": "learning dependency parsers", "start_pos": 220, "end_pos": 247, "type": "TASK", "confidence": 0.6078305641810099}]}, {"text": "Thus, in this paper, we focus on semi-supervised language learning, where we can make use of both labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "In particular, we investigate a semi-supervised approach for structured large margin training, where the objective is a combination of two convex functions, the structured large margin loss on labeled data and the least squares loss on unlabeled data.", "labels": [], "entities": [{"text": "structured large margin training", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.6364359185099602}]}, {"text": "We apply the result-fundss Investorss continuee too pourr cashh intoo moneyy: A dependency treeing semi-supervised convex objective to dependency parsing, and obtain significant improvement over the corresponding supervised structured SVM.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7870913147926331}]}, {"text": "Note that our approach is different from the self-training technique proposed in (), although both methods belong to semi-supervised training category.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first review the supervised structured large margin training technique.", "labels": [], "entities": []}, {"text": "Then we introduce the standard semisupervised structured large margin objective, which is non-convex and difficult to optimize.", "labels": [], "entities": []}, {"text": "Next we present anew semi-supervised training algorithm for structured SVMs which is convex optimization.", "labels": [], "entities": []}, {"text": "Finally, we apply this algorithm to dependency parsing and show improved dependency parsing accuracy for both Chinese and English.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8807988166809082}, {"text": "dependency parsing", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6713478416204453}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9721453189849854}]}], "datasetContent": [{"text": "Given a convex approach to semi-supervised structured large margin training, and an efficient training algorithm for achieving a global optimum, we now investigate its effectiveness for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 186, "end_pos": 204, "type": "TASK", "confidence": 0.8529607653617859}]}, {"text": "In particular, we investigate the accuracy of the results it produces.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9989570379257202}]}, {"text": "We applied the resulting algorithm to learn dependency parsers for both English and Chinese.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of Experimental Data (# of sentences)", "labels": [], "entities": [{"text": "Size of Experimental", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8850103616714478}]}, {"text": " Table 2: Supervised and Semi-supervised Dependency  Parsing Accuracy on Chinese (%)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8786853551864624}]}, {"text": " Table 3: Supervised and Semi-supervised Dependency  Parsing Accuracy on English (%)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8765190243721008}]}]}