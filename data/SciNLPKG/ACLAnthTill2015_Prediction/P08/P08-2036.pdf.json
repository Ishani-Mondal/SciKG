{"title": [], "abstractContent": [{"text": "Frequency counts from very large corpora, such as the Web 1T dataset, have recently become available for language modeling.", "labels": [], "entities": [{"text": "Web 1T dataset", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.8297381401062012}, {"text": "language modeling", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7231472730636597}]}, {"text": "Omission of low frequency n-gram counts is a practical necessity for datasets of this size.", "labels": [], "entities": []}, {"text": "Naive implementations of standard smoothing methods do not realize the full potential of such large datasets with missing counts.", "labels": [], "entities": []}, {"text": "In this paper I present anew smoothing algorithm that combines the Dirichlet prior form of (Mackay and Peto, 1995) with the modified back-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 226, "end_pos": 238, "type": "DATASET", "confidence": 0.7929230034351349}]}], "introductionContent": [{"text": "Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.7873808443546295}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.8234433531761169}]}, {"text": "More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.748634934425354}, {"text": "word sense disambiguation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.724412222703298}]}, {"text": "The recently introduced Web 1T 5-gram dataset () contains the counts of word sequences up to length five in a 10 12 word corpus derived from publicly accessible Web pages.", "labels": [], "entities": [{"text": "Web 1T 5-gram dataset", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.7316456139087677}]}, {"text": "As this corpus is several orders of magnitude larger than the ones used in previous language modeling studies, it holds the promise to provide more accurate domain independent probability estimates.", "labels": [], "entities": []}, {"text": "However, naive application of the well-known smoothing methods do not realize the full potential of this dataset.", "labels": [], "entities": []}, {"text": "In this paper I present experiments with modifications and combinations of various smoothing methods using the Web 1T dataset for model building and the Brown corpus for evaluation.", "labels": [], "entities": [{"text": "Web 1T dataset", "start_pos": 111, "end_pos": 125, "type": "DATASET", "confidence": 0.8979902068773905}, {"text": "model building", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.6956387311220169}, {"text": "Brown corpus", "start_pos": 153, "end_pos": 165, "type": "DATASET", "confidence": 0.8679842054843903}]}, {"text": "I describe anew smoothing method, Dirichlet-Kneser-Ney (DKN), that combines the Bayesian intuition of and the improved back-off estimation of and gives significantly better results than the baseline Kneser-Ney discounting.", "labels": [], "entities": []}, {"text": "The next section describes the general structure of n-gram models and smoothing.", "labels": [], "entities": []}, {"text": "Section 3 describes the data sets and the experimental methodology used.", "labels": [], "entities": []}, {"text": "Section 4 presents experiments with adaptations of various smoothing methods.", "labels": [], "entities": []}, {"text": "Section 5 describes the new algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, I describe several smoothing methods and give their performance on the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.9439206421375275}]}, {"text": "Each subsection describes a single idea and its impact on the performance.", "labels": [], "entities": []}, {"text": "All methods use interpolated models expressed by \u03b1(c|ab) and \u03b3(ab) based on Equation 2.", "labels": [], "entities": []}, {"text": "The Web 1T dataset does not include n-grams with counts less than 40, and I note the specific implementation decisions due to the missing counts where appropriate.", "labels": [], "entities": [{"text": "Web 1T dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8512906432151794}]}], "tableCaptions": []}