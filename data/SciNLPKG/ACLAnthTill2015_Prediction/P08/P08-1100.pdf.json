{"title": [{"text": "Analyzing the Errors of Unsupervised Learning", "labels": [], "entities": [{"text": "Analyzing the Errors of Unsupervised Learning", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7875203788280487}]}], "abstractContent": [{"text": "We identify four types of errors that unsu-pervised induction systems make and study each one in turn.", "labels": [], "entities": []}, {"text": "Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples.", "labels": [], "entities": [{"text": "EM", "start_pos": 287, "end_pos": 289, "type": "TASK", "confidence": 0.9318217635154724}]}, {"text": "We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9265046119689941}]}], "introductionContent": [{"text": "The unsupervised induction of linguistic structure from raw text is an important problem both for understanding language acquisition and for building language processing systems such as parsers from limited resources.", "labels": [], "entities": [{"text": "understanding language acquisition", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.6807386577129364}]}, {"text": "Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima.", "labels": [], "entities": []}, {"text": "Without additional constraints on bracketing) or on allowable rewrite rules, unsupervised grammar learning was ineffective.", "labels": [], "entities": []}, {"text": "Since then, there has been a large body of work addressing the flaws of the EM-based approach.", "labels": [], "entities": []}, {"text": "Syntactic models empirically more learnable than PCFGs have been developed).", "labels": [], "entities": []}, {"text": "proposed anew objective function; Smith and Eisner (2006) introduced anew training procedure.", "labels": [], "entities": []}, {"text": "Bayesian approaches can also improve performance).", "labels": [], "entities": []}, {"text": "Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9795572757720947}]}, {"text": "It is therefore important to better understand the behavior of unsupervised induction systems in general.", "labels": [], "entities": []}, {"text": "In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.792400598526001}]}, {"text": "We identify four types of error that a system can make: approximation, identifiability, estimation, and optimization errors (see).", "labels": [], "entities": []}, {"text": "We try to isolate each one in turn and study its properties.", "labels": [], "entities": []}, {"text": "Approximation error is caused by a mis-match between the likelihood objective optimized by EM and the true relationship between sentences and their syntactic structures.", "labels": [], "entities": [{"text": "Approximation error", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9692763090133667}]}, {"text": "Our key idea for understanding this mis-match is to \"cheat\" and initialize EM with the true relationship and then study the ways in which EM repurposes our desired syntactic structures to increase likelihood.", "labels": [], "entities": []}, {"text": "We present a metamodel of the changes that EM makes and show how this tool can shed some light on the undesired biases of the HMM, the PCFG, and the dependency model with valence ().", "labels": [], "entities": [{"text": "PCFG", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.9318985939025879}]}, {"text": "Identifiability error can be incurred when two distinct parameter settings yield the same probability distribution over sentences.", "labels": [], "entities": [{"text": "Identifiability error", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.7160598337650299}]}, {"text": "One type of nonidentifiability present in HMMs and PCFGs is label symmetry, which even makes computing a meaningful distance between parameters NP-hard.", "labels": [], "entities": []}, {"text": "We present a method to obtain lower and upper bounds on such a distance.", "labels": [], "entities": []}, {"text": "Estimation error arises from having too few training examples, and optimization error stems from EM getting stuck in local optima.", "labels": [], "entities": []}, {"text": "While it is to be expected that estimation error should decrease as the amount of data increases, we show that optimization error can also decrease.", "labels": [], "entities": [{"text": "estimation error", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.6982453018426895}]}, {"text": "We present striking experiments showing that if our data actually comes from the model family we are learning with, we can sometimes recover the true parameters by simply running EM without clever initialization.", "labels": [], "entities": []}, {"text": "This result runs counter to the conventional attitude that EM is doomed to local optima; it suggests that increasing the amount of data might bean effective way to partially combat local optima.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Lower and upper bounds on the distance from  the true \u03b8  *  for the HMM as we increase the number of  examples.", "labels": [], "entities": []}]}