{"title": [{"text": "Computing Confidence Scores for All Sub Parse Trees", "labels": [], "entities": []}], "abstractContent": [{"text": "Computing confidence scores for applications , such as dialogue system, information retrieving and extraction, is an active research area.", "labels": [], "entities": [{"text": "information retrieving and extraction", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.7755379304289818}]}, {"text": "However, its focus has been primarily on computing word-, concept-, or utterance-level confidences.", "labels": [], "entities": []}, {"text": "Motivated by the need from sophisticated dialogue systems for more effective dialogs, we generalize the confidence annotation to all the subtrees, the first effort in this line of research.", "labels": [], "entities": []}, {"text": "The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores.", "labels": [], "entities": []}, {"text": "Using Conditional Maximum Entropy (CME) classifier with all the selected features , we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the Charniak parser from (Kahn et al., 2005).", "labels": [], "entities": [{"text": "annotation error rate", "start_pos": 98, "end_pos": 119, "type": "METRIC", "confidence": 0.8637376427650452}, {"text": "SWBD corpus", "start_pos": 136, "end_pos": 147, "type": "DATASET", "confidence": 0.8831839859485626}]}], "introductionContent": [{"text": "There has been a good amount of interest in obtaining confidence scores for improving word or utterance accuracy, dialogue systems, information retrieving & extraction, and machine translation (;.", "labels": [], "entities": [{"text": "word or utterance accuracy", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.5355973169207573}, {"text": "information retrieving & extraction", "start_pos": 132, "end_pos": 167, "type": "TASK", "confidence": 0.8520588427782059}, {"text": "machine translation", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.8326522409915924}]}, {"text": "However, these confidence scores are limited to relatively simple systems, such as command-ncontrol dialogue systems.", "labels": [], "entities": []}, {"text": "For more sophisticated dialogue systems (e.g.,, identification of reliable phrases must be performed at different granularity to ensure effective and friendly dialogues.", "labels": [], "entities": []}, {"text": "For example, in a request of MP3 music domain \"Play a rock song by Cher\", if we want to communicate to the user that the system is not confident of the phrase \"a rock song,\" the confidence scores for each word, the artist name \"Cher,\" and the whole sentence would not be enough.", "labels": [], "entities": []}, {"text": "For tasks of information extraction, when extracted content has internal structures, confidence scores for such phrases are very useful for reliable returns.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.759318619966507}]}, {"text": "As a first attempt in this research, we generalize confidence annotation algorithms to all sub parse trees and tested on a human-human conversational corpus, the SWBD.", "labels": [], "entities": []}, {"text": "Technically, we also introduce a set of long distance features to address the challenges in computing multi-level confidence scores.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 introduces the tasks and the representation for parse trees; Section 3 presents the features used in the algorithm; Section 4 describes the experiments in the SWBD corpus; Section 5 concludes the paper.", "labels": [], "entities": [{"text": "SWBD corpus", "start_pos": 205, "end_pos": 216, "type": "DATASET", "confidence": 0.750916600227356}]}], "datasetContent": [{"text": "Experiments were conducted to seethe performance of our algorithm inhuman to human dialogsthe ultimate goal of a dialogue system.", "labels": [], "entities": []}, {"text": "In our work, we use aversion of the Charniak's parser from (Aug.) to parse the re-segmented SWBD corpus (), and extract the parse sub-trees from the parse trees as experimental data.", "labels": [], "entities": [{"text": "SWBD corpus", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.7880933284759521}]}, {"text": "The parser's training procedure is the same as ().", "labels": [], "entities": []}, {"text": "The only difference is that they use golden edits in the parsing experiments while we delete all the edits in the UW Switchboard corpus.", "labels": [], "entities": [{"text": "UW Switchboard corpus", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.9676965673764547}]}, {"text": "The F-score of the parsing result of the Charniak parser without edits is 88.24%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9994805455207825}]}, {"text": "The Charniak parser without edits is used to parse the training data, testing data and tuning data.", "labels": [], "entities": []}, {"text": "We remove the sentences with only one word and delete the interjections in the hypothesis parse trees.", "labels": [], "entities": []}, {"text": "Finally, we extract parse sub-trees from these hypothesis parse trees.", "labels": [], "entities": []}, {"text": "Based on the gold parse trees, a parse sub-tree is labeled with 1 (correct), if it has all the words, their POS tags and syntactic structures correct.", "labels": [], "entities": []}, {"text": "Otherwise, it is 0 (incorrect).", "labels": [], "entities": []}, {"text": "Among the 424,614 parse sub-trees from the training data, 316,182 sub-trees are labeled with 1; among the 38,774 parse sub-trees from testing data, 22,521 ones are labeled with 1; and among the 67,464 parse sub-trees from the tuning data, 38,619 ones are labeled with 1.", "labels": [], "entities": []}, {"text": "In the testing data, there are 5,590 sentences, and the percentage of complete bracket match 2 is 57.11%, and the percentage of parse sub-trees with correct labels at the sentence level is 48.57%.", "labels": [], "entities": []}, {"text": "The percentage of correct parse sub-trees is lower than that of the complete bracket match due to its stricter requirements.", "labels": [], "entities": []}, {"text": "shows our analysis of the testing data.", "labels": [], "entities": []}, {"text": "There, the first column indicates the phrase length categories from the parse sub-trees.", "labels": [], "entities": []}, {"text": "Among all the parse trees in the test data, 82.84% (first two rows) have a length equal to or shorter than 10 words.", "labels": [], "entities": []}, {"text": "We converted the original parse sub-trees from the Charniak parser into binary trees.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The analysis of testing data.", "labels": [], "entities": []}, {"text": " Table 2. Comparison of different feature space (on SWBD corpus).", "labels": [], "entities": [{"text": "SWBD corpus", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9136169254779816}]}, {"text": " Table 3. Summary of experiment results with different feature space (on SWBD corpus).", "labels": [], "entities": [{"text": "SWBD corpus", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.9497328400611877}]}, {"text": " Table 1: 41.91% =  27.14%+14.77%). So, our algorithm would also  help the best parsing algorithms during rescoring", "labels": [], "entities": [{"text": "parsing", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9646087288856506}, {"text": "rescoring", "start_pos": 106, "end_pos": 115, "type": "TASK", "confidence": 0.9295948147773743}]}, {"text": " Table 4. F-scores for various lengths in Set 15.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978101849555969}]}]}