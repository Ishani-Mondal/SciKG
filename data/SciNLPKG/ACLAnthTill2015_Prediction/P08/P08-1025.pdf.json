{"title": [{"text": "Efficient Multi-pass Decoding for Synchronous Context Free Grammars", "labels": [], "entities": [{"text": "Synchronous Context Free Grammars", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.7202531844377518}]}], "abstractContent": [{"text": "We take a multi-pass approach to machine translation decoding when using synchronous context-free grammars as the translation model and n-gram language models: the first pass uses a bigram language model, and the resulting parse forest is used in the second pass to guide search with a trigram language model.", "labels": [], "entities": [{"text": "machine translation decoding", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.8266105651855469}]}, {"text": "The trigram pass closes most of the performance gap between a bigram de-coder and a much slower trigram decoder, but takes time that is insignificant in comparison to the bigram pass.", "labels": [], "entities": []}, {"text": "An additional fast decoding pass maximizing the expected count of correct translation hypotheses increases the BLEU score significantly.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9859571158885956}]}], "introductionContent": [{"text": "Statistical machine translation systems based on synchronous grammars have recently shown great promise, but one stumbling block to their widespread adoption is that the decoding, or search, problem during translation is more computationally demanding than in phrase-based systems.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6339675684769949}]}, {"text": "This complexity arises from the interaction of the tree-based translation model with an n-gram language model.", "labels": [], "entities": []}, {"text": "Use of longer n-grams improves translation results, but exacerbates this interaction.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9228551983833313}]}, {"text": "In this paper, we present three techniques for attacking this problem in order to obtain fast, high-quality decoders.", "labels": [], "entities": []}, {"text": "First, we present a two-pass decoding algorithm, in which the first pass explores states resulting from an integrated bigram language model, and the second pass expands these states into trigram-based states.", "labels": [], "entities": []}, {"text": "The general bigram-to-trigram technique is common in speech recognition, where lattices from a bigram-based decoder are re-scored with a trigram language model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7372386008501053}]}, {"text": "We examine the question of whether, given the reordering inherent in the machine translation problem, lower order n-grams will provide as valuable a search heuristic as they do for speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7557808458805084}, {"text": "speech recognition", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.7722315192222595}]}, {"text": "Second, we explore heuristics for agenda-based search, and present a heuristic for our second pass that combines precomputed language model information with information derived from the first pass.", "labels": [], "entities": []}, {"text": "With this heuristic, we achieve the same BLEU scores and model cost as a trigram decoder with essentially the same speed as a bigram decoder.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9740869998931885}]}, {"text": "Third, given the significant speedup in the agenda-based trigram decoding pass, we can rescore the trigram forest to maximize the expected count of correct synchronous constituents of the model, using the product of inside and outside probabilities.", "labels": [], "entities": []}, {"text": "Maximizing the expected count of synchronous constituents approximately maximizes BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9979520440101624}]}, {"text": "We find a significant increase in BLEU in the experiments, with minimal additional time.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9997604489326477}]}], "datasetContent": [{"text": "We did our decoding experiments on the LDC 2002 MT evaluation data set for translation of Chinese newswire sentences into English.", "labels": [], "entities": [{"text": "LDC 2002 MT evaluation data set", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.96137535572052}, {"text": "translation of Chinese newswire sentences", "start_pos": 75, "end_pos": 116, "type": "TASK", "confidence": 0.897102165222168}]}, {"text": "The evaluation data set has 10 human translation references for each sentence.", "labels": [], "entities": [{"text": "evaluation data set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8009718060493469}]}, {"text": "There area total of 371 Chinese sentences of no more than 20 words in the data set.", "labels": [], "entities": []}, {"text": "These sentences are the test set for our different versions of language-model-integrated ITG decoders.", "labels": [], "entities": []}, {"text": "We evaluate the translation results by comparing them against the reference translations using the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9962208867073059}]}, {"text": "The word-to-word translation probabilities are from the translation model of IBM Model 4 trained on a 160-million-word English-Chinese parallel corpus using GIZA++.", "labels": [], "entities": [{"text": "word-to-word translation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6368891894817352}]}, {"text": "The phrase-to-phrase translation probabilities are trained on 833K parallel sentences.", "labels": [], "entities": [{"text": "phrase-to-phrase translation", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6264451444149017}]}, {"text": "758K of this was data made available by ISI, and another 75K was FBIS data.", "labels": [], "entities": [{"text": "ISI", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8987295031547546}, {"text": "FBIS data", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.7716359496116638}]}, {"text": "The language model is trained on a 30-million-word English corpus.", "labels": [], "entities": []}, {"text": "The rule probabilities for ITG are trained using EM on a corpus of 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words.", "labels": [], "entities": []}], "tableCaptions": []}