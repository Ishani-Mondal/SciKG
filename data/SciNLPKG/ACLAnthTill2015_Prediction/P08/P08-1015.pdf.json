{"title": [{"text": "Grounded Language Modeling for Automatic Speech Recognition of Sports Video", "labels": [], "entities": [{"text": "Automatic Speech Recognition of Sports Video", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.7698848446210226}]}], "abstractContent": [{"text": "Grounded language models represent the relationship between words and the non-linguistic context in which they are said.", "labels": [], "entities": []}, {"text": "This paper describes how they are learned from large corpora of unlabeled video, and are applied to the task of automatic speech recognition of sports video.", "labels": [], "entities": [{"text": "automatic speech recognition of sports video", "start_pos": 112, "end_pos": 156, "type": "TASK", "confidence": 0.7063946624596914}]}, {"text": "Results show that grounded language models improve perplexity and word error rate over text based language models, and further , support video information retrieval better than human generated speech transcriptions.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.6465309659639994}, {"text": "video information retrieval", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.6421339809894562}]}], "introductionContent": [{"text": "Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization.", "labels": [], "entities": [{"text": "video search", "start_pos": 103, "end_pos": 115, "type": "TASK", "confidence": 0.7084273993968964}, {"text": "summarization", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.9703110456466675}]}, {"text": "Although performance is often reasonable in controlled environments (such as studio news rooms), automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) (.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.7878815233707428}]}, {"text": "While many researches have examined how to compensate for such noise using acoustic techniques, few have attempted to leverage information in the visual stream to improve speech recognition performance (for an exception see.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.801995575428009}]}, {"text": "In many types of video, however, visual context can provide valuable clues as to what has been said.", "labels": [], "entities": []}, {"text": "For example, in video of Major League Baseball games, the likelihood of the phrase \"home run\" increases dramatically when a home run has actually been hit.", "labels": [], "entities": []}, {"text": "This paper describes a method for incorporating such visual information in an ASR system for sports video.", "labels": [], "entities": [{"text": "ASR", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9871789216995239}]}, {"text": "The method is based on the use of grounded language models to represent the relationship between words and the nonlinguistic context to which they refer . Grounded language models are based on research from cognitive science on grounded models of meaning.", "labels": [], "entities": []}, {"text": "(for a review see.", "labels": [], "entities": []}, {"text": "In such models, the meaning of a word is defined by its relationship to representations of the language users' environment.", "labels": [], "entities": []}, {"text": "Thus, fora robot operating in a laboratory setting, words for colors and shapes maybe grounded in the outputs of its computer vision system); while fora simulated agent operating in a virtual world, words for actions and events maybe mapped to representations of the agent's plans or goals.", "labels": [], "entities": []}, {"text": "This paper extends previous work on grounded models of meaning by learning a grounded language model from naturalistic data collected from broadcast video of Major League Baseball games.", "labels": [], "entities": []}, {"text": "A large corpus of unlabeled sports videos is collected and paired with closed captioning transcriptions of the announcers' speech.", "labels": [], "entities": []}, {"text": "This corpus is used to train the grounded language model, which like traditional language models encode the prior probability of words for an ASR system.", "labels": [], "entities": [{"text": "ASR system", "start_pos": 142, "end_pos": 152, "type": "TASK", "confidence": 0.9103619456291199}]}, {"text": "Unlike traditional language models, however, grounded language models represent the probability of a word conditioned not only on the previous word(s), but also on features of the non-linguistic context in which the word was uttered.", "labels": [], "entities": []}, {"text": "Our approach to learning grounded language models operates in two phases.", "labels": [], "entities": []}, {"text": "In the first phase, events that occur in the video are represented using hierarchical temporal pattern automatically mined.", "labels": [], "entities": []}, {"text": "a) Events are represented by first abstracting the raw video into visual context, camera motion, and audio context features.", "labels": [], "entities": []}, {"text": "b) Temporal data mining is then used to discover hierarchical temporal patterns in the parallel streams of features.", "labels": [], "entities": [{"text": "Temporal data mining", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.8476196726163229}]}, {"text": "c) Temporal patterns found significant in each iteration are stored in a codebook that is used to represent high level events in video. from low level features.", "labels": [], "entities": []}, {"text": "In the second phase, a conditional probability distribution is estimated that describes the probability that a word was uttered given such event representations.", "labels": [], "entities": []}, {"text": "In the following sections we describe these two aspects of our approach and evaluate the performance of our grounded language model on a speech recognition task using video highlights from Major League Baseball games.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.7879498402277628}]}, {"text": "Results indicate improved performance using three metrics: perplexity, word error rate, and precision on an information retrieval task.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.76107124487559}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9993459582328796}, {"text": "information retrieval task", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.7617566287517548}]}], "datasetContent": [{"text": "In order to evaluate our grounded language modeling approach, a parallel data set of 99 Major League Baseball games with corresponding closed captioning transcripts was recorded from live television.", "labels": [], "entities": []}, {"text": "These games represent data totaling approximately 275 hours and 20,000 distinct events from 25 teams in 23 stadiums, broadcast on five different television stations.", "labels": [], "entities": []}, {"text": "From this set, six games were held out for testing (15 hours, 1200 events, nine teams, four stations).", "labels": [], "entities": []}, {"text": "From this test set, baseball highlights (i.e., events which terminate with the player either out or safe) were hand annotated for use in evaluation, and manually transcribed in order to get clean text transcriptions for gold standard comparisons.", "labels": [], "entities": []}, {"text": "Of the 1200 events in the test set, 237 were highlights with a total word count of 12,626 (vocabulary of 1800 words).", "labels": [], "entities": []}, {"text": "The remaining 93 unlabeled games are used to train unigram, bigram, and trigram grounded language models.", "labels": [], "entities": []}, {"text": "Only unigrams, bigrams, and trigrams that are not proper names, appear greater than three times, and are not composed only of stop words were used.", "labels": [], "entities": []}, {"text": "These grounded language models are then combined in a backoff strategy with traditional unigram, bigram, and trigram language models generated from a combination of the closed captioning transcripts of all training games and data from the switchboard corpus (see below).", "labels": [], "entities": []}, {"text": "This backoff is necessary to account for the words not included in the grounded language model itself (i.e. stop words, proper names, low frequency words).", "labels": [], "entities": []}, {"text": "The traditional text-only language models (which are also used below as baseline comparisons) are generated with the SRI language modeling toolkit) using Chen and Goodman's modified Kneser-Ney discounting and interpolation).", "labels": [], "entities": []}, {"text": "The backoff strategy we employ here is very simple: if the ngram appears in the GLM then it is used, otherwise the traditional LM is used.", "labels": [], "entities": []}, {"text": "In future work we will examine more complex backoff strategies (Hsu, in review).", "labels": [], "entities": []}, {"text": "We evaluate our grounded language modeling approach using 3 metrics: perplexity, word error rate, and precision on an information retrieval task.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.7342802286148071}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9993463158607483}, {"text": "information retrieval task", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.7676585714022318}]}], "tableCaptions": [{"text": " Table 1. Perplexity measures for three different lan- guage models on a held out test set of baseball high- lights (12,626 words). We compare the grounded  language model to two text based language models: one  trained on the switchboard corpus alone; and interpo- lated with one trained on closed captioning transcrip- tions of baseball video.", "labels": [], "entities": []}]}