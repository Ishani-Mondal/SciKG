{"title": [{"text": "Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation", "labels": [], "entities": [{"text": "Referring Expression Generation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.8963779012362162}]}], "abstractContent": [{"text": "In this paper we present research in which we apply (i) the kind of intrinsic evaluation met-rics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task.", "labels": [], "entities": [{"text": "HLT evaluation", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.8192574977874756}]}, {"text": "We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, NLG evaluation has taken on a more comparative character.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9266732633113861}]}, {"text": "NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank) and systems that generate weather forecasts).", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9650422930717468}, {"text": "Penn Treebank", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.9956033229827881}]}, {"text": "The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in, with a second event (the Referring Expression Generation, or REG, Challenge) currently underway.", "labels": [], "entities": [{"text": "comparative evaluation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7873527705669403}]}, {"text": "In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of In this paper, we present research in which we applied both intrinsic and extrinsic evaluation methods to the same task, in order to shed light on how the two correlate for NLG tasks.", "labels": [], "entities": []}, {"text": "The results show a surprising lack of correlation between the two types of measures, suggesting that intrinsic metrics and extrinsic methods can represent two very different views of how well a system performs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted two task-performance evaluation experiments (the first was part of the ASGRE Challenge, the second is new), in which participants identified the referent denoted by a description by clicking on a picture in a visual display of target and distractor entities.", "labels": [], "entities": [{"text": "ASGRE Challenge", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.522855132818222}]}, {"text": "To enable subjects to read the outputs of peer systems, we converted them from the attribute-value format described above to something more readable, using a simple attribute-to-word converter.", "labels": [], "entities": []}, {"text": "Both experiments used a Repeated Latin Squares design, and involved 30 participants and 2,250 individual trials (see for full details).", "labels": [], "entities": [{"text": "Repeated Latin Squares", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.5257151226202647}]}, {"text": "In Exp 1, subjects were shown the domain on the same screen as the description.", "labels": [], "entities": []}, {"text": "Two dependent measures were used: (i) combined reading and identification time (RIT), measured from the point at which the description and pictures appeared on the screen to the point at which a picture was selected by mouse-click; and (ii) error rate (ER-1).", "labels": [], "entities": [{"text": "combined reading and identification time (RIT)", "start_pos": 38, "end_pos": 84, "type": "METRIC", "confidence": 0.7465263530611992}, {"text": "error rate", "start_pos": 241, "end_pos": 251, "type": "METRIC", "confidence": 0.9811131954193115}, {"text": "ER-1", "start_pos": 253, "end_pos": 257, "type": "METRIC", "confidence": 0.9495446681976318}]}, {"text": "In Exp 2, subjects first read the description and then initiated the presentation of domain entities.", "labels": [], "entities": []}, {"text": "We computed: (i) reading time (RT), measured from the presentation of a description to the point where a subject requested the presentation of the domain; (ii) identification time (IT), measured from the presentation of the domain to the point where a subject clicked on a picture; and (iii) error rate (ER-2).", "labels": [], "entities": [{"text": "reading time (RT)", "start_pos": 17, "end_pos": 34, "type": "METRIC", "confidence": 0.8903602480888366}, {"text": "identification time (IT)", "start_pos": 160, "end_pos": 184, "type": "METRIC", "confidence": 0.8511518120765686}, {"text": "error rate", "start_pos": 292, "end_pos": 302, "type": "METRIC", "confidence": 0.9767741560935974}, {"text": "ER-2", "start_pos": 304, "end_pos": 308, "type": "METRIC", "confidence": 0.9183672666549683}]}, {"text": "2. REG-specific intrinsic measures: Uniqueness is the proportion of attribute sets generated by a system which identify the referent uniquely (i.e. none of the distractors).", "labels": [], "entities": []}, {"text": "Minimality is the proportion of attribute sets which are minimal as well as unique (i.e. there is no smaller unique set of attributes).", "labels": [], "entities": []}, {"text": "These measures were included because they are commonly named as desiderata for attribute selection algorithms in the REG field.", "labels": [], "entities": []}, {"text": "The minimality check used in this paper treats referent type as a simple attribute, as the ASGRE systems tended to do.", "labels": [], "entities": []}, {"text": "3. Set-similarity measures: The Dice similarity coefficient computes the similarity between a peer attribute set A 1 and a (human-produced) reference attribute set A 2 as) is similar but biased in favour of similarity where one set is a subset of the other.", "labels": [], "entities": [{"text": "Dice similarity coefficient", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.6258204678694407}]}, {"text": "4. String-similarity measures: In order to apply string-similarity metrics, peer and reference outputs were converted to word-strings by the method described under 1 above.", "labels": [], "entities": []}, {"text": "String-edit distance (SE) is straightforward Levenshtein distance with a substitution cost of 2 and insertion/deletion cost of 1.", "labels": [], "entities": [{"text": "String-edit distance (SE)", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.7322614371776581}, {"text": "Levenshtein distance", "start_pos": 45, "end_pos": 65, "type": "METRIC", "confidence": 0.7890102863311768}]}, {"text": "We also used the version of string-edit distance ('SEB') of which normalises for length.", "labels": [], "entities": [{"text": "string-edit distance ('SEB')", "start_pos": 28, "end_pos": 56, "type": "METRIC", "confidence": 0.8301007986068726}]}, {"text": "BLEU computes the proportion of word ngrams (n \u2264 4 is standard) that a peer output shares with several reference outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9674757719039917}]}, {"text": "The NIST MT evaluation metric) is an adaptation of BLEU which gives more importance to less frequent (hence more informative) n-grams.", "labels": [], "entities": [{"text": "NIST MT evaluation metric", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.7232290878891945}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9949746131896973}]}, {"text": "We also used two versions of the ROUGE metric (, ROUGE-2 and ROUGE-SU4 (based on non-contiguous, or 'skip', n-grams), which were official scores in the DUC 2005 summarization task.", "labels": [], "entities": [{"text": "ROUGE metric", "start_pos": 33, "end_pos": 45, "type": "METRIC", "confidence": 0.9517534077167511}, {"text": "ROUGE-2", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.8960244059562683}, {"text": "ROUGE-SU4", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.933795154094696}, {"text": "DUC 2005 summarization task", "start_pos": 152, "end_pos": 179, "type": "DATASET", "confidence": 0.7825996428728104}]}], "tableCaptions": [{"text": " Table 1: Results for all systems and evaluation measures (ER-1 = error rate in Exp 1, ER-2 = error rate in Exp 2). (R =  ROUGE; system IDs as in the ASGRE papers, except GR = GRAPH; T = TITCH).", "labels": [], "entities": [{"text": "error rate", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.962863564491272}, {"text": "error rate", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9399743378162384}, {"text": "ROUGE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.7962424755096436}, {"text": "ASGRE papers", "start_pos": 150, "end_pos": 162, "type": "DATASET", "confidence": 0.9011016190052032}, {"text": "GRAPH", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.5244086384773254}, {"text": "TITCH", "start_pos": 187, "end_pos": 192, "type": "METRIC", "confidence": 0.7028396129608154}]}, {"text": " Table 2: Pairwise correlations between all automatic measures and the task-performance results from Exp 2. (  *  =  significant at .05;  *  *  at .01). R = ROUGE.", "labels": [], "entities": [{"text": "R", "start_pos": 153, "end_pos": 154, "type": "METRIC", "confidence": 0.9935417175292969}, {"text": "ROUGE", "start_pos": 157, "end_pos": 162, "type": "METRIC", "confidence": 0.9898171424865723}]}]}