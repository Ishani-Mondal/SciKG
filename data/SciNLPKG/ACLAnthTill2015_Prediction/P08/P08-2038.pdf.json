{"title": [{"text": "A Linguistically Annotated Reordering Model for BTG-based Statistical Machine Translation", "labels": [], "entities": [{"text": "BTG-based Statistical Machine Translation", "start_pos": 48, "end_pos": 89, "type": "TASK", "confidence": 0.7882188260555267}]}], "abstractContent": [{"text": "In this paper, we propose a linguistically annotated reordering model for BTG-based statistical machine translation.", "labels": [], "entities": [{"text": "BTG-based statistical machine translation", "start_pos": 74, "end_pos": 115, "type": "TASK", "confidence": 0.6125180646777153}]}, {"text": "The model incorporates linguistic knowledge to predict orders for both syntactic and non-syntactic phrases.", "labels": [], "entities": []}, {"text": "The linguistic knowledge is automatically learned from source-side parse trees through an annotation algorithm.", "labels": [], "entities": []}, {"text": "We empirically demonstrate that the proposed model leads to a significant improvement of 1.55% in the BLEU score over the baseline reordering model on the NIST MT-05 Chinese-to-English translation task.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9809397459030151}, {"text": "NIST MT-05 Chinese-to-English translation task", "start_pos": 155, "end_pos": 201, "type": "TASK", "confidence": 0.7717718362808228}]}], "introductionContent": [{"text": "In recent years, Bracketing Transduction Grammar (BTG) proposed by (Wu, 1997) has been widely used in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Bracketing Transduction Grammar (BTG)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.8615711132685343}, {"text": "statistical machine translation (SMT)", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.8286781311035156}]}, {"text": "However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases.", "labels": [], "entities": []}, {"text": "To address this problem, enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features.", "labels": [], "entities": [{"text": "BTG", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.782119631767273}]}, {"text": "Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.8141323924064636}]}, {"text": "integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7301723659038544}, {"text": "BTG-style", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.8553587794303894}]}, {"text": "However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic phrases as far as reordering is concerned, while non-syntactic phrases are combined monotonously with a flat reordering score.", "labels": [], "entities": []}, {"text": "In this paper, we propose a linguistically annotated reordering model for BTG-based SMT, which is a significant extension to the work mentioned above.", "labels": [], "entities": [{"text": "BTG-based SMT", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.6957727670669556}]}, {"text": "The new model annotates each BTG node with linguistic knowledge by projecting source-side parse trees onto the corresponding binary trees generated by BTG so that syntactic features can be used for phrase reordering.", "labels": [], "entities": [{"text": "BTG", "start_pos": 151, "end_pos": 154, "type": "DATASET", "confidence": 0.9308168292045593}, {"text": "phrase reordering", "start_pos": 198, "end_pos": 215, "type": "TASK", "confidence": 0.7774659395217896}]}, {"text": "Different from (, our annotation algorithm is able to label both syntactic and non-syntactic phrases.", "labels": [], "entities": []}, {"text": "This enables our model to reorder any phrases, not limited to syntactic phrases.", "labels": [], "entities": []}, {"text": "In addition, other linguistic information such as head words, is also used to improve reordering.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes our baseline system while Section 3 introduces the linguistically annotated reordering model.", "labels": [], "entities": []}, {"text": "Section 4 reports the experiments on a Chinese-to-English translation task.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.7587399780750275}]}, {"text": "We conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments in this section were carried out on the Chinese-to-English translation task of the NIST MT-05.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.7518345812956492}, {"text": "NIST MT-05", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.9253969490528107}]}, {"text": "The baseline system and the new system with the LAR model were trained on the FBIS corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8620654344558716}]}, {"text": "We removed 15,250 sentences, for which the Chinese parser () failed to produce syntactic parse trees.", "labels": [], "entities": []}, {"text": "The parser was trained on the Penn Chinese Treebank with a F1 score of 79.4%.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.9848493536313375}, {"text": "F1 score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9873545467853546}]}, {"text": "The remaining FBIS corpus (224,165 sentence pairs) was used to obtain standard bilingual phrases for the systems.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.7659783959388733}]}, {"text": "We extracted 2.8M reordering examples from these sentences.", "labels": [], "entities": []}, {"text": "From these examples, we generated 114.8K reordering features for the BWR model using the right boundary words of phrases and 85K features for the LAR model using linguistic annotations.", "labels": [], "entities": [{"text": "BWR", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.914320707321167}]}, {"text": "We ran the MaxEnt toolkit) to tune reordering feature weights with iteration number being set to 100 and Gaussian prior to 1 to avoid overfitting.", "labels": [], "entities": []}, {"text": "We built our four-gram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.8840015133221945}, {"text": "SRILM toolkit", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.9144589304924011}]}, {"text": "For the efficiency of minimum-errorrate training, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data.", "labels": [], "entities": [{"text": "NIST MT-02 evaluation test data", "start_pos": 148, "end_pos": 179, "type": "DATASET", "confidence": 0.9274231314659118}]}], "tableCaptions": []}