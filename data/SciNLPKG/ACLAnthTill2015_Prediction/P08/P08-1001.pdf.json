{"title": [{"text": "Mining Wiki Resources for Multilingual Named Entity Recognition", "labels": [], "entities": [{"text": "Multilingual Named Entity Recognition", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.6726574301719666}]}], "abstractContent": [{"text": "In this paper, we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition (NER) tags requiring minimal human intervention and no linguistic expertise.", "labels": [], "entities": []}, {"text": "This process, though of value in languages for which resources exist, is particularly useful for less commonly taught languages.", "labels": [], "entities": []}, {"text": "We show how the Wikipedia format can be used to identify possible named entities and discuss in detail the process by which we use the Category structure inherent to Wikipedia to determine the named entity type of a proposed entity.", "labels": [], "entities": []}, {"text": "We further describe the methods by which English language data can be used to bootstrap the NER process in other languages.", "labels": [], "entities": []}, {"text": "We demonstrate the system by using the generated corpus as training sets fora variant of BBN's Identifinder in French, Ukrainian, Spanish, Polish, Russian, and Portuguese, achieving overall F-scores as high as 84.7% on independent, human-annotated corpora, comparable to a system trained on up to 40,000 words of human-annotated newswire.", "labels": [], "entities": [{"text": "BBN's Identifinder", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.93007493019104}, {"text": "F-scores", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9973099231719971}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) has long been a major task of natural language processing.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7732607920964559}, {"text": "natural language processing", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6440978447596232}]}, {"text": "Most of the research in the field has been restricted to a few languages and almost all methods require substantial linguistic expertise, whether creating a rulebased technique specific to a language or manually annotating a body of text to be used as a training set fora statistical engine or machine learning.", "labels": [], "entities": []}, {"text": "In this paper, we focus on using the multilingual Wikipedia (wikipedia.org) to automatically create an annotated corpus of text in any given language, with no linguistic expertise required on the part of the user at run-time (and only English knowledge required during development).", "labels": [], "entities": []}, {"text": "The expectation is that for any language in which Wikipedia is sufficiently well-developed, a usable set of training data can be obtained with minimal human intervention.", "labels": [], "entities": []}, {"text": "As Wikipedia is constantly expanding, it follows that the derived models are continually improved and that increasingly many languages can be usefully modeled by this method.", "labels": [], "entities": []}, {"text": "In order to make sure that the process is as language-independent as possible, we declined to make use of any non-English linguistic resources outside of the Wikimedia domain (specifically, Wikipedia and the English language Wiktionary (en.wiktionary.org)).", "labels": [], "entities": []}, {"text": "In particular, we did not use any semantic resources such as WordNet or part of speech taggers.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9623706936836243}, {"text": "part of speech taggers", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.5508730933070183}]}, {"text": "We used our automatically annotated corpus along with an internally modified variant of BBN's IdentiFinder (), specifically modified to emphasize fast text processing, called \"PhoenixIDF,\" to create several language models that could be tested outside of the Wikipedia framework.", "labels": [], "entities": [{"text": "BBN's IdentiFinder", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.9079490303993225}]}, {"text": "We built on top of an existing system, and left existing lists and tables intact.", "labels": [], "entities": []}, {"text": "Depending on language, we evaluated our derived models against human or machine annotated data sets to test the system.", "labels": [], "entities": []}], "datasetContent": [{"text": "After each data set was generated, we used the text as a training set for input to PhoenixIDF.", "labels": [], "entities": [{"text": "PhoenixIDF", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.9529231190681458}]}, {"text": "We had three human annotated test sets, Spanish, French and Ukrainian, consisting of newswire.", "labels": [], "entities": []}, {"text": "When human annotated sets were not available, we held out more than 100,000 words of text generated by our wiki-mining process to use as a test set.", "labels": [], "entities": []}, {"text": "For the above languages, we included wiki test sets for comparison purposes.", "labels": [], "entities": []}, {"text": "We will give our results as F-scores in the Overall, DATE, GPE, ORGANIZATION, and PERSON categories using the scoring metric in).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9967767596244812}, {"text": "DATE", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9881916642189026}, {"text": "GPE", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9823350310325623}, {"text": "ORGANIZATION", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9863574504852295}, {"text": "PERSON", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9905223250389099}]}, {"text": "The other ACE categories are much less common, and contribute little to the overall score.", "labels": [], "entities": []}, {"text": "The Spanish Wikipedia is a substantial, well-developed Wikipedia, consisting of more than 290,000 articles as of October 2007.", "labels": [], "entities": [{"text": "Spanish Wikipedia", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8646123707294464}]}, {"text": "We used two test sets for comparison purposes.", "labels": [], "entities": []}, {"text": "The first consists of 25,000 words of human annotated newswire derived from the ACE 2007 test set, manually modified to conform to our extended MUC-style standards.", "labels": [], "entities": [{"text": "ACE 2007 test set", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9689774215221405}, {"text": "MUC-style standards", "start_pos": 144, "end_pos": 163, "type": "DATASET", "confidence": 0.8745651841163635}]}, {"text": "The second consists of 335,000 words of data generated by the Wiki process held-out during training.", "labels": [], "entities": []}, {"text": "There area few particularly interesting results to note.", "labels": [], "entities": []}, {"text": "First, because of the optional processing, recall was boosted in the PERSON category at the expense of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9995025396347046}, {"text": "PERSON", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9541458487510681}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9994670748710632}]}, {"text": "The fact that this category scores higher against newswire than against the wiki data suggests that the not-uncommon, but isolated, occurrences of non-entities being marked as PERSONs in training have little effect on the overall system.", "labels": [], "entities": []}, {"text": "Contrarily, we note that deletions are the dominant source of error in the ORGANI-ZATION category, as seen by the lower recall.", "labels": [], "entities": [{"text": "ORGANI-ZATION", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.9911234378814697}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9976473450660706}]}, {"text": "The better performance on the wiki set seems to suggest that either Wikipedia is relatively poor in Organizations or that PhoenixIDF underperforms when identifying Organizations relative to other categories or a combination.", "labels": [], "entities": []}, {"text": "An important question remains: \"How do these results compare to other methodologies?\"", "labels": [], "entities": []}, {"text": "In particular, while we can get these results for free, how much work would traditional methods require to achieve comparable results?", "labels": [], "entities": []}, {"text": "To attempt to answer this question, we trained PhoenixIDF on additional ACE 2007 Spanish language data converted to MUC-style tags, and scored its performance using the same set of newswire.", "labels": [], "entities": [{"text": "PhoenixIDF", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.9415813088417053}, {"text": "ACE 2007 Spanish language data", "start_pos": 72, "end_pos": 102, "type": "DATASET", "confidence": 0.9590866565704346}]}, {"text": "Evidently, comparable performance to our Wikipedia derived system requires between 20,000 and 40,000 words of human-annotated newswire.", "labels": [], "entities": []}, {"text": "It is worth noting that Wikipedia itself is not newswire, so we do not have a perfect comparison.", "labels": [], "entities": []}, {"text": "The French Wikipedia is one of the largest Wikipedias, containing more than 570,000 articles as of October 2007.", "labels": [], "entities": [{"text": "French Wikipedia", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9563198387622833}]}, {"text": "For this evaluation, we have 25,000 words of human annotated newswire (Agence France Presse, 30 April and 1 May 1997) covering diverse topics.", "labels": [], "entities": [{"text": "Agence France Presse, 30 April and 1 May 1997)", "start_pos": 71, "end_pos": 117, "type": "DATASET", "confidence": 0.9453204057433389}]}, {"text": "We used 920,000 words of Wiki-derived data for the second test.", "labels": [], "entities": []}, {"text": "The overall results seem comparable to the Spanish, with the slightly better overall performance likely correlated to the somewhat more developed Wikipedia.", "labels": [], "entities": []}, {"text": "We did not have sufficient quantities of annotated data to run a test of the traditional methods, but Spanish and French are sufficiently similar languages that we expect this model is comparable to one created with about 40,000 words of humanannotated data.", "labels": [], "entities": []}, {"text": "The Ukrainian Wikipedia is a medium-sized Wikipedia with 74,000 articles as of October 2007.", "labels": [], "entities": [{"text": "Ukrainian Wikipedia", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8861556053161621}]}, {"text": "Also, the typical article is shorter and less welllinked to other articles than in the French or Spanish versions.", "labels": [], "entities": []}, {"text": "Moreover, entities tend to appear in many surface forms depending on case, leading us to expect somewhat worse results.", "labels": [], "entities": []}, {"text": "In the Ukrainian case, the newswire consisted of approximately 25,000 words from various online news sites covering primarily political topics.", "labels": [], "entities": []}, {"text": "We also held out around 395,000 words for testing.", "labels": [], "entities": []}, {"text": "We were also able to run a comparison test as in Spanish.", "labels": [], "entities": [{"text": "comparison", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.9119133353233337}]}, {"text": "The Ukrainian newswire contained a much higher proportion of organizations than the French or Spanish versions, contributing to the overall lower score.", "labels": [], "entities": []}, {"text": "The Ukrainian language Wikipedia itself contains very few articles on organizations relative to other types, so the distribution of entities of the two test sets are quite different.", "labels": [], "entities": []}, {"text": "We also see that the Wiki-derived model performs comparably to a model trained on 15-20,000 words of humanannotated text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 7: Other Language Results", "labels": [], "entities": []}]}