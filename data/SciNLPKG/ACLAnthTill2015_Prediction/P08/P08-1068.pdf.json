{"title": [], "abstractContent": [{"text": "We present a simple and effective semi-supervised method for training dependency parsers.", "labels": [], "entities": [{"text": "training dependency parsers", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6626496116320292}]}, {"text": "We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.", "labels": [], "entities": [{"text": "lexical representation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7401537001132965}]}, {"text": "We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.729731097817421}, {"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.995193749666214}, {"text": "Prague Dependency Treebank", "start_pos": 120, "end_pos": 146, "type": "DATASET", "confidence": 0.941446840763092}]}, {"text": "For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%.", "labels": [], "entities": [{"text": "English unlabeled second-order parsing", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.43127352744340897}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.8843888640403748}, {"text": "Czech unlabeled second-order parsing", "start_pos": 144, "end_pos": 180, "type": "TASK", "confidence": 0.4595152661204338}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.8395346999168396}]}, {"text": "In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language parsing, lexical information is seen as crucial to resolving ambiguous relationships, yet lexicalized statistics are sparse and difficult to estimate directly.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6672516266504923}]}, {"text": "It is therefore attractive to consider intermediate entities which exist at a coarser level than the words themselves, yet capture the information necessary to resolve the relevant ambiguities.", "labels": [], "entities": []}, {"text": "In this paper, we introduce lexical intermediaries via a simple two-stage semi-supervised approach.", "labels": [], "entities": []}, {"text": "First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct anew cluster-based feature mapping fora discriminative learner.", "labels": [], "entities": []}, {"text": "We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research-e.g., see work in the CoNLL shared tasks on dependency parsing (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.8799543082714081}, {"text": "dependency parsing", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.6767241358757019}]}, {"text": "The idea of combining word clusters with discriminative learning has been previously explored by, in the context of namedentity recognition, and their work directly inspired our research.", "labels": [], "entities": [{"text": "namedentity recognition", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7302468419075012}]}, {"text": "However, our target task of dependency parsing involves more complex structured relationships than named-entity tagging; moreover, it is not at all clear that word clusters should have any relevance to syntactic structure.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8256923854351044}, {"text": "named-entity tagging", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7488174736499786}]}, {"text": "Nevertheless, our experiments demonstrate that word clusters can be quite effective in dependency parsing applications.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8398683965206146}]}, {"text": "In general, semi-supervised learning can be motivated by two concerns: first, given a fixed amount of supervised data, we might wish to leverage additional unlabeled data to facilitate the utilization of the supervised corpus, increasing the performance of the model in absolute terms.", "labels": [], "entities": []}, {"text": "Second, given a fixed target performance level, we might wish to use unlabeled data to reduce the amount of annotated data necessary to reach this target.", "labels": [], "entities": []}, {"text": "We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank and Prague Dependency Treebank) (see Sections 4.1 and 4.3).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.995773047208786}, {"text": "Prague Dependency Treebank", "start_pos": 140, "end_pos": 166, "type": "DATASET", "confidence": 0.9355094035466512}]}, {"text": "By conducting experiments on datasets of varying sizes, we demonstrate that for fixed levels of performance, the cluster-based approach can reduce the need for supervised data by roughly half, which is a substantial savings in data-annotation costs (see Sections 4.2 and 4.4).", "labels": [], "entities": []}, {"text": "The remainder of this paper is divided as follows: Ms.", "labels": [], "entities": []}, {"text": "Haag plays Elianti . * obj p root nmod sbj: An example of a labeled dependency tree.", "labels": [], "entities": []}, {"text": "The tree contains a special token \"*\" which is always the root of the tree.", "labels": [], "entities": []}, {"text": "Each arc is directed from head to modifier and has a label describing the function of the attachment.", "labels": [], "entities": []}, {"text": "Section 2 gives background on dependency parsing and clustering, Section 3 describes the cluster-based features, Section 4 presents our experimental results, Section 5 discusses related work, and Section 6 concludes with ideas for future research.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7908872067928314}]}], "datasetContent": [{"text": "In order to evaluate the effectiveness of the clusterbased feature sets, we conducted dependency parsing experiments in English and Czech.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.8041447699069977}]}, {"text": "We test the features in a wide range of parsing configurations, including first-order and second-order parsers, and labeled and unlabeled parsers.", "labels": [], "entities": []}, {"text": "The English experiments were performed on the Penn Treebank (, using a standard set of head-selection rules) to convert the phrase structure syntax of the Treebank to a dependency tree representation.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9963002502918243}]}, {"text": "We split the Treebank into a training set (Sections 2-21), a development set (Section 22), and several test sets (Sections 0, 7 1, 23, and 24).", "labels": [], "entities": []}, {"text": "The data partition and head rules were chosen to match previous work).", "labels": [], "entities": []}, {"text": "The part of speech tags for the development and test data were automatically assigned by MXPOST, where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.9031326174736023}]}, {"text": "8 English word clusters were derived from the BLLIP corpus (), which contains roughly 43 million words of Wall Street Journal text.", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8297912776470184}, {"text": "Wall Street Journal text", "start_pos": 106, "end_pos": 130, "type": "DATASET", "confidence": 0.9333108961582184}]}, {"text": "The Czech experiments were performed on the Prague Dependency Treebank 1.0), which is directly annotated with dependency structures.", "labels": [], "entities": [{"text": "Prague Dependency Treebank 1.0)", "start_pos": 44, "end_pos": 75, "type": "DATASET", "confidence": 0.9681772828102112}]}, {"text": "To facilitate comparisons with previous work), we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus.", "labels": [], "entities": []}, {"text": "Czech word clusters were derived from the raw text section of the PDT 1.0, which contains about 39 million words of newswire text.", "labels": [], "entities": [{"text": "PDT 1.0", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.8497752547264099}]}, {"text": "We trained the parsers using the averaged perceptron), which represents a balance between strong performance and fast training times.", "labels": [], "entities": []}, {"text": "To select the number   of iterations of perceptron training, we performed up to 30 iterations and chose the iteration which optimized accuracy on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.99904865026474}]}, {"text": "Our feature mappings are quite high-dimensional, so we eliminated all features which occur only once in the training data.", "labels": [], "entities": []}, {"text": "The resulting models still had very high dimensionality, ranging from tens of millions to as many as a billion features.", "labels": [], "entities": []}, {"text": "All results presented in this section are given in terms of parent-prediction accuracy, which measures the percentage of tokens that are attached to the correct head token.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8366447687149048}]}, {"text": "For labeled dependency structures, both the head token and dependency label must be correctly predicted.", "labels": [], "entities": []}, {"text": "In addition, in English parsing we ignore the parent-predictions of punctuation tokens, and in Czech parsing we retain the punctuation tokens; this matches previous work).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Parent-prediction accuracies of unlabeled English parsers on Section 22. Abbreviations: Size = #sentences in  training corpus; \u2206 = difference between cluster-based and baseline features; other abbreviations are as in", "labels": [], "entities": [{"text": "Abbreviations", "start_pos": 83, "end_pos": 96, "type": "METRIC", "confidence": 0.9667956233024597}]}, {"text": " Table 4: Parent-prediction accuracies of unlabeled Czech  parsers on the PDT 1.0 test set, for baseline features and  cluster-based features. Abbreviations are as in Table 2.", "labels": [], "entities": [{"text": "Parent-prediction", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9746696352958679}, {"text": "PDT 1.0 test set", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9580580294132233}, {"text": "Abbreviations", "start_pos": 143, "end_pos": 156, "type": "METRIC", "confidence": 0.9700291156768799}]}, {"text": " Table 5: Unlabeled parent-prediction accuracies of Czech  parsers on the PDT 1.0 test set, for our models and for  previous work.", "labels": [], "entities": [{"text": "PDT 1.0 test set", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9514442533254623}]}, {"text": " Table 6: Parent-prediction accuracies of unlabeled Czech  parsers on the PDT 1.0 development set. Abbreviations  are as in", "labels": [], "entities": [{"text": "Parent-prediction accuracies", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9039725959300995}, {"text": "PDT 1.0 development set", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.9598857462406158}, {"text": "Abbreviations", "start_pos": 99, "end_pos": 112, "type": "METRIC", "confidence": 0.9853286147117615}]}, {"text": " Table 7: Parent-prediction accuracies of unlabeled En- glish parsers on Section 22. Abbreviations: N = thresh- old value; other abbreviations are as in", "labels": [], "entities": [{"text": "Parent-prediction accuracies", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.720085084438324}, {"text": "Section 22", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.9522094428539276}, {"text": "Abbreviations", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.9838201999664307}, {"text": "thresh- old value", "start_pos": 104, "end_pos": 121, "type": "METRIC", "confidence": 0.8913195580244064}]}, {"text": " Table 8: Parent-prediction accuracies of unlabeled En- glish parsers on Section 22. Abbreviations: suffix -P =  model without POS; other abbreviations are as in Table 2.", "labels": [], "entities": [{"text": "Section 22", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.937100738286972}, {"text": "Abbreviations", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.9573055505752563}]}]}