{"title": [{"text": "Semi-Supervised Sequential Labeling and Segmentation using Giga-word Scale Unlabeled Data", "labels": [], "entities": [{"text": "Sequential Labeling and Segmentation", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.7846774309873581}, {"text": "Giga-word Scale Unlabeled Data", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.6345959082245827}]}], "abstractContent": [{"text": "This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.7023651152849197}, {"text": "syntactic chunking", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.7324179708957672}, {"text": "named entity recognition", "start_pos": 221, "end_pos": 245, "type": "TASK", "confidence": 0.6858628888924917}]}, {"text": "We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data.", "labels": [], "entities": []}, {"text": "Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL'00 and '03 shared task data for the above three NLP tasks, respectively.", "labels": [], "entities": [{"text": "PTB III data", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.7931780815124512}, {"text": "CoNLL'00", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.47386476397514343}]}, {"text": "We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement.", "labels": [], "entities": []}, {"text": "In addition, our results are superior to the best reported results for all of the above test collections.", "labels": [], "entities": []}], "introductionContent": [{"text": "Today, we can easily find a large amount of unlabeled data for many supervised learning applications in Natural Language Processing (NLP).", "labels": [], "entities": []}, {"text": "Therefore, to improve performance, the development of an effective framework for semi-supervised learning (SSL) that uses both labeled and unlabeled data is attractive for both the machine learning and NLP communities.", "labels": [], "entities": [{"text": "semi-supervised learning (SSL)", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.7224014282226563}]}, {"text": "We expect that such SSL will replace most supervised learning in real world applications.", "labels": [], "entities": []}, {"text": "In this paper, we focus on traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER).", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6245148956775666}, {"text": "syntactic chunking", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.6557222008705139}, {"text": "named entity recognition (NER)", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.7725626677274704}]}, {"text": "These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems.", "labels": [], "entities": []}, {"text": "In some cases, these tasks have relatively large amounts of labeled training data.", "labels": [], "entities": []}, {"text": "In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL.", "labels": [], "entities": []}, {"text": "In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning.", "labels": [], "entities": []}, {"text": "reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL'00 shared task data) and NER with the CoNLL'03 shared task data.", "labels": [], "entities": [{"text": "syntactic chunking", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.6803165972232819}, {"text": "CoNLL'00 shared task data", "start_pos": 138, "end_pos": 163, "type": "DATASET", "confidence": 0.8786846697330475}, {"text": "CoNLL'03 shared task data", "start_pos": 182, "end_pos": 207, "type": "DATASET", "confidence": 0.8784523904323578}]}, {"text": "One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible.", "labels": [], "entities": [{"text": "SSL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9472303986549377}]}, {"text": "This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data.", "labels": [], "entities": []}, {"text": "To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data.", "labels": [], "entities": []}, {"text": "We first propose a scalable model for SSL.", "labels": [], "entities": [{"text": "SSL", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9633074998855591}]}, {"text": "Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data for POS tagging, CoNLL'00 shared task data for syntactic chunking, and CoNLL'03 shared task data for NER.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) III data", "start_pos": 65, "end_pos": 93, "type": "DATASET", "confidence": 0.9676564165524074}, {"text": "POS tagging", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.8331796228885651}, {"text": "CoNLL'00 shared task data", "start_pos": 111, "end_pos": 136, "type": "DATASET", "confidence": 0.736552894115448}, {"text": "syntactic chunking", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.6732713729143143}, {"text": "CoNLL'03 shared task data", "start_pos": 165, "end_pos": 190, "type": "DATASET", "confidence": 0.7787186801433563}, {"text": "NER", "start_pos": 195, "end_pos": 198, "type": "TASK", "confidence": 0.806312084197998}]}, {"text": "We used up to 1G-words (one billion tokens) of unlabeled data to explore the performance improvement with respect to the unlabeled data size.", "labels": [], "entities": []}, {"text": "In addition, we investigate the performance improvement for 'unseen data' from the viewpoint of unlabeled data coverage.", "labels": [], "entities": []}, {"text": "Finally, we compare our results with those provided by the best current systems.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we present a simple, scalable, but powerful task-independent model for semi-supervised sequential labeling and segmentation.", "labels": [], "entities": []}, {"text": "Second, we report the best current results for the widely used test collections described above.", "labels": [], "entities": []}, {"text": "Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.8393816351890564}, {"text": "syntactic chunking", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.6833435595035553}, {"text": "NER", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9154073596000671}]}], "tableCaptions": [{"text": " Table 1: Details of training, development, and test data  (labeled data set) used in our experiments", "labels": [], "entities": []}, {"text": " Table 2: Unlabeled data used in our experiments", "labels": [], "entities": []}, {"text": " Table 4: Results for POS tagging (PTB III data), syntactic chunking (CoNLL'00 data), and NER (CoNLL'03 data)  incorporated with 1G-words of unlabeled data, and the performance gain from supervised CRF", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.7984690070152283}, {"text": "PTB III data", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.7612691720326742}, {"text": "syntactic chunking", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6960272192955017}, {"text": "CRF", "start_pos": 198, "end_pos": 201, "type": "TASK", "confidence": 0.594852089881897}]}, {"text": " Table 5: Comparison with L.\u00acapp and L.app sets obtained from both supervised CRF and JESS-CM with 1G-word  unlabeled data evaluated by the entire sentence accuracies, and the ratio of U.app.", "labels": [], "entities": [{"text": "JESS-CM", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.8825806975364685}]}, {"text": " Table 6: Influence of U.app in NER experiments: *(ex- cluding Dec. 06-07)", "labels": [], "entities": [{"text": "NER", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9150546789169312}]}, {"text": " Table 7: POS tagging results of the previous top systems  for PTB III data evaluated by label accuracy", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7401091754436493}, {"text": "PTB III data", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.8349266250928243}]}, {"text": " Table 8: Syntactic chunking results of the previous top  systems for CoNLL'00 shared task data (F \u03b2=1 score)", "labels": [], "entities": [{"text": "Syntactic chunking", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8059403896331787}, {"text": "CoNLL'00 shared task data", "start_pos": 70, "end_pos": 95, "type": "DATASET", "confidence": 0.6777638494968414}, {"text": "F \u03b2=1 score", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9223185539245605}]}, {"text": " Table 9: NER results of the previous top systems for  CoNLL'03 shared task data evaluated by F \u03b2=1 score", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.728210985660553}, {"text": "CoNLL'03 shared task data", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.6527741104364395}, {"text": "F \u03b2", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9852806329727173}]}]}