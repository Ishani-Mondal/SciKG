{"title": [{"text": "Weakly-Supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs", "labels": [], "entities": []}], "abstractContent": [{"text": "A new approach to large-scale information extraction exploits both Web documents and query logs to acquire thousands of open-domain classes of instances, along with relevant sets of open-domain class attributes at precision levels previously obtained only on small-scale, manually-assembled classes.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.695862665772438}]}], "introductionContent": [{"text": "Current methods for large-scale information extraction take advantage of unstructured text available from either Web documents () or, more recently, logs of Web search queries to acquire useful knowledge with minimal supervision.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7079904079437256}]}, {"text": "Given a manually-specified target attribute (e.g., birth years for people) and starting from as few as 10 seed facts such as, as many as a million facts of the same type can be derived from unstructured text within Web documents).", "labels": [], "entities": []}, {"text": "Similarly, given a manually-specified target class (e.g., Drug) with its instances (e.g., Vicodin and Xanax) and starting from as few as 5 seed attributes (e.g., side effects and maximum dose for Drug), other relevant attributes can be extracted for the same class from query logs).", "labels": [], "entities": []}, {"text": "These and other previous methods require the manual specification of the input classes of instances before any knowledge (e.g., facts or attributes) can be acquired for those classes.", "labels": [], "entities": []}, {"text": "* Contributions made during an internship at Google.", "labels": [], "entities": []}, {"text": "The extraction method introduced in this paper mines a collection of Web search queries and a collection of Web documents to acquire open-domain classes in the form of instance sets (e.g., {whales, seals, dolphins, sea lions,...}) associated with class labels (e.g., marine animals), as well as large sets of open-domain attributes for each class (e.g., circulatory system, life cycle, evolution, food chain and scientific name for the class marine animals).", "labels": [], "entities": []}, {"text": "In this light, the contributions of this paper are fourfold.", "labels": [], "entities": []}, {"text": "First, instead of separately addressing the tasks of collecting unlabeled sets of instances, assigning appropriate class labels to a given set of instances (), and identifying relevant attributes fora given set of classes, our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes.", "labels": [], "entities": []}, {"text": "Second, by exploiting the contents of query logs during the extraction of labeled classes of instances from Web documents, we acquire thousands (4,583, to be exact) of open-domain classes covering a wide range of topics and domains.", "labels": [], "entities": []}, {"text": "The accuracy reported in Section 3.2 exceeds 80% for both instance sets and class labels, although the extraction of classes requires a remarkably small amount of supervision, in the form of only a few commonly-used Is-A extraction patterns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996515512466431}]}, {"text": "Third, we conduct the first study in extracting attributes for thousands of open-domain, automatically-acquired classes, at precision levels over 70% at rank 10, and 67% at rank 20 as described in Section 3.3.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9890459179878235}]}, {"text": "The amount of supervision is limited to five seed attributes provided for only one reference class.", "labels": [], "entities": []}, {"text": "In comparison, the largest previous Figure 1: Overview of weakly-supervised extraction of class instances, class labels and class attributes from Web documents and query logs study in attribute extraction reports results on a set of 40 manually-assembled classes, and requires five seed attributes to be provided as input for each class.", "labels": [], "entities": []}, {"text": "Fourth, we introduce the first approach to information extraction from a combination of both Web documents and search query logs, to extract opendomain knowledge that is expected to be suitable for later use.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.771254152059555}]}, {"text": "In contrast, the textual data sources used in previous studies in large-scale information extraction are either Web documents or, recently, query logs (Pas\u00b8caPas\u00b8ca, 2007), but not both.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7312056571245193}]}], "datasetContent": [{"text": "Extraction Parameters: The set of instances that can be potentially acquired by the extraction algorithm described in Section 2.1 is heuristically limited to the top five million queries with the highest frequency within the input query logs.", "labels": [], "entities": []}, {"text": "In the extracted data, a class label (e.g., search engines) is associated with one or more instances (e.g., google).", "labels": [], "entities": []}, {"text": "Similarly, an instance (e.g., google) is associated with one or more class labels (e.g., search engines and internet search engines).", "labels": [], "entities": []}, {"text": "The values chosen for the weighting parameters J and K from Section 2.1 are 0.01 and 30 respectively.", "labels": [], "entities": [{"text": "Section 2.1", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9170055985450745}]}, {"text": "After discarding classes with fewer than 25 instances, the extracted set of classes consists of 4,583 class labels, each of them associated with 25 to 7,967 instances, with an average of 189 instances per class.", "labels": [], "entities": []}, {"text": "Accuracy of Class Labels: Built over many years of manual construction efforts, lexical gold standards such as WordNet) provide widecoverage upper ontologies of the English language.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.927895724773407}, {"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.954732358455658}]}, {"text": "Built-in morphological normalization routines make it straightforward to verify whether a class label (e.g., faculty members) exists as a concept in WordNet (e.g., faculty member).", "labels": [], "entities": []}, {"text": "When an extracted label (e.g., central nervous system disorders) is not found in WordNet, it is looked up again after iteratively removing its leading words (e.g., nervous system dis-: Correctness judgments for extracted classes whose class labels are found in WordNet only after removal of their leading words (C=Correctness, Y=correct, S=subjectively correct, N=incorrect) orders, system disorders and disorders).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9803825616836548}, {"text": "WordNet", "start_pos": 261, "end_pos": 268, "type": "DATASET", "confidence": 0.9631777405738831}]}, {"text": "As shown in, less than half of the 4,583 extracted class labels (e.g., baseball players) are found in their original forms in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.970649003982544}]}, {"text": "The majority of the class labels (2,614 out of 4,583) can be found in WordNet only after removal of one or more leading words (e.g., caribbean countries), which suggests that many of the class labels correspond to finer-grained, automatically-extracted concepts that are not available in the manually-built WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9664224982261658}, {"text": "WordNet", "start_pos": 307, "end_pos": 314, "type": "DATASET", "confidence": 0.9632589221000671}]}, {"text": "To test whether that is the case, a random sample of 200 class labels, out of the 2,614 labels found to be potentially-useful specific concepts, are manually annotated as correct, subjectively corrector incorrect, as shown in.", "labels": [], "entities": []}, {"text": "A class label is: correct, if it captures a relevant concept although it could not be found in WordNet; subjectively correct, if it is relevant not in general but only in a particular context, either from a subjective viewpoint (e.g., modern appliances), or relative to a particular temporal anchor (e.g., current players), or in connection to a particular geographical area (e.g., area hospitals); or incorrect, if it does not capture any useful concept (e.g., multiple languages).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9353566765785217}]}, {"text": "The manual analysis of the sample of 200 class labels indicates that 154 (77%) are relevant concepts and 27 (13.5%) are subjectively relevant concepts, fora total of 181 (90.5%) relevant concepts, whereas 19 (9.5%) of the labels are incorrect.", "labels": [], "entities": []}, {"text": "It is worth emphasizing the importance of automatically-collected classes judged as relevant and not present in WordNet: caribbean countries, computer manufacturers, entertainment companies, market research firms are arguably very useful and should probably be considered as part of: Comparison between manually-assembled instance sets of gold-standard classes (M ) and instance sets of automatically-extracted classes (E).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.956828236579895}]}, {"text": "Each gold-standard class (M ) was manually mapped into an extracted class (E), unless no relevant mapping was found.", "labels": [], "entities": []}, {"text": "Ratios ( M \u2229E M ) are shown as percentages any refinements to hand-built hierarchies, including any future extensions of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9453335404396057}]}, {"text": "Accuracy of Class Instances: The computation of the precision of the extracted instances (e.g., fifth element and kill bill for the class label movies) relies on manual inspection of all instances associated to a sample of the extracted class labels.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9707932472229004}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.998008668422699}]}, {"text": "Rather than inspecting a random sample of classes, the evaluation validates the results against a reference set of 40 gold-standard classes that were manually assembled as part of previous work.", "labels": [], "entities": []}, {"text": "A class from the gold standard consists of a manually-created class label (e.g., AircraftModel) associated with a manually-assembled, and therefore high-precision, set of representative instances of the class.", "labels": [], "entities": []}, {"text": "To evaluate the precision of the extracted instances, the manual label of each gold-standard class (e.g., SearchEngine) is mapped into a class label extracted from text (e.g., search engines).", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.995974600315094}]}, {"text": "As shown in the first two columns of, the mapping into extracted class labels succeeds for 37 of the 40 goldstandard classes.", "labels": [], "entities": []}, {"text": "28 of the 37 mappings involve linking an abstract class label (e.g., SearchEngine) with the corresponding plural forms among the extracted class labels (e.g., search engines).", "labels": [], "entities": []}, {"text": "The remaining 9 mappings link a manual class label with either an equivalent extracted class label (e.g., SoccerClub with football clubs), or a strongly-related class label (e.g., NationalPark with parks).", "labels": [], "entities": [{"text": "NationalPark", "start_pos": 180, "end_pos": 192, "type": "DATASET", "confidence": 0.976608157157898}]}, {"text": "No mapping is found for 3 out of the 40 classes, namely AircraftModel, Hurricane and Skyscraper, which are therefore removed from consideration.", "labels": [], "entities": [{"text": "AircraftModel", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9482108354568481}]}, {"text": "The sizes of the instance sets available for each class in the gold standard are compared in the third through fifth columns of.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9139370024204254}]}, {"text": "In the table, M stands for manually-assembled instance sets, and E for automatically-extracted instance sets.", "labels": [], "entities": []}, {"text": "For example, the gold-standard class SearchEngine contains 25 manually-collected instances, while the parallel class label search engines contains 133 automatically-extracted instances.", "labels": [], "entities": []}, {"text": "The fifth column shows the percentage of manually-collected instances (M ) that are also extracted automatically (E).", "labels": [], "entities": []}, {"text": "In the case of the class SearchEngine, 16 of the 25 manually-collected instances are among the 133 automatically-extracted instances of the same class,: Labels for assessing attribute correctness which corresponds to a relative coverage of 64% of the manually-collected instance set.", "labels": [], "entities": []}, {"text": "Some instances may occur within the manually-collected set but not the automatically-extracted set (e.g., zoominfo and brainbost for the class SearchEngine) or, more frequently, vice-versa (e.g., surfwax, blinkx, entireweb, web wombat, exalead etc.).", "labels": [], "entities": []}, {"text": "Overall, the relative coverage of automatically-extracted instance sets with respect to manually-collected instance sets is 26.89%, as an average over the 37 gold-standard classes.", "labels": [], "entities": []}, {"text": "More significantly, the size advantage of automatically-extracted instance sets is not the undesirable result of those sets containing many spurious instances.", "labels": [], "entities": []}, {"text": "Indeed, the manual inspection of the automatically-extracted instances sets indicates an average accuracy of 79.3% over the 37 gold-standard classes retained in the experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.999591052532196}]}, {"text": "To summarize, the method proposed in this paper acquires open-domain classes from unstructured text of arbitrary quality, without a-priori restrictions to specific domains of interest and with virtually no supervision (except for the ubiquitous Is-A extraction patterns), at accuracy levels of around 90% for class labels and 80% for class instances.", "labels": [], "entities": [{"text": "Is-A extraction", "start_pos": 245, "end_pos": 260, "type": "TASK", "confidence": 0.6836751997470856}, {"text": "accuracy", "start_pos": 275, "end_pos": 283, "type": "METRIC", "confidence": 0.9984473586082458}]}, {"text": "Extraction Parameters: Given a target class specified as a set of instances and a set of five seed attributes fora class (e.g., {quality, speed, number of users, market share, reliability} for SearchEngine), the method described in Section 2.2 extracts ranked lists of class attributes from the input query logs.", "labels": [], "entities": [{"text": "reliability", "start_pos": 176, "end_pos": 187, "type": "METRIC", "confidence": 0.9434369206428528}]}, {"text": "Internally, the ranking uses Jensen-Shannon to compute similarity scores between internal representations of seed attributes, on one hand, and each of the candidate attributes, on the other hand.", "labels": [], "entities": []}, {"text": "Evaluation Procedure: To remove any possible bias towards higher-ranked attributes during the assessment of class attributes, the ranked lists of attributes to be evaluated are sorted alphabetically into a merged list.", "labels": [], "entities": []}, {"text": "Each attribute of the merged list is  manually assigned a correctness label within its respective class.", "labels": [], "entities": []}, {"text": "An attribute is vital if it must be present in an ideal list of attributes of the class; okay if it provides useful but non-essential information; and wrong if it is incorrect.", "labels": [], "entities": []}, {"text": "To compute the overall precision score over a ranked list of extracted attributes, the correctness labels are converted to numeric values as shown in Table 4.", "labels": [], "entities": [{"text": "precision score", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.9803899526596069}]}, {"text": "Precision at some rank N in the list is thus measured as the sum of the assigned values of the first N candidate attributes, divided by N . Accuracy of Class Attributes: plots precision values for ranks 1 through 50 of the lists of attributes extracted through several runs over the 37 gold-standard classes described in the previous section.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9838858842849731}, {"text": "Accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9949797987937927}, {"text": "precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9943605065345764}]}, {"text": "The runs correspond to different amounts of supervision, specified through a particular choice in the number of seed attributes, and in the source of instances passed as input to the system: \u2022 number of input seed attributes: seed attributes are provided either for each of the 37 classes, fora total of 5\u00d737=185 attributes (the graphs at the top of; or only for one class (namely, Country),  fora total of 5 attributes overall classes (the graphs at the bottom of); \u2022 source of input instance sets: the instance sets for each class are either manually collected), or automatically extracted (E from Table 3).", "labels": [], "entities": []}, {"text": "The choices correspond to the two curves plotted in each graph in.", "labels": [], "entities": []}, {"text": "The graphs in show the precision over individual target classes (leftmost graphs), and as an average overall 37 classes (rightmost graphs).", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9991569519042969}]}, {"text": "As expected, the precision of the extracted attributes as an average overall classes is best when the input instance sets are hand-picked (M ), as opposed to automatically extracted (E).", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9991424083709717}]}, {"text": "However, the loss of precision from M to E is small at all measured ranks.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9994058609008789}]}, {"text": "offers an alternative view on the quality of the attributes extracted fora random sample of 25 classes out of the larger set of 4,583 classes acquired from text.", "labels": [], "entities": []}, {"text": "The 25 classes are passed as input for attribute extraction without modifications.", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7191308736801147}]}, {"text": "In particular, the instance sets are not manually postfiltered or otherwise changed in anyway.", "labels": [], "entities": []}, {"text": "To keep the time required to judge the correctness of all extracted attributes within reasonable limits, the evaluation considers only the top 20 (rather than 50) attributes extracted per class.", "labels": [], "entities": []}, {"text": "As shown in, the method proposed in this paper acquires attributes for automatically-extracted, open-domain classes, without a-priori restrictions to specific domains of interest and relying on only five seed attributes specified for only one class, at accuracy levels reaching 70% at rank 10, and 67% at rank 20.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9979045391082764}]}], "tableCaptions": [{"text": " Table 1: Class labels found in WordNet in original form,  or found in WordNet after removal of leading words, or  not found in WordNet at all", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9437774419784546}, {"text": "WordNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9272355437278748}, {"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9471728801727295}]}, {"text": " Table 3: Comparison between manually-assembled instance sets of gold-standard classes (M ) and instance sets of  automatically-extracted classes (E). Each gold-standard class (M ) was manually mapped into an extracted class (E),  unless no relevant mapping was found. Ratios ( M \u2229E  M ) are shown as percentages", "labels": [], "entities": []}, {"text": " Table 5: Precision of attributes extracted for a sample of 25 classes. Seed attributes are provided for only one class.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9078983068466187}]}]}