{"title": [{"text": "Joint Word Segmentation and POS Tagging using a Single Perceptron", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.6453950703144073}, {"text": "POS Tagging", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.799454540014267}]}], "abstractContent": [{"text": "For Chinese POS tagging, word segmentation is a preliminary step.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8061878085136414}, {"text": "word segmentation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7572392225265503}]}, {"text": "To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7224215120077133}, {"text": "segmentation", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9789107441902161}, {"text": "segmentation", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.9649907946586609}]}, {"text": "A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard.", "labels": [], "entities": []}, {"text": "Recent research has explored the integration of segmentation and POS tagging , by decoding under restricted versions of the full combined search space.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.9779447913169861}, {"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.8185765445232391}]}, {"text": "In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.7649186849594116}]}, {"text": "Fast decoding is achieved by using a novel multiple-beam search algorithm.", "labels": [], "entities": []}, {"text": "The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.", "labels": [], "entities": []}, {"text": "The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach.", "labels": [], "entities": [{"text": "error", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9720994830131531}, {"text": "segmentation", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9188795685768127}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8692005276679993}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.5961810350418091}]}], "introductionContent": [{"text": "Since Chinese sentences do not contain explicitly marked word boundaries, word segmentation is a necessary step before POS tagging can be performed.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7328777313232422}, {"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.855673611164093}]}, {"text": "Typically, a Chinese POS tagger takes segmented inputs, which are produced by a separate word segmentor.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.669784739613533}]}, {"text": "This two-step approach, however, has an obvious flaw of error propagation, since word segmentation errors cannot be corrected by the POS tagger.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.6747302412986755}, {"text": "word segmentation", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7134459167718887}]}, {"text": "A better approach would be to utilize POS information to improve word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7519714832305908}]}, {"text": "For example, the POS-word pattern \"number word\" + \" (a common measure word)\" can help in segmenting the character sequence \"\" into the word sequence \" (one) (measure word) (person)\" instead of \" (one) (personal; adj)\".", "labels": [], "entities": []}, {"text": "Moreover, the comparatively rare POS pattern \"number word\" + \"number word\" can help to prevent segmenting along number word into two words.", "labels": [], "entities": []}, {"text": "In order to avoid error propagation and make use of POS information for word segmentation, segmentation and POS tagging can be viewed as a single task: given a raw Chinese input sentence, the joint POS tagger considers all possible segmented and tagged sequences, and chooses the overall best output.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7247342169284821}, {"text": "word segmentation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7326503098011017}, {"text": "POS tagging", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.726730614900589}]}, {"text": "A major challenge for such a joint system is the large search space faced by the decoder.", "labels": [], "entities": []}, {"text": "For a sentence with n characters, the number of possible output sequences is O(2 n\u22121 \u00b7 T n ), where T is the size of the tag set.", "labels": [], "entities": [{"text": "O", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9748576283454895}]}, {"text": "Due to the nature of the combined candidate items, decoding can be inefficient even with dynamic programming.", "labels": [], "entities": []}, {"text": "Recent research on Chinese POS tagging has started to investigate joint segmentation and tagging, reporting accuracy improvements over the pipeline approach.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.6899999976158142}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9985008239746094}]}, {"text": "Various decoding approaches have been used to reduce the combined search space.", "labels": [], "entities": []}, {"text": "mapped the joint segmentation and POS tagging task into a single character sequence tagging problem.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8613186279932658}, {"text": "character sequence tagging", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.7530989249547323}]}, {"text": "Two types of tags are assigned to each character to represent its segmentation and POS.", "labels": [], "entities": [{"text": "POS", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.6079992055892944}]}, {"text": "For example, the tag \"b NN\" indicates a character at the beginning of a noun.", "labels": [], "entities": []}, {"text": "Using this method, POS features are allowed to interact with segmentation.", "labels": [], "entities": []}, {"text": "Since tagging is restricted to characters, the search space is reduced to O((4T ) n ), and beam search decoding is effective with a small beam size.", "labels": [], "entities": [{"text": "O", "start_pos": 74, "end_pos": 75, "type": "METRIC", "confidence": 0.992191731929779}, {"text": "beam search decoding", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.812548816204071}]}, {"text": "However, the disadvantage of this model is the difficulty in incorporating whole word information into POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.7564721703529358}]}, {"text": "For example, the standard \"word + POS tag\" feature is not explicitly applicable.", "labels": [], "entities": []}, {"text": "introduced POS information to segmentation by reranking.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.9799939393997192}]}, {"text": "N -best segmentation outputs are passed to a separately-trained POS tagger, and the best output is selected using the overall POSsegmentation probability score.", "labels": [], "entities": []}, {"text": "In this system, the decoding for word segmentation and POS tagging are still performed separately, and exact inference for both is possible.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7648813426494598}, {"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7347188740968704}]}, {"text": "However, the interaction between POS and segmentation is restricted by reranking: POS information is used to improve segmentation only for the N segmentor outputs.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9677121639251709}]}, {"text": "In this paper, we propose a novel joint model for Chinese word segmentation and POS tagging, which does not limiting the interaction between segmentation and POS information in reducing the combined search space.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.5718069970607758}, {"text": "POS tagging", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.7373768389225006}]}, {"text": "Instead, a novel multiple beam search algorithm is used to do decoding efficiently.", "labels": [], "entities": []}, {"text": "Candidate ranking is based on a discriminative joint model, with features being extracted from segmented words and POS tags simultaneously.", "labels": [], "entities": []}, {"text": "The training is performed by a single generalized perceptron.", "labels": [], "entities": []}, {"text": "In experiments with the Chinese Treebank data, the joint model gave an error reduction of 14.6% in segmentation accuracy and 12.2% in the overall segmentation and tagging accuracy, compared to the traditional pipeline approach.", "labels": [], "entities": [{"text": "Chinese Treebank data", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.9858145316441854}, {"text": "error", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9879343509674072}, {"text": "segmentation", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.9537915587425232}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9172874093055725}, {"text": "segmentation and tagging", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.7217091917991638}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.7450339794158936}]}, {"text": "In addition, the overall results are comparable to the best systems in the literature, which exploit knowledge outside the training data, even though our system is fully data-driven.", "labels": [], "entities": []}, {"text": "Different methods have been proposed to reduce error propagation between pipelined tasks, both in general () and for specific problems such as language modeling and utterance classification () and labeling and chunking ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.7220044881105423}, {"text": "utterance classification", "start_pos": 165, "end_pos": 189, "type": "TASK", "confidence": 0.7651481926441193}, {"text": "labeling and chunking", "start_pos": 197, "end_pos": 218, "type": "TASK", "confidence": 0.8141108552614847}]}, {"text": "Though our model is built specifically for Chinese word segmentation and POS tagging, the idea of using the perceptron model to solve multiple tasks simultaneously can be generalized to other tasks.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.5863713522752126}, {"text": "POS tagging", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.7497928738594055}]}], "datasetContent": [{"text": "The Chinese Treebank (CTB) 4 is used for the experiments.", "labels": [], "entities": [{"text": "Chinese Treebank (CTB) 4", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9754372835159302}]}, {"text": "It is separated into two parts: CTB 3 (420K characters in 150K words / 10364 sentences) is used for the final 10-fold cross validation, and the rest (240K characters in 150K words / 4798 sentences) is used as training and test data for development.", "labels": [], "entities": []}, {"text": "The standard F-scores are used to measure both the word segmentation accuracy and the overall segmentation and tagging accuracy, where the overall accuracy is T F = 2pr/(p + r), with the precision p being the percentage of correctly segmented and tagged words in the decoder output, and the recall r being the percentage of gold-standard tagged words that are correctly identified by the decoder.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9359846711158752}, {"text": "word segmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.6920519769191742}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.6844598054885864}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.7943230271339417}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9930758476257324}, {"text": "T F", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.84583580493927}, {"text": "recall r", "start_pos": 291, "end_pos": 299, "type": "METRIC", "confidence": 0.9936125576496124}]}, {"text": "For direct comparison with, the POS tagging accuracy is also calculated by the percentage of correct tags on each character.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.6322477012872696}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9043074250221252}]}, {"text": "The learning curves of the baseline and joint models are shown in, and, respectively.", "labels": [], "entities": []}, {"text": "These curves are used to show the conver-4 Apart from the beam search algorithm, we do impose some minor limitations on the search space by methods such as the tag dictionary, but these can be seen as optional pruning methods for optimization.", "labels": [], "entities": []}, {"text": "for the baseline segmentor, POS tagger, and the joint system are set to 8, 6, and 7, respectively for the remaining experiments.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.5716719031333923}]}, {"text": "There are many factors which can influence the accuracy of the joint model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9991132616996765}]}, {"text": "Here we consider the special character category features and the effect of the tag dictionary.", "labels": [], "entities": []}, {"text": "The character category features (templates 15 and 16 in) represent a Chinese character by all the tags associated with the character in the training data.", "labels": [], "entities": []}, {"text": "They have been shown to improve the accuracy of a Chinese POS tagger ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9994848966598511}, {"text": "POS tagger", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.5222823321819305}]}, {"text": "In the joint model, these features also represent segmentation information, since they concern the starting and ending characters of a word.", "labels": [], "entities": []}, {"text": "Development tests showed that the overall tagging F-score of the joint model increased from 84.54% to 84.93% using the character category features.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.8023298978805542}]}, {"text": "In the development test, the use of the tag dictionary improves the decoding speed of the joint model, reducing the decoding time from 416 seconds to 256 seconds.", "labels": [], "entities": []}, {"text": "The overall tagging accuracy also increased slightly, consistent with observations from the pure POS tagger.", "labels": [], "entities": [{"text": "tagging", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9592384099960327}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9717445373535156}, {"text": "POS tagger", "start_pos": 97, "end_pos": 107, "type": "TASK", "confidence": 0.5191204845905304}]}, {"text": "The error analysis for the development testis shown in.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9738162159919739}]}, {"text": "Here an error is counted when a word in the standard output is not produced by the decoder, due to incorrect segmentation or tag assignment.", "labels": [], "entities": []}, {"text": "Statistics about the six most frequently mistaken tags are shown in the table, where each row presents the analysis of one tag from the standard output, and each column gives a wrongly assigned value.", "labels": [], "entities": []}, {"text": "The column \"Seg\" represents segmentation errors.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9705228209495544}]}, {"text": "Each figure in the table shows the percentage of the corresponding error from all the errors.", "labels": [], "entities": []}, {"text": "It can be seen from the table that the NN-VV and VV-NN mistakes were the most commonly made by the decoder, while the NR-NN mistakes are also fre-  quent.", "labels": [], "entities": []}, {"text": "These three types of errors significantly outnumber the rest, together contributing 14.92% of all the errors.", "labels": [], "entities": [{"text": "errors", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9690454602241516}]}, {"text": "Moreover, the most commonly mistaken tags are NN and VV, while among the most frequent tags in the corpus, PU, DEG and M had comparatively less errors.", "labels": [], "entities": []}, {"text": "Lastly, segmentation errors contribute around half (51.47%) of all the errors.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.9674113988876343}]}], "tableCaptions": [{"text": " Table 3: Error analysis for the joint model", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9650573134422302}]}, {"text": " Table 4: The accuracies by 10-fold cross validation", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9935964345932007}]}, {"text": " Table 5: The comparison of overall accuracies by 10-fold  cross validation using CTB", "labels": [], "entities": [{"text": "accuracies", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9719511270523071}, {"text": "CTB", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.5451311469078064}]}]}