{"title": [{"text": "Resolving Personal Names in Email Using Context Expansion", "labels": [], "entities": [{"text": "Resolving Personal Names in Email Using Context Expansion", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8646809831261635}]}], "abstractContent": [{"text": "This paper describes a computational approach to resolving the true referent of a named mention of a person in the body of an email.", "labels": [], "entities": [{"text": "resolving the true referent of a named mention of a person in the body of an email", "start_pos": 49, "end_pos": 131, "type": "TASK", "confidence": 0.739509815678877}]}, {"text": "A generative model of mention generation is used to guide mention resolution.", "labels": [], "entities": [{"text": "mention generation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7331354022026062}, {"text": "mention resolution", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7794695496559143}]}, {"text": "Results on three relatively small collections indicate that the accuracy of this approach compares favorably to the best known techniques, and results on the full CMU Enron collection indicate that it scales well to larger collections.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9996821880340576}, {"text": "CMU Enron collection", "start_pos": 163, "end_pos": 183, "type": "DATASET", "confidence": 0.8881045977274576}]}], "introductionContent": [{"text": "The increasing prevalence of informal text from which a dialog structure can be reconstructed (e.g., email or instant messaging), raises new challenges if we are to help users make sense of this cacophony.", "labels": [], "entities": []}, {"text": "Large collections offer greater scope for assembling evidence to help with that task, but they pose additional challenges as well.", "labels": [], "entities": []}, {"text": "With well over 100,000 unique email addresses in the CMU version of the Enron collection (), common names (e.g., John) might easily refer to anyone of several hundred people.", "labels": [], "entities": [{"text": "CMU version of the Enron collection", "start_pos": 53, "end_pos": 88, "type": "DATASET", "confidence": 0.7201580107212067}]}, {"text": "In this paper, we associate named mentions in unstructured text (i.e., the body of an email and/or the subject line) to modeled identities.", "labels": [], "entities": []}, {"text": "We see at least two direct applications for this work: (1) helping searchers who are unfamiliar with the contents of an email collection (e.g., historians or lawyers) better understand the context of emails that they find, and (2) augmenting more typical social networks (based on senders and recipients) with additional links based on references found in unstructured text.", "labels": [], "entities": []}, {"text": "Most approaches to resolving identity can be decomposed into four sub-problems: (1) finding a reference that requires resolution, (2) identifying candidates, (3) assembling evidence, and (4) choosing * Department of Computer Science \u2020 College of Information Studies among the candidates based on the evidence.", "labels": [], "entities": [{"text": "resolving identity", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.921625018119812}]}, {"text": "For the work reported in this paper, we rely on the user to designate references requiring resolution (which we model as a predetermined set of mention-queries for which the correct referent is known).", "labels": [], "entities": []}, {"text": "Candidate identification is a computational expedient that permits the evidence assembly effort to be efficiently focused; we use only simple techniques for that task.", "labels": [], "entities": [{"text": "Candidate identification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9066411852836609}, {"text": "evidence assembly", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7468278110027313}]}, {"text": "Our principal contributions are the approaches we take to evidence generation (leveraging three ways of linking to other emails where evidence might be found: reply chains, social interaction, and topical similarity) and our approach to choosing among candidates (based on a generative model of reference production).", "labels": [], "entities": [{"text": "evidence generation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8816976547241211}]}, {"text": "We evaluate the effectiveness of our approach on four collections, three of which have previously reported results for comparison, and one that is considerably larger than the others.", "labels": [], "entities": []}, {"text": "The remainder of this paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 surveys prior work.", "labels": [], "entities": []}, {"text": "Section 3 then describes our approach to modeling identity and ranking candidates.", "labels": [], "entities": []}, {"text": "Section 4 presents results, and Section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our mention resolution approach using four test collections, all are based on the CMU version of the Enron collection; each was created by selecting a subset of that collection, selecting a set of query-mentions within emails from that subset, and creating an answer key in which each query-mention is associated with a single email address.", "labels": [], "entities": [{"text": "mention resolution", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7323972284793854}, {"text": "Enron collection", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.8414629995822906}]}, {"text": "The first two test collections were created by Minkov et al ().", "labels": [], "entities": []}, {"text": "These test collections correspond to two email accounts, \"sagere\" (the \"Sager\" collection) and \"shapiro-r\" (the \"Shapiro\" collection).", "labels": [], "entities": []}, {"text": "Their mention-queries and answer keys were generated automatically by identifying name mentions that correspond uniquely to individuals referenced in the cc header, and eliminating that cc entry from the header.", "labels": [], "entities": []}, {"text": "The third test collection, which we call the \"Enron-subset\" is an extended version of the test collection created by Diehl at al ().", "labels": [], "entities": []}, {"text": "Emails from all top-level folders were included in the collection, but only those that were both sent by and received by at least one email address of the form <name1>.<name2>@enron.com were retained.", "labels": [], "entities": []}, {"text": "A set of 78 mention-queries were manually selected and manually associated with the email address of the true referent by the third author using an interactive search system developed specifically to support that task.", "labels": [], "entities": []}, {"text": "The set of queries was limited to those that resolve to an address of the form <name1>.<name2>@enron.com.", "labels": [], "entities": []}, {"text": "Names found in salutation or signature lines or that exactly match <name1> or <name2> of any of the email participants were not selected as query-mentions.", "labels": [], "entities": []}, {"text": "Those 78 queries include the 54 used by Diehl et al.", "labels": [], "entities": []}, {"text": "For our fourth test collection (\"Enron-all\"), we used the same 78 mention-queries and the answer key from the Enron-subset collection, but we used the full CMU version of the Enron collection (with duplicates removed).", "labels": [], "entities": []}, {"text": "We use this collection to assess the scalability of our techniques.", "labels": [], "entities": []}, {"text": "Some descriptive statistics for each test collection are shown in.", "labels": [], "entities": []}, {"text": "The Sager and Shapiro collections are typical of personal collections, while the other two represent organizational collections.", "labels": [], "entities": []}, {"text": "These two types of collections differ markedly in the number of known identities and the candidate list sizes as shown in the table (the candidate list size is presented as an average over that collection's mention-queries and as the full range of values).", "labels": [], "entities": []}, {"text": "There are two commonly used single-valued evaluation measures for \"known item\"-retrieval tasks.", "labels": [], "entities": []}, {"text": "The \"Success @ 1\" measure characterizes the accuracy of one-best selection, computed as the mean across queries of the precision at the top rank for each query.", "labels": [], "entities": [{"text": "Success @ 1\" measure", "start_pos": 5, "end_pos": 25, "type": "METRIC", "confidence": 0.9319552302360534}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.999249279499054}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9886703491210938}]}, {"text": "For a single-valued figure of merit that considers every list position, we use \"Mean Reciprocal Rank\" (MRR), computed as the mean across queries of the inverse of the rank at which the correct referent is found.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank\" (MRR)", "start_pos": 80, "end_pos": 107, "type": "METRIC", "confidence": 0.9642798389707293}]}], "tableCaptions": [{"text": " Table 1: Test collections used in the experiments.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy results with different time periods.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990441203117371}]}]}