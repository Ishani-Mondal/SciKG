{"title": [{"text": "An Unsupervised Approach to Biography Production using Wikipedia", "labels": [], "entities": [{"text": "Biography Production", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7405107617378235}]}], "abstractContent": [{"text": "We describe an unsupervised approach to multi-document sentence-extraction based summarization for the task of producing biographies.", "labels": [], "entities": []}, {"text": "We utilize Wikipedia to automatically construct a corpus of biographical sentences and TDT4 to construct a corpus of non-biographical sentences.", "labels": [], "entities": [{"text": "TDT4", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.9351357221603394}]}, {"text": "We build a biographical-sentence classifier from these corpora and an SVM regression model for sentence ordering from the Wikipedia corpus.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7374246716499329}, {"text": "Wikipedia corpus", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.9249989092350006}]}, {"text": "We evaluate our work on the DUC2004 evaluation data and with human judges.", "labels": [], "entities": [{"text": "DUC2004 evaluation data", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.9609362085660299}]}, {"text": "Overall, our system significantly outperforms all systems that participated in DUC2004, according to the ROUGE-L metric, and is preferred by human subjects.", "labels": [], "entities": [{"text": "DUC2004", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.805403470993042}, {"text": "ROUGE-L metric", "start_pos": 105, "end_pos": 119, "type": "METRIC", "confidence": 0.971804141998291}]}], "introductionContent": [{"text": "Producing biographies by hand is a labor-intensive task, generally done only for famous individuals.", "labels": [], "entities": [{"text": "Producing biographies", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9234152436256409}]}, {"text": "The process is particularly difficult when persons of interest are not well known and when information must be gathered from a wide variety of sources.", "labels": [], "entities": []}, {"text": "We present an automatic, unsupervised, multi-document summarization (MDS) approach based on extractive techniques to producing biographies, answering the question \"Who is X?\"", "labels": [], "entities": [{"text": "multi-document summarization (MDS)", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.8011059403419495}]}, {"text": "There is growing interest in automatic MDS in general due in part to the explosion of multilingual and multimedia data available online.", "labels": [], "entities": [{"text": "MDS", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.6776145696640015}]}, {"text": "The goal of MDS is to automatically produce a concise, wellorganized, and fluent summary of a set of documents on the same topic.", "labels": [], "entities": [{"text": "MDS", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9036926031112671}]}, {"text": "MDS strategies have been employed to produce both generic summaries and query-focused summaries.", "labels": [], "entities": []}, {"text": "Due to the complexity of text generation, most summarization systems employ sentence-extraction techniques, in which the most relevant sentences from one or more documents are selected to represent the summary.", "labels": [], "entities": [{"text": "text generation", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7380709648132324}, {"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9776466488838196}]}, {"text": "This approach is guaranteed to produce grammatical sentences, although they must subsequently be ordered appropriately to produce a coherent summary.", "labels": [], "entities": []}, {"text": "In this paper we describe a sentence-extraction based MDS procedure to produce biographies from online resources automatically.", "labels": [], "entities": []}, {"text": "We make use of Wikipedia, the largest free multilingual encyclopedia on the internet, to build a biographical-sentence classifier and a component for ordering sentences in the output summary.", "labels": [], "entities": []}, {"text": "Section 2 presents an overview of our system.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our corpus and in Section 4 we discuss the components of our system in more detail.", "labels": [], "entities": []}, {"text": "In Section 5, we present an evaluation of our work on the Document Understanding Conference of 2004 (DUC2004), the biography task (task 5) test set.", "labels": [], "entities": [{"text": "Document Understanding Conference of 2004 (DUC2004)", "start_pos": 58, "end_pos": 109, "type": "DATASET", "confidence": 0.7051235288381577}]}, {"text": "In Section 6 we compare our research with previous work on biography generation.", "labels": [], "entities": [{"text": "biography generation", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7737006843090057}]}, {"text": "We conclude in Section 7 and identify directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our biography generation system, we use the document sets created for the biography evalua-  tion (task 5) of DUC2004.", "labels": [], "entities": [{"text": "biography generation", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7055934965610504}, {"text": "DUC2004", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9433400630950928}]}, {"text": "The task for systems participating in this evalution was \" Given each document cluster and a question of the form \"Who is X?\", where X is the name of a person or group of people, create a short summary (no longer than 665 bytes) of the cluster that responds to the question.\"", "labels": [], "entities": []}, {"text": "NIST assessors chose 50 clusters of TREC documents such that all the documents in a given cluster provide at least part of the answer to this question.", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9431561231613159}]}, {"text": "Each cluster contained on average 10 documents.", "labels": [], "entities": []}, {"text": "NIST had 4 human summaries written for each cluster.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.972451388835907}]}, {"text": "A baseline summary was also created for each cluster by extracting the first 665 bytes of the most recent document in the cluster.", "labels": [], "entities": []}, {"text": "22 systems participated in the competition, producing a total of 22 automatic summaries (restricted to 665 bytes) for each cluster.", "labels": [], "entities": []}, {"text": "We evaluate our system against the top performing of these 22 systems, according to ROUGE-L, which we denote top-DUC2004.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9934211373329163}]}, {"text": "As noted in Section 4.1, we experimented with a number of learning algorithms when building our biographical-sentence classifier.", "labels": [], "entities": []}, {"text": "For each machine learning algorithm tested, we build a system that initially classifies the input list of sentences into biographical and non-biographical sentences and then 3 http://duc.nist.gov/duc2004 4 Note that this system out-performed 19 of the 22 systems on ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2 (p < .05).", "labels": [], "entities": []}, {"text": "No ROUGE metric produced scores where this system scored significantly worse than any other system.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.9604908227920532}]}, {"text": "See below fora comparison of all DUC2004 systems with our top system where all systems are evaluated using ROUGE-L-1.5.5.", "labels": [], "entities": [{"text": "ROUGE-L-1.5.5", "start_pos": 107, "end_pos": 120, "type": "METRIC", "confidence": 0.9343321919441223}]}, {"text": "Next, we produce three versions of each system: one which implements a baseline ordering procedure, in which sentences from the clusters are ordered by their appearance in their source document (e.g. any sentence which occurred first in its original document is placed first in the summary, with ties ordered randomly within the set), a second which orders the biographical sentences by the confidence score obtained from the classifier, and a third which uses the SVM regression as the reordering component.", "labels": [], "entities": []}, {"text": "Finally, we run our reference rewriting component on each and trim the output to 665 bytes.", "labels": [], "entities": []}, {"text": "We evaluate first using the ROUGE-L metric (Lin and Hovy, 2003) with a 95% (ROUGE computed) confidence interval for all systems and compared these to the ROUGE-L score of the best-performing DUC2004 system.", "labels": [], "entities": [{"text": "ROUGE-L metric", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.9418154656887054}, {"text": "ROUGE computed) confidence interval", "start_pos": 76, "end_pos": 111, "type": "METRIC", "confidence": 0.9433575510978699}, {"text": "ROUGE-L", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9556882381439209}]}, {"text": "The higher the ROUGE score, the closer the summary is to the DUC2004 human reference summaries.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9817557334899902}, {"text": "DUC2004 human reference summaries", "start_pos": 61, "end_pos": 94, "type": "DATASET", "confidence": 0.9251379519701004}]}, {"text": "As shown in, our best performing system is the multinomial na\u00a8\u0131vena\u00a8\u0131ve Bayes classifier (MNB) using the classifier confidence scores to order the sentences in the biography.", "labels": [], "entities": []}, {"text": "This system significantly outperforms the top ranked DUC2004 system (top-DUC2004).", "labels": [], "entities": []}, {"text": "The success of this particularly learning algorithm on our task maybe due to: (1) the nature of our feature space -ngram frequencies are modeled properly by a multinomial distribution; (2) the simplicity of this classifier particularly given our large feature dimensional-ity; and (3) the robustness of na\u00a8\u0131vena\u00a8\u0131ve Bayes with respect to noisy data: Not all sentences in Wikipedia biographies are biographical sentences and some sentences in TDT4 are biographical.", "labels": [], "entities": [{"text": "TDT4", "start_pos": 442, "end_pos": 446, "type": "DATASET", "confidence": 0.976592481136322}]}, {"text": "While the SVM regression reordering component has a slight negative impact on the performance of the MNB system, the difference between the two versions is not significant.", "labels": [], "entities": []}, {"text": "Note however, that both the C4.5 and the SVM versions of our system are improved by the SVM regression sentence reordering.", "labels": [], "entities": []}, {"text": "While neither performs better than top-DUC2004 without this component, the C4.5 system with SVM reordering is significantly better than top-DUC2004 and the performance of the SVM system with SVM regression is comparable to top-DUC2004.", "labels": [], "entities": []}, {"text": "In fact, when we use only the SVM regression model to rank the hypothesis sentences, without employing any classifier, then remove redundant sentences, rewrite and trim the results, we find that, interestingly, this approach also outperforms top-DUC2004, although the difference is not statistically significant.", "labels": [], "entities": []}, {"text": "However, we believe that this is an area worth pursuing in future, with more sophisticated features.", "labels": [], "entities": []}, {"text": "The following biography of Brian Jones was produced by our MNB system and then the sentences were ordered using the SVM regression model: Born in Bristol in 1947, Brian Jones, the co-pilot on the Breitling mission, learned to fly at 16, dropping out of school a year later to join the Royal Air Force.", "labels": [], "entities": [{"text": "MNB system", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.9240207970142365}, {"text": "Royal Air", "start_pos": 285, "end_pos": 294, "type": "DATASET", "confidence": 0.8790515065193176}]}, {"text": "After earning his commercial balloon flying license, Jones became a ballooning instructor in 1989 and was certified as an examiner for balloon flight licenses by the British Civil Aviation Authority.", "labels": [], "entities": [{"text": "British Civil Aviation Authority", "start_pos": 166, "end_pos": 198, "type": "DATASET", "confidence": 0.9594321846961975}]}, {"text": "He helped organize Breitling's most recent around-the-world attempts, in 1997 and 1998.", "labels": [], "entities": []}, {"text": "Jones, 52, replaces fellow British flight engineer Tony Brown.", "labels": [], "entities": []}, {"text": "Jones, who is to turn 52 next week, is actually the team's third co-pilot.", "labels": [], "entities": []}, {"text": "After 13 years of service, he joined a catering business and, in the 1980s,...", "labels": [], "entities": []}, {"text": "illustrates the performance of our MNB system with classifier confidence score sentence ordering when compared to mean ROUGE-L-1.5.5 scores of DUC2004 human-generated summaries and the 22 DUC2004 systems' summaries across all summary tasks.", "labels": [], "entities": [{"text": "ROUGE-L-1.5.5", "start_pos": 119, "end_pos": 132, "type": "METRIC", "confidence": 0.922627866268158}]}, {"text": "Human summaries are labeled A-H, DUC2004 systems 1-22, and our MNB system is marked by the rectangle.", "labels": [], "entities": []}, {"text": "Results are sorted by mean ROUGE-L score.", "labels": [], "entities": [{"text": "ROUGE-L score", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.9382713139057159}]}, {"text": "Note that our system performance is actually comparable in ROUGE-L score to one of the human summary generators and is significantly better that all DUC2004 systems, including top-DUC2004, which is System 1 in the figure.", "labels": [], "entities": [{"text": "ROUGE-L score", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9821757376194}]}, {"text": "ROUGE evaluation is based on n-gram overlap between the automatically produced summary and the human reference summaries.", "labels": [], "entities": [{"text": "ROUGE evaluation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5805238783359528}]}, {"text": "Thus, it is notable to measure how fluent or coherent a summary is.", "labels": [], "entities": []}, {"text": "Sentence ordering is one factor in determining fluency and coherence.", "labels": [], "entities": [{"text": "Sentence ordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8874232470989227}]}, {"text": "So, we conducted two experiments to measure these qualities, one comparing our topperforming system according to ROUGE-L score (MNB) vs. the top-performing DUC2004 system (top-DUC2004) and another comparing our top system with two different ordering methods, classifierbased and SVM regression.", "labels": [], "entities": [{"text": "ROUGE-L score (MNB)", "start_pos": 113, "end_pos": 132, "type": "METRIC", "confidence": 0.9336992263793945}]}, {"text": "In each experiment, summaries were trimmed to 665 bytes.", "labels": [], "entities": []}, {"text": "In the first experiment, three native American English speakers were presented with the 50 questions (Who is X?).", "labels": [], "entities": []}, {"text": "For each question they were given a pair of summaries (presented in random order): one was the output of our MNB system and the other was the summary produced by the top-DUC2004 system.", "labels": [], "entities": [{"text": "MNB system", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.8255639970302582}]}, {"text": "Subjects were asked to decide which summary was more responsive inform and content to the question or whether both were equally responsive.", "labels": [], "entities": []}, {"text": "85.3% (128/150) of subject judgments preferred one summary over the other.", "labels": [], "entities": []}, {"text": "100/128 (78.1%) of these judgments preferred the summaries produced by our MNB system over those produced by top-DUC2004.", "labels": [], "entities": [{"text": "MNB system", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.8954885005950928}]}, {"text": "If we compute the majority vote, there were 42/50 summaries in which at least two subjects made the same choice.", "labels": [], "entities": []}, {"text": "37/42 (88.1%) of these majority judgments preferred our system's summary (using binomial test, p = 4.4e \u2212 7).", "labels": [], "entities": [{"text": "summary", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9751331210136414}]}, {"text": "We used the weighted kappa statistic with quadratic weighting to determine the inter-rater agreement, obtaining a mean pairwise \u03ba of 0.441.", "labels": [], "entities": []}, {"text": "Recall from Section 5.1 that our SVM regression reordering component slightly decreases the average ROUGE score (although not significantly) for our MNB system.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.9843686819076538}]}, {"text": "For our human evaluations, we decided to evaluate the quality of the presentation of our summaries with and without this compo- Figure 2: ROUGE-L scores for DUC2004 human summaries (A-H), our MNB system (rectangle), and the DUC2004 competing systems (1-22 anonymized), with the baseline system labeled BL.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.9975311160087585}, {"text": "BL", "start_pos": 302, "end_pos": 304, "type": "METRIC", "confidence": 0.9821637868881226}]}, {"text": "nent to see if this reordering component affected human judgments even if it did not improve ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 93, "end_pos": 105, "type": "METRIC", "confidence": 0.9695318043231964}]}, {"text": "For each question, we produced two summaries from the sentences classified as biographical by the MNB classifier, one ordered by the confidence score obtained by the MNB, in decreasing order, and the other ordered by the SVM regression values, in increasing order.", "labels": [], "entities": [{"text": "MNB classifier", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.8848446905612946}, {"text": "MNB", "start_pos": 166, "end_pos": 169, "type": "DATASET", "confidence": 0.8932055830955505}]}, {"text": "Note that, in three cases, the summary sentences were ordered identically by both procedures, so we used only 47 summaries for this evaluation.", "labels": [], "entities": []}, {"text": "Three (different) native American English speakers were presented with the 47 questions for which sentence ordering differed.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7040301710367203}]}, {"text": "For each question they were given the two summaries (presented in random order) and asked to determine which biography they preferred.", "labels": [], "entities": []}, {"text": "We found inter-rater agreement for these judgments using to be only moderate (\u03ba=0.362).", "labels": [], "entities": []}, {"text": "However, when we computed the majority vote for each question, we found that 61.7% (29/47) preferred the SVM regression ordering over the MNB classifier confidence score ordering.", "labels": [], "entities": []}, {"text": "Although this difference is not statistically significant, again we find the SVM regression ordering results encouraging enough to motivate our further research on improving such ordering procedures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Binary classification results: Wikipedia bi- ography class-based/lexical sentences vs. TDT4 class- based/lexical sentences", "labels": [], "entities": [{"text": "Binary classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9550209939479828}]}]}