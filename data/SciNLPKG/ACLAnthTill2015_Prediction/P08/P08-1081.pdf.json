{"title": [{"text": "Using Conditional Random Fields to Extract Contexts and Answers of Questions from Online Forums", "labels": [], "entities": [{"text": "Extract Contexts and Answers of Questions from Online Forums", "start_pos": 35, "end_pos": 95, "type": "TASK", "confidence": 0.7377558483017815}]}], "abstractContent": [{"text": "Online forum discussions often contain vast amounts of questions that are the focuses of discussions.", "labels": [], "entities": []}, {"text": "Extracting contexts and answers together with the questions will yield not only a coherent forum summary but also a valuable QA knowledge base.", "labels": [], "entities": []}, {"text": "In this paper, we propose a general framework based on Conditional Random Fields (CRFs) to detect the contexts and answers of questions from forum threads.", "labels": [], "entities": []}, {"text": "We improve the basic framework by Skip-chain CRFs and 2D CRFs to better accommodate the features of forums for better performance.", "labels": [], "entities": []}, {"text": "Experimental results show that our techniques are very promising.", "labels": [], "entities": []}], "introductionContent": [{"text": "Forums are web virtual spaces where people can ask questions, answer questions and participate in discussions.", "labels": [], "entities": []}, {"text": "The availability of vast amounts of thread discussions in forums has promoted increasing interests in knowledge acquisition and summarization for forum threads.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.707490012049675}, {"text": "summarization", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.984565794467926}]}, {"text": "Forum thread usually consists of an initiating post and a number of reply posts.", "labels": [], "entities": []}, {"text": "The initiating post usually contains several questions and the reply posts usually contain answers to the questions and perhaps new questions.", "labels": [], "entities": []}, {"text": "Forum participants are not physically co-present, and thus reply may not happen immediately after questions are posted.", "labels": [], "entities": []}, {"text": "The asynchronous nature and multiparticipants make multiple questions and answers * This work was done when Shilin Ding was a visiting student at the Microsoft Research Asia \u2020 This work was done when Gao Cong worked as a researcher at the Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 150, "end_pos": 173, "type": "DATASET", "confidence": 0.8215742309888204}, {"text": "Microsoft Research Asia", "start_pos": 239, "end_pos": 262, "type": "DATASET", "confidence": 0.922986626625061}]}, {"text": "<context id=1>S1: Hi I am looking fora pet friendly hotel in Hong Kong because all of my family is going therefor vacation.", "labels": [], "entities": []}, {"text": "S2: my family has 2 sons and a dog.</context> <question id=1>S3: Is there any recommended hotel near Sheung Wan or Tsing Sha Tsui?</question> <context id=2,3>S4: We also plan to go shopping in Causeway Bay.</context> <question id=2>S5: What's the traffic situation around those commercial areas?</question> <question id=3>S6: Is it necessary to take a taxi?</question>.", "labels": [], "entities": [{"text": "Causeway Bay.", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.9349915981292725}]}, {"text": "S7: Any information would be appreciated.", "labels": [], "entities": []}, {"text": "<answer qid=1>S8: The Comfort Lodge near Kowloon Park allows pet as I know, and usually fits well within normal budget.", "labels": [], "entities": [{"text": "Comfort Lodge near Kowloon Park", "start_pos": 22, "end_pos": 53, "type": "DATASET", "confidence": 0.7517632365226745}]}, {"text": "S9: It is also conveniently located, nearby the Kowloon railway station and subway.</answer> <answer qid=2,3> S10: It's very crowd in those areas, so I recommend MTR in Causeway Bay because it is cheap to take you around </answer> interweaved together, which makes it more difficult to summarize.", "labels": [], "entities": [{"text": "MTR in Causeway Bay", "start_pos": 162, "end_pos": 181, "type": "DATASET", "confidence": 0.7152502238750458}]}, {"text": "In this paper, we address the problem of detecting the contexts and answers from forum threads for the questions identified in the same threads.", "labels": [], "entities": []}, {"text": "gives an example of a forum thread with questions, contexts and answers annotated.", "labels": [], "entities": []}, {"text": "It contains three question sentences, S3, S5 and S6.", "labels": [], "entities": []}, {"text": "Sentences S1 and S2 are contexts of question 1 (S3).", "labels": [], "entities": []}, {"text": "Sentence S4 is the context of questions 2 and 3, but not 1.", "labels": [], "entities": [{"text": "Sentence S4", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8124521970748901}]}, {"text": "Sentence S8 is the answer to question 3.", "labels": [], "entities": []}, {"text": "(S4-S5-S10) is one example of question-context-answer triple that we want to detect in the thread.", "labels": [], "entities": []}, {"text": "As shown in the example, a forum question usually requires contextual information to provide background or constraints.", "labels": [], "entities": []}, {"text": "Moreover, it sometimes needs contextual information to provide explicit link to its answers.", "labels": [], "entities": []}, {"text": "For example, S8 is an answer of question 1, but they cannot be linked with any common word.", "labels": [], "entities": []}, {"text": "Instead, S8 shares word pet with S1, which is a context of question 1, and thus S8 could be linked with question 1 through S1.", "labels": [], "entities": []}, {"text": "We call contextual information the context of a question in this paper.", "labels": [], "entities": []}, {"text": "A summary of forum threads in the form of question-context-answer cannot only highlight the main content, but also provide a user-friendly organization of threads, which will make the access to forum information easier.", "labels": [], "entities": []}, {"text": "Another motivation of detecting contexts and answers of the questions in forum threads is that it could be used to enrich the knowledge base of community-based question and answering (CQA) services such as Live QnA and Yahoo!", "labels": [], "entities": [{"text": "detecting contexts and answers of the questions in forum threads", "start_pos": 22, "end_pos": 86, "type": "TASK", "confidence": 0.7764843344688416}]}, {"text": "Answers, where context is comparable with the question description while question corresponds to the question title.", "labels": [], "entities": []}, {"text": "For example, there were about 700,000 questions in the Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8765629231929779}]}, {"text": "Answers travel category as of January 2008.", "labels": [], "entities": [{"text": "Answers travel category", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.6422788898150126}]}, {"text": "We extracted about 3,000,000 travel related questions from six online travel forums.", "labels": [], "entities": []}, {"text": "One would expect that a CQA service with large QA data will attract more users to the service.", "labels": [], "entities": []}, {"text": "To enrich the knowledge base, not only the answers, but also the contexts are critical; otherwise the answer to a question such as How much is the taxi would be useless without context in the database.", "labels": [], "entities": []}, {"text": "However, it is challenging to detecting contexts and answers for questions in forum threads.", "labels": [], "entities": []}, {"text": "We assume the questions have been identified in a forum thread using the approach in (.", "labels": [], "entities": []}, {"text": "Although identifying questions in a forum thread is also nontrivial, it is beyond the focus of this paper.", "labels": [], "entities": []}, {"text": "First, detecting contexts of a question is important and non-trivial.", "labels": [], "entities": [{"text": "detecting contexts of a question", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.8749359846115112}]}, {"text": "We found that 74% of questions in our corpus, which contain 1,064 questions from 579 forum threads about travel, need contexts.", "labels": [], "entities": []}, {"text": "However, relative position information is far from adequate to solve the problem.", "labels": [], "entities": []}, {"text": "For example, in our corpus 63% of sentences preceding questions are contexts and they only represent 34% of all correct contexts.", "labels": [], "entities": []}, {"text": "To effectively detect contexts, the dependency between sentences is important.", "labels": [], "entities": []}, {"text": "For example in, both S1 and S2 are contexts of question 1.", "labels": [], "entities": []}, {"text": "S1 could be labeled as context based on word similarity, but it is not easy to link S2 with the question directly.", "labels": [], "entities": []}, {"text": "S1 and S2 are linked by the common word family, and thus S2 can be linked with question 1 through S1.", "labels": [], "entities": []}, {"text": "The challenge here is how to model and utilize the dependency for context detection.", "labels": [], "entities": [{"text": "context detection", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.792307049036026}]}, {"text": "Second, it is difficult to link answers with questions.", "labels": [], "entities": []}, {"text": "In forums, multiple questions and answers can be discussed in parallel and are interweaved together while the reply relationship between posts is usually unavailable.", "labels": [], "entities": []}, {"text": "To detect answers, we need to handle two kinds of dependencies.", "labels": [], "entities": []}, {"text": "One is the dependency relationship between contexts and answers, which should be leveraged especially when questions alone do not provide sufficient information to find answers; the other is the dependency between answer candidates (similar to sentence dependency described above).", "labels": [], "entities": []}, {"text": "The challenge is how to model and utilize these two kinds of dependencies.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel approach for detecting contexts and answers of the questions in forum threads.", "labels": [], "entities": [{"text": "detecting contexts and answers of the questions in forum threads", "start_pos": 46, "end_pos": 110, "type": "TASK", "confidence": 0.7535031914710999}]}, {"text": "To our knowledge this is the first work on this.", "labels": [], "entities": []}, {"text": "We make the following contributions: First, we employ Linear Conditional Random Fields (CRFs) to identify contexts and answers, which can capture the relationships between contiguous sentences.", "labels": [], "entities": []}, {"text": "Second, we also found that context is very important for answer detection.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.9665926694869995}]}, {"text": "To capture the dependency between contexts and answers, we introduce Skip-chain CRF model for answer detection.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.8648990392684937}]}, {"text": "We also extend the basic model to 2D CRFs to model dependency between contiguous questions in a forum thread for context and answer identification.", "labels": [], "entities": [{"text": "answer identification", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.6765469610691071}]}, {"text": "Finally, we conducted experiments on forum data.", "labels": [], "entities": [{"text": "forum data", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.837382435798645}]}, {"text": "Experimental results show that 1) Linear CRFs outperform SVM and decision tree in both context and answer detection; 2) Skip-chain CRFs outperform Linear CRFs for answer finding, which demonstrates that context improves answer finding; 3) 2D CRF model improves the performance of Linear CRFs and the combination of 2D CRFs and Skipchain CRFs achieves better performance for context detection.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.764548659324646}, {"text": "answer finding", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.8200316727161407}, {"text": "answer finding", "start_pos": 220, "end_pos": 234, "type": "TASK", "confidence": 0.7033810615539551}, {"text": "context detection", "start_pos": 374, "end_pos": 391, "type": "TASK", "confidence": 0.737349808216095}]}, {"text": "The rest of this paper is organized as follows: The next section discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the proposed techniques.", "labels": [], "entities": []}, {"text": "We evaluate our techniques in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes this paper and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We obtained about 1 million threads from TripAdvisor forum; we randomly selected 591 threads and removed 22 threads which has more than 40 sentences and 6 questions; the remaining 579 forum threads form our corpus 2 . Each thread in our  Metrics.", "labels": [], "entities": [{"text": "TripAdvisor forum", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9289071559906006}]}, {"text": "We calculated precision, recall, and F 1 -score for all tasks.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9998008608818054}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9997096657752991}, {"text": "F 1 -score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9866394400596619}]}, {"text": "All the experimental results are obtained through the average of 5 trials of 5-fold cross validation.", "labels": [], "entities": []}, {"text": "Linear CRFs for Context and Answer Detection.", "labels": [], "entities": [{"text": "Context and Answer Detection", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6619938462972641}]}, {"text": "This experiment is to evaluate Linear CRF model (Section 3.1) for context and answer detection by comparing with SVM and C4.5.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7751486599445343}]}, {"text": "For SVM, we use SVM light (Joachims, 1999).", "labels": [], "entities": [{"text": "SVM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8953884840011597}]}, {"text": "We tried linear, polynomial and RBF kernels and report the results on polynomial kernel using default parameters since it performs the best in the experiment.", "labels": [], "entities": []}, {"text": "SVM and C4.5 use the same set of features as Linear CRFs.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9219157099723816}]}, {"text": "As shown in   We next report a baseline of context detection using previous sentences in the same post with its question since contexts often occur in the question post or preceding posts.", "labels": [], "entities": [{"text": "context detection", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7269913107156754}]}, {"text": "Similarly, we report a baseline of answer detecting using following segments of a question as answers.", "labels": [], "entities": [{"text": "answer detecting", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.696687862277031}]}, {"text": "The results given in show that location information is far from adequate to detect contexts and answers.", "labels": [], "entities": []}, {"text": "This experiment is to evaluate the usefulness of contexts in answer detection, by adding the similarity between the context (obtained with different methods) and candidate answer as an extra feature for CRFs.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.9134796857833862}]}, {"text": "shows the impact of context on answer detection using Linear CRFs.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9237042963504791}]}, {"text": "Linear CRFs with contextual information perform better than those without context.", "labels": [], "entities": []}, {"text": "L-CRF+context is close to that using real context, while it is better than CRFs using the previous sentence as context.", "labels": [], "entities": []}, {"text": "The results clearly shows that contextual information greatly improves the performance of answer detection.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.911050409078598}]}, {"text": "This experiment is to evaluate the effectiveness of Skip-Chain CRFs (Section 3.2) and 2D CRFs (Section 3.3) for our tasks.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "In context detection, Skip-Chain CRFs have simi-  In answer detection, as expected, Skip-chain CRFs outperform L-CRF+context since Skip-chain CRFs can model the inter-dependency between contexts and answers while in L-CRF+context the context can only be reflected by the features on the observations.", "labels": [], "entities": [{"text": "context detection", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7472229599952698}, {"text": "answer detection", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.8577517569065094}]}, {"text": "We also observed that 2D CRFs improve the performance of L-CRF+context due to the dependency between contiguous questions.", "labels": [], "entities": []}, {"text": "In contrast with our expectation, the 2D+Skip-chain CRFs does not improve Skip-chain CRFs in terms of answer detection.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.8544808626174927}]}, {"text": "The possible reason could be that the structure of the graph is very complicated and too many parameters need to be learned on our training data.", "labels": [], "entities": []}, {"text": "We also evaluated the contributions of each category of features in to context detection.", "labels": [], "entities": [{"text": "context detection", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7754223942756653}]}, {"text": "We found that similarity features are the most important and structural feature the next.", "labels": [], "entities": []}, {"text": "We also observed the same trend for answer detection.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.9573728740215302}]}, {"text": "We omit the details here due to space limitation.", "labels": [], "entities": []}, {"text": "As a summary, 1) our CRF model outperforms SVM and C4.5 for both context and answer detections; 2) context is very useful in answer detection; 3) the Skip-chain CRF method is effective in leveraging context for answer detection; and 4) 2D CRF model improves the performance of Linear CRFs for both context and answer detection.", "labels": [], "entities": [{"text": "answer detections", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.6821354180574417}, {"text": "answer detection", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.8981586992740631}, {"text": "answer detection", "start_pos": 211, "end_pos": 227, "type": "TASK", "confidence": 0.8263571858406067}, {"text": "answer detection", "start_pos": 310, "end_pos": 326, "type": "TASK", "confidence": 0.7131400853395462}]}], "tableCaptions": [{"text": " Table 1: Contingency table(\u03c7 2 = 9,386,p-value<0.001)", "labels": [], "entities": []}, {"text": " Table 4: Context and Answer Detection", "labels": [], "entities": [{"text": "Answer Detection", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8182592988014221}]}, {"text": " Table 5: Using position information for detection", "labels": [], "entities": [{"text": "detection", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.708210289478302}]}, {"text": " Table 6: Contextual Information for Answer Detection.  Prev. sentence uses one previous sentence of the current  question as context. RealContext uses the context anno- tated by experts. L-CRF+context uses the context found  by Linear CRFs", "labels": [], "entities": [{"text": "Answer Detection", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.9396195113658905}]}, {"text": " Table 7 and Table 8.", "labels": [], "entities": []}, {"text": " Table 7: Skip-chain and 2D CRFs for context detection", "labels": [], "entities": [{"text": "Skip-chain", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8539868593215942}, {"text": "context detection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8568131625652313}]}, {"text": " Table 8: Skip-chain and 2D CRFs for answer detection", "labels": [], "entities": [{"text": "answer detection", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.9685038328170776}]}]}