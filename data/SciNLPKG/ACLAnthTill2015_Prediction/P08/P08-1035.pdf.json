{"title": [{"text": "A Generic Sentence Trimmer with CRFs", "labels": [], "entities": [{"text": "Generic Sentence Trimmer", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.6355239550272623}]}], "abstractContent": [{"text": "The paper presents a novel sentence trimmer in Japanese, which combines a non-statistical yet generic tree generation model and Conditional Random Fields (CRFs), to address improving the grammaticality of compression while retaining its relevance.", "labels": [], "entities": [{"text": "sentence trimmer", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7205222696065903}]}, {"text": "Experiments found that the present approach out-performs in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007) \u2212 the only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison.", "labels": [], "entities": [{"text": "Japanese compression)", "start_pos": 259, "end_pos": 280, "type": "TASK", "confidence": 0.7020490368207296}]}], "introductionContent": [{"text": "For better or worse, much of prior work on sentence compression () turned to a single corpus developed by (K&M, henceforth) for evaluating their approaches.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7790858447551727}]}, {"text": "The K&M corpus is a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived.", "labels": [], "entities": [{"text": "K&M corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7608649879693985}]}, {"text": "Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results;).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7743943929672241}]}, {"text": "It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data).", "labels": [], "entities": []}, {"text": "Our approach is broadly inline with prior work), in that we make use of some form of syntactic knowledge to constrain compressions we generate.", "labels": [], "entities": []}, {"text": "What sets this work apart from them, however, is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions ().", "labels": [], "entities": []}, {"text": "An obvious benefit of using CRFs for sentence compression is that the model provides a general (and principled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property K&M do not share.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.755535900592804}]}, {"text": "Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.7219664007425308}]}, {"text": "We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar.", "labels": [], "entities": []}, {"text": "Thus, unlike, and, we do not insist on finding a globally optimal solution in the space of 2 n possible compressions for an n word long sentence.", "labels": [], "entities": []}, {"text": "Rather we insist on finding a most plausible compression among those that are explicitly warranted by the grammar.", "labels": [], "entities": []}, {"text": "Later in the paper, we will introduce an approach called the 'Dependency Path Model' (DPM) from the previous literature (Section 4), which purports to provide a robust framework for sentence compres-sion in Japanese.", "labels": [], "entities": []}, {"text": "We will look at how the present approach compares with that of DPM in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created a corpus of sentence summaries based on email news bulletins we had received over five to six months from an on-line news provider called Nikkei Net, which mostly deals with finance and politics.", "labels": [], "entities": [{"text": "Nikkei Net", "start_pos": 149, "end_pos": 159, "type": "DATASET", "confidence": 0.9494333267211914}]}, {"text": "Each bulletin consists of six to seven news briefs, each with a few sentences.", "labels": [], "entities": []}, {"text": "Since a news brief contains nothing to indicate what its longer version makes good sense; only slightly flawed in grammar 5 makes perfect sense; no grammar flaws might look like, we manually searched the news site fora full-length article that might reasonably be considered along version of that brief.", "labels": [], "entities": []}, {"text": "We extracted lead sentences both from the brief and from its source article, and aligned them, using what is known as the Smith-Waterman algorithm, which produced 1,401 pairs of summary and source sentence.", "labels": [], "entities": []}, {"text": "For the ease of reference, we call the corpus so produced 'NICOM' for the rest of the paper.", "labels": [], "entities": [{"text": "NICOM", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.7931869029998779}]}, {"text": "A part of our system makes use of a modeling toolkit called GRMM ().", "labels": [], "entities": [{"text": "GRMM", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.740087628364563}]}, {"text": "Throughout the experiments, we call our approach 'Generic Sentence Trimmer' or GST.", "labels": [], "entities": [{"text": "Generic Sentence Trimmer", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6071097056070963}]}], "tableCaptions": [{"text": " Table 3: The rating scale on content overlap", "labels": [], "entities": []}, {"text": " Table 5: Semantic (Content) Overlap (Average)", "labels": [], "entities": [{"text": "Overlap", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9726434350013733}, {"text": "Average", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9207148551940918}]}]}