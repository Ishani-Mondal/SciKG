{"title": [{"text": "Learning to Rank Answers on Large Online QA Collections", "labels": [], "entities": [{"text": "Learning to Rank Answers on Large Online QA Collections", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.71181610888905}]}], "abstractContent": [{"text": "This work describes an answer ranking engine for non-factoid questions built using a large online community-generated question-answer collection (Yahoo! Answers).", "labels": [], "entities": []}, {"text": "We show how such collections maybe used to effectively setup large supervised learning experiments.", "labels": [], "entities": []}, {"text": "Furthermore we investigate a wide range of feature types, some exploiting NLP processors , and demonstrate that using them in combination leads to considerable improvements inaccuracy.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of Question Answering (QA) has received considerable attention in the past few years.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8785957634449005}]}, {"text": "Nevertheless, most of the work has focused on the task of factoid QA, where questions match short answers, usually in the form of named or numerical entities.", "labels": [], "entities": [{"text": "factoid QA", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.7962726056575775}]}, {"text": "Thanks to international evaluations organized by conferences such as the Text REtrieval Conference (TREC) or the Cross Language Evaluation Forum (CLEF) Workshop 2 , annotated corpora of questions and answers have become available for several languages, which has facilitated the development of robust machine learning models for the task.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.7991284976402918}]}, {"text": "The situation is different once one moves beyond the task of factoid QA.", "labels": [], "entities": [{"text": "factoid QA", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.765772134065628}]}, {"text": "Comparatively little research has focused on QA models for non-factoid questions such as causation, manner, or reason questions.", "labels": [], "entities": []}, {"text": "Because virtually no training data is available for this problem, most automated systems train either  on small hand-annotated corpora builtin house) or on question-answer pairs harvested from Frequently Asked Questions (FAQ) lists or similar resources).", "labels": [], "entities": []}, {"text": "None of these situations is ideal: the cost of building the training corpus in the former setup is high; in the latter scenario the data tends to be domain-specific, hence unsuitable for the learning of open-domain models.", "labels": [], "entities": []}, {"text": "On the other hand, recent years have seen an explosion of user-generated content (or social media).", "labels": [], "entities": []}, {"text": "Of particular interest in our context are communitydriven question-answering sites, such as Yahoo!", "labels": [], "entities": []}, {"text": "Answers , where users answer questions posed by other users and best answers are selected manually either by the asker or by all the participants in the thread.", "labels": [], "entities": []}, {"text": "The data generated by these sites has significant advantages over other web resources: (a) it has a high growth rate and it is already abundant; (b) it covers a large number of topics, hence it offers a better approximation of open-domain content; and (c) it is available for many languages.", "labels": [], "entities": []}, {"text": "Community QA sites, similar to FAQs, provide large number of questionanswer pairs.", "labels": [], "entities": [{"text": "FAQs", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.701524019241333}]}, {"text": "Nevertheless, this data has a significant drawback: it has high variance of quality, i.e., answers range from very informative to completely irrelevant or even abusive.", "labels": [], "entities": []}, {"text": "shows some examples of both high and low quality content.", "labels": [], "entities": []}, {"text": "In this paper we address the problem of answer ranking for non-factoid questions from social media content.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.8690113723278046}]}, {"text": "Our research objectives focus on answering the following two questions:", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our results using two measures: mean Precision at rank=1 (P@1) -i.e., the percentage of questions with the correct answer on the first position -and Mean Reciprocal Rank (MRR) -i.e., the score of a question is 1/k, where k is the position of the correct answer.", "labels": [], "entities": [{"text": "mean", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9838762879371643}, {"text": "Precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.7617375254631042}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 161, "end_pos": 187, "type": "METRIC", "confidence": 0.9615849256515503}]}, {"text": "We use as baseline the output of our answer retrieval component).", "labels": [], "entities": []}, {"text": "This component uses the BM25 criterion, the highest performing IR model in our experiments.", "labels": [], "entities": [{"text": "BM25", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9547521471977234}]}, {"text": "lists the results obtained using this baseline and our best model (\"Ranking\" in the table) on the testing partition.", "labels": [], "entities": []}, {"text": "Since we are interested in the performance of the ranking model, we evaluate on the subset of questions where the correct answer is retrieved by answer retrieval in the top N answers (similar to  results for several N values.", "labels": [], "entities": []}, {"text": "For completeness, we show the percentage of questions that match this criterion in the \"recall@N\" row.", "labels": [], "entities": [{"text": "recall@N\" row", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.9188607096672058}]}, {"text": "Our ranking model was tuned strictly on the development set (i.e., feature selection and parameters of the translation models).", "labels": [], "entities": []}, {"text": "During training, the presentation of the training instances is randomized, which generates a randomized ranking algorithm.", "labels": [], "entities": []}, {"text": "We exploit this property to estimate the variance in the results produced by each model and report the average result over 10 trials together with an estimate of the standard deviation.", "labels": [], "entities": []}, {"text": "The baseline result shows that, for N = 15, BM25 alone can retrieve in first rank 41% of the correct answers, and MRR tells us that the correct answer is often found within the first three answers (this is not so surprising if we remember that in this configuration only questions with the correct answer in the first 15 were kept for the experiment).", "labels": [], "entities": [{"text": "BM25", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.7105781435966492}, {"text": "MRR", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.992567777633667}]}, {"text": "The baseline results are interesting because they indicate that the problem is not hopelessly hard, but it is far from trivial.", "labels": [], "entities": []}, {"text": "In principle, we see much room for improvement over bag-of-word methods.", "labels": [], "entities": []}, {"text": "Next we see that learning a weighted combination of features yields consistently marked improvements: for example, for N = 15, the best model yields a 19% relative improvement in P@1 and 14% in MRR.", "labels": [], "entities": [{"text": "P", "start_pos": 179, "end_pos": 180, "type": "METRIC", "confidence": 0.9819570779800415}, {"text": "MRR", "start_pos": 194, "end_pos": 197, "type": "TASK", "confidence": 0.34409675002098083}]}, {"text": "More importantly, the results indicate that the model learned is stable: even though for the model analyzed in we used N = 15 in training, we measure approximately the same relative improvement as N increases during evaluation.", "labels": [], "entities": []}, {"text": "These results provide robust evidence that: (a) we can use publicly available online QA collections to investigate features for answer ranking without the need for costly human evaluation, (b) we can exploit large and noisy online QA collections to improve the accuracy of answer ranking systems and (c) readily available and scalable NLP technology can be used: Summary of the model selection process.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.9053253531455994}, {"text": "accuracy", "start_pos": 261, "end_pos": 269, "type": "METRIC", "confidence": 0.987316906452179}]}, {"text": "to improve lexical matching and translation models.", "labels": [], "entities": [{"text": "lexical matching", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7231966853141785}]}, {"text": "In the remaining of this section we analyze the performance of the different features.", "labels": [], "entities": []}, {"text": "summarizes the outcome of our automatic greedy feature selection process on the development set.", "labels": [], "entities": []}, {"text": "Where applicable, we show within parentheses the text representation for the corresponding feature.", "labels": [], "entities": []}, {"text": "The process is initialized with a single feature that replicates the baseline model (BM25 applied to the bag-of-words (W) representation).", "labels": [], "entities": [{"text": "BM25", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.765117347240448}]}, {"text": "The algorithm incrementally adds to the feature set the feature that provides the highest MRR improvement in the development partition.", "labels": [], "entities": [{"text": "MRR", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.582723081111908}]}, {"text": "The process stops when no features yield any improvement.", "labels": [], "entities": []}, {"text": "The table shows that, while the features selected span all the four feature groups introduced, the lion's share is taken by the translation features: approximately 60% of the MRR  improvement is achieved by these features.", "labels": [], "entities": [{"text": "MRR", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.7380059361457825}]}, {"text": "The frequency/density features are responsible for approximately 23% of the improvement.", "labels": [], "entities": []}, {"text": "The rest is due to the query-log correlation features.", "labels": [], "entities": []}, {"text": "This indicates that, even though translation models are the most useful, it is worth exploring approaches that combine several strategies for answer ranking.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9655123949050903}, {"text": "answer ranking", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.9205189645290375}]}, {"text": "Note that if some features do not appear in it does not necessarily mean that they are useless.", "labels": [], "entities": []}, {"text": "In some cases such features are highly correlated with features previously selected, which already exploited their signal.", "labels": [], "entities": []}, {"text": "For example, most similarity features (FG1) are correlated.", "labels": [], "entities": [{"text": "similarity features (FG1)", "start_pos": 18, "end_pos": 43, "type": "METRIC", "confidence": 0.8576838135719299}]}, {"text": "Because BM25(W) is part of the baseline model, the selection process chooses another FG1 feature only much later (iteration 9) when the model is significantly changed.", "labels": [], "entities": [{"text": "BM25", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.5820996761322021}]}, {"text": "On the other hand, some features do not provide a useful signal at all.", "labels": [], "entities": []}, {"text": "A notable example in this class is the web-based CCP feature, which was designed originally for factoid answer validation and does not adapt well to our problem.", "labels": [], "entities": [{"text": "factoid answer validation", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.7762322227160136}]}, {"text": "Because the length of non-factoid answers is typically significantly larger than in the factoid QA task, we have to discard a large part of the query when computing hits(Q+A) to reach non-zero counts.", "labels": [], "entities": []}, {"text": "This means that the final hit counts, hence the CCP value, are generally uncorrelated with the original (Q,A) tuple.", "labels": [], "entities": []}, {"text": "One interesting observation is that the first two features chosen by our model selection process use information from the NLP processors.", "labels": [], "entities": []}, {"text": "The first chosen feature is the translation probability computed between the B g question and answer representations (bigrams with words generalized to their WNSS tags).", "labels": [], "entities": []}, {"text": "The second feature selected measures the number of syntactic dependencies from the question that are matched in the answer.", "labels": [], "entities": []}, {"text": "These results provide empirical evidence that coarse semantic disambiguation and syntactic parsing have a positive contribution to non-factoid QA, even in broad-coverage noisy settings based on Web data.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7609591484069824}]}, {"text": "The above observation deserves a more detailed analysis.", "labels": [], "entities": []}, {"text": "shows the performance of our first three feature groups when they are applied to each of the five text representations or incremental combinations of representations.", "labels": [], "entities": []}, {"text": "For each model corresponding to a table cell we use only the features from the corresponding feature group and representation to avoid the correlation with features from other groups.", "labels": [], "entities": []}, {"text": "We generate each best model using the same feature selection process described above.", "labels": [], "entities": []}, {"text": "The left part of shows that, generally, the models using representations that include the output of our NLP processors ) improve over the baseline (FG1 and W).", "labels": [], "entities": [{"text": "FG1", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.8504276275634766}]}, {"text": "11 However, comparable improvements can be obtained with the simpler bigram representation (B).", "labels": [], "entities": []}, {"text": "This indicates that, in terms of individual contributions, our NLP processors can be approximated with simpler n-gram models in this task.", "labels": [], "entities": []}, {"text": "Hence, is it fair to say that syntactic and semantic analysis is useful for such Web QA tasks?", "labels": [], "entities": []}, {"text": "While the above analysis seems to suggest a negative answer, the right-hand side of tells a more interesting story.", "labels": [], "entities": []}, {"text": "It shows that the NLP analysis provides complementary information to the ngram-based models.", "labels": [], "entities": []}, {"text": "The best models for the FG2 and FG3 feature groups are obtained when combining the n-gram representations with the representations that use the output of the NLP processors (W + B + B g + D).", "labels": [], "entities": [{"text": "FG2", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9627518653869629}, {"text": "FG3 feature groups", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8477864464124044}]}, {"text": "The improvements are relatively small, but remarkable (e.g., see FG2) if we take into account the significant scale of the evaluation.", "labels": [], "entities": [{"text": "FG2", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.6040300726890564}]}, {"text": "This observation correlates well with the analysis shown in, which shows that features using semantic ) and syntactic (D) representations contribute the most on top of the IR model (BM25(W)).", "labels": [], "entities": [{"text": "BM25", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.6182151436805725}]}], "tableCaptions": [{"text": " Table 2: Overall results for the test partition.", "labels": [], "entities": []}, {"text": " Table 3: Summary of the model selection process.", "labels": [], "entities": []}]}