{"title": [{"text": "Learning Bilingual Lexicons from Monolingual Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method for learning bilingual translation lexicons from monolingual corpora.", "labels": [], "entities": [{"text": "learning bilingual translation lexicons from monolingual corpora", "start_pos": 24, "end_pos": 88, "type": "TASK", "confidence": 0.7423182938780103}]}, {"text": "Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.", "labels": [], "entities": []}, {"text": "Translations are induced using a generative model based on canonical correlation analysis , which explains the monolingual lexicons in terms of latent matchings.", "labels": [], "entities": [{"text": "Translations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9541875123977661}]}, {"text": "We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences beat the level of phrases), treelets (), or simply single words.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6221577624479929}]}, {"text": "Although parallel text is plentiful for some language pairs such as English-Chinese or EnglishArabic, it is scarce or even non-existent for most others, such as English-Hindi or French-Japanese.", "labels": [], "entities": []}, {"text": "Moreover, parallel text could be scarce fora language pair even if monolingual data is readily available for both languages.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of learning translations from monolingual sources alone.", "labels": [], "entities": []}, {"text": "This task, though clearly more difficult than the standard parallel text approach, can operate on language pairs and in domains where standard approaches cannot.", "labels": [], "entities": []}, {"text": "We take as input two monolingual corpora and perhaps some seed translations, and we produce as output a bilingual lexicon, defined as a list of word pairs deemed to be word-level translations.", "labels": [], "entities": []}, {"text": "Precision and recall are then measured over these bilingual lexicons.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9873533248901367}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9983311295509338}]}, {"text": "This setting has been considered before, most notably in and, but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions.", "labels": [], "entities": []}, {"text": "In our method, we represent each language as a monolingual lexicon (see): a list of word types characterized by monolingual feature vectors, such as context counts, orthographic substrings, and soon (section 5).", "labels": [], "entities": []}, {"text": "We define a generative model over (1) a source lexicon, (2) a target lexicon, and (3) a matching between them (section 2).", "labels": [], "entities": []}, {"text": "Our model is based on canonical correlation analysis (CCA) and explains matched word pairs via vectors in a common latent space.", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.7810946752627691}]}, {"text": "Inference in the model is done using an EM-style algorithm (section 3).", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, we show that it is possible to learn or extend a translation lexicon using monolingual corpora alone, in a variety of languages and using a variety of corpora, even in the absence of orthographic features.", "labels": [], "entities": []}, {"text": "As might be expected, the task is harder when no seed lexicon is provided, when the languages are strongly divergent, or when the monolingual corpora are from different domains.", "labels": [], "entities": []}, {"text": "Nonetheless, even in the more difficult cases, a sizable set of high-precision translations can be extracted.", "labels": [], "entities": []}, {"text": "As an example of the performance of the system, in English-Spanish induction with our best feature set, using corpora derived from topically similar but non-parallel sources, the system obtains 89.0% precision at 33% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.9984704852104187}, {"text": "recall", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.9978964328765869}]}], "datasetContent": [{"text": "In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, the distances between the words' mean latent concepts: where A is a thresholding constant, , and z * j is defined analogously.", "labels": [], "entities": [{"text": "English-Spanish lexicon induction", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6331604023774465}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9992581009864807}]}, {"text": "The increased accuracy may not bean accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. are presented for other languages in section 6.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9992382526397705}]}, {"text": "In this section, we describe the data and experimental methodology used throughout this work.", "labels": [], "entities": []}, {"text": "We evaluate a proposed lexicon L p against the evaluation lexicon Le using the F 1 measure in the standard fashion; precision is given by the number of proposed translations contained in the evaluation lexicon, and recall is given by the fraction of possible translation pairs proposed.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9152352809906006}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9995525479316711}, {"text": "recall", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.9995778203010559}]}, {"text": "naturally produces lexicons in which each entry is associated with a weight based on the model, we can give a full precision/recall curve (see).", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9988653659820557}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9588320851325989}]}, {"text": "We summarize these curves with both the best F 1 overall possible thresholds and various precisions p x at recalls x.", "labels": [], "entities": [{"text": "F 1 overall possible thresholds", "start_pos": 45, "end_pos": 76, "type": "METRIC", "confidence": 0.9303204536437988}]}, {"text": "All reported numbers exclude evaluation on the seed lexicon entries, regardless of how those seeds are derived or whether they are correct.", "labels": [], "entities": []}, {"text": "In all experiments, unless noted otherwise, we used a seed of size 100 obtained from Le and considered lexicons between the top n = 2, 000 most frequent source and target noun word types which were not in the seed lexicon; each system proposed an already-ranked one-to-one translation lexicon amongst these n words.", "labels": [], "entities": []}, {"text": "Where applicable, we compare against the EDITDIST baseline, which solves a maximum bipartite matching problem where edge weights are normalized edit distances.", "labels": [], "entities": [{"text": "EDITDIST baseline", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.8324221968650818}]}, {"text": "We will use MCCA (for matching CCA) to denote our model using the optimal feature set (see section 5.3).", "labels": [], "entities": [{"text": "MCCA", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9767892956733704}]}, {"text": "In this section we examine how system performance varies when crucial elements are altered.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of EDITDIST and our model with  various features sets on EN-ES-W. See section 5.", "labels": [], "entities": [{"text": "EN-ES-W", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9098213911056519}]}, {"text": " Table 2: (a) varying type of corpora used on system per- formance (section 6.1), (b) using a heuristically chosen  seed compared to one taken from the evaluation lexicon  (section 6.2), (c) a variety of language pairs (see sec- tion 6.3).", "labels": [], "entities": []}]}