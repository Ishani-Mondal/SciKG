{"title": [{"text": "Sentence Simplification for Semantic Role Labeling", "labels": [], "entities": [{"text": "Sentence Simplification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9120062291622162}, {"text": "Semantic Role Labeling", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.754155695438385}]}], "abstractContent": [{"text": "Parse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems.", "labels": [], "entities": []}, {"text": "These systems typically treat the paths as atomic (or nearly atomic) features; these features are quite sparse due to the immense variety of syntactic expression.", "labels": [], "entities": []}, {"text": "In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces.", "labels": [], "entities": []}, {"text": "Our method applies a series of handwritten transformation rules corresponding to basic syntactic patterns-for example, one rule \"depassivizes\" a sentence.", "labels": [], "entities": []}, {"text": "The model is parameterized by learned weights specifying preferences for some rules over others.", "labels": [], "entities": []}, {"text": "After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences.", "labels": [], "entities": []}, {"text": "We apply our simplification system to semantic role labeling (SRL).", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.8139754931131998}]}, {"text": "As we do not have labeled examples of correct simplifications, we use labeled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the simplification as a hidden variable.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 100, "end_pos": 108, "type": "TASK", "confidence": 0.8727110028266907}]}, {"text": "By extracting and labeling simplified sentences, this combined simplification/SRL system better generalizes across syntactic variation.", "labels": [], "entities": [{"text": "SRL", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.5754551291465759}]}, {"text": "It achieves a statistically significant 1.2% F1 measure increase over a strong baseline on the Conll-2005 SRL task, attaining near-state-of-the-art performance.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9828915596008301}, {"text": "Conll-2005 SRL task", "start_pos": 95, "end_pos": 114, "type": "DATASET", "confidence": 0.687131534020106}]}], "introductionContent": [{"text": "In semantic role labeling (SRL), given a sentence containing a target verb, we want to label the semantic arguments, or roles, of that verb.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7950945744911829}]}, {"text": "For the verb \"eat\", a correct labeling of \"Tom ate a salad\" is {ARG0(Eater)=\"Tom\", ARG1(Food)=\"salad\"}.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9824601411819458}, {"text": "ARG1", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.7124201059341431}]}, {"text": "Current semantic role labeling systems rely primarily on syntactic features in order to identify and classify roles.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.6170389652252197}]}, {"text": "Features derived from a syntactic parse of the sentence have proven particularly useful ().", "labels": [], "entities": []}, {"text": "For example, the syntactic subject of \"give\" is nearly always the Giver.", "labels": [], "entities": []}, {"text": "Path features allow systems to capture both general patterns, e.g., that the ARG0 of a sentence tends to be the subject of the sentence, and specific usage, e.g., that the ARG2 of \"give\" is often a post-verbal prepositional phrase headed by \"to\".", "labels": [], "entities": []}, {"text": "An example sentence with extracted path features is shown in.", "labels": [], "entities": []}, {"text": "A major problem with this approach is that the path from an argument to the verb can be quite complicated.", "labels": [], "entities": []}, {"text": "In the sentence \"He expected to receive a prize for winning,\" the path from \"win\" to its ARG0, \"he\", involves the verbs \"expect\" and \"receive\" and the preposition \"for.\"", "labels": [], "entities": [{"text": "ARG0", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.6899906992912292}]}, {"text": "The corresponding path through the parse tree likely occurs a relatively small number of times (or not at all) in the training corpus.", "labels": [], "entities": []}, {"text": "If the test set contained exactly the same sentence but with \"expected\" replaced by \"did not expect\" we would extract a different parse path feature; therefore, as far as the classifier is concerned, the syntax of the two sentences is totally unrelated.", "labels": [], "entities": []}, {"text": "In this paper we learn a mapping from full, complicated sentences to simplified sentences.", "labels": [], "entities": []}, {"text": "For example, given a correct parse, our system simplifies the above sentence with target verb \"win\" to \"He won.\"", "labels": [], "entities": []}, {"text": "Our method combines hand-written syntactic simplification rules with machine learning, which determines which rules to prefer.", "labels": [], "entities": []}, {"text": "We then use the output of the simplification system as input to a SRL system that is trained to label simplified sentences.", "labels": [], "entities": []}, {"text": "Compared to previous SRL models, our model has several qualitative advantages.", "labels": [], "entities": [{"text": "SRL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.918445885181427}]}, {"text": "First, we believe that the simplification process, which represents the syntax as a set of local syntactic transformations, is more linguistically satisfying than using the entire parse path as anatomic feature.", "labels": [], "entities": []}, {"text": "Improving the simplification process mainly involves adding more linguistic knowledge in the form of simplification rules.", "labels": [], "entities": [{"text": "simplification", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9650478363037109}]}, {"text": "Second, labeling simple sentences is much easier than labeling raw sentences and allows us to generalize more effectively across sentences with differing syntax.", "labels": [], "entities": [{"text": "labeling simple sentences", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.8824928005536398}]}, {"text": "This is particularly important for verbs with few labeled training instances; using training examples as efficiently as possible can lead to considerable gains in performance.", "labels": [], "entities": []}, {"text": "Third, our model is very effective at sharing information across verbs, since most of our simplification rules apply equally well regardless of the target verb.", "labels": [], "entities": []}, {"text": "A major difficulty in learning to simplify sentences is that we do not have labeled data for this task.", "labels": [], "entities": []}, {"text": "To address this problem, we simultaneously train our simplification system and the SRL system.", "labels": [], "entities": [{"text": "SRL", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.5602690577507019}]}, {"text": "We treat the correct simplification as a hidden variable, using labeled SRL data to guide us towards \"more useful\" simplifications.", "labels": [], "entities": []}, {"text": "Specifically, we train our model discriminatively to predict the correct role labeling assignment given an input sentence, treating the simplification as a hidden variable.", "labels": [], "entities": [{"text": "role labeling assignment", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.7571359872817993}]}, {"text": "Applying our combined simplification/SRL model to the Conll 2005 task, we show a significant improvement over a strong baseline model.", "labels": [], "entities": [{"text": "SRL", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8643481731414795}, {"text": "Conll 2005 task", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9266306360562643}]}, {"text": "Our model does best on verbs with little training data and on instances with paths that are rare or have never been seen before, matching our intuitions about the strengths of the model.", "labels": [], "entities": []}, {"text": "Our model outperforms all but the best few Conll 2005 systems, each of which uses multiple different automatically-generated parses (which would likely improve our model).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our system using the setup of the Conll 2005 semantic role labeling task.", "labels": [], "entities": [{"text": "Conll 2005 semantic role labeling task", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.7322791417439779}]}, {"text": "Thus, we trained on Sections 2-21 of PropBank and used Section 24 as development data.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9089688658714294}]}, {"text": "Our test data includes both the selected portion of Section 23 of PropBank, plus the extra data on the Brown corpus.", "labels": [], "entities": [{"text": "Section 23 of PropBank", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.8126364052295685}, {"text": "Brown corpus", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.9757600426673889}]}, {"text": "We used the Charniak parses provided by the Conll distribution.", "labels": [], "entities": [{"text": "Conll distribution", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.8148098289966583}]}, {"text": "We compared to a strong Baseline SRL system that learns a logistic regression model using the features of.", "labels": [], "entities": []}, {"text": "The first filters out nodes that are unlikely to be arguments.", "labels": [], "entities": []}, {"text": "The second stage labels each remaining node either as a particular role (e.g. \"ARGO\") or as a non-argument.", "labels": [], "entities": [{"text": "ARGO", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.6265721321105957}]}, {"text": "Note that the baseline feature set includes a feature corresponding to the subcategorization of the verb (specifically, the sequence of nonterminals which are children of the predicate's parent node).", "labels": [], "entities": []}, {"text": "Thus, Baseline does have access to something similar to our model's role pattern feature, although the Baseline subcategorization feature only includes post-verbal modifiers and is generally much noisier because it operates on the original sentence.", "labels": [], "entities": []}, {"text": "Our Transforms model takes as input the Charniak parses supplied by the Conll release, and labels every node with Core arguments (ARG0-ARG5).", "labels": [], "entities": [{"text": "Conll release", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9393828213214874}, {"text": "ARG0-ARG5", "start_pos": 130, "end_pos": 139, "type": "DATASET", "confidence": 0.8232600092887878}]}, {"text": "Our rule set does not currently handle either referent arguments (such as \"who\" in \"The man who ate . .", "labels": [], "entities": []}, {"text": "\") or non-core arguments (such as ARGM-TMP).", "labels": [], "entities": [{"text": "ARGM-TMP", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8185989856719971}]}, {"text": "For these arguments, we simply filled in using our baseline system (specifically, any non-core argument which did not overlap an argument predicted by our model was added to the labeling).", "labels": [], "entities": []}, {"text": "Also, on some sentences, our system did not generate any predictions because no valid simple sen-  tences were produced by the simplification system . Again, we used the baseline to fill in predictions (for all arguments) for these sentences.", "labels": [], "entities": []}, {"text": "Baseline and Transforms were regularized using a Gaussian prior; for both models, \u03c3 2 = 1.0 gave the best results on the development set.", "labels": [], "entities": []}, {"text": "For generating role predictions from our model, we have two reasonable options: use the labeling given by the single highest scoring simple labeling; or compute the distribution over predictions for each node by summing overall simple labelings.", "labels": [], "entities": []}, {"text": "The latter method worked slightly better, particularly when combined with the baseline model as described below, so all reported results use this method.", "labels": [], "entities": []}, {"text": "We also evaluated a hybrid model that combines the Baseline with our simplification model.", "labels": [], "entities": []}, {"text": "For a given sentence/verb pair (s, v), we find the set of constituents N sv that made it past the first (filtering) stage of Baseline.", "labels": [], "entities": []}, {"text": "For each candidate simple sentence/labeling pair c sv k = (t sv i , g v j ) proposed by our model, we check to see which of the constituents in N sv are already present in our simple sentence t sv i . Any constituents that are not present are then assigned a probability distribution over possible roles according to Baseline.", "labels": [], "entities": []}, {"text": "Thus, we fallback Baseline whenever the current simple sentence does not have an \"opinion\" about the role of a particular constituent.", "labels": [], "entities": []}, {"text": "The Combined model is thus able to correctly label sentences when the simplification process drops some of the arguments (generally due to unusual syntax).", "labels": [], "entities": []}, {"text": "Each of the two components was trained separately and combined only at testing time.", "labels": [], "entities": []}, {"text": "shows results of these three systems on the Conll-2005 task, plus the top-performing system) for reference.", "labels": [], "entities": [{"text": "Conll-2005 task", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8408485352993011}]}, {"text": "Baseline already achieves good performance on this task, placing at about 75 th percentile among evaluated systems.", "labels": [], "entities": []}, {"text": "Our Transforms model outperforms Baseline on all sets.", "labels": [], "entities": []}, {"text": "Finally, our Combined model improves over Transforms on all but the test Brown corpus,", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.7773818969726562}]}], "tableCaptions": [{"text": " Table 2: F1 Measure using Charniak parses", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9861190319061279}]}]}