{"title": [{"text": "Mining Wikipedia Revision Histories for Improving Sentence Compression", "labels": [], "entities": [{"text": "Mining Wikipedia Revision Histories", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5758030191063881}, {"text": "Improving Sentence Compression", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.8906274239222208}]}], "abstractContent": [{"text": "A well-recognized limitation of research on supervised sentence compression is the dearth of available training data.", "labels": [], "entities": [{"text": "supervised sentence compression", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6515129506587982}]}, {"text": "We propose anew and bountiful resource for such training data, which we obtain by mining the revision history of Wikipedia for sentence compressions and expansions.", "labels": [], "entities": [{"text": "sentence compressions and expansions", "start_pos": 127, "end_pos": 163, "type": "TASK", "confidence": 0.7073977217078209}]}, {"text": "Using only a fraction of the available Wikipedia data, we have collected a training corpus of over 380,000 sentence pairs, two orders of magnitude larger than the standardly used Ziff-Davis corpus.", "labels": [], "entities": []}, {"text": "Using this newfound data, we propose a novel lexical-ized noisy channel model for sentence compression , achieving improved results in gram-maticality and compression rate criteria with a slight decrease in importance.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7927014827728271}]}], "introductionContent": [{"text": "With the increasing success of machine translation (MT) in recent years, several researchers have suggested transferring similar methods for monolingual text rewriting tasks.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8564047634601593}, {"text": "text rewriting tasks", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.7814876735210419}]}, {"text": "In particular, (KM) applied a channel model to the task of sentence compression -dropping words from an individual sentence while retaining its important information, and without sacrificing its grammaticality.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7161377966403961}]}, {"text": "Compressed sentences can be useful either on their own, e.g., for subtitles, or as part of a larger summarization or MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9282727837562561}]}, {"text": "A well-recognized problem of this approach, however, is data sparsity.", "labels": [], "entities": []}, {"text": "While bilingual parallel corpora are abundantly available, monolingual parallel corpora, and especially collections of sentence compressions are vanishingly rare.", "labels": [], "entities": [{"text": "sentence compressions", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.7352303862571716}]}, {"text": "Indeed, most work on sentence compression has used the Ziff-Davis corpus, which consists of a mere 1067 sentence pairs.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7790503203868866}]}, {"text": "While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading to question the applicability of the channel model for this task altogether.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.7587553262710571}]}, {"text": "Our contribution in this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we solve the data sparsity issue by showing that abundant sentence compressions can be extracted from Wikipedia's revision history.", "labels": [], "entities": [{"text": "Wikipedia's revision history", "start_pos": 109, "end_pos": 137, "type": "DATASET", "confidence": 0.7926945090293884}]}, {"text": "Second, we use this data to validate the channel model approach for text compression, and improve upon it by creating a novel fully lexicalized compression model.", "labels": [], "entities": [{"text": "text compression", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.804194986820221}]}, {"text": "Our model improves grammaticality and compression rate with only a slight decrease in importance.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.9571011662483215}]}], "datasetContent": [{"text": "We evaluated our system using the same method as KM, using the same 32 sentences taken from the Ziff-Davis corpus.", "labels": [], "entities": []}, {"text": "We solicited judgments of importance (the value of the retained information), and grammaticality for our compression, the KM results, and human compressions from 8 judges, on a scale of 1 (worst) to 5 (best).", "labels": [], "entities": []}, {"text": "Mean and standard deviation are shown in.", "labels": [], "entities": [{"text": "Mean", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9948177933692932}, {"text": "standard deviation", "start_pos": 9, "end_pos": 27, "type": "METRIC", "confidence": 0.9293520450592041}]}, {"text": "Our model improves grammaticality and compression rate criteria with only a slight decrease in importance.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.9642681777477264}]}, {"text": "Here are some illustrative examples, with the deleted material shown in brackets: We suspect that the decrease in importance stems from our indiscriminative usage of compressions and expansions to train our system.", "labels": [], "entities": []}, {"text": "We hypothesize that in Wikipedia, expansions often add more useful information, as opposed to compressions which are more likely to drop superfluous or erroneous information.", "labels": [], "entities": []}, {"text": "Further work is required to classify sentence modifications.", "labels": [], "entities": [{"text": "classify sentence modifications", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.8542490402857462}]}, {"text": "Since one of our model's back-off levels simulates KM's model, we plan to perform an additional comparative evaluation of both models trained on the same data.", "labels": [], "entities": []}, {"text": "question the viability of a noisy channel model for the sentence compression task.", "labels": [], "entities": [{"text": "sentence compression task", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.8266286849975586}]}, {"text": "Briefly put, in the typically sparse data setting, there is noway to distinguish between the probability of a sentence as a short sentence and its probability as a regular sentence of English.", "labels": [], "entities": []}, {"text": "Furthermore, the channel model is likely to prefer to leave sentences intact, since that is the most prevalent pattern in the training data.", "labels": [], "entities": []}, {"text": "Thus, they argue, the channel model is not really compressing, and it is only by virtue of the length penalty that anything gets shortened at all.", "labels": [], "entities": []}, {"text": "Our hope here is that by using afar richer source of short sentences, as well as a huge source of compressions, we can overcome this problem.", "labels": [], "entities": []}, {"text": "The noisy channel model posits a virtual competition on each word of coming either from the source model (in which case it is retained in the compression) or from the channel model (in which case it is dropped).", "labels": [], "entities": []}, {"text": "By having access to a large data set for the first time, we hope to be able to learn which parts of the sentence are more likely to come from S \u2192 NP ADVP VP S \u2192 NP VP 5 parent = S, head-child = VP, child = ADVP parent = S, head-child = VP 6 parent = S, child = ADVP parent = S  which of the two parts of the model.", "labels": [], "entities": []}, {"text": "Further work is required in order to clarify this point.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Back off levels", "labels": [], "entities": []}]}