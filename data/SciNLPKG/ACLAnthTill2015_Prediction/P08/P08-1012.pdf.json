{"title": [{"text": "Bayesian Learning of Non-compositional Phrases with Synchronous Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We combine the strengths of Bayesian mod-eling and synchronous grammar in unsu-pervised learning of basic translation phrase pairs.", "labels": [], "entities": []}, {"text": "The structured space of asynchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large.", "labels": [], "entities": [{"text": "phrase pair probability estimation", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.747214138507843}]}, {"text": "Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results.", "labels": [], "entities": []}, {"text": "Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 162, "end_pos": 176, "type": "TASK", "confidence": 0.771199107170105}]}, {"text": "This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches .", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.8079481720924377}, {"text": "word alignment", "start_pos": 169, "end_pos": 183, "type": "TASK", "confidence": 0.7300598174333572}]}], "introductionContent": [{"text": "Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6277836958567301}]}, {"text": "These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of and.", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.704248309135437}]}, {"text": "As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7017265856266022}]}, {"text": "Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process.", "labels": [], "entities": []}, {"text": "While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems.", "labels": [], "entities": [{"text": "wordlevel alignments", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7393452823162079}, {"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7050624340772629}]}, {"text": "A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure.", "labels": [], "entities": []}, {"text": "Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words.", "labels": [], "entities": []}, {"text": "Furthermore it would obviate the need for heuristic combination of word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.697307214140892}]}, {"text": "A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words.", "labels": [], "entities": [{"text": "identification of non-compositional phrasal translations", "start_pos": 41, "end_pos": 97, "type": "TASK", "confidence": 0.859551751613617}]}, {"text": "In this direction, Expectation Maximization at the phrase level was proposed by, who, however, experienced two major difficulties: computational complexity and controlling overfitting.", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.9566391110420227}]}, {"text": "Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to address these two issues in order to apply EM above the word level.", "labels": [], "entities": []}, {"text": "We attack computational complexity by adopting the polynomial-time Inversion Transduction Grammar framework, and by only learning small noncompositional phrases.", "labels": [], "entities": [{"text": "Inversion Transduction Grammar", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.7424832185109457}]}, {"text": "We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases.", "labels": [], "entities": [{"text": "EM", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9424792528152466}]}, {"text": "We test our model by extracting longer phrases from our model's alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7242036163806915}, {"text": "MT", "start_pos": 170, "end_pos": 172, "type": "TASK", "confidence": 0.9890690445899963}]}], "datasetContent": [{"text": "The training data was a subset of 175K sentence pairs from the NIST Chinese-English training data, automatically selected to maximize character-level overlap with the source side of the test data.", "labels": [], "entities": [{"text": "NIST Chinese-English training data", "start_pos": 63, "end_pos": 97, "type": "DATASET", "confidence": 0.9478627741336823}]}, {"text": "We put a length limit of 35 on both sides, producing a training set of 141K sentence pairs.", "labels": [], "entities": []}, {"text": "500 Chinese-English pairs from this set were manually aligned and used as a gold standard.", "labels": [], "entities": []}, {"text": "First, using evaluations of alignment quality, we demonstrate the effectiveness of VB over EM, and explore the effect of the prior.", "labels": [], "entities": []}, {"text": "examines the difference between EM and VB with varying sparse priors for the word-based model of ITG on the 500 sentence pairs, both after 10 iterations of training.", "labels": [], "entities": []}, {"text": "Using EM, because of overfitting, AER drops first and increases again as the number of iterations varies from 1 to 10.", "labels": [], "entities": [{"text": "AER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9994958639144897}]}, {"text": "The lowest AER using EM is achieved after the second iteration, which is .40.", "labels": [], "entities": [{"text": "AER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9994737505912781}, {"text": "EM", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.847449779510498}]}, {"text": "At iteration 10, AER for EM increases to .42.", "labels": [], "entities": [{"text": "AER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9997923970222473}]}, {"text": "On the other hand, using VB, AER decreases monotonically over the 10 iterations and stabilizes at iteration 10.", "labels": [], "entities": [{"text": "AER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9995330572128296}]}, {"text": "When \u03b1 C is 1e \u2212 9, VB gets AER close to .35 at iteration 10.", "labels": [], "entities": [{"text": "VB", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9400588870048523}, {"text": "AER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9995885491371155}]}, {"text": "As we increase the bias toward sparsity, the AER decreases, following along slow plateau.", "labels": [], "entities": [{"text": "AER", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9995250701904297}]}, {"text": "Although the magnitude of improvement is not large, the trend is encouraging.", "labels": [], "entities": []}, {"text": "These experiments also indicate that a very sparse prior is needed for machine translation tasks., who found optimal performance when \u03b1 was approximately 10 \u22124 , we observed monotonic increases in performance as \u03b1 dropped.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7832085192203522}]}, {"text": "The dimensionality of this MT problem is significantly larger than that of the sequence problem, though, therefore it may take a stronger push from the prior to achieve the desired result.", "labels": [], "entities": [{"text": "MT problem", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.939253568649292}]}, {"text": "Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9955257773399353}]}, {"text": "Unfortunately these experiments are very slow.", "labels": [], "entities": []}, {"text": "Since we observed monotonic increases in alignment performance with smaller values of \u03b1 C , we simply fixed the prior at a very small value (10 \u2212100 ) for all translation experiments.", "labels": [], "entities": []}, {"text": "We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a sig-nificant impact on the output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9885330200195312}]}, {"text": "We also trained a baseline model with GIZA++) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9626919627189636}]}, {"text": "We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1.", "labels": [], "entities": [{"text": "English-to-Chinese word translation", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.6279139916102091}]}, {"text": "These values were used to perform tic-tac-toe pruning with \u03c4 b = 1 \u00d7 10 \u22123 and \u03c4 s = 1 \u00d7 10 \u22126 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB.", "labels": [], "entities": []}, {"text": "The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model.", "labels": [], "entities": []}, {"text": "Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments.", "labels": [], "entities": []}, {"text": "For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system).", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9891748428344727}]}, {"text": "The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7130611389875412}, {"text": "phrase extraction", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7186213433742523}]}, {"text": "Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units.", "labels": [], "entities": []}, {"text": "In addition the phrases were annotated with lexical weights using the IBM Model 1 tables.", "labels": [], "entities": [{"text": "IBM Model 1 tables", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.9065206497907639}]}, {"text": "The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features.", "labels": [], "entities": []}, {"text": "Minimum Error Rate training over BLEU was used to optimize the weights for each of these models over the development test data.", "labels": [], "entities": [{"text": "Error Rate training", "start_pos": 8, "end_pos": 27, "type": "METRIC", "confidence": 0.933652400970459}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9906557202339172}]}, {"text": "We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation.", "labels": [], "entities": [{"text": "NIST 2002 evaluation datasets", "start_pos": 12, "end_pos": 41, "type": "DATASET", "confidence": 0.9697146415710449}]}, {"text": "We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method.", "labels": [], "entities": [{"text": "phrasal translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7607718408107758}, {"text": "word alignment (or phrasal alignment)", "start_pos": 65, "end_pos": 102, "type": "TASK", "confidence": 0.6414530788149152}]}, {"text": "compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training.", "labels": [], "entities": [{"text": "GIZA++ baseline", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.7589134772618612}]}, {"text": "ITG-mwm-VB is our best model.", "labels": [], "entities": [{"text": "ITG-mwm-VB", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9187295436859131}]}, {"text": "We see an improvement of nearly 2 points dev set and nearly 1 point of improvement on the test set.", "labels": [], "entities": []}, {"text": "We also observe the consistent superiority of VB over EM.", "labels": [], "entities": [{"text": "VB", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.7594009041786194}]}, {"text": "The gain is especially large on the test data set, indicating VB is less prone to overfitting.", "labels": [], "entities": [{"text": "test data set", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8183728257815043}, {"text": "VB", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.41299718618392944}]}], "tableCaptions": [{"text": " Table 1: Translation results on Chinese-English, using  the subset of training data (141K sentence pairs) that have  length limit 35 on both sides. (No length limit in transla- tion. )", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9810245037078857}]}]}