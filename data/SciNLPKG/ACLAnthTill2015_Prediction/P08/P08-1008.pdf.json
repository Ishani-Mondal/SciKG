{"title": [{"text": "Contradictions and Justifications: Extensions to the Textual Entailment Task", "labels": [], "entities": []}], "abstractContent": [{"text": "The third PASCAL Recognizing Textual En-tailment Challenge (RTE-3) contained an optional task that extended the main entailment task by requiring a system to make three-way entailment decisions (entails, contradicts, neither) and to justify its response.", "labels": [], "entities": [{"text": "PASCAL Recognizing Textual En-tailment Challenge (RTE-3)", "start_pos": 10, "end_pos": 66, "type": "TASK", "confidence": 0.7462487146258354}]}, {"text": "Contradiction was rare in the RTE-3 test set, occurring in only about 10% of the cases, and systems found accurately detecting it difficult.", "labels": [], "entities": [{"text": "Contradiction", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9692631363868713}, {"text": "RTE-3 test set", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.8573104937871298}]}, {"text": "Subsequent analysis of the results shows a test set must contain many more entailment pairs for the three-way decision task than the traditional two-way task to have equal confidence in system comparisons.", "labels": [], "entities": []}, {"text": "Each of six human judges representing eventual end users rated the quality of a justification by assigning \"understand-ability\" and \"correctness\" scores.", "labels": [], "entities": []}, {"text": "Ratings of the same justification across judges differed significantly, signaling the need fora better characterization of the justification task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The PASCAL Recognizing Textual Entailment (RTE) workshop series (see www.pascal-network.", "labels": [], "entities": [{"text": "PASCAL Recognizing Textual Entailment (RTE) workshop", "start_pos": 4, "end_pos": 56, "type": "TASK", "confidence": 0.7638583108782768}]}, {"text": "org/Challenges/RTE3/) has been a catalyst for recent research in developing systems that are able to detect when the content of one piece of text necessarily follows from the content of another piece of text (;.", "labels": [], "entities": []}, {"text": "This ability is seen as a fundamental component in the solutions fora variety of natural language problems such as question answering, summarization, and information extraction.", "labels": [], "entities": [{"text": "question answering", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.8797157108783722}, {"text": "summarization", "start_pos": 135, "end_pos": 148, "type": "TASK", "confidence": 0.9598321914672852}, {"text": "information extraction", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.8246645033359528}]}, {"text": "In addition to the main entailment task, the most recent Challenge, RTE-3, contained a second optional task that extended the main task in two ways.", "labels": [], "entities": [{"text": "RTE-3", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.8348778486251831}]}, {"text": "The first extension was to require systems to make three-way entailment decisions; the second extension was for systems to return a justification or explanation of how its decision was reached.", "labels": [], "entities": []}, {"text": "In the main RTE entailment task, systems report whether the hypothesis is entailed by the text.", "labels": [], "entities": [{"text": "RTE entailment task", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9099475741386414}]}, {"text": "The system responds with YES if the hypothesis is entailed and NO otherwise.", "labels": [], "entities": [{"text": "YES", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.998763918876648}, {"text": "NO", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9850688576698303}]}, {"text": "But this binary decision conflates the case when the hypothesis actually contradicts the text-the two could not both be truewith simple lack of entailment.", "labels": [], "entities": []}, {"text": "The three-way entailment decision task requires systems to decide whether the hypothesis is entailed by the text (YES), contradicts the text (NO), or is neither entailed by nor contradicts the text (UNKNOWN).", "labels": [], "entities": [{"text": "YES", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9900276064872742}]}, {"text": "The second extension required a system to explain why it reached its conclusion in terms suitable for an eventual end user (i.e., not system developer).", "labels": [], "entities": []}, {"text": "Explanations are one way to build a user's trust in a system, but it is not known what kinds of information must be conveyed nor how best to present that information.", "labels": [], "entities": []}, {"text": "RTE-3 provided an opportunity to collect a diverse sample of explanations to begin to explore these questions.", "labels": [], "entities": [{"text": "RTE-3", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9126235842704773}]}, {"text": "This paper analyzes the extended task results, with the next section describing the three-way decision subtask and Section 3 the justification subtask.", "labels": [], "entities": []}, {"text": "Contradiction was rare in the RTE-3 test set, occurring in only about 10% of the cases, and systems found accurately detecting it difficult.", "labels": [], "entities": [{"text": "Contradiction", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9692631363868713}, {"text": "RTE-3 test set", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.8573108315467834}]}, {"text": "While the level of agreement among human annotators as to the correct answer for an entailment pair was within expected bounds, the test set was found to be too small to reliably distinguish among systems' threeway accuracy scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.800037682056427}]}, {"text": "Human judgments of the quality of a justification varied widely, signaling the need fora better characterization of the justification task.", "labels": [], "entities": []}, {"text": "Comments from the judges did include some common themes.", "labels": [], "entities": []}, {"text": "Judges prized conciseness, though they were uncomfortable with mathematical notation unless they had a mathematical background.", "labels": [], "entities": [{"text": "mathematical notation", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7002609819173813}]}, {"text": "Judges strongly disliked being shown system internals such as scores reported by various components.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Contingency table of responses over all 800 en- tailment pairs and all 12 runs.", "labels": [], "entities": []}, {"text": " Table 3: Agreement between NIST judges.", "labels": [], "entities": [{"text": "NIST judges", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.8848088383674622}]}]}