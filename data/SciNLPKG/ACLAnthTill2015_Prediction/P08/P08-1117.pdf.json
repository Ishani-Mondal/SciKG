{"title": [{"text": "Extraction of Entailed Semantic Relations Through Syntax-based Comma Resolution", "labels": [], "entities": [{"text": "Extraction of Entailed Semantic Relations", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8328118443489074}, {"text": "Syntax-based Comma Resolution", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6347979605197906}]}], "abstractContent": [{"text": "This paper studies textual inference by investigating comma structures, which are highly frequent elements whose major role in the extraction of semantic relations has not been hitherto recognized.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7238332033157349}]}, {"text": "We introduce the problem of comma resolution, defined as understanding the role of commas and extracting the relations they imply.", "labels": [], "entities": [{"text": "comma resolution", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.8136964440345764}]}, {"text": "We show the importance of the problem using examples from Tex-tual Entailment tasks, and present A Sentence Transformation Rule Learner (ASTRL), a machine learning algorithm that uses a syntactic analysis of the sentence to learn sentence transformation rules that can then be used to extract relations.", "labels": [], "entities": []}, {"text": "We have manually annotated a corpus identifying comma structures and relations they entail and experimented with both gold standard parses and parses created by a leading statistical parser, obtaining F-scores of 80.2% and 70.4% respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9956484436988831}]}], "introductionContent": [{"text": "Recognizing relations expressed in text sentences is a major topic in NLP, fundamental in applications such as Textual Entailment (or Inference), Question Answering and Text Mining.", "labels": [], "entities": [{"text": "Recognizing relations expressed in text sentences", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8970501820246378}, {"text": "Textual Entailment (or Inference)", "start_pos": 111, "end_pos": 144, "type": "TASK", "confidence": 0.8011095921198527}, {"text": "Question Answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.8627326190471649}, {"text": "Text Mining", "start_pos": 169, "end_pos": 180, "type": "TASK", "confidence": 0.8082465827465057}]}, {"text": "In this paper we address this issue from a novel perspective, that of understanding the role of the commas in a sentence, which we argue is a key component in sentence comprehension.", "labels": [], "entities": []}, {"text": "Consider for example the following three sentences: 1.", "labels": [], "entities": []}, {"text": "Authorities have arrested John Smith, a retired police officer.", "labels": [], "entities": [{"text": "John Smith", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.7683850824832916}]}], "datasetContent": [{"text": "To evaluate ASTRL, we used the WSJ derived corpus.", "labels": [], "entities": [{"text": "ASTRL", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.9205379486083984}, {"text": "WSJ derived corpus", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9499572515487671}]}, {"text": "We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees, and text annotated using a state-of-the-art statistical parser) (GoldCharniak), respectively.", "labels": [], "entities": [{"text": "gold standard trees", "start_pos": 74, "end_pos": 93, "type": "DATASET", "confidence": 0.9306077758471171}, {"text": "GoldCharniak", "start_pos": 205, "end_pos": 217, "type": "DATASET", "confidence": 0.8828176856040955}]}, {"text": "In the third, we trained and tested on the Charniak Parser (Charniak-Charniak).", "labels": [], "entities": []}, {"text": "In gold standard parse trees the syntactic categories are annotated with functional tags.", "labels": [], "entities": []}, {"text": "Since current statistical parsers do not annotate sentences with such tags, we augment the syntactic trees with the output of a Named Entity tagger.", "labels": [], "entities": []}, {"text": "For the Named Entity information, we used a publicly available NE Recognizer capable of recognizing a range of categories including Person, Location and Organization.", "labels": [], "entities": [{"text": "NE Recognizer", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9037040174007416}]}, {"text": "On the CoNLL-03 shared task, its f-score is about 90% 4 . We evaluate our system from different points of view, as described below.", "labels": [], "entities": [{"text": "f-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9852537512779236}]}, {"text": "For all the evaluation methods, we performed five-fold cross validation and report the average precision, recall and f-scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9978874325752258}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9980714917182922}, {"text": "f-scores", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.8145545125007629}]}], "tableCaptions": [{"text": " Table 1: Average inter-annotator agreement for identify- ing relations.", "labels": [], "entities": [{"text": "identify- ing relations", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.8415121883153915}]}, {"text": " Table 2: ASTRL performance (precision, recall and f- score) for relation extraction. The comma types were  used only to learn the rules. During evaluation, only the  relations were scored.", "labels": [], "entities": [{"text": "ASTRL", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8489872813224792}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9983866214752197}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9958535432815552}, {"text": "f- score", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.920326809088389}, {"text": "relation extraction", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8222095668315887}]}, {"text": " Table 3: Performance of STRs learned by ASTRL and the smallest valid STRs in identifying comma types and  generating relations.", "labels": [], "entities": []}, {"text": " Table 4: ASTRL performance (precision, recall and f- score) for OTHER identification.", "labels": [], "entities": [{"text": "ASTRL", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.965694010257721}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9960551261901855}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9974290728569031}, {"text": "f- score", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9705042243003845}, {"text": "OTHER identification", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8922403752803802}]}]}