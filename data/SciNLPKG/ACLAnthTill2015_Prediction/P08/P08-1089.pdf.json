{"title": [{"text": "Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora", "labels": [], "entities": [{"text": "Approach", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9396608471870422}, {"text": "Extracting Paraphrase Patterns from Bilingual Corpora", "start_pos": 19, "end_pos": 72, "type": "TASK", "confidence": 0.85526375969251}]}], "abstractContent": [{"text": "Paraphrase patterns are useful in paraphrase recognition and generation.", "labels": [], "entities": [{"text": "paraphrase recognition and generation", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.818689376115799}]}, {"text": "In this paper, we present a pivot approach for extracting paraphrase patterns from bilingual parallel corpora , whereby the English paraphrase patterns are extracted using the sentences in a foreign language as pivots.", "labels": [], "entities": []}, {"text": "We propose a log-linear model to compute the paraphrase likelihood of two patterns and exploit feature functions based on maximum likelihood estimation (MLE) and lexical weighting (LW).", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE)", "start_pos": 122, "end_pos": 157, "type": "METRIC", "confidence": 0.8215253949165344}]}, {"text": "Using the presented method, we extract over 1,000,000 pairs of paraphrase patterns from 2M bilingual sentence pairs, the precision of which exceeds 67%.", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9995057582855225}]}, {"text": "The evaluation results show that: (1) The pivot approach is effective in extracting paraphrase patterns, which significantly outperforms the conventional method DIRT.", "labels": [], "entities": [{"text": "extracting paraphrase patterns", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.8208516836166382}]}, {"text": "Especially, the log-linear model with the proposed feature functions achieves high performance.", "labels": [], "entities": []}, {"text": "(2) The coverage of the extracted paraphrase patterns is high, which is above 84%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9848189353942871}]}, {"text": "(3) The extracted paraphrase patterns can be classified into 5 types, which are useful in various applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Paraphrases are different expressions that convey the same meaning.", "labels": [], "entities": []}, {"text": "Paraphrases are important in plenty of natural language processing (NLP) applications, such as question answering (QA) (), machine translation (MT)), multi-document summarization (), and natural language generation (.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.8657532691955566}, {"text": "machine translation (MT))", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.8291568219661712}, {"text": "multi-document summarization", "start_pos": 150, "end_pos": 178, "type": "TASK", "confidence": 0.6472333371639252}, {"text": "natural language generation", "start_pos": 187, "end_pos": 214, "type": "TASK", "confidence": 0.6523368060588837}]}, {"text": "Paraphrase patterns are sets of semantically equivalent patterns, in which a pattern generally contains two parts, i.e., the pattern words and slots.", "labels": [], "entities": []}, {"text": "For example, in the pattern \"X solves Y\", \"solves\" is the pattern word, while \"X\" and \"Y\" are slots.", "labels": [], "entities": []}, {"text": "One can generate a text unit (phrase or sentence) by filling the pattern slots with specific words.", "labels": [], "entities": []}, {"text": "Paraphrase patterns are useful in both paraphrase recognition and generation.", "labels": [], "entities": [{"text": "paraphrase recognition and generation", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.794177383184433}]}, {"text": "In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.9364368617534637}]}, {"text": "In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.9220389723777771}]}, {"text": "A variety of methods have been proposed on paraphrase patterns extraction (;).", "labels": [], "entities": [{"text": "paraphrase patterns extraction", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.9111368060112}]}, {"text": "However, these methods have some shortcomings.", "labels": [], "entities": []}, {"text": "Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low.", "labels": [], "entities": [{"text": "precisions", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9991948008537292}]}, {"text": "In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach.", "labels": [], "entities": []}, {"text": "We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns.", "labels": [], "entities": []}, {"text": "This assumption is an extension of the one presented in), which was used for deriving phrasal paraphrases from bilingual corpora.", "labels": [], "entities": []}, {"text": "Our method involves three steps: (1) corpus preprocessing, including English monolingual dependency parsing and English-foreign language word alignment, (2) aligned patterns induction, which produces English patterns along with the aligned pivot patterns in the foreign language, (3) paraphrase patterns extraction, in which paraphrase patterns are extracted based on a log-linear model.", "labels": [], "entities": [{"text": "English monolingual dependency parsing", "start_pos": 69, "end_pos": 107, "type": "TASK", "confidence": 0.6357377767562866}, {"text": "English-foreign language word alignment", "start_pos": 112, "end_pos": 151, "type": "TASK", "confidence": 0.602880172431469}, {"text": "aligned patterns induction", "start_pos": 157, "end_pos": 183, "type": "TASK", "confidence": 0.6306491394837698}, {"text": "paraphrase patterns extraction", "start_pos": 284, "end_pos": 314, "type": "TASK", "confidence": 0.7577701807022095}]}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "Firstly, we are the first to use a pivot approach to extract paraphrase patterns from bilingual corpora, though similar methods have been used for learning phrasal paraphrases.", "labels": [], "entities": []}, {"text": "Our experiments show that the pivot approach significantly outperforms conventional methods.", "labels": [], "entities": []}, {"text": "Secondly, we propose a log-linear model for computing the paraphrase likelihood.", "labels": [], "entities": []}, {"text": "Besides, we use feature functions based on maximum likelihood estimation (MLE) and lexical weighting (LW), which are effective in extracting paraphrase patterns.", "labels": [], "entities": [{"text": "maximum likelihood estimation (MLE)", "start_pos": 43, "end_pos": 78, "type": "METRIC", "confidence": 0.7191241433223089}]}, {"text": "Using the proposed approach, we extract over 1,000,000 pairs of paraphrase patterns from 2M bilingual sentence pairs, the precision of which is above 67%.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9995375871658325}]}, {"text": "Experimental results show that the pivot approach evidently outperforms DIRT, a well known method that extracts paraphrase patterns from monolingual corpora).", "labels": [], "entities": []}, {"text": "Besides, the log-linear model is more effective than the conventional model presented in).", "labels": [], "entities": []}, {"text": "In addition, the coverage of the extracted paraphrase patterns is high, which is above 84%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9802157282829285}]}, {"text": "Further analysis shows that 5 types of paraphrase patterns can be extracted with our method, which can by used in multiple NLP applications.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work on paraphrase patterns extraction.", "labels": [], "entities": [{"text": "paraphrase patterns extraction", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.9022839268048605}]}, {"text": "Section 3 presents our method in detail.", "labels": [], "entities": []}, {"text": "We evaluate the proposed method in Section 4, and finally conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The E-C parallel corpus in our experiments was constructed using several LDC bilingual corpora . After filtering sentences that are too long (> 40 words) or too short (< 5 words), 2,048,009 pairs of parallel sentences were retained.", "labels": [], "entities": []}, {"text": "We used two constraints in the experiments to improve the efficiency of computation.", "labels": [], "entities": []}, {"text": "First, only subtrees containing no more than 10 words were used to induce English patterns.", "labels": [], "entities": []}, {"text": "Second, although any POS tag can form a slot in the induced patterns, we only focused on three kinds of POSes in the experiments, i.e., nouns (tags include NN, NNS, NNP, NNPS), verbs (VB, VBD, VBG, VBN, VBP, VBZ), and adjectives (JJ, JJS, JJR).", "labels": [], "entities": []}, {"text": "In addition, we constrained that a pattern must contain at least one content word  so as to filter patterns like \"the [NN 1]\".", "labels": [], "entities": []}, {"text": "As previously mentioned, in the log-linear model of this paper, we use both MLE based and LW based feature functions.", "labels": [], "entities": []}, {"text": "In this section, we evaluate the log-linear model (LL-Model) and compare it with the MLE based model (MLE-Model) presented by . We extracted paraphrase patterns using two models, respectively.", "labels": [], "entities": []}, {"text": "From the results of each model, we randomly picked 3,000 pairs of paraphrase patterns to evaluate the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.998761773109436}]}, {"text": "The 6,000 pairs of paraphrase patterns were mixed and presented to the human judges, so that the judges cannot know by which model each pair was produced.", "labels": [], "entities": []}, {"text": "The sampled patterns were then manually labeled and the precision was computed as described in Section 3.4.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9994028806686401}]}, {"text": "The number of the extracted paraphrase patterns (#PP) and the precision are depicted in the first two lines of.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9995960593223572}]}, {"text": "We can see that the numbers of paraphrase patterns extracted using the two models are comparable.", "labels": [], "entities": []}, {"text": "However, the precision of LLModel is significantly higher than MLE-Model.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9994617104530334}]}, {"text": "Actually, MLE-Model is a special case of LLModel and the enhancement of the precision is mainly due to the use of LW based features.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9994581341743469}]}, {"text": "It is not surprising, since have pointed out that word alignment error is the major factor that influences the performance of the methods learning paraphrases from bilingual corpora.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7449217438697815}]}, {"text": "The LW based features validate the quality of word alignment and assign low scores to those aligned E-C pattern pairs with incorrect alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.72536301612854}]}, {"text": "Hence the precision can be enhanced.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9996836185455322}]}, {"text": "In this experiment, we also estimated a threshold T for MLE-Model using the development set (T = \u22125.1).", "labels": [], "entities": [{"text": "MLE-Model", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.5334103107452393}, {"text": "T", "start_pos": 93, "end_pos": 94, "type": "METRIC", "confidence": 0.9590505361557007}]}, {"text": "The pattern pairs whose score based on Equation (1) exceed T were extracted as paraphrase patterns.", "labels": [], "entities": [{"text": "Equation", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.992789089679718}]}, {"text": "In Section 4.1, we have evaluated the precision of the paraphrase patterns without considering context information.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9984582662582397}]}, {"text": "In this section, we evaluate the paraphrase patterns within specific context sentences.", "labels": [], "entities": []}, {"text": "The open test set includes 119 English sentences.", "labels": [], "entities": [{"text": "open test set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7044278184572855}]}, {"text": "We parsed the sentences with MaltParser and induced patterns as described in Section 3.2.", "labels": [], "entities": []}, {"text": "For each pattern e in sentence SE , we searched e's paraphrase patterns from the database of the extracted paraphrase patterns.", "labels": [], "entities": []}, {"text": "The result shows that 101 of the 119 sentences contain at least one pattern that can be paraphrased using the extracted paraphrase patterns, the coverage of which is 84.87%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9592284560203552}]}, {"text": "Furthermore, since a pattern may have several paraphrase patterns, we exploited a method to automatically select the best one in the given context sentence.", "labels": [], "entities": []}, {"text": "In detail, a paraphrase pattern e of e was reranked based on a language model (LM): score(e |e, SE ) = \u03bbscore LL (e |e) + (1 \u2212 \u03bb)score LM (e |S E ) (7) Notice that, a pattern may contain more than one type of slots, thus the sum of the percentages is larger than 1.", "labels": [], "entities": []}, {"text": "Here, score LL (e |e) denotes the score based on Equation (3).", "labels": [], "entities": [{"text": "Equation", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9913149476051331}]}, {"text": "score LM (e |S E ) is the LM based score: score LM (e |S E ) = 1 n logP LM (S E ), where SE is the sentence generated by replacing e in SE withe . The language model in the experiment was a tri-gram model trained using the English sentences in the bilingual corpus.", "labels": [], "entities": []}, {"text": "We empirically set \u03bb = 0.7.", "labels": [], "entities": []}, {"text": "The selected best paraphrase patterns in context sentences were manually labeled.", "labels": [], "entities": []}, {"text": "The context information was also considered by our judges.", "labels": [], "entities": []}, {"text": "The result shows that the precision of the best paraphrase patterns is 59.39%.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.999631404876709}]}, {"text": "To investigate the contribution of the LM based score, we ran the experiment again with \u03bb = 1 (ignoring the LM based score) and found that the precision is 57.09%.", "labels": [], "entities": [{"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9988300204277039}]}, {"text": "It indicates that the LM based reranking can improve the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9991758465766907}]}, {"text": "However, the improvement is small.", "labels": [], "entities": []}, {"text": "Further analysis shows that about 70% of the correct paraphrase substitutes are in the type of phrase replacement.", "labels": [], "entities": [{"text": "phrase replacement", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7120779901742935}]}], "tableCaptions": [{"text": " Table 2: Comparison of paraphrasing methods.", "labels": [], "entities": []}, {"text": " Table 4: The statistics of the numbers of pattern slots.", "labels": [], "entities": []}, {"text": " Table 5: The statistics of the type of pattern slots.", "labels": [], "entities": []}]}