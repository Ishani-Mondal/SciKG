{"title": [], "abstractContent": [{"text": "The validity of semantic inferences depends on the contexts in which they are applied.", "labels": [], "entities": []}, {"text": "We propose a generic framework for handling contextual considerations within applied inference , termed Contextual Preferences.", "labels": [], "entities": []}, {"text": "This framework defines the various context-aware components needed for inference and their relationships.", "labels": [], "entities": []}, {"text": "Contextual preferences extend and generalize previous notions, such as se-lectional preferences, while experiments show that the extended framework allows improving inference quality on real application data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Applied semantic inference is typically concerned with inferring a target meaning from a given text.", "labels": [], "entities": [{"text": "Applied semantic inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6904001931349436}]}, {"text": "For example, to answer \"Who wrote Idomeneo?\", Question Answering (QA) systems need to infer the target meaning 'Mozart wrote Idomeneo' from a given text \"Mozart composed Idomeneo\".", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.7811355769634247}]}, {"text": "Following common Textual Entailment terminology, we denote the target meaning by h (for hypothesis) and the given text by t.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7356176972389221}]}, {"text": "A typical applied inference operation is matching.", "labels": [], "entities": []}, {"text": "Sometimes, h can be directly matched int (in the example above, if the given sentence would be literally \"Mozart wrote Idomeneo\").", "labels": [], "entities": []}, {"text": "Generally, the target meaning can be expressed int in many different ways.", "labels": [], "entities": []}, {"text": "Indirect matching is then needed, using inference knowledge that maybe captured through rules, termed here entailment rules.", "labels": [], "entities": []}, {"text": "In our example, 'Mozart wrote Idomeneo' can be inferred using the rule 'X compose Y \u2192 X write Y '.", "labels": [], "entities": []}, {"text": "Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bi-directional entailment rules) (.", "labels": [], "entities": []}, {"text": "A common practice is to try matching the structure of h, or of the left-hand-side of a ruler, within t.", "labels": [], "entities": []}, {"text": "However, context should be considered to allow valid matching.", "labels": [], "entities": []}, {"text": "For example, suppose that to find acquisitions of companies we specify the target template hypothesis (a hypothesis with variables) 'X acquire Y '.", "labels": [], "entities": []}, {"text": "This h should not be matched in \"children acquire language quickly\", because in this context Y is not a company.", "labels": [], "entities": []}, {"text": "Similarly, the rule 'X charge Y \u2192 X accuse Y ' should not be applied to \"This store charged my account\", since the assumed sense of 'charge' in the rule is different than its sense in the text.", "labels": [], "entities": []}, {"text": "Thus, the intended contexts for hand rand the context within the given t should be properly matched to verify valid inference.", "labels": [], "entities": []}, {"text": "Context matching at inference time was often approached in an application-specific manner (.", "labels": [], "entities": [{"text": "Context matching", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8842873275279999}]}, {"text": "Recently, some generic methods were proposed to handle context-sensitive inference, but these usually treat only a single aspect of context matching (see Section 6).", "labels": [], "entities": [{"text": "context matching", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.722729355096817}]}, {"text": "We propose a comprehensive framework for handling various contextual considerations, termed Contextual Preferences.", "labels": [], "entities": []}, {"text": "It extends and generalizes previous work, defining the needed contextual components and their relationships.", "labels": [], "entities": []}, {"text": "We also present and implement concrete representation models and un-supervised matching methods for these components.", "labels": [], "entities": []}, {"text": "While our presentation focuses on semantic inference using lexical-syntactic structures, the proposed framework and models seem suitable for other common types of representations as well.", "labels": [], "entities": []}, {"text": "We applied our models to a test set derived from the ACE 2005 event detection task, a standard Information Extraction (IE) benchmark.", "labels": [], "entities": [{"text": "ACE 2005 event detection task", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.8219927191734314}, {"text": "Information Extraction (IE)", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.7509896516799927}]}, {"text": "We show the benefits of our extended framework for textual inference and present component-wise analysis of the results.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7366563379764557}]}, {"text": "To the best of our knowledge, these are also the first unsupervised results for event argument extraction in the ACE 2005 dataset.", "labels": [], "entities": [{"text": "event argument extraction", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.6815242369969686}, {"text": "ACE 2005 dataset", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9719924728075663}]}], "datasetContent": [{"text": "Evaluating the contribution of Contextual Preferences models requires: (a) a sample of test hypotheses, and (b) a corresponding corpus that contains sentences which entail these hypotheses, where all hypothesis matches (either director via rules) are annotated.", "labels": [], "entities": []}, {"text": "We found that the available event mention annotations in the ACE 2005 training set 4 provide a useful test set that meets these generic criteria, with the added value of a standard real-world dataset.", "labels": [], "entities": [{"text": "ACE 2005 training set", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.980344608426094}]}, {"text": "The ACE annotation includes 33 types of events, for which all event mentions are annotated in the corpus.", "labels": [], "entities": []}, {"text": "The annotation of each mention includes the instantiated arguments for the predicates, which represent the participants in the event, as well as general attributes such as time and place.", "labels": [], "entities": []}, {"text": "ACE guidelines specify for each event type its possible arguments, where all arguments are optional.", "labels": [], "entities": [{"text": "ACE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8980546593666077}]}, {"text": "Each argument is associated with a semantic role and a list of possible named-entity types.", "labels": [], "entities": []}, {"text": "For instance, an Injure event may have the arguments {Agent, Victim, Instrument, Time, Place}, where Victim should be a person.", "labels": [], "entities": []}, {"text": "For each event type we manually created a small set of template hypotheses that correspond to the given event predicate, and specified the appropriate semantic roles for each variable.", "labels": [], "entities": []}, {"text": "We considered only binary hypotheses, due to the type of available entailment rules (see below).", "labels": [], "entities": []}, {"text": "For Injure, the set of hypotheses included 'A injure V' and 'injure V in T' where role(A)={Agent, Instrument}, role(V)={Victim}, and role(T)={Time, Place}.", "labels": [], "entities": []}, {"text": "Thus, correct match of an argument corresponds to correct role identification.", "labels": [], "entities": [{"text": "role identification", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.692119911313057}]}, {"text": "The templates were represented as Minipar (Lin, 1998b) dependency parse-trees.", "labels": [], "entities": [{"text": "Minipar (Lin, 1998b) dependency parse-trees", "start_pos": 34, "end_pos": 77, "type": "DATASET", "confidence": 0.8751183599233627}]}, {"text": "The Contextual Preferences for h were constructed manually: the named-entity types for cp v:n (h) were set by adapting the entity types given in the guidelines to the types supported by the Lingpipe NER (described in Section 3.2).", "labels": [], "entities": [{"text": "Lingpipe NER", "start_pos": 190, "end_pos": 202, "type": "DATASET", "confidence": 0.9512064158916473}]}, {"text": "cp g (h) was generated from a shortlist of nouns and verbs that were extracted from the verbal event definition in the ACE guidelines.", "labels": [], "entities": [{"text": "ACE guidelines", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9438444674015045}]}, {"text": "For Injure, this list included {injure:v, injury:n, wound:v}.", "labels": [], "entities": []}, {"text": "This assumes that when writing down an event definition the user would also specify such representative keywords.", "labels": [], "entities": []}, {"text": "Entailment-rules fora given h (rules in which RHS is equal to h) were learned automatically by the DIRT algorithm (), which also produces a quality score for each rule.", "labels": [], "entities": [{"text": "RHS", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9816555380821228}]}, {"text": "We implemented a canonized version of DIRT (  on the Reuters corpus parsed by Minipar.", "labels": [], "entities": [{"text": "DIRT", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8686710596084595}, {"text": "Reuters corpus parsed by Minipar", "start_pos": 53, "end_pos": 85, "type": "DATASET", "confidence": 0.956387197971344}]}, {"text": "Each rule's arguments for cp v (r) were also collected from this corpus.", "labels": [], "entities": []}, {"text": "We assessed the CP framework by its ability to correctly rank, for each predicate (event), all the candidate entailing mentions that are found for it in the test corpus.", "labels": [], "entities": []}, {"text": "Such ranking evaluation is suitable for unsupervised settings, with a perfect ranking placing all correct mentions before any incorrect ones.", "labels": [], "entities": []}, {"text": "The candidate mentions are found in the parsed test corpus by matching the specified event hypotheses, either directly or via the given set of entailment rules, using a syntactic matcher similar to the one in . Finally, the mentions are ranked by their match scores, as described in Section 3.3.", "labels": [], "entities": []}, {"text": "As detailed in the next section, those candidate mentions which are also annotated as mentions of the same event in ACE are considered correct.", "labels": [], "entities": [{"text": "ACE", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.7918301224708557}]}, {"text": "The evaluation aims to assess the correctness of inferring a target semantic meaning, which is de-noted by a specific predicate.", "labels": [], "entities": []}, {"text": "Therefore, we eliminated four ACE event types that correspond to multiple distinct predicates.", "labels": [], "entities": []}, {"text": "For instance, the TransferMoney event refers to both donating and lending money, which are not distinguished by the ACE annotation.", "labels": [], "entities": []}, {"text": "We also omitted three events with less than 10 mentions and two events for which the given set of learned rules could not match any mention.", "labels": [], "entities": []}, {"text": "We were left with 24 event types for evaluation, which amount to 4085 event mentions in the dataset.", "labels": [], "entities": []}, {"text": "Out of these, our binary templates can correctly match only mentions with at least two arguments, which appear 2076 times in the dataset.", "labels": [], "entities": []}, {"text": "Comparing with previous evaluation methodologies, in proper context matching was evaluated by post-hoc judgment of a sample of rule applications fora sample of rules.", "labels": [], "entities": []}, {"text": "Such annotation needs to be repeated each time the set of rules is changed.", "labels": [], "entities": []}, {"text": "In addition, since the corpus annotation is not exhaustive, recall could not be computed.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9992043375968933}]}, {"text": "By contrast, we use a standard real-world dataset, in which all mentions are annotated.", "labels": [], "entities": []}, {"text": "This allows immediate comparison of different rule sets and matching methods, without requiring any additional (post-hoc) annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when only matching template hypotheses  directly.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9463930279016495}, {"text": "Precision (P)", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9578028619289398}, {"text": "Mean Average Pre- cision (MAP)", "start_pos": 40, "end_pos": 70, "type": "METRIC", "confidence": 0.9691885337233543}]}, {"text": " Table 2: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when also using rules for matching.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.948292925953865}, {"text": "Precision (P)", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9590151011943817}, {"text": "Mean Average Pre- cision (MAP)", "start_pos": 40, "end_pos": 70, "type": "METRIC", "confidence": 0.9713907614350319}]}, {"text": " Table 3: MAP(%), under the '50 rules, All' setup, when  adding component match scores to Precision (P) or prior- only MAP baselines, and when ranking with allCP or  allCP+pr methods but ignoring that component scores.", "labels": [], "entities": []}]}