{"title": [{"text": "Searching Questions by Identifying Question Topic and Question Focus", "labels": [], "entities": [{"text": "Identifying Question Topic and Question Focus", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.6781385441621145}]}], "abstractContent": [{"text": "This paper is concerned with the problem of question search.", "labels": [], "entities": [{"text": "question search", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.8008596003055573}]}, {"text": "In question search, given a question as query, we are to return questions semantically equivalent or close to the queried question.", "labels": [], "entities": []}, {"text": "In this paper, we propose to conduct question search by identifying question topic and question focus.", "labels": [], "entities": [{"text": "question search", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8067034482955933}]}, {"text": "More specifically, we first summarize questions in a data structure consisting of question topic and question focus.", "labels": [], "entities": [{"text": "summarize questions", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9014509916305542}]}, {"text": "Then we model question topic and question focus in a language modeling framework for search.", "labels": [], "entities": []}, {"text": "We also propose to use the MDL-based tree cut model for identifying question topic and question focus automatically.", "labels": [], "entities": [{"text": "question focus", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.6839138865470886}]}, {"text": "Experimental results indicate that our approach of identifying question topic and question focus for search significantly outperforms the base-line methods such as Vector Space Model (VSM) and Language Model for Information Retrieval (LMIR).", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past few years, online services have been building up very large archives of questions and their answers, for example, traditional FAQ services and emerging community-based Q&A services (e.g., Yahoo! Answers 1 , Live QnA 2 , and Baidu Zhidao 3 ).", "labels": [], "entities": []}, {"text": "To make use of the large archives of questions and their answers, it is critical to have functionality facilitating users to search previous answers.", "labels": [], "entities": []}, {"text": "Typically, such functionality is achieved by first retrieving questions expected to have the same answers as a queried question and then returning the related answers to users.", "labels": [], "entities": []}, {"text": "For example, given question Q1 in, question Q2 can be re-turned and its answer will then be used to answer Q1 because the answer of Q2 is expected to partially satisfy the queried question Q1.", "labels": [], "entities": []}, {"text": "This is what we called question search.", "labels": [], "entities": [{"text": "question search", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.8014670312404633}]}, {"text": "In question search, returned questions are semantically equivalent or close to the queried question.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have conducted experiments to verify the effectiveness of our approach to question search.", "labels": [], "entities": [{"text": "question search", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.8099336326122284}]}, {"text": "Particularly, we have investigated the use of identifying question topic and question focus for search.", "labels": [], "entities": []}, {"text": "We We developed two test sets, one for the category 'travel' denoted as 'TRL-TST', and the other for 'computers & internet' denoted as 'CI-TST'.", "labels": [], "entities": [{"text": "TRL-TST", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9148616790771484}]}, {"text": "In order to create the test sets, we randomly selected 200 questions for each category.", "labels": [], "entities": []}, {"text": "To obtain the ground-truth of question search, we employed the Vector Space Model (VSM) to retrieve the top 20 results and obtained manual judgments.", "labels": [], "entities": [{"text": "question search", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7323857247829437}]}, {"text": "The top 20 results don't include the queried question itself.", "labels": [], "entities": []}, {"text": "Given a returned result by VSM, an assessor is asked to label it with 'relevant' or 'irrelevant'.", "labels": [], "entities": [{"text": "VSM", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7213390469551086}]}, {"text": "If a returned result is considered semantically equivalent (or close) to the queried question, the assessor will label it as 'relevant'; otherwise, the assessor will label it as 'irrelevant'.", "labels": [], "entities": []}, {"text": "Two assessors were involved in the manual judgments.", "labels": [], "entities": []}, {"text": "Each of them was asked to label 100 questions from 'TRL-TST' and 100 from 'CI-TST'.", "labels": [], "entities": [{"text": "TRL-TST", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.7171095609664917}]}, {"text": "In the process of manually judging questions, the assessors were presented only the titles of the questions (for both the queried questions and the returned questions).", "labels": [], "entities": []}, {"text": "We utilized two baseline methods for demonstrating the effectiveness of our approach, the VSM and the LMIR (language modeling method for information retrieval).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.7105665355920792}]}, {"text": "We made use of three measures for evaluating the results of question search methods.", "labels": [], "entities": []}, {"text": "They are MAP, R-precision, and MRR.", "labels": [], "entities": [{"text": "MAP", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8743166327476501}, {"text": "MRR", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.7018675804138184}]}], "tableCaptions": [{"text": " Table 2 pro- vides the statistics on the final test set.", "labels": [], "entities": [{"text": "final test set", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.7216428518295288}]}, {"text": " Table 3. Searching Questions about 'Travel'", "labels": [], "entities": [{"text": "Searching Questions about 'Travel'", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.638669510682424}]}, {"text": " Table 5. Searching Questions about 'Computers & In- ternet'", "labels": [], "entities": []}, {"text": " Table 6. Using Translation Probability", "labels": [], "entities": []}]}