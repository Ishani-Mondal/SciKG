{"title": [{"text": "Assessing the Costs of Sampling Methods in Active Learning for Annotation", "labels": [], "entities": [{"text": "Assessing the Costs of Sampling", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7776500940322876}, {"text": "Annotation", "start_pos": 63, "end_pos": 73, "type": "TASK", "confidence": 0.7856715321540833}]}], "abstractContent": [{"text": "Traditional Active Learning (AL) techniques assume that the annotation of each datum costs the same.", "labels": [], "entities": [{"text": "Active Learning (AL)", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.686053478717804}]}, {"text": "This is not the case when annotating sequences; some sequences will take longer than others.", "labels": [], "entities": []}, {"text": "We show that the AL technique which performs best depends on how cost is measured.", "labels": [], "entities": []}, {"text": "Applying an hourly cost model based on the results of an annotation user study, we approximate the amount of time necessary to annotate a given sentence.", "labels": [], "entities": []}, {"text": "This model allows us to evaluate the effectiveness of AL sampling methods in terms of time spent in annotation.", "labels": [], "entities": []}, {"text": "We acheive a 77% reduction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Tree-bank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9677025675773621}, {"text": "Penn Tree-bank", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9959867298603058}]}, {"text": "More significantly, we make the case for measuring cost in assessing AL methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Obtaining human annotations for linguistic data is labor intensive and typically the costliest part of the acquisition of an annotated corpus.", "labels": [], "entities": []}, {"text": "Hence, there is strong motivation to reduce annotation costs, but not at the expense of quality.", "labels": [], "entities": []}, {"text": "Active learning (AL) can be employed to reduce the costs of corpus annotation.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6082890331745148}]}, {"text": "With the assistance of AL, the role of the human oracle is either to label a datum or simply to correct the label from an automatic labeler.", "labels": [], "entities": []}, {"text": "For the present work, we assume that correction is less costly than annotation from scratch; testing this assumption is the subject of future work.", "labels": [], "entities": [{"text": "correction", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.7256807684898376}]}, {"text": "In AL, the learner leverages newly provided annotations to select more informative sentences which in turn can be used by the automatic labeler to provide more accurate annotations in future iterations.", "labels": [], "entities": []}, {"text": "Ideally, this process yields accurate labels with less human effort.", "labels": [], "entities": []}, {"text": "Annotation cost is project dependent.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.6848408579826355}]}, {"text": "For instance, annotators maybe paid for the number of annotations they produce or by the hour.", "labels": [], "entities": []}, {"text": "In the context of parse tree annotation, Hwa (2004) estimates cost using the number of constituents needing labeling and use a measure related to the number of possible parses.", "labels": [], "entities": [{"text": "parse tree annotation", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.9244168400764465}]}, {"text": "With few exceptions, previous work on AL has largely ignored the question of actual labeling time.", "labels": [], "entities": []}, {"text": "One exception is) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking.", "labels": [], "entities": [{"text": "noun phrase chunking", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.6801366806030273}]}, {"text": "In contrast, we focus on the performance of AL algorithms using different estimates of cost (including time) for part of speech (POS) tagging, although the results are applicable to AL for sequential labeling in general.", "labels": [], "entities": [{"text": "part of speech (POS) tagging", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.6217276922294072}, {"text": "sequential labeling", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.6704833209514618}]}, {"text": "We make the case for measuring cost in assessing AL methods by showing that the choice of a cost function significantly affects the choice of AL algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our test data consists of English prose from the POS-tagged Wall Street Journal text in the Penn Treebank (PTB) version 3.", "labels": [], "entities": [{"text": "POS-tagged Wall Street Journal text in the Penn Treebank (PTB) version 3", "start_pos": 49, "end_pos": 121, "type": "DATASET", "confidence": 0.9083649941853115}]}, {"text": "We use sections 2-21 as initially unannotated data.", "labels": [], "entities": []}, {"text": "We employ section 24 as the development test set on which tag accuracy is computed at the end of every iteration of AL.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9723289012908936}]}, {"text": "For tagging, we employ an order two Maximum Entropy Markov Model (MEMM).", "labels": [], "entities": [{"text": "tagging", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9726606011390686}]}, {"text": "For decoding, we found that abeam of size five sped up the decoder with almost no degradation inaccuracy from Viterbi.", "labels": [], "entities": []}, {"text": "The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by.", "labels": [], "entities": [{"text": "MEMM POS tagging", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.6693150003751119}]}, {"text": "In our implementation, QBU employs a single MEMM tagger.", "labels": [], "entities": []}, {"text": "We approximate the entropy of the per-sentence tag sequences by summing over perword entropy and have found that this approximation provides equivalent performance to the exact sequence entropy.", "labels": [], "entities": []}, {"text": "We also consider another selection algorithm introduced in () that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 \u2212 P ( \u02c6 t), wher\u00ea t is the Viterbi (best) tag sequence.", "labels": [], "entities": []}, {"text": "We label this scheme QBUOMM (OMM = \"One Minus Max\").", "labels": [], "entities": [{"text": "QBUOMM", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.837257444858551}, {"text": "OMM", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9301318526268005}]}, {"text": "Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following.", "labels": [], "entities": []}, {"text": "Each committee member's training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU.", "labels": [], "entities": [{"text": "QBU", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.9371151924133301}]}, {"text": "We follow in the implementation of vote entropy for sentence selection using these models.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.746646374464035}]}, {"text": "When comparing the relative performance of AL algorithms, learning curves can be challenging to interpret.", "labels": [], "entities": []}, {"text": "As curves proceed to the right, they can approach one another so closely that it maybe difficult to seethe advantage of one curve over another.", "labels": [], "entities": []}, {"text": "For this reason, we introduce the \"cost reduction curve\".", "labels": [], "entities": []}, {"text": "In such a curve, the accuracy is the independent variable.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995214939117432}]}, {"text": "We then compute the percent reduction in cost (e.g., number of words or hours) over the cost of the random baseline for the same accuracy a: Consequently, the random baseline represents the trajectory redux(a) = 0.0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9970629811286926}]}, {"text": "Algorithms less costly than the baseline appear above the baseline.", "labels": [], "entities": []}, {"text": "For a specific accuracy value on a learning curve, the corresponding value of the cost on the random baseline is estimated by interpolation between neighboring points on the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9963749051094055}]}, {"text": "Using hourly cost, shows the cost reduction curves of several AL algorithms, including those already considered in the learning curves of (except LS).", "labels": [], "entities": []}, {"text": "Restricting the discussion to the random baseline, QBC, and QBU: for low accuracies, random selection is the cheapest according to hourly cost; QBU begins to be cost-effective at around 91%; and QBC begins to outperform the baseline and QBU around 80%.", "labels": [], "entities": []}], "tableCaptions": []}