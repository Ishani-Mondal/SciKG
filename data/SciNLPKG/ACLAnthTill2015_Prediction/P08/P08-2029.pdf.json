{"title": [{"text": "Kernels on Linguistic Structures for Answer Extraction", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8155645430088043}]}], "abstractContent": [{"text": "Natural Language Processing (NLP) for Information Retrieval has always been an interesting and challenging research area.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6899407704671224}, {"text": "Information Retrieval", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7798572480678558}]}, {"text": "Despite the high expectations, most of the results indicate that successfully using NLP is very complex.", "labels": [], "entities": []}, {"text": "In this paper, we show how Support Vector Machines along with kernel functions can effectively represent syntax and semantics.", "labels": [], "entities": []}, {"text": "Our experiments on question/answer classification show that the above models highly improve on bag-of-words on a TREC dataset.", "labels": [], "entities": [{"text": "question/answer classification", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7140232771635056}, {"text": "TREC dataset", "start_pos": 113, "end_pos": 125, "type": "DATASET", "confidence": 0.865713506937027}]}], "introductionContent": [{"text": "Question Answering (QA) is an IR task where the major complexity resides in question processing and answer extraction) rather than document retrieval (a step usually carried out by off-the shelf IR engines).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.897019112110138}, {"text": "IR task", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9239923059940338}, {"text": "answer extraction", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7174473702907562}, {"text": "document retrieval", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.7103565633296967}]}, {"text": "In question processing, useful information is gathered from the question and a query is created.", "labels": [], "entities": [{"text": "question processing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8451982736587524}]}, {"text": "This is submitted to an IR module, which provides a ranked list of relevant documents.", "labels": [], "entities": [{"text": "IR", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.7479360699653625}]}, {"text": "From these, the QA system extracts one or more candidate answers, which can then be re-ranked following various criteria.", "labels": [], "entities": []}, {"text": "Although typical methods are based exclusively on word similarity between query and answer, recent work, e.g. has shown that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question.", "labels": [], "entities": [{"text": "automatic detection of correct answers to a target question", "start_pos": 219, "end_pos": 278, "type": "TASK", "confidence": 0.8033846318721771}]}, {"text": "In (, we proposed the Shallow Semantic Tree Kernel (SSTK) designed to encode PASs 1 in SVMs.", "labels": [], "entities": []}, {"text": "in PropBank format, (www.cis.upenn.edu/ \u02dc ace).", "labels": [], "entities": [{"text": "PropBank format", "start_pos": 3, "end_pos": 18, "type": "DATASET", "confidence": 0.9084351360797882}]}, {"text": "In this paper, similarly to our previous approach, we design an SVM-based answer extractor, that selects the correct answers from those provided by a basic QA system by applying tree kernel technology.", "labels": [], "entities": [{"text": "SVM-based answer extractor", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6881702542304993}]}, {"text": "However, we also provide: (i) anew kernel to process PASs based on the partial tree kernel algorithm (PAS-PTK), which is highly more efficient and more accurate than the SSTK and (ii) anew kernel called Part of Speech sequence kernel (POSSK), which proves very accurate to represent shallow syntactic information in the learning algorithm.", "labels": [], "entities": []}, {"text": "To experiment with our models, we built two different corpora, WEB-QA and TREC-QA by using the description questions from TREC 2001 and annotating the answers retrieved from Web resp.", "labels": [], "entities": [{"text": "WEB-QA", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.6407809853553772}, {"text": "TREC-QA", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.6424172520637512}]}, {"text": "TREC data (available at disi.unitn.it/ \u02dc silviaq).", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.78145432472229}]}, {"text": "Comparative experiments with re-ranking models of increasing complexity show that: (a) PAS-PTK is far more efficient and effective than SSTK, (b) POSSK provides a remarkable further improvement on previous models.", "labels": [], "entities": []}, {"text": "Finally, our experiments on the TREC-QA dataset, un-biased by the presence of typical Web phrasings, show that BOW is inadequate to learn relations between questions and answers.", "labels": [], "entities": [{"text": "TREC-QA dataset", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.90677610039711}, {"text": "BOW", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.861840546131134}]}, {"text": "This is the reason why our kernels on linguistic structures improve it by 63%, which is a remarkable result for an IR task).", "labels": [], "entities": [{"text": "IR task", "start_pos": 115, "end_pos": 122, "type": "TASK", "confidence": 0.9213249087333679}]}], "datasetContent": [{"text": "In our experiments we show that (a) the PAS-PTK shallow semantic tree kernel is more efficient and effective than the SSTK proposed in (, and (b) our POSSK jointly used with PAS-PTK and STK greatly improves on BOW.", "labels": [], "entities": [{"text": "BOW", "start_pos": 210, "end_pos": 213, "type": "METRIC", "confidence": 0.776038408279419}]}, {"text": "In our experiments, we implemented the BOW and POS kernels, WSK, POSSK, STK (on syntactic PTs derived automatically with Charniak's parser), SSTK and PTK (on PASs derived automatically with our SRL system) as well as their combinations in SVM-light-TK 3 . Since answers often contain more than one PAS (see, we sum PTK (or SSTK) applied to all pairs P 1 \u00d7 P 2 , P 1 and P 2 being the sets of PASs of the first two answers.", "labels": [], "entities": []}, {"text": "The experimental datasets were created by submitting the 138 TREC 2001 test questions labeled as \"description\" in () to our basic QA system, YourQA and by gathering the top 20 answer paragraphs.", "labels": [], "entities": [{"text": "TREC 2001 test questions", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.9164032638072968}, {"text": "YourQA", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.9402927160263062}]}, {"text": "YourQA was run on two sources: Web documents by exploiting Google (code.google.com/ apis/) and the AQUAINT data used for TREC'07 (trec.nist.gov/data/qa) by exploiting Lucene (lucene.apache.org), yielding two different corpora: WEB-QA and TREC-QA.", "labels": [], "entities": [{"text": "AQUAINT data", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.7634477019309998}, {"text": "Lucene", "start_pos": 167, "end_pos": 173, "type": "DATASET", "confidence": 0.9389010667800903}, {"text": "WEB-QA", "start_pos": 227, "end_pos": 233, "type": "DATASET", "confidence": 0.771590530872345}, {"text": "TREC-QA", "start_pos": 238, "end_pos": 245, "type": "DATASET", "confidence": 0.8096780776977539}]}, {"text": "Each sentence of the returned paragraphs was manually evaluated based on whether it contained a correct answer to the corresponding question.", "labels": [], "entities": []}, {"text": "To simplify our task, we isolated for each paragraph the sentence with the maximal judgment (such ass 1 and s 2 in Sec.", "labels": [], "entities": []}, {"text": "2.2) and labeled it as positive if it answered the question either concisely or with noise, negative otherwise.", "labels": [], "entities": []}, {"text": "The resulting WEB-QA corpus contains 1309 sentences, 416 of which positive; the TREC-QA corpus contains 2256 sentences, 261 of which positive.", "labels": [], "entities": [{"text": "WEB-QA corpus", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.8899816572666168}, {"text": "TREC-QA corpus", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.8711938560009003}]}], "tableCaptions": []}