{"title": [{"text": "Collecting a Why-question corpus for development and evaluation of an automatic QA-system", "labels": [], "entities": [{"text": "QA-system", "start_pos": 80, "end_pos": 89, "type": "TASK", "confidence": 0.60698002576828}]}], "abstractContent": [{"text": "Question answering research has only recently started to spread from short factoid questions to more complex ones.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9391996562480927}]}, {"text": "One significant challenge is the evaluation: manual evaluation is a difficult, time-consuming process and not applicable within efficient development of systems.", "labels": [], "entities": []}, {"text": "Automatic evaluation requires a corpus of questions and answers, a definition of what is a correct answer, and away to compare the correct answers to automatic answers produced by a system.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7171267867088318}]}, {"text": "For this purpose we present a Wikipedia-based corpus of Why-questions and corresponding answers and articles.", "labels": [], "entities": []}, {"text": "The corpus was built by a novel method: paid participants were contacted through a Web-interface, a procedure which allowed dynamic , fast and inexpensive development of data collection methods.", "labels": [], "entities": []}, {"text": "Each question in the corpus has several corresponding, partly overlapping answers, which is an asset when estimating the correctness of answers.", "labels": [], "entities": []}, {"text": "In addition , the corpus contains information related to the corpus collection process.", "labels": [], "entities": [{"text": "corpus collection process", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7788906693458557}]}, {"text": "We believe this additional information can be used to post-process the data, and to develop an automatic approval system for further data collection projects conducted in a similar manner.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic question answering (QA) is an alternative to traditional word-based search engines.", "labels": [], "entities": [{"text": "Automatic question answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7681175718704859}]}, {"text": "Instead of returning along list of documents more or less related to the query parameters, the aim of a QA system is to isolate the exact answer as accurately as possible, and to provide the user only a short text clip containing the required information.", "labels": [], "entities": []}, {"text": "One of the major development challenges is evaluation.", "labels": [], "entities": []}, {"text": "The conferences such as TREC , CLEF and NTCIR have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents.", "labels": [], "entities": [{"text": "TREC", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7023324370384216}, {"text": "CLEF", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.7766561508178711}, {"text": "NTCIR", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.933785080909729}, {"text": "QA evaluation", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.8583228886127472}]}, {"text": "However, these conferences have focused mainly on fact-based questions with short answers, so called factoid questions.", "labels": [], "entities": []}, {"text": "Recently more complex tasks such as list, definition and discoursebased questions have also been included in TREC in a limited fashion (.", "labels": [], "entities": []}, {"text": "More complex how-and why-questions (for Asian languages) were also included in the NTCIR07, but the provided data comprised only 100 questions, of which some were also factoids (.", "labels": [], "entities": [{"text": "NTCIR07", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9638619422912598}]}, {"text": "Not only is the available non-factoid data quite limited in size, it is also questionable whether the data sets are usable in development outside the conferences.", "labels": [], "entities": []}, {"text": "suggest that training data has to be more precise, and, that it should be collected, or at least cleaned, manually.", "labels": [], "entities": []}, {"text": "Some corpora of why-questions have been collected manually: corpora described in) and () both comprise fewer than 400 questions and corresponding answers (one or two per question) formulated by native speakers.", "labels": [], "entities": []}, {"text": "However, we believe one answer per question is not enough.", "labels": [], "entities": []}, {"text": "Even with factoid questions it is sometimes difficult to define what is a correct answer, and complex questions result in a whole new level of ambiguity.", "labels": [], "entities": []}, {"text": "Correctness depends greatly on the background knowledge and expectations of the person asking the question.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8535999059677124}]}, {"text": "For example, a correct answer to the question \"Why did Mr. X take Ms.", "labels": [], "entities": [{"text": "Why did Mr. X take Ms.", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.5354993981974465}]}, {"text": "Y to a coffee shop?\" could be very different depending on whether we knew that Mr. X does not drink coffee or that he normally drinks it alone, or that Mr. X and Ms.", "labels": [], "entities": []}, {"text": "The problem of several possible answers and, in consequence, automatic evaluation has been tackled for years within another field of study: automatic summarisation (.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 150, "end_pos": 163, "type": "TASK", "confidence": 0.6692759394645691}]}, {"text": "We believe that the best method of providing \"correct\" answers is to do what has been done in that field: combine a multitude of answers to ensure both diversity and consensus among the answers.", "labels": [], "entities": []}, {"text": "Correctness of an answer is also closely related to the required level of detail.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9523038268089294}, {"text": "detail", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.962599515914917}]}, {"text": "The Internet FAQ pages were successfully used to develop QA-systems (Jijkoun and de), as have the human-powered question sites such as Answers.com, Yahoo Answers and Google Answers, where individuals can post questions and receive answers from peers (.", "labels": [], "entities": []}, {"text": "Both resources can be assumed to contain adequately errorfree information.", "labels": [], "entities": []}, {"text": "FAQ pages are created so as to answer typical questions well enough that the questions do not need to be repeated.", "labels": [], "entities": [{"text": "FAQ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.796328067779541}]}, {"text": "Question sites typically rank the answers and offer bonuses for people providing good ones.", "labels": [], "entities": []}, {"text": "However, both sites suffer from excess of information.", "labels": [], "entities": []}, {"text": "FAQ-pages tend to also answer questions which are not asked, and also contain practical examples.", "labels": [], "entities": [{"text": "FAQ-pages", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8644576668739319}]}, {"text": "Human-powered answers often contain unrelated information and discourselike elements.", "labels": [], "entities": []}, {"text": "Additionally, the answers do not always have a connection to the source material from which they could be extracted.", "labels": [], "entities": []}, {"text": "One purpose of our project was to take part in the development of QA systems by providing the community with anew type of corpus.", "labels": [], "entities": []}, {"text": "The corpus includes not only the questions with multiple answers and corresponding articles, but also certain additional information that we believe is essential to enhance the usability of the data.", "labels": [], "entities": []}, {"text": "In addition to providing anew QA corpus, we hope our description of the data collection process will provide insight, resources and motivation for further research and projects using similar collection methods.", "labels": [], "entities": [{"text": "QA corpus", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.7083871066570282}]}, {"text": "We collected our corpus through Amazon Mechanical Turk service 4 (MTurk).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk service 4 (MTurk)", "start_pos": 32, "end_pos": 72, "type": "DATASET", "confidence": 0.9220222905278206}]}, {"text": "The MTurk infrastructure allowed us to distribute our tasks to a multitude of workers around the world, without the burden of advertising.", "labels": [], "entities": [{"text": "MTurk infrastructure", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8241076469421387}]}, {"text": "The system also allowed us to test the workers suitability, and to reward the work without the bureaucracy of employment.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first time that the MTurk service has been used in equivalent purpose.", "labels": [], "entities": [{"text": "MTurk service", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.733295351266861}]}, {"text": "We conducted the data collection in three steps: generation, answering and rephrasing of questions.", "labels": [], "entities": []}, {"text": "The workers were provided with a set of Wikipedia articles, based on which the questions were created and the answers determined by sentence selection.", "labels": [], "entities": []}, {"text": "The WhyQA-corpus consists of three parts: original questions along with their rephrased versions, 8-10 partly overlapping answers for each question, and the Wikipedia articles including the ones corresponding to the questions.", "labels": [], "entities": []}, {"text": "The WhyQA-corpus is in XML-format and can be downloaded and used under the GNU Free Documentation License from www.furui.cs.titech.ac.jp/ .", "labels": [], "entities": [{"text": "WhyQA-corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8601487278938293}]}], "datasetContent": [{"text": "As an example of the post-processing of the data, we conducted some preliminary experiments on the answer agreement between workers.", "labels": [], "entities": []}, {"text": "Out of the 695 questions, 159 were filtered out in the first part of QAHIT.", "labels": [], "entities": [{"text": "QAHIT", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8296420574188232}]}, {"text": "We first uploaded only 3 assignments, and the questions that 2 out of 3 workers deemed unanswerable were filtered out.", "labels": [], "entities": []}, {"text": "This left 536 questions which were considered answered, each one having 8-10 answers from different workers.", "labels": [], "entities": []}, {"text": "Even though in the majority of cases (83% of the questions) one of the workers replied with the NoA, the ones that answered did agree up to a point: of all the answers, 72% were such that all of their sentences were selected by at least two different workers.", "labels": [], "entities": [{"text": "NoA", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.517301082611084}]}, {"text": "On top of this, an additional 17% of answers shared at least one sentence that was selected by more than one worker.", "labels": [], "entities": []}, {"text": "To understand the agreement better, we also calculated the average agreement of selected sentences based on sentence ids and N-gram overlaps between the answers.", "labels": [], "entities": [{"text": "agreement", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9299094676971436}]}, {"text": "In both of these experiments, only those 536 questions that were considered answerable were included.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Answer agreement based on sentence ids.", "labels": [], "entities": [{"text": "Answer agreement", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8841928839683533}]}, {"text": " Table 4: Answer agreement: ROUGE-1, -2, -SU and -L.", "labels": [], "entities": [{"text": "Answer agreement", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8499206304550171}, {"text": "ROUGE-1", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9944625496864319}]}]}