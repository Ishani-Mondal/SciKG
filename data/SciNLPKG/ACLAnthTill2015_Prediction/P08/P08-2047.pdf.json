{"title": [{"text": "Semantic Types of Some Generic Relation Arguments: Detection and Evaluation", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.5490334033966064}]}], "abstractContent": [{"text": "This paper presents an approach to detection of the semantic types of relation arguments employing the WordNet hierarchy.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.900052547454834}]}, {"text": "Using the SemEval-2007 data, we show that the method allows to generalize relation arguments with high precision for such generic relations as Origin-Entity, Content-Container, Instrument-Agency and some other.", "labels": [], "entities": [{"text": "SemEval-2007 data", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.792685866355896}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9927141070365906}]}], "introductionContent": [], "datasetContent": [{"text": "Data For semantic type detection, we use 7 binary relations from the training set of the SemEval-2007 competition, all definitions of which share the requirement of the syntactic closeness of the arguments.", "labels": [], "entities": [{"text": "semantic type detection", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.8020787040392557}]}, {"text": "Further, their definitions have various restrictions on the nature of the arguments.", "labels": [], "entities": []}, {"text": "Short description of the relation types we study is given below.", "labels": [], "entities": []}, {"text": "Cause-Effect(X,Y) This relation takes place if, given a sentence S, it is possible to entail that X is the cause of Y . Y is usually not an entity but a nominal denoting occurrence (activity or event).", "labels": [], "entities": []}, {"text": "Instrument-Agency(X,Y) This relation is true if S entails the fact that X is the instrument of Y (Y uses X).", "labels": [], "entities": []}, {"text": "Further, X is an entity and Y is an actor or an activity.", "labels": [], "entities": []}, {"text": "Product-Producer(X,Y) X is a product of Y , or Y produces X, where X is any abstract or concrete object.", "labels": [], "entities": []}, {"text": "Origin-Entity(X,Y) X is the origin of Y where X can be spatial or material and Y is the entity derived from the origin.", "labels": [], "entities": []}, {"text": "Theme-Tool(X,Y) The tool Y is intended for X is either its result or something that is acted upon.", "labels": [], "entities": []}, {"text": "Part-Whole(X,Y) X is part of Y and this relation can be one of the following five types: PlaceArea, Stuff-Object, Portion-Mass, Member-Collection and Component-Integral object.", "labels": [], "entities": []}, {"text": "Content-Container(X,Y) A sentence S entails the fact that X is stored inside Y . Moreover, X is not a component of Y and can be removed from it.", "labels": [], "entities": []}, {"text": "We hypothesize that Cause-Effect and Part-Whole are the relation types which may require sentential information to be detected.", "labels": [], "entities": []}, {"text": "These two relations allow a greater variety of arguments and the semantic information alone might be not sufficient.", "labels": [], "entities": []}, {"text": "Such relation types as Product-Producer or InstrumentAgency are likely to benefit more from the external knowledge.", "labels": [], "entities": []}, {"text": "Our method depends on the positive and negative examples in the training set and on the semantic hierarchy we use.", "labels": [], "entities": []}, {"text": "If some parts of the hierarchy are more flat, the resulting patterns maybe too general.", "labels": [], "entities": []}, {"text": "As not all examples have been annotated with the information from WordNet, we removed them form the test data while conducting this experiment.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.978086531162262}]}, {"text": "Content-Container turned out to be the only relation type whose examples are fully annotated.", "labels": [], "entities": []}, {"text": "In contrast, Product-Producer is a relation type with the most information missing (9 examples removed).", "labels": [], "entities": []}, {"text": "There is no reason to treat relation mentions as mutually exclusive, therefore, only negative example provided fora particular relation type are used to determine semantic types of its arguments.", "labels": [], "entities": []}, {"text": "Discussion The entire generalization process results in a zero-error on the training set.", "labels": [], "entities": []}, {"text": "It does not, however, guarantee to hold given anew data set.", "labels": [], "entities": []}, {"text": "The loss in precision on the unseen exam- ples can be caused by the generalization pairs where both arguments are generalized to the higher level in the hierarchy than it ought to be.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993329644203186}]}, {"text": "To check how the algorithm behaves, we first evaluate the specialization step on the test data from the SemEval challenge.", "labels": [], "entities": [{"text": "SemEval challenge", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7364521622657776}]}, {"text": "Among all the relation types, only Instrument-Agency, Part-Whole and ContentContainer fail to obtain 100% precision after the specialization step.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9993464350700378}]}, {"text": "It means that, already at this stage, there are some false positives and the contextual classification is required to achieve better performance.", "labels": [], "entities": []}, {"text": "The results of the method introduced here are presented in.", "labels": [], "entities": []}, {"text": "Systems which participated in SemEval were categorized depending on the input information they have used.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9358841180801392}]}, {"text": "The category WordNet implies that WordNet was employed but it does not exclude a possibility of using other resources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9253840446472168}]}, {"text": "Therefore, to estimate how well our method performs, we calculated accuracy and compared it against a baseline that always returns the most frequent class label (B-A).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9995847344398499}]}, {"text": "Given the results of the teams participating in the challenge, the organizers mention Product-Producer as one of the easiest relations, while Origin-Entity and Theme-Tool are considered to be ones of the hardest to detect (.", "labels": [], "entities": []}, {"text": "Interestingly, Origin-Entity obtains the highest precision compared to the other relation types while using our approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9991264939308167}]}, {"text": "contains some examples of the semantic types we found for each relation.", "labels": [], "entities": []}, {"text": "Some of them are quite specific (e.g., Origin-Entity), while the other arguments maybe very general (e.g., CauseEffect).", "labels": [], "entities": []}, {"text": "The examples of the patterns for PartWhole can be divided in several subtypes, such as Member-Collection (person#1, social group#1), Place-Area (top side#1, whole#2) or Stuff-Object (germanium#1, mineral#1).", "labels": [], "entities": []}, {"text": "Instrument-(instrumentality#3, bad person#1) Agency (printing machine#1, employee#1) Cause-(cognitive operation#1, joy#1) Effect (entity#1, harm#2) (cognitive content#1, communication#2) Product-(knowledge#1, social unit#1) Producer (content#2, individual#1) (instrumentality#3, business organisation#1) Origin-(article#1, section#1) Entity (vegetation#1, plant part#1) (physical entity#1, fat#1) Theme-(abstract entity#1, implementation#2) Tool (animal#1, water#6) (nonaccomplishment#1, human action#1) Part-(top side#1, whole#2) Whole (germanium#1, mineral#1) (person#1, social group#1): Some examples per relation type.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the test data", "labels": [], "entities": []}]}