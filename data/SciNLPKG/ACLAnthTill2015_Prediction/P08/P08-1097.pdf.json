{"title": [], "abstractContent": [{"text": "This paper explores the relationship between discourse segmentation and coverbal gesture.", "labels": [], "entities": [{"text": "coverbal gesture", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.6408085525035858}]}, {"text": "Introducing the idea of gestural cohesion, we show that coherent topic segments are characterized by homogeneous gestural forms and that changes in the distribution of gestural features predict segment boundaries.", "labels": [], "entities": []}, {"text": "Gestu-ral features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model.", "labels": [], "entities": []}, {"text": "The resulting multimodal system outperforms text-only seg-mentation on both manual and automatically-recognized speech transcripts.", "labels": [], "entities": []}], "introductionContent": [{"text": "When people communicate face-to-face, discourse cues are expressed simultaneously through multiple channels.", "labels": [], "entities": []}, {"text": "Previous research has extensively studied how discourse cues correlate with lexico-syntactic and prosodic features; this work informs various text and speech processing applications, such as automatic summarization and segmentation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 201, "end_pos": 214, "type": "TASK", "confidence": 0.7516483068466187}]}, {"text": "Gesture is another communicative modality that frequently accompanies speech, yet it has not been exploited for computational discourse analysis.", "labels": [], "entities": [{"text": "computational discourse analysis", "start_pos": 112, "end_pos": 144, "type": "TASK", "confidence": 0.637168675661087}]}, {"text": "This paper empirically demonstrates that gesture correlates with discourse structure.", "labels": [], "entities": []}, {"text": "In particular, we show that automatically-extracted visual features can be combined with lexical cues in a statistical model to predict topic segmentation, a frequently studied form of discourse structure.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7583954632282257}]}, {"text": "Our method builds on the idea that coherent discourse segments are characterized by gestural cohesion; in other words, that such segments exhibit homogeneous gestural patterns.", "labels": [], "entities": []}, {"text": "Lexical cohesion forms the backbone of many verbal segmentation algorithms, on the theory that segmentation boundaries should be placed where the distribution of words changes.", "labels": [], "entities": [{"text": "verbal segmentation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7586060762405396}]}, {"text": "With gestural cohesion, we explore whether the same idea holds for gesture features.", "labels": [], "entities": []}, {"text": "The motivation for this approach comes from a series of psycholinguistic studies suggesting that gesture supplements speech with meaningful and unique semantic content).", "labels": [], "entities": []}, {"text": "We assume that repeated patterns in gesture are indicative of the semantic coherence that characterizes well-defined discourse segments.", "labels": [], "entities": []}, {"text": "An advantage of this view is that gestures can be brought to bear on discourse analysis without undertaking the daunting task of recognizing and interpreting individual gestures.", "labels": [], "entities": []}, {"text": "This is crucial because coverbal gesture -unlike formal sign language -rarely follows any predefined form or grammar, and may vary dramatically by speaker.", "labels": [], "entities": []}, {"text": "A key implementational challenge is automatically extracting gestural information from raw video and representing it in away that can applied to discourse analysis.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.7186338603496552}]}, {"text": "We employ a representation of visual codewords, which capture clusters of low-level motion patterns.", "labels": [], "entities": []}, {"text": "For example, one codeword may correspond to strong left-right motion in the upper part of the frame.", "labels": [], "entities": []}, {"text": "These codewords are then treated similarly to lexical items; our model identifies changes in their distribution, and predicts topic boundaries appropriately.", "labels": [], "entities": []}, {"text": "The overall framework is implemented as a hierarchical Bayesian model, supporting flexible integration of multiple knowledge sources.", "labels": [], "entities": []}, {"text": "Experimental results support the hypothesis that gestural cohesion is indicative of discourse structure.", "labels": [], "entities": []}, {"text": "Applying our algorithm to a dataset of faceto-face dialogues, we find that gesture communicates unique information, improving segmentation performance over lexical features alone.", "labels": [], "entities": []}, {"text": "The positive impact of gesture is most pronounced when automatically-recognized speech transcripts are used, but gestures improve performance by a significant margin even in combination with manual transcripts.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset Our dataset is composed of fifteen audiovideo recordings of dialogues limited to three minutes in duration.", "labels": [], "entities": []}, {"text": "The dataset includes nine different pairs of participants.", "labels": [], "entities": []}, {"text": "In each video one of five subjects is discussed.", "labels": [], "entities": []}, {"text": "The potential subjects include a \"Tom and Jerry\" cartoon, a \"Star Wars\" toy, and three mechanical devices: a latchbox, a piston, and a candy dispenser.", "labels": [], "entities": []}, {"text": "One participant -\"participant A\" -was familiarized with the topic, and is tasked with explaining it to participant B, who is permitted to ask questions.", "labels": [], "entities": []}, {"text": "Audio from both participants is used, but only video of participant A is used; we do not examine whether B's gestures are relevant to discourse segmentation.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.7208517789840698}]}, {"text": "Video was recorded using standard camcorders, with a resolution of 720 by 480 at 30 frames per second.", "labels": [], "entities": [{"text": "resolution", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.950825572013855}]}, {"text": "The video was reduced to 360 by 240 grayscale images before visual analysis is applied.", "labels": [], "entities": []}, {"text": "Audio was recorded using headset microphones.", "labels": [], "entities": []}, {"text": "No manual postprocessing is applied to the video.", "labels": [], "entities": []}, {"text": "Annotations and data processing All speech was transcribed by hand, and time stamps were obtained using the SPHINX-II speech recognition system for forced alignment.", "labels": [], "entities": [{"text": "SPHINX-II speech recognition", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.6417468984921774}, {"text": "forced alignment", "start_pos": 148, "end_pos": 164, "type": "TASK", "confidence": 0.6249174773693085}]}, {"text": "Sentence boundaries are annotated according to, and additional sentence boundaries are automatically inserted at all turn boundaries.", "labels": [], "entities": []}, {"text": "Commonlyoccurring terms unlikely to impact segmentation are automatically removed by using a stoplist.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.9773871302604675}]}, {"text": "For automatic speech recognition, the default Microsoft speech recognizer was applied to each sentence, and the top-ranked recognition result was reported.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6445652743180593}, {"text": "Microsoft speech recognizer", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.5318658252557119}]}, {"text": "As is sometimes the casein real-world applications, no speaker-specific training data is available.", "labels": [], "entities": []}, {"text": "The resulting recognition quality is very poor, yielding a word error rate of 77%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 59, "end_pos": 74, "type": "METRIC", "confidence": 0.6976675589879354}]}, {"text": "Annotators were instructed to select segment boundaries that divide the dialogue into coherent topics.", "labels": [], "entities": []}, {"text": "Segmentation points are required to coincide with sentence or turn boundaries.", "labels": [], "entities": []}, {"text": "A second annotator -who is not an author on any paper connected with this research -provided an additional set of segment annotations on six documents.", "labels": [], "entities": []}, {"text": "On this subset of documents, the P k between annotators was .306, and the WindowDiff was .325 (these metrics are explained in the next subsection).", "labels": [], "entities": []}, {"text": "This is similar to the interrater agreement reported by.", "labels": [], "entities": []}, {"text": "Over the fifteen dialogues, a total of 7458 words were transcribed (497 per dialogue), spread over 1440 sentences or interrupted turns (96 per dialogue).", "labels": [], "entities": []}, {"text": "There were a total of 102 segments (6.8 per dialogue), from a minimum of four to a maximum often.", "labels": [], "entities": []}, {"text": "This rate of fourteen sentences or interrupted turns per segment indicates relatively finegrained segmentation.", "labels": [], "entities": []}, {"text": "In the physics lecture corpus used by, there are roughly 100 sentences per segment.", "labels": [], "entities": []}, {"text": "On the ICSI corpus of meeting transcripts, report 7.5 segments per meeting, with 770 \"potential boundaries,\" suggesting a similar rate of roughly 100 sentences or interrupted turns per segment.", "labels": [], "entities": [{"text": "ICSI corpus of meeting transcripts", "start_pos": 7, "end_pos": 41, "type": "DATASET", "confidence": 0.957205307483673}]}, {"text": "The size of this multimodal dataset is orders of magnitude smaller than many other segmentation corpora.", "labels": [], "entities": []}, {"text": "For example, the Broadcast News corpus used by and others contains two million words.", "labels": [], "entities": [{"text": "Broadcast News corpus", "start_pos": 17, "end_pos": 38, "type": "DATASET", "confidence": 0.9454719424247742}]}, {"text": "The entire ICSI meeting corpus contains roughly 600,000 words, although only one third of this dataset was annotated for segmentation (.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.9338716864585876}, {"text": "segmentation", "start_pos": 121, "end_pos": 133, "type": "TASK", "confidence": 0.9687089920043945}]}, {"text": "The physics lecture corpus that was mentioned above contains 232,000 words ().", "labels": [], "entities": [{"text": "physics lecture corpus", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.6349790891011556}]}, {"text": "The task considered in this section is thus more difficult than much of the previous discourse segmentation work on two dimensions: there is less training data, and a finer-grained segmentation is required.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7169110774993896}]}, {"text": "Metrics All experiments are evaluated in terms of the commonly-used P k () and WindowDiff (WD)) scores.", "labels": [], "entities": []}, {"text": "These metrics are penalties, so lower values indicate better segmentations.", "labels": [], "entities": []}, {"text": "The P k metric expresses the probability that any randomly chosen pair of sentences is incorrectly segmented, if they are k sentences apart).", "labels": [], "entities": []}, {"text": "Following tradition, k is set to half of the mean seg-), applying a penalty whenever the number of segments within the k-sentence window differs for the reference and hypothesized segmentations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: For each method, the score of the best perform- ing configuration is shown. P k and WD are penalties, so  lower values indicate better performance.", "labels": [], "entities": [{"text": "WD", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9736216068267822}]}]}