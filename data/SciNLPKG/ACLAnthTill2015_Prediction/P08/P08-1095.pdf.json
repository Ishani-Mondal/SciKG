{"title": [{"text": "You talking to me? A Corpus and Algorithm for Conversation Disentanglement", "labels": [], "entities": [{"text": "Conversation Disentanglement", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.7419165074825287}]}], "abstractContent": [{"text": "When multiple conversations occur simultaneously , a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately.", "labels": [], "entities": []}, {"text": "We refer to this task as disentanglement.", "labels": [], "entities": [{"text": "disentanglement", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.9722327589988708}]}, {"text": "We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annota-tor reliability.", "labels": [], "entities": [{"text": "Internet Relay Chat (IRC) dialogue", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.6673274551119123}, {"text": "reliability", "start_pos": 150, "end_pos": 161, "type": "METRIC", "confidence": 0.7873318195343018}]}, {"text": "This is, to our knowledge, the first such corpus for internet chat.", "labels": [], "entities": []}, {"text": "We propose a graph-theoretic model for disentangle-ment, using discourse-based features which have not been previously applied to this task.", "labels": [], "entities": [{"text": "disentangle-ment", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.9609965682029724}]}, {"text": "The model's predicted disentanglements are highly correlated with manual annotations.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our dataset is recorded from the IRC (Internet Relay Chat) channel ##LINUX at freenode.net, using the freely-available gaim client.", "labels": [], "entities": [{"text": "LINUX", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.8272238373756409}]}, {"text": "##LINUX is an unofficial tech support line for the Linux operating system, selected because it is one of the most active chat rooms on freenode, leading to many simultaneous conversations, and because its content is typically inoffensive.", "labels": [], "entities": [{"text": "LINUX", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9374197125434875}]}, {"text": "Although it is notionally intended only for tech support, it includes large amounts of social chat as well, such as the conversation about factory work in the example above (figure 1).", "labels": [], "entities": []}, {"text": "The entire dataset contains 52:18 hours of chat, but we devote most of our attention to three annotated sections: development (706 utterances; 2:06 hr) and test (800 utts.; 1:39 hr) plus a short pilot section on which we tested our annotation system (359 utts.; 0:58 hr).", "labels": [], "entities": []}, {"text": "We annotate the 800 line test transcript using our system.", "labels": [], "entities": []}, {"text": "The annotation obtained has 63 conversations, with mean length 12.70.", "labels": [], "entities": []}, {"text": "The average density of conversations is 2.9, and the entropy is 3.79.", "labels": [], "entities": []}, {"text": "This places it within the bounds of our human annotations (see table 1), toward the more general end of the spectrum.", "labels": [], "entities": []}, {"text": "As a standard of comparison for our system, we provide results for several baselines-trivial systems which any useful annotation should outperform.", "labels": [], "entities": []}, {"text": "All different Each utterance is a separate conversation.", "labels": [], "entities": []}, {"text": "All same The whole transcript is a single conversation.", "labels": [], "entities": []}, {"text": "Blocks of k Each consecutive group of k utterances is a conversation.", "labels": [], "entities": []}, {"text": "Pause of k Each pause of k seconds or more separates two conversations.", "labels": [], "entities": [{"text": "Pause of k Each pause of k seconds", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.8611691445112228}]}, {"text": "Speaker Each speaker's utterances are treated as a monologue.", "labels": [], "entities": []}, {"text": "For each particular metric, we calculate the best baseline result among all of these.", "labels": [], "entities": []}, {"text": "To find the best block size or pause length, we search over multiples of 5 between 5 and 300.", "labels": [], "entities": []}, {"text": "This makes these baselines appear better than they really are, since their performance is optimized with respect to the test data.", "labels": [], "entities": []}, {"text": "Our results, in table 3, are encouraging.", "labels": [], "entities": []}, {"text": "On average, annotators agree more with each other than with any artificial annotation, and more with our model than with the baselines.", "labels": [], "entities": []}, {"text": "For the 1-to-1 accuracy metric, we cannot claim much beyond these general results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9881106615066528}]}, {"text": "The range of human variation is quite wide, and there are annotators who are closer to baselines than to any other human annotator.", "labels": [], "entities": []}, {"text": "As explained earlier, this is because some human annotations are much more specific than others.", "labels": [], "entities": []}, {"text": "For very specific annotations, the best baselines are short blocks or pauses.", "labels": [], "entities": []}, {"text": "For the most general, marking all utterances the same does very well (although for all other annotations, it is extremely poor).: Metric values between proposed annotations and human annotations.", "labels": [], "entities": []}, {"text": "Model scores typically fall between inter-annotator agreement and baseline performance.", "labels": [], "entities": []}, {"text": "For the local metric, the results are much clearer.", "labels": [], "entities": []}, {"text": "There is no overlap in the ranges; for every test annotation, agreement is highest with other annotator, then our model and finally the baselines.", "labels": [], "entities": [{"text": "agreement", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9985259175300598}]}, {"text": "The most competitive baseline is one conversation per speaker, which makes sense, since if a speaker makes two comments in a four-utterance window, they are very likely to be related.", "labels": [], "entities": []}, {"text": "The name mention features are critical for our model's performance.", "labels": [], "entities": []}, {"text": "Without this feature, the classifier's development F-score drops from 71 to 56.", "labels": [], "entities": [{"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.8423924446105957}]}, {"text": "The disentanglement system's test performance decreases proportionally; mean 1-to-1 falls to 36.08, and mean loc 3 to 63.00, essentially baseline performance.", "labels": [], "entities": [{"text": "mean 1-to-1", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9544812142848969}]}, {"text": "On the other hand, mentions are not sufficient; with only name mention and time gap features, mean 1-to-1 is 38.54 and loc 3 is 67.14.", "labels": [], "entities": []}, {"text": "For some utterances, of course, name mentions provide the only reasonable clue to the correct decision, which is why humans mention names in the first place.", "labels": [], "entities": []}, {"text": "But our system is probably overly dependent on them, since they are very reliable compared to our other features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on 6 annotations of 800 lines of chat  transcript. Inter-annotator agreement metrics (below the  line) are calculated between distinct pairs of annotations.", "labels": [], "entities": []}, {"text": " Table 3: Metric values between proposed annotations and human annotations. Model scores typically fall between  inter-annotator agreement and baseline performance.", "labels": [], "entities": []}]}