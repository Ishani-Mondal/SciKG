{"title": [{"text": "Efficient Processing of Underspecified Discourse Representations", "labels": [], "entities": [{"text": "Efficient Processing of Underspecified Discourse Representations", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.7201635738213857}]}], "abstractContent": [{"text": "Underspecification-based algorithms for processing partially disambiguated discourse structure must cope with extremely high numbers of readings.", "labels": [], "entities": []}, {"text": "Based on previous work on dominance graphs and weighted tree grammars , we provide the first possibility for computing an underspecified discourse description and a best discourse representation efficiently enough to process even the longest discourses in the RST Discourse Treebank.", "labels": [], "entities": [{"text": "RST Discourse Treebank", "start_pos": 260, "end_pos": 282, "type": "DATASET", "confidence": 0.8147531549135844}]}], "introductionContent": [{"text": "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation ( outline this and further applications).", "labels": [], "entities": [{"text": "Discourse processing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8299179673194885}, {"text": "information extraction", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.8856918811798096}, {"text": "automatic summarisation", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.5944105684757233}]}, {"text": "But discourse structures cannot always be described completely, either due to genuine ambiguity) or to the limitations of a discourse parser.", "labels": [], "entities": []}, {"text": "In either case, only partial information on discourse structure is available.", "labels": [], "entities": []}, {"text": "To handle such information, underspecification formalisms can be used.", "labels": [], "entities": []}, {"text": "Underspecification was originally introduced in computational semantics to model structural ambiguity without disjunctively enumerating the readings, and later applied to discourse parsing ().", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7208328992128372}]}, {"text": "However, while the existing algorithms for underspecification processing work well for semantic structures, they were not designed for discourse structures, which can be much larger.", "labels": [], "entities": []}, {"text": "Indeed, it has never been shown that underspecified discourse reprentations (UDRs) can be processed efficiently, since the general-purpose implementations are too slow for that task.", "labels": [], "entities": []}, {"text": "In this paper, we present anew way to implement and process discourse underspecification in terms of regular tree grammars (RTGs).", "labels": [], "entities": []}, {"text": "RTGs are used as an underspecification formalism in semantics (.", "labels": [], "entities": []}, {"text": "We show how to compute RTGs for discourse from dominance-based underspecified representations more efficiently (by atypical factor of 100) than before.", "labels": [], "entities": [{"text": "RTGs", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9205010533332825}]}, {"text": "Furthermore, we show how weighted RTGs can be used to represent constraints and preferences on the discourse structure.", "labels": [], "entities": []}, {"text": "Taking all these results together, we show for the first time how the globally optimal discourse representation based on some preference model can be computed efficiently from an UDR.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our runtimes with those of Utool (, the fasted known solver for general dominance graphs; it implements the Koller & Thater algorithm.", "labels": [], "entities": []}, {"text": "Utool runs very fast for underspecified representations in semantics, but the representations for discourse parsing are considerably larger: The largest underspecified semantic representation found in the Rondane treebank analysed with the English Resource Grammar, ERG) has 4.5 \u00d7 10 12 structural scope readings, but for 59% of the discourses in the RST Discourse Treebank (, RST-DT), there are more ways of configuring all EDUs into a binary tree than that.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7363035380840302}, {"text": "Rondane treebank analysed with the English Resource Grammar, ERG", "start_pos": 205, "end_pos": 269, "type": "DATASET", "confidence": 0.8011736780405044}, {"text": "RST Discourse Treebank", "start_pos": 351, "end_pos": 373, "type": "DATASET", "confidence": 0.7502850492795309}]}, {"text": "We evaluate the efficiency of our algorithm on 364 texts from the RST-DT, by converting each discourse into a chain with one lower fragment for each EDU and one upper fragment labelled with each annotated discourse relation.", "labels": [], "entities": []}, {"text": "We use our algorithm and Utool to generate the RTG from the chain, assign all soft constraints of to the grammar, and finally compute the best configuration according to this model.", "labels": [], "entities": []}, {"text": "The evaluation results are shown in.", "labels": [], "entities": []}, {"text": "The horizontal axis shows the chain length (= number of EDUs minus 1), rounded down to multiples often; the (logarithmic) vertical axis shows the average runtime in milliseconds for discourses of that length.", "labels": [], "entities": []}, {"text": "Both algorithms spend a bit over half the runtime on computing the RTGs.", "labels": [], "entities": []}, {"text": "As the diagram shows, our algorithm is up to 100 times faster than Utool for the same discourses.", "labels": [], "entities": []}, {"text": "It is capable of computing the best configuration for every tested discourse -in less than one second for 86% of the texts.", "labels": [], "entities": []}, {"text": "Utool exceeded the OS memory limit on 77 discourses, and generally couldn't process any text with more than 100 EDUs.", "labels": [], "entities": []}, {"text": "The longest text in the RST-DT has 304 EDUs, so the UDR has about 2.8 \u00d7 10 178 different configurations.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.7373548746109009}, {"text": "UDR", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.7973122000694275}]}, {"text": "Our algorithm computes the best configuration for this UDR in about three minutes.", "labels": [], "entities": []}], "tableCaptions": []}