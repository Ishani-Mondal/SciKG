{"title": [{"text": "Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz data: Bootstrapping and Evaluation", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 92, "end_pos": 102, "type": "TASK", "confidence": 0.6602880358695984}]}], "abstractContent": [{"text": "We address two problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and evaluating the result with real users.", "labels": [], "entities": []}, {"text": "We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is \"bootstrapped\" from small amounts of Wizard-of-Oz (WOZ) data.", "labels": [], "entities": []}, {"text": "This use of WOZ data allows development of optimal strategies for domains where no working prototype is available.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.7196571528911591}]}, {"text": "We compare the RL-based strategy against a supervised strategy which mimics the wizards' policies.", "labels": [], "entities": []}, {"text": "This comparison allows us to measure relative improvement over the training data.", "labels": [], "entities": []}, {"text": "Our results show that RL significantly outperforms Supervised Learning when interacting in simulation as well as for interactions with real users.", "labels": [], "entities": [{"text": "RL", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.8373863101005554}]}, {"text": "The RL-based policy gains on average 50-times more reward when tested in simulation, and almost 18-times more reward when interacting with real users.", "labels": [], "entities": []}, {"text": "Users also subjectively rate the RL-based policy on average 10% higher.", "labels": [], "entities": [{"text": "RL-based", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.7482677698135376}]}], "introductionContent": [{"text": "Designing a spoken dialogue system is a timeconsuming and challenging task.", "labels": [], "entities": []}, {"text": "A developer may spend a lot of time and effort anticipating the potential needs of a specific application environment and then deciding on the most appropriate system action (e.g. confirm, present items,.", "labels": [], "entities": []}, {"text": "). One of the key advantages of statistical optimisation methods, such as Reinforcement Learning (RL), for dialogue strategy design is that the problem can be formulated as a principled mathematical model which can be automatically trained on real data (.", "labels": [], "entities": [{"text": "dialogue strategy design", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.8377892176310221}]}, {"text": "In cases where a system is designed from scratch, however, there is often no suitable in-domain data.", "labels": [], "entities": []}, {"text": "Collecting dialogue data without a working prototype is problematic, leaving the developer with a classic chicken-and-egg problem.", "labels": [], "entities": []}, {"text": "We propose to learn dialogue strategies by simulation-based RL, where the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data.", "labels": [], "entities": [{"text": "RL", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.8111360669136047}]}, {"text": "Using WOZ data rather than data from real HumanComputer Interaction (HCI) allows us to learn optimal strategies for domains where no working dialogue system already exists.", "labels": [], "entities": []}, {"text": "To date, automatic strategy learning has been applied to dialogue systems which have already been deployed using handcrafted strategies.", "labels": [], "entities": [{"text": "strategy learning", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7830341160297394}]}, {"text": "In such work, strategy learning was performed based on already present extensive online operation experience, e.g. ().", "labels": [], "entities": [{"text": "strategy learning", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.9317448437213898}]}, {"text": "In contrast to this preceding work, our approach enables strategy learning in domains where no prior system is available.", "labels": [], "entities": [{"text": "strategy learning", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.920827180147171}]}, {"text": "Optimised learned strategies are then available from the first moment of online-operation, and tedious handcrafting of dialogue strategies is omitted.", "labels": [], "entities": []}, {"text": "This independence from large amounts of in-domain dialogue data allows researchers to apply RL to new application areas beyond the scope of existing dialogue systems.", "labels": [], "entities": [{"text": "RL", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9604304432868958}]}, {"text": "We call this method 'bootstrapping'.", "labels": [], "entities": []}, {"text": "Ina WOZ experiment, a hidden human operator, the so called \"wizard\", simulates (partly or com-pletely) the behaviour of the application, while subjects are left in the belief that they are interacting with areal system.", "labels": [], "entities": []}, {"text": "That is, WOZ experiments only simulate HCI.", "labels": [], "entities": []}, {"text": "We therefore need to show that a strategy bootstrapped from WOZ data indeed transfers to real HCI.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8447948396205902}]}, {"text": "Furthermore, we also need to introduce methods to learn useful user simulations (for training RL) from such limited data.", "labels": [], "entities": []}, {"text": "The use of WOZ data has earlier been proposed in the context of RL.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7279382348060608}, {"text": "RL", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.962312638759613}]}, {"text": "( utilise WOZ data to discover the state and action space for MDP design.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.6891159415245056}, {"text": "MDP design", "start_pos": 62, "end_pos": 72, "type": "TASK", "confidence": 0.9191552400588989}]}, {"text": "() use WOZ data to build a simulated user and noise model for simulation-based RL.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.7312987148761749}, {"text": "RL", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.8893749713897705}]}, {"text": "While both studies show promising first results, their simulated environment still contains many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 222, "end_pos": 230, "type": "DATASET", "confidence": 0.811847448348999}]}, {"text": "( propose to 'bootstrap' with a simulated user which is entirely hand-crafted.", "labels": [], "entities": []}, {"text": "In the following we propose an entirely data-driven approach, where all components of the simulated learning environment are learned from WOZ data.", "labels": [], "entities": [{"text": "WOZ data", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.8524971902370453}]}, {"text": "We also show that the resulting policy performs well for real users.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the user tests the RL policy is ported to a working ISU-based dialogue system via table look-up, which indicates the action with the highest expected reward for each state (cf.).", "labels": [], "entities": [{"text": "RL", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9302157163619995}]}, {"text": "The supervised baseline is implemented using standard threshold-based update rules.", "labels": [], "entities": []}, {"text": "The experimental conditions are similar to the WOZ study, i.e. we ask the users to solve similar tasks, and use similar questionnaires.", "labels": [], "entities": [{"text": "WOZ study", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.8133669495582581}]}, {"text": "Furthermore, we decided to use typed user input rather than ASR.", "labels": [], "entities": []}, {"text": "Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strategies; *** denotes significant difference between SL and RL at p < .001 allows us to target the experiments to the dialogue management decisions, and block ASR quality from interfering with the experimental results).", "labels": [], "entities": [{"text": "REAL", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9284390211105347}, {"text": "ASR", "start_pos": 259, "end_pos": 262, "type": "TASK", "confidence": 0.8034849762916565}]}, {"text": "17 subjects (8 female, 9 male) are given a set of 6\u00d72 predefined tasks, which they solve by interaction with the RL-based and the SLbased system in controlled order.", "labels": [], "entities": []}, {"text": "As a secondary task users are asked to count certain objects in a driving simulation.", "labels": [], "entities": []}, {"text": "In total, 204 dialogues with 1,115 turns are gathered in this setup.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Improved user ratings over the WOZ study  where *** denotes p < .001", "labels": [], "entities": [{"text": "Improved", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9099312424659729}, {"text": "WOZ study", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.9114072620868683}]}]}