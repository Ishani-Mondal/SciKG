{"title": [{"text": "Multi-Task Active Learning for Linguistic Annotations", "labels": [], "entities": []}], "abstractContent": [{"text": "We extend the classical single-task active learning (AL) approach.", "labels": [], "entities": [{"text": "single-task active learning (AL)", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6860737452904383}]}, {"text": "In the multi-task active learning (MTAL) paradigm, we select examples for several annotation tasks rather than fora single one as usually done in the context of AL.", "labels": [], "entities": [{"text": "multi-task active learning (MTAL) paradigm", "start_pos": 7, "end_pos": 49, "type": "TASK", "confidence": 0.7026629660810743}]}, {"text": "We introduce two MTAL meta-protocols, alternating selection and rank combination , and propose a method to implement them in practice.", "labels": [], "entities": []}, {"text": "We experiment with a two-task annotation scenario that includes named entity and syntactic parse tree annotations on three different corpora.", "labels": [], "entities": []}, {"text": "MTAL outperforms random selection and a stronger baseline, one-sided example selection, in which one task is pursued using AL and the selected examples are provided also to the other task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised machine learning methods have successfully been applied to many NLP tasks in the last few decades.", "labels": [], "entities": []}, {"text": "These techniques have demonstrated their superiority over both hand-crafted rules and unsupervised learning approaches.", "labels": [], "entities": []}, {"text": "However, they require large amounts of labeled training data for every level of linguistic processing (e.g., POS tags, parse trees, or named entities).", "labels": [], "entities": []}, {"text": "When, when domains and text genres change (e.g., moving from commonsense newspapers to scientific biology journal articles), extensive retraining on newly supplied training material is often required, since different domains may use different syntactic structures as well as different semantic classes (entities and relations).", "labels": [], "entities": []}, {"text": "* Both authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "Consequently, with an increasing coverage of a wide variety of domains inhuman language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data.", "labels": [], "entities": []}, {"text": "Creating annotated data is extremely laborintensive.", "labels": [], "entities": []}, {"text": "The Active Learning (AL) paradigm) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized.", "labels": [], "entities": []}, {"text": "AL has been successfully applied already fora wide range of NLP tasks, including POS tagging, chunking), statistical parsing, and named entity recognition.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.8318111002445221}, {"text": "statistical parsing", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8661440312862396}, {"text": "named entity recognition", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.6953015824158987}]}, {"text": "However, AL is designed in such away that it selects examples for manual annotation with respect to a single learning algorithm or classifier.", "labels": [], "entities": []}, {"text": "Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained.", "labels": [], "entities": []}, {"text": "In the following, we will refer to the annotations supplied fora classifier as the annotations fora single annotation task.", "labels": [], "entities": []}, {"text": "Modern HLT systems often utilize annotations resulting from different tasks.", "labels": [], "entities": []}, {"text": "For example, a machine translation system might use features extracted from parse trees and named entity annotations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7476151585578918}]}, {"text": "For such an application, we obviously need the different annotations to reside in the same text corpus.", "labels": [], "entities": []}, {"text": "It is not clear how to apply the single-task AL approach here, since a training example that is beneficial for one task might not be so for others.", "labels": [], "entities": []}, {"text": "We could annotate the same corpus independently by the two tasks and merge the resulting annotations, but that (as we show in this paper) would possibly yield sub-optimal usage of human annotation efforts.", "labels": [], "entities": []}, {"text": "There are two reasons why multi-task AL, and by this, a combined corpus annotated for various tasks, could be of immediate benefit.", "labels": [], "entities": [{"text": "multi-task AL", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.7571691572666168}]}, {"text": "First, annotators working on similar annotation tasks (e.g., considering named entities and relations between them), might exploit annotation data from one subtask for the benefit of the other.", "labels": [], "entities": []}, {"text": "If for each subtask a separate corpus is sampled by means of AL, annotators will definitely lack synergy effects and, therefore, annotation will be more laborious and is likely to suffer in terms of quality and accuracy.", "labels": [], "entities": [{"text": "AL", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.95908522605896}, {"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9936226010322571}]}, {"text": "Second, for dissimilar annotation tasks -take, e.g., a comprehensive HLT pipeline incorporating morphological, syntactic and semantic data -a classifier might require features as input which constitute the output of another preceding classifier.", "labels": [], "entities": []}, {"text": "As a consequence, training such a classifier which takes into account several annotation tasks will best be performed on a rich corpus annotated with respect to all inputrelevant tasks.", "labels": [], "entities": []}, {"text": "Both kinds of annotation tasks, similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems.", "labels": [], "entities": []}, {"text": "Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other.", "labels": [], "entities": []}, {"text": "In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (, sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank, while verbs and verb arguments are annotated with Propbank rolesets).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.902852475643158}, {"text": "Penn Treebank", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.8232987523078918}, {"text": "Penn Discourse Treebank", "start_pos": 187, "end_pos": 210, "type": "DATASET", "confidence": 0.9542930126190186}]}, {"text": "In the biomedical GENIA corpus (), scientific text is annotated with POS tags, parse trees, and named entities.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.6737577170133591}]}, {"text": "In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks.", "labels": [], "entities": [{"text": "multi-task active learning (MTAL)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.6423527598381042}]}, {"text": "We propose anew AL framework where the examples to be annotated are selected so that they are as informative as possible fora set of classifiers instead of a single classifier only.", "labels": [], "entities": []}, {"text": "This enables the creation of a single combined corpus annotated with respect to various annotation tasks, while preserving the advantages of AL with respect to the minimization of annotation efforts.", "labels": [], "entities": [{"text": "AL", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.8729017972946167}]}, {"text": "Ina proof-of-concept scenario, we focus on two highly dissimilar tasks, syntactic parsing and named entity recognition, study the effects of multi-task AL under rather extreme conditions.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7460951209068298}, {"text": "named entity recognition", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.660148541132609}, {"text": "multi-task AL", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.6721759736537933}]}, {"text": "We propose two MTAL meta-protocols and a method to implement them for these tasks.", "labels": [], "entities": []}, {"text": "We run experiments on three corpora for domains and genres that are very different (WSJ: newspapers, Brown: mixed genres, and GENIA: biomedical abstracts).", "labels": [], "entities": [{"text": "WSJ: newspapers", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.8406166831652323}]}, {"text": "Our protocols outperform two baselines (random and a stronger onesided selection baseline).", "labels": [], "entities": []}, {"text": "In Section 2 we introduce our MTAL framework and present two MTAL protocols.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss the evaluation of these protocols.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental setup, and results are presented in Section 5.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we point to open research issues for this new approach in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The notion of training quality (TQ) can be used to quantify the effectiveness of a protocol, and by this, annotation costs in a single-task AL scenario.", "labels": [], "entities": []}, {"text": "To actually quantify the overall training quality in a multiple annotation scenario one would have to sum overall the single task's TQs.", "labels": [], "entities": []}, {"text": "Of course, depending on the specific annotation task, one would not want to quantify the number of examples being annotated but different task-specific units of annotation.", "labels": [], "entities": []}, {"text": "While for entity annotations one does typically count the number of tokens being annotated, in the parsing scenario the number of constituents being annotated is a generally accepted measure.", "labels": [], "entities": []}, {"text": "As, however, the actual time needed for the annotation of one example usually differs for different annotation tasks, normalizing exchange rates have to be specified which can then be used as weighting factors.", "labels": [], "entities": []}, {"text": "In this paper, we do not define such weighting factors 4 , and leave this challenging question to be discussed in the context of psycholinguistic research.", "labels": [], "entities": []}, {"text": "We could quantify the overall efficiency score E of a MTAL protocol P by where u j denotes the individual annotation task's 4 Such weighting factors not only depend on the annotation level or task but also on the domain, and especially on the cognitive load of the annotation task.", "labels": [], "entities": []}, {"text": "number of units being annotated (e.g., constituents for parsing) and the task-specific weights are defined by \u03b1 j . Given weights are properly defined, such a score can be applied to directly compare different protocols and quantify their differences.", "labels": [], "entities": []}, {"text": "In practice, such task-specific weights might also be considered in the MTAL protocols.", "labels": [], "entities": []}, {"text": "In the alternating selection protocol, the numbers of consecutive iterations s i each single task protocol can be tuned according to the \u03b1 parameters.", "labels": [], "entities": []}, {"text": "As for the rank combination protocol, the weights can be considered when calculating the overall rank: r(e) = n j=1 \u03b2 j \u00b7 r X j (e) where the parameters \u03b2 1 . .", "labels": [], "entities": []}, {"text": "\u03b2 n reflect the values of \u03b1 1 . .", "labels": [], "entities": []}, {"text": "\u03b1 n (though they need not necessarily be the same).", "labels": [], "entities": []}, {"text": "In our experiments, we assumed the same weight for all annotation schemata, thus simply setting s i = 1, \u03b2 i = 1.", "labels": [], "entities": []}, {"text": "This was done for the sake of a clear framework presentation.", "labels": [], "entities": []}, {"text": "Finding proper weights for the single tasks and tuning the protocols accordingly is a subject for further research.", "labels": [], "entities": []}, {"text": "For the NE task we employed the classifier described by: The NE tagger is based on Conditional Random Fields () We randomly sampled L = 3 4 of the training data to create each committee member. and has a rich feature set including orthographical, lexical, morphological, POS, and contextual features.", "labels": [], "entities": [{"text": "NE task", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9064198732376099}]}, {"text": "For parsing, Dan Bikel's reimplementation of Collins' parser is employed, using gold POS tags.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9851678609848022}]}, {"text": "In each AL iteration we select 100 sentences for manual annotation.", "labels": [], "entities": []}, {"text": "We start with a randomly chosen seed set of 200 sentences.", "labels": [], "entities": []}, {"text": "Within a corpus we used the same seed set in all selection scenarios.", "labels": [], "entities": []}, {"text": "We compare the following five selection scenarios: Random selection (RS), which serves as our baseline; one-sided AL selection for both tasks (called NE-AL and PARSE-AL); and multi-task AL selection with the alternating selection protocol (alter-MTAL) and the rank combination protocol (ranks-MTAL).", "labels": [], "entities": []}, {"text": "We performed our experiments on three different corpora, namely one from the newspaper genre (WSJ), a mixed-genre corpus (Brown), and a biomedical corpus (Bio).", "labels": [], "entities": []}, {"text": "Our simulation corpora contain both entity annotations and (constituent) parse annotations.", "labels": [], "entities": []}, {"text": "For each corpus we have a pool set (from which we select the examples for annotation) and an evaluation set (used for generating the learning curves).", "labels": [], "entities": []}, {"text": "The WSJ corpus is based on the WSJ part of the PENN TREEBANK (; we used the first 10,000 sentences of section 2-21 as the pool set, and section 00 as evaluation set (1,921 sentences).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9661085307598114}, {"text": "WSJ part of the PENN TREEBANK", "start_pos": 31, "end_pos": 60, "type": "DATASET", "confidence": 0.7683937549591064}]}, {"text": "The Brown corpus is also based on the respective part of the PENN TREEBANK.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9747143089771271}, {"text": "PENN", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8532500267028809}, {"text": "TREEBANK", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.433819442987442}]}, {"text": "We created a sample consisting of 8 of any 10 consecutive sentences in the corpus.", "labels": [], "entities": []}, {"text": "This was done as Brown contains text from various English text genres, and we did that to create a representative sample of the corpus domains.", "labels": [], "entities": []}, {"text": "We finally selected the first 10,000 sentences from this sample as pool set.", "labels": [], "entities": []}, {"text": "Every 9th from every 10 consecutive sentences package went into the evaluation set which consists of 2,424 sentences.", "labels": [], "entities": []}, {"text": "For both WSJ and Brown only parse annotations though no entity annotations were available.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9272646307945251}, {"text": "Brown", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.5119446516036987}]}, {"text": "Thus, we enriched both corpora with entity annotations (three entities: person, location, and organization) by means of a tagger trained on the English data set of the CoNLL-2003 shared task.", "labels": [], "entities": [{"text": "English data set of the CoNLL-2003 shared task", "start_pos": 144, "end_pos": 190, "type": "DATASET", "confidence": 0.7851666733622551}]}, {"text": "The Bio corpus is based on the parsed section of the GENIA corpus ().", "labels": [], "entities": [{"text": "Bio corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7678306102752686}, {"text": "GENIA corpus", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.981654554605484}]}, {"text": "We performed the same divisions as for Brown, resulting in 2,213 sentences in our pool set and 276 sentences for the evaluation set.", "labels": [], "entities": []}, {"text": "This part of the GENIA corpus comes with entity annotations.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9528959393501282}]}, {"text": "We have collapsed the entity classes annotated in GENIA (cell line, cell type, DNA, RNA, protein) into a single, biological entity class.", "labels": [], "entities": [{"text": "GENIA", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9143812656402588}]}], "tableCaptions": []}