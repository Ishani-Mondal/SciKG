{"title": [], "abstractContent": [{"text": "Research on coreference resolution and sum-marization has modeled the way entities are realized as concrete phrases in discourse.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9729844033718109}]}, {"text": "In particular there exist models of the noun phrase syntax used for discourse-new versus discourse-old referents, and models describing the likely distance between a pronoun and its antecedent.", "labels": [], "entities": []}, {"text": "However, models of discourse coherence, as applied to information ordering tasks, have ignored these kinds of information.", "labels": [], "entities": [{"text": "information ordering tasks", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.7847253481547037}]}, {"text": "We apply a discourse-new classifier and pronoun coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence.", "labels": [], "entities": [{"text": "information ordering task", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.7939066290855408}]}], "introductionContent": [{"text": "Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand.", "labels": [], "entities": []}, {"text": "Models of coherence have been used to impose an order on sentences for multidocument summarization (), to evaluate the quality of human-authored essays (), and to insert new information into existing documents).", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.7416722476482391}]}, {"text": "These models typically view a sentence either as a bag of words () or as a bag of entities associated with various syntactic roles (Lapata and ).", "labels": [], "entities": []}, {"text": "However, a mention of an entity contains more information than just its head and syntactic role.", "labels": [], "entities": []}, {"text": "The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from non-salient.", "labels": [], "entities": []}, {"text": "These patterns have been studied extensively, by linguists and in the field of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.9723548591136932}]}, {"text": "We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling.", "labels": [], "entities": []}, {"text": "Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on.", "labels": [], "entities": []}, {"text": "Discourse-new NPs are those whose referents have not been previously mentioned in the discourse.", "labels": [], "entities": []}, {"text": "As noted by studies since, there are marked syntactic differences between the two classes.", "labels": [], "entities": []}, {"text": "Our second model describes pronoun coreference.", "labels": [], "entities": [{"text": "pronoun coreference", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7783687114715576}]}, {"text": "To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender.", "labels": [], "entities": []}, {"text": "Centering theory () describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center.", "labels": [], "entities": []}, {"text": "We use a model which probabilistically attempts to describe these preferences.", "labels": [], "entities": []}, {"text": "These two models can be combined with the entity grid described by  for significant improvement.", "labels": [], "entities": []}, {"text": "The magnitude of the improvement is particularly interesting given that Barzilay and  douse a coreference system but are unable to derive much advantage from it.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see).", "labels": [], "entities": []}, {"text": "In the discrimination task ( ), a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent 4 . Discrimination becomes easier for longer documents, since a random permutation is likely to be much less similar to the original.", "labels": [], "entities": []}, {"text": "Therefore we also test our systems on the task of insertion (, in which we remove a sentence from a document, then find the point of insertion which yields the highest coherence score.", "labels": [], "entities": []}, {"text": "The reported score is the average fraction of sentences per document reinserted in their original position (averaged over documents, not sentences, so that longer documents do not disproportionally influence the results) . We test on sections 14-24 of the Penn Treebank (1004 documents total).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 256, "end_pos": 269, "type": "DATASET", "confidence": 0.9244424402713776}]}, {"text": "Previous work has focused on the AIRPLANE corpus (), which contains short announcements of airplane crashes written by and for domain experts.", "labels": [], "entities": [{"text": "AIRPLANE corpus", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.832959771156311}]}, {"text": "These texts use a very constrained style, with few discourse-new markers or pronouns, and so our system is ineffective; the WSJ corpus is much more typical of normal informative writing.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 124, "end_pos": 134, "type": "DATASET", "confidence": 0.9353820383548737}]}, {"text": "Also unlike previous work, we do not test the task of completely reconstructing a document's order, since this is computationally intractable and results on WSJ documents 6 would likely be dominated by search errors.", "labels": [], "entities": [{"text": "WSJ documents 6", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.85247802734375}]}, {"text": "Our results are shown in table 5.", "labels": [], "entities": []}, {"text": "When run alone, the entity grid outperforms either of our models.", "labels": [], "entities": []}, {"text": "However, all three models are significantly better than random.", "labels": [], "entities": []}, {"text": "Combining all three models raises discrimination performance by 3.5% over the baseline and insertion by 3.4%.", "labels": [], "entities": [{"text": "insertion", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9974410533905029}]}, {"text": "Even the weakest component, pronouns, contributes to the joint model; when it is left out, the resulting EGrid + Disc-New model is significantly worse than the full combination.", "labels": [], "entities": []}, {"text": "We test significance using Wilcoxon's signed-rank test; all results are significant with p < .001.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on 1004 WSJ documents.", "labels": [], "entities": [{"text": "WSJ documents", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.8995426893234253}]}]}