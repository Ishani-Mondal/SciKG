{"title": [{"text": "Mapping between Compositional Semantic Representations and Lexical Semantic Resources: Towards Accurate Deep Semantic Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces a machine learning method based on bayesian networks which is applied to the mapping between deep semantic representations and lexical semantic resources.", "labels": [], "entities": []}, {"text": "A probabilistic model comprising Minimal Recursion Semantics (MRS) structures and lexicalist oriented semantic features is acquired.", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS) structures", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.6859364254134042}]}, {"text": "Lexical semantic roles enriching the MRS structures are inferred, which are useful to improve the accuracy of deep semantic parsing.", "labels": [], "entities": [{"text": "MRS", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9447771310806274}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.997721254825592}, {"text": "deep semantic parsing", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.600969264904658}]}, {"text": "Verb classes inference was also investigated, which, together with lexical semantic information provided by VerbNet and PropBank resources, can be substantially beneficial to the parse disambiguation task.", "labels": [], "entities": [{"text": "Verb classes inference", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6780731876691183}, {"text": "VerbNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9299166798591614}, {"text": "parse disambiguation task", "start_pos": 179, "end_pos": 204, "type": "TASK", "confidence": 0.9419556856155396}]}], "introductionContent": [{"text": "Recent studies of natural language parsing have shown a clear and steady shift of focus from pure syntactic analyses to more semantically informed structures.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.6471491456031799}]}, {"text": "As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8939955830574036}]}, {"text": "Some approaches have even tried to acquire semantic representations without full syntactic analyses.", "labels": [], "entities": []}, {"text": "The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences).", "labels": [], "entities": []}, {"text": "Manually annotated lexical semantic resources like PropBank (), VerbNet), or FrameNet () are usually used as gold standards for training and evaluation of such systems.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.911069929599762}]}, {"text": "In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs.", "labels": [], "entities": []}, {"text": "The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.).", "labels": [], "entities": []}, {"text": "To this effect, various semantic representations have been proposed and used in different parsing systems.", "labels": [], "entities": []}, {"text": "Generally speaking, such semantic representations should be capable of embedding shallow semantic information (i.e., predicate-argument or semantic roles).", "labels": [], "entities": []}, {"text": "However, it is non-trivial to map even the basic predicatearguments between different representations.", "labels": [], "entities": []}, {"text": "This becomes a barrier to both sides, making the crossfertilization of systems and resources using different semantic representations very difficult.", "labels": [], "entities": []}, {"text": "In this paper, we present a machine learning approach towards mapping between deep and shallow semantic representations.", "labels": [], "entities": []}, {"text": "More specifically, we use Bayesian networks to acquire a statistical model that enriches the Minimal Recursion Semantics structures produced by the English Resource Grammar (ERG)) with VerbNet-like semantic roles.", "labels": [], "entities": []}, {"text": "Evaluation results show that the mapping from MRS to semantic roles is reliable and beneficial to deep parsing.", "labels": [], "entities": [{"text": "MRS", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8665339946746826}, {"text": "deep parsing", "start_pos": 98, "end_pos": 110, "type": "TASK", "confidence": 0.7088785767555237}]}], "datasetContent": [{"text": "The experiment uses 10370 sentences from the PropBank corpus which have a mapping to VerbNet () and are successfully parsed by the ERG.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9613874554634094}, {"text": "ERG", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.9355737566947937}]}, {"text": "Up to 10 best parses are recorded for each sentence.", "labels": [], "entities": []}, {"text": "The total number of instances, considering that each sentence contains zero or more verbs, is 13589.", "labels": [], "entities": []}, {"text": "The algorithm described in section 3.1 managed to find at least one mapping for 10960 of these instances (1020 different verb lexemes).", "labels": [], "entities": []}, {"text": "If the number of parsing results is increased to 25 the results are improved (1460 different verb lexemes were found).", "labels": [], "entities": []}, {"text": "In the second experiment, the sentences without VerbNet mappings were also included.", "labels": [], "entities": []}, {"text": "The results for the probabilistic models for infering lexical semantic roles are shown in, where the term naive means that no WordNet features were included in the training of the models, but only simple features like noun rel for nouns.", "labels": [], "entities": []}, {"text": "On the contrary, when mode is complete, WordNet hypernyms up to the 5th level in the hierarchy were used.", "labels": [], "entities": []}, {"text": "In this set of experiments the VerbNet class was also included (in the marked cases) during the learning and inference phases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of role mapping with probabilistic model", "labels": [], "entities": [{"text": "role mapping", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9289833307266235}]}]}