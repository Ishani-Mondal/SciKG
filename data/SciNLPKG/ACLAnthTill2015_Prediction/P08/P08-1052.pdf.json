{"title": [{"text": "Solving Relational Similarity Problems Using the Web as a Corpus", "labels": [], "entities": [{"text": "Relational Similarity", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.7443332374095917}]}], "abstractContent": [{"text": "We present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns.", "labels": [], "entities": []}, {"text": "The approach leverages the vast size of the Web in order to build lexically-specific features.", "labels": [], "entities": []}, {"text": "The main idea is to look for verbs, prepositions , and coordinating conjunctions that can help make explicit the hidden relations between the target nouns.", "labels": [], "entities": []}, {"text": "Using these features in instance-based classifiers, we demonstrate state-of-the-art results on various rela-tional similarity problems, including mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER, characterizing noun-noun compounds in terms of abstract linguistic predicates like CAUSE, USE, and FROM, classifying the relations between nominals in context, and solving SAT verbal analogy problems.", "labels": [], "entities": [{"text": "USE", "start_pos": 321, "end_pos": 324, "type": "DATASET", "confidence": 0.7312292456626892}, {"text": "FROM", "start_pos": 330, "end_pos": 334, "type": "METRIC", "confidence": 0.8829012513160706}, {"text": "SAT verbal analogy", "start_pos": 403, "end_pos": 421, "type": "TASK", "confidence": 0.7031771540641785}]}, {"text": "In essence, the approach puts together some existing ideas, showing that they apply generally to various semantic tasks, finding that verbs are especially useful features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite the tremendous amount of work on word similarity (see) for an overview), there is surprisingly little research on the important related problem of relational similaritysemantic similarity between pairs of words.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6963163167238235}, {"text": "relational similaritysemantic similarity between pairs of words", "start_pos": 155, "end_pos": 218, "type": "TASK", "confidence": 0.7638808233397347}]}, {"text": "Students who took the SAT test before 2005 or who * After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg are taking the GRE test nowadays are familiar with an instance of this problem -verbal analogy questions, which ask whether, e.g., the relationship between ostrich and bird is more similar to that between lion and cat, or rather between primate and monkey.", "labels": [], "entities": [{"text": "GRE", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.8503612279891968}]}, {"text": "These analogies are difficult, and the average test taker gives a correct answer 57% of the time).", "labels": [], "entities": []}, {"text": "Many NLP applications could benefit from solving relational similarity problems, including but not limited to question answering, information retrieval, machine translation, word sense disambiguation, and information extraction.", "labels": [], "entities": [{"text": "question answering", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8452876210212708}, {"text": "information retrieval", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.8061938881874084}, {"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.8063525557518005}, {"text": "word sense disambiguation", "start_pos": 174, "end_pos": 199, "type": "TASK", "confidence": 0.6783792575200399}, {"text": "information extraction", "start_pos": 205, "end_pos": 227, "type": "TASK", "confidence": 0.8494982421398163}]}, {"text": "For example, a relational search engine like TextRunner, which serves queries like \"find all X such that X causes wrinkles\", asking for all entities that are in a particular relation with a given entity (), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT.", "labels": [], "entities": []}, {"text": "While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment ( . In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7453453242778778}]}, {"text": "Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER.", "labels": [], "entities": [{"text": "SAT verbal analogy", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7000545064608256}]}, {"text": "We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context.", "labels": [], "entities": [{"text": "USE", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.6374572515487671}, {"text": "FROM", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9931631088256836}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: SAT verbal analogy: 184 noun-only examples.  v stands for verb, p for preposition, and c for coordinating  conjunction. For each model, the number of correct (),  wrong (\u00d7), and nonclassified examples (\u2205) is shown, fol- lowed by accuracy and coverage (in %s).", "labels": [], "entities": [{"text": "SAT verbal analogy", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6906340519587199}, {"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.9984556436538696}, {"text": "coverage", "start_pos": 252, "end_pos": 260, "type": "METRIC", "confidence": 0.9924774765968323}]}, {"text": " Table 4: Head-modifier relations, 30 classes: evaluation  on the Diverse dataset, micro-averaged (in %s).", "labels": [], "entities": [{"text": "Diverse dataset", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.85088050365448}]}, {"text": " Table 6: Noun-noun compound relations, 12 classes: evaluation on Levi-214 dataset. Shown are micro-averaged  accuracy and coverage in %s, followed by average number of features (ANF) and average sum of feature frequencies  (ASF) per example. The righthand side reports the results when the query patterns involving THAT were not used. For  comparison purposes, the top rows show the performance with the human-proposed verbs used as features.", "labels": [], "entities": [{"text": "Levi-214 dataset", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.9322701096534729}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.960807204246521}, {"text": "coverage", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9884219765663147}, {"text": "average number of features (ANF)", "start_pos": 151, "end_pos": 183, "type": "METRIC", "confidence": 0.6639477866036552}, {"text": "average sum of feature frequencies  (ASF)", "start_pos": 188, "end_pos": 229, "type": "METRIC", "confidence": 0.7118236571550369}]}]}