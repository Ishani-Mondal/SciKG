{"title": [{"text": "When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging", "labels": [], "entities": [{"text": "Sentiment Tagging", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9221103489398956}]}], "abstractContent": [{"text": "This study presents a novel approach to the problem of system portability across different domains: a sentiment annotation system that integrates a corpus-based classifier trained on a small set of annotated in-domain data and a lexicon-based system trained on Word-Net.", "labels": [], "entities": [{"text": "system portability", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7109220325946808}, {"text": "Word-Net", "start_pos": 261, "end_pos": 269, "type": "DATASET", "confidence": 0.9771620631217957}]}, {"text": "The paper explores the challenges of system portability across domains and text gen-res (movie reviews, news, blogs, and product reviews), highlights the factors affecting system performance on out-of-domain and small-set in-domain data, and presents anew system consisting of the ensemble of two classi-fiers with precision-based vote weighting, that provides significant gains inaccuracy and recall over the corpus-based classifier and the lexicon-based system taken individually.", "labels": [], "entities": [{"text": "system portability", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7188832759857178}, {"text": "recall", "start_pos": 394, "end_pos": 400, "type": "METRIC", "confidence": 0.9986422657966614}]}], "introductionContent": [{"text": "One of the emerging directions in NLP is the development of machine learning methods that perform well not only on the domain on which they were trained, but also on other domains, for which training data is not available or is not sufficient to ensure adequate machine learning.", "labels": [], "entities": []}, {"text": "Many applications require reliable processing of heterogeneous corpora, such as the World Wide Web, where the diversity of genres and domains present in the Internet limits the feasibility of in-domain training.", "labels": [], "entities": []}, {"text": "In this paper, sentiment annotation is defined as the assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units.", "labels": [], "entities": [{"text": "sentiment annotation", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9519028067588806}]}, {"text": "Recent experiments assessing system portability across different domains, conducted by Aue and , demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains.", "labels": [], "entities": [{"text": "system portability", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.691177487373352}, {"text": "sentiment annotation classifiers", "start_pos": 115, "end_pos": 147, "type": "TASK", "confidence": 0.833676795164744}]}, {"text": "A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a lexicon-based system trained on WordNet.", "labels": [], "entities": [{"text": "system portability", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7038717716932297}, {"text": "WordNet", "start_pos": 230, "end_pos": 237, "type": "DATASET", "confidence": 0.9604001045227051}]}, {"text": "By adopting this approach, we sought to develop a system that relies on both general and domainspecific knowledge, as humans do when analyzing a text.", "labels": [], "entities": []}, {"text": "The information contained in lexicographical sources, such as WordNet, reflects a lay person's general knowledge about the world, while domainspecific knowledge can be acquired through classifier training on a small set of in-domain data.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9693263173103333}]}, {"text": "The first part of this paper reviews the extant literature on domain adaptation in sentiment analysis and highlights promising directions for research.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7167131304740906}, {"text": "sentiment analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8063861429691315}]}, {"text": "The second part establishes a baseline for system evaluation by drawing comparisons of system performance across four different domains/genresmovie reviews, news, blogs, and product reviews.", "labels": [], "entities": []}, {"text": "The final, third part of the paper presents our system, composed of an ensemble of two classifiersone trained on WordNet glosses and synsets and the other trained on a small in-domain training set.", "labels": [], "entities": []}], "datasetContent": [{"text": "Movie  The comparative analysis of the corpus-based and lexicon-based systems described above revealed that the errors produced by CBS and LBS were to a great extent complementary (i.e., where one classifier makes an error, the other tends to give the correct answer).", "labels": [], "entities": [{"text": "LBS", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.7046241760253906}]}, {"text": "This provided further justification to the integration of corpus-based and lexicon-based approaches in a single system.", "labels": [], "entities": []}, {"text": "below illustrates the complementarity of the performance CBS and LBS classifiers on the positive and negative categories.", "labels": [], "entities": []}, {"text": "In this experiment, the corpus-based classifier was trained on 400 annotated product review sentences 5 . The two systems were then evaluated on a test set of another 400 product review sentences.", "labels": [], "entities": []}, {"text": "The results reported in are statistically significant at \u03b1 = 0.01.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 57, "end_pos": 58, "type": "METRIC", "confidence": 0.9930104613304138}]}], "tableCaptions": [{"text": " Table 2: Accuracy of Na\u00a8\u0131veNa\u00a8\u0131ve Bayes on movie reviews.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980114698410034}]}, {"text": " Table 3: Accuracy of unigram, bigram and trigram mod- els across domains.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.990257740020752}]}, {"text": " Table 4: Accuracy of SVM with unigram model", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9966647028923035}, {"text": "SVM", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9013634920120239}]}, {"text": " Table 7: Performance of the ensemble classifier", "labels": [], "entities": []}]}