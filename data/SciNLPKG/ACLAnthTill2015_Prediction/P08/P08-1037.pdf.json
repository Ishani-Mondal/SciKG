{"title": [{"text": "Improving Parsing and PP attachment Performance with Sense Information", "labels": [], "entities": [{"text": "Improving Parsing and PP attachment", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7994578599929809}]}], "abstractContent": [{"text": "To date, parsers have made limited use of semantic information, but there is evidence to suggest that semantic features can enhance parse disambiguation.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 132, "end_pos": 152, "type": "TASK", "confidence": 0.8349295556545258}]}, {"text": "This paper shows that semantic classes help to obtain significant improvement in both parsing and PP attachment tasks.", "labels": [], "entities": [{"text": "PP attachment tasks", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7919495503107706}]}, {"text": "We devise a gold-standard sense-and parse tree-annotated dataset based on the intersection of the Penn Treebank and SemCor, and experiment with different approaches to both semantic representation and disambigua-tion.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9947282373905182}]}, {"text": "For the Bikel parser, we achieved a maximal error reduction rate over the base-line parser of 6.9% and 20.5%, for parsing and PP-attachment respectively, using an unsuper-vised WSD strategy.", "labels": [], "entities": [{"text": "maximal error reduction rate", "start_pos": 36, "end_pos": 64, "type": "METRIC", "confidence": 0.834677129983902}, {"text": "parsing", "start_pos": 114, "end_pos": 121, "type": "TASK", "confidence": 0.9605094194412231}]}, {"text": "This demonstrates that word sense information can indeed enhance the performance of syntactic disambiguation.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.7927293181419373}]}], "introductionContent": [{"text": "Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9384257197380066}]}, {"text": "There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.9376271069049835}]}, {"text": "For example, a number of different parsers have been shown to benefit from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent.", "labels": [], "entities": []}, {"text": "As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife.", "labels": [], "entities": []}, {"text": "It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to have the same attachment preferences.", "labels": [], "entities": []}, {"text": "In order to deal with this limitation, we propose to integrate directly the semantic classes of words into the process of training the parser.", "labels": [], "entities": []}, {"text": "This is done by substituting the original words with semantic codes that reflect semantic classes.", "labels": [], "entities": []}, {"text": "For example, in the above example we could substitute both knife and scissors with the semantic class TOOL, thus relating the training and test instances directly.", "labels": [], "entities": [{"text": "TOOL", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9060436487197876}]}, {"text": "We explore several models for semantic representation, based around WordNet.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7293183654546738}, {"text": "WordNet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9477111101150513}]}, {"text": "Our approach to exploring the impact of lexical semantics on parsing performance is to take two state-of-the-art statistical treebank parsers and preprocess the inputs variously.", "labels": [], "entities": [{"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9652600884437561}]}, {"text": "This simple method allows us to incorporate semantic information into the parser without having to reimplement a full statistical parser, and also allows for maximum comparability with existing results in the treebank parsing community.", "labels": [], "entities": []}, {"text": "We test the parsers over both a PP attachment and full parsing task.", "labels": [], "entities": []}, {"text": "In experimenting with different semantic representations, we require some strategy to disambiguate the semantic class of polysemous words in context (e.g. determining for each instance of crane whether it refers to an animal or a lifting device).", "labels": [], "entities": []}, {"text": "We explore a number of disambiguation strategies, including the use of hand-annotated (gold-standard) senses, the use of the most frequent sense, and an unsupervised word sense disambiguation (WSD) system.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 166, "end_pos": 197, "type": "TASK", "confidence": 0.768356055021286}]}, {"text": "This paper shows that semantic classes help to obtain significant improvements for both PP attachment and parsing.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.9387495517730713}]}, {"text": "We attain a 20.5% error reduction for PP attachment, and 6.9% for parsing.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.9863861501216888}, {"text": "PP attachment", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8680977523326874}, {"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9786191582679749}]}, {"text": "These results are achieved using most frequent sense information, which surprisingly outperforms both goldstandard senses and automatic WSD.", "labels": [], "entities": [{"text": "goldstandard", "start_pos": 102, "end_pos": 114, "type": "METRIC", "confidence": 0.9848072528839111}, {"text": "WSD", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.6992197632789612}]}, {"text": "The results are notable in demonstrating that very simple preprocessing of the parser input facilitates significant improvements in parser performance.", "labels": [], "entities": []}, {"text": "We provide the first definitive results that word sense information can enhance Penn Treebank parser performance, building on earlier results of Bikel and.", "labels": [], "entities": [{"text": "Penn Treebank parser", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.8916089534759521}]}, {"text": "Given our simple procedure for incorporating lexical semantics into the parsing process, our hope is that this research will open the door to further gains using more sophisticated parsing models and richer semantic options.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.9152217209339142}]}], "datasetContent": [{"text": "We evaluate the performance of our approach in two settings: (1) full parsing, and (2) PP attachment within a full parsing context.", "labels": [], "entities": []}, {"text": "Below, we outline the dataset used in this research and the parser evaluation methodology, explain the methodology used to perform PP attachment, present the different options for semantic representation, and finally detail the disambiguation methods.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.9566590785980225}]}, {"text": "One of the main requirements for our dataset is the availability of gold-standard sense and parse tree annotations.", "labels": [], "entities": []}, {"text": "The gold-standard sense annotations allow us to perform upper bound evaluation of the relative impact of a given semantic representation on parsing and PP attachment performance, to contrast with the performance in more realistic semantic disambiguation settings.", "labels": [], "entities": []}, {"text": "The gold-standard parse tree annotations are required in order to carryout evaluation of parser and PP attachment performance.", "labels": [], "entities": []}, {"text": "The only publicly-available resource with these two characteristics at the time of this work was the subset of the Brown Corpus that is included in both SemCor () and the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.9719204902648926}, {"text": "Penn Treebank (PTB)", "start_pos": 171, "end_pos": 190, "type": "DATASET", "confidence": 0.9735540270805358}]}, {"text": "This provided the basis of our dataset.", "labels": [], "entities": []}, {"text": "After sentence-and word-aligning the SemCor and PTB data (discarding sentences where there was a difference in tokenisation), we were left with a total of 8,669 sentences containing 151,928 words.", "labels": [], "entities": [{"text": "PTB data", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9072007834911346}]}, {"text": "Note that this dataset is smaller than the one described by in a similar exercise, the reason being our simple and conservative approach taken when merging the resources.", "labels": [], "entities": []}, {"text": "We relied on this dataset alone for all the experiments in this paper.", "labels": [], "entities": []}, {"text": "In order to maximise reproducibility and encourage further experimentation in the direction pioneered in this research, we partitioned the data into 3 sets: 80% training, 10% development and 10% test data.", "labels": [], "entities": []}, {"text": "This dataset is available on request to the research community.", "labels": [], "entities": []}, {"text": "We evaluate the parsers via labelled bracketing recall (R), precision (P) and F-score (F 1 ).", "labels": [], "entities": [{"text": "labelled bracketing recall (R)", "start_pos": 28, "end_pos": 58, "type": "METRIC", "confidence": 0.7339681188265482}, {"text": "precision (P)", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9583847969770432}, {"text": "F-score (F 1 )", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.9415874600410461}]}, {"text": "We use Bikel's randomized parsing evaluation comparator 3 (with p < 0.05 throughout) to test the statistical significance of the results using word sense information, relative to the respective baseline parser using only lexical features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parsing results with gold-standard senses (  *  in- dicates that the recall or precision is significantly better  than baseline; the best performing method in each col- umn is shown in bold)", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9443541169166565}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9987683892250061}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.8636327981948853}]}, {"text": " Table 3: PP attachment results with gold-standard senses  (  *  indicates that the recall or precision is significantly bet- ter than baseline; the best performing method in each col- umn is shown in bold)", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9076125025749207}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9990184307098389}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.803908109664917}]}, {"text": " Table 4: Parsing results with 1ST (  *  indicates that the  recall or precision is significantly better than baseline; the  best performing method in each column is shown in bold)", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9373172521591187}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9992576241493225}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.8644319176673889}]}, {"text": " Table 5: PP attachment results with 1ST (  *  indicates that  the recall or precision is significantly better than baseline;  the best performing method in each column is shown in  bold)", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8563700616359711}, {"text": "1ST", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9744463562965393}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9992913007736206}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.8658028244972229}]}, {"text": " Table 6: Parsing results with ASR (  *  indicates that the  recall or precision is significantly better than baseline; the  best performing method in each column is shown in bold)", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9180230498313904}, {"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9696202874183655}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9992750287055969}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.8926635980606079}]}, {"text": " Table 7: PP attachment results with ASR (  *  indicates that  the recall or precision is significantly better than baseline;  the best performance in each column is shown in bold)", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8797820806503296}, {"text": "ASR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9887037873268127}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9994094371795654}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9163383841514587}]}, {"text": " Table 8: Summary of F-score results with error reduc- tion rates and the best semantic representation(s) for each  setting (C = Charniak, B = Bikel)", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9849114418029785}]}]}