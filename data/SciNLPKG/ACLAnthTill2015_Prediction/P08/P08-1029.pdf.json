{"title": [{"text": "Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9375274181365967}, {"text": "Named Entity Recognition", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.5838643809159597}]}], "abstractContent": [{"text": "We present a novel hierarchical prior structure for supervised transfer learning in named entity recognition, motivated by the common structure of feature spaces for this task across natural language data sets.", "labels": [], "entities": [{"text": "supervised transfer learning in named entity recognition", "start_pos": 52, "end_pos": 108, "type": "TASK", "confidence": 0.6763290166854858}]}, {"text": "The problem of transfer learning, where information gained in one learning task is used to improve performance in another related task, is an important new area of research.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.9538242816925049}]}, {"text": "In the subproblem of domain adaptation, a model trained over a source domain is generalized to perform well on a related target domain, where the two domains' data are distributed similarly, but not identically.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7416835874319077}]}, {"text": "We introduce the concept of groups of closely-related domains, called genres, and show how inter-genre adaptation is related to domain adaptation.", "labels": [], "entities": [{"text": "inter-genre adaptation", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7186050117015839}, {"text": "domain adaptation", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7385233640670776}]}, {"text": "We also examine multi-task learning, where two domains maybe related , but where the concept to be learned in each case is distinct.", "labels": [], "entities": []}, {"text": "We show that our prior conveys useful information across domains, genres and tasks, while remaining robust to spurious signals not related to the target domain and concept.", "labels": [], "entities": []}, {"text": "We further show that our model generalizes a class of similar hierarchical priors, smoothed to varying degrees, and lay the groundwork for future exploration in this area.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluated the performance of various transfer learning methods on the data and tasks described in \u00a73.1.", "labels": [], "entities": []}, {"text": "Specifically, we compared our approximate hierarchical prior model (HIER), implemented as a CRF, against three baselines: \u2022 GAUSS: CRF model tuned on a single domain's data, using a standard N (0, 1) prior \u2022 CAT: CRF model tuned on a concatenation of multiple domains' data, using a N (0, 1) prior \u2022 CHELBA: CRF model tuned on one domain's data, using a prior trained on a different, related domain's data (cf. \u00a72.3) We use token-level F 1 as our main evaluation measure, combining precision and recall into one metric.", "labels": [], "entities": [{"text": "GAUSS", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.849074125289917}, {"text": "precision", "start_pos": 482, "end_pos": 491, "type": "METRIC", "confidence": 0.9984807372093201}, {"text": "recall", "start_pos": 496, "end_pos": 502, "type": "METRIC", "confidence": 0.9934834837913513}]}, {"text": "the data is closely related both in domain and task, has actually hurt the performance of our recognizer for training sizes of moderate to large size.", "labels": [], "entities": []}, {"text": "This is most likely because, although the MUC6 and MUC7 datasets are closely related, they are still drawn from different distributions and thus cannot be intermingled indiscriminately.", "labels": [], "entities": [{"text": "MUC6", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9510236978530884}, {"text": "MUC7 datasets", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9253907203674316}]}, {"text": "Line c shows the same combination of MUC6 and MUC7, only this time the datasets have been combined using the HIER prior.", "labels": [], "entities": [{"text": "MUC6", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8725076913833618}, {"text": "MUC7", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.848904550075531}, {"text": "HIER prior", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.8857019245624542}]}, {"text": "In this case, the performance actually does improve, both with respect to the single-dataset trained baseline (a) and the naively trained double-dataset (b).", "labels": [], "entities": []}, {"text": "Finally, lined shows the results of the CHELBA prior.", "labels": [], "entities": [{"text": "CHELBA", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.8366852402687073}]}, {"text": "Curiously, though the domains are closely related, it does more poorly than even the non-transfer GAUSS.", "labels": [], "entities": []}, {"text": "One possible explanation is that, although much of the vocabulary is shared across domains, the interpretation of the features of these words may differ.", "labels": [], "entities": []}, {"text": "Since CHELBA doesn't model the hierarchy among features like HIER, it is unable to smooth away these discrepancies.", "labels": [], "entities": [{"text": "HIER", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.7461857795715332}]}, {"text": "In contrast, we see that our HIER prior is able to successfully combine the relevant parts of data across domains while filtering the irrelevant, and possibly detrimental, ones.", "labels": [], "entities": []}, {"text": "This experiment was repeated for other sets of intra-genre tasks, and the results are summarized in \u00a73.2.3.", "labels": [], "entities": []}], "tableCaptions": []}