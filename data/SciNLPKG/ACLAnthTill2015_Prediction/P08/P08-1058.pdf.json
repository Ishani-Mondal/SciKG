{"title": [{"text": "Randomized Language Models via Perfect Hash Functions", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n-grams and their associated probabilities, backoff weights, or other parameters.", "labels": [], "entities": []}, {"text": "The scheme can represent any standard n-gram model and is easily combined with existing model reduction techniques such as entropy-pruning.", "labels": [], "entities": []}, {"text": "We demonstrate the space-savings of the scheme via machine translation experiments within a distributed language modeling framework.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.6897243559360504}]}], "introductionContent": [{"text": "Language models (LMs) area core component in statistical machine translation, speech recognition, optical character recognition and many other areas.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.7290147344271342}, {"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7752061486244202}, {"text": "optical character recognition", "start_pos": 98, "end_pos": 127, "type": "TASK", "confidence": 0.6862214406331381}]}, {"text": "They distinguish plausible word sequences from a set of candidates.", "labels": [], "entities": []}, {"text": "LMs are usually implemented as n-gram models parameterized for each distinct sequence of up ton words observed in the training corpus.", "labels": [], "entities": []}, {"text": "Using higher-order models and larger amounts of training data can significantly improve performance in applications, however the size of the resulting LM can become prohibitive.", "labels": [], "entities": []}, {"text": "With large monolingual corpora available in major languages, making use of all the available data is now a fundamental challenge in language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7098142504692078}]}, {"text": "Efficiency is paramount in applications such as machine translation which make huge numbers of LM requests per sentence.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9795387387275696}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8108609020709991}]}, {"text": "To scale LMs to larger corpora with higher-order dependencies, researchers * Work completed while this author was at Google Inc.", "labels": [], "entities": []}, {"text": "have considered alternative parameterizations such as class-based models, model reduction techniques such as entropy-based pruning, novel represention schemes such as suffix arrays, Golomb Coding ( and distributed language models that scale more readily (.", "labels": [], "entities": [{"text": "model reduction", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7546451985836029}, {"text": "Golomb Coding", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.6288014203310013}]}, {"text": "In this paper we propose a novel randomized language model.", "labels": [], "entities": []}, {"text": "Recent work has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability.", "labels": [], "entities": []}, {"text": "In contrast the representation scheme used by our model encodes parameters directly.", "labels": [], "entities": []}, {"text": "It can be combined with any n-gram parameter estimation method and existing model reduction techniques such as entropy-based pruning.", "labels": [], "entities": []}, {"text": "Parameters that are stored in the model are retrieved without error; however, false positives may occur whereby n-grams not in the model are incorrectly 'found' when requested.", "labels": [], "entities": []}, {"text": "The false positive rate is determined by the space usage of the model.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8680150111516317}]}, {"text": "Our randomized language model is based on the Bloomier filter ().", "labels": [], "entities": []}, {"text": "We encode fingerprints (random hashes) of n-grams together with their associated probabilities using a perfect hash function generated at random (.", "labels": [], "entities": []}, {"text": "Lookup is very efficient: the values of 3 cells in a large array are combined with the fingerprint of an n-gram.", "labels": [], "entities": []}, {"text": "This paper focuses on machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8267414569854736}]}, {"text": "However, many of our findings should transfer to other applications of language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.731166660785675}]}], "datasetContent": [{"text": "This section describes three sets of experiments: first, we encode the Web 1T 5-gram corpus as a randomized language model and compare the resulting size with other representations; then we measure false positive rates when requesting n-grams fora held-out data set; finally we compare translation quality when using conventional (lossless) languages models and our randomized language model.", "labels": [], "entities": [{"text": "Web 1T 5-gram corpus", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.809602752327919}]}, {"text": "Note that the standard practice of measuring perplexity is not meaningful here since (1) for efficient computation, the language model is not normalized; and (2) even if this were not the case, quantization and false positives would render it unnormalized.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Num. of n-grams in the Web 1T 5-gram corpus.", "labels": [], "entities": [{"text": "Web 1T 5-gram corpus", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.7758594304323196}]}, {"text": " Table 2: Web 1T 5-gram language model sizes with dif- ferent encodings. \"Randomized\" uses 12 error bits.", "labels": [], "entities": []}, {"text": " Table 3: Number of n-grams in test set and percentages  of n-grams that were seen/unseen in the training data.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9611574411392212}]}, {"text": " Table 4: False positive rates with 8 and 12 error bits.", "labels": [], "entities": [{"text": "False positive rates", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.936984141667684}]}, {"text": " Table 5: Baseline BLEU scores with lossless n-gram  model and different quantization levels (bits).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9348486065864563}]}, {"text": " Table 6: Combining randomization and entropy pruning.  All models use 8-bit values; \"rand\" uses 12 error bits.", "labels": [], "entities": []}]}