{"title": [{"text": "Unlexicalised Hidden Variable Models of Split Dependency Grammars *", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper investigates transforms of split dependency grammars into unlexicalised context-free grammars annotated with hidden symbols.", "labels": [], "entities": []}, {"text": "Our best unlexicalised grammar achieves an accuracy of 88% on the Penn Treebank data set, that represents a 50% reduction in error over previously published results on unlexicalised dependency parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.99954754114151}, {"text": "Penn Treebank data set", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.9966840595006943}, {"text": "error", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9892922639846802}, {"text": "unlexicalised dependency parsing", "start_pos": 168, "end_pos": 200, "type": "TASK", "confidence": 0.6816983819007874}]}], "introductionContent": [{"text": "Recent research in natural language parsing has extensively investigated probabilistic models of phrase-structure parse trees.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6503006815910339}, {"text": "phrase-structure parse trees", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.8015114466349283}]}, {"text": "As well as being the most commonly used probabilistic models of parse trees, probabilistic context-free grammars (PCFGs) are the best understood.", "labels": [], "entities": []}, {"text": "As shown in, the ability of PCFG models to disambiguate phrases crucially depends on the expressiveness of the symbolic backbone they use.", "labels": [], "entities": []}, {"text": "Treebank-specific heuristics have commonly been used both to alleviate inadequate independence assumptions stipulated by naive PCFGs).", "labels": [], "entities": []}, {"text": "Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks).", "labels": [], "entities": []}, {"text": "* Part of this work was done when Gabriele Musillo was visiting the MIT Computer Science and Artificial Intelligence Laboratory, funded by a grant from the Swiss NSF (PBGE2-117146).", "labels": [], "entities": [{"text": "Swiss NSF (PBGE2-117146)", "start_pos": 156, "end_pos": 180, "type": "DATASET", "confidence": 0.6912689745426178}]}, {"text": "Many thanks to Michael Collins and Xavier Carreras for their insightful comments on the work presented here.", "labels": [], "entities": []}, {"text": "This paper presents extensions of such grammar induction techniques to dependency grammars.", "labels": [], "entities": []}, {"text": "Our extensions rely on transformations of dependency grammars into efficiently parsable contextfree grammars (CFG) annotated with hidden symbols.", "labels": [], "entities": []}, {"text": "Because dependency grammars are reduced to CFGs, any learning algorithm developed for PCFGs can be applied to them.", "labels": [], "entities": []}, {"text": "Specifically, we use the Inside-Outside algorithm defined in to learn transformed dependency grammars annotated with hidden symbols.", "labels": [], "entities": []}, {"text": "What distinguishes our work from most previous work on dependency parsing is that our models are not lexicalised.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8449243605136871}]}, {"text": "Our models are instead decorated with hidden symbols that are designed to capture both lexical and structural information relevant to accurate dependency parsing without having to rely on any explicit supervision.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7527638971805573}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy results on the development and test  data set, where q denotes the number of hidden states and  h the number of hidden values annotating a PoS tag in- volved in our first-order (FOM) and second-order (SOM)  models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986440539360046}]}]}