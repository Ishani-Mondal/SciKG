{"title": [{"text": "MAXSIM: A Maximum Similarity Metric for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.8736149470011393}]}], "abstractContent": [{"text": "We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.8594086865584055}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9958773851394653}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9533343315124512}]}, {"text": "Unlike most metrics, we compute a similarity score between items across the two sentences.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 34, "end_pos": 50, "type": "METRIC", "confidence": 0.9511507749557495}]}, {"text": "We then find a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence.", "labels": [], "entities": []}, {"text": "This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison , such as n-grams, dependency relations , etc.", "labels": [], "entities": []}, {"text": "When evaluated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop .", "labels": [], "entities": [{"text": "ACL-07 MT workshop", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.6462875505288442}, {"text": "MT evaluation", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9205887019634247}]}], "introductionContent": [{"text": "In recent years, machine translation (MT) research has made much progress, which includes the introduction of automatic metrics for MT evaluation.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8862396955490113}, {"text": "MT evaluation", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.9604764878749847}]}, {"text": "Since human evaluation of MT output is time consuming and expensive, having a robust and accurate automatic MT evaluation metric that correlates well with human judgement is invaluable.", "labels": [], "entities": [{"text": "MT output", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.87735915184021}, {"text": "MT evaluation", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9405047595500946}]}, {"text": "Among all the automatic MT evaluation metrics, BLEU) is the most widely used.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9296302795410156}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9990891218185425}]}, {"text": "Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.99649578332901}, {"text": "MT research", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9159145653247833}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9964889287948608}]}, {"text": "During the recent ACL-07 workshop on statistical MT), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.7399497032165527}, {"text": "MT evaluation", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.8967840373516083}]}, {"text": "The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (, ParaEval-recall (), and METEOR () achieve higher correlation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.991415798664093}, {"text": "METEOR", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9679260849952698}, {"text": "correlation", "start_pos": 160, "end_pos": 171, "type": "METRIC", "confidence": 0.9696763157844543}]}, {"text": "In this paper, we propose anew automatic MT evaluation metric, MAXSIM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9309092164039612}, {"text": "MAXSIM", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.7939373850822449}]}, {"text": "Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other.", "labels": [], "entities": []}, {"text": "Having weighted matches between items means that there could be many possible ways to match, or link items from a system translation sentence to a reference translation sentence.", "labels": [], "entities": []}, {"text": "To match each system item to at most one reference item, we model the items in the sentence pair as nodes in a bipartite graph and use the Kuhn-Munkres algorithm to find a maximum weight matching (or alignment) between the items in polynomial time.", "labels": [], "entities": []}, {"text": "The weights (from the edges) of the resulting graph will then be added to determine the final similarity score between the pair of sentences.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.9108715057373047}]}, {"text": "Although a maximum weight bipartite graph was also used in the recent work of (), their focus was on learning supervised models for single word alignment between sentences from a source and target language.", "labels": [], "entities": [{"text": "single word alignment between sentences", "start_pos": 132, "end_pos": 171, "type": "TASK", "confidence": 0.7780454874038696}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "Current metrics (such as BLEU, METEOR, Semantic-role overlap, ParaEval-recall, etc.) do not assign different weights to their matches: either two items match, or they don't.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9989734888076782}, {"text": "METEOR", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9791087508201599}]}, {"text": "Also, metrics such as METEOR determine an alignment between the items of a sentence pair by using heuristics such as the least number of matching crosses.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9301921725273132}]}, {"text": "In contrast, we propose weighting different matches differently, and then obtain an optimal set of matches, or alignments, by using a maximum weight matching framework.", "labels": [], "entities": []}, {"text": "We note that this framework is not used by any of the 11 automatic MT metrics in the ACL-07 MT workshop.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.982774555683136}, {"text": "ACL-07 MT workshop", "start_pos": 85, "end_pos": 103, "type": "DATASET", "confidence": 0.6968652208646139}]}, {"text": "Also, this framework allows for defining arbitrary similarity functions between two matching items, and we could match arbitrary concepts (such as dependency relations) gathered from a sentence pair.", "labels": [], "entities": []}, {"text": "In contrast, most other metrics (notably BLEU) limit themselves to matching based only on the surface form of words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9846387505531311}]}, {"text": "Finally, when evaluated on the datasets of the recent ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all of the 11 automatic MT evaluation metrics evaluated during the workshop.", "labels": [], "entities": [{"text": "MT workshop", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.5401496142148972}, {"text": "MT evaluation", "start_pos": 173, "end_pos": 186, "type": "TASK", "confidence": 0.9254246950149536}]}, {"text": "In the next section, we describe several existing metrics.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss issues to consider when designing a metric.", "labels": [], "entities": []}, {"text": "In Section 4, we describe our proposed metric.", "labels": [], "entities": []}, {"text": "In Section 5, we present our experimental results.", "labels": [], "entities": []}, {"text": "Finally, we outline future work in Section 6, before concluding in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe BLEU, and the three metrics which achieved higher correlation results than BLEU in the recent ACL-07 MT workshop.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9945725798606873}, {"text": "correlation", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9745465517044067}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9955962300300598}, {"text": "ACL-07 MT workshop", "start_pos": 123, "end_pos": 141, "type": "DATASET", "confidence": 0.6549386978149414}]}, {"text": "For human evaluation of the MT submissions, four different criteria were used in the workshop: Adequacy (how much of the original meaning is expressed in a system translation), Fluency (the translation's fluency), Rank (different translations of a single source sentence are compared and ranked from best to worst), and Constituent (some constituents from the parse tree of the source sentence are translated, and human judges have to rank these translations).", "labels": [], "entities": [{"text": "MT submissions", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.929631918668747}, {"text": "Adequacy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9535512924194336}, {"text": "Fluency", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.9974536299705505}, {"text": "Rank", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9898752570152283}, {"text": "Constituent", "start_pos": 320, "end_pos": 331, "type": "METRIC", "confidence": 0.9938562512397766}]}, {"text": "During the workshop, Kappa values measured for inter-and intra-annotator agreement for rank and constituent are substantially higher than those for adequacy and fluency, indicating that rank and constituent are more reliable criteria for MT evaluation.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9518070816993713}, {"text": "MT evaluation", "start_pos": 238, "end_pos": 251, "type": "TASK", "confidence": 0.9312322437763214}]}, {"text": "We also conduct experiments on the test data (LDC2006T04) of NIST MT 2003 Chinese-English translation task.", "labels": [], "entities": [{"text": "NIST MT 2003 Chinese-English translation task", "start_pos": 61, "end_pos": 106, "type": "TASK", "confidence": 0.7984182139237722}]}, {"text": "For this dataset, human judgements are available on adequacy and fluency for six system submissions, and there are four English reference translation texts.", "labels": [], "entities": []}, {"text": "Since implementations of the BLEU and ME-TEOR metrics are publicly available, we score the system submissions using BLEU (version 11b with its default settings), METEOR, and MAXSIM, showing the resulting correlations in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9888810515403748}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.983845591545105}, {"text": "METEOR", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.5565886497497559}]}, {"text": "For METEOR, when used with its originally proposed parameter values of (\u03b1=0.9, \u03b2=3.0, \u03b3=0.5), which the METEOR researchers mentioned were based on some early experimental work (), we obtain an average correlation value of 0.915, as shown in the row \"METEOR\".", "labels": [], "entities": [{"text": "METEOR", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.5648167133331299}, {"text": "METEOR", "start_pos": 250, "end_pos": 256, "type": "METRIC", "confidence": 0.6144683361053467}]}, {"text": "In the recent work of, the values of these parameters were tuned to be (\u03b1=0., based on experiments on the NIST 2003 and 2004 Arabic-English evaluation datasets.", "labels": [], "entities": [{"text": "NIST 2003 and 2004 Arabic-English evaluation datasets", "start_pos": 106, "end_pos": 159, "type": "DATASET", "confidence": 0.9319655469485691}]}, {"text": "When METEOR was run with these new parameter values, it returned an average correlation value of 0.972, as shown in the row \"METEOR (optimized)\".", "labels": [], "entities": [{"text": "METEOR", "start_pos": 5, "end_pos": 11, "type": "DATASET", "confidence": 0.8106527328491211}, {"text": "correlation", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.8936523199081421}, {"text": "METEOR", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.5819098949432373}]}, {"text": "MAXSIM using only n-gram information (MAXSIM n ) gives an average correlation value of 0.800, while adding dependency information (MAXSIM n+d ) improves the correlation value to 0.915.", "labels": [], "entities": [{"text": "correlation", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9620366096496582}, {"text": "correlation", "start_pos": 157, "end_pos": 168, "type": "METRIC", "confidence": 0.9830740094184875}]}, {"text": "Note that so far, the parameters of MAXSIM are not optimized and we simply perform uniform averaging of the different n-grams and dependency scores.", "labels": [], "entities": [{"text": "MAXSIM", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.5973904728889465}]}, {"text": "Under this setting, the correlation achieved by MAXSIM is comparable to that achieved by METEOR.", "labels": [], "entities": [{"text": "correlation", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9840444326400757}, {"text": "METEOR", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.7428260445594788}]}], "tableCaptions": [{"text": " Table 1: Overall correlations on the Europarl and News Commentary datasets. The \"Semantic-role overlap\" metric  is abbreviated as \"Semantic-role\". Note that each figure above represents 6 translation tasks: the Europarl and News  Commentary datasets each with 3 language pairs (German-English, Spanish-English, French-English).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9472016096115112}, {"text": "News Commentary datasets", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.860565185546875}, {"text": "News  Commentary datasets", "start_pos": 225, "end_pos": 250, "type": "DATASET", "confidence": 0.8382723530133566}]}, {"text": " Table 2:  Correlations on the Europarl dataset.  Adq=Adequacy, Flu=Fluency, Con=Constituent, and  Avg=Average.", "labels": [], "entities": [{"text": "Europarl dataset", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9940360486507416}, {"text": "Adequacy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8305790424346924}, {"text": "Flu=Fluency", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.887397289276123}, {"text": "Avg", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.995462954044342}, {"text": "Average", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.5639635324478149}]}, {"text": " Table 3: Correlations on the News Commentary dataset.", "labels": [], "entities": [{"text": "News Commentary dataset", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.932735006014506}]}, {"text": " Table 4: Correlations on the NIST MT 2003 dataset.", "labels": [], "entities": [{"text": "NIST MT 2003 dataset", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9222520738840103}]}]}