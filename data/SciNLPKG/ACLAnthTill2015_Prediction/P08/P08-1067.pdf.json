{"title": [{"text": "Forest Reranking: Discriminative Parsing with Non-Local Features *", "labels": [], "entities": [{"text": "Forest Reranking", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6419013440608978}]}], "abstractContent": [{"text": "Conventional n-best reranking techniques often suffer from the limited scope of the n-best list, which rules out many potentially good alternatives.", "labels": [], "entities": []}, {"text": "We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses.", "labels": [], "entities": []}, {"text": "Since exact inference is intractable with non-local features , we present an approximate algorithm inspired by forest rescoring that makes discrim-inative training practical over the whole Tree-bank.", "labels": [], "entities": []}, {"text": "Our final result, an F-score of 91.7, out-performs both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9992996454238892}, {"text": "Treebank", "start_pos": 164, "end_pos": 172, "type": "DATASET", "confidence": 0.6315578818321228}]}], "introductionContent": [{"text": "Discriminative reranking has become a popular technique for many NLP problems, in particular, parsing) and machine translation).", "labels": [], "entities": [{"text": "parsing", "start_pos": 94, "end_pos": 101, "type": "TASK", "confidence": 0.9784485101699829}, {"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7734711766242981}]}, {"text": "Typically, this method first generates a list of top-n candidates from a baseline system, and then reranks this n-best list with arbitrary features that are not computable or intractable to compute within the baseline system.", "labels": [], "entities": []}, {"text": "But despite its apparent success, there remains a major drawback: this method suffers from the limited scope of the nbest list, which rules out many potentially good alternatives.", "labels": [], "entities": []}, {"text": "For example 41% of the correct parses were not in the candidates of \u223c30-best parses in).", "labels": [], "entities": []}, {"text": "This situation becomes worse with longer sentences because the number of possible interpretations usually grows exponentially with the * Part of this work was done while I was visiting Institute of Computing Technology, Beijing, and I thank Prof. Qun Liu and his lab for hosting me.", "labels": [], "entities": []}, {"text": "I am also grateful to Dan Gildea and Mark Johnson for inspirations, Eugene Charniak for help with his parser, and Wenbin Jiang for guidance on perceptron averaging.", "labels": [], "entities": [{"text": "perceptron averaging", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.7926329970359802}]}, {"text": "This project was supported by NSF ITR EIA-0205456.", "labels": [], "entities": [{"text": "NSF ITR EIA-0205456", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.8638466795285543}]}, {"text": "local non-local conventional reranking only at the root DP-based discrim.", "labels": [], "entities": []}, {"text": "parsing exact N/A this work: forest-reranking exact on-the-fly: Comparison of various approaches for incorporating local and non-local features.", "labels": [], "entities": []}, {"text": "As a result, we often see very few variations among the n-best trees, for example, 50-best trees typically just represent a combination of 5 to 6 binary ambiguities (since 2 5 < 50 < 2 6 ).", "labels": [], "entities": []}, {"text": "Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space ().", "labels": [], "entities": [{"text": "discriminative parsing", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.6396317183971405}]}, {"text": "However, we miss the benefits of non-local features that are not representable here.", "labels": [], "entities": []}, {"text": "Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and non-local features.", "labels": [], "entities": []}, {"text": "Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope.", "labels": [], "entities": [{"text": "exact search", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.7920606434345245}]}, {"text": "So we propose forest reranking, a technique inspired by forest rescoring) that approximately reranks the packed forest of exponentially many parses.", "labels": [], "entities": []}, {"text": "The key idea is to compute non-local features incrementally from bottom up, so that we can rerank the n-best subtrees at all internal nodes, instead of only at the root node as in conventional reranking (see).", "labels": [], "entities": []}, {"text": "This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing.", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.7106412798166275}]}, {"text": "Although previous work on discriminative parsing has mainly focused on short sentences (\u2264 15 words) (, our work scales to the whole Treebank, where Figure 1: A partial forest of the example sentence.", "labels": [], "entities": [{"text": "discriminative parsing", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6875965744256973}]}, {"text": "we achieved an F-score of 91.7, which is a 19% error reduction from the 1-best baseline, and outperforms both 50-best and 100-best reranking.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.999809205532074}, {"text": "error reduction", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9915481805801392}]}, {"text": "This result is also better than any previously reported systems trained on the Treebank.", "labels": [], "entities": [{"text": "Treebank", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.8903191089630127}]}], "datasetContent": [{"text": "We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank (.", "labels": [], "entities": [{"text": "Penn English Treebank", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.9855927030245463}]}, {"text": "The baseline parser is the Charniak parser, which we modified to output a, and others are from, with simplifications.", "labels": [], "entities": []}, {"text": "packed forest for each sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Forest reranking compared to n-best rerank- ing on sec. 23. The pre-comp. column is for feature  extraction, and training column shows the number  of perceptron iterations that achieved best results on  the dev set, and average time per iteration.", "labels": [], "entities": [{"text": "feature  extraction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7589192688465118}]}, {"text": " Table 4: Comparison of our final results with other  best-performing systems on the whole Section 23.  Types D, G, and S denote discriminative, generative,  and semi-supervised approaches, respectively.", "labels": [], "entities": []}]}