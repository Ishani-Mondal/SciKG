{"title": [{"text": "A Unified Syntactic Model for Parsing Fluent and Disfluent Speech *", "labels": [], "entities": [{"text": "Parsing Fluent and Disfluent Speech", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.8019197821617127}]}], "abstractContent": [{"text": "This paper describes a syntactic representation for modeling speech repairs.", "labels": [], "entities": [{"text": "modeling speech repairs", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.6368988851706187}]}, {"text": "This representation makes use of aright corner transform of syntax trees to produce a tree representation in which speech repairs require very few special syntax rules, making better use of training data.", "labels": [], "entities": []}, {"text": "PCFGs trained on syntax trees using this model achieve high accuracy on the standard Switchboard parsing task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9990293979644775}, {"text": "Switchboard parsing task", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8388320604960123}]}], "introductionContent": [{"text": "Speech repairs occur when a speaker makes a mistake and decides to partially retrace an utterance in order to correct it.", "labels": [], "entities": [{"text": "Speech repairs", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6719080358743668}]}, {"text": "Speech repairs are common in spontaneous speech -one study found 30% of dialogue turns contained repairs and another study found one repair every 4.8 seconds.", "labels": [], "entities": [{"text": "Speech repairs", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.665295422077179}]}, {"text": "Because of the relatively high frequency of this phenomenon, spontaneous speech recognition systems will need to be able to deal with repairs to achieve high levels of accuracy.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7131603211164474}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9957308173179626}]}, {"text": "The speech repair terminology used here follows that of.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8621095418930054}]}, {"text": "A speech repair consists of a reparandum, an interruption point, and the alteration.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 2, "end_pos": 15, "type": "TASK", "confidence": 0.7250506281852722}]}, {"text": "The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced.", "labels": [], "entities": []}, {"text": "The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mistake can begin.", "labels": [], "entities": []}, {"text": "The alteration contains the * This research was supported by NSF CAREER award 0447685.", "labels": [], "entities": [{"text": "NSF CAREER award 0447685", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.6451330184936523}]}, {"text": "The views expressed are not necessarily endorsed by the sponsors.", "labels": [], "entities": []}, {"text": "words that are meant to replace the words in the reparandum.", "labels": [], "entities": []}, {"text": "Recent advances in recognizing spontaneous speech with repairs () have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above.", "labels": [], "entities": []}, {"text": "One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error.", "labels": [], "entities": []}, {"text": "For instance, in the repair . .", "labels": [], "entities": []}, {"text": "a flight to Boston, uh, I mean, to Denver on Friday . .", "labels": [], "entities": []}, {"text": ", there is an exact match of the word 'to' between reparandum and repair, and apart of speech match between the words 'Boston' and 'Denver'.", "labels": [], "entities": []}, {"text": "Another sort of structure in repair is what called the well-formedness rule.", "labels": [], "entities": []}, {"text": "This rule states that the constituent started in the reparandum and repair are ultimately of syntactic types that could be grammatically joined by a conjunction.", "labels": [], "entities": []}, {"text": "For example, in the repair above, the well-formedness rule says that the repair is well formed if the fragment . .", "labels": [], "entities": []}, {"text": "a flight to Boston and to Denver.", "labels": [], "entities": []}, {"text": "In this case the repair is well formed since the conjunction is grammatical, if not meaningful.", "labels": [], "entities": []}, {"text": "The approach described here makes use of a transform on a tree-annotated corpus to build a syntactic model of speech repair which takes advantage of the structure of speech repairs as described above, while also providing a representation of repair structure that more closely adheres to intuitions about what happens when speakers make repairs.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.7289323210716248}]}], "datasetContent": [{"text": "The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the files sw4004.mrg to sw4153.mrg in directory 4 for evaluation, following.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.9692696630954742}]}, {"text": "The input to the system consists of the terminal symbols from the trees in the corpus section mentioned above.", "labels": [], "entities": []}, {"text": "The terminal symbol strings are first pre-processed by stripping punctuation and other non-vocalized terminal symbols, which could not be expected from the output of a speech recognizer.", "labels": [], "entities": []}, {"text": "Crucially, any information about repair is stripped from the input, including partial words, repair symbols 3 , and interruption point information.", "labels": [], "entities": []}, {"text": "While an integrated system for processing and parsing speech may use both acoustic and syntactic information to find repairs, and thus may have access to some of this information about where interruptions occur, this experiment is intended to evaluate the use of the right corner transform and syntactic information on parsing speech repair.", "labels": [], "entities": [{"text": "parsing speech", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.8850226104259491}, {"text": "parsing speech repair", "start_pos": 319, "end_pos": 340, "type": "TASK", "confidence": 0.9370446801185608}]}, {"text": "To make a fair comparison to the CYK baseline of (), the recognizer was given correct part-of-speech tags as input along with words.", "labels": [], "entities": []}, {"text": "The results presented here use two standard metrics for assessing accuracy of transcribed speech with repairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9937164187431335}]}, {"text": "The first metric, Parseval F-measure, takes into account precision and recall of all nonterminal (and non pre-terminal) constituents in a hypothesized tree relative to the gold standard.", "labels": [], "entities": [{"text": "Parseval F-measure", "start_pos": 18, "end_pos": 36, "type": "METRIC", "confidence": 0.6820448637008667}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9991269707679749}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.998873770236969}]}, {"text": "The second metric, EDIT-finding F, measures precision and recall of the words tagged as EDITED in the hypothesized tree relative to those tagged EDITED in the gold standard.", "labels": [], "entities": [{"text": "EDIT-finding F", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.8766917884349823}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.99928218126297}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.998446524143219}]}, {"text": "F score is defined as usual, 2pr/(p + r) for precision p and recall r.", "labels": [], "entities": [{"text": "F score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.984574019908905}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9895920157432556}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9957098960876465}]}, {"text": "The results in show that this system performs comparably to the state of the art in overall parsing accuracy and reasonably well in edit detection.) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words", "labels": [], "entities": [{"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9523969888687134}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9443629384040833}, {"text": "edit detection.", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.8052575588226318}, {"text": "EDIT-F score", "start_pos": 167, "end_pos": 179, "type": "METRIC", "confidence": 0.9102153182029724}]}], "tableCaptions": [{"text": " Table 1: Baseline results are from a standard CYK parser  with binarized grammar. We were unable to find the cor- rect configuration to match the baseline results from Hale  et al. RCT results are on the right-corner transformed  grammar (transformed back to flat treebank-style trees  for scoring purposes). CYK and TAG lines show relevant  results from related work.", "labels": [], "entities": [{"text": "CYK", "start_pos": 310, "end_pos": 313, "type": "DATASET", "confidence": 0.9330640435218811}]}]}