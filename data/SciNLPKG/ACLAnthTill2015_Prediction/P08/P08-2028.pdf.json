{"title": [{"text": "The Good, the Bad, and the Unknown: Morphosyllabic Sentiment Tagging of Unseen Words", "labels": [], "entities": [{"text": "Morphosyllabic Sentiment Tagging of Unseen Words", "start_pos": 36, "end_pos": 84, "type": "TASK", "confidence": 0.7833849440018336}]}], "abstractContent": [{"text": "The omnipresence of unknown words is a problem that any NLP component needs to address in some form.", "labels": [], "entities": []}, {"text": "While there exist many established techniques for dealing with unknown words in the realm of POS-tagging, for example, guessing unknown words' semantic properties is a less-explored area with greater challenges.", "labels": [], "entities": [{"text": "guessing unknown words' semantic properties", "start_pos": 119, "end_pos": 162, "type": "TASK", "confidence": 0.8654253244400024}]}, {"text": "In this paper, we study the semantic field of sentiment and propose five methods for assigning prior sentiment polarities to unknown words based on known sentiment carriers.", "labels": [], "entities": [{"text": "semantic field of sentiment", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6833448857069016}]}, {"text": "Tested on 2000 cases, the methods mirror human judgements closely in three-and two-way polarity classification tasks, and reach accuracies above 63% and 81%, respectively.", "labels": [], "entities": [{"text": "polarity classification tasks", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.8141047159830729}, {"text": "accuracies", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9939368367195129}]}], "introductionContent": [{"text": "One of the first challenges in sentiment analysis is the vast lexical diversity of subjective language.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9704896509647369}]}, {"text": "Gaps in lexical coverage will be a problem for any sentiment classification algorithm that does not have someway of intelligently guessing the polarity of unknown words.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.8844031393527985}]}, {"text": "The problem is exacerbated further by misspellings of known words and POS-tagging errors which are often difficult to distinguish from genuinely unknown words.", "labels": [], "entities": []}, {"text": "This study explores the extent to which it is possible to categorise words which present themselves as unknown, but which may contain known components using morphological, syllabic, and shallow parsing devices.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compiled a dataset of 2000 infrequent words containing hapax legomena from the BNC 3 and \"junk\" entries from the WAC 2006 corpus (Footnote 1).", "labels": [], "entities": [{"text": "BNC 3", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.9495076537132263}, {"text": "WAC 2006 corpus", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.9549205104509989}]}, {"text": "The dataset contains simple, mediumcomplexity, and extreme complex cases covering single words, (non-)hyphenated compounds, nonce forms, and spelling anomalies (e.g. antineo-nazi-initiatives, funny-because-its-true, and s'gonnacostyaguvna).", "labels": [], "entities": []}, {"text": "Three human annotators classified the entries as (+), (-), or (N) (with an optional UNSURE tag) with the following distribution: We report results using all polarities (ALL-POL) and non-neutral polarities (NON-NTR) resulting in average pairwise inter-annotator Kappa scores of.", "labels": [], "entities": []}, {"text": "BNC database and word frequency lists.", "labels": [], "entities": [{"text": "BNC database", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9655872285366058}]}, {"text": "www.kilgarriff.co.uk/bnc-readme.html .40 (ALL-POL) and .74 (NON-NTR), or .48 (ALL-POL) and .83 (NON-NTR) without UNSURE cases.", "labels": [], "entities": [{"text": "NON-NTR", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8430508375167847}, {"text": "UNSURE", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.7443508505821228}]}, {"text": "We used ANN-1's data to adjust the \u03a6 ntr coefficients of individual classifiers, and evaluated the system against both ANN-2 and ANN-3.", "labels": [], "entities": [{"text": "ANN-1's data", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8771916627883911}, {"text": "ANN-2", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.9002123475074768}, {"text": "ANN-3", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.9014577269554138}]}, {"text": "The average scores between ANN-2 and ANN-3 are given in.", "labels": [], "entities": [{"text": "ANN-2", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.8906410932540894}, {"text": "ANN-3", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.5394979119300842}]}, {"text": "Since even human polarity judgements become fuzzier near the neutral/non-neutral boundary due to differing personal degrees of sensitivity towards neutrality (cf. low (N) agreement in Ex.", "labels": [], "entities": []}, {"text": "2; Andreevskaia and Bergler), not all classification errors are equal for classifying a (+) case as (N) is more tolerable than classifying it as (-), for example.", "labels": [], "entities": []}, {"text": "We therefore found it useful to characterise three distinct disagreement classes between human H and machine M encompassing FATAL (H (+) M or H (-) M (+) ), GREEDY (H (N) M (-) or H (N) M (+) ), and The classifiers generally mimic human judgements in that accuracy is much lower in the threeway classification task -a pattern concurring with past observations (cf.;).", "labels": [], "entities": [{"text": "FATAL", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9932664632797241}, {"text": "GREEDY", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9981972575187683}, {"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.998761773109436}, {"text": "threeway classification", "start_pos": 286, "end_pos": 309, "type": "TASK", "confidence": 0.5929386764764786}]}, {"text": "Crucially, FA-TAL errors remain below 10% throughout.", "labels": [], "entities": [{"text": "FA-TAL errors", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.7923043370246887}]}, {"text": "Further advances can be made by fine-tuning the \u03a6 ntr coefficients, and by learning weights for individual classifiers which can currently mask each other and suppress the correct analysis when run collectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average (A)ccuracy, kappa, and error distribution against ANN-2 and ANN-3", "labels": [], "entities": [{"text": "Average (A)ccuracy", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.7416532516479493}, {"text": "kappa", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9624366164207458}, {"text": "error distribution", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.898825466632843}, {"text": "ANN-3", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.7089856863021851}]}]}