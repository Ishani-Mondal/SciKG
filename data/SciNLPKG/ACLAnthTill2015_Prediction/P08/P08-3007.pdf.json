{"title": [{"text": "Arabic Language Modeling with Finite State Transducers", "labels": [], "entities": [{"text": "Arabic Language Modeling", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5310376485188802}]}], "abstractContent": [{"text": "In morphologically rich languages such as Arabic, the abundance of word forms resulting from increased morpheme combinations is significantly greater than for languages with fewer inflected forms (Kirchhoff et al., 2006).", "labels": [], "entities": []}, {"text": "This exacerbates the out-of-vocabulary (OOV) problem.", "labels": [], "entities": []}, {"text": "Test set words are more likely to be unknown, limiting the effectiveness of the model.", "labels": [], "entities": []}, {"text": "The goal of this study is to use the regularities of Arabic inflectional morphology to reduce the OOV problem in that language.", "labels": [], "entities": [{"text": "OOV", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.7475448250770569}]}, {"text": "We hope that success in this task will result in a decrease in word error rate in Arabic automatic speech recognition.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.6858571370442709}, {"text": "Arabic automatic speech recognition", "start_pos": 82, "end_pos": 117, "type": "TASK", "confidence": 0.6313856318593025}]}], "introductionContent": [{"text": "The task of language modeling is to predict the next word in a sequence of words (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7175867706537247}]}, {"text": "Predicting words that have not yet been seen is the main obstacle (, and is called the Out of Vocabulary (OOV) problem.", "labels": [], "entities": []}, {"text": "In morphologically rich languages, the OOV problem is worsened by the increased number of morpheme combinations. and Geutner (1995) approached this problem in German, finding that language models built on decomposed words reduce the OOV rate of a test set.", "labels": [], "entities": [{"text": "OOV", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.8612191677093506}, {"text": "OOV rate", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9840799868106842}]}, {"text": "In, Turkish words are split into syllables for language modeling, also reducing the OOV rate (but not improving * This work was supported by a student-faculty fellowship from the AFRL/Dayton Area Graduate Studies Insititute, and worked on in partnership with Ray Slyh and Tim Anderson of the Air Force Research Labs. WER).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.6740758270025253}, {"text": "OOV rate", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9904518723487854}, {"text": "AFRL/Dayton Area Graduate Studies Insititute", "start_pos": 179, "end_pos": 223, "type": "DATASET", "confidence": 0.8823920488357544}, {"text": "Air Force Research Labs. WER", "start_pos": 292, "end_pos": 320, "type": "DATASET", "confidence": 0.8855971574783326}]}, {"text": "Morphological decomposition is also used to boost language modeling scores in Korean () and).", "labels": [], "entities": [{"text": "Morphological decomposition", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7812819182872772}, {"text": "language modeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6952314972877502}]}, {"text": "We approach the processing of Arabic morphology, both inflectional and derivational, with finite state machines (FSMs).", "labels": [], "entities": [{"text": "processing of Arabic morphology", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.6607590690255165}]}, {"text": "We use a technique that produces many morphological analyses for each word, retaining information about possible stems, affixes, root letters, and templates.", "labels": [], "entities": []}, {"text": "We build our language models on the morphemes generated by the analyses.", "labels": [], "entities": []}, {"text": "The FSMs generate spurious analyses.", "labels": [], "entities": [{"text": "FSMs", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6614257097244263}]}, {"text": "That is, although a word out of context may have several morphological analyses, in context only one such analysis is correct.", "labels": [], "entities": []}, {"text": "We expect that any incorrect morphemes that are generated will not affect the predictions of the model, because they will be rare, and the language model introduces bias towards frequent morphemes.", "labels": [], "entities": []}, {"text": "Although many words in a test set may not have occurred in a training set, the morphemes that makeup that word likely will have occurred.", "labels": [], "entities": []}, {"text": "Using many decompositions to describe each word sets apart this study from other similar studies, including those by and.", "labels": [], "entities": []}, {"text": "This study differs from previous research on Arabic language modeling and Arabic automatic speech recognition in two other ways.", "labels": [], "entities": [{"text": "Arabic language modeling", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6334797739982605}, {"text": "Arabic automatic speech recognition", "start_pos": 74, "end_pos": 109, "type": "TASK", "confidence": 0.5920931100845337}]}, {"text": "To promote crossdialectal use of the techniques, we use properties of Arabic morphology that we assume to be common to many dialects.", "labels": [], "entities": []}, {"text": "Also, we treat morphological analysis and vowel prediction with a single solution.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7501134872436523}, {"text": "vowel prediction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7381046414375305}]}, {"text": "An overview of Arabic morphology is given in Section 2.", "labels": [], "entities": []}, {"text": "A description of the finite state machine process used to decompose the Arabic words into morphemes follows in Section 3.", "labels": [], "entities": []}, {"text": "The experimental language model training procedure and the procedures for training two baseline language models are discussed in Section 4.", "labels": [], "entities": []}, {"text": "We evaluate all three models using average negative log probability and coverage statistics, discussed in Section 5.", "labels": [], "entities": [{"text": "coverage", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9474768042564392}]}], "datasetContent": [{"text": "We extract the vocabulary of the training data, and compile the word lattices as described in Section 3.", "labels": [], "entities": []}, {"text": "The union of all decompositions (a lattice) for each individual word is stored separately.", "labels": [], "entities": []}, {"text": "For each sentence of training data, we concatenate the lattices representing each word in that sentence.", "labels": [], "entities": []}, {"text": "We use SRILM) to calculate the posterior expected n-gram count for morpheme sequences up to 4-grams in the sentence-long lattice.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.808076798915863}]}, {"text": "The estimated frequency of an n-gram N is calculated as the number of occurrences of that n-gram in the lattice, divided by the number of paths in the lattice.", "labels": [], "entities": []}, {"text": "This is true so long as the paths are equally weighted; at this point in our study, this is the case.", "labels": [], "entities": []}, {"text": "We merge the n-gram counts overall sentences in all of the training files.", "labels": [], "entities": []}, {"text": "Next, we estimate a language model based on the n-gram counts, using only the 64000 most frequent morphemes, since we expect this vocabulary size maybe a limitation of our ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.9170575737953186}]}, {"text": "Also, by limiting the vocabulary size of all of our models (including the baseline models described below), we can make a fairer comparison among the models.", "labels": [], "entities": []}, {"text": "We use Good-Turing smoothing to account for unseen morphemes, all of which are replaced with a single \"unknown\" symbol.", "labels": [], "entities": []}, {"text": "In later work, we will apply our LM statistics to the lattices, and recalculate the path weights and estimated counts.", "labels": [], "entities": []}, {"text": "In this study, the paths remain equally weighted.", "labels": [], "entities": []}, {"text": "We evaluate this model, which we call FSM-LM, with respect to two baseline models.", "labels": [], "entities": [{"text": "FSM-LM", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.7393947839736938}]}, {"text": "For each model, the test set undergoes the same manipulation as the train set; words are left alone for the WORD model, split into a single segmentation each for the AFFIX model, or their FSM decompositions are concatenated.", "labels": [], "entities": []}, {"text": "Language models are often compared using the perplexity statistic: Perplexity represents the average branching factor of a model; that is, at each point in the test set, we calculate the entropy of the model.", "labels": [], "entities": []}, {"text": "Therefore, a lower perplexity is desired.", "labels": [], "entities": []}, {"text": "In the AFFIX and FSM-LM models, each word is split into several parts.", "labels": [], "entities": [{"text": "AFFIX", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8189298510551453}, {"text": "FSM-LM", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.7631287574768066}]}, {"text": "Therefore, the value 1 n would be approximately three times smaller for these models, giving them an advantage.", "labels": [], "entities": []}, {"text": "To make a more even comparison, we calculate the geometric mean of the n-gram transition probabilities, dividing by the number of words in the test set, not morphemes, as in.", "labels": [], "entities": []}, {"text": "The log of this equation is: where n is the number of morphemes or words in the test set, depending on the model, and N is the number of words in the test set, and log P (x i |x i\u22123 i\u22121 ) is the log probability of the item xi given the 3-item history (calculated in base 10, as this is how the SRILM Toolkit is implemented).", "labels": [], "entities": [{"text": "SRILM Toolkit", "start_pos": 294, "end_pos": 307, "type": "DATASET", "confidence": 0.8025137782096863}]}, {"text": "Again, we are looking fora low score.", "labels": [], "entities": []}, {"text": "In the FSM-LM, each test sentence is represented by a lattice of paths.", "labels": [], "entities": [{"text": "FSM-LM", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.8408200144767761}]}, {"text": "To determine the negative log probability of the sentence, we score all paths of the sentence according to the equations above, and record the maximum probability.", "labels": [], "entities": []}, {"text": "This reflects the likely procedure we would use in implementing this model within an ASR task.", "labels": [], "entities": [{"text": "ASR task", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.9383311569690704}]}, {"text": "We see in that the average negative log probability of the FSM-LM is lower than that of either the WORD or AFFIX model.", "labels": [], "entities": [{"text": "FSM-LM", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.5107296109199524}]}, {"text": "The average across 10 folds reflects the pattern of scores for each fold.", "labels": [], "entities": []}, {"text": "We conclude from this that the FSM model of predicting morphemes is more effective thanor more conservatively, at least as effective as -a static decomposition, as in the AFFIX model.", "labels": [], "entities": [{"text": "FSM", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.5374475121498108}, {"text": "predicting morphemes", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8812295794487}]}, {"text": "Furthermore, we have successfully reproduced the results of and, among others, that modeling Arabic with morphemes is more effective than modeling with whole word forms.", "labels": [], "entities": []}, {"text": "We also calculate the coverage of each model: the percentage of units in the test set that are given probabilities in the language model.", "labels": [], "entities": [{"text": "coverage", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9597443342208862}]}, {"text": "For the FSM model, only the morphemes in the best path are counted.", "labels": [], "entities": [{"text": "FSM", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.5130998492240906}]}, {"text": "The coverage results are reported in as the average coverage over the 10 folds.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9925336837768555}, {"text": "coverage", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9618563652038574}]}, {"text": "Both the AF-FIX and FSM-LM models showed improved coverage as compared to the WORD model, as expected.", "labels": [], "entities": [{"text": "AF-FIX", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.5512498617172241}, {"text": "FSM-LM", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.6963461637496948}, {"text": "coverage", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9901503324508667}]}, {"text": "This means that we reduce the OOV problem by using morphemes instead of whole words.", "labels": [], "entities": [{"text": "OOV", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.4851994812488556}]}, {"text": "The AF-FIX model has the best coverage of unigrams because only new stems, not new affixes, are proposed in the test set.", "labels": [], "entities": [{"text": "AF-FIX", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.6225540637969971}]}, {"text": "That is, the same fixed set of affixes are used to decompose the test set as the train set, however, unseem stems may appear.", "labels": [], "entities": []}, {"text": "In the FSM-LM, there are no restrictions on the affixes, therefore, unseen affixes may appear in the test set, as well as new stems, lowering the unigram coverage of the test set.", "labels": [], "entities": [{"text": "FSM-LM", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.8366049528121948}]}, {"text": "For larger n-grams, however, the FSM-LM model has the best coverage.", "labels": [], "entities": [{"text": "FSM-LM", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.5687009692192078}]}, {"text": "This is due to keeping all decompositions until test time, then allowing the language model to define the most likely sequences, rather than specifying a single decomposition for each word.", "labels": [], "entities": []}, {"text": "A 4-gram of words will tend to cover more context than a 4-gram of morphemes; therefore, the word 4-grams will exhibit more sparsity than the morpheme 4-grams.", "labels": [], "entities": []}, {"text": "We compare, fora single train-  test fold, how lower order n-grams compare among the models.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We find that for lower-order n-grams, the word model performs best.", "labels": [], "entities": []}, {"text": "As the n-grams get larger, the sparsity problem favors the FSM-LM, which has the best overall score of all models shown.", "labels": [], "entities": [{"text": "FSM-LM", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.6204092502593994}]}, {"text": "Apparently, the frequencies of 3-and 4-grams are not big enough to make a big difference in the evaluation.", "labels": [], "entities": []}, {"text": "This is likely due to the small size of our corpus, and we expect the result would change if we were to use all of the TDT4 corpus, rather than a 100 file portion of the corpus.", "labels": [], "entities": [{"text": "TDT4 corpus", "start_pos": 119, "end_pos": 130, "type": "DATASET", "confidence": 0.9747109711170197}]}], "tableCaptions": [{"text": " Table 2: Average negative log probability and coverage  results for one experimental language model (FSM-LM)  and two baseline language models. Results are averages  over 10 folds.", "labels": [], "entities": [{"text": "coverage", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.977977991104126}, {"text": "FSM-LM", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.9142324328422546}]}, {"text": " Table 3: Comparison of n-gram orders across language  model types.", "labels": [], "entities": []}]}