{"title": [{"text": "A Discriminative Latent Variable Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.8683038353919983}]}], "abstractContent": [{"text": "Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7109279483556747}]}, {"text": "We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.", "labels": [], "entities": []}, {"text": "We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discrimina-tive and globally optimised.", "labels": [], "entities": []}, {"text": "Results show that accounting for multiple derivations does indeed improve performance.", "labels": [], "entities": []}, {"text": "Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) has seen a resurgence in popularity in recent years, with progress being driven by a move to phrase-based and syntax-inspired approaches.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8673507670561472}]}, {"text": "Progress within these approaches however has been less dramatic.", "labels": [], "entities": []}, {"text": "We believe this is because these frequency count based 1 models cannot easily incorporate non-independent and overlapping features, which are extremely useful in describing the translation process.", "labels": [], "entities": []}, {"text": "Discriminative models of translation can include such features without making assumptions of independence or explicitly modelling their interdependence.", "labels": [], "entities": []}, {"text": "However, while discriminative models promise much, they have not been shown to deliver significant gains We class approaches using minimum error rate training frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features.", "labels": [], "entities": []}, {"text": "We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9940317273139954}]}, {"text": "These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on the type of system.", "labels": [], "entities": []}, {"text": "Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist.", "labels": [], "entities": []}, {"text": "Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation.", "labels": [], "entities": []}, {"text": "However, doing so exactly is NP-complete.", "labels": [], "entities": []}, {"text": "For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely, or else ignore the problem and treat derivations as translations (;.", "labels": [], "entities": []}, {"text": "In this paper we directly address the problem of spurious ambiguity in discriminative models.", "labels": [], "entities": []}, {"text": "We use asynchronous context free grammar (SCFG) translation system, a model which has yielded state-of-the-art results on many translation tasks.", "labels": [], "entities": [{"text": "context free grammar (SCFG) translation", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.6394896166665214}]}, {"text": "We present two main contributions.", "labels": [], "entities": []}, {"text": "First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences.", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9573567509651184}]}, {"text": "This model maximises the conditional likelihood of the data, p(e|f ), where e and fare the English and foreign sentences, respectively.", "labels": [], "entities": []}, {"text": "Our estimation method is theoretically sound, avoiding the biases of the heuristic relative frequency estimates (.", "labels": [], "entities": []}, {"text": "Second, within this framework, we model the derivation, d, as a latent variable, p(e, d|f ), which is marginalised out in training and decoding.", "labels": [], "entities": []}, {"text": "We show empirically that this treatment results in significant improvements over a maximum-derivation model.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we list the challenges that discriminative SMT must face above and beyond the current systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8718832731246948}]}, {"text": "We situate our work, and previous work, on discriminative systems in this context.", "labels": [], "entities": []}, {"text": "We present our model in Section 3, including our means of training and decoding.", "labels": [], "entities": []}, {"text": "Section 4 reports our experimental setup and results, and finally we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model evaluation was motivated by the following questions: (1) the effect of maximising translations rather than derivations in training and decoding; (2) whether a regularised model performs better than a maximum likelihood model; (3) how the performance of our model compares with a frequency count based hierarchical system; and (4) how translation performance scales with the number of training examples.", "labels": [], "entities": []}, {"text": "We performed all of our experiments on the Europarl V2 French-English parallel corpus.", "labels": [], "entities": [{"text": "Europarl V2 French-English parallel corpus", "start_pos": 43, "end_pos": 85, "type": "DATASET", "confidence": 0.9719440817832947}]}, {"text": "The training data was created by filtering the full corpus for all the French sentences between five and fifteen words in length, resulting in 170K sentence pairs.", "labels": [], "entities": []}, {"text": "These limits were chosen as a compromise between experiment turnaround time and leaving a large enough corpus to obtain indicative results.", "labels": [], "entities": []}, {"text": "The development and test data was taken from the 2006 NAACL and 2007 ACL workshops on machine translation, also filtered for sentence length.", "labels": [], "entities": [{"text": "NAACL and 2007 ACL", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.7392332702875137}, {"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7950402200222015}]}, {"text": "Tuning of the regularisation parameter and MERT training of the benchmark models was performed on dev2006, while the test set was the concatenation of devtest2006, test2006 and test2007, amounting to 315 development and 1164 test sentences.", "labels": [], "entities": [{"text": "MERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9959142804145813}]}, {"text": "Here we focus on evaluating our model's basic ability to learn a conditional distribution from simple binary features, directly comparable to those currently employed in frequency count models.", "labels": [], "entities": []}, {"text": "As such, our base model includes a single binary identity feature per-rule, equivalent to the p(e|f ) parameters defined on each rule in standard models.", "labels": [], "entities": []}, {"text": "As previously noted, our model must be able to derive the reference sentence from the source for it to be included in training.", "labels": [], "entities": []}, {"text": "For both our discriminative and benchmark (Hiero) we extracted our grammar on the 170K sentence corpus using the approach described in, resulting in 7.8 million rules.", "labels": [], "entities": [{"text": "170K sentence corpus", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.9150003989537557}]}, {"text": "The discriminative model was then trained on the training partition, however only 130K of the sentences were used as the model could not produce a derivation of the reference for the remaining sentences.", "labels": [], "entities": []}, {"text": "There were many grammar rules that the discriminative model did not observe in a reference derivation, and thus could not assign their feature a positive weight.", "labels": [], "entities": []}, {"text": "While the benchmark model has a positive count for every rule (7.8M), the discriminative model only observes 1.7M rules in actual reference derivations.", "labels": [], "entities": []}, {"text": "illustrates the massive ambiguity present in the training data, with fifteen word sentences averaging over 70M reference derivations.", "labels": [], "entities": []}, {"text": "Performance is evaluated using cased BLEU4 score on the test set.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9687362909317017}]}, {"text": "Although there is no direct relationship between BLEU and likelihood, it provides a rough measure for comparing performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9993031024932861}, {"text": "likelihood", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9833571910858154}]}, {"text": "shows the impact of accounting for derivational ambiguity in training and decoding.", "labels": [], "entities": []}, {"text": "There are two options for training, we could use our latent variable model and optimise the probability of all derivations of the reference translation, or choose a single derivation that yields the reference and optimise its probability alone.", "labels": [], "entities": []}, {"text": "The second option raises the difficult question of which one, of the thousands available, we should choose?", "labels": [], "entities": []}, {"text": "We use the derivation which contains the most rules.", "labels": [], "entities": []}, {"text": "The intuition is that small rules are likely to appear more frequently, and thus generalise better to a test set.", "labels": [], "entities": []}, {"text": "In decoding we can search for the maximum probability derivation, which is the standard practice in SMT, or for the maximum probability translation which is what we actually want from our model, i.e. the best translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9933182001113892}]}], "tableCaptions": [{"text": " Table 1. A comparison on the impact of accounting for all  derivations in training and decoding (development set).", "labels": [], "entities": []}, {"text": " Table 2. Comparison of the susceptibility to degenerate  solutions for a ML and MAP optimised model, using a sim- ple grammar with one parameter per rule and a monotone  glue rule: X \u2192 X 1 X 2 , X 1 X 2", "labels": [], "entities": []}]}