{"title": [{"text": "Unsupervised Translation Induction for Chinese Abbreviations using Monolingual Corpora", "labels": [], "entities": [{"text": "Unsupervised Translation Induction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6615442931652069}]}], "abstractContent": [{"text": "Chinese abbreviations are widely used in modern Chinese texts.", "labels": [], "entities": []}, {"text": "Compared with English abbreviations (which are mostly acronyms and truncations), the formation of Chinese abbreviations is much more complex.", "labels": [], "entities": []}, {"text": "Due to the richness of Chinese abbreviations, many of them may not appear in available parallel corpora, in which case current machine translation systems simply treat them as unknown words and leave them untranslated.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel unsupervised method that automatically extracts the relation between a full-form phrase and its abbreviation from monolingual corpora, and induces translation entries for the abbreviation by using its full-form as abridge.", "labels": [], "entities": []}, {"text": "Our method does not require any additional annotated data other than the data that a regular translation system uses.", "labels": [], "entities": []}, {"text": "We integrate our method into a state-of-the-art baseline translation system and show that it consistently improves the performance of the baseline system on various NIST MT test sets.", "labels": [], "entities": [{"text": "NIST MT test sets", "start_pos": 165, "end_pos": 182, "type": "DATASET", "confidence": 0.7344745695590973}]}], "introductionContent": [{"text": "The modern Chinese language is a highly abbreviated one due to the mixed use of ancient singlecharacter words with modern multi-character words and compound words.", "labels": [], "entities": []}, {"text": "According to, approximately 20% of sentences in atypical news article have abbreviated words in them.", "labels": [], "entities": []}, {"text": "Abbreviations have become even more popular along with the development of Internet media (e.g., online chat, weblog, newsgroup, and so on).", "labels": [], "entities": []}, {"text": "While English words are normally abbreviated by either their first letters (i.e. acronyms) or via truncation, the formation of Chinese abbreviations is much more complex.", "labels": [], "entities": []}, {"text": "shows two examples for Chinese abbreviations.", "labels": [], "entities": []}, {"text": "Clearly, an abbreviated form of a word can be obtained by selecting one or more characters from this word, and the selected characters can beat any position in the word.", "labels": [], "entities": []}, {"text": "In an extreme case, there are even re-ordering between a full-form phrase and its abbreviation.", "labels": [], "entities": []}, {"text": "While the research in statistical machine translation (SMT) has made significant progress, most SMT systems () rely on parallel corpora to extract translation entries.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.8092695027589798}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.988459050655365}]}, {"text": "The richness and complexness of Chinese abbreviations imposes challenges to the SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9915980100631714}]}, {"text": "In particular, many Chinese abbreviations may not appear in available parallel corpora, in which case current SMT systems treat them as unknown words and leave them untranslated.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9806932806968689}]}, {"text": "This affects the translation quality significantly.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9630062580108643}]}, {"text": "To be able to translate a Chinese abbreviation that is unseen in available parallel corpora, one may annotate more parallel data.", "labels": [], "entities": []}, {"text": "However, this is very expensive as there are too many possible abbreviations and new abbreviations are constantly created.", "labels": [], "entities": []}, {"text": "Another approach is to transform the abbreviation into its full-form for which the current SMT system knows how to translate.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9746916890144348}]}, {"text": "For example, if the baseline system knows that the translation for \" \" is \"Hong Kong Governor\", and it also knows that \" \" is an abbreviation of \" \" , then it can translate \"\" to \"Hong Kong Governor\".", "labels": [], "entities": []}, {"text": "Even if an abbreviation has been seen in parallel corpora, it may still be worth to consider its fullform phrase as an additional alternative to the abbreviation since abbreviated words are normally semantically ambiguous, while its full-form contains more context information that helps the MT system choose aright translation for the abbreviation.", "labels": [], "entities": [{"text": "MT", "start_pos": 292, "end_pos": 294, "type": "TASK", "confidence": 0.925116240978241}]}, {"text": "Conceptually, the approach of translating an abbreviation by using its full-form as abridge involves four components: identifying abbreviations, learning their full-forms, inducing their translations, and integrating the abbreviation translations into the baseline SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 265, "end_pos": 268, "type": "TASK", "confidence": 0.9788676500320435}]}, {"text": "None of these components is trivial to realize.", "labels": [], "entities": []}, {"text": "For example, for the first two components, we may need manually annotated data that tags an abbreviation with its full-form.", "labels": [], "entities": []}, {"text": "We also need to make sure that the baseline system has at least one valid translation for the full-form phrase.", "labels": [], "entities": []}, {"text": "On the other hand, integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation (WSD) into SMT systems: different ways of integration lead to conflicting conclusions on whether WSD helps MT performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9787638187408447}, {"text": "word sense disambiguation (WSD)", "start_pos": 146, "end_pos": 177, "type": "TASK", "confidence": 0.7576580991347631}, {"text": "SMT", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.8720642924308777}, {"text": "MT", "start_pos": 279, "end_pos": 281, "type": "TASK", "confidence": 0.9955472946166992}]}, {"text": "In this paper, we present an unsupervised approach to translate Chinese abbreviations.", "labels": [], "entities": [{"text": "translate Chinese abbreviations", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.9006127119064331}]}, {"text": "Our approach exploits the data co-occurrence phenomena and does not require any additional annotated data except the parallel and monolingual corpora that the baseline SMT system uses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.9787276387214661}]}, {"text": "Moreover, our approach integrates the abbreviation translation component into the baseline system in a natural way, and thus is able to make use of the minimum-error-rate training to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system.", "labels": [], "entities": [{"text": "abbreviation translation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.685551106929779}]}, {"text": "We carryout experiments on a state-of-the-art SMT system, i.e.,, and show that the abbreviation translations consistently improve the translation performance (in terms of BLEU ()) on various NIST MT test sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9934624433517456}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9989141225814819}, {"text": "NIST MT test sets", "start_pos": 191, "end_pos": 208, "type": "DATASET", "confidence": 0.7599171102046967}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Full-abbreviation Relation Extraction Precision", "labels": [], "entities": [{"text": "Relation Extraction Precision", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8395953377087911}]}, {"text": " Table 7: MT Performance measured by BLEU Score", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9781020283699036}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9955961108207703}]}, {"text": " Table 8: Weights obtained by MERT", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9398485422134399}, {"text": "MERT", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.38169991970062256}]}]}