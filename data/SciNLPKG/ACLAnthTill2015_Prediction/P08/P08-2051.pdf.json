{"title": [{"text": "Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.7802008986473083}, {"text": "Human Evaluation of Extractive Meeting Summaries", "start_pos": 30, "end_pos": 78, "type": "TASK", "confidence": 0.6073413640260696}]}], "abstractContent": [{"text": "Automatic summarization evaluation is critical to the development of summarization systems.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9539965391159058}, {"text": "summarization", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.9857935309410095}]}, {"text": "While ROUGE has been shown to correlate well with human evaluation for content match in text summa-rization, there are many characteristics in multiparty meeting domain, which may pose potential problems to ROUGE.", "labels": [], "entities": []}, {"text": "In this paper, we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summariza-tion.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9965630173683167}]}, {"text": "Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries.", "labels": [], "entities": [{"text": "correlation", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9669640064239502}]}], "introductionContent": [{"text": "Meeting summarization has drawn an increasing attention recently; therefore a study on the automatic evaluation metrics for this task is timely.", "labels": [], "entities": [{"text": "Meeting summarization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8934084177017212}]}, {"text": "Automatic evaluation helps to advance system development and avoids the labor-intensive and potentially inconsistent human evaluation.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7031976878643036}]}, {"text": "ROUGE) has been widely used for summarization evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9571686387062073}, {"text": "summarization evaluation", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.9683791100978851}]}, {"text": "In the news article domain, ROUGE scores have been shown to be generally highly correlated with human evaluation in content match).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9851710200309753}]}, {"text": "However, there are many differences between written texts (e.g., news wire) and spoken documents, especially in the meeting domain, for example, the presence of disfluencies and multiple speakers, and the lack of structure in spontaneous utterances.", "labels": [], "entities": []}, {"text": "The question of whether ROUGE is a good metric for meeting summarization is unclear.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9917464256286621}, {"text": "meeting summarization", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7541587054729462}]}, {"text": "() have reported that ROUGE-1 (unigram match) scores have low correlation with human evaluation in meetings.", "labels": [], "entities": [{"text": "ROUGE-1 (unigram match)", "start_pos": 22, "end_pos": 45, "type": "METRIC", "confidence": 0.7361598491668702}]}, {"text": "In this paper we investigate the correlation between ROUGE and human evaluation of extractive meeting summaries and focus on two issues specific to the meeting domain: disfluencies and multiple speakers.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.5841147303581238}]}, {"text": "Both human and system generated summaries are used.", "labels": [], "entities": []}, {"text": "Our analysis shows that by integrating meeting characteristics into ROUGE settings, better correlation can be achieved between the ROUGE scores and human evaluation based on Spearman's rho in the meeting domain.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.8269603252410889}]}], "datasetContent": [{"text": "ROUGE) measures the n-gram match between system generated summaries and human summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9334789514541626}]}, {"text": "In most of this study, we used the same options in ROUGE as in the DUC summarization evaluation, and modify the input to ROUGE to account for the following two phenomena.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9147201776504517}, {"text": "DUC summarization evaluation", "start_pos": 67, "end_pos": 95, "type": "DATASET", "confidence": 0.7593252658843994}]}, {"text": "\u2022 Disfluencies Meetings contain spontaneous speech with many disfluencies, such as filled pauses (uh, um), discourse markers (e.g., I mean, you know), repetitions, corrections, and incomplete sentences.", "labels": [], "entities": []}, {"text": "There have been efforts on the study of the impact of disfluencies on summarization techniques () and human readability (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9912206530570984}]}, {"text": "However, it is not clear whether disfluencies impact automatic evaluation of extractive meeting summarization.", "labels": [], "entities": [{"text": "extractive meeting summarization", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.7360800703366598}]}, {"text": "Since we use extractive summarization, summary sentences may contain difluencies.", "labels": [], "entities": []}, {"text": "We hand annotated the transcripts for the 6 meetings and marked the disfluencies such that we can remove them to obtain cleaned up sentences for those selected summary sentences.", "labels": [], "entities": []}, {"text": "To study the impact of disfluencies, we run ROUGE using two different inputs: summaries based on the original transcription, and the summaries with disfluencies removed.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.641676127910614}]}, {"text": "\u2022 Speaker information The existence of multiple speakers in meetings raises questions about the evaluation method.", "labels": [], "entities": []}, {"text": "() considered some location constrains in meeting summarization evaluation, which utilizes speaker information to some extent.", "labels": [], "entities": [{"text": "meeting summarization evaluation", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.7883236606915792}]}, {"text": "In this study we use the data in separate channels for each speaker and thus have the speaker information available for each sentence.", "labels": [], "entities": []}, {"text": "We associate the speaker ID with each word, treat them together as anew 'word' in the input to ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.6790685653686523}]}, {"text": "Five human subjects (all undergraduate students in Computer Science) participated inhuman evaluation.", "labels": [], "entities": []}, {"text": "In total, there are 20 different summaries for each of the 6 test meetings: 6 human-generated, 4 system-generated, and their corresponding ones with disfluencies removed.", "labels": [], "entities": []}, {"text": "We assigned 4 summaries with different configurations to each human subject: human vs. system generated summaries, with or without disfluencies.", "labels": [], "entities": []}, {"text": "Each human evaluated 24 summaries in total, for the 6 test meetings.", "labels": [], "entities": []}, {"text": "For each summary, the human subjects were asked to rate the following statements using a scale of 1-5 according to the extent of their agreement with them.", "labels": [], "entities": []}, {"text": "\u2022 S1: The summary reflects the discussion flow in the meeting very well.", "labels": [], "entities": []}, {"text": "\u2022 S2: Almost all the important topic points of the meeting are represented.", "labels": [], "entities": []}, {"text": "\u2022 S3: Most of the sentences in the summary are relevant to the original meeting.", "labels": [], "entities": []}, {"text": "\u2022 S4: The information in the summary is not redundant.", "labels": [], "entities": []}, {"text": "\u2022 S5: The relationship between the importance of each topic in the meeting and the amount of summary space given to that topic seems appropriate.", "labels": [], "entities": []}, {"text": "\u2022 S6: The relationship between the role of each speaker and the amount of summary speech selected for that speaker seems appropriate.", "labels": [], "entities": []}, {"text": "\u2022 S7: Some sentences in the summary convey the same meaning.", "labels": [], "entities": []}, {"text": "\u2022 S8: Some sentences are not necessary (e.g., in terms of importance) to be included in the summary.", "labels": [], "entities": []}, {"text": "\u2022 S9: The summary is helpful to someone who wants to know what are discussed in the meeting.", "labels": [], "entities": []}, {"text": "These statements are an extension of those used in () for human evaluation of meeting summaries.", "labels": [], "entities": [{"text": "human evaluation of meeting summaries", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.5327282607555389}]}, {"text": "The additional ones we added were designed to account for the discussion flow in the meetings.", "labels": [], "entities": []}, {"text": "Some of the statements above are used to measure similar aspects, but from different perspectives, such as S5 and S6, S4 and S7.", "labels": [], "entities": []}, {"text": "This may reduce some accidental noise inhuman evaluation.", "labels": [], "entities": []}, {"text": "We grouped these statements into 4 categories: Informative Structure (IS): S1, S5 and S6; Informative Coverage (IC): S2 and S9; Informative Relevance (IRV): S3 and S8; and Informative Redundancy (IRD): S4 and S7.", "labels": [], "entities": []}, {"text": "Original ROUGE Score Similar to (), we also use Spearman's rank coefficient (rho) to investigate the correlation between ROUGE and human evaluation.", "labels": [], "entities": [{"text": "Spearman's rank coefficient (rho)", "start_pos": 48, "end_pos": 81, "type": "METRIC", "confidence": 0.7406123195375715}]}, {"text": "We have 36 human summaries and 24 system summaries for the 6 meetings in our study.", "labels": [], "entities": []}, {"text": "For each of the human summaries, the ROUGE scores are generated using the other 5 human summaries as references.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9953814744949341}]}, {"text": "For system generated summaries, we calculate the ROUGE score using 5 human references, and then obtain the average from 6 such setups.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9807930290699005}]}, {"text": "The correlation results are presented in.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9577137231826782}]}, {"text": "In addition to the overall average for human evaluation (H AVG), we calculated the average score for each evaluation category (see Section 3.3).", "labels": [], "entities": [{"text": "H AVG)", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.8398113449414571}]}, {"text": "For ROUGE evaluation, we chose the F-measure for R-1 (unigram) and R-SU4 (skip-bigram with maximum gap length of 4), which is based on our observation that other scores in ROUGE are always highly correlated (rho>0.9) to either of them for this task.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9958361387252808}]}, {"text": "We compute the correlation separately for the human and system summaries in order to avoid the impact due to the inherent difference between the two different summaries.", "labels": [], "entities": []}, {"text": "We can see that R-SU4 obtains a higher correlation with human evaluation than R-1 on the whole, but still very low, which is consistent with the previous conclusion from).", "labels": [], "entities": []}, {"text": "Among the four categories, better correlation is achieved for information structure (IS) and information coverage (IC) compared to the other two categories.", "labels": [], "entities": [{"text": "correlation", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.990334689617157}, {"text": "information coverage (IC)", "start_pos": 93, "end_pos": 118, "type": "METRIC", "confidence": 0.6399258673191071}]}, {"text": "This is consistent with what ROUGE is designed for, \"recall oriented understudy gisting evaluation\" -we expect it to model IS and IC well by ngram and skip-bigram matching but not relevancy (IRV) and redundancy (IRD) effectively.", "labels": [], "entities": []}, {"text": "In addition, we found low correlation on system generated summaries, suggesting it is more challenging to evaluate those summaries both by humans and the automatic metrics.", "labels": [], "entities": []}, {"text": "shows the correlation results between ROUGE (R-SU4) and human evaluation on the original and cleaned up summaries respectively.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9983319640159607}]}, {"text": "For human summaries, after removing disfluencies, the correlation between ROUGE and human evaluation improves on the whole, but degrades on information structure (IS) and information coverage (IC) categories.", "labels": [], "entities": [{"text": "summaries", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.8347731828689575}, {"text": "ROUGE", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9515682458877563}]}, {"text": "However, for system summaries, there is a significant gain of correlation on those two evaluation categories, even though no improvement on the overall average score.", "labels": [], "entities": [{"text": "correlation", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9805770516395569}]}, {"text": "Our hypothesis for this is that removing disfluencies helps remove the noise in the system generated summaries and make them more easily to be evaluated by human and machines.", "labels": [], "entities": []}, {"text": "In contrast, the human created summaries have better quality in terms of the information content and may not suffer as much from the disfluencies contained in the summary.: Effect of disfluencies on the correlation between R-SU4 and human evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1.  In addition to the overall average for human evaluation  (H AVG), we calculated the average score for each evalu- ation category (see Section 3.3). For ROUGE evaluation,  we chose the F-measure for R-1 (unigram) and R-SU4  (skip-bigram with maximum gap length of 4), which is  based on our observation that other scores in ROUGE are  always highly correlated (rho>0.9) to either of them for  this task. We compute the correlation separately for the  human and system summaries in order to avoid the im- pact due to the inherent difference between the two dif- ferent summaries.", "labels": [], "entities": [{"text": "H AVG)", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8466250896453857}, {"text": "F-measure", "start_pos": 195, "end_pos": 204, "type": "METRIC", "confidence": 0.9890194535255432}]}, {"text": " Table 1: Spearman's rho between human evaluation (H) and  ROUGE (R) with basic setting.", "labels": [], "entities": [{"text": "ROUGE (R)", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9456597715616226}]}, {"text": " Table 2: Effect of disfluencies on the correlation between R- SU4 and human evaluation.", "labels": [], "entities": []}, {"text": " Table 3: Effect of speaker information on the correlation be- tween R-SU4 and human evaluation.", "labels": [], "entities": []}]}