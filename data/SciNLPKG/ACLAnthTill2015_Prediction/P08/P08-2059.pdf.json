{"title": [], "abstractContent": [{"text": "Active learning is a machine learning approach to achieving high-accuracy with a small amount of labels by letting the learning algorithm choose instances to be labeled.", "labels": [], "entities": [{"text": "Active learning", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7399163842201233}]}, {"text": "Most of previous approaches based on dis-criminative learning use the margin for choosing instances.", "labels": [], "entities": []}, {"text": "We present a method for incorporating confidence into the margin by using a newly introduced online learning algorithm and show empirically that confidence improves active learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Successful applications of supervised machine learning to natural language rely on quality labeled training data, but annotation can be costly, slow and difficult.", "labels": [], "entities": []}, {"text": "One popular solution is Active Learning, which maximizes learning accuracy while minimizing labeling efforts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9681755304336548}]}, {"text": "In active learning, the learning algorithm itself selects unlabeled examples for annotation.", "labels": [], "entities": []}, {"text": "A variety of techniques have been proposed for selecting examples that maximize system performance as compared to selecting instances randomly.", "labels": [], "entities": []}, {"text": "Two learning methodologies dominate NLP applications: probabilistic methods -naive Bayes, logistic regression -and margin methods -support vector machines and passive-aggressive.", "labels": [], "entities": []}, {"text": "Active learning for probabilistic methods often uses uncertainty sampling: label the example with the lowest probability prediction (the most \"uncertain\") (.", "labels": [], "entities": []}, {"text": "The equivalent technique for margin learning associates the margin with prediction certainty: label the example with the lowest margin).", "labels": [], "entities": [{"text": "margin learning", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.8862987160682678}]}, {"text": "Common intuition equates large margins with high prediction confidence.", "labels": [], "entities": [{"text": "prediction confidence", "start_pos": 49, "end_pos": 70, "type": "METRIC", "confidence": 0.7265624105930328}]}, {"text": "However, confidence and margin are two distinct properties.", "labels": [], "entities": [{"text": "confidence", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9979534149169922}, {"text": "margin", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9970284104347229}]}, {"text": "For example, an instance may receive a large margin based on a single feature which has been updated only a small number of times.", "labels": [], "entities": []}, {"text": "Another example may receive a small margin, but its features have been learned from a large number of examples.", "labels": [], "entities": []}, {"text": "While the first example has a larger margin it has low confidence compared to the second.", "labels": [], "entities": []}, {"text": "Both the margin value and confidence should be considered in choosing which example to label.", "labels": [], "entities": [{"text": "margin", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9714381098747253}, {"text": "confidence", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9703181385993958}]}, {"text": "We present active learning with confidence using a recently introduced online learning algorithm called Confidence-Weighted linear classification.", "labels": [], "entities": [{"text": "Confidence-Weighted linear classification", "start_pos": 104, "end_pos": 145, "type": "TASK", "confidence": 0.631629486878713}]}, {"text": "The classifier assigns labels according to a Gaussian distribution over margin values instead of a single value, which arises from parameter confidence (variance).", "labels": [], "entities": [{"text": "parameter confidence (variance)", "start_pos": 131, "end_pos": 162, "type": "METRIC", "confidence": 0.717566841840744}]}, {"text": "The variance of this distribution represents the confidence in the mean (margin).", "labels": [], "entities": [{"text": "confidence in the mean (margin)", "start_pos": 49, "end_pos": 80, "type": "METRIC", "confidence": 0.7650931988443647}]}, {"text": "We then employ this distribution fora new active learning criteria, which in turn could improve other margin based active learning techniques.", "labels": [], "entities": []}, {"text": "Additionally, we favor the use of an online method since online methods have achieved good NLP performance and are fast to train -an important property for interactive learning.", "labels": [], "entities": []}, {"text": "Experimental validation on a number of datasets shows that active learning with confidence can improve standard methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our active learning methods we used a similar experimental setup to.", "labels": [], "entities": []}, {"text": "Each active learning algorithm was given two labeled examples, one from each class, for initial training of a classifier, and remaining data as unlabeled examples.", "labels": [], "entities": []}, {"text": "On each round the algorithm selected a single instance for which it was then given the correct label.", "labels": [], "entities": []}, {"text": "The algorithm updated the online classifier and evaluated it on held out test data to measure learning progress.", "labels": [], "entities": []}, {"text": "We selected four binary NLP datasets for evaluation: 20 Newsgroups 1 and Reuters () (used by and sentiment classification () and spam).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.8220630288124084}]}, {"text": "For each dataset we extracted binary unigram features and sentiment was prepared according to.", "labels": [], "entities": []}, {"text": "From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar labels from computers, science and talk.", "labels": [], "entities": []}, {"text": "We created 3 similar problems from Reuters from insurance, business services and retail distribution.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9096346497535706}]}, {"text": "Sentiment used 4 Amazon domains (book, dvd, electronics, kitchen).", "labels": [], "entities": []}, {"text": "Spam used the three users from task A data.", "labels": [], "entities": [{"text": "Spam", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9209249019622803}]}, {"text": "Each problem had 2000 instances except for 20 Newsgroups, which used between 1850 and 1971 instances.", "labels": [], "entities": []}, {"text": "This created 13 classification problems across four tasks.", "labels": [], "entities": []}, {"text": "Each active learning algorithm was evaluated using a PA (with slack variable c = 1) or CW classifier (\u03c6 = 1) using 10-fold cross validation.", "labels": [], "entities": [{"text": "PA", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9451210498809814}]}, {"text": "We evaluated several methods in the Simple margin framework: PA Margin and CW Margin, which select examples with the smallest margin, and ACL.", "labels": [], "entities": [{"text": "PA Margin", "start_pos": 61, "end_pos": 70, "type": "TASK", "confidence": 0.600499838590622}]}, {"text": "As a baseline we included selecting a random instance.", "labels": [], "entities": []}, {"text": "We also evaluated CW and a PA classifier trained on all training instances.", "labels": [], "entities": [{"text": "CW", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.8679316639900208}]}, {"text": "Each method was evaluated by 1 http://people.csail.mit.edu/jrennie/20Newsgroups/ labeling up to 500 labels, about 25% of the training data.", "labels": [], "entities": []}, {"text": "The 10 runs on each dataset for each problem appear in the left and middle panel of, which show the test accuracy after each round of active learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.994877278804779}]}, {"text": "Horizontal lines indicate CW (solid) and PA (dashed) training on all instances.", "labels": [], "entities": [{"text": "PA (dashed) training", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.8454687833786011}]}, {"text": "Legend numbers are accuracy after 500 labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9981623291969299}]}, {"text": "The left panel averages results over 20 Newsgroups, and the middle panel averages results overall 13 datasets.", "labels": [], "entities": [{"text": "Newsgroups", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9696285724639893}]}, {"text": "To achieve 80% of the accuracy of training on all data, a realistic goal for less than 100 labels, PA Margin required 93% the number of labels of PA Random, while CW Margin needed only 73% of the labels of CW Random.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9994140863418579}, {"text": "PA Margin", "start_pos": 99, "end_pos": 108, "type": "TASK", "confidence": 0.5478087663650513}]}, {"text": "By using fewer labels compared to random selection baselines, CW Margin learns faster in the active learning setting as compared with PA.", "labels": [], "entities": []}, {"text": "Furthermore, adding confidence reduced labeling cost compared to margin alone.", "labels": [], "entities": [{"text": "labeling", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.9583145380020142}]}, {"text": "ACL improved over CW Margin on every task and after almost every round; it required 63% of the labels of CW Random to reach the 80% mark.", "labels": [], "entities": [{"text": "CW Margin", "start_pos": 18, "end_pos": 27, "type": "TASK", "confidence": 0.627286970615387}]}, {"text": "We computed the fraction of labels CW Margin and ACL required (compared to CW Random) to achieve the 80% accuracy mark of training with all data.", "labels": [], "entities": [{"text": "accuracy mark", "start_pos": 105, "end_pos": 118, "type": "METRIC", "confidence": 0.9853312969207764}]}, {"text": "The results are summarized in the right panel of, where we plot one point per dataset.", "labels": [], "entities": []}, {"text": "Points above the diagonal-line demonstrate the superiority of ACL over CW Margin.", "labels": [], "entities": [{"text": "CW Margin", "start_pos": 71, "end_pos": 80, "type": "TASK", "confidence": 0.5815982222557068}]}, {"text": "ACL required fewer labels than CW margin twice as often as the opposite occurred.", "labels": [], "entities": [{"text": "ACL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8020011186599731}, {"text": "CW margin", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.754408210515976}]}, {"text": "Note that CW Margin used more labels than CW Random in three cases, while ACL only once, and this onetime only about a dozen labels were needed.", "labels": [], "entities": [{"text": "CW Margin", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7778232991695404}, {"text": "ACL", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.7655696272850037}]}, {"text": "To conclude, not only does CW Margin outperforms PA Margin for active-learning, CW maintains additional valuable information (confidence), which further improves performance.", "labels": [], "entities": []}], "tableCaptions": []}