{"title": [], "abstractContent": [{"text": "Traditional wisdom holds that once documents are turned into bag-of-words (unigram count) vectors, word orders are completely lost.", "labels": [], "entities": []}, {"text": "We introduce an approach that, perhaps surprisingly, is able to learn a bigram language model from a set of bag-of-words documents.", "labels": [], "entities": []}, {"text": "At its heart, our approach is an EM algorithm that seeks a model which maximizes the regularized marginal likelihood of the bag-of-words documents.", "labels": [], "entities": []}, {"text": "In experiments on seven corpora, we observed that our learned bigram language models: i) achieve better test set per-plexity than unigram models trained on the same bag-of-words documents, and are not far behind \"oracle bigram models\" trained on the corresponding ordered documents; ii) assign higher probabilities to sensible bigram word pairs; iii) improve the accuracy of ordered-document recovery from a bag-of-words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 363, "end_pos": 371, "type": "METRIC", "confidence": 0.9987196922302246}]}, {"text": "Our approach opens the door to novel phenomena, for example, privacy leakage from index files.", "labels": [], "entities": [{"text": "privacy leakage from index files", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.8330161392688751}]}], "introductionContent": [{"text": "A bag-of-words (BOW) is a basic document representation in natural language processing.", "labels": [], "entities": []}, {"text": "In this paper, we consider a BOW in its simplest form, i.e., a unigram count vector or word histogram over the vocabulary.", "labels": [], "entities": []}, {"text": "When performing the counting, word order is ignored.", "labels": [], "entities": [{"text": "counting", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9600271582603455}]}, {"text": "For example, the phrases \"really neat\" and \"neat really\" contribute equally to a BOW.", "labels": [], "entities": [{"text": "BOW", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.6208136677742004}]}, {"text": "Obviously, once a set of documents is turned into a set of BOWs, the word order information within them is completely lost-or is it?", "labels": [], "entities": []}, {"text": "In this paper, we show that one can in fact partly recover the order information.", "labels": [], "entities": []}, {"text": "Specifically, given a set of documents in unigram-count BOW representation, one can recover a non-trivial bigram language model (LM) 1 , which has part of the power of a bigram LM trained on ordered documents.", "labels": [], "entities": []}, {"text": "At first glance this seems impossible: How can one learn bigram information from unigram counts?", "labels": [], "entities": []}, {"text": "However, we will demonstrate that multiple BOW documents enable us to recover some higher-order information.", "labels": [], "entities": [{"text": "BOW documents", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.7072305679321289}]}, {"text": "Our results have implications in a wide range of natural language problems, in particular document privacy.", "labels": [], "entities": [{"text": "document privacy", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7200051546096802}]}, {"text": "With the wide adoption of natural language applications like desktop search engines, software programs are increasingly indexing computer users' personal files for fast processing.", "labels": [], "entities": []}, {"text": "Most index files include some variant of the BOW.", "labels": [], "entities": [{"text": "BOW", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.5766162276268005}]}, {"text": "As we demonstrate in this paper, if a malicious party gains access to BOW index files, it can recover more than just unigram frequencies: (i) the malicious party can recover a higher-order LM; (ii) with the LM it may attempt to recover the original ordered document from a BOW by finding the most-likely word permutation 2 . Future research will quantify the extent to which such a privacy breach is possible in theory, and will find solutions to prevent it.", "labels": [], "entities": []}, {"text": "There is avast literature on language modeling; see, e.g.,.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7396032810211182}]}, {"text": "How-ever, to the best of our knowledge, none addresses this reverse direction of learning higher-order LMs from lower-order data.", "labels": [], "entities": []}, {"text": "This work is inspired by recent advances in inferring network structure from co-occurrence data, for example, for computer networks and biological pathways).", "labels": [], "entities": []}], "datasetContent": [{"text": "We show experimentally that the proposed algorithm is indeed able to recover reasonable bigram LMs from BOW corpora.", "labels": [], "entities": []}, {"text": "Good test set perplexity: Using test (heldout) set perplexity (PP) as an objective measure of LM quality, we demonstrate that our recovered bigram LMs are much better than na\u00a8\u0131vena\u00a8\u0131ve unigram LMs trained on the same BOW corpus.", "labels": [], "entities": [{"text": "BOW corpus", "start_pos": 217, "end_pos": 227, "type": "DATASET", "confidence": 0.9594240486621857}]}, {"text": "Furthermore, they are not far behind the \"oracle\" bigram LMs trained on ordered documents that correspond to the BOWs.", "labels": [], "entities": [{"text": "BOWs", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.9267400503158569}]}, {"text": "2. Sensible bigram pairs: We inspect the recovered bigram LMs and find that they assign higher probabilities to sensible bigram pairs (e.g., \"i mean\", \"oh boy\", \"that's funny\"), and lower probabilities to nonsense pairs (e.g., \"i yep\", \"you let's\", \"right lot\").", "labels": [], "entities": []}, {"text": "3. Document recovery from BOW: With the bigram LMs, we show improved accuracy in recovering ordered documents from BOWs.", "labels": [], "entities": [{"text": "Document recovery from BOW", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.8627013862133026}, {"text": "bigram LMs", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8489360213279724}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9989448189735413}]}, {"text": "We describe these experiments in detail below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Corpora statistics: vocabulary size, document  count, total token count, and mean document length.", "labels": [], "entities": []}, {"text": " Table 3: Mean test set perplexities of prior LMs and bi- gram LMs recovered after 2 EM iterations.", "labels": [], "entities": []}, {"text": " Table 4: Mean test set perplexities for oracle bigram LMs  trained on z 1 , . . . , z n and tested on z n+1 , . . . , z m . For  reference, the rightmost column lists the best result using  a recovered bigram LM (\u03b8 perm for the first three corpora,  \u03b8 f dc for the latter four).", "labels": [], "entities": []}]}