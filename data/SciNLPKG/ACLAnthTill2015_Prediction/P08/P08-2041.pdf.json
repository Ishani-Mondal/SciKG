{"title": [{"text": "Partial Matching Strategy for Phrase-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Partial Matching", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8023536503314972}, {"text": "Phrase-based Statistical Machine Translation", "start_pos": 30, "end_pos": 74, "type": "TASK", "confidence": 0.8626674711704254}]}], "abstractContent": [{"text": "This paper presents a partial matching strategy for phrase-based statistical machine translation (PBSMT).", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 52, "end_pos": 104, "type": "TASK", "confidence": 0.7392483481339046}]}, {"text": "Source phrases which do not appear in the training corpus can be translated byword substitution according to partially matched phrases.", "labels": [], "entities": []}, {"text": "The advantage of this method is that it can alleviate the data sparse-ness problem if the amount of bilingual corpus is limited.", "labels": [], "entities": []}, {"text": "We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Currently, most of the phrase-based statistical machine translation (PBSMT) models) adopt full matching strategy for phrase translation, which means that a phrase pair ( \ud97b\udf59 f , \ud97b\udf59 e) can be used for translating a source phrase \u00af f , only if \ud97b\udf59 f = \u00af f . Due to lack of generalization ability, the full matching strategy has some limitations.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 23, "end_pos": 75, "type": "TASK", "confidence": 0.7075686923095158}, {"text": "phrase translation", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.8733655512332916}]}, {"text": "On one hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited.", "labels": [], "entities": []}, {"text": "On the other hand, fora certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of translation of unseen phrases, the source phrases that are not observed in the training corpus.", "labels": [], "entities": [{"text": "translation of unseen phrases", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.8917443007230759}]}, {"text": "The alignment template model ( enhanced phrasal generalizations by using words classes rather than the words themselves.", "labels": [], "entities": []}, {"text": "But the phrases are overly generalized.", "labels": [], "entities": []}, {"text": "The hierarchical phrase-based model () used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings.", "labels": [], "entities": []}, {"text": "However, the huge grammar table greatly increases computational complexity.", "labels": [], "entities": []}, {"text": "used paraphrases of the trainig corpus for translating unseen phrases.", "labels": [], "entities": [{"text": "translating unseen phrases", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.8954147100448608}]}, {"text": "But they only found and used the semantically similar phrases.", "labels": [], "entities": []}, {"text": "Another method is to use multi-parallel corpora to improve phrase coverage and translation quality.", "labels": [], "entities": [{"text": "phrase coverage", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.8196916580200195}, {"text": "translation quality", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.8490681350231171}]}, {"text": "This paper presents a partial matching strategy for translating unseen phrases.", "labels": [], "entities": [{"text": "translating unseen phrases", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.9107197920481364}]}, {"text": "When encountering unseen phrases in a source sentence, we search partially matched phrase pairs from the phrase table.", "labels": [], "entities": []}, {"text": "Then we keep the translations of the matched part and translate the unmatched part byword substitution.", "labels": [], "entities": [{"text": "byword substitution", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7210043668746948}]}, {"text": "The advantage of our approach is that we alleviate the data sparseness problem without increasing the amount of bilingual corpus.", "labels": [], "entities": []}, {"text": "Moreover, the partially matched phrases are not necessarily synonymous.", "labels": [], "entities": []}, {"text": "We incorporate the partial matching method into the state-of-the-art PBSMT system, Moses.", "labels": [], "entities": []}, {"text": "Experiments show that, our approach achieves statistically significant improvements not only on small corpus, but also on large corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carryout experiments on Chinese-to-English translation on two tasks: Small-scale task, the training corpus consists of 30k sentence pairs (840K + 950K words); Large-scale task, the training corpus consists of 2.54M sentence pairs (68M + 74M words).", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6546860635280609}]}, {"text": "The 2002 NIST MT evaluation test data is used as the development set and the 2005 NIST MT test data is the test set.", "labels": [], "entities": [{"text": "NIST MT evaluation test data", "start_pos": 9, "end_pos": 37, "type": "DATASET", "confidence": 0.917143714427948}, {"text": "NIST MT test data", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9187539964914322}]}, {"text": "The baseline system we used for comparison is the state-of-the-art PBSMT system, Moses.", "labels": [], "entities": []}, {"text": "We use the ICTCLAS toolkit 2 to perform Chinese word segmentation and POS tagging.", "labels": [], "entities": [{"text": "ICTCLAS toolkit", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.8504414856433868}, {"text": "Chinese word segmentation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.5653968652089437}, {"text": "POS tagging", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.8092926740646362}]}, {"text": "The training script of Moses is used to train the bilingual corpus.", "labels": [], "entities": []}, {"text": "We set the maximum length of the source phrase to 7, and record word alignment information in the phrase table.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.6207207441329956}]}, {"text": "For the language model, we use the SRI Language Modeling Toolkit) to train a 4-gram model on the Xinhua portion of the Gigaword corpus.", "labels": [], "entities": [{"text": "SRI Language Modeling Toolkit", "start_pos": 35, "end_pos": 64, "type": "DATASET", "confidence": 0.760945662856102}, {"text": "Gigaword corpus", "start_pos": 119, "end_pos": 134, "type": "DATASET", "confidence": 0.8015689551830292}]}, {"text": "To run the decoder, we set ttable-limit=20, distortion-limit=6, stack=100.", "labels": [], "entities": [{"text": "distortion-limit", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9871701598167419}, {"text": "stack", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9806744456291199}]}, {"text": "The translation quality is evaluated by BLEU-4 (case-sensitive).", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9526436924934387}, {"text": "BLEU-4", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9972966313362122}]}, {"text": "We perform minimum-error-rate training to tune the feature weights of the translation model to maximize the BLEU score on development set.: Effect of matching threshold on BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9793861508369446}, {"text": "BLEU score", "start_pos": 172, "end_pos": 182, "type": "METRIC", "confidence": 0.9758297502994537}]}, {"text": "shows the effect of matching threshold on translation quality.", "labels": [], "entities": []}, {"text": "The baseline uses full matching (\u03b1=1.0) for phrase translation and achieves a BLEU score of 24.44.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.853713184595108}, {"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9831528067588806}]}, {"text": "With the decrease of the matching threshold, the BLEU scores increase.", "labels": [], "entities": [{"text": "matching threshold", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.9492649137973785}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9996598958969116}]}, {"text": "when \u03b1=0.3, the system obtains the highest BLEU score of 25.31, which achieves an absolute improvement of 0.87 over the baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9879271984100342}]}, {"text": "However, if the threshold continue decreasing, the BLEU score decreases.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9817833006381989}]}, {"text": "The reason is that low threshold increases noise for partial matching.", "labels": [], "entities": [{"text": "noise", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9526006579399109}]}], "tableCaptions": [{"text": " Table 1: Effect of matching threshold on BLEU score.", "labels": [], "entities": [{"text": "matching threshold", "start_pos": 20, "end_pos": 38, "type": "METRIC", "confidence": 0.9626967906951904}, {"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9747171998023987}]}, {"text": " Table 2: Phrase number of 1-best output. \u03b1=1.0 means full matching. For \u03b1=0.3, SIM =1.0 means full matching,  0.3 \u2264 SIM < 1.0 means partial matching.", "labels": [], "entities": []}]}