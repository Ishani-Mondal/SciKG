{"title": [{"text": "Pairwise Document Similarity in Large Collections with MapReduce", "labels": [], "entities": [{"text": "Pairwise Document Similarity", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.778166393438975}]}], "abstractContent": [{"text": "This paper presents a MapReduce algorithm for computing pairwise document similarity in large document collections.", "labels": [], "entities": []}, {"text": "MapReduce is an attractive framework because it allows us to decompose the inner products involved in computing document similarity into separate multiplication and summation stages in away that is well matched to efficient disk access patterns across several machines.", "labels": [], "entities": [{"text": "computing document similarity", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.681706041097641}]}, {"text": "On a collection consisting of approximately 900,000 newswire articles, our algorithm exhibits linear growth in running time and space in terms of the number of documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computing pairwise similarity on large document collections is a task common to a variety of problems such as clustering and cross-document coreference resolution.", "labels": [], "entities": [{"text": "cross-document coreference resolution", "start_pos": 125, "end_pos": 162, "type": "TASK", "confidence": 0.799155076344808}]}, {"text": "For example, in the PubMed search engine, 1 which provides access to the life sciences literature, a \"more like this\" browsing feature is implemented as a simple lookup of documentdocument similarity scores, computed offline.", "labels": [], "entities": [{"text": "PubMed search engine, 1", "start_pos": 20, "end_pos": 43, "type": "DATASET", "confidence": 0.9127930760383606}]}, {"text": "This paper considers a large class of similarity functions that can be expressed as an inner product of term weight vectors.", "labels": [], "entities": []}, {"text": "For document collections that fit into randomaccess memory, the solution is straightforward.", "labels": [], "entities": []}, {"text": "As collection size grows, however, it ultimately becomes necessary to resort to disk storage, at which point aligning computation order with disk access patterns becomes a challenge.", "labels": [], "entities": []}, {"text": "Further growth in the * Department of Computer Science \u2020 The iSchool, College of Information Studies 1 http://www.ncbi.nlm.nih.gov/PubMed document collection will ultimately make it desirable to spread the computation over several processors, at which point interprocess communication becomes a second potential bottleneck for which the computation order must be optimized.", "labels": [], "entities": [{"text": "PubMed document collection", "start_pos": 131, "end_pos": 157, "type": "DATASET", "confidence": 0.9039266308148702}]}, {"text": "Although tailored implementations can be designed for specific parallel processing architectures, the MapReduce framework) offers an attractive solution to these challenges.", "labels": [], "entities": []}, {"text": "In this paper, we describe how pairwise similarity computation for large collections can be efficiently implemented with MapReduce.", "labels": [], "entities": []}, {"text": "We empirically demonstrate that removing high frequency (and therefore low entropy) terms results in approximately linear growth in required disk space and running time with increasing collection size for collections containing several hundred thousand documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we used Hadoop version 0.16.0, 3 an open-source Java implementation of MapReduce, running on a cluster with 20 machines (1 master, 19 slave).", "labels": [], "entities": []}, {"text": "Each machine has two single-core processors (running at either 2.4GHz or 2.8GHz), 4GB memory, and 100GB disk.", "labels": [], "entities": []}, {"text": "We implemented the symmetric variant of Okapi-BM25 ( as the similarity function.", "labels": [], "entities": []}, {"text": "We used the AQUAINT-2 collection of newswire text, containing 906k documents, totaling approximately 2.5 gigabytes.", "labels": [], "entities": [{"text": "AQUAINT-2 collection of newswire text", "start_pos": 12, "end_pos": 49, "type": "DATASET", "confidence": 0.937275767326355}]}, {"text": "To test the scalability of our technique, we sampled the collection into subsets of, and 100 percent of the documents.", "labels": [], "entities": []}, {"text": "After stopword removal (using Lucene's stopword list), we implemented a df-cut, where a fraction of the terms with the highest document frequencies is eliminated.", "labels": [], "entities": [{"text": "stopword removal", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7424259781837463}]}, {"text": "This has the effect of removing non-discriminative terms.", "labels": [], "entities": []}, {"text": "In our experiments, we adopt a 99% cut, which means that the most frequent 1% of terms were discarded (9,093 terms out of a total vocabulary size of 909,326).", "labels": [], "entities": []}, {"text": "This technique greatly increases the efficiency of our algorithm, since the number of tuples emitted by the mappers in the pairwise similarity phase is dominated by the length of the longest posting (in the worst case, if a term appears in all documents, it would generate approximately 10 12 tuples).", "labels": [], "entities": []}, {"text": "shows the running time of the pairwise similarity phase for different collection sizes.", "labels": [], "entities": []}, {"text": "The computation for the entire collection finishes in approximately two hours.", "labels": [], "entities": []}, {"text": "Empirically, we find that running time increases linearly with collection size, which is an extremely desirable property.", "labels": [], "entities": []}, {"text": "To get a sense of the space complexity, we compute the number of intermediate document pairs that are emitted by the mappers.", "labels": [], "entities": []}, {"text": "The space savings are large (3.7 billion rather than 8.1 trillion intermediate pairs for the entire collection), and space requirements grow linearly with collection size over this region (R 2 = 0.9975).", "labels": [], "entities": [{"text": "R 2", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9565457999706268}]}, {"text": "df-cut at 99% df-cut at 99.9% df-cut at 99.99% df-cut at 99.999% no df-cut: Effect of changing df -cut thresholds on the number of intermediate document-pairs emitted, for subsets of AQUAINT-2.", "labels": [], "entities": [{"text": "AQUAINT-2", "start_pos": 183, "end_pos": 192, "type": "DATASET", "confidence": 0.8462085723876953}]}], "tableCaptions": []}