{"title": [{"text": "Can you summarize this? Identifying correlates of input difficulty for generic multi-document summarization", "labels": [], "entities": [{"text": "summarize", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.9866610169410706}, {"text": "summarization", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.7081634998321533}]}], "abstractContent": [{"text": "Different summarization requirements could make the writing of a good summary more difficult , or easier.", "labels": [], "entities": []}, {"text": "Summary length and the characteristics of the input are such constraints influencing the quality of a potential summary.", "labels": [], "entities": [{"text": "Summary length", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8401477336883545}]}, {"text": "In this paper we report the results of a quantitative analysis on data from large-scale evaluations of multi-document summarization, empirically confirming this hypothesis.", "labels": [], "entities": []}, {"text": "We further show that features measuring the cohe-siveness of the input are highly correlated with eventual summary quality and that it is possible to use these as features to predict the difficulty of new, unseen, summarization inputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "In certain situations even the best automatic summarizers or professional writers can find it hard to write a good summary of a set of articles.", "labels": [], "entities": []}, {"text": "If there is no clear topic shared across the input articles, or if they follow the development of the same event in time fora longer period, it could become difficult to decide what information is most representative and should be conveyed in a summary.", "labels": [], "entities": []}, {"text": "Similarly, length requirements could pre-determine summary quality-a short outline of a story might be confusing and unclear but a page long discussion might give an excellent overview of the same issue.", "labels": [], "entities": [{"text": "length", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9580789804458618}]}, {"text": "Even systems that perform well on average produce summaries of poor quality for some inputs.", "labels": [], "entities": []}, {"text": "For this reason, understanding what aspects of the input make it difficult for summarization becomes an interesting and important issue that has not been addressed in the summarization community untill now.", "labels": [], "entities": [{"text": "summarization", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.987701952457428}, {"text": "summarization", "start_pos": 171, "end_pos": 184, "type": "TASK", "confidence": 0.9747443795204163}]}, {"text": "In information retrieval, for example, the variable system performance has been recognized as a research challenge and numerous studies on identifying query difficulty have been carried out (most recently).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.8104932606220245}]}, {"text": "In this paper we present results supporting the hypotheses that input topicality cohesiveness and summary length are among the factors that determine summary quality regardless of the choice of summarization strategy (Section 2).", "labels": [], "entities": [{"text": "summary length", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.6529741287231445}]}, {"text": "The data used for the analyses comes from the annual Document Understanding Conference (DUC) in which various summarization approaches are evaluated on common data, with new test sets provided each year.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.7357265651226044}]}, {"text": "In later sections we define a suite of features capturing aspects of the topicality cohesiveness of the input (Section 3) and relate these to system performance, identifying reliable correlates of input difficulty (Section 4).", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we demonstrate that the features can be used to build a classifier predicting summarization input difficulty with accuracy considerably above chance level.", "labels": [], "entities": [{"text": "predicting summarization input", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.6939847866694132}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9991099238395691}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Average human, system and baseline coverage  scores for different summary lengths of N words. N =  50, 100, 200, and 400.", "labels": [], "entities": []}, {"text": " Table 1: Analysis of variance for coverage scores of automatic systems with input, summarizer, and length as factors.", "labels": [], "entities": [{"text": "length", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9582593441009521}]}, {"text": " Table 3: Pearson correlation between average human and  system coverage scores on the DUC 2001 dataset. Sig- nificance levels: *p < 0.05 and **p < 0.00001.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9565236568450928}, {"text": "DUC 2001 dataset", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.9916185935338339}, {"text": "Sig", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.870599091053009}]}, {"text": " Table 4: Comparison of non-cohesive (average system  coverage score < median average system score) vs cohe- sive sets for summary length of 100 words", "labels": [], "entities": []}, {"text": " Table 5: Correlation between coverage score and feature  values for the 29 DUC'01 100-word summaries.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9304923415184021}, {"text": "coverage score", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9776045382022858}, {"text": "DUC'01 100-word summaries", "start_pos": 76, "end_pos": 101, "type": "DATASET", "confidence": 0.9068573911984762}]}, {"text": " Table 6: Logistic regression classification results (accu- racy, precision, recall and f-measure) for balanced data of  100-word summaries from DUC'02 through DUC'04.", "labels": [], "entities": [{"text": "Logistic regression classification", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.7673511505126953}, {"text": "accu- racy", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9187134305636088}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.985837996006012}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9961523413658142}, {"text": "f-measure", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9216704964637756}, {"text": "DUC'02", "start_pos": 145, "end_pos": 151, "type": "DATASET", "confidence": 0.9439419507980347}, {"text": "DUC'04", "start_pos": 160, "end_pos": 166, "type": "DATASET", "confidence": 0.511948823928833}]}]}