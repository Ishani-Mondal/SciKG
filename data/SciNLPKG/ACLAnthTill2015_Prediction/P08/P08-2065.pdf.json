{"title": [], "abstractContent": [{"text": "This paper addresses anew task in sentiment classification, called multi-domain sentiment classification, that aims to improve performance through fusing training data from multiple domains.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.9731509387493134}, {"text": "multi-domain sentiment classification", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.7401130795478821}]}, {"text": "To achieve this, we propose two approaches of fusion, feature-level and classi-fier-level, to use training data from multiple domains simultaneously.", "labels": [], "entities": []}, {"text": "Experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification (using the training data individually).", "labels": [], "entities": [{"text": "multi-domain sentiment classification", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.8044257164001465}, {"text": "single domain classification", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.7697301308314005}]}], "introductionContent": [{"text": "Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of, or sentiment toward a given subject (e.g., if an opinion is supported or not) ().", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9329627454280853}, {"text": "text categorization", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7252698540687561}]}, {"text": "This task has created a considerable interest due to its wide applications.", "labels": [], "entities": []}, {"text": "Sentiment classification is a very domainspecific problem; training a classifier using the data from one domain may fail when testing against data from another.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9486839175224304}]}, {"text": "As a result, real application systems usually require some labeled data from multiple domains, guaranteeing an acceptable performance for different domains.", "labels": [], "entities": []}, {"text": "However, each domain has a very limited amount of training data due to the fact that creating largescale high-quality labeled corpora is difficult and time-consuming.", "labels": [], "entities": []}, {"text": "Given the limited multi-domain training data, an interesting task arises, how to best make full use of all training data to improve sentiment classification performance.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.9580374956130981}]}, {"text": "We name this new task, 'multi-domain sentiment classification'.", "labels": [], "entities": [{"text": "multi-domain sentiment classification", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.7450777987639109}]}, {"text": "In this paper, we propose two approaches to multi-domain sentiment classification.", "labels": [], "entities": [{"text": "multi-domain sentiment classification", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.7573058307170868}]}, {"text": "In the first, called feature-level fusion, we combine the feature sets from all the domains into one feature set.", "labels": [], "entities": [{"text": "feature-level fusion", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7389481961727142}]}, {"text": "Using the unified feature set, we train a classifier using all the training data regardless of domain.", "labels": [], "entities": []}, {"text": "In the second approach, classifier-level fusion, we train abase classifier using the training data from each domain and then apply combination methods to combine the base classifiers.", "labels": [], "entities": [{"text": "classifier-level fusion", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.8697856664657593}]}], "datasetContent": [{"text": "Data Set: We carryout our experiments on the labeled product reviews from four domains: books, DVDs, electronics, and kitchen appliances 1 . Each domain contains 1,000 positive and 1,000 negative reviews.", "labels": [], "entities": []}, {"text": "Experiment Implementation: We apply SVM algorithm to construct our classifiers which has been shown to perform better than many other classification algorithms ().", "labels": [], "entities": []}, {"text": "Here, we use LIBSVM 2 with a linear kernel function for training and testing.", "labels": [], "entities": []}, {"text": "In our experiments, the data in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 20% and 10% respectively.", "labels": [], "entities": []}, {"text": "The development data are used to train the meta-classifier.", "labels": [], "entities": []}, {"text": "Baseline: The baseline uses the single domain classification approach mentioned in sub-Section 2.1.", "labels": [], "entities": [{"text": "domain classification", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7215792536735535}]}, {"text": "We test four different feature sets to construct our feature vector.", "labels": [], "entities": []}, {"text": "First, we use unigrams (e.g., 'happy') as features and perform the standard feature selection process to find the optimal feature set of unigrams (1Gram).", "labels": [], "entities": []}, {"text": "The selection method is Bi-Normal Separation (BNS) that is reported to be excellent in many text categorization tasks).", "labels": [], "entities": [{"text": "Bi-Normal Separation (BNS)", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.7928428709506988}]}, {"text": "The criterion of the optimization is to find the set of unigrams with the best performance on the development data through selecting the features with high BNS scores.", "labels": [], "entities": [{"text": "BNS", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.7957131862640381}]}, {"text": "Then, we get the optimal word bi-gram (e.g., 'very happy') (2Gram) and mixed feature set (1+2Gram) in the same way.", "labels": [], "entities": []}, {"text": "The fourth feature set (1Gram+2Gram) also consists of unigrams and bi-grams just like the third one.", "labels": [], "entities": []}, {"text": "The difference between them lies in their selection strategy.", "labels": [], "entities": []}, {"text": "The third feature set is obtained through selecting the unigrams and bi-grams with high BNS scores while the fourth one is obtained through simply uniting the two optimal sets of 1Gram and 2Gram.", "labels": [], "entities": [{"text": "BNS", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.990825355052948}]}, {"text": "From, we see that 1Gram+2Gram features perform much better than other types of features, which implies that we need to select good unigram and bi-gram features separately before combine them.", "labels": [], "entities": []}, {"text": "Although the size of our training data are smaller than that reported in Blitzer et al.", "labels": [], "entities": []}, {"text": "We implement the fusion using 1+2Gram and 1Gram+2Gram respectively.", "labels": [], "entities": []}, {"text": "From, we see that both the two fusion approaches generally outperform single domain classification when using 1+2Gram features.", "labels": [], "entities": []}, {"text": "They increase the average accuracy from 0.8 to 0.82375 and 0.83875, a significant relative error reduction of 11.87% and 19.38% over baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9431823492050171}, {"text": "relative error reduction", "start_pos": 82, "end_pos": 106, "type": "METRIC", "confidence": 0.8048367301623026}]}, {"text": "However, when the performance of baseline increases, the feature level approach fails to help the performance improvement in three domains.", "labels": [], "entities": []}, {"text": "This is mainly because the base classifiers perform extremely unbalanced on the testing data of these domains.", "labels": [], "entities": []}, {"text": "For example, the four base classifiers from Books, DVDs, Electronics, and Kitchen achieve the accuracies of 0.675, 0.62, 0.85, and 0.79 on the testing data from Electronics respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9826699495315552}]}, {"text": "Dealing with such an unbalanced performance, we definitely need to put enough high weight on the training data from Electronics.", "labels": [], "entities": [{"text": "Electronics", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.8287019729614258}]}, {"text": "However, the feature-level fusion approach simply pools all training data from different domains and treats them equally.", "labels": [], "entities": []}, {"text": "Thus it cannot capture the unbalanced information.", "labels": [], "entities": []}, {"text": "In contrast, metalearning is able to learn the unbalance automatically through training the meta-classifier using the development data.", "labels": [], "entities": []}, {"text": "Therefore, it can still increase the average accuracy from 0.8325 to 0.8625, an impressive relative error reduction of 17.91% over baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9693546295166016}, {"text": "relative error reduction", "start_pos": 91, "end_pos": 115, "type": "METRIC", "confidence": 0.7510824799537659}]}], "tableCaptions": []}