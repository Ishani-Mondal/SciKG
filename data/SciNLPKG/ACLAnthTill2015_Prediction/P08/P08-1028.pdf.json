{"title": [], "abstractContent": [{"text": "This paper proposes a framework for representing the meaning of phrases and sentences in vector space.", "labels": [], "entities": []}, {"text": "Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.", "labels": [], "entities": [{"text": "vector composition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7094051986932755}]}, {"text": "Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector-based models of word meaning) have become increasingly popular in natural language processing (NLP) and cognitive science.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.7990906437238058}]}, {"text": "The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar.", "labels": [], "entities": []}, {"text": "A variety of NLP tasks have made good use of vector-based models.", "labels": [], "entities": []}, {"text": "Examples include automatic thesaurus extraction, word sense discrimination) and disambiguation (), collocation extraction (), text segmentation () , and notably information retrieval (.", "labels": [], "entities": [{"text": "automatic thesaurus extraction", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6626689235369364}, {"text": "word sense discrimination", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6944805582364401}, {"text": "collocation extraction", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.7239377498626709}, {"text": "text segmentation", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.754759669303894}, {"text": "information retrieval", "start_pos": 161, "end_pos": 182, "type": "TASK", "confidence": 0.8247891068458557}]}, {"text": "In cognitive science vector-based models have been successful in simulating semantic priming) and text comprehension).", "labels": [], "entities": []}, {"text": "Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments) and word association norms).", "labels": [], "entities": []}, {"text": "Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.", "labels": [], "entities": []}, {"text": "In fact, the commonest method for combining the vectors is to average them.", "labels": [], "entities": []}, {"text": "Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary.", "labels": [], "entities": [{"text": "Vector averaging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9221014082431793}]}, {"text": "This is illustrated in the example below taken from Landauer et al..", "labels": [], "entities": []}, {"text": "Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.", "labels": [], "entities": []}, {"text": "It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.", "labels": [], "entities": []}, {"text": "While vector addition has been effective in some applications such as essay grading) and coherence assessment (), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing ( and modulate cognitive behavior in sentence priming ( and inference tasks.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7583012878894806}, {"text": "coherence assessment", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.7042512446641922}, {"text": "sentence and discourse processing", "start_pos": 217, "end_pos": 250, "type": "TASK", "confidence": 0.6772576570510864}]}, {"text": "Computational models of semantics which use symbolic logic representations can account naturally for the meaning of phrases or sentences.", "labels": [], "entities": []}, {"text": "Central in these models is the notion of compositionality -the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.", "labels": [], "entities": []}, {"text": "Here, semantic analysis is guided by syntactic structure, and therefore sentences (1-a) and (1-b) receive distinct representations.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.8235565423965454}]}, {"text": "The downside of this approach is that differences in meaning are qualitative rather than quantitative, and degrees of similarity cannot be expressed easily.", "labels": [], "entities": []}, {"text": "In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7229395508766174}]}, {"text": "We present a general framework for vector-based composition which allows us to consider different classes of models.", "labels": [], "entities": []}, {"text": "Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.", "labels": [], "entities": [{"text": "sentence similarity rating", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.6683401862780253}]}, {"text": "Our results show that the multiplicative models are superior and correlate significantly with behavioral data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by.", "labels": [], "entities": [{"text": "sentence similarity task", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7470690906047821}]}, {"text": "In his study, Kintsch builds a model of how a verb's meaning is modified in the context of its subject.", "labels": [], "entities": []}, {"text": "He argues that the subjects of ran in The color ran and The horse ran select different senses of ran.", "labels": [], "entities": []}, {"text": "This change in the verb's sense is equated to a shift in its position in semantic space.", "labels": [], "entities": []}, {"text": "To quantify this shift, Kintsch proposes measuring similarity relative to other verbs acting as landmarks, for example gallop and dissolve.", "labels": [], "entities": []}, {"text": "The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.", "labels": [], "entities": []}, {"text": "Conversely, when color is combined with ran, the resulting vector will be closer to dissolve than gallop.", "labels": [], "entities": []}, {"text": "Focusing on a single compositional structure, namely intransitive verbs and their subjects, is a good point of departure for studying vector combination.", "labels": [], "entities": []}, {"text": "Any adequate model of composition must be able to represent argument-verb meaning.", "labels": [], "entities": []}, {"text": "Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.", "labels": [], "entities": []}, {"text": "Unfortunately, demonstrates how his own composition algorithm works intuitively on a few hand selected examples but does not provide a comprehensive test set.", "labels": [], "entities": []}, {"text": "In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.", "labels": [], "entities": []}, {"text": "In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.", "labels": [], "entities": []}, {"text": "We evaluated the proposed composition models in two ways.", "labels": [], "entities": []}, {"text": "First, we used the models to estimate the cosine similarity between the reference sentence and its landmarks.", "labels": [], "entities": []}, {"text": "We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 46, "end_pos": 63, "type": "METRIC", "confidence": 0.9461041688919067}]}, {"text": "A more scrupulous evaluation requires directly correlating all the individual participants' similarity judgments with those of the models.", "labels": [], "entities": []}, {"text": "We used Spearman's \u03c1 for our correlation analyses.", "labels": [], "entities": []}, {"text": "Again, better models should correlate better with the experimental data.", "labels": [], "entities": []}, {"text": "We assume that the inter-subject agreement can serve as an upper bound for comparing the fit of our models against the human judgments.", "labels": [], "entities": []}], "tableCaptions": []}