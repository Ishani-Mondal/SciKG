{"title": [{"text": "Task-oriented Evaluation of Syntactic Parsers and Their Representations", "labels": [], "entities": [{"text": "Task-oriented Evaluation of Syntactic Parsers and Their Representations", "start_pos": 0, "end_pos": 71, "type": "TASK", "confidence": 0.6703253053128719}]}], "abstractContent": [{"text": "This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks.", "labels": [], "entities": []}, {"text": "Our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (PPI) identification in biomedical papers.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7398099303245544}, {"text": "protein-protein interaction (PPI) identification in biomedical papers", "start_pos": 134, "end_pos": 203, "type": "TASK", "confidence": 0.7574718727005852}]}, {"text": "We evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using five different parse representations.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.706129252910614}, {"text": "phrase structure parsing", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6635798911253611}]}, {"text": "We run a PPI system with several combinations of parser and parse representation , and examine their impact on PPI identification accuracy.", "labels": [], "entities": [{"text": "PPI identification", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8608900010585785}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8445904850959778}]}, {"text": "Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.998437225818634}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9989136457443237}]}], "introductionContent": [{"text": "Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks, but also include dependency parsers and deep parsers (.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.7642048597335815}]}, {"text": "However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited.", "labels": [], "entities": []}, {"text": "The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy.", "labels": [], "entities": [{"text": "parser comparison", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.971973180770874}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9224540591239929}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.4718036651611328}, {"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9902698993682861}]}, {"text": "This assumes the existence of a gold-standard test corpus, such as the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9947872459888458}]}, {"text": "It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and differ from parser to parser ().", "labels": [], "entities": []}, {"text": "The lack of such comparisons is a serious obstacle for NLP researchers in choosing an appropriate parser for their purposes.", "labels": [], "entities": []}, {"text": "In this paper, we present a comparative evaluation of syntactic parsers and their output representations based on different frameworks: dependency parsing, phrase structure parsing, and deep parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.81404909491539}, {"text": "phrase structure parsing", "start_pos": 156, "end_pos": 180, "type": "TASK", "confidence": 0.7459968328475952}]}, {"text": "Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.9738296270370483}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.998082160949707}, {"text": "identifying protein-protein interaction (PPI) information in biomedical papers", "start_pos": 84, "end_pos": 162, "type": "TASK", "confidence": 0.6846369832754136}]}, {"text": "PPI identification is a reasonable task for parser evaluation, because it is atypical information extraction (IE) application, and because recent studies have shown the effectiveness of syntactic parsing in this task.", "labels": [], "entities": [{"text": "PPI identification", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8508653938770294}, {"text": "parser evaluation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.9721136689186096}, {"text": "information extraction (IE)", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.8085262060165406}, {"text": "syntactic parsing", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.7132676541805267}]}, {"text": "Since our evaluation method is applicable to any parser output, and is grounded in areal application, it allows fora fair comparison of syntactic parsers based on different frameworks.", "labels": [], "entities": []}, {"text": "Parser evaluation in PPI extraction also illuminates domain portability.", "labels": [], "entities": [{"text": "PPI extraction", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9457093179225922}]}, {"text": "Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) portion of the Penn Treebank, and high accuracy has been reported for WSJ text; however, these parsers rely on lexical information to attain high accuracy, and it has been criticized that these parsers may overfit to WSJ text.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 64, "end_pos": 118, "type": "DATASET", "confidence": 0.926232787695798}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9986534118652344}, {"text": "accuracy", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.9810882806777954}]}, {"text": "Another issue for discussion is the portability of training methods.", "labels": [], "entities": []}, {"text": "When training data in the target domain is available, as is the case with the GENIA Treebank ( for biomedical papers, a parser can be retrained to adapt to the target domain, and larger accuracy improvements are expected, if the training method is sufficiently general.", "labels": [], "entities": [{"text": "GENIA Treebank", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9839470088481903}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.997885525226593}]}, {"text": "We will examine these two aspects of domain portability by comparing the original parsers with the retrained parsers.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our approach to parser evaluation, we measure the accuracy of a PPI extraction system, in which This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9640485644340515}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.999502420425415}, {"text": "PPI extraction", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.827442854642868}]}, {"text": "The molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR) is not useful to assess vitamin A status during infection in hospitalised children.", "labels": [], "entities": [{"text": "molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR)", "start_pos": 4, "end_pos": 77, "type": "METRIC", "confidence": 0.7596565676586968}]}, {"text": "the parser output is embedded as statistical features of a machine learning classifier.", "labels": [], "entities": []}, {"text": "We run a classifier with features of every possible combination of a parser and a parse representation, by applying conversions between representations when necessary.", "labels": [], "entities": []}, {"text": "We also measure the accuracy improvements obtained by parser retraining with GENIA, to examine the domain portability, and to evaluate the effectiveness of domain adaptation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9992290735244751}, {"text": "parser retraining", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.913021594285965}, {"text": "GENIA", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.9312238693237305}, {"text": "domain adaptation", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.6934676021337509}]}, {"text": "In the following experiments, we used AImed (), which is a popular corpus for the evaluation of PPI extraction systems.", "labels": [], "entities": [{"text": "PPI extraction", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.8619013726711273}]}, {"text": "The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.", "labels": [], "entities": []}, {"text": "We use gold protein annotations given in the corpus.", "labels": [], "entities": []}, {"text": "Multi-word protein names are concatenated and treated as single words.", "labels": [], "entities": []}, {"text": "The accuracy is measured by abstract-wise 10-fold cross validation and the one-answer-per-occurrence criterion ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994919300079346}]}, {"text": "A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for each setting.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.8402016162872314}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9993380904197693}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9989688396453857}]}, {"text": "show the accuracy obtained by using the output of each parser in each parse representation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995115995407104}]}, {"text": "The row \"baseline\" indicates the accuracy obtained with bag-of-words features.", "labels": [], "entities": [{"text": "baseline", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9265310168266296}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9992260932922363}]}, {"text": "shows the time for parsing the entire AImed corpus, and shows the time required for 10-fold cross validation with GENIA-retrained parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9812731742858887}, {"text": "AImed corpus", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.6950941979885101}, {"text": "GENIA-retrained", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.8684789538383484}]}], "tableCaptions": [{"text": " Table 1: Accuracy on the PPI task with WSJ-trained parsers (precision/recall/f-score)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9967969059944153}, {"text": "precision/recall/f-score", "start_pos": 61, "end_pos": 85, "type": "METRIC", "confidence": 0.7388596892356872}]}, {"text": " Table 2: Accuracy on the PPI task with GENIA-retrained parsers (precision/recall/f-score)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975208640098572}, {"text": "GENIA-retrained", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.6602978706359863}, {"text": "precision/recall/f-score", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.7496330857276916}]}, {"text": " Table 3: Parsing time (sec.)", "labels": [], "entities": [{"text": "Parsing time", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.6775737404823303}]}, {"text": " Table 4: Evaluation time (sec.)", "labels": [], "entities": []}]}