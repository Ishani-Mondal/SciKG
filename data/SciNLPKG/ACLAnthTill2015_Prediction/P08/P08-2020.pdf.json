{"title": [{"text": "Recent Improvements in the CMU Large Scale Chinese-English SMT System", "labels": [], "entities": [{"text": "CMU Large Scale Chinese-English SMT", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.7678866147994995}]}], "abstractContent": [{"text": "In this paper we describe recent improvements to components and methods used in our statistical machine translation system for Chinese-English used in the January 2008 GALE evaluation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.630191574494044}, {"text": "GALE evaluation", "start_pos": 168, "end_pos": 183, "type": "DATASET", "confidence": 0.5732889622449875}]}, {"text": "Main improvements are results of consistent data processing, larger statistical models and a POS-based word reordering approach .", "labels": [], "entities": [{"text": "POS-based word reordering", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.6399603088696798}]}], "introductionContent": [{"text": "Building a full scale Statistical Machine Translation (SMT) system involves many preparation and training steps and it consists of several components, each of which contribute to the overall system performance.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.825944205125173}]}, {"text": "Between 2007 and 2008 our system improved by 5 points in BLEU from 26.60 to 31.85 for the unseen MT06 test set, which can be mainly attributed to two major points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9971579313278198}, {"text": "MT06 test set", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9176541368166605}]}, {"text": "The fast growth of computing resources over the years make it possible to use larger and larger amounts of data in training.", "labels": [], "entities": []}, {"text": "In Section 3 we show how parallelizing model training can reduce training time by an order of magnitude and how using larger training data as well as more extensive models improve translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 180, "end_pos": 191, "type": "TASK", "confidence": 0.9665406346321106}]}, {"text": "Word reordering is still a difficult problem in SMT.", "labels": [], "entities": [{"text": "Word reordering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7170580923557281}, {"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9935909509658813}]}, {"text": "In Section 4 we apply a Part Of Speech (POS) based syntactic reordering model successfully to our large Chinese system.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper we report results using the BLEU metric (), however as the evaluation criterion in GALE is HTER), we also report in TER ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.980270504951477}, {"text": "GALE", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.7183461785316467}, {"text": "HTER", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9657794833183289}, {"text": "TER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.998498797416687}]}, {"text": "We used the test sets from the NIST MT evaluations from the years 2003 and 2006 as development and unseen test data.", "labels": [], "entities": [{"text": "NIST MT evaluations", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.8787505229314169}]}], "tableCaptions": [{"text": " Table 1: Number tagging experiments, BLEU/TER", "labels": [], "entities": [{"text": "Number tagging", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8868073523044586}, {"text": "BLEU/TER", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7828392585118612}]}, {"text": " Table 2: 4-and 5-gram LM,260M system, BLEU/TER", "labels": [], "entities": [{"text": "BLEU/TER", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8006376425425211}]}, {"text": " Table 4: In-domain only or all training data, BLEU/TER", "labels": [], "entities": [{"text": "BLEU/TER", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.7499792575836182}]}, {"text": " Table 5: Reordering lattice decoding in BLEU/TER", "labels": [], "entities": [{"text": "Reordering lattice decoding", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8871273597081503}, {"text": "BLEU/TER", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.7246978084246317}]}]}