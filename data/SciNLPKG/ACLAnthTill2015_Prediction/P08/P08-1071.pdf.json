{"title": [{"text": "Assessing Dialog System User Simulation Evaluation Measures Using Human Judges", "labels": [], "entities": [{"text": "Assessing Dialog System User Simulation Evaluation Measures", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.9270342673574176}]}], "abstractContent": [{"text": "Previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systems' logs.", "labels": [], "entities": []}, {"text": "However, the validity of these automatic measures has not been fully proven.", "labels": [], "entities": []}, {"text": "In this study, we first recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures.", "labels": [], "entities": []}, {"text": "We observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives.", "labels": [], "entities": []}, {"text": "However, the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models.", "labels": [], "entities": []}, {"text": "When building prediction models of human judgments using previously proposed automatic measures, we find that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model.", "labels": [], "entities": []}], "introductionContent": [{"text": "User simulation has been widely used in different phases in spoken dialog system development.", "labels": [], "entities": [{"text": "User simulation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7121507525444031}, {"text": "spoken dialog system development", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.7687824815511703}]}, {"text": "In the system development phase, user simulation is used in training different system components.", "labels": [], "entities": []}, {"text": "For example, () and) exploit user simulations to generate large corpora for using Reinforcement Learning to develop dialog strategies, while) implement user simulation to train the speech recognizer and understanding components.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 181, "end_pos": 198, "type": "TASK", "confidence": 0.6821128129959106}]}, {"text": "While user simulation is considered to be more low-cost and time-efficient than experiments with human subjects, one major concern is how well the state-of-the-art user simulations can mimic human user behaviors and how well they can substitute for human users in a variety of tasks.", "labels": [], "entities": []}, {"text": "() propose a set of evaluation measures to assess the quality of simulated corpora.", "labels": [], "entities": []}, {"text": "They find that these evaluation measures are sufficient to discern simulated from real dialogs.", "labels": [], "entities": []}, {"text": "Since this multiple-measure approach does not offer a easily reportable statistic indicating the quality of a user simulation, proposes a single measure for evaluating and rank-ordering user simulations based on the divergence between the simulated and real users' performance.", "labels": [], "entities": []}, {"text": "This new approach also offers a lookup table that helps to judge whether an observed ordering of two user simulations is statistically significant.", "labels": [], "entities": []}, {"text": "In this study, we also strive to develop a prediction model of the rankings of the simulated users' performance.", "labels": [], "entities": []}, {"text": "However, our approach use human judgments as the gold standard.", "labels": [], "entities": []}, {"text": "Although to date there are few studies that use human judges to directly assess the quality of user simulation, we believe that this is a reliable approach to assess the simulated corpora as well as an important step towards developing a comprehensive set of user simulation evaluation measures.", "labels": [], "entities": []}, {"text": "First, we can estimate the difficulty of the task of distinguishing real and simulated corpora by knowing how hard it is for human judges to reach an agreement.", "labels": [], "entities": []}, {"text": "Second, human judgments can be used as the gold standard of the automatic evaluation measures.", "labels": [], "entities": []}, {"text": "Third, we can validate the automatic measures by correlating the conclusions drawn from the automatic measures with the human judgments.", "labels": [], "entities": []}, {"text": "In this study, we recruit human judges to assess the quality of three user simulation models.", "labels": [], "entities": []}, {"text": "Judges are asked to read the transcripts of the dialogs between a computer tutoring system and the simulation models and to rate the dialogs on a 5-point scale from different perspectives.", "labels": [], "entities": []}, {"text": "Judges are also given the transcripts between human users and the computer tutor.", "labels": [], "entities": []}, {"text": "We first assess human judges' abilities in distinguishing real from simulated users.", "labels": [], "entities": []}, {"text": "We find that it is hard for human judges to reach good agreement on the ratings.", "labels": [], "entities": []}, {"text": "However, these ratings give consistent ranking on the quality of the real and the simulated user models.", "labels": [], "entities": []}, {"text": "Similarly, when we use previously proposed automatic measures to predict human judgments, we cannot reliably predict human ratings using a regression model, but we can consistently mimic human judges' rankings using a ranking model.", "labels": [], "entities": []}, {"text": "We suggest that this ranking model can be used to quickly assess the quality of anew simulation model without manual efforts by ranking the new model against the old models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Percent agreements on 5-point scale", "labels": [], "entities": []}, {"text": " Table 2: Agreements on 3-point scale", "labels": [], "entities": []}, {"text": " Table 3: Confusion Matrix on d TUR", "labels": [], "entities": [{"text": "d TUR", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.6481778174638748}]}, {"text": " Table 4: Rankings on Dialog Level Questions", "labels": [], "entities": []}, {"text": " Table 6: A Made-up Example of the Ranking Model", "labels": [], "entities": []}, {"text": " Table 7: LOSS scores for Regular and Minus-one-model  (during training) Cross Validations", "labels": [], "entities": [{"text": "LOSS", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9826995134353638}]}]}