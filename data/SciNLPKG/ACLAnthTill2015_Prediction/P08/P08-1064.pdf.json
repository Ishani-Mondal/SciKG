{"title": [{"text": "A Tree Sequence Alignment-based Tree-to-Tree Translation Model", "labels": [], "entities": [{"text": "Tree Sequence Alignment-based Tree-to-Tree Translation", "start_pos": 2, "end_pos": 56, "type": "TASK", "confidence": 0.7610499739646912}]}], "abstractContent": [{"text": "This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of sub-trees that covers a phrase.", "labels": [], "entities": [{"text": "tree sequence alignment", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6925416588783264}]}, {"text": "The model leverages on the strengths of both phrase-based and linguistically syntax-based method.", "labels": [], "entities": []}, {"text": "It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned bi-parsed parallel texts.", "labels": [], "entities": []}, {"text": "Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span.", "labels": [], "entities": [{"text": "multi-level structure reordering", "start_pos": 159, "end_pos": 191, "type": "TASK", "confidence": 0.6445717016855875}]}, {"text": "This gives our model stronger expressive power than other reported models.", "labels": [], "entities": []}, {"text": "Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.", "labels": [], "entities": [{"text": "NIST MT-2005 Chinese-English translation task", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7762227654457092}]}], "introductionContent": [{"text": "Phrase-based modeling method () is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.", "labels": [], "entities": [{"text": "Phrase-based modeling", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7996614277362823}, {"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7365584373474121}]}, {"text": "However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features).", "labels": [], "entities": []}, {"text": "Recently, many syntax-based models have been proposed to address the above deficiencies (;; ;.", "labels": [], "entities": []}, {"text": "Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.", "labels": [], "entities": [{"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9902417063713074}, {"text": "non-syntactic phrase modeling", "start_pos": 167, "end_pos": 196, "type": "TASK", "confidence": 0.703214148680369}]}, {"text": "In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.", "labels": [], "entities": [{"text": "tree-to-tree translation", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6593110859394073}]}, {"text": "It is designed to combine the strengths of phrase-based and syntax-based methods.", "labels": [], "entities": []}, {"text": "The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.", "labels": [], "entities": []}, {"text": "Therefore, it not only describes non-syntactic phrases with syntactic structure information, but also supports multi-level tree structure reordering in larger span.", "labels": [], "entities": [{"text": "multi-level tree structure reordering", "start_pos": 111, "end_pos": 148, "type": "TASK", "confidence": 0.6216180473566055}]}, {"text": "These give our model much more expressive power and flexibility than those previous models.", "labels": [], "entities": []}, {"text": "Experiment results on the NIST MT-2005 ChineseEnglish translation task show that our method significantly outperforms Moses (, a state-of-the-art phrase-based SMT system, and other linguistically syntax-based methods, such as SCFG-based and STSG-based methods (.", "labels": [], "entities": [{"text": "NIST MT-2005 ChineseEnglish translation task", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.7732382655143738}, {"text": "SMT", "start_pos": 159, "end_pos": 162, "type": "TASK", "confidence": 0.7982528805732727}]}, {"text": "In addition, our study further demonstrates that 1) structure reordering rules in our model are very useful for performance improvement while discontinuous phrase rules have less contribution and 2) tree sequence rules are able to model non-syntactic phrases with syntactic structure information, and thus contribute much to the performance improvement, but those rules consisting of more than three sub-trees have almost no contribution.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 reviews previous work.", "labels": [], "entities": []}, {"text": "Section 3 elabo-rates the modelling process while Sections 4 and 5 discuss the training and decoding algorithms.", "labels": [], "entities": []}, {"text": "The experimental results are reported in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted Chinese-to-English translation experiments.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.6644217520952225}]}, {"text": "We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits) with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9557896554470062}, {"text": "English Gigaword corpus", "start_pos": 135, "end_pos": 158, "type": "DATASET", "confidence": 0.785855213801066}]}, {"text": "We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT-2005 test set as our test set.", "labels": [], "entities": [{"text": "NIST MT-2002 test set", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.9277339428663254}, {"text": "NIST MT-2005 test set", "start_pos": 109, "end_pos": 130, "type": "DATASET", "confidence": 0.941639170050621}]}, {"text": "We used the Stanford parser ( to parse bilingual sentences on the training set and Chinese sentences on the development and test sets.", "labels": [], "entities": []}, {"text": "The evaluation metric is case-sensitive BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9854169487953186}]}, {"text": "We used GIZA++ () and the heuristics \"grow-diag-final\" to generate m-to-n word alignments.", "labels": [], "entities": []}, {"text": "For the MER training, we modified Koehn's MER trainer) for our tree sequence-based system.", "labels": [], "entities": [{"text": "MER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.979610800743103}]}, {"text": "For significance test, we used Zhang et al's implementation ().", "labels": [], "entities": [{"text": "significance", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7086155414581299}]}, {"text": "We set three baseline systems: Moses (, and SCFG-based and STSG-based treeto-tree translation models ().", "labels": [], "entities": []}, {"text": "For Moses, we used its default settings.", "labels": [], "entities": []}, {"text": "For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h 4 d = and 6 h = for our model).", "labels": [], "entities": [{"text": "SCFG/STSG", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.6902246872584025}]}, {"text": "We optimized these parameters on the training and development sets: c =3, \u03b1 =20, \u03b2 =-100 and \u03b3 =100.", "labels": [], "entities": []}, {"text": "We carried out a number of experiments to examine the proposed tree sequence alignment-based translation model.", "labels": [], "entities": [{"text": "tree sequence alignment-based translation", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.7049231827259064}]}, {"text": "In this subsection, we first report the rule distributions and compare our model with the three baseline systems.", "labels": [], "entities": []}, {"text": "Then we study the model's expressive ability by comparing the contributions made by different kinds of rules, including strict tree sequence rules, non-syntactic phrase rules, structure reordering rules and discontinuous phrase rules 2 . Finally, we investigate the impact of maximal sub-tree number and sub-tree depth in our model.", "labels": [], "entities": []}, {"text": "All of the following discussions are held on the training and test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: # of rules used in the testing (", "labels": [], "entities": []}, {"text": " Table 2: Contributions of TSRs (see", "labels": [], "entities": []}, {"text": " Table 3: Effect of Structure Reordering Rules (SRR:  refers to the structure reordering rules that have at least  two non-terminal leaf nodes with inverted order in the  source and target sides, which are usually not captured  by phrase-based models. Note that the reordering be- tween lexical words and non-terminal leaf nodes is not  considered here) and Discontinuous Phrase Rules (DPR:  refers to these rules having at least one non-terminal  leaf node between two lexicalized leaf nodes) in our  tree sequence-based model (", "labels": [], "entities": []}, {"text": " Table 4: numbers of SRR and DPR rules", "labels": [], "entities": [{"text": "numbers", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.939825177192688}, {"text": "SRR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.6932343244552612}]}]}