{"title": [{"text": "Applying a Grammar-based Language Model to a Simplified Broadcast-News Transcription Task", "labels": [], "entities": [{"text": "Simplified Broadcast-News Transcription", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.6821386416753134}]}], "abstractContent": [{"text": "We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree.", "labels": [], "entities": []}, {"text": "The language model is applied by means of an N-best rescor-ing step, which allows to directly measure the performance gains relative to the baseline system without rescoring.", "labels": [], "entities": []}, {"text": "To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task.", "labels": [], "entities": [{"text": "broad-domain speech recognition tasks", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.698578268289566}, {"text": "German broadcast-news transcription task", "start_pos": 145, "end_pos": 185, "type": "TASK", "confidence": 0.6173233985900879}]}, {"text": "We report a significant reduction in word error rate compared to a state-of-the-art baseline system.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.6161497533321381}]}], "introductionContent": [{"text": "It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance.", "labels": [], "entities": []}, {"text": "More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g., and.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8385146856307983}]}, {"text": "Other linguistically inspired language models like and have been applied to continuous speech recognition.", "labels": [], "entities": [{"text": "continuous speech recognition", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.6930245558420817}]}, {"text": "These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of.", "labels": [], "entities": []}, {"text": "The probability of a rule expansion or parser operation is conditioned on various contextual information and the derivation history.", "labels": [], "entities": []}, {"text": "An important reason for the success of these models is the fact that they are lexicalized: the probability distributions are also conditioned on the actual words occuring in the utterance, and not only on their parts of speech.", "labels": [], "entities": []}, {"text": "Most statistical parsers achieve a high robustness with respect to out-of-grammar sentences by allowing for arbitrary derivations and rule expansions.", "labels": [], "entities": []}, {"text": "On the other hand, they are not suited to reliably decide on the grammaticality of a given phrase, as they do not accurately model the linguistic constraints inherent in natural language.", "labels": [], "entities": []}, {"text": "We take a completely different position.", "labels": [], "entities": []}, {"text": "In the first place, we want our language model to reliably distinguish between grammatical and ungrammatical phrases.", "labels": [], "entities": []}, {"text": "To this end, we have developed a precise, linguistically motivated grammar.", "labels": [], "entities": []}, {"text": "To distinguish between common and uncommon phrases, we use a statistical model that estimates the probability of a phrase based on the syntactic dependencies established by the parser.", "labels": [], "entities": []}, {"text": "We achieve some degree of robustness by letting the grammar accept arbitrary sequences of words and phrases.", "labels": [], "entities": []}, {"text": "To keep the grammar restrictive, such sequences are penalized by the statistical model.", "labels": [], "entities": []}, {"text": "Accurate hand-crafted grammars have been applied to speech recognition before, e.g. and van.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7511755228042603}]}, {"text": "However, they primarily served as a basis fora speech understanding component and were applied to narrowdomain tasks such as appointment scheduling or public transport information.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7689515352249146}, {"text": "appointment scheduling or public transport information", "start_pos": 125, "end_pos": 179, "type": "TASK", "confidence": 0.6863969614108404}]}, {"text": "We are mainly concerned with speech recognition performance on broad-domain recognition tasks.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8306009769439697}, {"text": "broad-domain recognition tasks", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.7690847218036652}]}, {"text": "However, their grammar-based language model did not make use of a probabilistic component, and it was applied to a rather simple recognition task (dictation texts for pupils read and recorded under good acoustic conditions, no out-of-vocabulary words).", "labels": [], "entities": []}, {"text": "Besides proposing an improved language model, this paper presents experimental results fora much more difficult and realistic task and compares them to the performance of a state-of-the-art baseline system.", "labels": [], "entities": []}, {"text": "In the following Section, we will first describe our grammar-based language model.", "labels": [], "entities": []}, {"text": "Next, we will turn to the linguistic components of the model, namely the grammar, the lexicon and the parser.", "labels": [], "entities": []}, {"text": "We will point out some of the challenges arising from the broad-domain speech recognition application and propose ways to deal with them.", "labels": [], "entities": [{"text": "broad-domain speech recognition", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.6505747040112814}]}, {"text": "Finally, we will describe our experiments on broadcast news data and discuss the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment was designed to measure how much a given speech recognition system can benefit from our grammar-based language model.", "labels": [], "entities": []}, {"text": "To this end, we used a baseline speech recognition system which provided the N best hypotheses of an utterance along with their respective scores.", "labels": [], "entities": []}, {"text": "The grammarbased language model was then applied to the N best hypotheses as described in Section 2.1, yielding anew best hypothesis.", "labels": [], "entities": []}, {"text": "For a given test set we could then compare the word error rate of the baseline system with that of the extended system employing the grammar-based language model.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.714301586151123}]}], "tableCaptions": []}