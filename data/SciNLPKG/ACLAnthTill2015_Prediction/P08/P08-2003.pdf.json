{"title": [{"text": "Improving the Performance of the Random Walk Model for Answering Complex Questions", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9868693947792053}, {"text": "Answering Complex Questions", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.9408457279205322}]}], "abstractContent": [{"text": "We consider the problem of answering complex questions that require inferencing and synthesizing information from multiple documents and can be seen as a kind of topic-oriented, informative multi-document summa-rization.", "labels": [], "entities": []}, {"text": "The stochastic, graph-based method for computing the relative importance of tex-tual units (i.e. sentences) is very successful in generic summarization.", "labels": [], "entities": [{"text": "generic summarization", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.863394558429718}]}, {"text": "In this method, a sentence is encoded as a vector in which each component represents the occurrence frequency (TF*IDF) of a word.", "labels": [], "entities": [{"text": "occurrence frequency (TF*IDF)", "start_pos": 89, "end_pos": 118, "type": "METRIC", "confidence": 0.7822134920528957}]}, {"text": "However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information.", "labels": [], "entities": [{"text": "TF*IDF", "start_pos": 37, "end_pos": 43, "type": "TASK", "confidence": 0.563675324122111}]}, {"text": "In this paper , we study the impact of syntactic and shallow semantic information in the graph-based method for answering complex questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "After having made substantial headway in factoid and list questions, researchers have turned their attention to more complex information needs that cannot be answered by simply extracting named entities like persons, organizations, locations, dates, etc.", "labels": [], "entities": []}, {"text": "Unlike informationally-simple factoid questions, complex questions often seek multiple different types of information simultaneously and do not presupposed that one single answer could meet all of its information needs.", "labels": [], "entities": []}, {"text": "For example, with complex questions like \"What are the causes of AIDS?\", the wider focus of this question suggests that the submitter may not have a single or well-defined information need and therefore maybe amenable to receiving additional supporting information that is relevant to some (as yet) undefined informational goal.", "labels": [], "entities": []}, {"text": "This type of questions require inferencing and synthesizing information from multiple documents.", "labels": [], "entities": []}, {"text": "In Natural Language Processing (NLP), this information synthesis can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7041216890017191}, {"text": "information synthesis", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7281276732683182}]}, {"text": "Recently, the graph-based method (LexRank) is applied successfully to generic, multi-document summarization (. A topicsensitive LexRank is proposed in).", "labels": [], "entities": []}, {"text": "In this method, a sentence is mapped to a vector in which each element represents the occurrence frequency (TF*IDF) of a word.", "labels": [], "entities": [{"text": "occurrence frequency (TF*IDF)", "start_pos": 86, "end_pos": 115, "type": "METRIC", "confidence": 0.7730882849012103}]}, {"text": "However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information thus cannot distinguish between \"The hero killed the villain\" and \"The villain killed the hero\".", "labels": [], "entities": []}, {"text": "The task like answering complex questions that requires the use of more complex syntactic and semantics, the approaches with only TF*IDF are often inadequate to perform fine-level textual analysis.", "labels": [], "entities": []}, {"text": "In this paper, we extensively study the impact of syntactic and shallow semantic information in measuring similarity between the sentences in the random walk model for answering complex questions.", "labels": [], "entities": []}, {"text": "We argue that for this task, similarity measures based on syntactic and semantic information performs better and can be used to characterize the relation between a question and a sentence (answer) in a more effective way than the traditional TF*IDF based similarity measures.", "labels": [], "entities": [{"text": "characterize the relation between a question and a sentence (answer)", "start_pos": 128, "end_pos": 196, "type": "TASK", "confidence": 0.780356302857399}]}], "datasetContent": [{"text": "The Document Understanding Conference (DUC) series is run by the National Institute of Standards and Technology (NIST) to further progress in summarization and enable researchers to participate in large-scale experiments.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.7964362104733785}, {"text": "summarization", "start_pos": 142, "end_pos": 155, "type": "TASK", "confidence": 0.9796082973480225}]}, {"text": "We used the DUC 2007 datasets for evaluation.", "labels": [], "entities": [{"text": "DUC 2007 datasets", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.9851372241973877}]}, {"text": "We carried out automatic evaluation of our summaries using ROUGE) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.", "labels": [], "entities": [{"text": "summaries", "start_pos": 43, "end_pos": 52, "type": "TASK", "confidence": 0.958488941192627}, {"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9108512997627258}, {"text": "DUC", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9140570759773254}, {"text": "summarization evaluation", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.942062646150589}]}, {"text": "It measures summary quality by counting overlapping units such as the n-gram (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary.", "labels": [], "entities": []}, {"text": "ROUGE parameters were set as the same as DUC 2007 evaluation setup.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9134466648101807}, {"text": "DUC 2007 evaluation", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9267071882883707}]}, {"text": "All the ROUGE measures were calculated by running ROUGE-1.5.5 with stemming but no removal of stopwords.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9397666454315186}, {"text": "ROUGE-1.5.5", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.8590243458747864}]}, {"text": "The ROUGE run-time parameters are: ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a The purpose of our experiments is to study the impact of the syntactic and semantic representation for complex question answering task.", "labels": [], "entities": [{"text": "ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a", "start_pos": 35, "end_pos": 96, "type": "METRIC", "confidence": 0.9310337786491101}, {"text": "question answering task", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.764462540547053}]}, {"text": "To accomplish this, we generate summaries for the topics of DUC 2007 by each of our four systems defined as below: (1) TF*IDF: system is the original topic-sensitive LexRank described in Section 2 that uses the similarity measures based on tf*idf.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9254182577133179}, {"text": "TF*IDF", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.7834464311599731}]}, {"text": "(2) SYN: system measures the similarity between the sentences using the syntactic tree and the general tree kernel function defined in Section 4.1.", "labels": [], "entities": []}, {"text": "(3) SEM: system measures the similarity between the sentences using the shallow semantic tree and the shallow semantic tree kernel function defined in Section 4.2.", "labels": [], "entities": [{"text": "SEM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9348817467689514}]}, {"text": "(4) SYNSEM: system measures the similarity between the sentences using both the syntactic and shallow semantic trees and their associated kernels.", "labels": [], "entities": []}, {"text": "For each sentence it measures the syntactic and semantic similarity with the query and takes the average of these measures.", "labels": [], "entities": []}, {"text": "The comparison between the systems in terms of their F-scores is given in.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9902774691581726}]}, {"text": "The SYN system improves the ROUGE-1, ROUGE-L and ROUGE-W scores over the TF*IDF system by 2.84%, 0.53% and 2.14% respectively.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9772511720657349}, {"text": "ROUGE-L", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9144048094749451}, {"text": "ROUGE-W", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9654160141944885}, {"text": "IDF", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.500852108001709}]}, {"text": "The SEM system improves the ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-SU scores over the TF*IDF system by 8.46%, 6.54%, 6.56%, and 11.68%, and over the SYN system by 5.46%, 5.98%, 4.33%, and 12.97% respectively.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.8900459408760071}, {"text": "ROUGE-SU", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9187794327735901}]}, {"text": "The SYNSEM system improves the ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-SU scores over the TF*IDF system by 4.64%, 1.63%, 2.15%, and 4.06%, and over the SYN system by 1.74%, 1.09%, 0%, and 5.26% respectively.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.915867805480957}, {"text": "ROUGE-W", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.8389320373535156}, {"text": "ROUGE-SU", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9307960867881775}]}, {"text": "The SEM system improves the ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-SU scores over the SYNSEM system by 3.65%, 4.84%, 4.32%, and 7.33% respectively which indicates that including syntactic feature with the semantic feature degrades the performance.", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.911029040813446}]}], "tableCaptions": [{"text": " Table 1: ROUGE F-scores for different systems", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9930190443992615}, {"text": "F-scores", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.7572228908538818}]}]}