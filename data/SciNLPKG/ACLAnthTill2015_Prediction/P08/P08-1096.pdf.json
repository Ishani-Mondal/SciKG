{"title": [{"text": "An Entity-Mention Model for Coreference Resolution with Inductive Logic Programming", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9695698022842407}]}], "abstractContent": [{"text": "The traditional mention-pair model for coref-erence resolution cannot capture information beyond mention pairs for both learning and testing.", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8501242101192474}]}, {"text": "To deal with this problem, we present an expressive entity-mention model that performs coreference resolution at an entity level.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.936076819896698}]}, {"text": "The model adopts the Inductive Logic Programming (ILP) algorithm, which provides a relational way to organize different knowledge of entities and mentions.", "labels": [], "entities": []}, {"text": "The solution can explicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for corefer-ence decision.", "labels": [], "entities": [{"text": "corefer-ence decision", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.8473173975944519}]}, {"text": "The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task.", "labels": [], "entities": [{"text": "ACE data set", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.9795712232589722}, {"text": "coreference resolution task", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.9496892293294271}]}], "introductionContent": [{"text": "Coreference resolution is the process of linking multiple mentions that refer to the same entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9170250296592712}]}, {"text": "Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g.;;;).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.9539320468902588}]}, {"text": "Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing.", "labels": [], "entities": []}, {"text": "As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone.", "labels": [], "entities": []}, {"text": "An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs ().", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9334272742271423}]}, {"text": "Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level.", "labels": [], "entities": []}, {"text": "Classification is done to determine whether a mention is a referent of a partially found entity.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9362806081771851}]}, {"text": "A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results.", "labels": [], "entities": []}, {"text": "One problem that arises with the entity-mention model is how to represent the knowledge related to an entity.", "labels": [], "entities": []}, {"text": "Ina document, an entity may have more than one mention.", "labels": [], "entities": []}, {"text": "It is impractical to enumerate all the mentions in an entity and record their information in a single feature vector, as it would make the feature space too large.", "labels": [], "entities": []}, {"text": "Even worse, the number of mentions in an entity is not fixed, which would result in variant-length feature vectors and make trouble for normal machine learning algorithms.", "labels": [], "entities": []}, {"text": "A solution seen in previous work () is to design a set of first-order features summarizing the information of the mentions in an entity, for example, \"whether the entity has any mention that is a name alias of the active mention?\" or \"whether most of the mentions in the entity have the same headword as the active mention?\"", "labels": [], "entities": []}, {"text": "These features, nevertheless, are designed in an ad-hoc manner and lack the capability of describing each individual mention in an entity.", "labels": [], "entities": []}, {"text": "In this paper, we present a more expressive entity-mention model for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.9815289378166199}]}, {"text": "The model employs Inductive Logic Programming (ILP) to represent the relational knowledge of an active mention, an entity, and the mentions in the entity.", "labels": [], "entities": []}, {"text": "On top of this, a set of first-order rules is automatically learned, which can capture the information of each individual mention in an entity, as well as the global information of the entity, to make coreference decision.", "labels": [], "entities": []}, {"text": "Hence, our model has a more powerful representation capability than the traditional mention-pair or entity-mention model.", "labels": [], "entities": []}, {"text": "And our experimental results on the ACE data set shows the model is effective for coreference resolution.", "labels": [], "entities": [{"text": "ACE data set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9825575153032938}, {"text": "coreference resolution", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.9839559495449066}]}], "datasetContent": [{"text": "In our study, we did evaluation on the ACE-2003 corpus, which contains two data sets, training and devtest, used for training and testing respectively.", "labels": [], "entities": [{"text": "ACE-2003 corpus", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9844091534614563}]}, {"text": "Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews).", "labels": [], "entities": []}, {"text": "The number of entities with more than one mention, as well as the number of the contained mentions, is summarized in.", "labels": [], "entities": []}, {"text": "For both training and resolution, an input raw document was processed by a pipeline of NLP modules including Tokenizer, Part-of-Speech tagger, NP Chunker and Named-Entity (NE) Recognizer.", "labels": [], "entities": [{"text": "resolution", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9058712720870972}, {"text": "Part-of-Speech tagger", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.677648514509201}, {"text": "NP Chunker", "start_pos": 143, "end_pos": 153, "type": "DATASET", "confidence": 0.8154529631137848}]}, {"text": "Trained and tested on Penn WSJ TreeBank, the POS tagger could obtain an accuracy of 97% and the NP chunker could produce an F-measure above 94% ().", "labels": [], "entities": [{"text": "Penn WSJ TreeBank", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9506285190582275}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9994872808456421}, {"text": "NP chunker", "start_pos": 96, "end_pos": 106, "type": "TASK", "confidence": 0.5692284405231476}, {"text": "F-measure", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9950610995292664}]}, {"text": "Evaluated for the MUC-6 and MUC-7 Named-Entity task, the NER module () could provide an F-measure of 96.6% (MUC-6) and 94.1%(MUC-7).", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.8896030187606812}, {"text": "MUC-7", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.8310351371765137}, {"text": "F-measure", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9990384578704834}, {"text": "MUC-6", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.8643671274185181}, {"text": "MUC-7", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.8987888693809509}]}, {"text": "For evaluation,'s scoring algorithm was adopted to compute recall and precision rates.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9984190464019775}, {"text": "precision rates", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.9732564389705658}]}, {"text": "By default, the ALEPH algorithm only generates rules that have 100% accuracy for the training data.", "labels": [], "entities": [{"text": "ALEPH", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.8068804144859314}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.998643696308136}]}, {"text": "And each rule contains at most three predicates.", "labels": [], "entities": []}, {"text": "To accommodate for coreference resolution, we loosened the restrictions to allow rules that have above 50% accuracy and contain up to ten predicates.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.9654735326766968}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9981837868690491}]}, {"text": "Default parameters were applied for all the other settings in ALEPH as well as other learning algorithms used in the experiments.", "labels": [], "entities": []}, {"text": "examined the C4.5 algorithm 4 which is widely used for the coreference resolution task.", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.9540340105692545}]}, {"text": "The first line of the table shows the baseline system that employs the traditional mention-pair model (MP) as described in Section 3.1.", "labels": [], "entities": []}, {"text": "From the table, our baseline system achieves a recall of around 66%-68% and a precision of around 50%-60%.", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9996863603591919}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9996211528778076}]}, {"text": "The overall F-measure for NWire, NPaper and BNews is 60.4%, 57.9% and 62.9% respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9987313151359558}, {"text": "NWire", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8730725646018982}, {"text": "NPaper", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9228751063346863}, {"text": "BNews", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.6462603807449341}]}, {"text": "The results are comparable to those reported in) which uses similar features and gets an F-measure ranging in 50-60% for the same data set.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9992400407791138}]}, {"text": "As our system relies only on simple and knowledge-poor features, the achieved Fmeasure is around 2-4% lower than the state-of-theart systems do, like and) which utilized sophisticated semantic or realworld knowledge.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.600128173828125}]}, {"text": "Since ILP has a strong capability in knowledge management, our system could be further improved if such helpful knowledge is incorporated, which will be explored in our future work.", "labels": [], "entities": [{"text": "ILP", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.8864913582801819}, {"text": "knowledge management", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7923485338687897}]}, {"text": "The second line of is for the system that employs the entity-mention model (EM) with \"Any-X\" based entity features, as described in Section 3.2.", "labels": [], "entities": []}, {"text": "We can find that the EM model does not show superiority over the baseline MP model.", "labels": [], "entities": []}, {"text": "It achieves a higher precision (up to 2.6%), but a lower recall (2.9%), than MP.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9994982481002808}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9997047781944275}]}, {"text": "As a result, we only see \u00b10.4% difference between the F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9915162324905396}]}, {"text": "The results are consistent with the reports by that the entity-mention model with the \"Any-X\" first-order features performs worse than the normal mention-pair model.", "labels": [], "entities": []}, {"text": "In our study, we also tested the \"Most-X\" strategy for the first-order features as in (, but got similar results without much difference (\u00b10.5% F-measure) in perfor-mance.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.997702419757843}]}, {"text": "Besides, as with our entity-mention predicates described in Section 4.2, we also tried the \"All-X\" strategy for the entity-level agreement features, that is, whether all mentions in a partial entity agree in number and gender with an active mention.", "labels": [], "entities": []}, {"text": "However, we found this bring no improvement against the \"Any-X\" strategy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A sample text", "labels": [], "entities": []}, {"text": " Table 3: statistics of entities (length > 1) and contained  mentions", "labels": [], "entities": []}, {"text": " Table 4: Results of different systems for coreference resolution", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.9844538867473602}]}]}