{"title": [{"text": "Automatic Image Annotation Using Auxiliary Text Information", "labels": [], "entities": [{"text": "Automatic Image Annotation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6205781499544779}]}], "abstractContent": [{"text": "The availability of databases of images labeled with keywords is necessary for developing and evaluating image annotation models.", "labels": [], "entities": []}, {"text": "Dataset collection is however a costly and time consuming task.", "labels": [], "entities": [{"text": "Dataset collection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7500512003898621}]}, {"text": "In this paper we exploit the vast resource of images available on the web.", "labels": [], "entities": []}, {"text": "We create a database of pictures that are naturally embedded into news articles and propose to use their captions as a proxy for annotation keywords.", "labels": [], "entities": []}, {"text": "Experimental results show that an image annotation model can be developed on this dataset alone without the overhead of manual annotation.", "labels": [], "entities": []}, {"text": "We also demonstrate that the news article associated with the picture can be used to boost image annotation performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "As the number of image collections is rapidly growing, so does the need to browse and search them.", "labels": [], "entities": []}, {"text": "Recent years have witnessed significant progress in developing methods for image retrieval 1 , many of which are query-based.", "labels": [], "entities": [{"text": "image retrieval 1", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.8635256290435791}]}, {"text": "Given a database of images, each annotated with keywords, the query is used to retrieve relevant pictures under the assumption that the annotations can essentially capture their semantics.", "labels": [], "entities": []}, {"text": "One stumbling block to the widespread use of query-based image retrieval systems is obtaining the keywords for the images.", "labels": [], "entities": []}, {"text": "Since manual annotation is expensive, time-consuming and practically infeasible for large databases, there has been great in-terest in automating the image annotation process (see references).", "labels": [], "entities": []}, {"text": "More formally, given an image I with visual features V i = {v 1 , v 2 , . .", "labels": [], "entities": []}, {"text": ", v N } and a set of keywords W = {w 1 , w 2 , . .", "labels": [], "entities": []}, {"text": ", w M }, the task consists in finding automatically the keyword subset WI \u2282 W , which can appropriately describe the image I.", "labels": [], "entities": []}, {"text": "Indeed, several approaches have been proposed to solve this problem under a variety of learning paradigms.", "labels": [], "entities": []}, {"text": "These range from supervised classification (;) to instantiations of the noisy-channel model ( ), to clustering ( ), and methods inspired by information retrieval ().", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.7344985008239746}]}, {"text": "Obviously in order to develop accurate image annotation models, some manually labeled data is required.", "labels": [], "entities": []}, {"text": "Previous approaches have been developed and tested almost exclusively on the Corel database.", "labels": [], "entities": [{"text": "Corel database", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9737032949924469}]}, {"text": "The latter contains 600 CD-ROMs, each containing about 100 images representing the same topic or concept, e.g., people, landscape, male.", "labels": [], "entities": []}, {"text": "Each topic is associated with keywords and these are assumed to also describe the images under this topic.", "labels": [], "entities": []}, {"text": "As an example consider the pictures in which are classified under the topic male and have the description keywords man, male, people, cloth, and face.", "labels": [], "entities": []}, {"text": "Current image annotation methods work well when large amounts of labeled images are available but can run into severe difficulties when the number of images and keywords fora given topic is relatively small.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7032664865255356}]}, {"text": "Unfortunately, databases like Corel are few and far between and somewhat idealized.", "labels": [], "entities": [{"text": "Corel", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9473763108253479}]}, {"text": "Corel contains clusters of many closely related images which in turn share keyword descriptions, thus allowing models to learn image-keyword associations reliably.", "labels": [], "entities": []}, {"text": "It is unlikely that models trained on this database will perform well out-of-domain on other image collections which are more noisy and do not share these characteristics.", "labels": [], "entities": []}, {"text": "Furthermore, in order to develop robust image annotation models, it is crucial to have large and diverse datasets both for training and evaluation.", "labels": [], "entities": []}, {"text": "In this work, we aim to relieve the data acquisition bottleneck associated with automatic image annotation by taking advantage of resources where images and their annotations co-occur naturally.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7000875920057297}]}, {"text": "News articles associated with images and their captions spring readily to mind (e.g., BBC News, Yahoo News).", "labels": [], "entities": [{"text": "BBC News, Yahoo News", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.855159604549408}]}, {"text": "So, rather than laboriously annotating images with their keywords, we simply treat captions as labels.", "labels": [], "entities": []}, {"text": "These annotations are admittedly noisy and far from ideal.", "labels": [], "entities": []}, {"text": "Captions can be denotative (describing the objects the image depicts) but also connotative (describing sociological, political, or economic attitudes reflected in the image).", "labels": [], "entities": []}, {"text": "Importantly, our images are not standalone, they come with news articles whose content is shared with the image.", "labels": [], "entities": []}, {"text": "So, by processing the accompanying document, we can effectively learn about the image and reduce the effect of noise due to the approximate nature of the caption labels.", "labels": [], "entities": []}, {"text": "To give a simple example, if two words appear both in the caption and the document, it is more likely that the annotation is genuine.", "labels": [], "entities": []}, {"text": "In what follows, we present anew database consisting of articles, images, and their captions which we collected from an on-line news source.", "labels": [], "entities": []}, {"text": "We then propose an image annotation model which can learn from our noisy annotations and the auxiliary documents.", "labels": [], "entities": []}, {"text": "Specifically, we extend and modify continuous relevance model to suit our task.", "labels": [], "entities": []}, {"text": "Our experimental results show that this model can successfully scale to our database, without making use of explicit human annotations in anyway.", "labels": [], "entities": []}, {"text": "We also show that the auxiliary document contains important information for generating more accurate image descriptions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss our experimental design for assessing the performance of the model presented above.", "labels": [], "entities": []}, {"text": "We give details on our training procedure and parameter estimation, describe our features, and present the baseline methods used for comparison with our approach.", "labels": [], "entities": []}, {"text": "Our evaluation follows the experimental methodology proposed in . We are given an un-annotated image I and are asked to automatically produce suitable annotations for I.", "labels": [], "entities": []}, {"text": "Given a set of image regions VI , we use equation to derive the conditional distribution P(w|V I ).", "labels": [], "entities": []}, {"text": "We consider the k-best words as the annotations for I.", "labels": [], "entities": []}, {"text": "We present results using the top 10, 15, and 20 annotation words.", "labels": [], "entities": []}, {"text": "We assess our model's performance using precision/recall and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9997057318687439}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9523478150367737}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9997701048851013}]}, {"text": "In our task, precision is the percentage of correctly annotated words overall annotations that the system suggested.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9993991851806641}]}, {"text": "Recall, is the percentage of correctly annotated words over the number of genuine annotations in the test data.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9911389946937561}]}, {"text": "F1 is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9955728054046631}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9994508624076843}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9962784647941589}]}, {"text": "These measures are averaged over the set of test words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic image annotation results on the BBC News database.", "labels": [], "entities": [{"text": "BBC News database", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9699643850326538}]}]}