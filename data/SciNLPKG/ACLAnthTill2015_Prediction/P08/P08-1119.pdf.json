{"title": [{"text": "Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based extractions: popularity and productivity.", "labels": [], "entities": []}, {"text": "Intuitively , a candidate is popular if it was discovered many times by other instances in the hyponym pattern.", "labels": [], "entities": []}, {"text": "A candidate is productive if it frequently leads to the discovery of other instances.", "labels": [], "entities": []}, {"text": "Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.", "labels": [], "entities": []}, {"text": "We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances.", "labels": [], "entities": []}, {"text": "We conducted experiments on four semantic classes and consistently achieved high accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9816488027572632}]}], "introductionContent": [{"text": "Knowing the semantic classes of words (e.g., \"trout\" is a kind of FISH) can be extremely valuable for many natural language processing tasks.", "labels": [], "entities": [{"text": "FISH", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.7824582457542419}, {"text": "natural language processing", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.6549454132715861}]}, {"text": "Although some semantic dictionaries do exist (e.g.,), they are rarely complete, especially for large open classes (e.g., classes of people and objects) and rapidly changing categories (e.g., computer technology).", "labels": [], "entities": []}, {"text": "reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.971950352191925}]}, {"text": "Automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized categories or domains.", "labels": [], "entities": [{"text": "semantic lexicon acquisition", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7315085331598917}, {"text": "WordNet", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9443861246109009}]}, {"text": "A variety of methods have been developed for automatic semantic class identification, under the rubrics of lexical acquisition, hyponym acquisition, semantic lexicon induction, semantic class learning, and web-based information extraction.", "labels": [], "entities": [{"text": "semantic class identification", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.6221088469028473}, {"text": "hyponym acquisition", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.6831462383270264}, {"text": "semantic lexicon induction", "start_pos": 149, "end_pos": 175, "type": "TASK", "confidence": 0.691120425860087}, {"text": "semantic class learning", "start_pos": 177, "end_pos": 200, "type": "TASK", "confidence": 0.7206500768661499}, {"text": "web-based information extraction", "start_pos": 206, "end_pos": 238, "type": "TASK", "confidence": 0.6171618004639944}]}, {"text": "Many of these approaches employ surface-level patterns to identify words and their associated semantic classes.", "labels": [], "entities": []}, {"text": "However, such patterns tend to overgenerate (i.e., deliver incorrect results) and hence require additional filtering mechanisms.", "labels": [], "entities": []}, {"text": "To overcome this problem, we employed one single powerful doubly-anchored hyponym pattern to query the web and extract semantic class instances: CLASS NAME such as CLASS MEMBER and *.", "labels": [], "entities": []}, {"text": "We hypothesized that a doubly-anchored pattern, which includes both the class name and a class member, would achieve high accuracy because of its specificity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9983574748039246}]}, {"text": "To address concerns about coverage, we embedded the search in a bootstrapping process.", "labels": [], "entities": []}, {"text": "This method produced many correct instances, but despite the highly restrictive nature of the pattern, still produced many incorrect instances.", "labels": [], "entities": []}, {"text": "This result led us to explore new ways to improve the accuracy of hyponym patterns without requiring additional training resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9972599744796753}]}, {"text": "The main contribution of this work is a novel method for combining hyponym patterns with graph structures that capture two properties associated with pattern extraction: popularity and productivity.", "labels": [], "entities": [{"text": "pattern extraction", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7582979202270508}]}, {"text": "Intuitively, a candidate word (or phrase) is popular if it was discovered many times by other words (or phrases) in a hyponym pattern.", "labels": [], "entities": []}, {"text": "A candidate word is productive if it frequently leads to the discovery of other words.", "labels": [], "entities": []}, {"text": "Together, these two measures capture not only frequency of occurrence, but also crosschecking that the word occurs both near the class name and near other class members.", "labels": [], "entities": [{"text": "crosschecking", "start_pos": 80, "end_pos": 93, "type": "METRIC", "confidence": 0.950873851776123}]}, {"text": "We present two algorithms that use hyponym pattern linkage graphs (HPLGs) to represent popularity and productivity information.", "labels": [], "entities": []}, {"text": "The first method uses a dynamically constructed HPLG to assess the popularity of each candidate and steer the bootstrapping process.", "labels": [], "entities": []}, {"text": "This approach produces an efficient bootstrapping process that performs reasonably well, but it cannot take advantage of productivity information because of the dynamic nature of the process.", "labels": [], "entities": []}, {"text": "The second method is a two-step procedure that begins with an exhaustive pattern search that acquires popularity and productivity information about candidate instances.", "labels": [], "entities": []}, {"text": "The candidates are then ranked based on properties of the HPLG.", "labels": [], "entities": [{"text": "HPLG", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9662788510322571}]}, {"text": "We conducted experiments with four semantic classes, achieving high accuracies and outperforming the results reported by others who have worked on the same classes.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9647372961044312}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 shows the results for 4 iterations of reck- less bootstrapping for four semantic categories: U.S.  states, countries, singers, and fish. The first two  categories are relatively small, closed sets (our gold  standard contains 50 U.S. states and 194 countries).  The singers and fish categories are much larger, open  sets (see Section 4 for details).  Table 1 reveals that the doubly-anchored pattern  achieves high accuracy during the first iteration, but", "labels": [], "entities": [{"text": "reck- less bootstrapping", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8513955920934677}, {"text": "accuracy", "start_pos": 425, "end_pos": 433, "type": "METRIC", "confidence": 0.9967755675315857}]}, {"text": " Table 2: Accuracies for each semantic class", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9946104288101196}]}]}