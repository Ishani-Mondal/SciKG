{"title": [{"text": "Adapting a WSJ-Trained Parser to Grammatically Noisy Text", "labels": [], "entities": [{"text": "Adapting", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9702407121658325}]}], "abstractContent": [{"text": "We present a robust parser which is trained on a treebank of ungrammatical sentences.", "labels": [], "entities": []}, {"text": "The treebank is created automatically by modifying Penn treebank sentences so that they contain one or more syntactic errors.", "labels": [], "entities": []}, {"text": "We evaluate an existing Penn-treebank-trained parser on the ungrammatical treebank to see how it reacts to noise in the form of grammatical errors.", "labels": [], "entities": []}, {"text": "We retrain this parser on the training section of the ungrammatical treebank, leading to an significantly improved performance on the ungrammatical test sets.", "labels": [], "entities": []}, {"text": "We show how a classifier can be used to prevent performance degradation on the original grammatical data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The focus in English parsing research in recent years has moved from Wall Street Journal parsing to improving performance on other domains.", "labels": [], "entities": [{"text": "English parsing", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7277887165546417}, {"text": "Wall Street Journal parsing", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.8523309677839279}]}, {"text": "Our research aim is to improve parsing performance on text which is mildly ungrammatical, i.e. text which is well-formed enough to be understood by people yet which contains the kind of grammatical errors that are routinely produced by both native and nonnative speakers of a language.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9625574350357056}]}, {"text": "The intention is not to detect and correct the error, but rather to ignore it.", "labels": [], "entities": []}, {"text": "Our approach is to introduce grammatical noise into WSJ sentences while retaining as much of the structure of the original trees as possible.", "labels": [], "entities": []}, {"text": "These sentences and their associated trees are then used as training material fora statistical parser.", "labels": [], "entities": []}, {"text": "It is important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed.", "labels": [], "entities": [{"text": "parsing on grammatical sentences", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.7916368842124939}]}], "datasetContent": [{"text": "In order to obtain training data for our parsing experiments, we introduce syntactic noise into the usual WSJ training material, Sections 2-21, using the procedures outlined in Section 3, i.e. for every sentence-tree pair in WSJ2-21, we introduce an error into the sentence and then transform the tree so that it covers the newly created ungrammatical sentence.", "labels": [], "entities": [{"text": "WSJ training material", "start_pos": 106, "end_pos": 127, "type": "DATASET", "confidence": 0.8525211612383524}, {"text": "WSJ2-21", "start_pos": 225, "end_pos": 232, "type": "DATASET", "confidence": 0.965009331703186}]}, {"text": "For 4 of the 20 sections in WSJ2-21, we apply the noise introduction procedure to its own output to: Automatically Generated Ungrammatical Sentences create even noisier data.", "labels": [], "entities": [{"text": "WSJ2-21", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9711799025535583}]}, {"text": "Our first development set is a noisy version of WSJ00, Noisy00, produced by applying the noise introduction procedure to the 1,921 sentences in WSJ00.", "labels": [], "entities": [{"text": "WSJ00", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.9253929257392883}, {"text": "WSJ00", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9604840278625488}]}, {"text": "Our second development set is an even noisier version of WSJ00, Noisiest00, which is created by applying our noise introduction procedure to the output of Noisy00.", "labels": [], "entities": [{"text": "WSJ00", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.8561527729034424}]}, {"text": "We apply the same process to WSJ23 to obtain our two test sets.", "labels": [], "entities": [{"text": "WSJ23", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9456712603569031}]}, {"text": "For all our parsing experiments, we use the June 2006 version of the two-stage parser reported in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.97735595703125}]}, {"text": "Evaluation is carried out using Parseval labelled precision/recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.7525742053985596}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9130639433860779}]}, {"text": "For extra word errors, there maybe more than one gold standard tree (see (6) and).", "labels": [], "entities": []}, {"text": "When this happens the parser output tree is evaluated against all gold standard trees and the maximum f-score is chosen.", "labels": [], "entities": []}, {"text": "In the first experiment, E0, we apply the parser, trained on wellformed data, to noisy input.", "labels": [], "entities": []}, {"text": "The purpose of E0 is to ascertain how well a parser trained on grammatical sentences, can ignore grammatical noise.", "labels": [], "entities": []}, {"text": "E0 provides a baseline against which the subsequent experimental results can be judged.", "labels": [], "entities": [{"text": "E0", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8273115158081055}]}, {"text": "In the E1 experiments, the parser is retrained using the ungrammatical version of WSJ2-21.", "labels": [], "entities": [{"text": "WSJ2-21", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9755182266235352}]}, {"text": "In experiment E1error, the parser is trained on ungrammatical material only, i.e. the noisy version of WSJ2-21.", "labels": [], "entities": [{"text": "WSJ2-21", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9710793495178223}]}, {"text": "In experiment E1mixed, the parser is trained on grammatical and ungrammatical material, i.e. the original WSJ2-21 is merged with the noisy WSJ2-21.", "labels": [], "entities": [{"text": "WSJ2-21", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9407123923301697}, {"text": "WSJ2-21", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.9394205212593079}]}, {"text": "In the E2 experiments, a classifier is applied to the input sentence.", "labels": [], "entities": []}, {"text": "If the sentence is classified as ungrammatical, aversion of the parser that has been trained on ungrammatical data is employed.", "labels": [], "entities": []}, {"text": "In the E2ngram experiment, we train a J48 decision tree classifier.", "labels": [], "entities": [{"text": "J48 decision tree classifier", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6875217780470848}]}, {"text": "Following, the decision tree features are part-of-speech n-gram frequency counts, with n ranging from 2 to 7 and with a subset of the BNC as the frequency reference corpus.", "labels": [], "entities": [{"text": "BNC", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.9211573600769043}]}, {"text": "The decision tree is trained on the original WSJ2-21 and the ungrammatical WSJ2-21.", "labels": [], "entities": [{"text": "WSJ2-21", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.985486626625061}, {"text": "WSJ2-21", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9767634868621826}]}, {"text": "In the E2prob experiment, the input sentence is parsed with two parsers, the original parser (the E0 parser) and the parser trained on ungrammatical material (either the E1error or the E1mixed parser).", "labels": [], "entities": []}, {"text": "Avery simple classifier is used to decide which parser output to choose: if the E1 parser returns a higher parse probability for the most likely tree than the E0 parser, the E1 parser output is returned.", "labels": [], "entities": []}, {"text": "Otherwise the E0 parser output is returned.", "labels": [], "entities": []}, {"text": "The baseline E0 results are in the first column of.", "labels": [], "entities": [{"text": "E0", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.48342326283454895}]}, {"text": "As expected, the performance of a parser trained on well-formed input degrades when faced with ungrammatical input.", "labels": [], "entities": []}, {"text": "It is also not surprising that its performance is worse on Noisiest00 (-8.8% f-score) than it is on Noisy00 (-4.3%) since the Noisiest00 sentences contain two errors rather than one.", "labels": [], "entities": [{"text": "Noisiest00", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.8880904316902161}, {"text": "f-score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9551091194152832}]}, {"text": "The E1 results occupy the second and third columns of.", "labels": [], "entities": []}, {"text": "An up arrow indicates a statistically significant improvement over the baseline results, a down arrow a statistically significant decline and a dash a change which is not statistically significant (p < 0.01).", "labels": [], "entities": []}, {"text": "Training the parser on ungrammatical data has a positive effect on its performance on Noisy00 and Noisiest00 but has a negative effect on its performance on WSJ00.", "labels": [], "entities": [{"text": "WSJ00", "start_pos": 157, "end_pos": 162, "type": "DATASET", "confidence": 0.9652655124664307}]}, {"text": "Training on a combination of grammatical and ungrammatical material gives the best results for all three development sets.", "labels": [], "entities": []}, {"text": "Therefore, for the E2 experiments we use the E1mixed parser rather than the E1error parser.", "labels": [], "entities": []}, {"text": "The E2 results are shown in the last two columns of and the accuracy of the two classifiers in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996287822723389}]}, {"text": "Over the three test sets, the E2prob classifier outperforms the E2ngram classifier.", "labels": [], "entities": []}, {"text": "Both classifiers misclassify approximately 45% of the Noisy00 sentences.", "labels": [], "entities": [{"text": "Noisy00 sentences", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.8723963499069214}]}, {"text": "However, the sentences misclassified by the E2prob classifier are those that are handled well by the E0 parser, and this is reflected in the parsing results for Noisy00.", "labels": [], "entities": [{"text": "Noisy00", "start_pos": 161, "end_pos": 168, "type": "DATASET", "confidence": 0.9355951547622681}]}, {"text": "An important feature of the    E2prob classifier is that its use results in a constant performance on the grammatical data -with no significant degradation from the baseline.", "labels": [], "entities": []}, {"text": "Taking the E2prob results as our optimum, we carryout the same experiment again on our WSJ23 test sets.", "labels": [], "entities": [{"text": "WSJ23 test sets", "start_pos": 87, "end_pos": 102, "type": "DATASET", "confidence": 0.9772733847300211}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The same effect can be seen for the test sets as for the development sets -a significantly improved performance on the ungrammatical data without an accompanying performance decrease for the grammatical data.", "labels": [], "entities": []}, {"text": "The Noisy23 breakdown by error type is shown in.", "labels": [], "entities": []}, {"text": "The error type which the original parser is most able to ignore is an agreement error.", "labels": [], "entities": []}, {"text": "For this error type alone, the ungrammatical training material seems to hinder the parser.", "labels": [], "entities": []}, {"text": "The biggest improvement occurs for real word spelling errors.", "labels": [], "entities": [{"text": "real word spelling errors", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7144689783453941}]}], "tableCaptions": [{"text": " Table 2: Results of Parsing Experiments", "labels": [], "entities": []}, {"text": " Table 3: E2 Classifier Accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9356539249420166}]}, {"text": " Table 4: Final Results for Section 23 Test Sets", "labels": [], "entities": [{"text": "Section 23 Test Sets", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8784846216440201}]}, {"text": " Table 5: Noisy23: Breakdown by Error Type", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.9667060375213623}]}]}