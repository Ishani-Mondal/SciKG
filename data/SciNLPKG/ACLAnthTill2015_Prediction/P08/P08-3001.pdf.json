{"title": [{"text": "A Supervised Learning Approach to Automatic Synonym Identification based on Distributional Features", "labels": [], "entities": [{"text": "Automatic Synonym Identification", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.6816984812418619}]}], "abstractContent": [{"text": "Distributional similarity has been widely used to capture the semantic relatedness of words in many NLP tasks.", "labels": [], "entities": []}, {"text": "However, various parameters such as similarity measures must be hand-tuned to make it work effectively.", "labels": [], "entities": []}, {"text": "Instead, we propose a novel approach to synonym identification based on supervised learning and distributional features, which correspond to the commonality of individual context types shared byword pairs.", "labels": [], "entities": [{"text": "synonym identification", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.9791289865970612}]}, {"text": "Considering the integration with pattern-based features, we have built and compared five synonym classifiers.", "labels": [], "entities": []}, {"text": "The evaluation experiment has shown a dramatic performance increase of over 120% on the F-1 measure basis, compared to the conventional similarity-based classification.", "labels": [], "entities": [{"text": "F-1 measure", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.9803864061832428}]}, {"text": "On the other hand, the pattern-based features have appeared almost redundant.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic similarity of words is one of the most important lexical knowledge for NLP tasks including word sense disambiguation and automatic thesaurus construction.", "labels": [], "entities": [{"text": "Semantic similarity of words", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.832222655415535}, {"text": "word sense disambiguation", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.6882425546646118}, {"text": "automatic thesaurus construction", "start_pos": 130, "end_pos": 162, "type": "TASK", "confidence": 0.646760086218516}]}, {"text": "To measure the semantic relatedness of words, a concept called distributional similarity has been widely used.", "labels": [], "entities": []}, {"text": "Distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis, which states that semantically similar words share similar contexts.", "labels": [], "entities": []}, {"text": "A number of researches which utilized distributional similarity have been conducted, including and many others.", "labels": [], "entities": []}, {"text": "Although they have been successful in acquiring related words, various parameters such as similarity measures and weighting are involved.", "labels": [], "entities": []}, {"text": "As pointed out, \"it is not at all obvious that one universally best measure exists for all application,\" thus they must be tuned by hand in an ad-hoc manner.", "labels": [], "entities": []}, {"text": "The fact that no theoretic basis is given is making the matter more difficult.", "labels": [], "entities": []}, {"text": "On the other hand, if we pay attention to lexical knowledge acquisition in general, a variety of systems which utilized syntactic patterns are found in the literature.", "labels": [], "entities": [{"text": "lexical knowledge acquisition", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6930292646090189}]}, {"text": "In her landmark paper in the field, Hearst (1992) utilized syntactic patterns such as \"such X as Y\" and \"Y and other X,\" and extracted hypernym/hyponym relation of X and Y. applied this idea to extraction of words which belong to the same categories, utilizing syntactic relations such as conjunctions and appositives.", "labels": [], "entities": []}, {"text": "What is worth attention here is that supervised machine learning is easily incorporated with syntactic patterns.", "labels": [], "entities": []}, {"text": "For example, further extended Hearst's idea and built hypernym classifiers based on machine learning and syntactic pattern-based features, with a considerable success.", "labels": [], "entities": []}, {"text": "These two independent approaches, distributional similarity and syntactic patterns, were finally integrated by.", "labels": [], "entities": []}, {"text": "Although they reported that their system successfully improved the performance, it did not achieve a complete integration and was still relying on an independent module to compute the similarity.", "labels": [], "entities": []}, {"text": "This configuration inherits a large portion of drawbacks of the similaritybased approach mentioned above.", "labels": [], "entities": []}, {"text": "To achieve a full integration of both approaches, we suppose that re-formalization of similarity-based approach would be essential, as pattern-based approach is enhanced with the supervised machine learning.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to automatic synonym identification based on supervised learning technique.", "labels": [], "entities": [{"text": "synonym identification", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.8394632041454315}]}, {"text": "Firstly, we re-formalize synonym acquisition as a classification problem: one which classifies word pairs into synonym/nonsynonym classes, without depending on a single value of distributional similarity.", "labels": [], "entities": [{"text": "synonym acquisition", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.9216850101947784}]}, {"text": "Instead, classification is done using a set of distributional features, which correspond to the degree of commonality of individual context types shared byword pairs.", "labels": [], "entities": []}, {"text": "This formalization also enables to incorporate pattern-based features, and we finally build five classifiers based on distributional and/or pattern-based features.", "labels": [], "entities": []}, {"text": "In the experiment, their performances are compared in terms of synonym acquisition precision and recall, and the differences of actually acquired synonyms are to be clarified.", "labels": [], "entities": [{"text": "synonym acquisition", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7328468859195709}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.7718295454978943}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9989389777183533}]}, {"text": "The rest of this paper is organized as follows: in Sections 2 and 3, distributional and pattern-based features are defined, along with the extraction methods.", "labels": [], "entities": []}, {"text": "Using the features, in Section 4 we build five types of synonym classifiers, and compare their performances in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes this paper, mentioning the future direction of this study.", "labels": [], "entities": []}], "datasetContent": [{"text": "Finally, this section describes the experimental setting and the comparison of synonym classifiers.", "labels": [], "entities": []}, {"text": "Corpus and Preprocessing As for the corpus, New York Times section of English Gigaword 1 , consisting of approx. 46,000 documents, 922,000 sentences, and 30 million words, was analyzed to obtain word-context co-occurrences.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9885643124580383}, {"text": "New York Times section of English Gigaword 1", "start_pos": 44, "end_pos": 88, "type": "DATASET", "confidence": 0.8526845499873161}]}, {"text": "This can yield 10,000 or more context types, thus we applied feature selection and reduced the dimensionality.", "labels": [], "entities": []}, {"text": "Firstly, we simply applied frequency cutoff to filter out any words and contexts with low frequency.", "labels": [], "entities": []}, {"text": "More specifically, any words w such that \u2211 c N (w, c) < \u03b8 f and any contexts c such that \u2211 w N (w, c) < \u03b8 f , with \u03b8 f = 5, were removed.", "labels": [], "entities": []}, {"text": "DF (document frequency) thresholding is then applied, and context types with the lowest values of DF were removed until 10% of the original contexts were left.", "labels": [], "entities": [{"text": "DF", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5161630511283875}]}, {"text": "We verified through a preliminary experiment that this feature selection keeps the performance loss at minimum.", "labels": [], "entities": []}, {"text": "As a result, this process left a total of 8,558 context types, or feature dimensionality.", "labels": [], "entities": []}, {"text": "The feature selection was also applied to patternbased features to avoid high sparseness -only syntactic patterns which occurred more than or equal to 7 times were used.", "labels": [], "entities": []}, {"text": "The number of syntactic pattern types left after this process is 17,964.", "labels": [], "entities": []}, {"text": "Supervised Learning Training and test sets were created as follows: firstly, the nouns listed in the Longman Defining Vocabulary (LDV) 2 were chosen as the target words of classification.", "labels": [], "entities": []}, {"text": "Then, all the LDV pairs which co-occur more than or equal to 3 times with any of the syntactic patterns, i.e., {(w 1 , w 2 )|w 1 , w 2 \u2208 LDV, were classified into synonym/non-synonym classes as mentioned in Section 5.2.", "labels": [], "entities": []}, {"text": "All the positive-marked pair, as well as randomly chosen 1 out of 5 negativemarked pairs, were collected as the example set E.", "labels": [], "entities": []}, {"text": "This random selection is to avoid extreme bias toward the negative examples.", "labels": [], "entities": []}, {"text": "The example set E ended up with 2,148 positive and 13,855 negative examples, with their ratio being approx. 6.45.", "labels": [], "entities": []}, {"text": "The example set E was then divided into five partitions to conduct five-fold cross validation, of which four partitions were used for learning and the one for testing.", "labels": [], "entities": []}, {"text": "SVM light was adopted for machine learning, and RBF as the kernel.", "labels": [], "entities": [{"text": "SVM light", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8446745276451111}, {"text": "machine learning", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7714510858058929}, {"text": "RBF", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7151825428009033}]}, {"text": "The parameters, i.e., the similarity threshold of DSIM classifier, gamma parameter of RBF kernel, and the cost-factor j of SVM, i.e., the ratio by which training errors on positive examples outweight errors on negative ones, were optimized using one of the 5-fold cross validation train-test pair on the basis of F-1 measure.", "labels": [], "entities": [{"text": "F-1", "start_pos": 313, "end_pos": 316, "type": "METRIC", "confidence": 0.9487003684043884}]}, {"text": "The performance was evaluated for the other four traintest pairs and the average values were recorded.", "labels": [], "entities": []}, {"text": "To test whether or not a given word pair (w 1 , w 2 ) is a synonym pair, three existing thesauri were consulted: Roget's Thesaurus, Collins COBUILD Thesaurus, and WordNet.", "labels": [], "entities": [{"text": "Collins COBUILD Thesaurus", "start_pos": 132, "end_pos": 157, "type": "DATASET", "confidence": 0.8794208367665609}, {"text": "WordNet", "start_pos": 163, "end_pos": 170, "type": "DATASET", "confidence": 0.9750375747680664}]}, {"text": "The union of synonyms obtained when the headword is looked up as a noun is used as the answer set, except for words marked as \"idiom,\" \"informal,\" \"slang\" and phrases comprised of two or more words.", "labels": [], "entities": []}, {"text": "The pair (w 1 , w 2 ) is marked as synonyms if and only if w 2 is contained in the answer set of w 1 , or w 1 is contained in that of w 2 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparison of synonym classifiers", "labels": [], "entities": []}]}