{"title": [{"text": "Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure", "labels": [], "entities": []}], "abstractContent": [{"text": "Adaptor grammars (Johnson et al., 2007b) area non-parametric Bayesian extension of Prob-abilistic Context-Free Grammars (PCFGs) which in effect learn the probabilities of entire subtrees.", "labels": [], "entities": []}, {"text": "In practice, this means that an adaptor grammar learns the structures useful for generating the training data as well as their probabilities.", "labels": [], "entities": []}, {"text": "We present several different adaptor grammars that learn to segment phonemic input into words by modeling different linguistic properties of the input.", "labels": [], "entities": []}, {"text": "One of the advantages of a grammar-based framework is that it is easy to combine grammars, and we use this ability to compare models that capture different kinds of linguistic structure.", "labels": [], "entities": []}, {"text": "We show that incorporating both unsupervised syllabification and collocation-finding into the adaptor grammar significantly improves un-supervised word-segmentation accuracy over that achieved by adaptor grammars that model only one of these linguistic phenomena.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9615161418914795}]}], "introductionContent": [{"text": "How humans acquire language is arguably the central issue in the scientific study of language.", "labels": [], "entities": []}, {"text": "Human language is richly structured, but it is still hotly debated as to whether this structure can be learnt, or whether it must be innately specified.", "labels": [], "entities": []}, {"text": "Computational linguistics can contribute to this debate by identifying which aspects of language can potentially be learnt from the input available to a child.", "labels": [], "entities": []}, {"text": "Here we try to identify linguistic properties that convey information useful for learning to segment streams of phonemes into words.", "labels": [], "entities": []}, {"text": "We show that simultaneously learning syllable structure and collocations improves word segmentation accuracy compared to models that learn these independently.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7249284535646439}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8784354329109192}]}, {"text": "This suggests that there might be a synergistic interaction in learning several aspects of linguistic structure simultaneously, as compared to learning each kind of linguistic structure independently.", "labels": [], "entities": []}, {"text": "Because learning collocations and word-initial syllable onset clusters requires the learner to be able to identify word boundaries, it might seem that we face a chicken-and-egg problem here.", "labels": [], "entities": []}, {"text": "One of the important properties of the adaptor grammar inference procedure is that it gives us away of learning these interacting linguistic structures simultaneously.", "labels": [], "entities": []}, {"text": "Adaptor grammars are also interesting because they can be viewed as directly inferring linguistic structure.", "labels": [], "entities": [{"text": "Adaptor grammars", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8557496070861816}]}, {"text": "Most well-known machine-learning and statistical inference procedures are parameter estimation procedures, i.e., the procedure is designed to find the values of a finite vector of parameters.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.7207472771406174}]}, {"text": "Standard methods for learning linguistic structure typically try to reduce structure learning to parameter estimation, say, by using an iterative generate-andprune procedure in which each iteration consists of a rule generation step that proposes new rules according to some scheme, a parameter estimation step that estimates the utility of these rules, and pruning step that removes low utility rules.", "labels": [], "entities": []}, {"text": "For example, the Bayesian unsupervised PCFG estimation procedure devised by uses a model-merging procedure to propose new sets of PCFG rules and a Bayesian version of the EM procedure to estimate their weights.", "labels": [], "entities": [{"text": "Bayesian unsupervised PCFG estimation", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.5207710191607475}]}, {"text": "Recently, methods have been developed in the statistical community for Bayesian inference of increasingly sophisticated non-parametric models.", "labels": [], "entities": [{"text": "Bayesian inference", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7320840060710907}]}, {"text": "(\"Non-parametric\" here means that the models are not characterized by a finite vector of parameters, so the complexity of the model can vary depending on the data it describes).", "labels": [], "entities": []}, {"text": "Adaptor grammars area framework for specifying a wide range of such models for grammatical inference.", "labels": [], "entities": []}, {"text": "They can be viewed as a nonparametric extension of PCFGs.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9095858335494995}]}, {"text": "Informally, there seem to beat least two natural ways to construct non-parametric extensions of a PCFG.", "labels": [], "entities": []}, {"text": "First, we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFG's nonterminals into increasingly finer states; this leads to the iPCFG or \"infinite PCFG\" (.", "labels": [], "entities": []}, {"text": "Second, we can generalize over arbitrary subtrees rather than local trees in much the way done in DOP or tree substitution grammar, which leads to adaptor grammars.", "labels": [], "entities": []}, {"text": "Informally, the units of generalization of adaptor grammars are entire subtrees, rather than just local trees, as in PCFGs.", "labels": [], "entities": []}, {"text": "Just as in tree substitution grammars, each of these subtrees behaves as anew context-free rule that expands the subtree's root node to its leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as their probabilities, are learnt from the training data.", "labels": [], "entities": []}, {"text": "In order to make parsing and inference tractable we require the leaves of these subtrees to be terminals, as explained in section 2.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9761626720428467}]}, {"text": "Thus adaptor grammars are simple models of structure learning, where the subtrees that constitute the units of generalization are in effect new context-free rules learnt during the inference process.", "labels": [], "entities": []}, {"text": "(In fact, the inference procedure for adaptor grammars described in relies on a PCFG approximation that contains a rule for each subtree generalization in the adaptor grammar).", "labels": [], "entities": []}, {"text": "This paper applies adaptor grammars to word segmentation and morphological acquisition.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7603537142276764}, {"text": "morphological acquisition", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7748720049858093}]}, {"text": "Linguistically, these exhibit considerable cross-linguistic variation, and so are likely to be learned by human learners.", "labels": [], "entities": []}, {"text": "It's also plausible that semantics and contextual information is less important for their acquisition than, say, syntax.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word segmentation f-score results for all mod- els, as a function of DP concentration parameter \u03b1. \"U\"  indicates unigram-based grammars, while \"C\" indicates  collocation-based grammars.", "labels": [], "entities": [{"text": "Word segmentation f-score", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6445815165837606}]}]}