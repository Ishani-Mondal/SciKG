{"title": [{"text": "Unsupervised Lexicon-Based Resolution of Unknown Words for Full Morphological Analysis", "labels": [], "entities": [{"text": "Unsupervised Lexicon-Based Resolution of Unknown Words", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7375108699003855}, {"text": "Full Morphological Analysis", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.615033229192098}]}], "abstractContent": [{"text": "Morphological disambiguation proceeds in 2 stages: (1) an analyzer provides all possible analyses fora given token and (2) a stochastic disambiguation module picks the most likely analysis in context.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9035651981830597}]}, {"text": "When the analyzer does not recognize a given token, we hit the problem of unknowns.", "labels": [], "entities": []}, {"text": "In large scale corpora, un-knowns appear at a rate of 5 to 10% (depend-ing on the genre and the maturity of the lexicon).", "labels": [], "entities": []}, {"text": "We address the task of computing the distribution p(t|w) for unknown words for full morphological disambiguation in Hebrew.", "labels": [], "entities": []}, {"text": "We introduce a novel algorithm that is language independent: it exploits a maximum entropy letters model trained over the known words observed in the corpus and the distribution of the unknown words in known tag contexts, through iterative approximation.", "labels": [], "entities": []}, {"text": "The algorithm achieves 30% error reduction on dis-ambiguation of unknown words over a competitive baseline (to a level of 70% accurate full disambiguation of unknown words).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.9624956846237183}]}, {"text": "We have also verified that taking advantage of a strong language-specific model of morphological patterns provides the same level of disam-biguation.", "labels": [], "entities": []}, {"text": "The algorithm we have developed exploits distributional information latent in a wide-coverage lexicon and large quantities of unlabeled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The term unknowns denotes tokens in a text that cannot be resolved in a given lexicon.", "labels": [], "entities": []}, {"text": "For the task of full morphological analysis, the lexicon must provide all possible morphological analyses for any given token.", "labels": [], "entities": [{"text": "full morphological analysis", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6153741578261057}]}, {"text": "In this case, unknown tokens can be categorized into two classes of missing information: unknown tokens are not recognized at all by the lexicon, and unknown analyses, where the set of analyses fora lexeme does not contain the correct analysis fora given token.", "labels": [], "entities": []}, {"text": "Despite efforts on improving the underlying lexicon, unknowns typically represent 5% to 10% of the number of tokens in large-scale corpora.", "labels": [], "entities": []}, {"text": "The alternative to continuously investing manual effort in improving the lexicon is to design methods to learn possible analyses for unknowns from observable features: their letter structure and their context.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the characteristics of Hebrew unknowns for full morphological analysis, and propose anew method for handling such unavoidable lack of information.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7027094662189484}]}, {"text": "Our method generates a distribution of possible analyses for unknowns.", "labels": [], "entities": []}, {"text": "In our evaluation, these learned distributions include the correct analysis for unknown words in 85% of the cases, contributing an error reduction of over 30% over a competitive baseline for the overall task of full morphological analysis in Hebrew.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 131, "end_pos": 146, "type": "METRIC", "confidence": 0.9661335349082947}]}, {"text": "The task of a morphological analyzer is to produce all possible analyses fora given token.", "labels": [], "entities": []}, {"text": "In Hebrew, the analysis for each token is of the form lexeme-and-features 1 : lemma, affixes, lexical cate-gory (POS), and a set of inflection properties (according to the POS) -gender, number, person, status and tense.", "labels": [], "entities": []}, {"text": "In this work, we refer to the morphological analyzer of MILA -the Knowledge Center for Processing Hebrew.", "labels": [], "entities": []}, {"text": "It is a synthetic analyzer, composed of two data resources -a lexicon of about 2,400 lexemes, and a set of generation rules (see).", "labels": [], "entities": []}, {"text": "In addition, we use an unlabeled text corpus, composed of stories taken from three Hebrew daily news papers (Aruts 7, Haaretz, The Marker), of 42M tokens.", "labels": [], "entities": [{"text": "The Marker)", "start_pos": 127, "end_pos": 138, "type": "DATASET", "confidence": 0.9122180541356405}]}, {"text": "We observed 3,561 different composite tags (e.g., noun-sing-fem-prepPrefix:be) over this corpus.", "labels": [], "entities": []}, {"text": "These 3,561 tags form the large tagset over which we train our learner.", "labels": [], "entities": []}, {"text": "On the one hand, this tagset is much larger than the largest tagset used in English (from 17 tags inmost unsupervised POS tagging experiments, to the 46 tags of the WSJ corpus and the about 150 tags of the LOB corpus).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.6300838440656662}, {"text": "WSJ corpus", "start_pos": 165, "end_pos": 175, "type": "DATASET", "confidence": 0.9768664240837097}, {"text": "LOB corpus", "start_pos": 206, "end_pos": 216, "type": "DATASET", "confidence": 0.9580568075180054}]}, {"text": "On the other hand, our tagset is intrinsically factored as a set of dependent sub-features, which we explicitly represent.", "labels": [], "entities": []}, {"text": "The task we address in this paper is morphological disambiguation: given a sentence, obtain the list of all possible analyses for each word from the analyzer, and disambiguate each word in context.", "labels": [], "entities": [{"text": "morphological disambiguation", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.7336734235286713}]}, {"text": "On average, each token in the 42M corpus is given 2.7 possible analyses by the analyzer (much higher than the average 1.41 POS tag ambiguity reported in English).", "labels": [], "entities": [{"text": "42M corpus", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.8923506438732147}]}, {"text": "In previous work, we report disambiguation rates of 89% for full morphological disambiguation (using an unsupervised EM-HMM model) and 92.5% for part of speech and segmentation (without assigning all the inflectional features of the words).", "labels": [], "entities": []}, {"text": "In order to estimate the importance of unknowns in Hebrew, we analyze tokens in several aspects: (1) the number of unknown tokens, as observed on the corpus of 42M tokens; (2) a manual classification of a sample of 10K unknown token types out of the 200K unknown types identified in the corpus;  training corpus were unknown tokens (45% of the 450K token types).", "labels": [], "entities": []}, {"text": "For less edited text, such as random text sampled from the Web, the percentage is much higher -about 7.5%.", "labels": [], "entities": []}, {"text": "In order to classify these unknown tokens, we sampled 10K unknown token types and examined them manually.", "labels": [], "entities": []}, {"text": "The classification of these tokens with their distribution is shown in 3 . As can be seen, there are two main classes of unknown token types: Neologisms (32%) and Proper nouns (48%), which cover about 80% of the unknown token instances.", "labels": [], "entities": []}, {"text": "The POS distribution of the unknown tokens of our annotated corpus is shown in.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7857075333595276}]}, {"text": "As expected, most unknowns are open class words: proper names, nouns or adjectives.", "labels": [], "entities": []}, {"text": "Regarding unknown analyses, in our annotated corpus, we found 3% of the 100K token instances were missing the correct analysis in the lexicon (3.65% of the token types).", "labels": [], "entities": []}, {"text": "The POS distribution of the unknown analyses is listed in.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9330320954322815}]}, {"text": "The high rate of unknown analyses for prepositions at about 3% is a specific phenomenon in Hebrew, where prepositions are often prefixes agglutinated to the first word of the noun phrase they head.", "labels": [], "entities": []}, {"text": "We observe the very low rate of unknown verbs (2%) -which are well marked morphologically in Hebrew, and where the rate of neologism introduction seems quite low.", "labels": [], "entities": [{"text": "neologism introduction", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.695249393582344}]}, {"text": "This evidence illustrates the need for resolution of unknowns: The naive policy of selecting 'proper name' for all unknowns will cover only half of the errors caused by unknown tokens, i.e., 30% of the whole unknown tokens and analyses.", "labels": [], "entities": [{"text": "resolution of unknowns", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.8641570806503296}]}, {"text": "The other 70% of the unknowns ( 5.3% of the words in the text in our experiments) will be assigned a wrong tag.", "labels": [], "entities": []}, {"text": "As a result of this observation, our strategy is to focus on full morphological analysis for unknown tokens and apply a proper name classifier for unknown analyses and unknown tokens.", "labels": [], "entities": []}, {"text": "In this paper, we investigate various methods for achieving full morphological analysis distribution for unknown tokens.", "labels": [], "entities": []}, {"text": "The methods are not based on an annotated corpus, nor on hand-crafted rules, but instead exploit the distribution of words in an available lexicon and the letter similarity of the unknown words with known words.", "labels": [], "entities": []}], "datasetContent": [{"text": "For testing, we manually tagged the text which is used in the Hebrew Treebank (consisting of about 90K tokens), according to our tagging guideline (?).", "labels": [], "entities": [{"text": "Hebrew Treebank", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9536182284355164}]}, {"text": "We measured the effectiveness of the three models with respect to the tags that were assigned to the unknown tokens in our test corpus (the 'correct tag'), according to three parameters: The coverage of the model, i.e., we count cases where p(t|w) contains the correct tag with a probability larger than 0.01; (2) the ambiguity level of the model, i.e., the average number of analyses suggested for each token; (3) the average probability of the 'correct tag', according to the predicted p(t|w).", "labels": [], "entities": []}, {"text": "In addition, for each experiment, we run the full morphology disambiguation system where unknowns are analyzed according by the model.", "labels": [], "entities": [{"text": "morphology disambiguation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6961714327335358}]}, {"text": "Our baseline proposes the most frequent tag (proper name) for all possible segmentations of the token, in a uniform distribution.", "labels": [], "entities": []}, {"text": "We compare the following models: the 3 context free models (patterns, letters and the combined patterns and letters) and the same models combined with the word and tag context models.", "labels": [], "entities": []}, {"text": "Note that the context models have low coverage (about 40% for the word context and 80% for the tag context models), and therefore, the context models cannot be used on their own.", "labels": [], "entities": []}, {"text": "The highest coverage is obtained for the combined model (tag context, pattern, letter) at 86.1%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9952913522720337}]}, {"text": "We first show the results for full morphological disambiguation, over 3,561 distinct tags in.", "labels": [], "entities": [{"text": "morphological disambiguation", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6565441191196442}]}, {"text": "The highest coverage is obtained for the model combining the tag context, patterns and letters models.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9867085218429565}]}, {"text": "The tag context model is more effective because it covers 80% of the unknown words, whereas the word context model only covers 40%.", "labels": [], "entities": []}, {"text": "As expected, our simple baseline has the highest precision, since the most frequent proper name tag covers over 50% of the unknown words.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9994162321090698}]}, {"text": "the method is measured by its impact on the eventual disambiguation of the unknown words.", "labels": [], "entities": []}, {"text": "For full morphological disambiguation, our method achieves an error reduction of 30% (57% to 70%).", "labels": [], "entities": [{"text": "morphological disambiguation", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.8379851281642914}, {"text": "error reduction", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.9836651682853699}]}, {"text": "Overall, with the level of 4.5% of unknown words observed in our corpus, the algorithm we have developed contributes to an error reduction of 5.5% for full morphological disambiguation.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 123, "end_pos": 138, "type": "METRIC", "confidence": 0.9680755436420441}]}, {"text": "The best result is obtained for the model combining pattern and letter features.", "labels": [], "entities": []}, {"text": "However, the model combining the word context and letter features achieves almost identical results.", "labels": [], "entities": []}, {"text": "This is an interesting result, as the pattern features encapsulate significant linguistic knowledge, which apparently can be approximated by a purely distributional approximation.", "labels": [], "entities": []}, {"text": "While the disambiguation level of 70% is lower than the rate of 85% achieved in English, it must be noted that the task of full morphological disambiguation in Hebrew is much harder -we manage to select one tag out of 3,561 for unknown words as opposed to one out of 46 in English.", "labels": [], "entities": [{"text": "full morphological disambiguation", "start_pos": 123, "end_pos": 156, "type": "TASK", "confidence": 0.6257202426592509}]}, {"text": "shows the result of the disambiguation when we only take into account the POS tag of the unknown tokens.", "labels": [], "entities": [{"text": "POS tag", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.87189781665802}]}, {"text": "The same models reach the best results in this case as well (Pattern+Letters and WordContext+Letters).", "labels": [], "entities": []}, {"text": "The best disambiguation result is 78.5% -still much lower than the 85% achieved in English.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.9328668117523193}]}, {"text": "The main reason for this lower level is that the task in Hebrew includes segmentation of prefixes and suffixes in addition to POS classification.", "labels": [], "entities": [{"text": "POS classification", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.821343719959259}]}, {"text": "We are currently investigating models that will take into account the specific nature of prefixes in Hebrew (which encode conjunctions, definite articles and prepositions) to better predict the segmentation of unknown words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Evaluation of unknown token full morphological analysis.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of unknown token POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.6953283548355103}]}]}