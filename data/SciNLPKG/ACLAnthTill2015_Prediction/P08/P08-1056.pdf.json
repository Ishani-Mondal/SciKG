{"title": [{"text": "Word Clustering and Word Selection based Feature Reduction for MaxEnt based Hindi NER", "labels": [], "entities": []}], "abstractContent": [{"text": "Statistical machine learning methods are employed to train a Named Entity Recognizer from annotated data.", "labels": [], "entities": []}, {"text": "Methods like Maximum Entropy and Conditional Random Fields make use of features for the training purpose.", "labels": [], "entities": []}, {"text": "These methods tend to overfit when the available training corpus is limited especially if the number of features is large or the number of values fora feature is large.", "labels": [], "entities": []}, {"text": "To overcome this we proposed two techniques for feature reduction based on word clustering and selection.", "labels": [], "entities": [{"text": "feature reduction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7383320331573486}, {"text": "word clustering", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.7380078434944153}]}, {"text": "A number of word similarity measures are proposed for clustering words for the Named Entity Recognition task.", "labels": [], "entities": [{"text": "Named Entity Recognition task", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.7585261911153793}]}, {"text": "A few corpus based statistical measures are used for important word selection.", "labels": [], "entities": [{"text": "word selection", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.8174657225608826}]}, {"text": "The feature reduction techniques lead to a substantial performance improvement over baseline Maximum Entropy technique.", "labels": [], "entities": [{"text": "feature reduction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6675376892089844}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) involves locating and classifying the names in a text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7969187547763189}]}, {"text": "NER is an important task, having applications in information extraction, question answering, machine translation and inmost other Natural Language Processing (NLP) applications.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9296257495880127}, {"text": "information extraction", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8142445683479309}, {"text": "question answering", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.926529586315155}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8172619342803955}]}, {"text": "NER systems have been developed for English and few other languages with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9977473616600037}]}, {"text": "These belong to two main categories based on machine learning ( and language or domain specific rules).", "labels": [], "entities": []}, {"text": "In English, the names are usually capitalized which is an important clue for identifying a name.", "labels": [], "entities": []}, {"text": "Absence of capitalization makes the Hindi NER task difficult.", "labels": [], "entities": [{"text": "Hindi NER", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.6470807492733002}]}, {"text": "Also, person names are more diverse in Indian languages, many common words being used as names.", "labels": [], "entities": []}, {"text": "A pioneering work on Hindi NER is by where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition.", "labels": [], "entities": [{"text": "Hindi NER", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.5549314320087433}]}, {"text": "In an effort to reduce overfitting, they use a combination of a Gaussian prior and early-stopping.", "labels": [], "entities": []}, {"text": "In their Maximum Entropy (MaxEnt) based approach for Hindi NER development, also observed that the performance of the MaxEnt based model often decreases when huge number of features are used in the model.", "labels": [], "entities": [{"text": "Hindi NER development", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7198988000551859}]}, {"text": "This is due to overfitting which is a serious problem inmost of the NLP tasks in resource poor languages where annotated data is scarce.", "labels": [], "entities": []}, {"text": "This paper is a study on effectiveness of word clustering and selection as feature reduction techniques for MaxEnt based NER.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7617787718772888}, {"text": "MaxEnt based NER", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.5946371952692667}]}, {"text": "For clustering we use a number of word similarities like cosine similarity among words and co-occurrence, along with the k-means clustering algorithm.", "labels": [], "entities": []}, {"text": "The clusters are then used as features instead of words.", "labels": [], "entities": []}, {"text": "For important word selection we use corpus based statistical measurements to find the importance of the words in the NER task.", "labels": [], "entities": [{"text": "word selection", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.718566969037056}, {"text": "NER task", "start_pos": 117, "end_pos": 125, "type": "TASK", "confidence": 0.921326071023941}]}, {"text": "A significant performance improvement over baseline MaxEnt was observed after using the above feature reduction techniques.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "The MaxEnt based NER system is described in Section 2.", "labels": [], "entities": [{"text": "NER", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8041610717773438}]}, {"text": "Various approaches for word clustering are discussed in Section 3.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7933571040630341}]}, {"text": "Next section presents the procedure for selecting the important words.", "labels": [], "entities": []}, {"text": "In Section 5 experimental results and related discussions are given.", "labels": [], "entities": []}, {"text": "Finally Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The following subsections contain the experimental results using word clustering and important word selection.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.8018629848957062}, {"text": "word selection", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.7289060950279236}]}, {"text": "The results demonstrate the effectiveness of  word clustering and important word selection over the baseline MaxEnt model.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.8043533563613892}, {"text": "word selection", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7200026214122772}]}], "tableCaptions": [{"text": " Table 2: F-values for different features in the MaxEnt based Hindi NER system", "labels": [], "entities": [{"text": "F-values", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9879785776138306}, {"text": "MaxEnt based Hindi NER system", "start_pos": 49, "end_pos": 78, "type": "DATASET", "confidence": 0.8037705898284913}]}, {"text": " Table 3. In this table we have  given the word vectors for a few Hindi words, which  are, sthita (located), shahara (city), jAkara (go), na- gara (township), gA.nva (village), nivAsI (resident),  mishrA (a surname) and limiTeDa (ltd.). From the  table we observe that the word vectors are close for  sthita", "labels": [], "entities": []}, {"text": " Table 3: Example of some word vectors for next (+1)  position (see text for glosses)", "labels": [], "entities": []}, {"text": " Table 4: Variation of MaxEnt based system accuracy de- pending on number of clusters (k)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9755866527557373}]}, {"text": " Table 5: F-values for different features in a MaxEnt based Hindi NER with clustering based feature reduction  [window(\u2212m, +n) refers to the cluster or word features corresponding to previous m positions and next n posi- tions; C1 is the clusters which use sentence level co-occurrence based cosine similarity (3.1), C2 denotes the clusters  which use proximal word based cosine similarity (3.2), C3 denotes the clusters for each positions related to NE (3.3)]", "labels": [], "entities": [{"text": "F-values", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9708774089813232}, {"text": "MaxEnt based Hindi NER", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.6769392117857933}]}, {"text": " Table 6: F-values for different features in a MaxEnt based Hindi NER with important word based feature reduction  [window(\u2212m, +n) refers to the important word or baseline word features corresponding to previous m positions and  next n positions; I1 is the class independent important words (4.1), I2 denotes the important words for each class (4.2),  I3 denotes the important words for each positions (4.3)]", "labels": [], "entities": [{"text": "F-values", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9801607728004456}, {"text": "MaxEnt based Hindi NER", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.6687110736966133}]}]}