{"title": [{"text": "The Complexity of Phrase Alignment Problems", "labels": [], "entities": [{"text": "Phrase Alignment", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.8417616486549377}]}], "abstractContent": [{"text": "Many phrase alignment models operate over the combinatorial space of bijective phrase alignments.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.8001593947410583}, {"text": "bijective phrase alignments", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.7016180356343588}]}, {"text": "We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.", "labels": [], "entities": []}, {"text": "On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 189, "end_pos": 205, "type": "TASK", "confidence": 0.7909739911556244}]}], "introductionContent": [{"text": "Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7857531905174255}]}, {"text": "For some restricted combinatorial spaces of alignments-those that arise in ITG-based phrase models or local distortion models ()-inference can be accomplished using polynomial time dynamic programs.", "labels": [], "entities": []}, {"text": "However, for more permissive models such as and, which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited.", "labels": [], "entities": []}, {"text": "Indeed, conjectures that none exist.", "labels": [], "entities": []}, {"text": "In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.", "labels": [], "entities": []}, {"text": "On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP).", "labels": [], "entities": []}, {"text": "Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers.", "labels": [], "entities": [{"text": "ILP solvers", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.7158407270908356}]}, {"text": "While ILP is of course also NP-hard, we show that, empirically, exact solutions are found very quickly for most problem instances.", "labels": [], "entities": []}, {"text": "In an experiment intended to illustrate the practicality of the ILP approach, we show speed and search accuracy results for aligning phrases under a standard phrase translation model.", "labels": [], "entities": [{"text": "speed", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9704033136367798}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9383860230445862}, {"text": "phrase translation", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7413102090358734}]}], "datasetContent": [], "tableCaptions": []}