{"title": [{"text": "Decompounding query keywords from compounding languages", "labels": [], "entities": [{"text": "Decompounding query keywords from compounding languages", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8186216453711191}]}], "abstractContent": [{"text": "Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR).", "labels": [], "entities": [{"text": "Splitting compound words", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9088718692461649}, {"text": "Machine Translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7815841138362885}, {"text": "Speech Recognition", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7113751471042633}, {"text": "Information Retrieval (IR)", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.834992253780365}]}, {"text": "Furthermore, real-time IR systems (such as search engines) need to cope with noisy data, as user queries are sometimes written quickly and submitted without review.", "labels": [], "entities": [{"text": "IR", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.918931245803833}]}, {"text": "In this paper we apply a state-of-the-art procedure for German decompounding to other compounding languages, and we show that it is possible to have a single decompounding model that is applicable across languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Compounding languages), such as German, Dutch, Danish, Norwegian, Swedish, Greek or Finnish, allow the generation of complex words by merging together simpler ones.", "labels": [], "entities": []}, {"text": "So, for instance, the flower bouquet can be expressed in German as Blumenstr\u00e4u\u00dfe, made up of Blumen (flower) and str\u00e4u\u00dfe (bouquet), and in Finnish as kukkakimppu, from kukka (flower) and kimppu (bunch, collection).", "labels": [], "entities": []}, {"text": "For many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out-of-vocabulary terms.", "labels": [], "entities": []}, {"text": "Decompounders have been used successfully in Information Retrieval (), Machine Translation and Speech Recognition (Adda-).", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7687119543552399}, {"text": "Machine Translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8184158205986023}, {"text": "Speech Recognition", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7189051359891891}]}, {"text": "The Cross Language Evaluation Forum (CLEF) competitions have shown that very simple approaches can produce big gains in Cross Language Information Retrieval (CLIR) for German and Dutch (Monz and de) and for Finnish ().", "labels": [], "entities": [{"text": "Cross Language Information Retrieval (CLIR)", "start_pos": 120, "end_pos": 163, "type": "TASK", "confidence": 0.6989894381591252}]}, {"text": "When working with web data, which has not necessarily been reviewed for correctness, many of the words are more difficult to analyze than when working with standard texts.", "labels": [], "entities": []}, {"text": "There are more words with spelling mistakes, and many texts mix words from different languages.", "labels": [], "entities": []}, {"text": "This problem exists to a larger degree when handling user queries: they are written quickly, not paying attention to mistakes.", "labels": [], "entities": []}, {"text": "However, being able to identify that achzigerjahre should be decompounded as achzig+jahre (where achzig is a misspelled variation of achtzig) is still useful in obtaining some meaning from the user query and in helping the spelling correction system.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 223, "end_pos": 242, "type": "TASK", "confidence": 0.9180973768234253}]}, {"text": "This paper evaluates a state-of-the-art procedure for German splitting, robust enough to handle query data, on different languages, and shows that it is possible to have a single decompounding model that can be applied to all the languages understudy.", "labels": [], "entities": [{"text": "German splitting", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7849220335483551}]}], "datasetContent": [{"text": "Any set of query keywords contains a large amount of noisy data, such as words in foreign languages or misspelled words.", "labels": [], "entities": []}, {"text": "In order to be robust enough to handle this kind of corpus, we require the following fora decompounder: first, obviously, compounds should be split, and non-compounds should be left untouched.", "labels": [], "entities": []}, {"text": "This also applies if they are misspelled.", "labels": [], "entities": []}, {"text": "Unknown words or words involving apart in a foreign language are split if there is a plausible interpretation of them being a compound word.", "labels": [], "entities": []}, {"text": "An example is Turingmaschine (Turing machine) in German, where Turing is an English word.", "labels": [], "entities": []}, {"text": "Finally, words that are not really grammatical compounds, but due to the user forgetting to input the blankspace between the words (like desktopcomputer) are split.", "labels": [], "entities": []}, {"text": "For the evaluation, we have built and manually annotated gold standard sets for German, Dutch, Danish, Norwegian, Swedish and Finnish from fully anonymized search query logs.", "labels": [], "entities": []}, {"text": "Because people do not use capitalization consistently when writing queries, all the query logs are lowercased.", "labels": [], "entities": []}, {"text": "By randomly sampling keywords we would get few compounds (as their frequency is small compared to that of non-compounds), so we have proceeded in the following way to ensure that the gold-standards contain a substantial amount of compounds: we started by building a very naive decompounder that splits a word in several parts using a frequency-based compound splitting method (.", "labels": [], "entities": []}, {"text": "Using this procedure, we obtain two random samples with possibly repeated words: one with words that are considered non-compounds, and the other with words that are considered compounds by this naive approach.", "labels": [], "entities": []}, {"text": "Next, we removed all the duplicates from the previous list, and we had them annotated manually as compounds or non-compounds, including the correct splittings.", "labels": [], "entities": []}, {"text": "The sizes of the final training sets vary between 2,000 and 3,600 words depending on the language.", "labels": [], "entities": []}, {"text": "Each compound was annotated by two human judges who had received the previous instructions on when to consider that a keyword is a compound.", "labels": [], "entities": []}, {"text": "For all the languages considered, exactly one of the two judges was a native speaker living in a country where it is the official language . shows the percentage of agreement in classifying words as compounds or non-compounds (Compound Classification Agreement, CCA) for each language and the Kappa score obtained from it, and the percentage of words for which also the decomposition provided was identical (Decompounding Agreement, DA).", "labels": [], "entities": []}, {"text": "The most common source of disagreement were long words that could be split into two or more  parts.", "labels": [], "entities": []}, {"text": "The evaluation is done using the metrics precision, recall and accuracy, defined in the following way (): \u2022 Correct splits: no. of compounds that are split correctly.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9979360103607178}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9990881681442261}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9986255168914795}, {"text": "Correct", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9898061156272888}]}, {"text": "\u2022 Correct non-splits: no. non-compounds that are not split.", "labels": [], "entities": [{"text": "Correct", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9756262302398682}]}, {"text": "\u2022 Wrong non-splits: no. of compounds and are not split.", "labels": [], "entities": []}, {"text": "\u2022 Wrong faulty splits: no. of compounds that are incorrectly split.", "labels": [], "entities": []}, {"text": "\u2022 Wrong splits: no. of non-compounds that are split.", "labels": [], "entities": []}, {"text": "P recision = correct splits correct splits + wrong faulty splits + wrong splits Recall = correct splits correct splits + wrong faulty splits + wrong non-splits Accuracy = correct splits correct splits + wrong splits  The first motivation of this work is to test whether the results reported for German are easy to reproduce in other languages.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.993925154209137}]}, {"text": "The results, shown in, are very similar across languages, having precision and recall values over 80% for most languages.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9994627833366394}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9967427849769592}]}, {"text": "A notable exception is Dutch, for which the inter-judge agreement was the highest, so we expected the set of words to be easier to classify.", "labels": [], "entities": []}, {"text": "An analysis of the errors reported in the 10-fold crossvalidation indicates that most errors in Dutch were wrong non-splits (in 147 cases) and wrong splits (in 139 cases), with wrong faulty splits happening only in 20 occasions.", "labels": [], "entities": []}, {"text": "Many of the wrong splits are location names and trademarks, like youtube, piratebay or smallville.", "labels": [], "entities": [{"text": "piratebay", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.794751763343811}, {"text": "smallville", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.6996860504150391}]}, {"text": "While the supervised model gives much better results than the unsupervised ones, it still requires the construction of a goldstandard from which to train, which is usually costly.", "labels": [], "entities": []}, {"text": "Therefore, we ran another experiment to check whether the models trained from some languages are applicable to other languages.", "labels": [], "entities": []}, {"text": "shows the results obtained in this case, the last column indicating the results when the model is trained from the training instances from all the other languages together.", "labels": [], "entities": []}, {"text": "For each row, the highest value and those which are inside its 95% confidence interval are highlighted.", "labels": [], "entities": [{"text": "95% confidence interval", "start_pos": 63, "end_pos": 86, "type": "METRIC", "confidence": 0.6291995942592621}]}, {"text": "Interestingly, apart from a few exceptions, the results are rather good for all the pairs of training and test language.", "labels": [], "entities": []}, {"text": "Thus, the use of features like frequencies, probabilities or mutual information of compound parts is truly language-independent and the models learned from one language can safely be applied for decompounding a different language without the need of annotating a gold-standard for it.", "labels": [], "entities": []}, {"text": "Still, some trends in the results can be observed: training with the Danish corpus produced the best results in terms of recall for all the languages, but recall for Danish still improved when we trained on data from all languages.", "labels": [], "entities": [{"text": "Danish corpus", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.8571339547634125}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9982814788818359}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9989470839500427}]}, {"text": "We believe that this indicates that the Danish dataset contains items with a more varied sets of feature combinations, so that the models trained from it have a good coverage on different kinds of compounds, but models trained in other languages are notable to identify many of the compounds in the Danish dataset.", "labels": [], "entities": [{"text": "Danish dataset", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.949251800775528}, {"text": "Danish dataset", "start_pos": 299, "end_pos": 313, "type": "DATASET", "confidence": 0.9296656847000122}]}, {"text": "Concerning precision, training with either the Norwegian or the Finnish data produced very good results for most languages.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9989913105964661}, {"text": "Finnish data", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.6577326655387878}]}, {"text": "This is consistent with the monolingual experiments (see) in which these languages had the best results.", "labels": [], "entities": []}, {"text": "We believe these trends are probably due to the quality of the training data.", "labels": [], "entities": []}, {"text": "Interestingly, the size of the training data is not so relevant, as most of the best results are not located at the last column in the table.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-judge agreement metrics.", "labels": [], "entities": []}, {"text": " Table 3: Results of the several configurations.", "labels": [], "entities": []}, {"text": " Table 4: Results in all the different languages.", "labels": [], "entities": []}]}