{"title": [{"text": "Which Are the Best Features for Automatic Verb Classification", "labels": [], "entities": [{"text": "Automatic Verb Classification", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.6639486054579417}]}], "abstractContent": [{"text": "In this work, we develop and evaluate a wide range of feature spaces for deriving Levin-style verb classifications (Levin, 1993).", "labels": [], "entities": []}, {"text": "We perform the classification experiments using Bayesian Multinomial Regression (an efficient log-linear modeling framework which we found to outperform SVMs for this task) with the proposed feature spaces.", "labels": [], "entities": []}, {"text": "Our experiments suggest that subcategorization frames are not the most effective features for automatic verb classification.", "labels": [], "entities": [{"text": "automatic verb classification", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.6093850334485372}]}, {"text": "A mixture of syntactic information and lexical information works best for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much research in lexical acquisition of verbs has concentrated on the relation between verbs and their argument frames.", "labels": [], "entities": [{"text": "lexical acquisition of verbs", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.7913958355784416}]}, {"text": "Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression of arguments and the assignment of semantic roles is to a large extent driven by deep semantic regularities.", "labels": [], "entities": []}, {"text": "Thus measurements of verb frame patterns can perhaps be used to probe for linguistically relevant aspects of verb meanings.", "labels": [], "entities": []}, {"text": "The correspondence between meaning regularities and syntax has been extensively studied in (hereafter Levin).", "labels": [], "entities": []}, {"text": "Levin's verb classes are based on the ability of a verb to occur or not occur in pairs of syntactic frames that are in some sense meaning preserving (diathesis alternation).", "labels": [], "entities": []}, {"text": "The focus is on verbs for which distribution of syntactic frames is a useful indicator of class membership, and, correspondingly, on classes which are relevant for such verbs.", "labels": [], "entities": []}, {"text": "By using Levin's classification, we obtain a window on some (but not all) of the potentially useful semantic properties of verbs.", "labels": [], "entities": []}, {"text": "Levin's verb classification, like others, helps reduce redundancy in verb descriptions and enables generalizations across semantically similar verbs with respect to their usage.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7391165196895599}]}, {"text": "When the information about a verb type is not available or sufficient for us to draw firm conclusions about its usage, the information about the class to which the verb type belongs can compensate for it, addressing the pervasive problem of data sparsity in a wide range of NLP tasks, such as automatic extraction of subcategorization frames), semantic role labeling (), natural language generation for machine translation (, and deriving predominant verb senses from unlabeled data (.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 344, "end_pos": 366, "type": "TASK", "confidence": 0.7114490071932474}, {"text": "natural language generation", "start_pos": 371, "end_pos": 398, "type": "TASK", "confidence": 0.6802635391553243}, {"text": "machine translation", "start_pos": 403, "end_pos": 422, "type": "TASK", "confidence": 0.7625616192817688}]}, {"text": "Although there exist several manually-created verb lexicons or ontologies, including Levin's verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (), building and tuning lexical information specific to different domains (), and bootstrapping verb lexicons for new languages).", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.8613619804382324}, {"text": "automatic verb classification (AVC)", "start_pos": 131, "end_pos": 166, "type": "TASK", "confidence": 0.6096281160910925}]}, {"text": "AVC helps avoid the expensive hand-coding of such information, but appropriate features must be identified and demonstrated to be effective.", "labels": [], "entities": [{"text": "AVC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.672179639339447}]}, {"text": "In this work, our primary goal is not necessarily to obtain the optimal classification, but rather to investigate the linguistic conditions which are crucial for lexical semantic classification of verbs.", "labels": [], "entities": [{"text": "lexical semantic classification of verbs", "start_pos": 162, "end_pos": 202, "type": "TASK", "confidence": 0.7800639152526856}]}, {"text": "We develop feature sets that combine syntactic and lexical information, which are in principle useful for any Levinstyle verb classification.", "labels": [], "entities": [{"text": "Levinstyle verb classification", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.8130374352137247}]}, {"text": "We test the general applicability and scalability of each feature set to the distinctions among 48 verb classes involving 1,300 verbs, which is, to our knowledge, the largest investigation on English verb classification by far.", "labels": [], "entities": [{"text": "English verb classification", "start_pos": 192, "end_pos": 219, "type": "TASK", "confidence": 0.6129826505978903}]}, {"text": "To preview our results, a feature set that combines both syntactic information and lexical information works much better than either of them used alone.", "labels": [], "entities": []}, {"text": "In addition, mixed feature sets also show potential for scaling well when dealing with larger number of verbs and verb classes.", "labels": [], "entities": []}, {"text": "In contrast, subcategorization frames, at least on their own, are largely ineffective for AVC, despite their evident effectiveness in supporting Levin's initial intuitions.", "labels": [], "entities": [{"text": "AVC", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.7114997506141663}]}], "datasetContent": [{"text": "Following, we adopt a single evaluation measure -macro-averaged recall -for all of our classification tasks.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9563165307044983}]}, {"text": "As discussed below, since we always use balanced training sets for each individual task, it makes sense for our accuracy metric to give equal weight to each class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9985508322715759}]}, {"text": "Macro-averaged recall treats each verb class equally, so that the size of a class does not affect macro-averaged recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.8681908249855042}]}, {"text": "It usually gives a better sense of the quality of classification across all classes.", "labels": [], "entities": []}, {"text": "To calculate macro-averaged recall, the recall value for each individual verb class has to be computed first.", "labels": [], "entities": [{"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9723766446113586}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.998197615146637}]}], "tableCaptions": [{"text": " Table 1: Results from Joanis et al. (2007) (%)", "labels": [], "entities": []}, {"text": " Table 5: Experimental results for Joanis15 (%)", "labels": [], "entities": []}, {"text": " Table 6: Experimental results for Levin48 (%)", "labels": [], "entities": [{"text": "Levin48", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.8965857625007629}]}]}