{"title": [], "abstractContent": [{"text": "A desirable quality of a coreference resolution system is the ability to handle transitivity constraints , such that even if it places high likelihood on a particular mention being corefer-ent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9424259662628174}]}, {"text": "This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.954132616519928}]}, {"text": "We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments.", "labels": [], "entities": []}, {"text": "We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b 3 scorer, and up to 16.5% using cluster f-measure.", "labels": [], "entities": [{"text": "enforcement of transitive closure", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.6329366937279701}]}], "introductionContent": [{"text": "Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9768267571926117}]}, {"text": "They built a decision tree classifier to label pairs of mentions as coreferent or not.", "labels": [], "entities": []}, {"text": "Using their classifier, they would buildup coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed.", "labels": [], "entities": []}, {"text": "Transitive closure in this model was done implicitly.", "labels": [], "entities": [{"text": "Transitive closure", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7146735787391663}]}, {"text": "If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair.", "labels": [], "entities": []}, {"text": "Much work that followed improved upon this strategy, by improving the features (, the type of classifier, and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent.", "labels": [], "entities": []}, {"text": "This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. and highlight the problem of determining whether or not common noun phrases are anaphoric.", "labels": [], "entities": []}, {"text": "They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner.", "labels": [], "entities": []}, {"text": "More recently, utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers.", "labels": [], "entities": [{"text": "integer linear programming (ILP) solver", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.6362656099455697}]}, {"text": "However, when encoding constraints into their ILP solver, they did not enforce transitivity.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.7105326652526855}]}, {"text": "The goal of the present work is simply to show that transitivity constraints area useful source of information, which can and should be incorporated into an ILP-based coreference system.", "labels": [], "entities": []}, {"text": "For this goal, we put aside the anaphoricity classifier and focus on the pairwise classifier and transitivity constraints.", "labels": [], "entities": []}, {"text": "We build a pairwise logistic classifier, trained on all pairs of mentions, and then attest time we use an ILP solver equipped with transitivity constraints to find the most likely legal assignment to the variables which represent the pairwise decisions.", "labels": [], "entities": []}, {"text": "1 Our results show a significant improvement compared to the na\u00a8\u0131vena\u00a8\u0131ve use of the pairwise classifier.", "labels": [], "entities": []}, {"text": "Other work on global models of coreference (as opposed to pairwise models) has included: who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; who defined several conditional random field-based models; who took a reranking approach; and who use a probabilistic first-order logic model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used lp solve 3 to solve our ILP optimization problems.", "labels": [], "entities": [{"text": "ILP optimization", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.8926488757133484}]}, {"text": "We ran experiments on two datasets.", "labels": [], "entities": []}, {"text": "We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus.", "labels": [], "entities": [{"text": "MUC-6 formal training and test data", "start_pos": 12, "end_pos": 47, "type": "DATASET", "confidence": 0.7460743884245554}, {"text": "NWIRE", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9660230278968811}, {"text": "BNEWS", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.8040285110473633}, {"text": "ACE (Phase 2) corpus", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.8316845198472341}]}, {"text": "This corpus had a third portion, NPAPER, but we found that several documents where too long for lp solve to find a solution.", "labels": [], "entities": [{"text": "NPAPER", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.8302932381629944}]}, {"text": "We added named entity (NE) tags to the data using the tagger of.", "labels": [], "entities": []}, {"text": "The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger.", "labels": [], "entities": [{"text": "ACE data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9069248139858246}]}, {"text": "We also added part of speech (POS) tags to the data using the tagger of, and used the tags to decide if mentions were plural or singular.", "labels": [], "entities": []}, {"text": "The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC-6 data is not, so the POS and NE tags were used to infer this information.", "labels": [], "entities": [{"text": "ACE data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9373162388801575}, {"text": "MUC-6 data", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.9585334658622742}]}, {"text": "Our feature set was simple, and included many features from (), including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features.", "labels": [], "entities": []}, {"text": "We had additional features for NE tags, head matching and head substring matching.", "labels": [], "entities": [{"text": "head matching", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8622119724750519}, {"text": "head substring matching", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.6693753798802694}]}, {"text": "The MUC scorer) is a popular coreference evaluation metric, but we found it to be fatally flawed.", "labels": [], "entities": [{"text": "MUC scorer)", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.745096762975057}, {"text": "coreference evaluation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.8924562036991119}]}, {"text": "As observed by, if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score -significantly higher than any published system.", "labels": [], "entities": [{"text": "MUC-6 formal test set", "start_pos": 101, "end_pos": 122, "type": "DATASET", "confidence": 0.8694433122873306}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.999700665473938}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9986693859100342}, {"text": "F1 score", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9855471551418304}]}, {"text": "The b 3 scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer.", "labels": [], "entities": [{"text": "b 3 scorer", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8918467362721761}, {"text": "MUC scorer", "start_pos": 93, "end_pos": 103, "type": "DATASET", "confidence": 0.8091388046741486}]}, {"text": "However, coreference resolution is a clustering task, and many cluster scorers already exist.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.9637103378772736}]}, {"text": "In addition to the MUC and b 3 scorers, we also evaluate using cluster f-measure, which is the standard f-measure computed over true/false coreference decisions for pairs of mentions; the Rand index, which is pairwise accuracy of the clustering; and variation of information, which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better).", "labels": [], "entities": [{"text": "MUC", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.7253459692001343}, {"text": "Rand index", "start_pos": 188, "end_pos": 198, "type": "METRIC", "confidence": 0.9480377435684204}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9840710163116455}]}], "tableCaptions": [{"text": " Table 1: Results on all three datasets with all five scoring metrics. For VOI a lower number is better.", "labels": [], "entities": [{"text": "VOI", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.738137423992157}]}]}