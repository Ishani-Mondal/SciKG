{"title": [], "abstractContent": [{"text": "This paper describes how external resources can be used to improve parser performance for heavily lexicalised grammars, looking at both robustness and efficiency.", "labels": [], "entities": []}, {"text": "In terms of robust-ness, we try using different types of external data to increase lexical coverage, and find that simple POS tags have the most effect, increasing coverage on unseen data by up to 45%.", "labels": [], "entities": []}, {"text": "We also show that filtering lexical items in a su-pertagging manner is very effective in increasing efficiency.", "labels": [], "entities": []}, {"text": "Even using vanilla POS tags we achieve some efficiency gains, but when using detailed lexical types as supertags we manage to halve parsing time with minimal loss of coverage or precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9707043766975403}]}], "introductionContent": [{"text": "Heavily lexicalised grammars have been used in applications such as machine translation and information extraction because they can produce semantic structures which provide more information than less informed parsers.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8388726115226746}, {"text": "information extraction", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.8003927767276764}]}, {"text": "In particular, because of the structural and semantic information attached to lexicon items, these grammars do well at describing complex relationships, like non-projectivity and center embedding.", "labels": [], "entities": []}, {"text": "However, the cost of this additional information sometimes makes deep parsers that use these grammars impractical.", "labels": [], "entities": []}, {"text": "Firstly because, if the information is not available, the parsers may fail to produce an analysis, a failure of robustness.", "labels": [], "entities": []}, {"text": "Secondly, the effect of analysing the extra information can slow the parser down, causing efficiency problems.", "labels": [], "entities": []}, {"text": "This paper describes experiments aimed at improving parser performance in these two areas, by annotating the input given to one such deep parser, the PET parser, which uses lexicalised grammars developed under the HPSG formalism.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 214, "end_pos": 218, "type": "DATASET", "confidence": 0.9389467835426331}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results obtained when restricting the parser lex- icon according to the POS tag, where words are restricted  according to a threshold of POS probabilities.", "labels": [], "entities": []}, {"text": " Table 2: Results obtained when restricting the parser lex- icon according to the predicted lexical type, where words  are restricted according to a threshold of tag probabilities.  Two models, with and without POS tags as features, were  used.", "labels": [], "entities": []}, {"text": " Table 4: Parser coverage with baseline using no unknown word handling and unknown word handling using POS tags,  SProUT named entity data as the only annotation, or SProUT tags in addition to POS annotation.", "labels": [], "entities": [{"text": "Parser coverage", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8852702081203461}]}, {"text": " Table 5: Parser coverage using a lexical type predictor for unknown word handling. The predictor was run in single tag  mode, and then in multi-tag mode. Two different tagging models were used, with and without POS tags as features.", "labels": [], "entities": [{"text": "Parser coverage", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9279610514640808}, {"text": "word handling", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.7451238036155701}]}]}