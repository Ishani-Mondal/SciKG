{"title": [{"text": "Learning Document-Level Semantic Properties from Free-text Annotations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper demonstrates anew method for leveraging free-text annotations to infer semantic properties of documents.", "labels": [], "entities": []}, {"text": "Free-text annotations are becoming increasingly abundant, due to the recent dramatic growth in semi-structured, user-generated online content.", "labels": [], "entities": []}, {"text": "An example of such content is product reviews, which are often annotated by their authors with pros/cons keyphrases such as \"a real bar-gain\" or \"good value.\"", "labels": [], "entities": []}, {"text": "To exploit such noisy annotations, we simultaneously find a hidden paraphrase structure of the keyphrases, a model of the document texts, and the underlying semantic properties that link the two.", "labels": [], "entities": []}, {"text": "This allows us to predict properties of unannotated documents.", "labels": [], "entities": []}, {"text": "Our approach is implemented as a hierarchical Bayesian model with joint inference , which increases the robustness of the keyphrase clustering and encourages the document model to correlate with semantically meaningful properties.", "labels": [], "entities": []}, {"text": "We perform several evaluations of our model, and find that it substantially outperforms alternative approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "A central problem in language understanding is transforming raw text into structured representations.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.7255403101444244}]}, {"text": "Learning-based approaches have dramatically increased the scope and robustness of this type of automatic language processing, but they are typically dependent on large expert-annotated datasets, which are costly to produce.", "labels": [], "entities": [{"text": "automatic language processing", "start_pos": 95, "end_pos": 124, "type": "TASK", "confidence": 0.6877899865309397}]}, {"text": "In this paper, we show how novice-generated free-text annotations available online can be leveraged to automatically infer document-level semantic properties.", "labels": [], "entities": []}, {"text": "With the rapid increase of online content created by end users, noisy free-text annotations have pros/cons: great nutritional value ...", "labels": [], "entities": []}, {"text": "combines it all: an amazing product, quick and friendly service, cleanliness, great nutrition ...", "labels": [], "entities": []}, {"text": "pros/cons: a bit pricey, healthy ... is an awesome place to go if you are health conscious.", "labels": [], "entities": []}, {"text": "They have some really great low calorie dishes and they publish the calories and fat grams per serving.", "labels": [], "entities": []}, {"text": "For example, consider reviews of consumer products and services.", "labels": [], "entities": []}, {"text": "Often, such reviews are annotated with keyphrase lists of pros and cons.", "labels": [], "entities": []}, {"text": "We would like to use these keyphrase lists as training labels, so that the properties of unannotated reviews can be predicted.", "labels": [], "entities": []}, {"text": "Having such a system would facilitate structured access and summarization of this data.", "labels": [], "entities": [{"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9795556664466858}]}, {"text": "However, novicegenerated keyphrase annotations are incomplete descriptions of their corresponding review texts.", "labels": [], "entities": []}, {"text": "Furthermore, they lack consistency: the same underlying property maybe expressed in many ways, e.g., \"healthy\" and \"great nutritional value\" (see.", "labels": [], "entities": [{"text": "consistency", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9815513491630554}]}, {"text": "To take advantage of such noisy labels, a system must both uncover their hidden clustering into properties, and learn to predict these properties from review text.", "labels": [], "entities": []}, {"text": "This paper presents a model that addresses both problems simultaneously.", "labels": [], "entities": []}, {"text": "We assume that both the document text and the selection of keyphrases are governed by the underlying hidden properties of the document.", "labels": [], "entities": []}, {"text": "Each property indexes a language model, thus allowing documents that incorporate the same property to share similar features.", "labels": [], "entities": []}, {"text": "In addition, each keyphrase is associated with a property; keyphrases that are associated with the same property should have similar distributional and surface features.", "labels": [], "entities": []}, {"text": "We link these two ideas in a joint hierarchical Bayesian model.", "labels": [], "entities": []}, {"text": "Keyphrases are clustered based on their distributional and lexical properties, and a hidden topic model is applied to the document text.", "labels": [], "entities": []}, {"text": "Crucially, the keyphrase clusters and document topics are linked, and inference is performed jointly.", "labels": [], "entities": []}, {"text": "This increases the robustness of the keyphrase clustering, and ensures that the inferred hidden topics are indicative of salient semantic properties.", "labels": [], "entities": [{"text": "keyphrase clustering", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7746719717979431}]}, {"text": "Our model is broadly applicable to many scenarios where documents are annotated in a noisy manner.", "labels": [], "entities": []}, {"text": "In this work, we apply our method to a collection of reviews in two categories: restaurants and cell phones.", "labels": [], "entities": []}, {"text": "The training data consists of review text and the associated pros/cons lists.", "labels": [], "entities": []}, {"text": "We then evaluate the ability of our model to predict review properties when the pros/cons list is hidden.", "labels": [], "entities": []}, {"text": "Across a variety of evaluation scenarios, our algorithm consistently outperforms alternative strategies by a wide margin.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Sets We evaluate our system on reviews from two categories, restaurants and cell phones.", "labels": [], "entities": []}, {"text": "These reviews were downloaded from the popular Epinions 2 website.", "labels": [], "entities": [{"text": "Epinions 2 website", "start_pos": 47, "end_pos": 65, "type": "DATASET", "confidence": 0.8767732580502828}]}, {"text": "Users of this website evaluate products by providing both a textual description of their opinion, as well as concise lists of keyphrases (pros and cons) summarizing the review.", "labels": [], "entities": []}, {"text": "The statistics of this dataset are provided in standard, we hand-annotated a subset of the reviews from the restaurant category.", "labels": [], "entities": []}, {"text": "The annotation effort focused on eight commonly mentioned properties, such as those underlying the keyphrases \"pleasant atmosphere\" and \"attentive staff.\"", "labels": [], "entities": []}, {"text": "Two raters annotated 160 reviews, 30 of which were annotated by both.", "labels": [], "entities": []}, {"text": "Cohen's kappa, a measure of interrater agreement ranging from zero to one, was 0.78 for this subset, indicating high agreement.", "labels": [], "entities": []}, {"text": "Each review was annotated with 2.56 properties on average.", "labels": [], "entities": []}, {"text": "Each manually-annotated property corresponded to an average of 19.1 keyphrases in the restaurant data, and 6.7 keyphrases in the cellphone data.", "labels": [], "entities": []}, {"text": "This supports our intuition that a single semantic property maybe expressed using a variety of different keyphrases.", "labels": [], "entities": []}, {"text": "Training Our model needs to be provided with the number of clusters K.", "labels": [], "entities": []}, {"text": "We set K large enough for the model to learn effectively on the development set.", "labels": [], "entities": []}, {"text": "For the restaurant data -where the gold standard identified eight semantic properties -we set K to 20, allowing the model to account for keyphrases not included in the eight most common properties.", "labels": [], "entities": []}, {"text": "For the cell phones category, we set K to 30.", "labels": [], "entities": [{"text": "K", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9945498108863831}]}, {"text": "To improve the model's convergence rate, we perform two initialization steps for the Gibbs sampler.", "labels": [], "entities": []}, {"text": "First, sampling is done only on the keyphrase clustering component of the model, ignoring document text.", "labels": [], "entities": [{"text": "keyphrase clustering", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.6731570810079575}]}, {"text": "Second, we fix this clustering and sample the remaining model parameters.", "labels": [], "entities": []}, {"text": "These two steps are run for 5,000 iterations each.", "labels": [], "entities": []}, {"text": "The full joint model is then sampled for 100,000 iterations.", "labels": [], "entities": []}, {"text": "Inspection of the parameter estimates confirms model convergence.", "labels": [], "entities": []}, {"text": "On a 2GHz dual-core desktop machine, a multi-threaded C++ implementation of model training takes about two hours for each dataset.", "labels": [], "entities": []}, {"text": "Inference The final point estimate used for testing is an average (for continuous variables) or a mode (for discrete variables) over the last 1,000 Gibbs sampling iterations.", "labels": [], "entities": []}, {"text": "Averaging is a heuristic that is applicable in our case because our sample histograms are unimodal and exhibit low skew.", "labels": [], "entities": [{"text": "Averaging", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.867970883846283}]}, {"text": "The model usually works equally well using singlesample estimates, but is more prone to estimation noise.", "labels": [], "entities": []}, {"text": "As previously mentioned, we convert word topic assignments to document properties by examining the proportion of words supporting each property.", "labels": [], "entities": []}, {"text": "A threshold for this proportion is set for each property via the development set.", "labels": [], "entities": []}, {"text": "Evaluation Our first evaluation examines the accuracy of our model and the baselines by comparing their output against the keyphrases provided by the review authors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9987549781799316}]}, {"text": "More specifically, the model first predicts the properties supported by a given review.", "labels": [], "entities": []}, {"text": "We then test whether the original authors' keyphrases are contained in the clusters associated with these properties.", "labels": [], "entities": []}, {"text": "As noted above, the authors' keyphrases are often incomplete.", "labels": [], "entities": []}, {"text": "To perform a noise-free comparison, we based our second evaluation on the manually constructed gold standard for the restaurant category.", "labels": [], "entities": []}, {"text": "We took the most commonly observed keyphrase from each of the eight annotated properties, and tested whether they are supported by the model based on the document text.", "labels": [], "entities": []}, {"text": "In both types of evaluation, we measure the model's performance using precision, recall, and Fscore.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9997490048408508}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9995781779289246}, {"text": "Fscore", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9994805455207825}]}, {"text": "These are computed in the standard manner, based on the model's keyphrase predictions compared against the corresponding references.", "labels": [], "entities": []}, {"text": "The sign test was used for statistical significance testing).", "labels": [], "entities": [{"text": "statistical significance testing", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7816759943962097}]}, {"text": "Baselines To the best of our knowledge, this task not been previously addressed in the literature.", "labels": [], "entities": []}, {"text": "We therefore consider five baselines that allow us to explore the properties of this task and our model.", "labels": [], "entities": []}, {"text": "Random: Each keyphrase is supported by a document with probability of one half.", "labels": [], "entities": [{"text": "Random", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8700925707817078}]}, {"text": "This baseline's results are computed (in expectation) rather than actually run.", "labels": [], "entities": []}, {"text": "This method is expected to have a recall of 0.5, because in expectation it will select half of the correct keyphrases.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996567964553833}]}, {"text": "Its precision is the proportion of supported keyphrases in the test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994720816612244}]}, {"text": "Phrase in text: A keyphrase is supported by a document if it appears verbatim in the text.", "labels": [], "entities": []}, {"text": "Because of this narrow requirement, precision should be high whereas recall will below.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.999783456325531}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9997285008430481}]}], "tableCaptions": [{"text": " Table 1: Statistics of the reviews dataset by category.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of the property predictions made by our model and the baselines in the two categories as evaluated  against the gold and free-text annotations. Results for our model using the fixed, manually-created gold clusterings are  also shown. The methods against which our model has significantly better results on the sign test are indicated with a   *  for p <= 0.05, and \u22c4 for p <= 0.1.", "labels": [], "entities": []}, {"text": " Table 3: Rand Index scores of our model's clusters, using  only keyphrase similarity vs. using keyphrases and text  jointly. Comparison of cluster quality is against the gold  standard.", "labels": [], "entities": [{"text": "Rand Index scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8432806134223938}]}]}