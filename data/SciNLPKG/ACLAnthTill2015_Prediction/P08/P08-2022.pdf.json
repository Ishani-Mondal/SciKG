{"title": [{"text": "Dictionary Definitions based Homograph Identification using a Generative Hierarchical Model", "labels": [], "entities": [{"text": "Homograph Identification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9327571392059326}]}], "abstractContent": [{"text": "A solution to the problem of homograph (words with multiple distinct meanings) identification is proposed and evaluated in this paper.", "labels": [], "entities": [{"text": "homograph (words with multiple distinct meanings) identification", "start_pos": 29, "end_pos": 93, "type": "TASK", "confidence": 0.629827747742335}]}, {"text": "It is demonstrated that a mixture model based framework is better suited for this task than the standard classification algorithms-relative improvement of 7% in F1 measure and 14% in Cohen's kappa score is observed.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9891764223575592}]}], "introductionContent": [{"text": "Lexical ambiguity resolution is an important research problem for the fields of information retrieval and machine translation.", "labels": [], "entities": [{"text": "Lexical ambiguity resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9185945391654968}, {"text": "information retrieval", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7908659875392914}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.8148995637893677}]}, {"text": "However, making fine-grained sense distinctions for words with multiple closelyrelated meanings is a subjective task, which makes it difficult and error-prone.", "labels": [], "entities": []}, {"text": "Fine-grained sense distinctions aren't necessary for many tasks, thus a possiblysimpler alternative is lexical disambiguation at the level of homographs ().", "labels": [], "entities": []}, {"text": "Homographs area special case of semantically ambiguous words: Words that can convey multiple distinct meanings.", "labels": [], "entities": []}, {"text": "For example, the word bark can imply two very different concepts -'outer layer of a tree trunk', or, 'the sound made by a dog' and thus is a homograph.", "labels": [], "entities": []}, {"text": "Ironically, the definition of the word 'homograph' is itself ambiguous and much debated; however, in this paper we consistently use the above definition.", "labels": [], "entities": []}, {"text": "If the goal is to do word-sense disambiguation of homographs in a very large corpus, a manuallygenerated homograph inventory maybe impractical.", "labels": [], "entities": [{"text": "word-sense disambiguation of homographs", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.7819748818874359}]}, {"text": "In this case, the first step is to determine which words in a lexicon are homographs.", "labels": [], "entities": []}, {"text": "This problem is the subject of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "A stratified division of the gold standard data in the proportion of 0.75 and 0.25 was done in the first step.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.8910820285479227}]}, {"text": "The smaller portion of this division was held out as the testing dataset.", "labels": [], "entities": []}, {"text": "The bigger portion was further divided into two portions of 0.75 and 0.25 for the training set and the tuning set, respectively.", "labels": [], "entities": []}, {"text": "The best and the worst kappa between a human annotator and the test set are 0.92 and 0.78.", "labels": [], "entities": [{"text": "kappa", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9060916900634766}]}, {"text": "Each of the three models described in Section 2.2 were experimented with both Dirichlet and MVN as the conditional.", "labels": [], "entities": []}, {"text": "An additional experiment using two standard classification algorithms -Kernel Based Na\u00efve Bayes (NB) and Support Vector Machines (SVM) was performed.", "labels": [], "entities": []}, {"text": "We refer to this as the baseline experiment.", "labels": [], "entities": []}, {"text": "The Na\u00efve Bayes classifier outperformed SVM on the tuning as well as the test set and thus we report NB results only.", "labels": [], "entities": [{"text": "NB", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9235926866531372}]}, {"text": "A four-fold cross-validation was employed for the all the experiments on the tuning set.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "The reported precision, recall and F1 values are for the homograph class.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9980605244636536}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9997065663337708}, {"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.999555766582489}]}, {"text": "The na\u00efve assumption of class conditional feature independence is common to simple Na\u00efve Bayes classifier, a kernel based NB classifier; however, unlike simple NB it is capable of modeling non-Gaussian distributions.", "labels": [], "entities": []}, {"text": "Note that in spite of this advantage the kernel based NB is outperformed by the MVN based hierarchical model.", "labels": [], "entities": []}, {"text": "Our nine features are by definition correlated and thus it was our hypothesis that a multivariate distribution such as MVN which can capture the covariance amongst the features will be a better fit.", "labels": [], "entities": []}, {"text": "The above finding confirms this hypothesis.", "labels": [], "entities": []}, {"text": "One of the known situations when mixture models out-perform standard classification algorithms is when the data comes from highly overlapping distributions.", "labels": [], "entities": []}, {"text": "In such cases the classification algorithms that try to place the decision boundary in a sparse area are prone to higher error-rates than mixture model based approach.", "labels": [], "entities": []}, {"text": "We believe that this is explanations of the observed results.", "labels": [], "entities": []}, {"text": "On the test set a relative improvement of 7% in F1 and 14% in kappa statistic is obtained using the MVN mixture model.", "labels": [], "entities": [{"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9994521737098694}]}, {"text": "The results for the semi-supervised models are non-conclusive.", "labels": [], "entities": []}, {"text": "Our post-experimental analysis reveals that the parameter updation process using the unlabeled data has an effect of overly separating the two overlapping distributions.", "labels": [], "entities": []}, {"text": "This is triggered by our threshold based EM methodology which includes only those data-points for which the model is highly confident; however such datapoints are invariable from the non-overlapping regions of the distribution, which gives a false view to the learner that the distributions are less overlapping.", "labels": [], "entities": []}, {"text": "We believe that the unsupervised models also suffer from the above problem in addition to the possibility of poor initializations.", "labels": [], "entities": []}], "tableCaptions": []}