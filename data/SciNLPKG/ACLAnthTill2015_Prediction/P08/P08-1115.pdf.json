{"title": [], "abstractContent": [{"text": "Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well.", "labels": [], "entities": [{"text": "Word lattice decoding", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6420745253562927}, {"text": "spoken language translation", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.7583937048912048}, {"text": "translation of text genres", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.8553992360830307}]}, {"text": "We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammar-based models.", "labels": [], "entities": []}, {"text": "Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models.", "labels": [], "entities": []}, {"text": "Our experiments evaluating the approach demonstrate substantial gains for Chinese-English and Arabic-English translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "When Brown and colleagues introduced statistical machine translation in the early 1990s, their key insight -harkening back to Weaver in the late 1940s -was that translation could be viewed as an instance of noisy channel modeling ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6367084483305613}, {"text": "translation", "start_pos": 161, "end_pos": 172, "type": "TASK", "confidence": 0.9612526893615723}]}, {"text": "They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models).", "labels": [], "entities": []}, {"text": "Today, virtually all statistical translation systems seek the best hypothesis e fora given input fin the source language, according t\u00f4 t\u00f4 e = arg max e P r(e|f ) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f . There, Bertoldi and others have recently found that, rather than translating a single-best transcription f , it is advantageous to allow the MT decoder to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6575655043125153}, {"text": "translation of speech recognition output", "start_pos": 182, "end_pos": 222, "type": "TASK", "confidence": 0.7863599419593811}]}, {"text": "Why, however, should this advantage be limited to translation from spoken input?", "labels": [], "entities": [{"text": "translation from spoken input", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.8439885824918747}]}, {"text": "Even for text, there are often multiple ways to derive a sequence of words from the input string.", "labels": [], "entities": []}, {"text": "Segmentation of Chinese, decompounding in German, morphological analysis for Arabic -across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence.", "labels": [], "entities": []}, {"text": "Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in.", "labels": [], "entities": []}, {"text": "In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7381831556558609}]}, {"text": "In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.6711471527814865}]}, {"text": "Formally, the approach we take can bethought of as a \"noisier channel\", where an observed signal o gives rise to a set of source-language strings f \u2208 F(o) and we seek\u00ea seek\u02c6seek\u00ea = arg max P r(e)P r(f |e)P r(o|f ).", "labels": [], "entities": []}, {"text": "Following, we use the maximum entropy framework ( to directly model the posterior P r(e, f |o) with parameters tuned to minimize a loss function representing the quality only of the resulting translations.", "labels": [], "entities": []}, {"text": "Thus, we make use of the following general decision rule: In principle, one could decode according to (2) simply by enumerating and decoding each f \u2208 F(o); however, for any interestingly large F(o) this will be impractical.", "labels": [], "entities": []}, {"text": "We assume that for many interesting cases of F(o), there will be identical substrings that express the same content, and therefore a lattice representation is appropriate.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss decoding with this model in general, and then show how two classes of translation models can easily be adapted for lattice translation; we achieve a unified treatment of finite-state and hierarchical phrase-based models by treating lattices as a subcase of weighted finite state automata (FSAs).", "labels": [], "entities": [{"text": "lattice translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.760465532541275}]}, {"text": "In Section 3, we identify and solve issues that arise with reordering in non-linear FSAs, i.e. FSAs where every path does not pass through every node.", "labels": [], "entities": []}, {"text": "Section 4 presents two applications of the noisier channel paradigm, demonstrating substantial performance gains in Arabic-English and Chinese-English translation.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss relevant prior work, and we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6534140110015869}]}, {"text": "We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance.", "labels": [], "entities": []}, {"text": "For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus ().", "labels": [], "entities": []}, {"text": "summarizes the results of the phrasebased systems.", "labels": [], "entities": []}, {"text": "On both test sets, the shortest path metric improved the BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9992361068725586}]}, {"text": "As expected, the lexicalized reordering model improved translation quality over the baseline; however, the improvement was more substantial in the model that used the shortest-path distance metric (which was already a higher baseline).", "labels": [], "entities": []}, {"text": "summarizes the results of our experiment comparing the performance of two distance metrics to determine whether a rule has exceeded the decoder's span limit.", "labels": [], "entities": []}, {"text": "The pattern is the same, showing a clear increase in BLEU for the shortest path metric over the baseline.: Effect of distance metric on hierarchical model performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.999554455280304}]}, {"text": "In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (), and one developed at Stanford University ().", "labels": [], "entities": [{"text": "Chinese word segmenters", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7398410042126974}]}, {"text": "In addition, we used a character-based segmentation.", "labels": [], "entities": []}, {"text": "In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation.", "labels": [], "entities": [{"text": "character segmentation", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8020457923412323}, {"text": "Harbin segmentation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.733505368232727}, {"text": "Stanford segmentation", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.747622162103653}]}, {"text": "We built two types of lattices: one that combines the Harbin and Stanford segmenters (hs+ss), and one which uses all three segmentations (hs+ss+cs).", "labels": [], "entities": []}, {"text": "The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences).", "labels": [], "entities": [{"text": "NIST MT06 Eval corpus", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.8890042304992676}, {"text": "UN data", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.7622146904468536}]}, {"text": "The corpus was analyzed with the three segmentation schemes.", "labels": [], "entities": []}, {"text": "For the systems using word lattices, the training data contained the versions of the corpus appropriate for the segmentation schemes used in the input.", "labels": [], "entities": []}, {"text": "That is, for the hs+ss condition, the training data consisted of two copies of the corpus: one segmented with the Harbin segmenter and the other with the Stanford segmenter.", "labels": [], "entities": []}, {"text": "A trigram English language model with modified Kneser-Ney smoothing was trained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments.", "labels": [], "entities": [{"text": "Gigaword v2 English Corpus", "start_pos": 148, "end_pos": 174, "type": "DATASET", "confidence": 0.8979655802249908}]}, {"text": "The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate train-ing.", "labels": [], "entities": [{"text": "NIST MT03 test set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9048523753881454}]}, {"text": "The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06).", "labels": [], "entities": [{"text": "NIST 2005 and 2006 evaluation sets", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.9626305202643076}, {"text": "MT05", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.8484354615211487}, {"text": "MT06", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8728868961334229}]}, {"text": "Experimental results: Word-lattices improve translation quality.", "labels": [], "entities": []}, {"text": "We used both a phrase-based translation model, decoded using our modified version of Moses (, and a hierarchical phrase-based translation model, using our modified version of Hiero (.", "labels": [], "entities": []}, {"text": "These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3.", "labels": [], "entities": []}, {"text": "We observed that the coverage of named entities (NEs) in our baseline systems was rather poor.", "labels": [], "entities": [{"text": "coverage of named entities (NEs)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.5753678934914725}]}, {"text": "Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger ().", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "We see that using word lattices improves BLEU scores both in the phrase-based model and hierarchical model as compared to the single-best segmentation approach.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9970690608024597}]}, {"text": "All results using our word-lattice decoding for the hierarchical models (hs+ss and hs+ss+cs) are significantly better than the best segmentation (ss).", "labels": [], "entities": []}, {"text": "For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05.", "labels": [], "entities": [{"text": "MT05", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.9685038328170776}]}, {"text": "The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically significant.", "labels": [], "entities": [{"text": "segmentation (hs)", "start_pos": 46, "end_pos": 63, "type": "METRIC", "confidence": 0.7503681108355522}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9988812804222107}]}, {"text": "Even if the results are not statistically significant for MT06, there is a high decrease in OOV items when using word-lattices.", "labels": [], "entities": [{"text": "MT06", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.8195919990539551}, {"text": "OOV items", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9716664850711823}]}, {"text": "For example, for MT06 the number of OOVs in the hs translation is 484.", "labels": [], "entities": [{"text": "MT06", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.7005280256271362}]}, {"text": "During the summer period , most media buzz was supportive of the general . The number of OOVs decreased by 19% for hs+ss and by 75% for hs+ss+cs.", "labels": [], "entities": [{"text": "number of OOVs", "start_pos": 79, "end_pos": 93, "type": "METRIC", "confidence": 0.7878571152687073}]}, {"text": "As mentioned in Section 3, using lexical reordering for word-lattices further improves the translation quality.", "labels": [], "entities": []}, {"text": "We created lattices from an unsegmented version of the Arabic test data and generated alternative arcs where clitics as well as the definiteness marker and the future tense marker were segmented into tokens.", "labels": [], "entities": [{"text": "Arabic test data", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.7864409685134888}]}, {"text": "We used the Buckwalter morphological analyzer and disambiguated the analysis using a simple unigram model trained on the Penn Arabic Treebank.", "labels": [], "entities": [{"text": "Buckwalter morphological analyzer", "start_pos": 12, "end_pos": 45, "type": "DATASET", "confidence": 0.9057199756304423}, {"text": "Penn Arabic Treebank", "start_pos": 121, "end_pos": 141, "type": "DATASET", "confidence": 0.9917211135228475}]}, {"text": "For these experiments we made use of the entire NIST MT08 training data, although for training of the system, we used a subsampling method proposed by Kishore Papineni that aims to include training sentences containing ngrams in the test data (personal communication).", "labels": [], "entities": [{"text": "NIST MT08 training data", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.9076861590147018}]}, {"text": "For all systems, we used a 5-gram English LM trained on 250M words of English training data.", "labels": [], "entities": []}, {"text": "The NIST MT03 test set was used as development set for optimizing the interpolation weights using MER training.", "labels": [], "entities": [{"text": "NIST MT03 test set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8992093652486801}]}, {"text": "Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06).", "labels": [], "entities": [{"text": "NIST 2005 and 2006 evaluation sets", "start_pos": 34, "end_pos": 68, "type": "DATASET", "confidence": 0.96274334192276}, {"text": "MT05", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8591840863227844}, {"text": "MT06", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8549526333808899}]}, {"text": "Experimental results: Word-lattices improve translation quality.", "labels": [], "entities": []}, {"text": "Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9964457154273987}]}], "tableCaptions": [{"text": " Table 1: Topologically ordered chart encoding of the  three lattices in", "labels": [], "entities": [{"text": "Topologically ordered chart encoding", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6479435414075851}]}, {"text": " Table 2: Effect of distance metric on phrase-based model  performance.", "labels": [], "entities": []}, {"text": " Table 3: Effect of distance metric on hierarchical model  performance.", "labels": [], "entities": []}, {"text": " Table 4: Chinese Word Segmentation Results", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6357903877894083}]}, {"text": " Table 5: Arabic Morphology Results", "labels": [], "entities": [{"text": "Arabic Morphology", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.61676886677742}]}]}