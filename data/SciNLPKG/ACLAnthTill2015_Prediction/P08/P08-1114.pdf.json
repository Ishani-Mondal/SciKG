{"title": [{"text": "Soft Syntactic Constraints for Hierarchical Phrased-Based Translation", "labels": [], "entities": [{"text": "Hierarchical Phrased-Based Translation", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.7486732403437296}]}], "abstractContent": [{"text": "In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.7774572968482971}]}, {"text": "A number of previous efforts have tackled this trade-off by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment.", "labels": [], "entities": []}, {"text": "We present an approach that explores the trade-off from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language.", "labels": [], "entities": []}, {"text": "We obtain substantial improvements in performance for translation from Chinese and Arabic to English.", "labels": [], "entities": [{"text": "translation from Chinese and Arabic to English", "start_pos": 54, "end_pos": 100, "type": "TASK", "confidence": 0.8467705505234855}]}], "introductionContent": [{"text": "The statistical revolution in machine translation, beginning with) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7284974902868271}, {"text": "detailed language analysis", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.6854239503542582}]}, {"text": "Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source-and particularly target-language syntactic analysis (e.g. (;;) and numerous others).", "labels": [], "entities": []}, {"text": "Chiang (2005) distinguishes statistical MT approaches that are \"syntactic\" in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.8677516579627991}]}, {"text": "The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g.,), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. ().", "labels": [], "entities": [{"text": "syntactic modeling", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8195828795433044}]}, {"text": "Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses.", "labels": [], "entities": []}, {"text": "In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data.", "labels": [], "entities": []}, {"text": "The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it.", "labels": [], "entities": []}, {"text": "For example, begin with a tree-to-string model, using treebank-based target language analysis, and find it useful to modify it in order to accommodate useful \"phrasal\" chunks that are present in parallel training data but not licensed by linguistically motivated parses of the target language.", "labels": [], "entities": []}, {"text": "Similarly,) focus on using syntactically rich representations of source and target parse trees, but they resort to phrase-based translation for modifiers within clauses.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.7252275198698044}]}, {"text": "Finding the right way to balance linguistic analysis with unconstrained data-driven modeling is clearly a key challenge.", "labels": [], "entities": []}, {"text": "In this paper we address this challenge from a less explored direction.", "labels": [], "entities": []}, {"text": "Rather than starting with a system based on linguistically motivated parse trees, we begin with a model that is syntactic only in the formal sense.", "labels": [], "entities": []}, {"text": "We then introduce soft constraints that take source-language parses into account to a limited extent.", "labels": [], "entities": []}, {"text": "Introducing syntactic constraints in this restricted way allows us to take maximal advantage of what can be learned from parallel training data, while effectively factoring in key aspects of linguistically motivated analysis.", "labels": [], "entities": []}, {"text": "As a result, we obtain substantial improvements in performance for both Chinese-English and Arabic-English translation.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly review the Hiero statistical MT framework, upon which this work builds, and we discuss Chiang's initial effort to incorporate soft source-language constituency constraints for Chinese-English translation.", "labels": [], "entities": [{"text": "Hiero statistical MT", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.562882661819458}]}, {"text": "In Section 3, we suggest that an insufficiently fine-grained view of constituency constraints was responsible for Chiang's lack of strong results, and introduce finer grained constraints into the model.", "labels": [], "entities": []}, {"text": "Section 4 demonstrates the the value of these constraints via substantial improvements in ChineseEnglish translation performance, and extends the approach to Arabic-English.", "labels": [], "entities": [{"text": "ChineseEnglish translation", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.5478211343288422}]}, {"text": "Section 5 discusses the results, and Section 6 considers related work.", "labels": [], "entities": []}, {"text": "Finally we conclude in Section 7 with a summary and potential directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang's Hiero system.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9933003187179565}, {"text": "translation from Chinese to English", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.8740283846855164}, {"text": "Chiang's Hiero system", "start_pos": 120, "end_pos": 141, "type": "DATASET", "confidence": 0.6334986463189125}]}, {"text": "Language models were built using the SRI Language Modeling Toolkit) with modified Kneser-Ney smoothing).", "labels": [], "entities": [{"text": "SRI Language Modeling", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6588319937388102}]}, {"text": "Word-level alignments were obtained using GIZA++).", "labels": [], "entities": []}, {"text": "The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature.", "labels": [], "entities": [{"text": "rule-based number translation", "start_pos": 126, "end_pos": 155, "type": "TASK", "confidence": 0.6578940550486246}]}, {"text": "In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers for Chinese, and the Stanford parser v.2007-08-19 for Arabic ().", "labels": [], "entities": []}, {"text": "In addition to the baseline condition, and baseline plus Chiang's (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3.", "labels": [], "entities": []}, {"text": "All models were optimized and tested using the BLEU metric () with the NISTimplemented (\"shortest\") effective reference length, on lowercased, tokenized outputs/references.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9727740585803986}, {"text": "NISTimplemented (\"shortest\") effective reference length", "start_pos": 71, "end_pos": 126, "type": "METRIC", "confidence": 0.6380714135510581}]}, {"text": "Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9639250934123993}]}], "tableCaptions": [{"text": " Table 4: Arabic-English Experiments. Results are  sorted by MT06 BLEU score. *: Better than baseline  (p < .05). **: Better than baseline (p < .01)", "labels": [], "entities": [{"text": "MT06", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.5443878173828125}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9482659101486206}]}]}