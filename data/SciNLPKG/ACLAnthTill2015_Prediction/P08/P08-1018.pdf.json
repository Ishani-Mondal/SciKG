{"title": [{"text": "Selecting Query Term Alterations for Web Search by Exploiting Query Contexts", "labels": [], "entities": [{"text": "Selecting Query Term Alterations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8079765737056732}]}], "abstractContent": [{"text": "Query expansion byword alterations (alterna-tive forms of a word) is often used in Web search to replace word stemming.", "labels": [], "entities": [{"text": "Query expansion byword alterations (alterna-tive forms of a word)", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.7705028463493694}, {"text": "word stemming", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.7236720323562622}]}, {"text": "This allows users to specify particular word forms in a query.", "labels": [], "entities": []}, {"text": "However, if many alterations are added, query traffic will be greatly increased.", "labels": [], "entities": [{"text": "traffic", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9262827038764954}]}, {"text": "In this paper, we propose methods to select only a few useful word alterations for query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.8372119069099426}]}, {"text": "The selection is made according to the appropriateness of the alteration to the query context (using a bigram language model), or according to its expected impact on the retrieval effectiveness (using a regression model).", "labels": [], "entities": []}, {"text": "Our experiments on two TREC collections will show that both methods only select a few expansion terms, but the retrieval effectiveness can be improved significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word stemming is a basic NLP technique used inmost of Information Retrieval (IR) systems.", "labels": [], "entities": [{"text": "Word stemming", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7017629891633987}, {"text": "Information Retrieval (IR)", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.8138158321380615}]}, {"text": "It transforms words into their root forms so as to increase the chance to match similar words/terms that are morphological variants.", "labels": [], "entities": []}, {"text": "For example, with stemming, \"controlling\" can match \"controlled\" because both have the same root \"control\".", "labels": [], "entities": []}, {"text": "Most stemmers, such as the Porter stemmer and Krovetz stemmer, deal with stemming by stripping word suffixes according to a set of morphological rules.", "labels": [], "entities": []}, {"text": "Rule-based approaches are intuitive and easy to implement.", "labels": [], "entities": []}, {"text": "However, while in general, most words can be stemmed correctly; there is often erroneous stemming that unifies unrelated words.", "labels": [], "entities": []}, {"text": "For instance, \"jobs\" is stemmed to \"job\" in both \"find jobs in Apple\" and \"Steve Jobs at Apple\".", "labels": [], "entities": []}, {"text": "This is particularly problematic in Web search, where users often use special or new words in their queries.", "labels": [], "entities": []}, {"text": "A standard stemmer such as Porter's will wrongly stem them.", "labels": [], "entities": []}, {"text": "To better determine stemming rules, propose a selective stemming method based on corpus analysis.", "labels": [], "entities": [{"text": "stemming", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9565104246139526}]}, {"text": "They refine the Porter stemmer by means of word clustering: words are first clustered according to their co-occurrences in the text collection.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6995927542448044}]}, {"text": "Only word variants belonging to the same cluster will be conflated.", "labels": [], "entities": []}, {"text": "Despite this improvement, the basic idea of word stemming is to transform words in both documents and queries to a standard form.", "labels": [], "entities": [{"text": "word stemming", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7571281790733337}]}, {"text": "Once this is done, there is no means for users to require a specific word form in a query -the word form will be automatically transformed, otherwise, it will not match documents.", "labels": [], "entities": []}, {"text": "This approach does not seem to be appropriate for Web search, where users often specify particular word forms in their queries.", "labels": [], "entities": []}, {"text": "An example of this is a quoted query such as \"Steve Jobs\", or \"US Policy\".", "labels": [], "entities": [{"text": "US Policy\"", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8552387158075968}]}, {"text": "If documents are stemmed, many pages about job offerings or US police maybe returned (\"policy\" conflates with \"police\" in Porter stemmer).", "labels": [], "entities": []}, {"text": "Another drawback of stemming is that it usually enhances recall, but may hurt precision (.", "labels": [], "entities": [{"text": "stemming", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9786970615386963}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9990991353988647}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9992607235908508}]}, {"text": "However, general Web search is basically a precision-oriented task.", "labels": [], "entities": [{"text": "general Web search", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.5923780600229899}, {"text": "precision-oriented", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.978227972984314}]}, {"text": "One alternative approach to word stemming is to do query expansion at query time.", "labels": [], "entities": [{"text": "word stemming", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.7645224630832672}, {"text": "query expansion", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7241639643907547}]}, {"text": "The original query terms are expanded by their related forms having the same root.", "labels": [], "entities": []}, {"text": "All expansions can be combined by the Boolean operator \"OR\".", "labels": [], "entities": [{"text": "OR", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.8328103423118591}]}, {"text": "For example, the query \"controlling acid rain\" can be expanded to \"(control OR controlling OR controller OR controlled OR controls) (acid OR acidic OR acidify) (rain OR raining OR rained OR rains)\".", "labels": [], "entities": []}, {"text": "We will call each such expansion term an alteration to the original query term.", "labels": [], "entities": []}, {"text": "Once a set of possible alterations is determined, the simplest approach to perform expansion is to add all possible alterations.", "labels": [], "entities": []}, {"text": "We call this approach Naive Expansion.", "labels": [], "entities": []}, {"text": "One can easily show that stemming at indexing time is equivalent to Naive Expansion at retrieval time.", "labels": [], "entities": [{"text": "stemming", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.9629213809967041}]}, {"text": "This approach has been adopted by most commercial search engines (.", "labels": [], "entities": []}, {"text": "However, the expansion approaches proposed previously can have several serious problems: First, they usually do not consider expansion ambiguity -each query term is usually expanded independently.", "labels": [], "entities": []}, {"text": "However, some expansion terms may not be appropriate.", "labels": [], "entities": []}, {"text": "The case of \"Steve Jobs\" is one such example, for which the word \"job\" can be proposed as an expansion term.", "labels": [], "entities": []}, {"text": "Second, as each query term may have several alterations, the na\u00efve approach using all the alterations will create a very long query.", "labels": [], "entities": []}, {"text": "As a consequence, query traffic (the time required for the evaluation of a query) is greatly increased.", "labels": [], "entities": []}, {"text": "Query traffic is a critical problem, as each search engine serves millions of users at the same time.", "labels": [], "entities": []}, {"text": "It is important to limit the query traffic as much as possible.", "labels": [], "entities": []}, {"text": "In practice, we can observe that some word alterations are irrelevant and undesirable (as in the \"Steve Jobs\" case), and some other alterations have little impact on the retrieval effectiveness (for example, if we expand a word by a rarely used word form).", "labels": [], "entities": []}, {"text": "In this study, we will address these two problems.", "labels": [], "entities": []}, {"text": "Our goal is to select only appropriate word alterations to be used in query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7802940309047699}]}, {"text": "This is done for two purposes: On the one hand, we want to limit query traffic as much as possible when query expansion is performed.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.7571343779563904}]}, {"text": "On the other hand, we also want to remove irrelevant expansion terms so that fewer irrelevant documents will be retrieved, thereby improve the retrieval effectiveness.", "labels": [], "entities": []}, {"text": "To deal with the two problems we mentioned above, we will propose two methods to select alterations.", "labels": [], "entities": []}, {"text": "In the first method, we make use of the query context to select only the alterations that fit the query.", "labels": [], "entities": []}, {"text": "The query context is modeled by a bigram language model.", "labels": [], "entities": []}, {"text": "To reduce query traffic, we select only one alteration for each query term, which is the most coherent with the bigram model.", "labels": [], "entities": []}, {"text": "We call this model Bigram Expansion.", "labels": [], "entities": []}, {"text": "Despite the fact that this method adds far fewer expansion terms than the na\u00efve expansion, our experiments will show that we can achieve comparable or even better retrieval effectiveness.", "labels": [], "entities": []}, {"text": "Both the Naive Expansion and the Bigram Expansion determine word alterations solely according to general knowledge about the language (bigram model or morphological rules), and no consideration about the possible effect of the expansion term is made.", "labels": [], "entities": []}, {"text": "In practice, some alterations will have virtually no impact on retrieval effectiveness.", "labels": [], "entities": []}, {"text": "Therefore, in our second method, we will try to predict whether an alteration will have some positive impact on retrieval effectiveness.", "labels": [], "entities": []}, {"text": "Only the alterations with positive impact will be retained.", "labels": [], "entities": []}, {"text": "In this paper, we will use a regression model to predict the impact on retrieval effectiveness.", "labels": [], "entities": []}, {"text": "Compared to the bigram expansion method, the regression method results in even fewer alterations, but experiments show that the retrieval effectiveness is even better.", "labels": [], "entities": []}, {"text": "Experiments will be conducted on two TREC collections, Gov2 data for Web Track and TREC6&7&8 for ad-hoc retrieval.", "labels": [], "entities": [{"text": "TREC collections", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.8168370723724365}, {"text": "Gov2 data", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.9326328933238983}]}, {"text": "The results show that the two methods we propose both outperform the original queries significantly with less than two alterations per query on average.", "labels": [], "entities": []}, {"text": "Compared to the Naive Expansion method, the two methods can perform at least equally well, while query traffic is dramatically reduced.", "labels": [], "entities": []}, {"text": "In the following section, we provide a brief review of related work.", "labels": [], "entities": []}, {"text": "Section 3 shows how to generate alteration candidates using a similar approach to.", "labels": [], "entities": []}, {"text": "In section 4 and 5, we describe the Bigram Expansion method and Regression method respectively.", "labels": [], "entities": [{"text": "Regression", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9854898452758789}]}, {"text": "Section 6 presents some experiments on TREC benchmarks to evaluate our methods.", "labels": [], "entities": [{"text": "TREC benchmarks", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.6082228273153305}]}, {"text": "Section 7 concludes this paper and suggests some avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, our aim is to evaluate the two context-sensitive word alteration selection methods.", "labels": [], "entities": [{"text": "word alteration selection", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.7570239007472992}]}, {"text": "The ideal evaluation corpus should be composed of some Web data.", "labels": [], "entities": []}, {"text": "Unfortunately, such data are not publicly available and the results also could not be compared with other published results.", "labels": [], "entities": []}, {"text": "Therefore, we use two TREC collections.", "labels": [], "entities": [{"text": "TREC collections", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.7261205315589905}]}, {"text": "The first one is the ad-hoc retrieval test collections used for TREC6&7& 8.", "labels": [], "entities": [{"text": "TREC6&7& 8", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.7877296447753906}]}, {"text": "This collection is relative small and homogeneous.", "labels": [], "entities": []}, {"text": "The second one is the Gov2 data.", "labels": [], "entities": [{"text": "Gov2 data", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.9537249207496643}]}, {"text": "It is obtained by crawling the entire .gov domain and has been used for three TREC Terabyte tracks (TREC2004-2006).", "labels": [], "entities": [{"text": "TREC Terabyte tracks", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.6915365060170492}]}, {"text": "shows some statistics of the two collections.", "labels": [], "entities": []}, {"text": "For each collection, we use 150 queries.", "labels": [], "entities": []}, {"text": "Since the Regression model needs some data for training, we divided the queries into three parts, each containing 50 queries.", "labels": [], "entities": []}, {"text": "We then use leave-one-out cross-validation.", "labels": [], "entities": []}, {"text": "The evaluation metrics shown below are the average value of the Table1: Overview of Test Collections three-fold cross-validation.", "labels": [], "entities": []}, {"text": "Because the queries in Web are usually very short, we use only the title field of each query.", "labels": [], "entities": []}, {"text": "To correspond to Web search practice, both documents and queries are not stemmed.", "labels": [], "entities": []}, {"text": "We do not filter the stop words either.", "labels": [], "entities": []}, {"text": "Two main metrics are used: the Mean Average Precision (MAP) for the top 1000 documents to measure retrieval effectiveness, and the number of terms in the query to reflect query traffic.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 31, "end_pos": 59, "type": "METRIC", "confidence": 0.9791193008422852}]}, {"text": "In addition, we also provide precision for the top 30 documents (P@30) to show the impact on top ranked documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9993284940719604}]}, {"text": "We also conducted t-tests to determine whether the improvement is statistically significant.", "labels": [], "entities": []}, {"text": "The Indri 2.5 search engine) is used as our basic retrieval system.", "labels": [], "entities": [{"text": "Indri 2.5 search engine", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8858985155820847}]}, {"text": "It provides fora rich query language allowing disjunctive combinations of words in queries.", "labels": [], "entities": []}, {"text": "The first baseline method we compare with only uses the original query, which is named Original.", "labels": [], "entities": []}, {"text": "In addition to this, we also compare with the following methods: Na\u00efve Exp: The Na\u00efve expansion model expands each query term with all terms in the vocabulary sharing the same root with it.", "labels": [], "entities": []}, {"text": "This model is equivalent to the traditional stemming method.", "labels": [], "entities": []}, {"text": "UMASS: This is the result reported in) using Porter stemming for both document and query terms.", "labels": [], "entities": [{"text": "UMASS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8047090172767639}]}, {"text": "This reflects a state-of-the-art result using Porter stemming.", "labels": [], "entities": [{"text": "Porter stemming", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.5931898504495621}]}, {"text": "Similarity: We select the alterations (at most 5) with the highest similarity to the original term.", "labels": [], "entities": []}, {"text": "This is the method described in section 3.", "labels": [], "entities": []}, {"text": "The two methods we propose in this paper are the following ones: Bigram Exp: the alteration is chosen by a Bigram Expansion model.", "labels": [], "entities": []}, {"text": "Regression: the alteration is chosen by a Regression model.", "labels": [], "entities": [{"text": "Regression", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9556987881660461}]}, {"text": "show the results of Gov2 data while table show the results of the TREC6&7&8 collection.", "labels": [], "entities": [{"text": "Gov2 data", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.9593013525009155}, {"text": "TREC6&7&8 collection", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.9109884202480316}]}, {"text": "In the tables, the * mark indicates that the improvement over the original model is statistically significant with p-value<0.05, and ** means the p-values<0.01.", "labels": [], "entities": []}, {"text": "From the tables, we see that both word stemming (UMASS) and expansion with word alterations can improve MAP for all six tasks.", "labels": [], "entities": [{"text": "word stemming", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.731078565120697}, {"text": "MAP", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9692012071609497}]}, {"text": "In most cases (except in table 4 and 6), it also improve the precision of top ranked documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9987155199050903}]}, {"text": "This shows the usefulness of word stemming or word alteration expansion for IR.", "labels": [], "entities": [{"text": "word stemming", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7539709210395813}, {"text": "word alteration expansion", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8061834971110026}, {"text": "IR", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9852425456047058}]}, {"text": "We can make several additional observations: 1).", "labels": [], "entities": []}, {"text": "UMASS uses document and query stemming while Naive Exp uses expansion byword alteration.", "labels": [], "entities": [{"text": "UMASS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9411836266517639}, {"text": "document and query stemming", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.5923037827014923}]}, {"text": "We stated that both approaches are equivalent.", "labels": [], "entities": []}, {"text": "The equivalence is confirmed by our experiment results: for all Gov2 collections, these approaches perform equivalently.", "labels": [], "entities": []}, {"text": "The Similarity model performs very well.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.6358104944229126}]}, {"text": "Compared with the Na\u00efve Expansion model, it produces quite similar retrieval effectiveness, while the query traffic is dramatically reduced.", "labels": [], "entities": []}, {"text": "This approach is similar to the work of, and can be considered as another state-ofthe-art result.", "labels": [], "entities": []}, {"text": "In comparison, the Bigram Expansion model performs better than the Similarity model.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.7781307101249695}]}, {"text": "This shows that it is useful to consider query context in selecting word alterations.", "labels": [], "entities": []}, {"text": "The Regression model performs the best of all the models.", "labels": [], "entities": [{"text": "Regression", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9865483641624451}]}, {"text": "Compared with the Original query, it adds fewer than 2 alterations for each query on average (since each group has 50 queries); nevertheless we obtained improvements on all the six collections.", "labels": [], "entities": []}, {"text": "Moreover, the improvements on five collections are statistically significant.", "labels": [], "entities": []}, {"text": "It also performs slightly better than the Similarity and Bigram Expansion methods, but with fewer alterations.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.7471876740455627}]}, {"text": "This shows that the supervised learning approach, if used in the correct way, is superior to an unsupervised approach.", "labels": [], "entities": []}, {"text": "Another advantage over the two other models is that the Regression model can reduce the number of alterations further.", "labels": [], "entities": [{"text": "Regression", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9889078736305237}]}, {"text": "Because the Regression model selects alterations according to their expected improvement, the improvement of the alterations to one query term can be compared with that of the alterations to other query terms.", "labels": [], "entities": []}, {"text": "Therefore, we can select at most one optimal alteration for the whole query.", "labels": [], "entities": []}, {"text": "However, with the Similarity or Bigram Expansion models, the selection value, either similarity or query likelihood, cannot be compared across the query terms.", "labels": [], "entities": [{"text": "similarity", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9757542610168457}]}, {"text": "As a consequence, more alterations need to be selected, leading to heavier query traffic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of Query 701-750 Over Gov2 Data", "labels": [], "entities": [{"text": "Gov2 Data", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9473932087421417}]}, {"text": " Table 3: Results of Query 751-800 over Gov2 Data", "labels": [], "entities": [{"text": "Gov2", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.9243554472923279}]}, {"text": " Table 4: Results of Query 801-850 over Gov2 Data", "labels": [], "entities": []}, {"text": " Table 5: Results of Query 301-350 over TREC6&7&8", "labels": [], "entities": [{"text": "TREC6&7", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.8377813895543417}]}, {"text": " Table 6: Results of Query 351-400 over TREC6&7&8", "labels": [], "entities": [{"text": "TREC6", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.6950021386146545}]}]}