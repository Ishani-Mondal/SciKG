{"title": [{"text": "Language Dynamics and Capitalization using Maximum Entropy", "labels": [], "entities": [{"text": "Language Dynamics", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6752754896879196}]}], "abstractContent": [{"text": "This paper studies the impact of written language variations and the way it affects the capitalization task overtime.", "labels": [], "entities": []}, {"text": "A discriminative approach, based on maximum entropy models , is proposed to perform capitalization, taking the language changes into consideration.", "labels": [], "entities": []}, {"text": "The proposed method makes it possible to use large corpora for training.", "labels": [], "entities": []}, {"text": "The evaluation is performed over newspaper corpora using different testing periods.", "labels": [], "entities": []}, {"text": "The achieved results reveal a strong relation between the capitalization performance and the elapsed time between the training and testing data periods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The capitalization task, also known as truecasing (, consists of rewriting each word of an input text with its proper case information.", "labels": [], "entities": []}, {"text": "The capitalization of a word sometimes depends on its current context, and the intelligibility of texts is strongly influenced by this information.", "labels": [], "entities": []}, {"text": "Different practical applications benefit from automatic capitalization as a preprocessing step: when applied to speech recognition output, which usually consists of raw text, automatic capitalization provides relevant information for automatic content extraction, named entity recognition, and machine translation; many computer applications, such as word processing and e-mail clients, perform automatic capitalization along with spell corrections and grammar check.", "labels": [], "entities": [{"text": "automatic content extraction", "start_pos": 234, "end_pos": 262, "type": "TASK", "confidence": 0.7767951885859171}, {"text": "named entity recognition", "start_pos": 264, "end_pos": 288, "type": "TASK", "confidence": 0.6169037918249766}, {"text": "machine translation", "start_pos": 294, "end_pos": 313, "type": "TASK", "confidence": 0.7949622869491577}, {"text": "spell corrections", "start_pos": 431, "end_pos": 448, "type": "TASK", "confidence": 0.7012330740690231}]}, {"text": "The capitalization problem can be seen as a sequence tagging problem (), where each lower-case word is associated to a tag that describes its capitalization form.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6642428934574127}]}, {"text": "() study the impact of using increasing amounts of training data as well as a small amount of adaptation.", "labels": [], "entities": []}, {"text": "This work uses a Maximum Entropy Markov Model (MEMM) based approach, which allows to combine different features.", "labels": [], "entities": []}, {"text": "A large written newspaper corpora is used for training and the test data consists of Broadcast News (BN) data.", "labels": [], "entities": [{"text": "Broadcast News (BN) data", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.8469875653584799}]}, {"text": "( builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with case information, and then uses dynamic programming to disambiguate overall possible tag assignments on a sentence.", "labels": [], "entities": []}, {"text": "Other related work includes a bilingual capitalization model for capitalizing machine translation (MT) outputs, using conditional random fields (CRFs) reported by ().", "labels": [], "entities": [{"text": "capitalizing machine translation (MT) outputs", "start_pos": 65, "end_pos": 110, "type": "TASK", "confidence": 0.8722377930368695}]}, {"text": "This work exploits case information both from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9499146342277527}]}, {"text": "A preparatory study on the capitalization of Portuguese BN has been performed by (.", "labels": [], "entities": [{"text": "capitalization of Portuguese BN", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.5760410651564598}]}, {"text": "One important aspect related with capitalization concerns the language dynamics: new words are introduced everyday in our vocabularies and the usage of some other words decays with time.", "labels": [], "entities": []}, {"text": "Concerning this subject, shows that, as the time gap between training and test data increases, the performance of a named tagger based on co-training decreases.", "labels": [], "entities": []}, {"text": "This paper studies and evaluates the effects of language dynamics in the capitalization of newspaper corpora.", "labels": [], "entities": []}, {"text": "Section 2 describes the corpus and presents a short analysis on the lexicon variation.", "labels": [], "entities": []}, {"text": "Section 3 presents experiments concerning the capitalization task, either using isolated training sets or by retraining with different training sets.", "labels": [], "entities": [{"text": "capitalization task", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9091188311576843}]}, {"text": "Section 4 concludes and presents future plans.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Using 8 subsets of each year for training.", "labels": [], "entities": []}, {"text": " Table 3: Retraining from Jan. 1999 to Sep. 2004.", "labels": [], "entities": [{"text": "Retraining", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8376715779304504}]}]}