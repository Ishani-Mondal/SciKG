{"title": [], "abstractContent": [{"text": "This paper proposes a method to correct En-glish verb form errors made by non-native speakers.", "labels": [], "entities": []}, {"text": "A basic approach is template matching on parse trees.", "labels": [], "entities": [{"text": "template matching", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7004702985286713}]}, {"text": "The proposed method improves on this approach in two ways.", "labels": [], "entities": []}, {"text": "To improve recall, irregularities in parse trees caused by verb form errors are taken into account ; to improve precision, n-gram counts are utilized to filter proposed corrections.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9969178438186646}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9992364645004272}]}, {"text": "Evaluation on non-native corpora, representing two genres and mother tongues, shows promising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to describe the nuances of an action, a verb maybe associated with various concepts such as tense, aspect, voice, mood, person and number.", "labels": [], "entities": []}, {"text": "In some languages, such as Chinese, the verb itself is not inflected, and these concepts are expressed via other words in the sentence.", "labels": [], "entities": []}, {"text": "In highly inflected languages, such as Turkish, many of these concepts are encoded in the inflection of the verb.", "labels": [], "entities": []}, {"text": "In between these extremes, English uses a combination of inflections (see) and \"helping words\", or auxiliaries, to form complex verb phrases.", "labels": [], "entities": []}, {"text": "It should come as no surprise, then, that the misuse of verb forms is a common error category for some non-native speakers of English.", "labels": [], "entities": []}, {"text": "For example, in the Japanese Learners of English corpus (, errors related to verbs are among the most frequent categories.", "labels": [], "entities": [{"text": "Japanese Learners of English corpus", "start_pos": 20, "end_pos": 55, "type": "DATASET", "confidence": 0.774729061126709}]}, {"text": "shows some sentences with these errors.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two corpora were used for evaluation.", "labels": [], "entities": []}, {"text": "They were selected to represent two different genres, and two different mother tongues.", "labels": [], "entities": []}, {"text": "HKUST This corpus 6 of short essays was collected from students, all native Chinese speakers, at the Hong Kong University of Science and Technology.", "labels": [], "entities": [{"text": "HKUST This corpus 6 of short essays", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.9524983252797808}]}, {"text": "It contains a total of 2556 sentences.", "labels": [], "entities": []}, {"text": "They tend to be longer and have more complex structures than their counterparts in the JLE.", "labels": [], "entities": [{"text": "JLE", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.7008289098739624}]}, {"text": "Corrections are not provided; however, part-of-speech tags are given for the original words, and for the intended (but unwritten) corrections.", "labels": [], "entities": [{"text": "Corrections", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9717581272125244}]}, {"text": "Implications on our evaluation procedure are discussed in \u00a75.4.", "labels": [], "entities": []}, {"text": "For each verb in the input sentence, a change in verb form maybe hypothesized.", "labels": [], "entities": []}, {"text": "There are five possible outcomes for this hypothesis, as enumerated in Table 4.", "labels": [], "entities": []}, {"text": "To penalize \"false alarms\", a strict definition is used for false positives -even when the hypothesized correction yields a good sentence, it is still considered a false positive so long as the original sentence is acceptable.", "labels": [], "entities": []}, {"text": "It can sometimes be difficult to determine which words should be considered verbs, as they are not clearly demarcated in our evaluation corpora.", "labels": [], "entities": []}, {"text": "We will thus apply the outcomes in at the sentence level; that is, the output sentence is considered a true positive only if the original sentence contains errors, and only if valid corrections are offered for all errors.", "labels": [], "entities": []}, {"text": "The following statistics are computed: Accuracy The proportion of sentences which, after being treated by the system, have correct verb forms.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9991092085838318}]}, {"text": "That is, (true neg + true pos) divided by the total number of sentences.", "labels": [], "entities": []}, {"text": "Recall Out of all sentences with verb form errors, the percentage whose errors have been successfully corrected by the system.", "labels": [], "entities": []}, {"text": "That is, true pos divided by (true pos + false neg + inv pos).", "labels": [], "entities": []}, {"text": "Detection Precision This is the first of two types of precision to be reported, and is defined as follows: Out of all sentences for which the system has hypothesized corrections, the percentage that actually contain errors, without regard to the validity of the corrections.", "labels": [], "entities": [{"text": "Detection Precision This", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.6880419254302979}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9979667663574219}]}, {"text": "That is, (true pos + inv pos) divided by (true pos + inv pos + false pos).", "labels": [], "entities": []}, {"text": "Correction Precision This is the more stringent type of precision.", "labels": [], "entities": [{"text": "Correction Precision", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.7426690757274628}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9993457198143005}]}, {"text": "In addition to successfully determining that a correction is needed, the system must offer a valid correction.", "labels": [], "entities": []}, {"text": "Formally, it is true pos divided by (true pos + false pos + inv pos).", "labels": [], "entities": []}, {"text": "For the JLE corpus, all figures above will be reported.", "labels": [], "entities": [{"text": "JLE corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8897610604763031}]}, {"text": "The HKUST corpus, however, will not be evaluated on subject-verb agreement, since a sizable number of these errors are induced by other changes in the sentence . Furthermore, the HKUST corpus will require manual evaluation, since the corrections are not annotated.", "labels": [], "entities": [{"text": "HKUST corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9748515784740448}, {"text": "HKUST corpus", "start_pos": 179, "end_pos": 191, "type": "DATASET", "confidence": 0.9653733968734741}]}, {"text": "Two native speakers of English were given the edited sentences, as well as the original input.", "labels": [], "entities": []}, {"text": "For each pair, they were asked to select one of four statements: one of the two is better, or both are equally correct, or both are equally incorrect.", "labels": [], "entities": []}, {"text": "The right column shows the resulting trees when the correct verb form crr is replaced by err.", "labels": [], "entities": [{"text": "err", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9801132678985596}]}, {"text": "Detailed comments are provided in \u00a76.1.", "labels": [], "entities": []}, {"text": "correction precision is thus the proportion of pairs where the edited sentence is deemed better.", "labels": [], "entities": [{"text": "correction", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9728102684020996}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.5008390545845032}]}, {"text": "Accuracy and recall cannot be computed, since it was impossible to distinguish syntactic errors from semantic ones (see \u00a72).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9923648834228516}, {"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9984089732170105}]}, {"text": "Corresponding to the issues discussed in \u00a73.2 and \u00a73.3, our experiment consists of two main steps.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 8: Results on the JLE and HKUST corpora for auxiliary agreement and complementation. The majority baseline  accuracy is 98.47% for JLE. The verb-only baseline accuracy is 98.85%, as indicated on the second row. \"All\" denotes  the complete proposed method. See  \u00a76.4 for detailed comments.", "labels": [], "entities": [{"text": "HKUST corpora", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8947654664516449}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9674263596534729}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.8158344626426697}]}]}