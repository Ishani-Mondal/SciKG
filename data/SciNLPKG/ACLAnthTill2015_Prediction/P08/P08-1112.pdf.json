{"title": [{"text": "Better Alignments = Better Translations?", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic word alignment is a key step in training statistical machine translation systems.", "labels": [], "entities": [{"text": "Automatic word alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6096345881621043}, {"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.5834845205148061}]}, {"text": "Despite much recent work on word alignment methods, alignment accuracy increases often produce little or no improvements in machine translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8195222914218903}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9385936260223389}, {"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7342649400234222}]}, {"text": "In this work we analyze a recently proposed agreement-constrained EM algorithm for un-supervised alignment models.", "labels": [], "entities": []}, {"text": "We attempt to tease apart the effects that this simple but effective modification has on alignment precision and recall trade-offs, and how rare and common words are affected across several language pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9602925777435303}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9702094197273254}]}, {"text": "We propose and extensively evaluate a simple method for using alignment models to produce alignments better-suited for phrase-based MT systems, and show significant gains (as measured by BLEU score) in end-to-end translation systems for six languages pairs used in recent MT competitions.", "labels": [], "entities": [{"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.8526064157485962}, {"text": "BLEU score", "start_pos": 187, "end_pos": 197, "type": "METRIC", "confidence": 0.9799979031085968}, {"text": "MT competitions", "start_pos": 272, "end_pos": 287, "type": "TASK", "confidence": 0.9338153600692749}]}], "introductionContent": [{"text": "The typical pipeline fora machine translation (MT) system starts with a parallel sentence-aligned corpus and proceeds to align the words in every sentence pair.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8802946925163269}]}, {"text": "The word alignment problem has received much recent attention, but improvements in standard measures of word alignment performance often do not result in better translations.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8416538536548615}, {"text": "word alignment", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7544448971748352}]}, {"text": "note that none of the tens of papers published over the last five years has shown that significant decreases in alignment error rate (AER) result in significant increases in translation performance.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 112, "end_pos": 138, "type": "METRIC", "confidence": 0.903663287560145}, {"text": "translation", "start_pos": 174, "end_pos": 185, "type": "TASK", "confidence": 0.9610665440559387}]}, {"text": "In this work, we show that by changing the way the word alignment models are trained and used, we can get not only improvements in alignment performance, but also in the performance of the MT system that uses those alignments.", "labels": [], "entities": [{"text": "MT", "start_pos": 189, "end_pos": 191, "type": "TASK", "confidence": 0.9607712626457214}]}, {"text": "We present extensive experimental results evaluating anew training scheme for unsupervised word alignment models: an extension of the Expectation Maximization algorithm that allows effective injection of additional information about the desired alignments into the unsupervised training process.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.7161264717578888}, {"text": "Expectation Maximization", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.758269339799881}]}, {"text": "Examples of such information include \"one word should not translate to many words\" or that directional translation models should agree.", "labels": [], "entities": []}, {"text": "The general framework for the extended EM algorithm with posterior constraints of this type was proposed by.", "labels": [], "entities": []}, {"text": "Our contribution is a large scale evaluation of this methodology for word alignments, an investigation of how the produced alignments differ and how they can be used to consistently improve machine translation performance (as measured by BLEU score) across many languages on training corpora with up to hundred thousand sentences.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.749458909034729}, {"text": "machine translation", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.6317232251167297}, {"text": "BLEU score)", "start_pos": 238, "end_pos": 249, "type": "METRIC", "confidence": 0.9723281462987264}]}, {"text": "In 10 out of 12 cases we improve BLEU score by at least 1 4 point and by more than 1 point in 4 out of 12 cases.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9601869285106659}]}, {"text": "After presenting the models and the algorithm in Sections 2 and 3, in Section 4 we examine how the new alignments differ from standard models, and find that the new method consistently improves word alignment performance, measured either as alignment error rate or weighted F-score.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 194, "end_pos": 208, "type": "TASK", "confidence": 0.7467855513095856}, {"text": "alignment error rate", "start_pos": 241, "end_pos": 261, "type": "METRIC", "confidence": 0.8495221535364786}, {"text": "F-score", "start_pos": 274, "end_pos": 281, "type": "METRIC", "confidence": 0.9004260897636414}]}, {"text": "Section 5 explores how the new alignments lead to consistent and significant improvement in a state of the art phrase base machine translation by using posterior decoding rather than Viterbi decoding.", "labels": [], "entities": [{"text": "phrase base machine translation", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.6176137551665306}]}, {"text": "We propose a heuristic for tuning posterior decoding in the absence of annotated alignment data and show improvements over baseline systems for six different language pairs used in recent MT competitions.", "labels": [], "entities": [{"text": "MT competitions", "start_pos": 188, "end_pos": 203, "type": "TASK", "confidence": 0.9406376481056213}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the corpora used in MT evaluation.  The training size is measured in thousands of sentences  and Len refers to average (English) sentence length. Test  is the number of sentences in the test set. Rare and Unk  are the percentage of tokens in the test set that are rare  and unknown in the training data, for each language.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9611326158046722}, {"text": "Len", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9649925827980042}]}, {"text": " Table 2: BLEU scores for all language pairs using up to  100k sentences. Results are after MERT optimization.  The marks (++) and (+) denote that agreement with poste- rior decoding is better by 1 BLEU point and 0.25 BLEU  points respectively than the best baseline HMM model;  analogously for (\u2212\u2212) , (\u2212) ; while (\u2248) denotes smaller dif- ferences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9980791807174683}, {"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9978517293930054}, {"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9676592350006104}]}, {"text": " Table 3: BLEU scores for all language pairs using all  available data. Markings as in Table 2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983067512512207}]}]}