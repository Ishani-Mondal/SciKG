{"title": [{"text": "Further Meta-Evaluation of Broad-Coverage Surface Realization", "labels": [], "entities": [{"text": "Broad-Coverage Surface Realization", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.5667148331801096}]}], "abstractContent": [{"text": "We present the first evaluation of the utility of automatic evaluation metrics on surface real-izations of Penn Treebank data.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.9931329687436422}]}, {"text": "Using outputs of the OpenCCG and XLE realizers, along with ranked WordNet synonym substitutions, we collected a corpus of generated surface re-alizations.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9515742063522339}, {"text": "WordNet synonym substitutions", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.7620964447657267}]}, {"text": "These outputs were then rated and post-edited by human annotators.", "labels": [], "entities": []}, {"text": "We evaluated the realizations using seven automatic metrics, and analyzed correlations obtained between the human judgments and the automatic scores.", "labels": [], "entities": []}, {"text": "In contrast to previous NLG meta-evaluations, we find that several of the metrics correlate moderately well with human judgments of both adequacy and fluency, with the TER family performing best overall.", "labels": [], "entities": [{"text": "TER family", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9270619451999664}]}, {"text": "We also find that all of the metrics correctly predict more than half of the significant system-level differences, though none are correct in all cases.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of the implications for the utility of such metrics in evaluating generation in the presence of variation.", "labels": [], "entities": []}, {"text": "A further result of our research is a corpus of post-edited realizations, which will be made available to the research community.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "From the data sources described in the previous section, a corpus of realizations to be evaluated by the human judges was constructed by randomly choosing 305 sentences from section 00, then selecting surface realizations of these sentences using the following algorithm: 1.", "labels": [], "entities": []}, {"text": "Add OpenCCG's best-scored realization.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8920074701309204}]}, {"text": "2. Add other OpenCCG realizations until all four models are represented, to a maximum of 4. 3. Add up to 4 realizations from either the XLE system or the WordNet pool, chosen randomly.", "labels": [], "entities": [{"text": "WordNet pool", "start_pos": 154, "end_pos": 166, "type": "DATASET", "confidence": 0.9516521394252777}]}, {"text": "The intent was to give reasonable coverage of all realizer systems discussed in Section 2 without overloading the human judges.", "labels": [], "entities": []}, {"text": "\"System\" here means any instantiation that emits surface realizations, including various configurations of OpenCCG (using different language models or ranking systems), and these can be multiple-output, such as an n-best list, or single-output (best-only, worst-only, etc.).", "labels": [], "entities": []}, {"text": "Accordingly, more realizations were selected from the OpenCCG realizer because 5 different systems were being represented.", "labels": [], "entities": [{"text": "OpenCCG realizer", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9672196805477142}]}, {"text": "Realizations were chosen randomly, rather than according to sentence types or other criteria, in order to produce a representative sample of the corpus.", "labels": [], "entities": []}, {"text": "In total, 2,114 realizations were selected for evaluation.", "labels": [], "entities": []}, {"text": "The realizations were also evaluated using seven automatic metrics: \u2022 IBM's BLEU, which scores a hypothesis by counting n-gram matches with the reference sentence (), with smoothing as described in ( \u2022 The NIST n-gram evaluation metric, similar to BLEU, but rewarding rarer n-gram matches, and using a different length penalty \u2022 METEOR, which measures the harmonic mean of unigram precision and recall, with a higher weight for recall ( \u2022 TER (Translation Edit Rate), a measure of the number of edits required to transform a hypothesis sentence into the reference sentence () \u2022 TERP, an augmented version of TER which performs phrasal substitutions, stemming, and checks for synonyms, among other improvements ( \u2022 TERPA, an instantiation of TERP with edit weights optimized for correlation with adequacy in MT evaluations \u2022 GTM (General Text Matcher), a generalization of the F-measure that rewards contiguous matching spans ( Additionally, targeted versions of BLEU, ME-TEOR, TER, and GTM were computed by using the human-repaired outputs as the reference set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9925887584686279}, {"text": "BLEU", "start_pos": 248, "end_pos": 252, "type": "METRIC", "confidence": 0.9759096503257751}, {"text": "recall", "start_pos": 395, "end_pos": 401, "type": "METRIC", "confidence": 0.9572082161903381}, {"text": "recall", "start_pos": 428, "end_pos": 434, "type": "METRIC", "confidence": 0.9931303858757019}, {"text": "TER (Translation Edit Rate)", "start_pos": 439, "end_pos": 466, "type": "METRIC", "confidence": 0.6489441593488058}, {"text": "MT evaluations", "start_pos": 807, "end_pos": 821, "type": "TASK", "confidence": 0.8891729116439819}, {"text": "BLEU", "start_pos": 962, "end_pos": 966, "type": "METRIC", "confidence": 0.9570131897926331}]}, {"text": "The human repair was different from the reference sentence in 193 cases (about 9% of the total), and we expected this to result in better scores and correlations with the human judgments overall.", "labels": [], "entities": []}, {"text": "summarizes the dataset, as well as the mean adequacy and fluency scores garnered from the human evaluation.", "labels": [], "entities": []}, {"text": "Overall adequacy and fluency judgments were high (4.16, 3.63) for the realizer systems on average, and the best-rated realizer systems achieved mean fluency scores above 4.", "labels": [], "entities": []}, {"text": "To determine how well the automatic evaluation methods described in Section 3 correlate with the human judgments, we averaged the human judgments for adequacy and fluency, respectively, for each of the rated realizations, and then computed both Pearson's correlation coefficient and Spearman's rank correlation coefficient between these scores and each of the metrics.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 245, "end_pos": 278, "type": "METRIC", "confidence": 0.9249492734670639}, {"text": "Spearman's rank correlation coefficient", "start_pos": 283, "end_pos": 322, "type": "METRIC", "confidence": 0.6570999622344971}]}, {"text": "Spearman's correlation makes fewer assumptions about the distribution of the data, but may not reflect a linear relationship that is actually present.", "labels": [], "entities": []}, {"text": "Both are frequently reported in the literature.", "labels": [], "entities": []}, {"text": "Due to space constraints, we show only Spearman's correlation, although the TER family scored slightly better on Pearson's coefficient, relatively.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 39, "end_pos": 61, "type": "METRIC", "confidence": 0.8524231314659119}, {"text": "TER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9759460687637329}, {"text": "Pearson's coefficient", "start_pos": 113, "end_pos": 134, "type": "METRIC", "confidence": 0.9132836659749349}]}, {"text": "The results for Spearman's correlation are given in.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 16, "end_pos": 38, "type": "METRIC", "confidence": 0.6563933193683624}]}, {"text": "Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following, and the Spearman's correlation of each of the automatic metrics with these scores are given in.", "labels": [], "entities": []}, {"text": "All reported correlations are significant at p < 0.001.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Corpora-wise inter-annotator agreement (absolute and relative \u03ba values shown)", "labels": [], "entities": []}, {"text": " Table 3: Spearman's correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),  TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (-Adq: adequacy and -Flu: Fluency); Scores  which fall within the 95 %CI of the best are italicized.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9976425766944885}, {"text": "METEOR", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9412555694580078}, {"text": "TERp", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9842968583106995}, {"text": "TERpa", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9460873007774353}, {"text": "TER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.7607735991477966}]}, {"text": " Table 4: Spearman's correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),  TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (combined adequacy and fluency scores)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9983741044998169}, {"text": "METEOR", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9442097544670105}, {"text": "TERp (TP)", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9246439039707184}, {"text": "TERpa (TA)", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9146824926137924}]}, {"text": " Table 5: Spearman's correlation analysis (bootstrap sampling) of the BLEU scores of various systems with human  adequacy and fluency scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9963043928146362}]}, {"text": " Table 6: Spearman's correlations of NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA), human  variants (HT, HN, HB, HM, HG), and individual human judgments (combined adq. and flu. scores)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9986055493354797}, {"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9549263715744019}, {"text": "TERp (TP)", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9292286485433578}, {"text": "TERpa (TA)", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9199630916118622}]}, {"text": " Table 7: Factors influencing TERP ranking errors for 50 worst-ranked realization groups", "labels": [], "entities": [{"text": "TERP ranking errors", "start_pos": 30, "end_pos": 49, "type": "METRIC", "confidence": 0.8651513655980428}]}, {"text": " Table 8: Metric-wise ranking performance in terms of agreement with a ranking induced by combined adequacy and  fluency scores; each metric gets a score out of 5 (i.e. number of system-level comparisons that emerged significant as  per the Tukey's HSD test)  Legend: Perceptron Best (PB); Perceptron Worst (PW); XLE Best (XB); XLE Worst (XW); OpenCCG baseline mod- els 1 to 3 (C1 ... C3)", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 241, "end_pos": 257, "type": "DATASET", "confidence": 0.6169512569904327}, {"text": "OpenCCG baseline mod- els 1", "start_pos": 344, "end_pos": 371, "type": "METRIC", "confidence": 0.7448775370915731}]}]}