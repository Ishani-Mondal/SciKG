{"title": [], "abstractContent": [{"text": "The task of selecting information and rendering it appropriately appears in multiple contexts in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 97, "end_pos": 110, "type": "TASK", "confidence": 0.9687604308128357}]}, {"text": "In this paper we present a model that simultaneously optimizes selection and rendering preferences.", "labels": [], "entities": []}, {"text": "The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs.", "labels": [], "entities": [{"text": "PCFG parse trees", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.7828092177708944}]}, {"text": "Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions.", "labels": [], "entities": []}, {"text": "Based on an integer linear programming formulation , the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met.", "labels": [], "entities": []}, {"text": "Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8229274650414785}]}], "introductionContent": [{"text": "Summarization is the process of condensing a source text into a shorter version while preserving its information content.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9826596975326538}]}, {"text": "Humans summarize on a daily basis and effortlessly, yet the automatic production of high-quality summaries remains a challenge.", "labels": [], "entities": []}, {"text": "Most work today focuses on extractive summarization, where a summary is created by identifying and subsequently concatenating the most important sentences in a document.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6892309486865997}]}, {"text": "The advantage of this approach is that it does not require a great deal of linguistic analysis to generate grammatical sentences, assuming the source document was well written.", "labels": [], "entities": []}, {"text": "Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information.", "labels": [], "entities": []}, {"text": "The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input).", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7736266553401947}, {"text": "sentence compression", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7238043546676636}]}, {"text": "An alternative abstractive or \"bottom-up\" approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (.", "labels": [], "entities": []}, {"text": "This approach has the potential to work well, breaking out of the single-sentence paradigm.", "labels": [], "entities": []}, {"text": "Unfortunately, the resulting summaries are not always coherent -individual constituent phrases are often combined without any semantic constraints -or grammatical beyond the n-gram horizon imposed by the language model.", "labels": [], "entities": []}, {"text": "Constituent deletion and recombination are merely two of the many rewrite operations professional editors and abstractors employ when creating summaries).", "labels": [], "entities": [{"text": "Constituent deletion", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8155558705329895}]}, {"text": "Additional operations include truncating sentences, aggregating them, and paraphrasing at word or syntax level.", "labels": [], "entities": []}, {"text": "Furthermore, professionals write summaries in a task-specific style.", "labels": [], "entities": []}, {"text": "News headlines for example are typically short (three to six words), written in the present tense and active voice, and often leave out forms of the verb be.", "labels": [], "entities": []}, {"text": "There are also different ways of writing a headline either directly by stating what the docu-ment is about or indirectly by raising a question in the reader's mind, which the document answers.", "labels": [], "entities": []}, {"text": "The automatic generation of summaries similar to those produced by human abstractors is challenging because of the many constraints imposed by the task: the summary must be maximally informative and minimally redundant, grammatical, coherent, adhere to a pre-specified length and stylistic conventions.", "labels": [], "entities": [{"text": "automatic generation of summaries", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.5811731442809105}]}, {"text": "Importantly, these constraints are conflicting; the deletion of certain phrases may avoid redundancy but result in ungrammatical output and information loss.", "labels": [], "entities": []}, {"text": "In this paper we propose a model for summarization that attempts to capture and optimize these constraints jointly.", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.991795003414154}]}, {"text": "We learn both how to select the most important information (the content), and how to render it appropriately (the style).", "labels": [], "entities": []}, {"text": "Selection preferences are learned discriminatively, while a quasisynchronous grammar) captures rendering preferences such as paraphrases and compressions.", "labels": [], "entities": []}, {"text": "The entire solution space of possible extractions and QG-generated paraphrases is searched efficiently through use of integer linear programming.", "labels": [], "entities": []}, {"text": "The ILP framework allows us to model naturally as constraints, additional requirements such as sentence length, overall summary length, topic coverage and, importantly, grammaticality.", "labels": [], "entities": []}, {"text": "We argue that QG is attractive for describing rewrite operations common in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9787554740905762}]}, {"text": "Rather than assuming a strictly synchronous structure over the source and target sentences, QG identifies a \"sloppy\" alignment of parse trees assuming that the target tree is in someway \"inspired by\" the source tree.", "labels": [], "entities": []}, {"text": "A key insight in our approach is to formulate the summarization problem at the phrase level: both QG rules and information extraction operate over individual phrases rather than (as is the norm) sentences.", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9757437109947205}, {"text": "information extraction", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.7200725376605988}]}, {"text": "At this smaller unit level, QG rules become more widely applicable and compression falls naturally because only phrases deemed important should appear in the summary.", "labels": [], "entities": []}, {"text": "We evaluate the proposed model on headline generation and the related task of image caption generation.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.892650306224823}, {"text": "image caption generation", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8305205901463827}]}, {"text": "However, there is nothing inherent in our formulation that is specific to those two tasks; it is possible for the model to generate longer or shorter summaries, fora single or multiple documents.", "labels": [], "entities": []}, {"text": "Experimental results show that our method obtains state-of-the-art performance, both in terms of grammaticality and informativeness for both tasks using the same summarization model.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned earlier we evaluated the performance of our model on two title generation tasks, namely headline and caption generation.", "labels": [], "entities": [{"text": "title generation", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6817484200000763}, {"text": "caption generation", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7375503778457642}]}, {"text": "In this section we give details on the corpora and grammars we used, model parameters and features.", "labels": [], "entities": []}, {"text": "We also describe the baselines used for comparison with our approach, and explain how system output was evaluated.", "labels": [], "entities": []}, {"text": "Training We obtained phrase-based salience scores using a supervised machine learning algorithm.", "labels": [], "entities": []}, {"text": "For the headline generation task, the full DUC-03 (Task 1) corpus was used for training; it contains 500 documents and 4 headline-style summaries per document.", "labels": [], "entities": [{"text": "headline generation task", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.9156161348025004}, {"text": "DUC-03 (Task 1) corpus", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.8804153005282084}]}, {"text": "For the captions, training data was gathered from the CNN news website.", "labels": [], "entities": [{"text": "CNN news website", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9812723596890768}]}, {"text": "We used 200 documents and their corresponding captions.", "labels": [], "entities": []}, {"text": "Sentences were first tokenized to separate words and punctuation, and then parsed to obtain phrases and dependencies as described in Section 3 using the Stanford parser (.", "labels": [], "entities": []}, {"text": "Document phrases were marked as positive or negative automatically.", "labels": [], "entities": []}, {"text": "If there was a unigram overlap (excluding stop words) between the phrase and any of the original title or caption, we marked this phrase with a positive label.", "labels": [], "entities": []}, {"text": "Non-overlapping phrases were given negative labels.", "labels": [], "entities": []}, {"text": "Our feature set comprised surface features such as sentence and paragraph position information, POS tags, and whether high-scoring tf.idf words were present in the phrase.", "labels": [], "entities": []}, {"text": "Additionally, the caption training set contained features for unigram and bigram overlap with the title.", "labels": [], "entities": []}, {"text": "We learned the feature weights with a linear SVM, using the software SVM-OOPS.", "labels": [], "entities": []}, {"text": "This tool gave us directly the feature weights as well as support vector values, and it allowed different penalties to be applied to positive and negative misclassifications, enabling us to compensate for the unbalanced data set.", "labels": [], "entities": []}, {"text": "The penalty hyper-parameters chosen were the ones that gave the best F-scores, using 10-fold validation.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9956915974617004}]}, {"text": "For each of the two tasks, QG rules were extracted from the same data used to train the SVM, resulting in 2,910 distinct rules for headlines and 2,757 rules for the captions.", "labels": [], "entities": []}, {"text": "shows that for both tasks, the majority of rules apply to PP and NP phrases.", "labels": [], "entities": []}, {"text": "Both tasks involve considerable compression, but the proportions of the rewrite operations involved indicate differences in style between them.", "labels": [], "entities": []}, {"text": "Compared Label Prop'n Proportion for Label of set Unmod: QG rules generated for (a) headline and (b) caption tasks (top 4 labels shown).", "labels": [], "entities": []}, {"text": "The columns show label of root node, proportion of the full ruleset, then the proportions of rules for this label involving no modification, deletions, insertions and re-orderings.", "labels": [], "entities": []}, {"text": "to headlines, captions involve slightly less deletion and a higher proportion of the phrases are unmodified.", "labels": [], "entities": []}, {"text": "The QG learning mechanism also discovers more alignments between source sentences and captions than it does for the headline task.", "labels": [], "entities": []}, {"text": "Title generation For the headline generation task, we evaluated our model on a testing partition from the DUC-04 corpus (75 documents, Task 1).", "labels": [], "entities": [{"text": "headline generation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.841957688331604}, {"text": "DUC-04 corpus", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.9815253019332886}]}, {"text": "For the caption task, we used the test set (240 documents) described in.", "labels": [], "entities": [{"text": "caption task", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.9213648438453674}]}, {"text": "Their corpus was downloaded from the BBC news site and contains documents, images, and their captions.", "labels": [], "entities": [{"text": "BBC news site", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9818235039710999}]}, {"text": "We created and solved an ILP for each document.", "labels": [], "entities": []}, {"text": "For each phrase, features were extracted and salience scores calculated from the feature weights determined through SVM training.", "labels": [], "entities": []}, {"text": "The distance from the SVM hyperplane represents the salience score.", "labels": [], "entities": [{"text": "SVM hyperplane", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8967304527759552}]}, {"text": "Parameters for the ILP models for the two tasks are shown in.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9752541780471802}]}, {"text": "The \u03bb parameter was set to 0.2 to ensure that paraphrases were included; other parameters were chosen to capture the prop-  erties seen in the majority of the training set.", "labels": [], "entities": [{"text": "prop-  erties", "start_pos": 117, "end_pos": 130, "type": "METRIC", "confidence": 0.8050513863563538}]}, {"text": "Note the maximum number of sentences allowed to form a headline is set to 5 as some of the headlines in the DUC dataset contained multiple sentences.", "labels": [], "entities": [{"text": "DUC dataset", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.967903196811676}]}, {"text": "To solve the ILP model we used the ZIB Optimization Suite software).", "labels": [], "entities": []}, {"text": "The solution was converted into a sentence by removing nodes not chosen from the tree representation, then concatenating the remaining leaf nodes in order.", "labels": [], "entities": []}, {"text": "Model Comparison For the headline task, we compared our model to the DUC-04 standard baseline of the first sentence, truncated at the first word boundary after 75 characters; and the output of the Topiary system (), which came top in almost all measures in the DUC-04 evaluation.", "labels": [], "entities": [{"text": "DUC-04 standard baseline", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.8323448300361633}, {"text": "DUC-04 evaluation", "start_pos": 261, "end_pos": 278, "type": "DATASET", "confidence": 0.880803257226944}]}, {"text": "In order to generate a headline, Topiary first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords.", "labels": [], "entities": []}, {"text": "For the captions, we compared our model against the highest-scoring document sentence according to the SVM and against the probabilistic model presented in.", "labels": [], "entities": []}, {"text": "The latter estimates the probability of a phrase appearing in the caption given the same phrase appearing in the corresponding document and uses a language model to select among many different surface realizations.", "labels": [], "entities": []}, {"text": "The language model is adapted with probabilities from an image annotation model).", "labels": [], "entities": []}, {"text": "We evaluated the quality of the headlines using ROUGE ().", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9953973889350891}]}, {"text": "The DUC-04 dataset provides four reference headlines per document.", "labels": [], "entities": [{"text": "DUC-04 dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9855366349220276}]}, {"text": "We report unigram overlap (ROUGE-1) and bigram overlap (ROUGE-2) as a means of assessing informativeness, and the longest common subsequence (ROUGE-L) as a means of as-sessing fluency.", "labels": [], "entities": [{"text": "unigram overlap (ROUGE-1)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8614458560943603}, {"text": "bigram overlap (ROUGE-2)", "start_pos": 40, "end_pos": 64, "type": "METRIC", "confidence": 0.901343822479248}, {"text": "longest common subsequence (ROUGE-L)", "start_pos": 114, "end_pos": 150, "type": "METRIC", "confidence": 0.5865721901257833}]}, {"text": "Original DUC-04 ROUGE parameters were used.", "labels": [], "entities": [{"text": "DUC-04", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.511046290397644}, {"text": "ROUGE", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.855746328830719}]}, {"text": "We also use ROUGE to evaluate the automatic captions with the original BBC captions as reference.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9971861243247986}, {"text": "BBC captions", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.975117027759552}]}, {"text": "In addition, we evaluated the generated headlines by eliciting human judgments.", "labels": [], "entities": []}, {"text": "Participants were presented with a news article and its corresponding headline and were asked to rate the latter along two dimensions: informativeness (does the headline capture the article's most important information?), and grammaticality (is it fluent and easy to understand?).", "labels": [], "entities": []}, {"text": "The subjects used a seven point rating scale; an ideal system would receive high numbers for both measures.", "labels": [], "entities": []}, {"text": "We randomly selected twelve documents from the test set and generated headlines with our model.", "labels": [], "entities": []}, {"text": "We also included the output of Topiary and the human written DUC-04 headlines as a gold standard.", "labels": [], "entities": [{"text": "Topiary", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8825408220291138}, {"text": "DUC-04 headlines", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.935471385717392}]}, {"text": "We thus obtained ratings for 48 (12 \u00d7 4) document-highlights pairs.", "labels": [], "entities": []}, {"text": "We elicited judgments for the generated captions in a similar fashion.", "labels": [], "entities": []}, {"text": "Participants were presented with a document, an associated image, and its caption, and asked to rate the latter (using a 1-7 rating scale) with respect to grammaticality and informativeness (does it describe succinctly the content of the image and document?).", "labels": [], "entities": []}, {"text": "Again, we randomly selected 12 document-image pairs from the test set and generated captions for them using the highest scoring document sentence according to the SVM, our ILP-based model, and the output of Feng and Lapata's (2010a) system.", "labels": [], "entities": []}, {"text": "We also included the original BBC captions as an upper bound.", "labels": [], "entities": [{"text": "BBC captions", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9678114056587219}]}, {"text": "Both studies were conducted over the Internet using).", "labels": [], "entities": []}, {"text": "80 unpaid volunteers rated the headlines and 65 the captions, all self reported native English speakers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: QG rules generated for (a) headline and  (b) caption tasks (top 4 labels shown). The columns  show label of root node, proportion of the full rule- set, then the proportions of rules for this label in- volving no modification, deletions, insertions and  re-orderings.", "labels": [], "entities": []}, {"text": " Table 2. The \u03bb parameter was  set to 0.2 to ensure that paraphrases were included;  other parameters were chosen to capture the prop-Parameter  Headlines Captions  Min length  L min  8  8  Max length  L max  16  20  Min keywords T min  0  2  Max sentences N S  5  1  Paraphrase  \u03bb  0.2  0.1", "labels": [], "entities": []}, {"text": " Table 5: Average human ratings of DUC-04 head- lines, for our ILP model, the lead sentence baseline,  the output of Topiary and the human-written refer- ence.", "labels": [], "entities": []}, {"text": " Table 6: Average human ratings of captions, for  our ILP model, the sentence baseline chosen by the  SVM, Feng and Lapata's (2010) model and the ref- erence BBC caption.", "labels": [], "entities": [{"text": "SVM", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9388444423675537}, {"text": "ref- erence BBC caption", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.4979720950126648}]}]}