{"title": [{"text": "Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing", "labels": [], "entities": [{"text": "Unsupervised Induction of Tree Substitution Grammars", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7330616066853205}, {"text": "Parsing", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.7491271495819092}]}], "abstractContent": [{"text": "Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics.", "labels": [], "entities": [{"text": "Inducing a grammar directly from text", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8019700050354004}, {"text": "Computational Linguistics", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.822793185710907}]}, {"text": "Significant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models.", "labels": [], "entities": []}, {"text": "In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8900971015294393}]}, {"text": "We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions.", "labels": [], "entities": []}, {"text": "This approach significantly improves the state-of-the-art, when measured by head attachment accuracy.", "labels": [], "entities": [{"text": "head attachment", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6772139072418213}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.7568223476409912}]}], "introductionContent": [{"text": "Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7787370681762695}]}, {"text": "Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches).", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.7346639037132263}]}, {"text": "Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels.", "labels": [], "entities": [{"text": "Phrase-structure parser induction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9547168016433716}]}, {"text": "In particular the constituent labels are highly ambiguous, firstly we don't know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category fora clause) rely on the correct inference of all the latent labels below them.", "labels": [], "entities": []}, {"text": "However recent work on the induction of dependency grammars has proved more fruitful ().", "labels": [], "entities": [{"text": "induction of dependency grammars", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7292582541704178}]}, {"text": "Dependency grammars should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence.", "labels": [], "entities": []}, {"text": "Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models), especially in comparison to supervised parsing models).", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7767340242862701}]}, {"text": "Simple models are attractive for grammar induction because they have a limited capacity to overfit, however they are incapable of modelling many known linguistic phenomena.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8345124125480652}]}, {"text": "We posit that more complex grammars could be used to better model the unsupervised task, provided that active measures are taken to prevent overfitting.", "labels": [], "entities": []}, {"text": "In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8555633425712585}]}, {"text": "This allows the model to learn large dependency fragments to best describe the text, with the prior biasing the model towards fewer and smaller grammar productions.", "labels": [], "entities": []}, {"text": "We adopt the split-head construction to map dependency parses to context free grammar (CFG) derivations, over which we apply a model of TSG induction.", "labels": [], "entities": [{"text": "TSG induction", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.8417877554893494}]}, {"text": "The model uses a hierarchical Pitman-Yor process to encode a backoff path from TSG to CFG rules, and from lexicalised to unlexicalised rules.", "labels": [], "entities": []}, {"text": "Our best lexicalised model achieves ahead attachment accuracy of of 55.7% on Section 23 of the WSJ data set, which significantly improves over state-ofthe-art and far exceeds an EM baseline () which obtains 35.9%.", "labels": [], "entities": [{"text": "ahead attachment", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.7634401023387909}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.5734713673591614}, {"text": "Section 23 of the WSJ data set", "start_pos": 77, "end_pos": 107, "type": "DATASET", "confidence": 0.8937495265688215}]}], "datasetContent": [{"text": "We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn.) and reporting head attachment accuracy.", "labels": [], "entities": [{"text": "WSJ section of the Penn.)", "start_pos": 107, "end_pos": 132, "type": "DATASET", "confidence": 0.9365995625654856}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9447090029716492}]}, {"text": "Like previous work we pre-process the training and test data to remove punctuation, training our unlexicalised models on the gold-standard part-of-speech tags, and including words occurring more than 100 times in our lexicalised models.", "labels": [], "entities": []}, {"text": "It is very difficult for an unsupervised model to learn from long training sentences as they contain a great deal of ambiguity, therefore the majority of DMV based models have been trained on sentences restricted in length to \u2264 10 tokens.", "labels": [], "entities": []}, {"text": "This has the added benefit of decreasing the runtime for experiments.", "labels": [], "entities": []}, {"text": "We present experiments with this training scenario.", "labels": [], "entities": []}, {"text": "The training data comes from sections 2-21, while section 23 is used for evaluation.", "labels": [], "entities": []}, {"text": "An advantage of our sampling based approach over previous work is that we infer all the hyperparameters, as such we don't require the use of section 22 for tuning the model.", "labels": [], "entities": []}, {"text": "The models are evaluated in terms of head attachment accuracy (the percentage of correctly predicted head indexes for each token in the test data), on two subsets of the testing data.", "labels": [], "entities": [{"text": "head attachment accuracy", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.5435057580471039}]}, {"text": "Although we can argue that unsupervised models are better learnt from short sentences, it is much harder to argue that we don't then need to be able to parse long sentences with a trained model.", "labels": [], "entities": []}, {"text": "The most commonly employed test set mirrors the training data by only including sentences \u2264 10.", "labels": [], "entities": []}, {"text": "In this work we focus on the accuracy of our models on the whole of section 23, without any pruning for length.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9992582201957703}, {"text": "length", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9767532348632812}]}, {"text": "The training and testing corpora statistics are presented in.", "labels": [], "entities": []}, {"text": "Subsequent to the evaluation reported in we use section 22 to report the correlation between heldout accuracy and the model log-likelihood (LLH) for analytic purposes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9312245845794678}, {"text": "model log-likelihood (LLH)", "start_pos": 118, "end_pos": 144, "type": "METRIC", "confidence": 0.7474399685859681}]}, {"text": "As we are using a sampler during training, the result of any single run is non-deterministic and will exhibit a degree of variance.", "labels": [], "entities": []}, {"text": "All our reported results are the mean and standard deviation (\u03c3) from forty sampling runs.", "labels": [], "entities": [{"text": "standard deviation (\u03c3)", "start_pos": 42, "end_pos": 64, "type": "METRIC", "confidence": 0.9252025365829468}]}, {"text": "shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models.", "labels": [], "entities": [{"text": "head attachment", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6717308759689331}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9710035920143127}, {"text": "TSG-DMV", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.6829146146774292}]}, {"text": "The subset of hierarchical priors used by each model is noted in brackets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Corpus statistics for the training and testing data  for the TSG-DMV model. All models are trained on the  gold standard part-of-speech tags after removing punctu- ation.", "labels": [], "entities": []}, {"text": " Table 4: Mean and variance for the head attachment accu- racy of our TSG-DMV models (highlighted) with varying  backoff paths, and many other high performing models.  Citations indicate where the model and result were re- ported. Our models labelled TSG used an unlexicalised  top level G c PYP, while those labelled LexTSG used the  full lexicalised G c .", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9968372583389282}, {"text": "head attachment accu- racy", "start_pos": 36, "end_pos": 62, "type": "METRIC", "confidence": 0.729542362689972}, {"text": "TSG", "start_pos": 251, "end_pos": 254, "type": "DATASET", "confidence": 0.9499694108963013}, {"text": "LexTSG", "start_pos": 318, "end_pos": 324, "type": "DATASET", "confidence": 0.9862886667251587}]}, {"text": " Table 7: The ten most frequent LexTSG-DMV rules in a final training sample that contain has.", "labels": [], "entities": [{"text": "LexTSG-DMV", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.6426378488540649}]}, {"text": " Table 5: Per tag type predicted count and accuracy,  for the most frequent 15 un/lexicalised tokens on the  WSJ Section 22 |w| \u2264 10 heldout set (LexTSG-DMV  (P lcfg , P cfg , P sh )).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9992656111717224}, {"text": "WSJ Section 22", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.9356120626131693}, {"text": "LexTSG-DMV", "start_pos": 146, "end_pos": 156, "type": "DATASET", "confidence": 0.9164395332336426}]}]}