{"title": [{"text": "Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe anew scalable algorithm for semi-supervised training of conditional random fields (CRF) and its application to part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "semi-supervised training of conditional random fields (CRF)", "start_pos": 40, "end_pos": 99, "type": "TASK", "confidence": 0.6322207583321465}, {"text": "part-of-speech (POS) tagging", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.6078518927097321}]}, {"text": "The algorithm uses a similarity graph to encourage similar n-grams to have similar POS tags.", "labels": [], "entities": []}, {"text": "We demonstrate the efficacy of our approach on a domain adaptation task, where we assume that we have access to large amounts of unlabeled data from the target domain, but no additional labeled data.", "labels": [], "entities": [{"text": "domain adaptation task", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7986737291018168}]}, {"text": "The similarity graph is used during training to smooth the state posteriors on the target domain.", "labels": [], "entities": []}, {"text": "Standard inference can be used attest time.", "labels": [], "entities": []}, {"text": "Our approach is able to scale to very large problems and yields significantly improved target domain accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9867807030677795}]}], "introductionContent": [{"text": "Semi-supervised learning (SSL) is the use of small amounts of labeled data with relatively large amounts of unlabeled data to train predictors.", "labels": [], "entities": [{"text": "Semi-supervised learning (SSL)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7898581981658935}]}, {"text": "In some cases, the labeled data can be sufficient to provide reasonable accuracy on in-domain data, but performance on even closely related out-of-domain data may lag far behind.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9982744455337524}]}, {"text": "Annotating training data for all sub-domains of a varied domain such as all of Web text is impractical, giving impetus to the development of SSL techniques that can learn from unlabeled data to perform well across domains.", "labels": [], "entities": []}, {"text": "The earliest SSL algorithm is self-training, where one makes use of a previously trained model to annotate unlabeled data which is then used to re-train the model.", "labels": [], "entities": [{"text": "SSL", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.981966495513916}]}, {"text": "While self-training is widely used and can yield good results in some applications, it has no theoretical guarantees except under certain stringent conditions, which rarely hold in practice.", "labels": [], "entities": []}, {"text": "Other SSL methods include co-training, transductive support vector machines (SVMs), and graph-based SSL (.", "labels": [], "entities": []}, {"text": "Several surveys cover abroad range of methods.", "labels": [], "entities": []}, {"text": "A majority of SSL algorithms are computationally expensive; for example, solving a transductive SVM exactly is intractable.", "labels": [], "entities": [{"text": "SSL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9730644822120667}]}, {"text": "Thus we have a conflict between wanting to use SSL with large unlabeled data sets for best accuracy, but being unable to do so because of computational complexity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.994875967502594}]}, {"text": "Some researchers attempted to resolve this conflict by resorting to approximations (), but those lead to suboptimal results (.", "labels": [], "entities": []}, {"text": "Graph-based SSL algorithms ( are an important subclass of SSL techniques that have received much attention in the recent past, as they outperform other approaches and also scale easily to large problems.", "labels": [], "entities": [{"text": "SSL", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.6967176795005798}]}, {"text": "Here one assumes that the data (both labeled and unlabeled) is represented by vertices in a graph.", "labels": [], "entities": []}, {"text": "Graph edges link vertices that are likely to have the same label.", "labels": [], "entities": []}, {"text": "Edge weights govern how strongly the labels of the nodes linked by the edge should agree.", "labels": [], "entities": []}, {"text": "Most previous work in SSL has focused on unstructured classification problems, that is, problems with a relatively small set of atomic labels.", "labels": [], "entities": [{"text": "SSL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9824252724647522}, {"text": "unstructured classification", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6318231821060181}]}, {"text": "There has been much less work on SSL for structured prediction where labels are composites of many atomic labels with constraints between them.", "labels": [], "entities": [{"text": "SSL", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9667503833770752}, {"text": "structured prediction", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.764098584651947}]}, {"text": "While the number of atomic labels might be small, there will generally be exponentially many ways to combine them into the final structured label.", "labels": [], "entities": []}, {"text": "Structured prediction problems over sequences appear for example in speech recognition, named-entity recognition, and part-of-speech tagging; in machine translation and syntactic parsing, the output maybe treestructured.", "labels": [], "entities": [{"text": "Structured prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8265187442302704}, {"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7729648351669312}, {"text": "named-entity recognition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7268784940242767}, {"text": "part-of-speech tagging", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7190483957529068}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7260179221630096}, {"text": "syntactic parsing", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.715341106057167}]}, {"text": "proposed a max-margin objective for semi-supervised learning over structured spaces.", "labels": [], "entities": []}, {"text": "Their objective is similar to that of manifold regularization ( ) and they make use of a graph as a smoothness regularizer.", "labels": [], "entities": [{"text": "manifold regularization", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7270432710647583}]}, {"text": "However their solution involves inverting a matrix whose size depends on problem size, making it impractical for very large problems.", "labels": [], "entities": []}, {"text": "present a modified version of the co-training algorithm for structured output spaces.", "labels": [], "entities": []}, {"text": "In both of the above cases, the underlying model is based on structured SVM, which does not scale well to very large datasets.", "labels": [], "entities": []}, {"text": "More recently proposed to train a conditional random field (CRF) () using an entropy-based regularizer.", "labels": [], "entities": []}, {"text": "Their approach is similar to the entropy minimization algorithm).", "labels": [], "entities": [{"text": "entropy minimization", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7568466663360596}]}, {"text": "The problem here is that their objective is not convex and thus can pose issues for large problems.", "labels": [], "entities": []}, {"text": "Further, graphbased SSL algorithms outperform algorithms based on entropy minimization (.", "labels": [], "entities": [{"text": "entropy minimization", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.749833881855011}]}, {"text": "In this work, we propose a graph-based SSL method for CRFs that is computationally practical for very large problems, unlike the methods in the studies cited above.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.9156152009963989}]}, {"text": "Our method is scalable because it trains with efficient standard building blocks for CRF inference and learning and also standard graph label propagation machinery.", "labels": [], "entities": [{"text": "CRF inference", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.8901696801185608}, {"text": "graph label propagation", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.6951004266738892}]}, {"text": "Graph regularizer computations are only used for training, so attest time, standard CRF inference can be used, unlike in graph-based transductive methods.", "labels": [], "entities": []}, {"text": "Briefly, our approach starts by training a CRF on the source domain labeled data, and then uses it to decode unlabeled data from the target domain.", "labels": [], "entities": []}, {"text": "The state posteriors on the target domain are then smoothed using the graph regularizer.", "labels": [], "entities": []}, {"text": "Best state sequences for the unlabeled target data are then created by Viterbi decoding with the smoothed state posteriors, and this automatic target domain annotation is combined with the labeled source domain data to retrain the CRF.", "labels": [], "entities": []}, {"text": "We demonstrate our new method in domain adaptation fora CRF part-of-speech (POS) tagger.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7555604577064514}, {"text": "CRF part-of-speech (POS) tagger", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.5834884544213613}]}, {"text": "While POS tagging accuracies have reached the level of inter-annotator agreement (>97%) on the standard PennTreebank test set (, performance on out-of-domain data is often well below 90%, impairing language processing tasks that need syntactic information.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.7842408716678619}, {"text": "PennTreebank test set", "start_pos": 104, "end_pos": 125, "type": "DATASET", "confidence": 0.9879786769549052}]}, {"text": "For example, on the question domain used in this paper, the tagging accuracy of a supervised CRF is only 84%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9495006799697876}]}, {"text": "Our domain adaptation algorithm improves performance to 87%, which is still far below in-domain performance, but a significant reduction in error.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.848932683467865}, {"text": "error", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.9905863404273987}]}], "datasetContent": [{"text": "We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source domain training set.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section of the Penn Treebank", "start_pos": 11, "end_pos": 65, "type": "DATASET", "confidence": 0.9492481513456865}]}, {"text": "We follow standard setup procedures for this task and train on sections 00-18, comprising of 38,219 POS-tagged sentences with a total of 912,344 words.", "labels": [], "entities": []}, {"text": "To evaluate our domain-adaptation approach, we consider two different target domains: questions and biomedical data.", "labels": [], "entities": []}, {"text": "Both target domains are relatively far from the source domain (newswire), making this a very challenging task.", "labels": [], "entities": []}, {"text": "The QuestionBank (), provides an excellent corpus consisting of 4,000 questions that were manually annotated with POS tags and parse trees.", "labels": [], "entities": []}, {"text": "We used the first half as our development set and the second half as our test set.", "labels": [], "entities": []}, {"text": "Questions are difficult to tag with WSJ-trained taggers primarily because the word order is very different than that of the mostly declarative sentences in the training data.", "labels": [], "entities": [{"text": "WSJ-trained taggers", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7169095277786255}]}, {"text": "Additionally, the unknown word rate is more than twice as high as on the in-domain development set (7.29% vs. 3.39%).", "labels": [], "entities": [{"text": "word rate", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.6910001635551453}]}, {"text": "As our unlabeled data, we use a set of 10 million questions collected from anonymized Internet search queries.", "labels": [], "entities": []}, {"text": "These queries were selected to be similar in style and length to the questions in the QuestionBank.", "labels": [], "entities": [{"text": "length", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.8474071621894836}, {"text": "QuestionBank", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.959693968296051}]}, {"text": "As running the CRF over 10 million sentences can be rather cumbersome and probably unnecessary, we randomly select 100,000 of these queries and treat this as Du . Because the graph nodes and the features used in the similarity function are based on n-grams, data sparsity can be a serious problem, and we therefore use the entire unlabeled data set for graph construction.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 353, "end_pos": 371, "type": "TASK", "confidence": 0.7266313880681992}]}, {"text": "We estimate the mutual information-based features for each trigram type overall the 10 million questions, and then construct the graph over only the set of trigram types that actually occurs in the 100,000 random subset and the WSJ training set.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 228, "end_pos": 244, "type": "DATASET", "confidence": 0.967053751150767}]}, {"text": "For our second target domain, we use the Penn BioTreebank (.", "labels": [], "entities": [{"text": "Penn BioTreebank", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.942384272813797}]}, {"text": "This corpus consists of 1,061 sentences that have been manually annotated with POS tags.", "labels": [], "entities": []}, {"text": "We used the first 500 sentences as a development set and the remaining 561 sentences as our final test set.", "labels": [], "entities": []}, {"text": "The high unknown word rate (23.27%) makes this corpus very difficult to tag.", "labels": [], "entities": []}, {"text": "Furthermore, the POS tag set for this data is a super-set of the Penn Treebank's, including the two new tags HYPH (for hyphens) and AFX (for common post-modifiers of biomedical entities such as genes).", "labels": [], "entities": [{"text": "POS tag set", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.7668236295382181}, {"text": "Penn Treebank's", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9937408566474915}, {"text": "HYPH", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9771715998649597}, {"text": "AFX", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9863152503967285}]}, {"text": "These tags were introduced due to the importance of hyphenated entities in biomedical text, and are used for 1.8% of the words in the test set.", "labels": [], "entities": []}, {"text": "Any tagger trained only on WSJ text will automatically predict wrong tags for those words.", "labels": [], "entities": []}, {"text": "For unlabeled data we used 100,000 sentences that were chosen by searching MEDLINE for abstracts pertaining to cancer, in particular genomic variations and mutations ().", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9215381741523743}]}, {"text": "Since we did not have access to additional unlabeled data, we used the same set of sentences as target domain unlabeled data, Du . The graph here was constructed over the 100,000 unlabeled sentences and the WSJ training set.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 207, "end_pos": 223, "type": "DATASET", "confidence": 0.9710602760314941}]}, {"text": "Finally, we remind the reader that we did not use label information for graph construction in either corpus.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7585295736789703}]}], "tableCaptions": [{"text": " Table 2: Domain adaptation experiments. POS tagging accuracies in %.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8328574597835541}, {"text": "POS tagging", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.6791083961725235}]}, {"text": " Table 3: Analysis of the graphs constructed for the two  datasets discussed in Section 6. Unlabeled trigrams occur  in the target domain only. Labeled trigrams occur at least  once in the WSJ training data.", "labels": [], "entities": [{"text": "WSJ training data", "start_pos": 189, "end_pos": 206, "type": "DATASET", "confidence": 0.924020012219747}]}]}