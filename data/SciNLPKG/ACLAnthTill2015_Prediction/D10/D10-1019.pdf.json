{"title": [{"text": "Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks", "labels": [], "entities": [{"text": "Cascaded Segmentation and Tagging Tasks", "start_pos": 52, "end_pos": 91, "type": "TASK", "confidence": 0.800101125240326}]}], "abstractContent": [{"text": "Many sequence labeling tasks in NLP require solving a cascade of segmentation and tagging subtasks, such as Chinese POS tagging, named entity recognition, and soon.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.5994446724653244}, {"text": "Chinese POS tagging", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.5599349538485209}, {"text": "named entity recognition", "start_pos": 129, "end_pos": 153, "type": "TASK", "confidence": 0.6441609660784403}]}, {"text": "Traditional pipeline approaches usually suffer from error propagation.", "labels": [], "entities": []}, {"text": "Joint training/decoding in the cross-product state space could cause too many parameters and high inference complexity.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel method which integrates graph structures of two sub-tasks into one using virtual nodes, and performs joint training and decoding in the fac-torized state space.", "labels": [], "entities": []}, {"text": "Experimental evaluations on CoNLL 2000 shallow parsing data set and Fourth SIGHAN Bakeoff CTB POS tagging data set demonstrate the superiority of our method over cross-product, pipeline and candidate reranking approaches.", "labels": [], "entities": [{"text": "CoNLL 2000 shallow parsing data set", "start_pos": 28, "end_pos": 63, "type": "DATASET", "confidence": 0.852777381738027}, {"text": "Fourth SIGHAN Bakeoff CTB POS tagging data set", "start_pos": 68, "end_pos": 114, "type": "DATASET", "confidence": 0.7439298816025257}]}], "introductionContent": [{"text": "There is atypical class of sequence labeling tasks in many natural language processing (NLP) applications, which require solving a cascade of segmentation and tagging subtasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.7331878741582235}]}, {"text": "For example, many Asian languages such as Japanese and Chinese which do not contain explicitly marked word boundaries, word segmentation is the preliminary step for solving part-of-speech (POS) tagging problem.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.729634240269661}, {"text": "part-of-speech (POS) tagging problem", "start_pos": 173, "end_pos": 209, "type": "TASK", "confidence": 0.6574787298838297}]}, {"text": "Sentences are firstly segmented into words, then each word is assigned with a part-of-speech tag.", "labels": [], "entities": []}, {"text": "Both syntactic parsing and dependency parsing usually start with a textual input that is tokenized, and POS tagged.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7084240913391113}, {"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8426073789596558}]}, {"text": "The most commonly approach solves cascaded subtasks in a pipeline, which is very simple to implement and allows fora modular approach.", "labels": [], "entities": []}, {"text": "While, the key disadvantage of such method is that errors propagate between stages, significantly affecting the quality of the final results.", "labels": [], "entities": []}, {"text": "To cope with this problem, proposed a reranking framework in which N-best segment candidates generated in the first stage are passed to the tagging model, and the final output is the one with the highest overall segmentation and tagging probability score.", "labels": [], "entities": []}, {"text": "The main drawback of this method is that the interaction between tagging and segmentation is restricted by the number of candidate segmentation outputs.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.9350144863128662}]}, {"text": "Razvan C. presented an improved pipeline model in which upstream subtask outputs are regarded as hidden variables, together with their probabilities are used as probabilistic features in the downstream subtasks.", "labels": [], "entities": []}, {"text": "One shortcoming of this method is that calculation of marginal probabilities of features maybe inefficient and some approximations are required for fast computation.", "labels": [], "entities": []}, {"text": "Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure.", "labels": [], "entities": []}, {"text": "On the other hand, joint learning and decoding using cross-product of segmentation states and tagging states does not suffer from error propagation problem and achieves higher accuracy on both subtasks ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9985876083374023}]}, {"text": "However, two problems arises due to the large state space, one is that the amount of parameters increases rapidly, which is apt to overfit on the training corpus, the other is that the inference by dynamic programming could be inefficient.", "labels": [], "entities": []}, {"text": "proposed Dynamic Conditional Random Fields (DCRFs) to perform joint training/decoding of subtasks using much fewer parameters than the cross-product approach.", "labels": [], "entities": []}, {"text": "How-ever, DCRFs do not guarantee non-violation of hardconstraints that nodes within the same segment get a single consistent tagging label.", "labels": [], "entities": []}, {"text": "Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable.", "labels": [], "entities": []}, {"text": "Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together.", "labels": [], "entities": []}, {"text": "The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation.", "labels": [], "entities": []}, {"text": "On the other hand, proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features.", "labels": [], "entities": []}, {"text": "As pipeline models, error propagation problem exists for such method.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel graph structure that exploits joint training and decoding in the factorized state space.", "labels": [], "entities": []}, {"text": "Our method does not suffer from error propagation, and guards against violations of those hard-constraints imposed by segmentation subtask.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7095735371112823}]}, {"text": "The motivation is to integrate two Markov chains for segmentation and tagging subtasks into a single chain, which contains two types of nodes, then standard dynamic programming based exact inference is employed on the hybrid structure.", "labels": [], "entities": []}, {"text": "Experiments are conducted on two different tasks, CoNLL 2000 shallow parsing and SIGHAN 2008 Chinese word segmentation and POS tagging.", "labels": [], "entities": [{"text": "CoNLL 2000 shallow parsing", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.8484871983528137}, {"text": "SIGHAN 2008 Chinese word segmentation", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.5828890442848206}, {"text": "POS tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.7676149606704712}]}, {"text": "Evaluation results of shallow parsing task show the superiority of our proposed method over traditional joint training/decoding approach using crossproduct state space, and achieves the best reported results when no additional resources at hand.", "labels": [], "entities": [{"text": "shallow parsing task", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6494896610577902}]}, {"text": "For Chinese word segmentation and POS tagging task, a strong baseline pipeline model is built, experimental results show that the proposed method yields a more substantial improvement over the baseline than candidate reranking approach.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.5978527863820394}, {"text": "POS tagging task", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8455434242884318}]}, {"text": "The rest of this paper is organized as follows: In Section 2, we describe our novel graph structure.", "labels": [], "entities": []}, {"text": "In Section 3, we analyze complexity of our proposed method.", "labels": [], "entities": []}, {"text": "Experimental results are shown in Section 4.", "labels": [], "entities": []}, {"text": "We conclude the work in Section 5. 2 Multi-chain integration using Virtual Nodes", "labels": [], "entities": [{"text": "Multi-chain integration", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8131079077720642}]}], "datasetContent": [{"text": "Our first experiment is the shallow parsing task.", "labels": [], "entities": [{"text": "shallow parsing task", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7491090695063273}]}, {"text": "We use corpus from CoNLL 2000 shared task, which contains 8936 sentences for training and 2012 sentences for testing.", "labels": [], "entities": [{"text": "CoNLL 2000 shared task", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.9451812952756882}]}, {"text": "There are 11 tagging labels: noun phrase(NP), verb phrase(VP) , . .", "labels": [], "entities": []}, {"text": ". and other (O), the segmentation state space we used is BIES label set, since we find that it yields a little improvement over BIO set.", "labels": [], "entities": [{"text": "segmentation state", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.875744491815567}, {"text": "BIES label set", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.8558444778124491}]}, {"text": "We use the standard evaluation metrics, which are precision P (percentage of output phrases that exactly match the reference phrases), recall R (percentage of reference phrases returned by our system), and their harmonic mean, the F1 score F 1 = 2P RP +R (which we call F score in what follows).", "labels": [], "entities": [{"text": "precision P", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9796415567398071}, {"text": "recall R", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9838710725307465}, {"text": "F1 score F 1", "start_pos": 231, "end_pos": 243, "type": "METRIC", "confidence": 0.9677404761314392}, {"text": "F score", "start_pos": 270, "end_pos": 277, "type": "METRIC", "confidence": 0.986297607421875}]}, {"text": "We compare our approach with traditional crossproduct method.", "labels": [], "entities": []}, {"text": "To find good feature templates, development data are required.", "labels": [], "entities": []}, {"text": "Since CoNLL2000 does not provide development data set, we divide the training data into 10 folds, of which 9 folds for training and 1 fold for developing.", "labels": [], "entities": [{"text": "CoNLL2000", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.897741436958313}]}, {"text": "After selecting feature templates by cross validation, we extract features and learn their weights on the whole training data set.", "labels": [], "entities": []}, {"text": "Feature templates are summarized in, where w i denotes the i th word, pi denotes the i th POS tag.", "labels": [], "entities": []}, {"text": "Notice that in the second row, feature templates of the hybrid CRFs does not contain w i\u22122 s i , w i+2 s i , since we find that these two templates degrade performance in cross validation.", "labels": [], "entities": []}, {"text": "However, w i\u22122 ti , w i+2 ti are useful, which implies that the proper context window size for segmentation is smaller than tagging.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 95, "end_pos": 107, "type": "TASK", "confidence": 0.9664738178253174}]}, {"text": "Similarly, for hybrid CRFs, the window size of POS bigram features for segmentation is 5 (from p i\u22122 top i+2 , seethe eighth row in the second column); while for tagging, the size is 7 (from p i\u22123 top i+3 , seethe ninth row in the second column).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.9694467186927795}]}, {"text": "However for cross-product method, their window sizes must be consistent.", "labels": [], "entities": []}, {"text": "For traditional cross-product CRFs and our hybrid CRFs, we use fixed gaussian prior \u03c3 = 1.0 for both methods, we find that this parameter does not signifi- cantly affect the results when it varies between 1 and 10.", "labels": [], "entities": []}, {"text": "LBFGS(Nocedal and Wright, 1999) method is employed for numerical optimization.", "labels": [], "entities": [{"text": "LBFGS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6662401556968689}, {"text": "numerical optimization", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8712923526763916}]}, {"text": "Experimental results are shown in.", "labels": [], "entities": []}, {"text": "Our proposed CRFs achieve a performance gain of 0.43 points in F-score over cross-product CRFs that use state space while require less training time.", "labels": [], "entities": [{"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9976346492767334}]}, {"text": "For comparison, we also listed the results of previous top systems, as shown in.", "labels": [], "entities": []}, {"text": "Our proposed method outperforms other systems when no additional resources at hand.", "labels": [], "entities": []}, {"text": "Though recently semisupervised learning that incorporates large mounts of unlabeled data has been shown great improvement over traditional supervised methods, such as the last row in, supervised learning is fundamental.", "labels": [], "entities": []}, {"text": "We believe that combination of our method and semi-supervised learning will achieve further improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for shallow parsing task, Hybrid CRFs  significantly outperform Cross-Product CRFs (McNe- mar's test; p < 0.01)", "labels": [], "entities": [{"text": "parsing task", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.7201442122459412}, {"text": "McNe- mar's test", "start_pos": 102, "end_pos": 118, "type": "METRIC", "confidence": 0.592061984539032}]}, {"text": " Table 4: Comparison with other systems on shallow pars- ing task", "labels": [], "entities": [{"text": "shallow pars- ing", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.6208445131778717}]}, {"text": " Table 6: Word segmentation results on Fourth SIGHAN  Bakeoff CTB corpus", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7584648132324219}, {"text": "Fourth SIGHAN  Bakeoff CTB", "start_pos": 39, "end_pos": 65, "type": "DATASET", "confidence": 0.8060998693108559}]}, {"text": " Table 7: POS results on Fourth SIGHAN Bakeoff CTB  corpus", "labels": [], "entities": [{"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9339335560798645}, {"text": "Fourth SIGHAN Bakeoff CTB", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.7544631883502007}]}, {"text": " Table 8: Comparison of word segmentation and POS tag- ging, such comparison is indirect due to different data  sets and resources.  Model  F1  Pipeline (ours)  90.40  100-Best Reranking (ours)  90.53  Hybrid CRFs (ours)  90.88  Pipeline (Shi and Wang, 2007)  91.67  20-Best Reranking (Shi and Wang,  2007)", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.699640765786171}, {"text": "POS tag- ging", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.596993662416935}]}]}