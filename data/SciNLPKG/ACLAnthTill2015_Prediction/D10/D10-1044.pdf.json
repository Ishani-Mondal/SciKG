{"title": [{"text": "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation", "labels": [], "entities": [{"text": "Discriminative Instance Weighting", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7541903853416443}, {"text": "Domain Adaptation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7192008346319199}, {"text": "Statistical Machine Translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.7750842769940695}]}], "abstractContent": [{"text": "We describe anew approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain , determined by both how similar to it they appear to be, and whether they belong to general language or not.", "labels": [], "entities": [{"text": "SMT adaptation", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9971472024917603}]}, {"text": "This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components , and using a simpler training procedure.", "labels": [], "entities": [{"text": "discriminative weighting", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6185518503189087}]}, {"text": "We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain adaptation is a common concern when optimizing empirical NLP applications.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.798890084028244}]}, {"text": "Even when there is training data available in the domain of interest, there is often additional data from other domains that could in principle be used to improve performance.", "labels": [], "entities": []}, {"text": "Realizing gains in practice can be challenging, however, particularly when the target domain is distant from the background data.", "labels": [], "entities": []}, {"text": "For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.), which precludes a single universal approach to adaptation.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.8276223937670389}, {"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.9597663283348083}]}, {"text": "In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material-though adequate for reasonable performance-is also available.", "labels": [], "entities": []}, {"text": "This is a standard adaptation problem for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9965157508850098}]}, {"text": "It is difficult when IN and OUT are dissimilar, as they are in the cases we study.", "labels": [], "entities": [{"text": "IN", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9965803027153015}, {"text": "OUT", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9469796419143677}]}, {"text": "For simplicity, we assume that OUT is homogeneous.", "labels": [], "entities": [{"text": "OUT", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.44621822237968445}]}, {"text": "The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.", "labels": [], "entities": []}, {"text": "There is a fairly large body of work on SMT adaptation.", "labels": [], "entities": [{"text": "SMT adaptation", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.9961893558502197}]}, {"text": "We introduce several new ideas.", "labels": [], "entities": []}, {"text": "First, we aim to explicitly characterize examples from OUT as belonging to general language or not.", "labels": [], "entities": []}, {"text": "Previous approaches have tried to find examples that are similar to the target domain.", "labels": [], "entities": []}, {"text": "This is less effective in our setting, where IN and OUT are disparate.", "labels": [], "entities": [{"text": "IN", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9968507885932922}, {"text": "OUT", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9439809918403625}]}, {"text": "The idea of distinguishing between general and domain-specific examples is due to, who used a maximum-entropy model with latent variables to capture the degree of specificity.", "labels": [], "entities": []}, {"text": "applies a related idea in a simpler way, by splitting features into general and domain-specific versions.", "labels": [], "entities": []}, {"text": "This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach to downweight domain-specific examples in OUT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9660133719444275}]}, {"text": "Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.", "labels": [], "entities": []}, {"text": "Our second contribution is to apply instance weighting at the level of phrase pairs.", "labels": [], "entities": []}, {"text": "Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9949489831924438}]}, {"text": "For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in.", "labels": [], "entities": []}, {"text": "Phrase-level granularity distinguishes our work from previous work by, who weight sentences according to sub-corpus and genre membership.", "labels": [], "entities": []}, {"text": "Finally, we make some improvements to baseline approaches.", "labels": [], "entities": []}, {"text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.", "labels": [], "entities": []}, {"text": "This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.992741584777832}]}, {"text": "A similar maximumlikelihood approach was used by, but for language models only.", "labels": [], "entities": []}, {"text": "For comparison to information-retrieval inspired baselines, eg, we select sentences from OUT using language model perplexities from IN.", "labels": [], "entities": []}, {"text": "This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries, then pooling the match results.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our baseline techniques for SMT adaptation, and section 3 describes the instance-weighting approach.", "labels": [], "entities": [{"text": "SMT adaptation", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9967567622661591}]}, {"text": "Experiments are presented in section 4.", "labels": [], "entities": []}, {"text": "Section 5 covers relevant previous work on SMT adaptation, and section 6 concludes.", "labels": [], "entities": [{"text": "SMT adaptation", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.99740931391716}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results, for EMEA/EP translation into English  (fren) and French (enfr); and for NIST Chinese to En- glish translation with NIST06 and NIST08 evaluation  sets. Numbers are BLEU scores.", "labels": [], "entities": [{"text": "EMEA/EP translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.626744955778122}, {"text": "NIST Chinese to En- glish translation", "start_pos": 91, "end_pos": 128, "type": "TASK", "confidence": 0.7387265477861676}, {"text": "NIST06", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.9600724577903748}, {"text": "NIST08 evaluation  sets", "start_pos": 145, "end_pos": 168, "type": "DATASET", "confidence": 0.8727407455444336}, {"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9993507266044617}]}]}