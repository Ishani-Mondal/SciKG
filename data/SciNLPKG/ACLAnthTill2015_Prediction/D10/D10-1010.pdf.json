{"title": [{"text": "Learning the Relative Usefulness of Questions in Community QA", "labels": [], "entities": [{"text": "Learning the Relative Usefulness of Questions in Community QA", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8035409715440538}]}], "abstractContent": [{"text": "We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to anew, unanswered reference question.", "labels": [], "entities": []}, {"text": "The ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group.", "labels": [], "entities": []}, {"text": "Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward , unsupervised similarity measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open domain Question Answering (QA) is one of the most complex and challenging tasks in natural language processing.", "labels": [], "entities": [{"text": "Open domain Question Answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7702666691371373}, {"text": "natural language processing", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6381491223971049}]}, {"text": "In general, a question answering system may need to integrate knowledge coming from a wide variety of linguistic processing tasks such as syntactic parsing, semantic role labeling, named entity recognition, and anaphora resolution).", "labels": [], "entities": [{"text": "question answering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8183127343654633}, {"text": "syntactic parsing", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7643039226531982}, {"text": "semantic role labeling", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.610422670841217}, {"text": "named entity recognition", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.6056773960590363}, {"text": "anaphora resolution", "start_pos": 211, "end_pos": 230, "type": "TASK", "confidence": 0.7439312040805817}]}, {"text": "State of the art implementations of these linguistic analysis tasks are still limited in their performance, with errors that compound and propagate into the final performance of the QA system ().", "labels": [], "entities": []}, {"text": "Consequently, the performance of open domain QA systems has yet to arrive at a level at which it would become a feasible alternative to the current paradigms for information access based on keyword searches.", "labels": [], "entities": []}, {"text": "Recently, community-driven QA sites such as Yahoo!", "labels": [], "entities": []}, {"text": "Answers and WikiAnswers have established answers.yahoo.com, anew approach to question answering that shifts the inherent complexity of open domain QA from the computer system to volunteer contributors.", "labels": [], "entities": [{"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8303495347499847}]}, {"text": "The computer is no longer required to perform a deep linguistic analysis of questions and generate corresponding answers, and instead acts as a mediator between users submitting questions and volunteers providing the answers.", "labels": [], "entities": []}, {"text": "An important objective in community QA is to minimize the time elapsed between the submission of questions by users and the subsequent posting of answers by volunteer contributors.", "labels": [], "entities": []}, {"text": "One useful strategy for minimizing the response latency is to search the QA repository for similar questions that have already been answered, and provide the corresponding ranked list of answers, if such a question is found.", "labels": [], "entities": []}, {"text": "The success of this approach depends on the definition and implementation of the question-to-question similarity function.", "labels": [], "entities": []}, {"text": "In the simplest solution, the system searches for previously answered questions based on exact string matching with the reference question.", "labels": [], "entities": []}, {"text": "Alternatively, sites such as WikiAnswers allow the users to mark questions they think are rephrasings (\"alternate wordings\", or paraphrases) of existing questions.", "labels": [], "entities": []}, {"text": "These question clusters are then taken into account when performing exact string matching, therefore increasing the likelihood of finding previously answered questions that are semantically equivalent to the reference question.", "labels": [], "entities": [{"text": "exact string matching", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.5963609913984934}]}, {"text": "In order to lessen the amount of work required from the contributors, an alternative approach is to build a system that automatically finds rephrasings of questions, especially since question rephrasing seems to be computationally less demanding than question answering.", "labels": [], "entities": [{"text": "question rephrasing", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.6957825273275375}, {"text": "question answering", "start_pos": 251, "end_pos": 269, "type": "TASK", "confidence": 0.7675944864749908}]}, {"text": "According to previous work in this domain, a question is considered a rephrasing of a reference question Q 0 if it uses an alternate wording to express an identical information need.", "labels": [], "entities": []}, {"text": "For example, Q 0 and Q 1 below are rephrasings of each other, and consequently they are expected to have the same answer.", "labels": [], "entities": []}, {"text": "Q 0 What should I feed my turtle?", "labels": [], "entities": []}, {"text": "Paraphrasings of anew question cannot always be found in the community QA repository.", "labels": [], "entities": []}, {"text": "We believe that computing a ranked list of existing questions that at least partially address the original information need could also be useful to the user, at least until other users volunteer to give an exact answer to the original, unanswered reference question.", "labels": [], "entities": []}, {"text": "For example, in the absence of any additional information about the reference question Q 0 , the expected answers to questions Q 2 and Q 3 below maybe seen as partially overlapping in information content with the expected answer for the reference question Q 0 . An answer to question Q 4 , on the other hand, is less likely to benefit the user, even though it has a significant lexical overlap with the reference question.", "labels": [], "entities": []}, {"text": "In this paper, we propose a supervised learning approach to the question ranking problem, a generalization of the question paraphrasing problem in which questions are ranked in a partial order based on the relative information overlap between their expected answers and the expected answer of the reference question.", "labels": [], "entities": [{"text": "question ranking problem", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.8010364770889282}, {"text": "question paraphrasing", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.7320498675107956}]}, {"text": "Underlying the question ranking task is the expectation that the user who submits a reference question will find the answers of the highly ranked questions to be more useful than the answers associated with the lower ranked questions.", "labels": [], "entities": []}, {"text": "For the reference question Q 0 above, the learned ranking model is expected to produce a partial order in which Q 1 is ranked higher than Q 2 , Q 3 and Q 4 , whereas Q 2 and Q 3 are ranked higher than Q 4 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to enable the evaluation of question ranking approaches, we have previously created a dataset of 60 groups of questions (.", "labels": [], "entities": [{"text": "question ranking", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7773881554603577}]}, {"text": "Each group consists of a reference question (e.g. Q 0 above) that is associated with a partially ordered set of questions (e.g. Q 1 to Q 4 above).", "labels": [], "entities": []}, {"text": "For each reference questions, its corresponding partially ordered set is created from questions in Yahoo!", "labels": [], "entities": []}, {"text": "Answers and other online repositories that have a high cosine similarity with the reference question.", "labels": [], "entities": []}, {"text": "Out of the 26 top categories in Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.8482262194156647}]}, {"text": "Answers, the 60 reference questions span a diverse set of categories.", "labels": [], "entities": []}, {"text": "lists the 20 categories covered, where each category is shown with the number of corresponding reference questions between parentheses.", "labels": [], "entities": []}, {"text": "Inside each group, the questions are manually annotated with a partial order relation, according to their utility with respect to the reference question.", "labels": [], "entities": []}, {"text": "We use the notation Q i \u227b Q j |Q r to encode the fact that question Q i is more useful than question Q j with respect to the reference question Q r . Similarly, Q i = Q j will be used to express the fact that questions Q i and Q j are reformulations of each other (the reformulation relation is independent of the reference question).", "labels": [], "entities": []}, {"text": "The partial ordering among the questions Q 0 to Q 4 above can therefore be expressed concisely as follows: Note that we do not explicitly annotate the relation Q 1 \u227b Q 4 |Q 0 , since it can be inferred based on the transitivity of the more useful than relation: Also note that no relation is specified between Q 2 and Q 3 , and similarly no relation can be inferred between these two questions.", "labels": [], "entities": []}, {"text": "This reflects our belief that, in the absence of any additional information regarding the user or the \"turtle\" referenced in Q 0 , we cannot compare questions Q 2 and Q 3 in terms of their usefulness with respect to Q 0 . shows another reference question Q 5 from our dataset, together with its annotated group of questions Q 6 to Q 20 . In order to make the annotation process easier and reproducible, we have divided it into two levels of annotation.", "labels": [], "entities": []}, {"text": "During the first annotation stage, each question group is partitioned manually into 3 subgroups of questions: \u2022 P is the set of paraphrasing questions.", "labels": [], "entities": []}, {"text": "\u2022 U is the set of useful questions.", "labels": [], "entities": []}, {"text": "\u2022 N is the set of neutral questions.", "labels": [], "entities": []}, {"text": "A question is deemed useful if its expected answer may overlap in information content with the expected answer of the reference question.", "labels": [], "entities": []}, {"text": "The expected answer of a neutral question, on the other hand, should be irrelevant with respect to the reference question.", "labels": [], "entities": []}, {"text": "Let Q r be the reference question, Q p \u2208 Pa paraphrasing question, Q u \u2208 U a useful question, and Q n \u2208 Na neutral question.", "labels": [], "entities": []}, {"text": "Then the following relations are assumed to hold among these questions: Note that as long as these relations hold between the 3 types of questions, the names of the subgroups and their definitions are irrelevant with respect to the implied set of more useful than relations, since only the implied ternary relations will be used for training and evaluating question ranking approaches.", "labels": [], "entities": [{"text": "question ranking", "start_pos": 357, "end_pos": 373, "type": "TASK", "confidence": 0.6846908628940582}]}, {"text": "We also assume that, by transitivity, the following ternary relations also hold: For the vast majority of questions, the first annotation stage is straightforward and non-controversial.", "labels": [], "entities": []}, {"text": "In the second annotation stage, we perform a finer annotation of relations between questions in the middle group U. shows two such relations (using indentation): Q 8 \u227b Q 9 |Q 5 and Q 8 \u227b Q 10 |Q 5 . Question Q 8 would have been a rephrasing of the reference question, were it not for the noun \"art\" modifying the focus noun phrase \"summer camp\".", "labels": [], "entities": []}, {"text": "Therefore, the information content of the answer to Q 8 is strictly subsumed in the information content associated with the answer to Q 5 . Similarly, in Q 9 the focus noun phrase is further specialized through the prepositional phrase \"for girls\".", "labels": [], "entities": []}, {"text": "Therefore, (an answer to) Q 9 is less useful to Q 5 than (an answer to) Q 8 , i.e. Q 8 \u227b Q 9 |Q 5 . Furthermore, the focus \"art summer camp\" in Q 8 conceptually subsumes the focus \"summer camps for singing\" in Q 10 , therefore Q 8 \u227b Q 10 |Q 5 . We call this dataset simple since most of the reference questions are shorter than the other questions in their group.", "labels": [], "entities": []}, {"text": "We have also created a complex version of the same dataset, by selecting as the reference question in each group a longer question from the same group.", "labels": [], "entities": []}, {"text": "For example, if Q 0 were a reference question, it would be replaced with a more complex question, such as Q 2 , or Q 3 . The annotation is redone to reflect the relative usefulness relations with respect to the new reference questions.", "labels": [], "entities": []}, {"text": "We believe that the new complex dataset is closer to the actual distribution of questions in community QA repositories: unanswered questions tend to be more specific (longer), whereas general questions (shorter) are more likely to have been answered already.", "labels": [], "entities": []}, {"text": "Each dataset is annotated by two annotators, leading to a total of 4 datasets: Simple 1 , Simple 2 , Complex 1 , and Complex 2 . presents the following statistics on the two types of datasets (Simple, Complex) for each annotator (1, 2): the total number of paraphrasings (P), the total number of useful questions (U), the total number of neutral questions (N ), the total number of more useful than ordered pairs encoded in the dataset, either explicitly or through transitivity, and the Inter-Annotator Agreement (ITA).", "labels": [], "entities": []}, {"text": "We compute the ITA as the precision (P) and recall (R) with respect to the more useful than ordered pairs encoded in one annotation (P airs 1 ) relative to the ordered  pairs encoded in the other annotation (P airs 2 ).", "labels": [], "entities": [{"text": "ITA", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9601210951805115}, {"text": "precision (P)", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.9380551725625992}, {"text": "recall (R)", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.7811909019947052}]}, {"text": "The statistics in indicate that the second annotator was in general more conservative in tagging questions as paraphrases or useful questions.", "labels": [], "entities": []}, {"text": "We use the four question ranking datasets described in Section 2 to evaluate the three similarity measures cos, mcs, and \u03c6 t , as well as the SVM ranking model.", "labels": [], "entities": []}, {"text": "We report one set of results for each of the four word similarity measures wup, res, lin or jcn.", "labels": [], "entities": []}, {"text": "Each question similarity measure is evaluated in terms of its accuracy on the set of ordered pairs, and the performance is averaged between the two annotators for the Simple and Complex datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9991225600242615}]}, {"text": "If Q i \u227b Q j |Q r is a relation specified in the annotation, we consider the tuple Q i , Q j , Q r correctly classified if and only if u( where u is the question similarity measure.", "labels": [], "entities": []}, {"text": "We used the SVM light 2 implementation of ranking SVMs, with a cubic kernel and the standard parameters.", "labels": [], "entities": []}, {"text": "The SVM ranking model was trained and tested using 10-fold cross-validation, and the overall accuracy was computed by averaging over the 10 folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9995335340499878}]}, {"text": "We used the NLTK 3 implementation of the four similarity measures wup, res, lin or jcn.", "labels": [], "entities": []}, {"text": "The idf values for each word were computed from frequency counts over the entire Wikipedia.", "labels": [], "entities": []}, {"text": "For each question, the focus is identified automatically by an SVM tagger trained on a separate corpus of 2,000 questions manually annotated with focus information ().", "labels": [], "entities": []}, {"text": "The SVM tagger uses a combination of lexico-syntactic features and a quadratic kernel to achieve a 93.5% accuracy in a 10-fold cross validation evaluation on the 2,000 questions.", "labels": [], "entities": [{"text": "SVM tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.7137532234191895}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9981708526611328}]}, {"text": "The head-modifier dependencies were derived automatically from the syntactic parse tree using the head finding rules from.", "labels": [], "entities": []}, {"text": "The syntactic tree is obtained using Spear 4 , a syntactic parser which comes pre-trained on an additional treebank of questions.", "labels": [], "entities": []}, {"text": "The main verb of a question is identified deterministically using a breadth first traversal of the dependency tree.", "labels": [], "entities": []}, {"text": "The overall accuracy results presented in show that the SVM ranking model obtains by far the best performance on both datasets, a substantial 10% higher than cos, which is the best performing unsupervised method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995231628417969}]}, {"text": "The random baseline -assigning a random similarity value to each pair of questionsresults in 50% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9994144439697266}]}, {"text": "Even though its use of word senses was expected to lead to superior results, mcs does not perform better than cos on this dataset.", "labels": [], "entities": []}, {"text": "Our implementation of mcs did however perform better than cos on the Microsoft paraphrase corpus ().", "labels": [], "entities": [{"text": "Microsoft paraphrase corpus", "start_pos": 69, "end_pos": 96, "type": "DATASET", "confidence": 0.6917638381322225}]}, {"text": "One possible reason for this behavior is that mcs seems to be less resilient than cos to differences in question length.", "labels": [], "entities": []}, {"text": "Whereas the Microsoft paraphrase corpus was specifically designed such that \"the length of the shorter of the two sentences, in words, is at least 66% that of the longer\"), the question ranking datasets place no constraints on the lengths of the   questions.", "labels": [], "entities": []}, {"text": "However, even though by themselves the meaning aware mcs and the structure-and-meaning aware \u03c6 t do not outperform the bag-of-words cos, they do help in increasing the performance of the SVM ranking model, as can be inferred from the corresponding columns in If we take Q 32 as reference question, the fact that the distance between Los Angeles and Anaheim is smaller than the distance between Vista and Anaheim leads the ranking system to rank Q 33 as more useful than Q 34 with respect to Q 32 , which is the expected result.", "labels": [], "entities": []}, {"text": "The preposition \"around\" from the city context in the first pattern is a good indicator that proximity relations are relevant in this case.", "labels": [], "entities": []}, {"text": "When the same three cities are used for instantiating the other two patterns, it can be seen that the proximity relations are no longer as relevant for judging the relative usefulness of questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Pairwise accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9858617782592773}]}]}