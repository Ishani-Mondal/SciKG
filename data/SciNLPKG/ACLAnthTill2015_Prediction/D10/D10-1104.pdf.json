{"title": [{"text": "SRL-based Verb Selection for ESL 1,2", "labels": [], "entities": [{"text": "SRL-based Verb Selection", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8713885148366293}]}], "abstractContent": [{"text": "In this paper we develop an approach to tackle the problem of verb selection for learners of English as a second language (ESL) by using features from the output of Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "verb selection", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7152564525604248}, {"text": "Semantic Role Labeling (SRL)", "start_pos": 165, "end_pos": 193, "type": "TASK", "confidence": 0.7872716883818308}]}, {"text": "Unlike existing approaches to verb selection that use local features such as n-grams, our approach exploits semantic features which explicitly model the usage context of the verb.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7217072248458862}]}, {"text": "The verb choice highly depends on its usage context which is not consistently captured by local features.", "labels": [], "entities": []}, {"text": "We then combine these semantic features with other local features under the generalized perceptron learning framework.", "labels": [], "entities": []}, {"text": "Experiments on both in-domain and out-of-domain corpora show that our approach outperforms the baseline and achieves state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Verbs in English convey actions or states of being.", "labels": [], "entities": []}, {"text": "In addition, they also communicate sentiments and imply circumstances, e.g., in \"He got the scholarship after three interviews.\", the verb \"gained\" may indicate that the \"scholarship\" was competitive and required the agent's efforts; in contrast, \"got\" sounds neutral and less descriptive.", "labels": [], "entities": []}, {"text": "Since verbs carry multiple important functions, misusing them can be misleading, e.g., the native speaker could be confused when reading \"I like looking books\".", "labels": [], "entities": []}, {"text": "Unfortunately, according to), more than 30% of the errors in the Chinese Learner English Corpus (CLEC) are verb choice errors.", "labels": [], "entities": [{"text": "Chinese Learner English Corpus (CLEC)", "start_pos": 65, "end_pos": 102, "type": "DATASET", "confidence": 0.8810205374445234}]}, {"text": "Hence, it is useful to develop an approach to automatically detect and correct verb selection errors made by ESL learners.", "labels": [], "entities": []}, {"text": "However, verb selection is a challenging task because verbs often exhibit a variety of usages and each usage depends on a particular context, which can hardly be adequately described by conventional n-gram features.", "labels": [], "entities": [{"text": "verb selection", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7509023249149323}]}, {"text": "For instance, both \"made\" and \"received\" can complete \"I have __ a telephone call.\", where the usage context can be represented as \"made/received a telephone call\"; however, in \"I have __ a telephone call from my boss\", the prepositional phrase \"from my boss\" becomes a critical part of the context, which now cannot be described by n-gram features, resulting in only \"received\" being suitable.", "labels": [], "entities": []}, {"text": "Some researchers exploited syntactic information and n-gram features to represent verb usage context.", "labels": [], "entities": []}, {"text": "introduced an unsupervised web-based proofing method for correcting verb-noun collocation errors.", "labels": [], "entities": [{"text": "correcting verb-noun collocation errors", "start_pos": 57, "end_pos": 96, "type": "TASK", "confidence": 0.8399933129549026}]}, {"text": "employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors.", "labels": [], "entities": [{"text": "phrasal Statistical Machine Translation (SMT)", "start_pos": 9, "end_pos": 54, "type": "TASK", "confidence": 0.74986332654953}]}, {"text": "None of their methods incorporated semantic information.", "labels": [], "entities": []}, {"text": "Unlike the other papers, we derive features from the output of an SRL) system to explicitly model verb usage context.", "labels": [], "entities": []}, {"text": "SRL is generally understood as the task of identifying the arguments of a given verb and assigning them semantic labels describing the roles they play.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9680307507514954}]}, {"text": "For example, given a sentence \"I want to watch TV tonight\" and the target predicate \"watch\", the output of SRL will be something like \"I [A0] want to watch TV tonight.\", meaning that the action \"watch\" is conducted by the agent \"I\", on the patient \"TV\", and the action happens \"tonight\".", "labels": [], "entities": [{"text": "SRL", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.5115623474121094}]}, {"text": "We believe that SRL results are excellent features for characterizing verb usage context for three reasons: (i) Intuitively, the predicateargument structures generated by SRL systems capture major relationships between a verb and its contextual participants and consequently largely determine whether or not the verb usage is proper.", "labels": [], "entities": [{"text": "SRL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9677927494049072}]}, {"text": "For example, in \"I want to watch a match tonight.\", \"match\" is the patient of \"watch\", and \"watch \u2026 match\" forms a collocation, suggesting \"watch\" is appropriately used.", "labels": [], "entities": []}, {"text": "(ii) Predicate-argument structures abstract away syntactic differences in sentences with similar meanings, and therefore can potentially filter out lots of noise from the usage context.", "labels": [], "entities": []}, {"text": "For example, consider \"I want to watch a football match on TV tonight\": if \"match\" is successfully identified as the agent of \"watch\", \"watch \u2026 football\", which is unrelated to the usage of \"watch\" in this case, can be easily excluded from the usage context.", "labels": [], "entities": []}, {"text": "(iii) Research on SRL has made great achievements, including humanannotated training corpora and state-of-the-art systems, which can be directly leveraged.", "labels": [], "entities": [{"text": "SRL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9709070920944214}]}, {"text": "Taking an English sentence as input, our method first generates correction candidates by replacing each verb with verbs in its pre-defined confusion set; then for every candidate, it extracts SRLderived features; finally our method scores every candidate using a linear function trained by the generalized perceptron learning algorithm and selects the best candidate as output.", "labels": [], "entities": []}, {"text": "Experimental results show that SRL-derived features are effective in verb selection, but we also observe that noise in SRL output adversely increases feature space dimensions and the number of false suggestions.", "labels": [], "entities": [{"text": "SRL-derived", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9423626661300659}, {"text": "verb selection", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.8295713365077972}, {"text": "SRL", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9741887450218201}]}, {"text": "To alleviate this issue, we use local features, e.g., n-gram-related features, and achieve state-of-the-art performance when all features are integrated.", "labels": [], "entities": []}, {"text": "Our contributions can be summarized as follows: 1.", "labels": [], "entities": []}, {"text": "We propose to exploit SRL-derived features to explicitly model verb usage context.", "labels": [], "entities": [{"text": "SRL-derived", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9267061352729797}]}, {"text": "2. We propose to use the generalized perceptron framework to integrate SRL-derived (and other) features and achieve state-ofthe-art performance on both in-domain and out-of-domain test sets.", "labels": [], "entities": []}, {"text": "Our paper is organized as follows: In the next section, we introduce related work.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our method.", "labels": [], "entities": []}, {"text": "Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude our paper with a discussion of future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare our approach with the SMT-based approach.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.9935051202774048}]}, {"text": "Furthermore, we study the contribution of predicate-argument-related features, and the performances on verbs with varying distance to their arguments.", "labels": [], "entities": []}, {"text": "The training corpus for perceptron learning was taken from LDC2005T12.", "labels": [], "entities": [{"text": "perceptron learning", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7958762943744659}, {"text": "LDC2005T12", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.9574587345123291}]}, {"text": "We randomly selected newswires containing target verbs from the New York Times as the training data.", "labels": [], "entities": []}, {"text": "We then used the OpenNLP package 31 to extract sentences from the newswire text and to parse them into the corresponding tokens, POS tags, and chunks.", "labels": [], "entities": [{"text": "OpenNLP package 31", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.9314631621042887}]}, {"text": "The SRL system is built according to, using the CoNLL-2008 shared task data for training.", "labels": [], "entities": [{"text": "SRL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7803188562393188}, {"text": "CoNLL-2008 shared task data", "start_pos": 48, "end_pos": 75, "type": "DATASET", "confidence": 0.9055068641901016}]}, {"text": "We assume that the newswire data is of high quality and free of linguistic errors, and finally we gathered 20000 sentences that contain any of the target verbs we were focusing on.", "labels": [], "entities": [{"text": "newswire data", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9145969152450562}]}, {"text": "We experimentally set the number of training rounds to T = 50.", "labels": [], "entities": [{"text": "T", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9982959628105164}]}, {"text": "We constructed two sets of testing data for indomain and out-of-domain test purposes, respectively.", "labels": [], "entities": []}, {"text": "To construct the in-domain test data, we first collected all the sentences that contain any of the verbs we were interested in from the previous unused LDC dataset; then we replaced any target verb in our list with a verb in its confusion set; next, we used the language-model-based pruning strategy described in 3.4 to drop possibly correct manipulations from the test data; and finally we randomly sampled 5000 sentences for testing.", "labels": [], "entities": [{"text": "LDC dataset", "start_pos": 152, "end_pos": 163, "type": "DATASET", "confidence": 0.895124077796936}]}, {"text": "To build the out-of-domain test dataset, we gathered 186 samples that contained errors related to the verbs we were interested in from English blogs written by Chinese and from the CLEC corpus, which were then corrected by an English native speaker.", "labels": [], "entities": [{"text": "CLEC corpus", "start_pos": 181, "end_pos": 192, "type": "DATASET", "confidence": 0.911761462688446}]}, {"text": "Furthermore, for every error involving the verbs in our target list, both the verb and the word that determines the error are marked by the English native speaker.", "labels": [], "entities": []}, {"text": "We employed the following metrics adapted from (: revised precision (RP), recall of the correction (RC) and false alarm (FA).", "labels": [], "entities": [{"text": "revised precision (RP)", "start_pos": 50, "end_pos": 72, "type": "METRIC", "confidence": 0.8659407138824463}, {"text": "recall of the correction (RC)", "start_pos": 74, "end_pos": 103, "type": "METRIC", "confidence": 0.9325443080493382}, {"text": "false alarm (FA)", "start_pos": 108, "end_pos": 124, "type": "METRIC", "confidence": 0.9138020873069763}]}, {"text": "RP reflects how many outputs are correct usages.", "labels": [], "entities": [{"text": "RP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5473999977111816}]}, {"text": "The output is regarded as a correct suggestion if and only if it is exactly the same as the answer.", "labels": [], "entities": []}, {"text": "Paraphrasing scenarios, for example, the case that the output is \"take notes\" and the answer is \"make notes\", are counted as errors.", "labels": [], "entities": []}, {"text": "FA is related to the cases where a correct verb is mistakenly replaced by an inappropriate one.", "labels": [], "entities": [{"text": "FA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9855560660362244}]}, {"text": "These false suggestions are likely to disturb or even annoy users, and thus should be avoided as much as possible.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 9. Performance on verbs with different distance to  their arguments on out-of-domain test data.", "labels": [], "entities": []}, {"text": " Table 7. In-domain test results.", "labels": [], "entities": []}, {"text": " Table 8. Out-of-domain test results.", "labels": [], "entities": []}]}