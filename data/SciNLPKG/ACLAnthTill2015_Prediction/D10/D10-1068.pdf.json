{"title": [], "abstractContent": [{"text": "Parser disambiguation with precision grammars generally takes place via statistical ranking of the parse yield of the grammar using a supervised parse selection model.", "labels": [], "entities": [{"text": "Parser disambiguation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9272737801074982}]}, {"text": "In the standard process, the parse selection model is trained over a hand-disambiguated treebank, meaning that without a significant investment of effort to produce the treebank, parse selection is not possible.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.9570169448852539}, {"text": "parse selection", "start_pos": 179, "end_pos": 194, "type": "TASK", "confidence": 0.9754392802715302}]}, {"text": "Furthermore, as treebank-ing is generally streamlined with parse selection models, creating the initial treebank without a model requires more resources than subsequent treebanks.", "labels": [], "entities": []}, {"text": "In this work, we show that, by taking advantage of the constrained nature of these HPSG grammars, we can learn a dis-criminative parse selection model from raw text in a purely unsupervised fashion.", "labels": [], "entities": []}, {"text": "This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parsing with precision grammars is generally a twostage process: (1) the full parse yield of the precision grammar is calculated fora given item, often in the form of a packed forest for efficiency; and (2) the individual analyses in the parse forest are ranked using a statistical model (\"parse selection\").", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9822841882705688}]}, {"text": "In the domain of treebank parsing, the reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first stage.", "labels": [], "entities": [{"text": "treebank parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.6140114367008209}]}, {"text": "For both styles of parsing, however, parse selection is based on a statistical model learned from a pre-existing treebank associated with the grammar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9631479978561401}, {"text": "parse selection", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.9647843539714813}]}, {"text": "Our interest in this paper is in completely removing this requirement of parse selection on explicitly treebanked data, ie the development of fully unsupervised parse selection models.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.973393052816391}]}, {"text": "The particular style of precision grammar we experiment within this paper is HPSG, in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/).", "labels": [], "entities": []}, {"text": "One of the main focuses of the DELPH-IN collaboration effort is multilinguality.", "labels": [], "entities": [{"text": "DELPH-IN collaboration", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7375199198722839}]}, {"text": "To this end, the Grammar Matrix project ) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar fora language of their choice.", "labels": [], "entities": []}, {"text": "showed that by using and expanding on this core grammar, she was able to produce a broad-coverage precision grammar of Wambaya in a very short amount of time.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9452766180038452}]}, {"text": "However, the Grammar Matrix can only help with the first stage of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9689314365386963}]}, {"text": "The statistical model used in the second stage of parsing (ie parse selection) requires a treebank to learn the features, but as we explain in Section 2, the treebanks are created by parsing, preferably with a statistical model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9772835373878479}]}, {"text": "In this work, we look at methods for bootstrapping the production of these statistical models without having an annotated treebank.", "labels": [], "entities": []}, {"text": "Since many of the languages that people are building new grammars for are under-resourced, we can't depend on having any external information or NLP tools, and so the methods we examine are purely unsupervised, using nothing more than the grammars them-selves and raw text.", "labels": [], "entities": []}, {"text": "We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data, which have been successfully used in applications ( ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9992402791976929}]}], "datasetContent": [{"text": "Our ultimate goal is to use these methods for underresourced languages but, since there are no preexisting treebanks for these languages, we have no means to measure which method produces the best results.", "labels": [], "entities": []}, {"text": "Hence, in this work, we experiment with languages and grammars where we have gold standard data, in order to be able to evaluate the quality of the parse selection models.", "labels": [], "entities": []}, {"text": "Since we have gold-standard trained models to compare with, this enables us to fully explore how these unsupervised methods work, and show which methods are worth trying in the more time-consuming and resourceintensive future experiments on other languages.", "labels": [], "entities": []}, {"text": "It is worth reinforcing that the gold-standard data is used for evaluation only, except in calculating the supervised parse selection accuracy as an upperbound.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.7229242920875549}]}, {"text": "The English Resource Grammar (ERG: Flickinger) is an HPSG-based grammar of English that has been underdevelopment for many person years.", "labels": [], "entities": [{"text": "English Resource Grammar (ERG: Flickinger)", "start_pos": 4, "end_pos": 46, "type": "DATASET", "confidence": 0.7632872015237808}]}, {"text": "In order to examine the cross-lingual applicability of our methods, we also use Jacy, an HPSG-based grammar of Japanese ().", "labels": [], "entities": [{"text": "HPSG-based grammar of Japanese", "start_pos": 89, "end_pos": 119, "type": "DATASET", "confidence": 0.8851987421512604}]}, {"text": "In both cases, we use grammar versions from the \"Barcelona\" release, from mid-2009.", "labels": [], "entities": [{"text": "Barcelona\" release", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.9526024659474691}]}, {"text": "The exact match metric is the most common accuracy metric used in work with the DELPH-IN tool set, and refers to the percentage of sentences for which the top parse matched the gold parse in every way.", "labels": [], "entities": [{"text": "exact match metric", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9351712465286255}, {"text": "accuracy metric", "start_pos": 42, "end_pos": 57, "type": "METRIC", "confidence": 0.9749662578105927}, {"text": "DELPH-IN tool set", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9284512996673584}]}, {"text": "This is akin to the sentence accuracy that is occasionally reported in the parsing literature, except that it also includes fine-grained syntactico-semantic features that are not often present in other parsing frameworks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9702483415603638}]}, {"text": "Exact match is a useful metric for parse selection evaluation, but it is very blunt-edged, and gives noway of evaluating how close the top parse was to the gold standard.", "labels": [], "entities": [{"text": "Exact match", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9448984563350677}, {"text": "parse selection evaluation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.9820488095283508}]}, {"text": "Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis.", "labels": [], "entities": []}, {"text": "Hence, in addition to exact match, we also use the EDM N A evaluation defined by.", "labels": [], "entities": [{"text": "EDM N A evaluation", "start_pos": 51, "end_pos": 69, "type": "METRIC", "confidence": 0.6660865545272827}]}, {"text": "This is a predicateargument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics ().", "labels": [], "entities": []}, {"text": "This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar ( , and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank.", "labels": [], "entities": [{"text": "ENJU grammar", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.9471186101436615}, {"text": "grammatical relations (GR)", "start_pos": 177, "end_pos": 203, "type": "METRIC", "confidence": 0.6793883740901947}, {"text": "DepBank", "start_pos": 249, "end_pos": 256, "type": "DATASET", "confidence": 0.7603961229324341}]}, {"text": "The EDM N A metric matches triples consisting of predicate names and the argument type that connects them.", "labels": [], "entities": []}, {"text": "All of our experiments are based on the same basic process: (1) for each sentence in the training data described in Section 3.1, label a subset of analyses as correct and the remainder as incorrect; (2) train a model using the same features and learner as in the standard process of Section 2; (3) parse the test data using that model; and (4) evaluate the accuracy of the top analyses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 357, "end_pos": 365, "type": "METRIC", "confidence": 0.9991509914398193}]}, {"text": "The differences lay in how the 'correct' analyses are selected each time.", "labels": [], "entities": []}, {"text": "Each of the following sections detail different methods for nominating which of the (up to 500) analyses from the training data should be considered pseudo-gold for training the parse selection model.", "labels": [], "entities": []}, {"text": "The term supertags was first used by to describe fine-grained part of speech tags which include some structural or dependency information.", "labels": [], "entities": []}, {"text": "In that original work, the supertags were LTAG ( elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types.", "labels": [], "entities": [{"text": "parsing", "start_pos": 117, "end_pos": 124, "type": "TASK", "confidence": 0.9745250344276428}]}, {"text": "Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest.", "labels": [], "entities": [{"text": "parse selection.", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.9805873930454254}]}, {"text": "This work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse.", "labels": [], "entities": []}, {"text": "present a similar exploration but also goon to include probabilities from a HMM model into the parse selection model as features.", "labels": [], "entities": []}, {"text": "There has also been some work on using lexical probabilities for domain adaptation of a model (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7073390483856201}]}, {"text": "In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9873859882354736}]}, {"text": "We use a similar method, one level removed, in that we use the tag sequences to select the 'gold' parse(s) that are then used to train a model, as in the previous sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Initial model training data, showing the average  word length per sentence, and also the ambiguity mea- sured as the average number of parses found per sentence.", "labels": [], "entities": [{"text": "ambiguity mea- sured", "start_pos": 99, "end_pos": 119, "type": "METRIC", "confidence": 0.8231605291366577}]}, {"text": " Table 2. The tc-006 data set is from Test Set Language Sentences  Average Average  words  parses  tc-006  Japanese  904  10.7  383.9  jhpstg t  English  748  12.8  4115.1  catb  English  534  17.6  9427.3", "labels": [], "entities": [{"text": "tc-006 data set", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.7979004383087158}, {"text": "Japanese  904  10.7  383.9  jhpstg t  English  748  12.8  4115.1  catb  English  534  17.6  9427.3", "start_pos": 107, "end_pos": 205, "type": "DATASET", "confidence": 0.8843225081761678}]}, {"text": " Table 3: Accuracy of the gold standard-based parse se- lection model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963017702102661}]}, {"text": " Table 4: Accuracy of the baseline model, trained on ran- domly selected pseudo-gold analyses.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9916875958442688}]}, {"text": " Table 5: Accuracy for each test set, measured both as per- centage of sentences that exactly matched the gold stan- dard, and f-score over elementary dependencies.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99928879737854}]}, {"text": " Table 6: Accuracy using gold tag sequence compatibility  to select the 'gold' parse(s).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9752771854400635}]}, {"text": " Table 7: Training data for the HMM tagger (both the  parsed data from which the initial probabilities were de- rived, and the raw data which was used to estimated the  EM trained models).", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.6208883821964264}]}, {"text": " Table 8: Accuracy using tag sequences from a HMM tag- ger to select the 'correct' parse(s). The initial counts  model was based on using counts from a parse forest  to approximate the emission and transition probabilities.  The EM trained model used the Baum Welch algorithm to  estimate the probabilities, starting from the initial counts  state.", "labels": [], "entities": []}, {"text": " Table 9: Tagger accuracy over the training data, using  both the initial counts and the EM trained models.", "labels": [], "entities": [{"text": "Tagger", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9754893183708191}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9864320158958435}]}, {"text": " Table 10: Accuracy results over the out-of-domain catb  data set, using the initial counts unsupervised model to  produce in-domain training data in a self-training set up.  The previous results are shown for easy comparison.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9844921827316284}, {"text": "catb  data set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.7869243919849396}]}]}