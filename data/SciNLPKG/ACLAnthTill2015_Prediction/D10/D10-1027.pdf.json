{"title": [{"text": "Efficient Incremental Decoding for Tree-to-String Translation", "labels": [], "entities": []}], "abstractContent": [{"text": "Syntax-based translation models should in principle be efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration.", "labels": [], "entities": [{"text": "Syntax-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.64264976978302}, {"text": "language model integration", "start_pos": 180, "end_pos": 206, "type": "TASK", "confidence": 0.6243727008501688}]}, {"text": "In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in average-case polynomial-time in theory, and linear-time with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice).", "labels": [], "entities": []}, {"text": "Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++).", "labels": [], "entities": []}], "introductionContent": [{"text": "Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.65395587682724}]}, {"text": "From a theoretical point of view, phrasebased models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order.", "labels": [], "entities": []}, {"text": "In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm.", "labels": [], "entities": []}, {"text": "In practice, the decoder has to employ beam search to make it tractable).", "labels": [], "entities": []}, {"text": "However, even beam search runs in quadratic-time in general (see Sec.", "labels": [], "entities": [{"text": "beam search", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9330811500549316}]}, {"text": "2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a \"jump\" in theory in practice phrase-based exponential quadratic tree-to-string polynomial linear: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based.", "labels": [], "entities": [{"text": "Time", "start_pos": 285, "end_pos": 289, "type": "METRIC", "confidence": 0.9562813639640808}]}, {"text": "In practice means \"approximate search with beams.\" longer than d.", "labels": [], "entities": []}, {"text": "This has been the standard practice with phrase-based models (, which fails to capture important long-distance reorderings like SVO-to-SOV.", "labels": [], "entities": []}, {"text": "Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars.", "labels": [], "entities": []}, {"text": "In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based).", "labels": [], "entities": []}, {"text": "But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice.", "labels": [], "entities": []}, {"text": "Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation, whereas a phrase-based decoder only needs to do this atone end since the translation is always growing left-to-right.", "labels": [], "entities": [{"text": "language model integration", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6808075110117594}]}, {"text": "As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful.", "labels": [], "entities": []}, {"text": "Can we combine the merits of both approaches?", "labels": [], "entities": []}, {"text": "While other authors have explored the possibilities of enhancing phrase-based decoding with syntaxaware reordering (, we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model).", "labels": [], "entities": []}, {"text": "However, this algorithm even with the beam search still runs in quadratic-time in practice.", "labels": [], "entities": []}, {"text": "Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice.", "labels": [], "entities": []}, {"text": "We take afresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (), since this is the simplest and fastest among syntax-based approaches.", "labels": [], "entities": [{"text": "treeto-string translation", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.7462390065193176}]}, {"text": "We develop an incremental dynamic programming algorithm and make the following contributions: \u2022 we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see); \u2022 large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++) (); \u2022 furthermore, on the same tree-to-string system, incremental decoding is slightly faster than the standard cube pruning method at the same level of translation quality; \u2022 this is also the first linear-time incremental decoder that performs global reordering.", "labels": [], "entities": []}, {"text": "We will first briefly review phrase-based decoding in this section, which inspires our incremental algorithm in the next section.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses.", "labels": [], "entities": []}, {"text": "Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Final BLEU score and speed results on the test  data (691 sentences), compared with Moses and cube  pruning. Time is in seconds per sentence, including pars- ing time (0.21s) for the two tree-to-string decoders.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9411801695823669}, {"text": "speed", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9776925444602966}, {"text": "Time", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9553945660591125}, {"text": "pars- ing time", "start_pos": 162, "end_pos": 176, "type": "METRIC", "confidence": 0.9145081043243408}]}]}