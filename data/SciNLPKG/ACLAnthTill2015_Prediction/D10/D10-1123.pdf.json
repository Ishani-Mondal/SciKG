{"title": [{"text": "Identifying Functional Relations in Web Text", "labels": [], "entities": [{"text": "Identifying Functional Relations in Web Text", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8567030231157938}]}], "abstractContent": [{"text": "Determining whether a textual phrase denotes a functional relation (i.e., a relation that maps each domain element to a unique range element) is useful for numerous NLP tasks such as synonym resolution and contradiction detection.", "labels": [], "entities": [{"text": "synonym resolution", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.9698950946331024}, {"text": "contradiction detection", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.7887489199638367}]}, {"text": "Previous work on this problem has relied on either counting methods or lexico-syntactic patterns.", "labels": [], "entities": []}, {"text": "However, determining whether a relation is functional, by analyzing mentions of the relation in a corpus, is challenging due to ambiguity, synonymy, anaphora, and other linguistic phenomena.", "labels": [], "entities": []}, {"text": "We present the LEIBNIZ system that overcomes these challenges by exploiting the syn-ergy between the Web corpus and freely-available knowledge resources such as Free-base.", "labels": [], "entities": []}, {"text": "It first computes multiple typed function-ality scores, representing functionality of the relation phrase when its arguments are constrained to specific types.", "labels": [], "entities": []}, {"text": "It then aggregates these scores to predict the global functionality for the phrase.", "labels": [], "entities": []}, {"text": "LEIBNIZ outperforms previous work, increasing area under the precision-recall curve from 0.61 to 0.88.", "labels": [], "entities": [{"text": "LEIBNIZ", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9763656854629517}, {"text": "area", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9690245389938354}, {"text": "precision-recall curve", "start_pos": 61, "end_pos": 83, "type": "METRIC", "confidence": 0.9691889882087708}]}, {"text": "We utilize LEIBNIZ to generate the first public repository of automatically-identified functional relations .", "labels": [], "entities": []}], "introductionContent": [{"text": "The paradigm of Open Information Extraction (IE) ( has scaled extraction technology to the massive set of relations expressed in Web text.", "labels": [], "entities": [{"text": "Open Information Extraction (IE)", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.795871709783872}]}, {"text": "However, additional work is needed to better understand these relations, and to place them in richer semantic structures.", "labels": [], "entities": []}, {"text": "A step in that direction is identifying the properties of these relations, e.g., symmetry, transitivity and our focus in this paper -functionality.", "labels": [], "entities": []}, {"text": "We refer to this problem as functionality identification.", "labels": [], "entities": [{"text": "functionality identification", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7582021355628967}]}, {"text": "A binary relation is functional if, fora given arg1, there is exactly one unique value for arg2.", "labels": [], "entities": []}, {"text": "Examples of functional relations are father, death date, birth city, etc.", "labels": [], "entities": []}, {"text": "We define a relation phrase to be functional if all semantic relations commonly expressed by that phrase are functional.", "labels": [], "entities": []}, {"text": "For example, we say that the phrase 'was born in' denotes a functional relation, because the different semantic relations expressed by the phrase (e.g., birth city, birth year, etc.) are all functional.", "labels": [], "entities": []}, {"text": "Knowing that a relation is functional is helpful for numerous NLP inference tasks.", "labels": [], "entities": []}, {"text": "Previous work has used functionality for the tasks of contradiction detection (, quantifier scope disambiguation, and synonym resolution.", "labels": [], "entities": [{"text": "contradiction detection", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.7515290677547455}, {"text": "quantifier scope disambiguation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.6471019685268402}, {"text": "synonym resolution", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.9313565194606781}]}, {"text": "It could also aid in other tasks such as ontology generation and information extraction.", "labels": [], "entities": [{"text": "ontology generation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.904725968837738}, {"text": "information extraction", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8724435865879059}]}, {"text": "For example, consider two sentences from a contradiction detection task: (1) \"George Washington was born in Virginia.\" and (2) \"George Washington was born in Texas.\"", "labels": [], "entities": [{"text": "contradiction detection task", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.8214346965154012}]}, {"text": "As points out, we can only determine that the two sentences are contradictory if we know that the semantic relation referred to by the phrase 'was born in' is functional, and that both Virginia and Texas are distinct states.", "labels": [], "entities": []}, {"text": "Automatic functionality identification is essential when dealing with a large number of relations as in Open IE, or in complex domains where expert help is scarce or expensive (e.g., biomedical texts).", "labels": [], "entities": [{"text": "Automatic functionality identification", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.622375508149465}]}, {"text": "This paper tackles automatic functionality identification using Web text.", "labels": [], "entities": [{"text": "automatic functionality identification", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.6175584097703298}]}, {"text": "While functionality identification has been utilized as a module in various NLP systems, this is the first paper to focus exclusively on functionality identification as a bona fide NLP inference task.", "labels": [], "entities": [{"text": "functionality identification", "start_pos": 6, "end_pos": 34, "type": "TASK", "confidence": 0.7959264516830444}, {"text": "functionality identification", "start_pos": 137, "end_pos": 165, "type": "TASK", "confidence": 0.712712287902832}]}, {"text": "It is natural to identify functions based on triples extracted from text instead of analyzing sentences directly.", "labels": [], "entities": []}, {"text": "Thus, as our input, we utilize tuples extracted by TEXTRUNNER ( when run over a corpus of 500 million webpages.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.6563223004341125}]}, {"text": "TEXTRUNNER maps sentences to tuples of the form <arg1, relation phrase, arg2> and enables our LEIBNIZ system to focus on the problem of deciding whether the relation phrase is a function.", "labels": [], "entities": []}, {"text": "The naive approach, which classifies a relation phrase as non-functional if several arg1s have multiple arg2s in our extraction set, fails due to several reasons: synonymy -a unique entity maybe referred by multiple strings, polysemy of both entities and relations -a unique string may refer to multiple entities/relations, metaphorical usage, extraction errors and more.", "labels": [], "entities": []}, {"text": "These phenomena conspire to make the functionality determination task inherently statistical and surprisingly challenging.", "labels": [], "entities": [{"text": "functionality determination task", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.8610411683718363}]}, {"text": "In addition, a functional relation phrase may appear non-functional until we consider the types of its arguments.", "labels": [], "entities": []}, {"text": "In our 'was born in' example, <George Washington, was born in, 1732> does not contradict <George Washington, was born in, Virginia> even though we see two distinct arg2s for the same arg1.", "labels": [], "entities": [{"text": "arg2s", "start_pos": 164, "end_pos": 169, "type": "METRIC", "confidence": 0.9634937047958374}, {"text": "arg1", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9586194753646851}]}, {"text": "To solve functionality identification, we need to consider typed relations where the relations analyzed are constrained to have specific argument types.", "labels": [], "entities": [{"text": "functionality identification", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.7745122015476227}]}, {"text": "We develop several approaches to overcome these challenges.", "labels": [], "entities": []}, {"text": "Our first scheme employs approximate argument merging to overcome the synonymy and anaphora problems.", "labels": [], "entities": [{"text": "approximate argument merging", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6065283815066019}]}, {"text": "Our second approach, DIS-TRDIFF, takes a statistical view of the problem and learns a separator for the typical count distributions of functional versus non-functional relations.", "labels": [], "entities": []}, {"text": "Finally, our third and most successful scheme, CLEANLISTS, identifies and processes a cleaner subset of the data by intersecting the corpus with entities in a secondary knowledge-base (in our case, Freebase).", "labels": [], "entities": []}, {"text": "Utilizing pre-defined types, CLEANLISTS first identifies typed functionality for suitable types for that relation phrase, and then combines them to output a final functionality label.", "labels": [], "entities": []}, {"text": "LEIBNIZ, a hybrid of CLEAN-LISTS and DISTRDIFF, returns state-of-the-art results for our task.", "labels": [], "entities": [{"text": "LEIBNIZ", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9700498580932617}, {"text": "DISTRDIFF", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.8529127240180969}]}, {"text": "Our work makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We identify several linguistic phenomena that make the problem of corpus-based functionality identification surprisingly difficult.", "labels": [], "entities": [{"text": "corpus-based functionality identification", "start_pos": 66, "end_pos": 107, "type": "TASK", "confidence": 0.721311092376709}]}, {"text": "2. We designed and implemented three novel techniques for identifying functionality based on instance-based counting, distributional differences, and use of external knowledge bases.", "labels": [], "entities": []}, {"text": "3. Our best method, LEIBNIZ, outperforms the existing approaches by wide margins, increasing area under the precision-recall curve from 0.61 to 0.88.", "labels": [], "entities": [{"text": "LEIBNIZ", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9865007996559143}, {"text": "precision-recall curve", "start_pos": 108, "end_pos": 130, "type": "METRIC", "confidence": 0.9650912880897522}]}, {"text": "It is also capable of distinguishing functionality of typed relation phrases, when the arguments are restricted to specific types.", "labels": [], "entities": []}, {"text": "4. Utilizing LEIBNIZ, we created the first public repository of functional relations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our evaluation, we wish to answer three questions: (1) How do our three approaches, Instance Based Counting (IBC), DISTRDIFF, and CLEAN-LISTS, compare on the functionality identification task?", "labels": [], "entities": [{"text": "Instance Based Counting (IBC)", "start_pos": 87, "end_pos": 116, "type": "METRIC", "confidence": 0.7136059651772181}, {"text": "DISTRDIFF", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.8000640869140625}, {"text": "functionality identification task", "start_pos": 161, "end_pos": 194, "type": "TASK", "confidence": 0.8180494507153829}]}, {"text": "(2) How does our final system, LEIBNIZ, compare against the existing state of the art techniques?", "labels": [], "entities": [{"text": "LEIBNIZ", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.986423671245575}]}, {"text": "(3) How well is LEIBNIZ able to identify typed functionality for different types in the same relation phrase?", "labels": [], "entities": []}, {"text": "For our experiments we test on the set of 887 relations used by in their experiments.", "labels": [], "entities": []}, {"text": "We use the Open IE corpus generated by running TEXTRUNNER on 500 million high quality Webpages ( as the source of instance data for these relations.", "labels": [], "entities": [{"text": "Open IE corpus generated", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.7104843333363533}]}, {"text": "Extractor and corpus differences lead to some relations not occurring (or not occurring with sufficient frequency to properly analyze, i.e., \u2265 5 arg1 with \u2265 10 evidence), leaving a dataset of 629 relations on which to test.", "labels": [], "entities": []}, {"text": "Two human experts tagged these relations for functionality.", "labels": [], "entities": []}, {"text": "Tagging the functionality of relation phrases can be a bit subjective, as it requires the experts to imagine the various senses of a phrase and judge functionality overall those senses.", "labels": [], "entities": [{"text": "Tagging the functionality of relation phrases", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7333332796891531}]}, {"text": "The inter-annotator agreement between the experts was 95.5%.", "labels": [], "entities": []}, {"text": "We limit ourselves to the subset of the data on which the two experts agreed (a subset of 601 relation phrases).", "labels": [], "entities": []}, {"text": "In CLEANLISTS a factor that affects the quality of the results is the exact set of lists that is used.", "labels": [], "entities": []}, {"text": "If the lists are not clean, results get noisy.", "labels": [], "entities": []}, {"text": "For example, Freebase's list of films contains 73,000 entries, many of which (e.g., \"Egg\") are not films in their primary senses.", "labels": [], "entities": [{"text": "Freebase's list of films", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.9253292202949523}]}, {"text": "Even with heuristics such as assigning terms to their smallest lists and disqualifying dictionary words that occur from large type lists, there is still significant noise left.", "labels": [], "entities": []}, {"text": "Using LEIBNIZ with a set of 35 clean lists on OCCAM's extraction corpus, we generated a repository of 5,520 typed functional relations.", "labels": [], "entities": [{"text": "OCCAM's extraction corpus", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.9521477967500687}]}, {"text": "To evaluate this resource a human expert tagged a random subset of the top 1,000 relations.", "labels": [], "entities": []}, {"text": "Of these relations 22% were either ill-formed or had non-sensical type constraints.", "labels": [], "entities": []}, {"text": "From the well-formed typed relations the precision was estimated to be 0.8.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9996706247329712}]}, {"text": "About half the errors were due to textual functionality and the rest were LEIBNIZ errors.", "labels": [], "entities": [{"text": "LEIBNIZ errors", "start_pos": 74, "end_pos": 88, "type": "METRIC", "confidence": 0.9549641311168671}]}, {"text": "Some examples of good functions found include isTheSequelTo(videogame) and areTheBirthstoneFor(month).", "labels": [], "entities": []}, {"text": "An example of a textually functional relation found is wasTheFounderOf(company).", "labels": [], "entities": []}, {"text": "This is the first public repository of automaticallyidentified functional relations.", "labels": [], "entities": []}, {"text": "Scaling up our data set forced us to confront new sources of noise including extractor errors, errors due to mismatched types, and errors due to sparse evidence.", "labels": [], "entities": []}, {"text": "Still, our initial results are encouraging and we hope that our resource will be valuable as a baseline for future work.", "labels": [], "entities": []}], "tableCaptions": []}