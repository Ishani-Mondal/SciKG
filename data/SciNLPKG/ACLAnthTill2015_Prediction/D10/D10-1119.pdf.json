{"title": [{"text": "Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.", "labels": [], "entities": []}, {"text": "Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.", "labels": [], "entities": []}, {"text": "The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.", "labels": [], "entities": []}, {"text": "We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.", "labels": [], "entities": []}, {"text": "Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9988250136375427}]}], "introductionContent": [{"text": "A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6561545928319296}]}, {"text": "Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations.", "labels": [], "entities": [{"text": "learning semantic parsers given sentences paired with logical meaning representations", "start_pos": 42, "end_pos": 127, "type": "TASK", "confidence": 0.7891658872365952}]}, {"text": "For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: Sentence: which states border texas Meaning: \u03bbx.state(x) \u2227 next to Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning.", "labels": [], "entities": []}, {"text": "Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or both.", "labels": [], "entities": []}, {"text": "Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning.", "labels": [], "entities": []}, {"text": "In addition to data like the above, this approach can also learn from examples such as: Sentence: hangi eyaletin texas ye siniri vardir where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (.", "labels": [], "entities": []}, {"text": "The reason for generalizing to multiple languages is obvious.", "labels": [], "entities": []}, {"text": "The need to learn over multiple representations arises from the fact that there is no standard representation for logical form for natural language.", "labels": [], "entities": []}, {"text": "Instead, existing representations are ad hoc, tailored to the application of interest.", "labels": [], "entities": []}, {"text": "For example, the variable-free representation above was designed for building natural language interfaces to databases.", "labels": [], "entities": []}, {"text": "Our approach works by inducing a combinatory categorial grammar (CCG).", "labels": [], "entities": []}, {"text": "A CCG grammar consists of a language-specific lexicon, whose entries pair individual words and phrases with both syntactic and semantic information, and a universal set of combinatory rules that project that lexicon onto the sentences and meanings of the language via syntactic derivations.", "labels": [], "entities": []}, {"text": "The learning process starts by postulating, for each sentence in the training data, a single multi-word lexical item pairing that sentence with its complete logical form.", "labels": [], "entities": []}, {"text": "These entries are iteratively refined with a restricted higher-order unification procedure) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning.", "labels": [], "entities": []}, {"text": "For the data sets we consider, the space of possible grammars is too large to explicitly enumerate.", "labels": [], "entities": []}, {"text": "The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence.", "labels": [], "entities": []}, {"text": "Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.5649653524160385}]}, {"text": "We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars.", "labels": [], "entities": []}, {"text": "The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence.", "labels": [], "entities": [{"text": "grammar refinement", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7614090740680695}]}, {"text": "We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography.", "labels": [], "entities": [{"text": "US Geography", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9026816785335541}]}, {"text": "We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above.", "labels": [], "entities": []}, {"text": "We also compare performance to previous methods, which are designed with either language-or representation-specific constraints that limit generalization, as discussed in more detail in Section 6.", "labels": [], "entities": []}, {"text": "Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases.", "labels": [], "entities": []}], "datasetContent": [{"text": "Features We use two types of features in our model.", "labels": [], "entities": []}, {"text": "First, we include a set of lexical features: For each lexical item L \u2208 \u039b, we include a feature \u03c6 L that fires when L is used.", "labels": [], "entities": []}, {"text": "Second, we include semantic features that are functions of the output logical expression z.", "labels": [], "entities": []}, {"text": "Each time a predicate pin z takes an argument a with type T (a) in position i it triggers two binary indicator features: \u03c6 (p,a,i) for the predicate-argument relation; and \u03c6 (p,T (a),i) for the predicate argument-type relation.", "labels": [], "entities": []}, {"text": "Initialization The weights for the semantic features are initialized to zero.", "labels": [], "entities": [{"text": "Initialization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9390982985496521}]}, {"text": "The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++) implementation of IBM Model 1.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 134, "end_pos": 145, "type": "DATASET", "confidence": 0.8662921190261841}]}, {"text": "We compute translation scores for (word, constant) pairs that cooccur in examples in the training data.", "labels": [], "entities": []}, {"text": "The initial weight for each \u03c6 L is set to ten times the average score over the (word, constant) pairs in L, except for the weights of seed lexical entries in \u039b NP which are set to 10 (equivalent to the highest possible coocurrence score).", "labels": [], "entities": []}, {"text": "We used the learning rate \u03b1 0 = 1.0 and cooling rate c = 10 \u22125 in all training scenarios, and ran the algorithm for T = 20 iterations.", "labels": [], "entities": [{"text": "learning rate \u03b1 0", "start_pos": 12, "end_pos": 29, "type": "METRIC", "confidence": 0.9135271459817886}, {"text": "cooling rate c", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.948474645614624}]}, {"text": "These values were selected with cross validation on the Geo880 development set, described below.", "labels": [], "entities": [{"text": "Geo880 development set", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9695194760958353}]}, {"text": "Data and Evaluation We evaluate our system on the GeoQuery datasets, which contain naturallanguage queries of a geographical database paired with logical representations of each query's meaning.", "labels": [], "entities": [{"text": "GeoQuery datasets", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.9521035850048065}]}, {"text": "The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following.", "labels": [], "entities": [{"text": "Geo880 dataset", "start_pos": 9, "end_pos": 23, "type": "DATASET", "confidence": 0.9244853854179382}]}, {"text": "The Geo250 dataset is a subset of Geo880 containing 250 sentences that have been translated into Turkish, Spanish and Japanese as well as the original English.", "labels": [], "entities": [{"text": "Geo250 dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9646754562854767}]}, {"text": "Due to the small size of this dataset we use 10-fold cross validation for evaluation.", "labels": [], "entities": []}, {"text": "We use the same folds as and, allowing a direct comparison.", "labels": [], "entities": []}, {"text": "The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper.", "labels": [], "entities": [{"text": "GeoQuery data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9067077934741974}]}, {"text": "We report results for both representations, using the standard measures of Recall (percentage of test sentences assigned correct logical forms), Precision (percentage of logical forms returned that are correct) and F1 (the harmonic mean of Precision and Recall).", "labels": [], "entities": [{"text": "Recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9952231049537659}, {"text": "Precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9455188512802124}, {"text": "F1", "start_pos": 215, "end_pos": 217, "type": "METRIC", "confidence": 0.9997368454933167}]}, {"text": "Two-Pass Parsing To investigate the trade-off between precision and recall, we report results with a two-pass parsing strategy.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9991748929023743}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9954600930213928}]}, {"text": "When the parser fails to return an analysis fora test sentence due to novel words or usage, we reparse the sentence and allow the parser to skip words, with a fixed cost.", "labels": [], "entities": []}, {"text": "Skipping words can potentially increase recall, if the ignored word is an unknown function word that does not contribute semantic content.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9983136653900146}]}], "tableCaptions": [{"text": " Table 1: Performance across languages on Geo250 with  variable-free meaning representations.", "labels": [], "entities": []}, {"text": " Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.", "labels": [], "entities": []}, {"text": " Table 3: Performance on the Geo880 data set, with varied  meaning representations.", "labels": [], "entities": [{"text": "Geo880 data set", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9749755859375}]}]}