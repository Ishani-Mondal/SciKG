{"title": [{"text": "Translingual Document Representations from Discriminative Projections", "labels": [], "entities": [{"text": "Translingual Document Representations from Discriminative Projections", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.8755163053671519}]}], "abstractContent": [{"text": "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categoriza-tion.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.770206868648529}]}, {"text": "We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given the growth of multiple languages on the Internet, Natural Language Processing must operate on dozens of languages.", "labels": [], "entities": []}, {"text": "It is becoming critical that computers reach high performance on the following two tasks: \u2022 Comparable and parallel document retrieval -Cross-language information retrieval and text categorization have become important with the growth of the Web).", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7295199185609818}, {"text": "Cross-language information retrieval", "start_pos": 136, "end_pos": 172, "type": "TASK", "confidence": 0.6775752504666647}, {"text": "text categorization", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.736548125743866}]}, {"text": "In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web ().", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8493521094322205}]}, {"text": "Comparable documents can also be used for learning word-level translation lexicons ().", "labels": [], "entities": [{"text": "learning word-level translation lexicons", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.6826962828636169}]}, {"text": "\u2022 Cross-language text categorization -Applications of text categorization, such as sentiment classification (), are now required to run on multiple languages.", "labels": [], "entities": [{"text": "Cross-language text categorization", "start_pos": 2, "end_pos": 36, "type": "TASK", "confidence": 0.7123996714750925}, {"text": "sentiment classification", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.9312866926193237}]}, {"text": "Categorization is usually trained on the language of the developer: it needs to be easily extended to other languages.", "labels": [], "entities": []}, {"text": "There are two broad approaches to comparable document retrieval and cross-language text categorization.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.6624170243740082}, {"text": "cross-language text categorization", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.6729724307854971}]}, {"text": "One approach is to translate queries or a training set from different languages into a single target language.", "labels": [], "entities": []}, {"text": "Standard monolingual retrieval and classification algorithms can then be applied in the target language.", "labels": [], "entities": []}, {"text": "Alternatively, a cross-language system can project a bag-of-words vector into a translingual lowerdimensional vector space.", "labels": [], "entities": []}, {"text": "Ideally, vectors in this space represent the semantics of a document, independent of the language.", "labels": [], "entities": []}, {"text": "The advantage of pre-translation is that MT systems tend to preserve the meaning of documents.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9905747771263123}]}, {"text": "However, MT can be very slow (more than 1 second per document), preventing its use on large training sets.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9767657518386841}]}, {"text": "When full MT is not practical, a fast word-byword translation model can be used instead,) but maybe less accurate.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9878866672515869}, {"text": "word-byword translation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6913570612668991}]}, {"text": "Conversely, applying a projection into a lowdimensional space is quick.", "labels": [], "entities": []}, {"text": "Linear projection algorithms use matrix-sparse vector multiplication, which can be easily parallelized.", "labels": [], "entities": []}, {"text": "However, as seen in section 3, the accuracies of previous projection techniques are not as high as machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7436417639255524}]}, {"text": "This paper presents two techniques: Oriented PCA and Coupled PLSA.", "labels": [], "entities": []}, {"text": "These techniques retain the high speed of projection, while approaching or exceeding the quality level of word glossing.", "labels": [], "entities": []}, {"text": "We improve the quality of the projections by the use of discriminative training: we minimize the difference between comparable documents in the projected vector space.", "labels": [], "entities": []}, {"text": "Oriented PCA minimizes the difference by modifying the eigensystem of PCA (, while Coupled PLSA uses posterior regularization () on the topic assignments of the comparable documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the proposed discriminative projections versus more established cross-language models on the two tasks described in the introduction: retrieving comparable documents from a corpus, and training a classifier in one language and using it in another.", "labels": [], "entities": []}, {"text": "We measure accuracy on a test set, and also examine the sensitivity to dimensionality of the projection on development sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9991570711135864}]}, {"text": "We first test the speed of the various algorithms discussed in this paper, compared to a full machine translation system.", "labels": [], "entities": []}, {"text": "When finding document projections, CL-LSI, OPCA, CCA, JPLSA, and CPLSA are equally fast: they perform a matrix multiplication and require O(nk) operations, where n is the number of distinct words in the documents and k is the dimensionality of the projection.", "labels": [], "entities": [{"text": "O", "start_pos": 138, "end_pos": 139, "type": "METRIC", "confidence": 0.9636476635932922}]}, {"text": "3 A single CPU core can read the indexed documents into memory and take logarithms at 216K words per second.", "labels": [], "entities": []}, {"text": "Projecting into a 2000-dimensional space operates at 41K words per second.", "labels": [], "entities": []}, {"text": "Translating word-by-word operates at 274K words per second.", "labels": [], "entities": []}, {"text": "In contrast, machine translation processes 50 words per second, approximately 3 orders of magnitude slower.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7376291155815125}]}, {"text": "Total training time for OPCA on 43,380 pairs of comparable documents was 90 minutes, running on an 8-core CPU for 2000 dimensions.", "labels": [], "entities": []}, {"text": "On the same corpus, JPLSA requires 31 minutes per iteration and CPLSA requires 377 minutes per iteration.", "labels": [], "entities": []}, {"text": "CPLSA requires a factor of five times fewer iterations: overall, it is twice as slow as JPLSA.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test results for comparable document retrieval  in Europarl. Boldface indicates statistically significant  superior results.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9912562370300293}]}, {"text": " Table 2: Test results for comparable document retrieval  in Wikipedia. Boldface indicates statistically significant  best result.", "labels": [], "entities": []}, {"text": " Table 3: Test results for cross-language text categoriza- tion", "labels": [], "entities": [{"text": "cross-language text categoriza- tion", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.786728847026825}]}]}