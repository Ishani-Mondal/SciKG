{"title": [], "abstractContent": [{"text": "Syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse.", "labels": [], "entities": [{"text": "Syntactic consistency", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7877301275730133}]}, {"text": "We present an analysis of the WSJ portion of the Penn Tree-bank, and show that syntactic consistency is pervasive across productions with various left-hand side nonterminals.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8021519780158997}, {"text": "Penn Tree-bank", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.8225864171981812}]}, {"text": "Then, we implement a reranking constituent parser that makes use of extra-sentential context in its feature set.", "labels": [], "entities": []}, {"text": "Using a linear-chain conditional random field, we improve parsing accuracy over the gen-erative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context.", "labels": [], "entities": [{"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9562670588493347}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9670233130455017}, {"text": "Penn Treebank WSJ corpus", "start_pos": 119, "end_pos": 143, "type": "DATASET", "confidence": 0.9821574687957764}]}, {"text": "We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement.", "labels": [], "entities": []}, {"text": "We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies .", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent corpus linguistics work has produced evidence of syntactic consistency, the preference to reuse a syntactic construction shortly after its appearance in a discourse (.", "labels": [], "entities": []}, {"text": "In addition, experimental studies have confirmed the existence of syntactic priming, the psycholinguistic phenomenon of syntactic consistency . Both types of studies, however, have Whether or not corpus-based studies of consistency have any bearing on syntactic priming as a reality in the human mind limited the constructions that are examined to particular syntactic constructions and alternations.", "labels": [], "entities": []}, {"text": "For instance, and examine specific constructions such as the passive voice, dative alternation and particle placement in phrasal verbs, and deal with the internal structure of noun phrases.", "labels": [], "entities": []}, {"text": "In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 118, "end_pos": 142, "type": "DATASET", "confidence": 0.9870288819074631}]}, {"text": "We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones.", "labels": [], "entities": []}, {"text": "Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. () rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed.", "labels": [], "entities": []}, {"text": "We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence.", "labels": [], "entities": []}, {"text": "The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context.", "labels": [], "entities": [{"text": "generative baseline parser", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.88718181848526}]}, {"text": "We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models would provide further improvement.", "labels": [], "entities": []}, {"text": "Analysis of the rerankings made provides cases where contextual information has clearly improved parsing peris a subject of debate.", "labels": [], "entities": [{"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.9807799458503723}]}, {"text": "See () and () for opposing viewpoints.", "labels": [], "entities": []}, {"text": "formance, indicating the potential of extra-sentential contextual information to aid parsing, especially for structural dependencies between sentences, such as parallelism effects.", "labels": [], "entities": [{"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9691828489303589}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Weighted average by production frequency among non-singleton production-types of prior and positive adap- tation probabilities, and the ratio between them. The columns on the right show the number of production-types  for which the positive adaptation probability is significantly greater than, not different from, or less than the prior  probability. We exclude LHSs with a weighted average prior of less than 0.005, due to the small sample size.", "labels": [], "entities": []}, {"text": " Table 2: Some instances of consistency effects of productions. All productions' pos adapt probability is significantly  greater than its prior probability at p < 10 \u22126 .", "labels": [], "entities": [{"text": "pos adapt probability", "start_pos": 81, "end_pos": 102, "type": "METRIC", "confidence": 0.75546795129776}]}, {"text": " Table 3: Correlation between rule overlap and F1 com- pared to the generative baseline for the 50-best parses in  the development set.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9590047001838684}, {"text": "F1 com- pared", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.9343030750751495}]}, {"text": " Table 4: Development set (section 22) results of various  models that we trained. Italicized are the models we use  for the test set.", "labels": [], "entities": []}, {"text": " Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of", "labels": [], "entities": [{"text": "Penn Treebank WSJ", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9887707233428955}]}, {"text": " Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)", "labels": [], "entities": [{"text": "Penn Treebank WSJ", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9901709953943888}]}, {"text": " Table 7: Context-aware vs. context-ignorant reranking  results, by sentential F1.", "labels": [], "entities": []}, {"text": " Table 8: A comparison of parsers specialized to exploit  intra-or extra-sentential syntactic parallelism on section  23 in terms of the generative baseline they compare them- selves against, the best F1 their non-baseline models  achieve, and the absolute and relative improvements.", "labels": [], "entities": []}]}