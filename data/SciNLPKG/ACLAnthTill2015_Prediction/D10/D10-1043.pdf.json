{"title": [], "abstractContent": [{"text": "This paper studies two issues, non-isomorphic structure translation and target syntactic structure usage, for statistical machine translation in the context of forest-based tree to tree sequence translation.", "labels": [], "entities": [{"text": "structure translation", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7679856419563293}, {"text": "statistical machine translation", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.6977811455726624}, {"text": "forest-based tree to tree sequence translation", "start_pos": 160, "end_pos": 206, "type": "TASK", "confidence": 0.7799757321675619}]}, {"text": "For the first issue, we propose a novel non-isomorphic translation framework to capture more non-isomorphic structure mappings than traditional tree-based and tree-sequence-based translation methods.", "labels": [], "entities": []}, {"text": "For the second issue, we propose a parallel space searching method to generate hypothesis using tree-to-string model and evaluate its syntactic goodness using tree-to-tree/tree sequence model.", "labels": [], "entities": [{"text": "parallel space searching", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.683987557888031}]}, {"text": "This not only reduces the search complexity by merging spurious-ambiguity translation paths and solves the data sparseness issue in training, but also serves as a syntax-based target language model for better grammatical generation.", "labels": [], "entities": [{"text": "grammatical generation", "start_pos": 209, "end_pos": 231, "type": "TASK", "confidence": 0.7574804723262787}]}, {"text": "Experiment results on the benchmark data show our proposed two solutions are very effective, achieving significant performance improvement over baselines when applying to different translation models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently syntax-based methods have achieved very promising results and attracted increasing interests in statistical machine translation (SMT) research community due to their ability to provide informative context structure information and convenience in carrying out word transformation and sub-span reordering.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.7779084146022797}, {"text": "word transformation", "start_pos": 268, "end_pos": 287, "type": "TASK", "confidence": 0.7396220117807388}]}, {"text": "Fundamentally, syntax-based SMT views translation as a structural transformation process.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8822423219680786}]}, {"text": "Generally speaking, from modeling viewpoint, a syntax-based model tries to convert the source structures into target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations.", "labels": [], "entities": []}, {"text": "Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination.", "labels": [], "entities": []}, {"text": "For the first issue, a number of models have been proposed to model the structure mapping between tree and string ( and between tree and tree.", "labels": [], "entities": []}, {"text": "However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node) to one target frontier non-terminal node in a bilingual translation rule.", "labels": [], "entities": []}, {"text": "Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models.", "labels": [], "entities": []}, {"text": "This may largely compromise the modeling ability of translation rules.", "labels": [], "entities": []}, {"text": "For the second problem, currently, the combination is driven by only the source side (both tree-to-string model and tree-to-tree model only check the source span compatibility when combining different target structures in decoding) or only the target side (string to tree model).", "labels": [], "entities": []}, {"text": "There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination.", "labels": [], "entities": []}, {"text": "In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding.", "labels": [], "entities": []}, {"text": "In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest 1 instead of 1-best parse tree to carryout training ) and decoding ) in order to reduce the side effect caused by parsing errors of the one-best tree.", "labels": [], "entities": []}, {"text": "However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated.", "labels": [], "entities": []}, {"text": "In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing anyone source frontier non-terminal node to be translated into any number of target frontier non-terminal nodes.", "labels": [], "entities": []}, {"text": "For the second issue, we propose a technology to model the combination task by considering both sides' syntactic structure information.", "labels": [], "entities": []}, {"text": "We evaluate and integrate the two technologies into forest-based tree to tree sequence translation.", "labels": [], "entities": [{"text": "forest-based tree to tree sequence translation", "start_pos": 52, "end_pos": 98, "type": "TASK", "confidence": 0.6756842086712519}]}, {"text": "Experimental results on the Chinese-English translation tasks show that our methods significantly outperform the forest-based tree to string and previous tree to tree models as well as the phrase-based model.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.7055095632870992}]}, {"text": "The remaining of the paper is organized as following.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related work.", "labels": [], "entities": []}, {"text": "In section 3 and section 4, we discuss the proposed forest-based rule extraction (non-isomorphic mapping) and decoding algorithms (target syntax information usage).", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7348862141370773}]}, {"text": "Finally we report the experimental results in section 5 and conclude the paper in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on the Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.7548761367797852}]}, {"text": "We first carryout a series empirical study on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size.", "labels": [], "entities": []}, {"text": "We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set.", "labels": [], "entities": [{"text": "NIST 2002 test set", "start_pos": 11, "end_pos": 29, "type": "DATASET", "confidence": 0.9743420332670212}, {"text": "NIST 2003 and NIST 2005 test sets", "start_pos": 50, "end_pos": 83, "type": "DATASET", "confidence": 0.8702551807676043}]}, {"text": "A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits) with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "SRILM Toolkits", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.925658643245697}]}, {"text": "We train Charniak's parser) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest.", "labels": [], "entities": [{"text": "CTB5.0", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9459676146507263}]}, {"text": "GIZA++) and the heuristics \"grow-diag-final-and\" are used to generate m-to-n word alignments.", "labels": [], "entities": []}, {"text": "For the MER training, Koehn's MER trainer is modified for our system.", "labels": [], "entities": [{"text": "MER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.964800238609314}]}, {"text": "For significance test, we use Zhang et al.'s implementation ().", "labels": [], "entities": []}, {"text": "Our evaluation metrics is case-sensitive closest BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9976769089698792}]}, {"text": "We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the average number of unmatched nodes in the target forest.", "labels": [], "entities": [{"text": "bidirectional lexical translation", "start_pos": 142, "end_pos": 175, "type": "TASK", "confidence": 0.6389702558517456}]}, {"text": "7) the length of the target translation, 8) the number of glue rules used.", "labels": [], "entities": [{"text": "length", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9613547921180725}]}], "tableCaptions": [{"text": " Table 1. Performance comparison of different methods", "labels": [], "entities": []}, {"text": " Table 2. Statistics on node mapping in forest, where  \"1to1\" means the number of nodes in source forest  that can be translated into one node in target forest  and \"1toN\" means the number of nodes in source  forest that have to be translated into more than one  node in target forest, where the node refers to  non-terminal nodes only", "labels": [], "entities": []}, {"text": " Table 3. Statistics of rule coverage, where \"T2S  covered\" means the percentage of tree-to-string  rules that can be covered by the model", "labels": [], "entities": [{"text": "T2S", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9238296747207642}]}, {"text": " Table 4. Performance and speed comparison", "labels": [], "entities": []}, {"text": " Table 5. Performance on larger data set", "labels": [], "entities": []}]}