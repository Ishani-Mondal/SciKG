{"title": [{"text": "Confidence in Structured-Prediction using Confidence-Weighted Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Confidence-Weighted linear classifiers (CW) and its successors were shown to perform well on binary and multiclass NLP problems.", "labels": [], "entities": []}, {"text": "In this paper we extend the CW approach for sequence learning and show that it achieves state-of-the-art performance on four noun phrase chucking and named entity recognition tasks.", "labels": [], "entities": [{"text": "sequence learning", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8380450904369354}, {"text": "noun phrase chucking and named entity recognition tasks", "start_pos": 125, "end_pos": 180, "type": "TASK", "confidence": 0.657499186694622}]}, {"text": "We then derive few algorith-mic approaches to estimate the prediction's correctness of each label in the output sequence.", "labels": [], "entities": []}, {"text": "We show that our approach provides a reliable relative correctness information as it outperforms other alternatives in ranking label-predictions according to their error.", "labels": [], "entities": []}, {"text": "We also show empirically that our methods output close to absolute estimation of error.", "labels": [], "entities": [{"text": "absolute estimation of error", "start_pos": 58, "end_pos": 86, "type": "METRIC", "confidence": 0.799431249499321}]}, {"text": "Finally, we show how to use this information to improve active learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past decade structured classification has seen much interest by the machine learning community.", "labels": [], "entities": [{"text": "structured classification", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.8797891438007355}]}, {"text": "After the introduction of conditional random fields (CRFs) (), and maximum margin Markov networks (, which are batch algorithms, new online method were introduced.", "labels": [], "entities": []}, {"text": "For example the passive-aggressive algorithm was adapted to chunking), parsing), learning preferences () and text segmentation ().", "labels": [], "entities": [{"text": "chunking", "start_pos": 60, "end_pos": 68, "type": "TASK", "confidence": 0.9715175628662109}, {"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.9740803837776184}, {"text": "text segmentation", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7706151008605957}]}, {"text": "These new online algorithms are fast to train and simple to implement, yet they generate models that output merely a prediction with no additional information, as opposed to probabilistic models like CRFs or HMMs.", "labels": [], "entities": []}, {"text": "In this work we fill this gap proposing few alternatives to compute confidence in the output of discriminative non-probabilistic algorithms.", "labels": [], "entities": []}, {"text": "As before, our algorithms output the highest-scoring labeling.", "labels": [], "entities": []}, {"text": "However, they also compute additional labelings, that are used to compute the per word confidence in its labelings.", "labels": [], "entities": []}, {"text": "We build on the recently introduced confidence-weighted learning) and induce a distribution over labelings from the distribution maintained over weight-vectors.", "labels": [], "entities": []}, {"text": "We show how to compute confidence estimates in the label predicted per word, such that the confidence reflects the probability that the label is not correct.", "labels": [], "entities": []}, {"text": "We then use this confidence information to rank all labeled words (in all sentences).", "labels": [], "entities": []}, {"text": "This can bethought of as a retrieval of the erroneous words, which can than be passed to human annotator for an examination, either to correct these mistakes or as a quality control component.", "labels": [], "entities": []}, {"text": "Next, we show how to apply our techniques to active learning over sequences.", "labels": [], "entities": []}, {"text": "We evaluate our methods on four NP chunking and NER datasets and demonstrate the usefulness of our methods.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.6937765628099442}, {"text": "NER datasets", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.6670268028974533}]}, {"text": "Finally, we report the performance of obtained by CW-like adapted to sequence prediction, which are comparable with current state-of-the-art algorithms.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7464934885501862}]}, {"text": "where the difference between CW and AROW is the specific instance-dependent rule used to set the values of \u03b1 i and \u03b2 i .", "labels": [], "entities": [{"text": "AROW", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.5732119083404541}]}], "datasetContent": [{"text": "For the experiments described in this paper we used four large sequential classification datasets taken from the shared tasks: noun-phrase (NP) chunking (), and named-entity recognition (NER) in Spanish, Dutch) and English (.", "labels": [], "entities": [{"text": "noun-phrase (NP) chunking", "start_pos": 127, "end_pos": 152, "type": "TASK", "confidence": 0.5955112397670745}, {"text": "named-entity recognition (NER)", "start_pos": 161, "end_pos": 191, "type": "TASK", "confidence": 0.8427587151527405}]}, {"text": "The properties of the four datasets are summarized in.", "labels": [], "entities": []}, {"text": "We followed the feature generation process of.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7223902642726898}]}, {"text": "Although our primary goal is estimating confidence in prediction and not the actual performance itself, we first report the results of using AROW and CW for sequence learning.", "labels": [], "entities": [{"text": "AROW", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.7013007402420044}, {"text": "sequence learning", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7819927632808685}]}, {"text": "We compared the performance CW and AROW of Alg.", "labels": [], "entities": [{"text": "AROW", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9650735855102539}, {"text": "Alg.", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.958633154630661}]}, {"text": "1 with two standard online baseline algorithms: Averaged-Perceptron algorithm and 5-best PA (the value of five was shown to be optimal for various tasks).", "labels": [], "entities": [{"text": "Averaged-Perceptron algorithm", "start_pos": 48, "end_pos": 77, "type": "METRIC", "confidence": 0.950799822807312}, {"text": "PA", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.7394091486930847}]}, {"text": "The update rule described in Alg.", "labels": [], "entities": [{"text": "update", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9278609156608582}, {"text": "Alg.", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9445973634719849}]}, {"text": "1 assumes a full covariance matrix, which is not feasible in our  setting.", "labels": [], "entities": []}, {"text": "Three options are possible: compute a full \u03a3 and then take its diagonal elements; compute a full inverse \u03a3, take its diagonal elements and then compute its inverse; assume that \u03a3 is diagonal and compute the optimal update for this choice.", "labels": [], "entities": []}, {"text": "We found the first method to work best, and thus employ it from now on.", "labels": [], "entities": []}, {"text": "The hyper parameters (r for AROW, \u03c6 for CW, C for PA) were tuned for each task by a single run over a random split of the data into a three-fourths training set and a one-fourth test set.", "labels": [], "entities": [{"text": "PA", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9722591042518616}]}, {"text": "We used parameter averaging with all methods.", "labels": [], "entities": []}, {"text": "For each of the four datasets we used 10-fold cross validation.", "labels": [], "entities": []}, {"text": "All algorithms (Perceptron, PA, CW and AROW) are online, and as mentioned above work in rounds.", "labels": [], "entities": [{"text": "AROW", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.7844400405883789}]}, {"text": "For each of the ten folds, each of the four algorithm performed ten (10) iterations over the training set and the performance (Recall, Precision and F-measure) was evaluated on the test set after each iteration.", "labels": [], "entities": [{"text": "Recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9949356317520142}, {"text": "Precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9605064392089844}, {"text": "F-measure", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9863677620887756}]}, {"text": "The F-measure of the four algorithms after 10 iterations over the four datasets is summarized in Table 2.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9976073503494263}]}, {"text": "The general trend is that AROW slightly outperforms CW, which is better than PA that is better than the Perceptron.", "labels": [], "entities": [{"text": "AROW", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9474425315856934}, {"text": "PA", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9877646565437317}, {"text": "Perceptron", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.9329490661621094}]}, {"text": "The difference between AROW and the Perceptron is significant, and between AROW and PA is significant in two datasets.", "labels": [], "entities": [{"text": "AROW", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.6301409602165222}, {"text": "PA", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9073519110679626}]}, {"text": "The difference between AROW and CW is not significant although it is consistent.", "labels": [], "entities": [{"text": "AROW", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.6149943470954895}]}, {"text": "We further investigate the convergence properties of the algorithms in.", "labels": [], "entities": []}, {"text": "The figure shows the recall and precision results after each training round averaged across the 10 folds.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9995700716972351}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.998408854007721}]}, {"text": "Each panel summarizes the results on a single dataset, and in each panel a single set of connected points corresponds to one algorithm.", "labels": [], "entities": []}, {"text": "Points in the left-bottom of the plot correspond to early iterations and points in the right-top correspond to later iterations.", "labels": [], "entities": []}, {"text": "Long segments indicate a big improvement in performance between two consecutive iterations.", "labels": [], "entities": []}, {"text": "Few points are in order.", "labels": [], "entities": []}, {"text": "First, high (in the y-axis) values indicate better precision and right (in the xaxis) values indicate better recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9992508292198181}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.998124897480011}]}, {"text": "Second, the performance of all algorithms is converging in about 10 iterations as indicated by the fact the points in the top-right of the plot are close to each other.", "labels": [], "entities": []}, {"text": "Third, the long segments in the bottom-left for the Perceptron algorithm indicate that this algorithm benefits more from more than one pass compared with the other.", "labels": [], "entities": []}, {"text": "Fourth, on the three NER datasets after 10 iterations AROW gets slightly higher precision values than CW, while CW gets slightly higher recall values than AROW.", "labels": [], "entities": [{"text": "NER datasets", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.8335908949375153}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9971078038215637}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9987560510635376}]}, {"text": "This is indicated by the fact that the top-right red square is left and above to the topright blue circle.", "labels": [], "entities": []}, {"text": "Finally, in two datasets, PA get slightly better recall than CW and AROW, but paying in terms of precision and overall F-measure performance.", "labels": [], "entities": [{"text": "PA", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.895503044128418}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9996343851089478}, {"text": "AROW", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6458454132080078}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9995478987693787}, {"text": "F-measure", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9557876586914062}]}, {"text": "In addition to online algorithms we also compared the performance of CW with the CRF algo- rithm which is a batch algorithm.", "labels": [], "entities": []}, {"text": "We used Mallet toolkit) for CRF implementation.", "labels": [], "entities": [{"text": "Mallet toolkit", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9473744034767151}, {"text": "CRF implementation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9538527429103851}]}, {"text": "For feature generation we used a combination of standard methods provided with Mallet toolkit (called pipes).", "labels": [], "entities": [{"text": "feature generation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7822068631649017}, {"text": "Mallet toolkit", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9325917363166809}]}, {"text": "We chose a combination yielding a feature set that is close as possible to the feature set we used in our system but it was not a perfect match, CRF generated about 20% fewer features in all datasets.", "labels": [], "entities": []}, {"text": "Nevertheless, any other combination of pipes we tried only hurt CRF performance.", "labels": [], "entities": [{"text": "CRF", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.6217573881149292}]}, {"text": "The precision, recall, F-measure and percentage of mislabeled words of CW algorithm compared with CRF measured over a single split of the data into a threefourths training set and a one-fourth test set is summarized in.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995369911193848}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9990222454071045}, {"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.999046266078949}]}, {"text": "We see that in three of the four datasets CW outperforms CRF and in one dataset CRF performs better.", "labels": [], "entities": []}, {"text": "Some of the performance differences maybe due to the differences in features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Properties of datasets.", "labels": [], "entities": []}, {"text": " Table 2: Averaged F-measure of methods. Statistical sig- nificance (t-test) are with respect to AROW, where * in- dicates 0.001 and ** indicates 0.01", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9472917914390564}, {"text": "AROW", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9540524482727051}]}, {"text": " Table 3: Precision, Recall, F-measure and percentage of  mislabeled words results of CW vs. CRF", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984917640686035}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9967300891876221}, {"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9977411031723022}]}]}