{"title": [{"text": "Improved Fully Unsupervised Parsing with Zoomed Learning", "labels": [], "entities": [{"text": "Improved Fully Unsupervised Parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7475972026586533}]}], "abstractContent": [{"text": "We introduce a novel training algorithm for unsupervised grammar induction, called Zoomed Learning.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.6860475987195969}]}, {"text": "Given a training set T and a test set S, the goal of our algorithm is to identify subset pairs Ti , Si of T and S such that when the unsupervised parser is trained on a training subset Ti its results on its paired test subset Si are better than when it is trained on the entire training set T.", "labels": [], "entities": []}, {"text": "A successful application of zoomed learning improves overall performance on the full test set S.", "labels": [], "entities": []}, {"text": "We study our algorithm's effect on the leading algorithm for the task of fully unsupervised parsing (Seginer, 2007) in three different En-glish domains, WSJ, BROWN and GENIA, and show that it improves the parser F-score by up to 4.47%.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.7432835698127747}, {"text": "BROWN", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.9379544854164124}, {"text": "GENIA", "start_pos": 168, "end_pos": 173, "type": "DATASET", "confidence": 0.6215945482254028}, {"text": "F-score", "start_pos": 212, "end_pos": 219, "type": "METRIC", "confidence": 0.9513534903526306}]}], "introductionContent": [{"text": "Grammar induction is the task of learning grammatical structure from plain text without human supervision.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8264044225215912}, {"text": "learning grammatical structure from plain text", "start_pos": 33, "end_pos": 79, "type": "TASK", "confidence": 0.7393117249011993}]}, {"text": "The task is of great importance both for the understanding of human language acquisition and since its output can be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora.", "labels": [], "entities": [{"text": "understanding of human language acquisition", "start_pos": 45, "end_pos": 88, "type": "TASK", "confidence": 0.6181417286396027}]}, {"text": "Many recent works have addressed the task (e.g. () and its importance has increased due to the recent availability of huge corpora.", "labels": [], "entities": []}, {"text": "A basic challenge to this research direction is how to utilize training data in the best possible way.", "labels": [], "entities": []}, {"text": "report results for their dependency model with valence (DMV) for unsupervised dependency parsing when it is trained and tested on the same corpus (both when sentence length restriction is imposed, such as for WSJ10, and when it is not, such as for the entire WSJ).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7163964062929153}, {"text": "WSJ10", "start_pos": 209, "end_pos": 214, "type": "DATASET", "confidence": 0.9542545080184937}, {"text": "WSJ", "start_pos": 259, "end_pos": 262, "type": "DATASET", "confidence": 0.9517655968666077}]}, {"text": "Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., and train on WSJ10 even when the test set includes longer sentences.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.976889967918396}]}, {"text": "Recently, demonstrated that training the DMV model on sentences of up to 15 words length yields better results on the entire section 23 of WSJ (with no sentence length restriction) than training with the entire WSJ corpus.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.8083362579345703}, {"text": "WSJ corpus", "start_pos": 211, "end_pos": 221, "type": "DATASET", "confidence": 0.9673366248607635}]}, {"text": "In contrast to these dependency models, the Seginer constituency parser achieves its best performance when trained on the entire WSJ corpus either if sentence length restriction is imposed on the test corpus or not.", "labels": [], "entities": [{"text": "Seginer constituency parser", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.5692306359608968}, {"text": "WSJ corpus", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.8206231892108917}]}, {"text": "The sentence length restriction training protocol of (, harms this parser.", "labels": [], "entities": [{"text": "sentence length restriction training", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7760631367564201}]}, {"text": "When the parser is trained with the entire WSJ corpus its F-score performance on the WSJ10, WSJ20 and the entire WSJ corpora are 76, 64.8 and 56.7 respectively.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9560013115406036}, {"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.995509147644043}, {"text": "WSJ10", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.9846177101135254}, {"text": "WSJ20", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9240197539329529}, {"text": "WSJ corpora", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.9034010469913483}]}, {"text": "When training is done with WSJ10 (WSJ20) performance degrades to 60 (72.2), 37.4 (61.9) and 29.7 (48) respectively.", "labels": [], "entities": [{"text": "WSJ10 (WSJ20", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.7348966797192892}]}, {"text": "In this paper we introduce the Zoomed Learning (ZL) technique for unsupervised parser training: given a training set T and a test set S, it identifies subset pairs Ti , Si of T and S such that when the unsupervised parser is trained on a training subset Ti its results on its paired test subset Si are better than when it is trained on the entire training set T . A successful application of zoomed learning improves performance on the full test set S.", "labels": [], "entities": []}, {"text": "We describe ZL algorithms of increasing sophistication.", "labels": [], "entities": []}, {"text": "In the simplest algorithm the subsets are randomly selected while in the more sophisticated versions subset selection is done using a fully unsupervised measure of constituency parse tree quality.", "labels": [], "entities": []}, {"text": "We apply ZL to the Seginer parser, the best algorithm for fully unsupervised constituency parsing.", "labels": [], "entities": [{"text": "Seginer parser", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.6053957343101501}, {"text": "constituency parsing", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.7026205509901047}]}, {"text": "The input is a plain text corpus without any annotation, not even POS tagging 1 , and the output is an unlabeled bracketing for each sentence.", "labels": [], "entities": []}, {"text": "We experiment in three different English domains: WSJ (economic newspaper), GENIA (biological articles) and BROWN (heterogeneous domains), and show that ZL improves the parser F-score by as much as 4.47%.", "labels": [], "entities": [{"text": "WSJ (economic newspaper)", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.8041074752807618}, {"text": "GENIA", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.5967330932617188}, {"text": "BROWN", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9936541318893433}, {"text": "F-score", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.8771244883537292}]}], "datasetContent": [{"text": "We experimented with three English corpora: the WSJ Penn Treebank () consisting of economic newspaper texts, the BROWN corpus () consisting of texts of various English genres (e.g. fiction, humor, romance, mystery and adventure) and the GENIA corpus () consisting of abstracts of scientific articles from the biological domain.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.8995172182718912}, {"text": "BROWN", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9509055018424988}, {"text": "English genres (e.g. fiction, humor, romance, mystery and adventure", "start_pos": 160, "end_pos": 227, "type": "TASK", "confidence": 0.6492763918179733}, {"text": "GENIA corpus", "start_pos": 237, "end_pos": 249, "type": "DATASET", "confidence": 0.8650405406951904}]}, {"text": "All corpora were stripped of all annotation (bracketing and POS tags).", "labels": [], "entities": []}, {"text": "For all corpora we report the parser performance on the entire corpus (WSJ: 49206 sentences, BROWN: 24243 sentences, GENIA: 4661 sentences).", "labels": [], "entities": [{"text": "WSJ: 49206 sentences", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.8543107956647873}, {"text": "BROWN", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9892130494117737}, {"text": "GENIA: 4661 sentences", "start_pos": 117, "end_pos": 138, "type": "DATASET", "confidence": 0.6789492815732956}]}, {"text": "For WSJ we also provide an analysis of the performance of the parser when applied to sentences of bounded length.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7493026256561279}]}, {"text": "These sub-corpora are WSJ10 (7422 sentences), WSJ20 (25522 sentences) and WSJ40 (47513 sentences) where WSJY denotes the subset of WSJ containing sentences of length at most Y (excluding punctuation).", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9657605886459351}, {"text": "WSJ20", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9369719624519348}, {"text": "WSJ40", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9321671724319458}]}, {"text": "Seginer's parser achieves its best reported results when trained on the full WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9142472743988037}]}, {"text": "Consequently, for all corpora, we compare the performance of the parser when trained with the ZL algorithms to its performance when trained with the full corpus.", "labels": [], "entities": []}, {"text": "The POS tags required as input by the PUPA algorithm are induced by the fully unsupervised POS induction algorithm of Clark . Reichart and Rappoport (2009b) demonstrated an unsupervised technique for the estimation of the number of induced POS tags with which the correlation between PUPA's score and the parse F-score is maximized.", "labels": [], "entities": [{"text": "F-score", "start_pos": 311, "end_pos": 318, "type": "METRIC", "confidence": 0.7490857243537903}]}, {"text": "When exploring an experimental setup identical to our WSJ setup, they set the number of induced tags to be 5.", "labels": [], "entities": []}, {"text": "We therefore induced 5 POS tags for each corpus, using all its sentences as input for Clark's algorithm.", "labels": [], "entities": []}, {"text": "Our implementation of the PUPA algorithm will be made available online.", "labels": [], "entities": [{"text": "PUPA algorithm", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.7492980062961578}]}, {"text": "For each corpus we performed K experiments with each of the three ZL algorithms, where K equals to the number of sentences in the corpus divided by 1000 (rounded upwards).", "labels": [], "entities": []}, {"text": "In each experiment the size of the high quality H and lower quality L training subsets is different.", "labels": [], "entities": []}, {"text": "H consists of the NH top ranked sentences according to PUPA (or NH randomly selected sentences for RZL), with NH changing from 1000 upwards in steps of 1000.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.49640217423439026}]}, {"text": "L consists of the rest of the sentences in the training corpus (WSJ).", "labels": [], "entities": [{"text": "training corpus (WSJ)", "start_pos": 47, "end_pos": 68, "type": "DATASET", "confidence": 0.6799095749855042}]}, {"text": "The results reported for RZL are averaged over 10 runs.", "labels": [], "entities": [{"text": "RZL", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.7296788096427917}]}, {"text": "We report the parser performance on the test corpus for each training protocol.", "labels": [], "entities": []}, {"text": "Following the unsupervised parsing literature multiple brackets and brackets covering a single word are not counted, but the sentence level bracket is.", "labels": [], "entities": []}, {"text": "We exclude punctua-WSJ10, F(Full) = 76 WSJ20, F(Full) = 64.82 WSJ40, F(Full) = 57.54 WSJ, F(Full) = 56 tion and null elements as in ().", "labels": [], "entities": [{"text": "F", "start_pos": 26, "end_pos": 27, "type": "METRIC", "confidence": 0.9530457854270935}, {"text": "WSJ20", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.7194210290908813}, {"text": "WSJ40", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9243615865707397}, {"text": "WSJ", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.7604383230209351}]}, {"text": "To evaluate the quality of a parse tree with respect to its gold standard, the unlabeled parsing F-score is used.", "labels": [], "entities": [{"text": "F-score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.8633710145950317}]}], "tableCaptions": [{"text": " Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.", "labels": [], "entities": [{"text": "BZL", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8992444276809692}, {"text": "BROWN", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9689498543739319}, {"text": "WSJ40", "start_pos": 191, "end_pos": 196, "type": "DATASET", "confidence": 0.881995677947998}]}, {"text": " Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.", "labels": [], "entities": [{"text": "BROWN", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9627837538719177}, {"text": "GENIA", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.7874894738197327}]}]}