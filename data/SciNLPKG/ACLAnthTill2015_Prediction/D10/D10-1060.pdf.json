{"title": [{"text": "Statistical Machine Translation with a Factorized Grammar", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7760286927223206}]}], "abstractContent": [{"text": "In modern machine translation practice, a statistical phrasal or hierarchical translation system usually relies on a huge set of translation rules extracted from bilingual training data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7401807904243469}, {"text": "statistical phrasal or hierarchical translation", "start_pos": 42, "end_pos": 89, "type": "TASK", "confidence": 0.6833206295967102}]}, {"text": "This approach not only results in space and efficiency issues, but also suffers from the sparse data problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use factorized grammars, an idea widely accepted in the field of linguistic grammar construction, to generalize translation rules, so as to solve these two problems.", "labels": [], "entities": [{"text": "linguistic grammar construction", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.6616309781869253}]}, {"text": "We designed a method to take advantage of the XTAG English Grammar to facilitate the extraction of factorized rules.", "labels": [], "entities": [{"text": "XTAG English Grammar", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.9281730651855469}]}, {"text": "We experimented on various setups of low-resource language translation, and showed consistent significant improvement in BLEU over state-of-the-art string-to-dependency baseline systems with 200K words of bilingual training data.", "labels": [], "entities": [{"text": "low-resource language translation", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.690750261147817}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9970628619194031}]}], "introductionContent": [{"text": "A statistical phrasal () or hierarchical) machine translation system usually relies on a very large set of translation rules extracted from bi-lingual training data with heuristic methods on word alignment results.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.728089228272438}, {"text": "word alignment", "start_pos": 191, "end_pos": 205, "type": "TASK", "confidence": 0.7461730241775513}]}, {"text": "According to our own experience, we obtain about 200GB of rules from training data of about 50M words on each side.", "labels": [], "entities": []}, {"text": "This immediately becomes an engineering challenge on space and search efficiency.", "labels": [], "entities": [{"text": "space and search efficiency", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6247471794486046}]}, {"text": "A common practice to circumvent this problem is to filter the rules based on development sets in the step of rule extraction or before the decoding phrase, instead of building areal distributed system.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.7390877902507782}]}, {"text": "However, this strategy only works for research systems, for which the segments for translation are always fixed.", "labels": [], "entities": [{"text": "translation", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.9674596786499023}]}, {"text": "However, do we really need such a large rule set to represent information from the training data of much smaller size?", "labels": [], "entities": []}, {"text": "Linguists in the grammar construction field already showed us a perfect solution to a similar problem.", "labels": [], "entities": [{"text": "grammar construction", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7558534443378448}]}, {"text": "The answer is to use a factorized grammar.", "labels": [], "entities": []}, {"text": "Linguists decompose lexicalized linguistic structures into two parts, (unlexicalized) templates and lexical items.", "labels": [], "entities": []}, {"text": "Templates are further organized into families.", "labels": [], "entities": [{"text": "Templates", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9598240852355957}]}, {"text": "Each family is associated with a set of lexical items which can be used to lexicalize all the templates in this family.", "labels": [], "entities": []}, {"text": "For example, the XTAG English Grammar), a hand-crafted grammar based on the Tree Adjoining Grammar (TAG)) formalism, is a grammar of this kind, which employs factorization with LTAG e-tree templates and lexical items.", "labels": [], "entities": [{"text": "XTAG English Grammar", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.743318498134613}]}, {"text": "Factorized grammars not only relieve the burden on space and search, but also alleviate the sparse data problem, especially for low-resource language translation with few training data.", "labels": [], "entities": [{"text": "low-resource language translation", "start_pos": 128, "end_pos": 161, "type": "TASK", "confidence": 0.7722207109133402}]}, {"text": "With a factored model, we do not need to observe exact \"template -lexical item\" occurrences in training.", "labels": [], "entities": []}, {"text": "New rules can be generated from template families and lexical items either offline or on the fly, explicitly or implicitly.", "labels": [], "entities": []}, {"text": "In fact, the factorization approach has been successfully applied on the morphological level in previous study on MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9802777171134949}]}, {"text": "In this work, we will go further to investigate factorization of rule structures by exploiting the rich XTAG English Grammar.", "labels": [], "entities": [{"text": "XTAG English Grammar", "start_pos": 104, "end_pos": 124, "type": "DATASET", "confidence": 0.9026033083597819}]}, {"text": "We evaluate the effect of using factorized translation grammars on various setups of low-resource language translation, since low-resource MT suffers greatly on poor generalization capability of trans-lation rules.", "labels": [], "entities": [{"text": "low-resource language translation", "start_pos": 85, "end_pos": 118, "type": "TASK", "confidence": 0.8121792674064636}, {"text": "MT", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.9665602445602417}]}, {"text": "With the help of high-level linguistic knowledge for generalization, factorized grammars provide consistent significant improvement in BLEU () over string-todependency baseline systems with 200K words of bi-lingual training data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9985933899879456}]}, {"text": "This work also closes the gap between compact hand-crafted translation rules and large-scale unorganized automatic rules.", "labels": [], "entities": []}, {"text": "This may lead to a more effective and efficient statistical translation model that could better leverage generic linguistic knowledge in MT.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6874869614839554}, {"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9678075313568115}]}, {"text": "In the rest of this paper, we will first provide a short description of our baseline system in Section 2.", "labels": [], "entities": []}, {"text": "Then, we will introduce factorized translation grammars in Section 3.", "labels": [], "entities": [{"text": "translation grammars", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7833396792411804}]}, {"text": "We will illustrate the use of the XTAG English Grammar to facilitate the extraction of factorized rules in Section 4.", "labels": [], "entities": [{"text": "XTAG English Grammar", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8868829607963562}]}, {"text": "Implementation details are provided in Section 5.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9455214738845825}]}, {"text": "Experimental results are reported in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the performance of using factorized grammars on low-resource MT setups.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9496907591819763}]}, {"text": "As what we noted above, the sparse data problem is a major issue when there is not enough training data.", "labels": [], "entities": []}, {"text": "This is one of the cases that a factorized grammar would help.", "labels": [], "entities": []}, {"text": "We did not tested on real low-resource languages.", "labels": [], "entities": []}, {"text": "Instead, we mimic the low-resource setup with two of the most frequently used language pairs, Arabicto-English and Chinese-to-English, on newswire and web genres.", "labels": [], "entities": []}, {"text": "Experiments on these setups will be reported in Section 6.1.", "labels": [], "entities": [{"text": "Section 6.1", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.9342176020145416}]}, {"text": "Working on a language which actually has more resources allows us to study the effect of training data size.", "labels": [], "entities": []}, {"text": "This will be reported in Section 6.2.", "labels": [], "entities": []}, {"text": "In Section 6.3, we will show examples of templates learned from the Arabic-to-English training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results on Arabic-to-English / Chinese-to-English newswire and web data. %BL stands for  BLEU scores for documents whose BLEU scores are in the bottom 75% to 90% range of all documents. MET stands  for METEOR scores.", "labels": [], "entities": [{"text": "BL", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9840766787528992}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9974368810653687}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9942512512207031}, {"text": "MET", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.9274417757987976}, {"text": "METEOR", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.8445268869400024}]}, {"text": " Table 2: Experimental results on Arabic web. %BL stands for BLEU scores for documents whose BLEU scores are  in the bottom 75% to 90% range of all documents. MET stands for METEOR scores.", "labels": [], "entities": [{"text": "BL", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9897477626800537}, {"text": "BLEU scores", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.972376674413681}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9933930039405823}, {"text": "MET", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.8963028788566589}]}]}