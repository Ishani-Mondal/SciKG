{"title": [{"text": "Maximum Entropy Based Phrase Reordering for Hierarchical Phrase-based Translation", "labels": [], "entities": [{"text": "Phrase Reordering", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7126379013061523}, {"text": "Hierarchical Phrase-based Translation", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.7058852116266886}]}], "abstractContent": [{"text": "Hierarchical phrase-based (HPB) translation provides a powerful mechanism to capture both short and long distance phrase reorder-ings.", "labels": [], "entities": [{"text": "Hierarchical phrase-based (HPB) translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5684152940909067}]}, {"text": "However, the phrase reorderings lack of contextual information in conventional HPB systems.", "labels": [], "entities": []}, {"text": "This paper proposes a context-dependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder select appropriate reordering patterns.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.665706217288971}]}, {"text": "We classify translation rules into several reordering patterns, and build a MaxEnt model for each pattern based on various contextual features.", "labels": [], "entities": []}, {"text": "We integrate the MaxEnt models into the HPB model.", "labels": [], "entities": [{"text": "HPB", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9424530863761902}]}, {"text": "Experimental results show that our approach achieves significant improvements over a standard HPB system on large-scale translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.8844626247882843}]}, {"text": "On Chinese-to-English translation, the absolute improvements in BLEU (case-insensitive) range from 1.2 to 2.1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9990432858467102}]}], "introductionContent": [{"text": "The hierarchical phrase-based (HPB) model has been widely adopted in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.8033861766258875}]}, {"text": "It utilizes synchronous context free grammar (SCFG) rules to perform translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.964853048324585}]}, {"text": "Typically, there are three types of rules (see): phrasal rule, a phrase pair consisting of consecutive words; hierarchical rule, a hierarchical phrase pair consisting of both words and variables; and glue rule, which is used to merge phrases serially.", "labels": [], "entities": []}, {"text": "Phrasal rule captures short distance reorderings within phrases, while hierarchical rule captures long distance reorderings be- tween phrases.", "labels": [], "entities": []}, {"text": "Therefore, the HPB model outperforms conventional phrase-based models on phrase reorderings.", "labels": [], "entities": []}, {"text": "However, HPB translation suffers from a limitation, in that the phrase reorderings lack of contextual information, such as the surrounding words of a phrase and the content of sub-phrases that represented by variables.", "labels": [], "entities": [{"text": "HPB translation", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7218478322029114}]}, {"text": "Consider the following two hierarchical rules in translating a Chinese sentence into English: X \u2192 X 1 X 2 , X 2 X 1 (2) with Russia 's talks talks with Russia Both pattern-match the source sentence, but produce quite different phrase reorderings.", "labels": [], "entities": []}, {"text": "The first rule generates a monotone translation, while the second rule swaps the source phrases covered by X 1 and X 2 on the target side.", "labels": [], "entities": []}, {"text": "During decoding, the first rule is more likely to be used, as it occurs more frequently in a training corpus.", "labels": [], "entities": []}, {"text": "However, the example is not a noun possessive case because the subphrase covered by X 1 is not a noun but a prepositional phrase.", "labels": [], "entities": []}, {"text": "Thus, without considering information of sub-phrases, the decoder may make errors on phrase reordering.", "labels": [], "entities": []}, {"text": "Contextual information has been widely used to improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9759775400161743}]}, {"text": "It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation fora source text.", "labels": [], "entities": []}, {"text": "Several researchers observed that word sense disambiguation improves translation quality on lexical translation).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.7251011431217194}]}, {"text": "These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.991523027420044}]}, {"text": "Zens and Ney (2006) and utilized contextual information to improve phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8169242441654205}]}, {"text": "They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.755556583404541}]}, {"text": "They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases.", "labels": [], "entities": []}, {"text": "presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation.", "labels": [], "entities": [{"text": "lexical translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.700849324464798}, {"text": "phrase reordering", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7387203276157379}, {"text": "HPB translation", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.7931235730648041}]}, {"text": "They built a MaxEnt model for each ambiguous source side based on contextual features.", "labels": [], "entities": []}, {"text": "The method was also successfully applied to improve syntax-based SMT translation ( , using more sophisticated syntactical features.", "labels": [], "entities": [{"text": "SMT translation", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.9603480994701385}]}, {"text": "integrated various contextual and linguistic features into an HPB system, using surrounding words and dependency information for building context and dependency language models, respectively.", "labels": [], "entities": []}, {"text": "In this paper, we focus on improving phrase reordering for HPB translation.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8269990682601929}, {"text": "HPB translation", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.8412665128707886}]}, {"text": "We classify SCFG rules into several reordering patterns consisting of two variables X and F (or E) 1 , such as X 1 F X 2 and X 2 EX 1 . We treat phrase reordering as a classification problem and build a MaxEnt model for each source reordering pattern based on various contex-tual features.", "labels": [], "entities": []}, {"text": "We propose a method to integrate the MaxEnt models into an HPB system.", "labels": [], "entities": []}, {"text": "Specifically: \u2022 For hierarchical rules, we classify the sourceside and the target-side into 7 and 17 reordering patterns, respectively.", "labels": [], "entities": []}, {"text": "Target reordering patterns are treated as possible labels.", "labels": [], "entities": []}, {"text": "We then build a classifier for each source pattern to predict phrase reorderings.", "labels": [], "entities": []}, {"text": "This is different from , in which they built a classifier for each ambiguous hierarchical sourceside.", "labels": [], "entities": []}, {"text": "Therefore, the training examples for each MaxEnt model is small and the model maybe unstable.", "labels": [], "entities": []}, {"text": "Here, we classify source hierarchical phrases into 7 reordering patterns according to the arrangement of words and variables.", "labels": [], "entities": []}, {"text": "We can obtain sufficient samples for each MaxEnt model from large-scale bilingual corpus.", "labels": [], "entities": []}, {"text": "\u2022 For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule.", "labels": [], "entities": [{"text": "bracketing transduction grammar (BTG)", "start_pos": 51, "end_pos": 88, "type": "METRIC", "confidence": 0.7014715870221456}]}, {"text": "By doing this, there are two options for the decoder to merge phrases: serial or inverse.", "labels": [], "entities": []}, {"text": "We then build a classifier for glue rules to predict reorderings of neighboring phrases, analogous to.", "labels": [], "entities": []}, {"text": "\u2022 We integrate the MaxEnt based phrase reordering models as features into the HPB model ().", "labels": [], "entities": []}, {"text": "The feature weights can be tuned together with other feature functions by MERT algorithm.", "labels": [], "entities": [{"text": "MERT", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7259626984596252}]}, {"text": "Experimental results show that the presented method achieves significant improvement over the baseline.", "labels": [], "entities": []}, {"text": "On Chinese-to-English translation tasks of NIST evluation, improvements in BLEU (case-insensitive) are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set, and 2.1 on MT08.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9991015195846558}, {"text": "MT06 GALE set", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.8139038880666097}, {"text": "MT06 NIST set", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.8373091816902161}, {"text": "MT08", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.9601100087165833}]}, {"text": "The rest of the paper is structured as follows: Section 2 describes the MaxEnt based phrase reordering method.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.6118255853652954}]}, {"text": "Section 3 integrates the MaxEnt models into the translation model.", "labels": [], "entities": []}, {"text": "In Section 4, we report experimental results.", "labels": [], "entities": []}, {"text": "We analyze the presented method and experimental results in Section 5 and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out experiments on four systems: \u2022 HPB: replication of the Hiero system); \u2022 HPB+MEHR: HPB with MaxEnt based classifier for hierarchical rules, as described in Section 2.1; \u2022 HPB+MEGR: HPB with MaxEnt based classifier for glue rules, as described in Section 2.2; \u2022 HPB+MER: HPB with MaxEnt based classifier for both hierarchical and glue rules.", "labels": [], "entities": [{"text": "Hiero system", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9564597308635712}, {"text": "MEHR", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9474350810050964}]}, {"text": "All systems were tuned on NIST MT03 and tested on MT06 and MT08.", "labels": [], "entities": [{"text": "NIST", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9809360504150391}, {"text": "MT03", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.5720090866088867}, {"text": "MT06", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.982553243637085}, {"text": "MT08", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9685463309288025}]}, {"text": "The evaluation metric was BLEU () with case-insensitive matching of n-grams, where n = 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9991896748542786}]}, {"text": "We evaluated our approach on Chinese-toEnglish translation.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.59550641477108}]}, {"text": "The training data contained 77M Chinese words and 81M English words.", "labels": [], "entities": []}, {"text": "These data come from 17 corpora: LDC2006E92, LDC2006E93, LDC2004T08 (HK News, HK Hansards).", "labels": [], "entities": [{"text": "HK News, HK Hansards)", "start_pos": 69, "end_pos": 90, "type": "DATASET", "confidence": 0.8684289256731669}]}, {"text": "To obtain word alignments, we first ran GIZA++) in both translation directions and then refined the results using the \"grow-diagfinal\" method (.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7462301254272461}]}, {"text": "For the language model, we used the SRI Language Modeling Toolkit) to train two 4-gram models on the Xinhua portion of the GigaWord corpus and the English side of the training corpus.", "labels": [], "entities": [{"text": "SRI Language Modeling Toolkit", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.8063383549451828}, {"text": "GigaWord corpus", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.953079491853714}]}], "tableCaptions": [{"text": " Table 5: Statistical information of reordering pattern clas- sification for hierarchical source phrases.", "labels": [], "entities": []}, {"text": " Table 6: Percentage of target reordering pattern for each  source pattern containing one variable.", "labels": [], "entities": []}, {"text": " Table 7: Percentage of target reordering pattern for each source pattern containing two variables.", "labels": [], "entities": []}, {"text": " Table 8: BLEU percentage scores and translation speed (words/second) on test data. G=GALE set, N=NIST set. All  improvements are statistically significant (p < 0.01). Note that MT06G has one reference for each source sentence,  while the MT06N and MT08 have four references.", "labels": [], "entities": [{"text": "BLEU percentage scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.9620892604192098}, {"text": "GALE set", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9696659445762634}, {"text": "MT06G", "start_pos": 178, "end_pos": 183, "type": "DATASET", "confidence": 0.9311872720718384}, {"text": "MT06N", "start_pos": 239, "end_pos": 244, "type": "DATASET", "confidence": 0.9819412231445312}, {"text": "MT08", "start_pos": 249, "end_pos": 253, "type": "DATASET", "confidence": 0.9321423172950745}]}]}