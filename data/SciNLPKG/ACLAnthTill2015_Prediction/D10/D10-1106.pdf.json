{"title": [{"text": "Learning First-Order Horn Clauses from Web Text", "labels": [], "entities": [{"text": "Learning First-Order Horn Clauses from Web Text", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7354569435119629}]}], "abstractContent": [{"text": "Even the entire Web corpus does not explicitly answer all questions, yet inference can uncover many implicit answers.", "labels": [], "entities": []}, {"text": "But where do inference rules come from?", "labels": [], "entities": []}, {"text": "This paper investigates the problem of learning inference rules from Web text in an un-supervised, domain-independent manner.", "labels": [], "entities": []}, {"text": "The SHERLOCK system, described herein, is a first-order learner that acquires over 30,000 Horn clauses from Web text.", "labels": [], "entities": []}, {"text": "SHERLOCK embodies several innovations, including a novel rule scoring function based on Statistical Relevance (Salmon et al., 1971) which is effective on ambiguous, noisy and incomplete Web extractions.", "labels": [], "entities": []}, {"text": "Our experiments show that inference over the learned rules discovers three times as many facts (at precision 0.8) as the TEXTRUNNER system which merely extracts facts explicitly stated in Web text.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9708421230316162}, {"text": "TEXTRUNNER", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9080106019973755}]}], "introductionContent": [{"text": "Today's Web search engines locate pages that match keyword queries.", "labels": [], "entities": []}, {"text": "Even sophisticated Web-based Q/A systems merely locate pages that contain an explicit answer to a question.", "labels": [], "entities": []}, {"text": "These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages.", "labels": [], "entities": []}, {"text": "To solve this problem, introduced the HOLMES system, which infers answers from tuples extracted from text.", "labels": [], "entities": []}, {"text": "HOLMES's distinction is that it is domain independent and that its inference time is linear in the size of its input corpus, which enables it to scale to the Web.", "labels": [], "entities": []}, {"text": "However, HOLMES's Achilles heel is that it requires hand-coded, first-order, Horn clauses as input.", "labels": [], "entities": []}, {"text": "Thus, while HOLMES's inference run time is highly scalable, it requires substantial labor and expertise to hand-craft the appropriate set of Horn clauses for each new domain.", "labels": [], "entities": []}, {"text": "Is it possible to learn effective first-order Horn clauses automatically from Web text in a domainindependent and scalable manner?", "labels": [], "entities": []}, {"text": "We refer to the set of ground facts derived from Web text as opendomain theories.", "labels": [], "entities": []}, {"text": "Learning Horn clauses has been studied extensively in the Inductive Logic Programming (ILP) literature.", "labels": [], "entities": []}, {"text": "However, learning Horn clauses from opendomain theories is particularly challenging for several reasons.", "labels": [], "entities": []}, {"text": "First, the theories denote instances of an unbounded and unknown set of relations.", "labels": [], "entities": []}, {"text": "Second, the ground facts in the theories are noisy, and incomplete.", "labels": [], "entities": []}, {"text": "Negative examples are mostly absent, and certainly we cannot make the closed-world assumption typically made by ILP systems.", "labels": [], "entities": []}, {"text": "Finally, the names used to denote both entities and relations are rife with both synonyms and polysymes making their referents ambiguous and resulting in a particularly noisy and ambiguous set of ground facts.", "labels": [], "entities": []}, {"text": "This paper presents anew ILP method, which is optimized to operate on open-domain theories derived from massive and diverse corpora such as the Web, and experimentally confirms both its effectiveness and superiority over traditional ILP algorithms in this context.", "labels": [], "entities": []}, {"text": "shows some example rules that were learned by SHERLOCK.", "labels": [], "entities": [{"text": "SHERLOCK", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.5864319205284119}]}, {"text": "This work makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We describe the design and implementation of the SHERLOCK system, which utilizes a novel, unsupervised ILP method to learn first-order Horn clauses from open-domain Web text.", "labels": [], "entities": []}, {"text": "IsHeadquarteredIn(Company, State) :-IsBasedIn(Company, City) \u2227 IsLocatedIn(City, State); Contains(Food, Chemical) :-IsMadeFrom(Food, Ingredient) \u2227 Contains(Ingredient, Chemical); Reduce(Medication, Factor) :-KnownGenericallyAs(Medication, Drug) \u2227 Reduce(Drug, Factor); ReturnTo(Writer, Place) :-BornIn(Writer, City) \u2227 CapitalOf(City, Place); Make(Company1, Device) :-Buy(Company1, Company2) \u2227 Make(Company2, Device);: Example rules learned by SHERLOCK from Web extractions.", "labels": [], "entities": [{"text": "ReturnTo", "start_pos": 269, "end_pos": 277, "type": "METRIC", "confidence": 0.936442494392395}]}, {"text": "Note that the italicized rules are unsound.", "labels": [], "entities": []}, {"text": "2. We derive an innovative scoring function that is particularly well-suited to unsupervised learning from noisy text.", "labels": [], "entities": []}, {"text": "For Web text, the scoring function yields more accurate rules than several functions from the ILP literature.", "labels": [], "entities": []}, {"text": "3. We demonstrate the utility of SHERLOCK's automatically learned inference rules.", "labels": [], "entities": [{"text": "SHERLOCK", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.5209344625473022}]}, {"text": "Inference using SHERLOCK's learned rules identifies three times as many high quality facts (e.g., precision \u2265 0.8) as were originally extracted from the Web text corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9975610971450806}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start by describing previous work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the SHERLOCK rule learning system, with Section 3.4 describing how it estimates rule quality.", "labels": [], "entities": [{"text": "SHERLOCK rule learning", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.6452597280343374}]}, {"text": "We empirically evaluate SHERLOCK in Section 4, and conclude.", "labels": [], "entities": [{"text": "SHERLOCK", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8358493447303772}]}], "datasetContent": [{"text": "One can attempt to evaluate a rule learner by estimating the quality of learned rules, or by measuring their impact on a system that uses the learned rules.", "labels": [], "entities": []}, {"text": "Since the notion of 'rule quality' is vague except in the context of an application, we evaluate SHER-LOCK in the context of the HOLMES inference-based question answering system.", "labels": [], "entities": [{"text": "SHER-LOCK", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9741069674491882}, {"text": "HOLMES inference-based question answering", "start_pos": 129, "end_pos": 170, "type": "TASK", "confidence": 0.5549653246998787}]}, {"text": "Our evaluation focuses on three main questions: 1.", "labels": [], "entities": []}, {"text": "Does inference utilizing learned Horn rules improve the precision/recall of question answering and by how much?", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9991863369941711}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9315921068191528}, {"text": "question answering", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6787460148334503}]}, {"text": "2. How do different rule-scoring functions affect the performance of learning?", "labels": [], "entities": []}, {"text": "3. What role does each of SHERLOCK's components have in the resulting performance?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: SHERLOCK's modified weight learning algo- rithm gives better probability estimates over noisy and in- complete Web extractions. Most of the gains come from  penalizing longer rules more, but using weighted ground- ing counts further improves recall by 0.07, which corre- sponds to almost 100,000 additional facts at precision 0.8.", "labels": [], "entities": [{"text": "recall", "start_pos": 252, "end_pos": 258, "type": "METRIC", "confidence": 0.9986926913261414}]}]}