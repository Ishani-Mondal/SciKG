{"title": [{"text": "Turbo Parsers: Dependency Parsing by Approximate Variational Inference", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.6463001817464828}]}], "abstractContent": [{"text": "We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al.", "labels": [], "entities": [{"text": "loopy belief propagation parser", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.7020530253648758}]}, {"text": "By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method.", "labels": [], "entities": []}, {"text": "We also propose anew aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation.", "labels": [], "entities": []}, {"text": "The algorithm does not require a learning rate parameter and provides a single framework fora wide family of convex loss functions, including CRFs and structured SVMs.", "labels": [], "entities": []}, {"text": "Experiments show state-of-the-art performance for 14 languages .", "labels": [], "entities": []}], "introductionContent": [{"text": "Feature-rich discriminative models that break locality/independence assumptions can boost a parser's performance.", "labels": [], "entities": []}, {"text": "Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms.", "labels": [], "entities": [{"text": "approximate parsing", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.4867887645959854}]}, {"text": "In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation and linear programming relaxation).", "labels": [], "entities": [{"text": "non-projective dependency parsing", "start_pos": 110, "end_pos": 143, "type": "TASK", "confidence": 0.7978137135505676}, {"text": "loopy belief propagation", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.6744597355524699}, {"text": "linear programming relaxation", "start_pos": 174, "end_pos": 203, "type": "TASK", "confidence": 0.6160871386528015}]}, {"text": "While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both optimize objective functions over local approximations of the marginal polytope.", "labels": [], "entities": []}, {"text": "The connection is made clear by writing the explicit declarative optimization problem underlying and by showing the factor graph underlying.", "labels": [], "entities": []}, {"text": "The success of both approaches parallels similar approximations in other fields, such as statistical image processing and error-correcting coding.", "labels": [], "entities": [{"text": "statistical image processing", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.7490247289339701}]}, {"text": "Throughtout, we call these turbo parsers.", "labels": [], "entities": []}, {"text": "Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints ( \u00a72), which extends some combinatorial factors considered by.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8184482157230377}]}, {"text": "After presenting a geometric view of the variational approximations underlying message-passing algorithms ( \u00a73), and closing the gap between the two aforementioned parsers ( \u00a74), we consider the problem of learning the model parameters ( \u00a75).", "labels": [], "entities": []}, {"text": "To this end, we propose an aggressive online algorithm that generalizes MIRA () to arbitrary loss functions.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.516765296459198}]}, {"text": "We adopt a family of losses subsuming CRFs () and structured SVMs ().", "labels": [], "entities": []}, {"text": "Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs.", "labels": [], "entities": []}, {"text": "Our experiments ( \u00a76) show state-of-the-art performance on dependency parsing benchmarks.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.791298121213913}]}], "datasetContent": [{"text": "We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task () and two datasets for English: one from the CoNLL-2008 shared task (, which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of, in which all parse trees are projective.", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.8555357456207275}, {"text": "Penn Treebank", "start_pos": 244, "end_pos": 257, "type": "DATASET", "confidence": 0.9927225112915039}]}, {"text": "1,  The loopy BP algorithm managed to converge for nearly all sentences (with message damping).", "labels": [], "entities": []}, {"text": "The last three columns show the beneficial effect of unsupported features for the SVM case (with a more powerful model with non-projectivity features).", "labels": [], "entities": []}, {"text": "For most languages, unsupported features convey helpful information, which can be used with little extra cost (on average, 2.5 times more features are instantiated).", "labels": [], "entities": []}, {"text": "A combination of the techniques discussed here yields parsers that are inline with very strong competitors-for example, the parser of , which is exact, third-order, and constrains the outputs to be projective, does not outperform ours on the projective English dataset.", "labels": [], "entities": []}, {"text": "Finally, shows results obtained for different settings of \u03b2 and \u03b3.", "labels": [], "entities": []}, {"text": "Interestingly, we observe that higher scores are obtained for loss functions that are \"between\" SVMs and CRFs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Varying \u03b2 and \u03b3: neither the CRF nor the  SVM is optimal. Results are UAS on the English Non- Projective dataset, with \u03bb tuned with dev.-set validation.", "labels": [], "entities": [{"text": "UAS", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9158895611763}, {"text": "English Non- Projective dataset", "start_pos": 91, "end_pos": 122, "type": "DATASET", "confidence": 0.7074048399925232}]}]}