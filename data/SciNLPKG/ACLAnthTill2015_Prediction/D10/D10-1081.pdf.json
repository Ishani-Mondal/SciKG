{"title": [{"text": "An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.674332782626152}]}], "abstractContent": [{"text": "This paper proposes a fast and simple unsuper-vised word segmentation algorithm that utilizes the local predictability of adjacent character sequences, while searching fora least-effort representation of the data.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7501611709594727}]}, {"text": "The model uses branching entropy as a means of constraining the hypothesis space, in order to efficiently obtain a solution that minimizes the length of a two-part MDL code.", "labels": [], "entities": []}, {"text": "An evaluation with corpora in Japanese, Thai, English, and the \"CHILDES\" corpus for research in language development reveals that the algorithm achieves an accuracy, comparable to that of the state-of-the-art methods in unsupervised word segmentation, in a significantly reduced computational time.", "labels": [], "entities": [{"text": "CHILDES\" corpus", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8069371779759725}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9993769526481628}, {"text": "word segmentation", "start_pos": 233, "end_pos": 250, "type": "TASK", "confidence": 0.7390692830085754}]}], "introductionContent": [{"text": "As an inherent preprocessing step to nearly all NLP tasks for writing systems without orthographical marking of word boundaries, such as Japanese and Chinese, the importance of word segmentation has lead to the emergence of a micro-genre in NLP focused exclusively on this problem.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.7240119874477386}]}, {"text": "Supervised probabilistic models such as Conditional Random Fields (CRF) () have a wide application to the morphological analysis of these languages.", "labels": [], "entities": []}, {"text": "However, the development of the annotated training corpora necessary for their functioning is a labor-intensive task, which involves multiple stages of manual tagging.", "labels": [], "entities": []}, {"text": "Because of the scarcity of labeled data, the domain adaptation of morphological analyzers is also problematic, and semi-supervised algorithms that address this issue have also been proposed (e.g..", "labels": [], "entities": []}, {"text": "Recent advances in unsupervised word segmentation have been promoted by human cognition research, where it is involved in the modeling of the mechanisms that underlie language acquisition.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7142975479364395}]}, {"text": "Another motivation to study unsupervised approaches is their potential to support the domain adaptation of morphological analyzers through the incorporation of unannotated training data, thus reducing the dependency on costly manual work.", "labels": [], "entities": []}, {"text": "Apart from the considerable difficulties in discovering reliable criteria for word induction, the practical application of such approaches is impeded by their prohibitive computational cost.", "labels": [], "entities": [{"text": "word induction", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.8117274641990662}]}, {"text": "In this paper, we address the issue of achieving high accuracy in a practical computational time through an efficient method that relies on a combination of evidences: the local predictability of character patterns, and the reduction of effort achieved by a given representation of the language data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9894396662712097}]}, {"text": "Both of these criteria are assumed to play a key role in native language acquisition.", "labels": [], "entities": [{"text": "native language acquisition", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6944360335667928}]}, {"text": "The proposed model allows experimentation in a more realistic setting, where the learner is able to apply them simultaneously.", "labels": [], "entities": []}, {"text": "The method shows a high performance in terms of accuracy and speed, can be applied to language samples of substantial length, and generalizes well to corpora in different languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9991017580032349}, {"text": "speed", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9662488102912903}]}], "datasetContent": [{"text": "We evaluated the proposed model against four datasets.", "labels": [], "entities": []}, {"text": "The first one is the Bernstein-Ratner corpus for language acquisition based on transcripts from the CHILDES database.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7125605791807175}, {"text": "CHILDES database", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.9163202047348022}]}, {"text": "It comprises phonetically transcribed utterances of adult speech directed to 13 through 21-month-old children.", "labels": [], "entities": []}, {"text": "We evaluated the performance of our learner in the cases when the few boundaries among the individual sentences are available to it (B), and when it starts from a blank state (N).", "labels": [], "entities": []}, {"text": "The Kyoto University Corpus () is a standard dataset for Japanese morphological and dependency structure analysis, which comprises newspaper articles and editorials from the Mainichi Shimbun.", "labels": [], "entities": [{"text": "Kyoto University Corpus", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9209691882133484}, {"text": "Japanese morphological and dependency structure analysis", "start_pos": 57, "end_pos": 113, "type": "TASK", "confidence": 0.7039112647374471}, {"text": "Mainichi Shimbun", "start_pos": 174, "end_pos": 190, "type": "DATASET", "confidence": 0.9716905653476715}]}, {"text": "The BEST corpus for word segmentation and named entity recognition in Thai language combines text from a variety of sources in-  cluding encyclopedias (E), newspaper articles (N), scientific articles (A), and novels (F).", "labels": [], "entities": [{"text": "BEST corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.866348385810852}, {"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.699980229139328}, {"text": "named entity recognition", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6129603187243143}]}, {"text": "The WSJ subset of the Penn Treebank II Corpus incorporates selected stories from the Wall Street.", "labels": [], "entities": [{"text": "WSJ subset of the Penn Treebank II Corpus", "start_pos": 4, "end_pos": 45, "type": "DATASET", "confidence": 0.9529041573405266}, {"text": "Wall Street", "start_pos": 85, "end_pos": 96, "type": "DATASET", "confidence": 0.9685416519641876}]}, {"text": "Both the original text (O), and aversion in which all characters were converted to lowercase (L) were used.", "labels": [], "entities": []}, {"text": "The datasets listed above were built by removing the tags and blank spaces found in the corpora, and concatenating the remaining text.", "labels": [], "entities": []}, {"text": "We added two more training datasets for Japanese, which were used in a separate experiment solely for the acquisition of frequency statistics.", "labels": [], "entities": []}, {"text": "One of them was created from 200,000 randomly chosen Wikipedia articles, stripped from structural elements.", "labels": [], "entities": []}, {"text": "The other one contains text from the year 2005 issues of Asahi Newspaper.", "labels": [], "entities": [{"text": "Asahi Newspaper", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8779045045375824}]}, {"text": "Statistics regarding all described datasets are presented in.", "labels": [], "entities": []}, {"text": "One whole corpus is segmented in each experiment, in order to avoid the statement of an extended model that would allow the separation of training and test data.", "labels": [], "entities": []}, {"text": "This setting is also necessary for the direct comparison between the proposed model and other recent methods evaluated against the entire CHILDES corpus.", "labels": [], "entities": [{"text": "CHILDES corpus", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.9425630569458008}]}, {"text": "We report the obtained precision, recall and Fscore values calculated using boundary, token and type counts.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9995255470275879}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996201992034912}, {"text": "Fscore", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9994750618934631}]}, {"text": "Precision (P) and recall (R) are defined as: P = #correct units # output units , R = #correct units #gold standard units . Boundary, token and lexicon F-scores, denoted as B-F and T -F and L-F , are calculated as the  harmonic averages of the corresponding precision and recall values (F = 2P R/(P + R)).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9778202176094055}, {"text": "recall (R)", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.946465864777565}, {"text": "precision", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9978829026222229}, {"text": "recall", "start_pos": 271, "end_pos": 277, "type": "METRIC", "confidence": 0.9692752361297607}]}, {"text": "As a rule, boundary-based evaluation produces the highest scores among the three evaluation modes, as it only considers the correspondence between the proposed and the gold standard boundaries at the individual positions of the corpora.", "labels": [], "entities": []}, {"text": "Token-based evaluation is more strict -it accepts a word as correct only if its beginning and end are identified accurately, and no additional boundaries lie in between.", "labels": [], "entities": [{"text": "Token-based evaluation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9087026417255402}]}, {"text": "Lexiconbased evaluation reflects the extent to which the vocabulary of the original text has been recovered.", "labels": [], "entities": []}, {"text": "It provides another useful perspective for the error analysis, which in combination with token scores can give a better idea of the relationship between the accuracy of induction and item frequency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9982678890228271}]}, {"text": "The system was implemented in Java, however it handled the suffix arrays through an external C library called Sary.", "labels": [], "entities": []}, {"text": "1 All experiments were conducted on a 2 GHz Core2Duo T7200 machine with 2 GB RAM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Length in bits of the solutions proposed by Al- gorithm 1 with respect to the character n-gram order.", "labels": [], "entities": [{"text": "Length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9980467557907104}]}, {"text": " Table 2: Corpora used during the evaluation. Precise to- ken and type counts have been omitted for Wikipedia and  Asahi, as no gold standard segmentations are available.", "labels": [], "entities": [{"text": "Precise to- ken and type counts", "start_pos": 46, "end_pos": 77, "type": "METRIC", "confidence": 0.764674574136734}, {"text": "Wikipedia", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.9511536359786987}, {"text": "Asahi", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.7380645871162415}]}, {"text": " Table 3: Comparison of the proposed method (2a, 2b) with the model of Jin and Tanaka-Ishii (2006) (1). Execution  times include the obtaining of frequency statistics, and are represented by averages over 10 runs.", "labels": [], "entities": []}, {"text": " Table 4: Results obtained after the termination of Algo- rithm 3.", "labels": [], "entities": [{"text": "Algo- rithm", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.8910879294077555}]}, {"text": " Table 6: Experimental results for CHILDES-N with ran- domized initialization and search path. The numbers in  brackets represent the seed boundaries/character ratios.", "labels": [], "entities": [{"text": "CHILDES-N", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.7222380042076111}]}, {"text": " Table 7: Comparison of the proposed method (Ent-MDL) with the methods of Mochihashi et al., 2009 (NPY) and  Goldwater et al., 2009 (HDP).", "labels": [], "entities": []}]}