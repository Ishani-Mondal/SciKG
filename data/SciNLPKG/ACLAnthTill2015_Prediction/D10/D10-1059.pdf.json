{"title": [{"text": "Minimum Error Rate Training by Sampling the Translation Lattice", "labels": [], "entities": [{"text": "Minimum Error Rate", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.78428715467453}, {"text": "Translation Lattice", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.885394811630249}]}], "abstractContent": [{"text": "Minimum Error Rate Training is the algorithm for log-linear model parameter training most used in state-of-the-art Statistical Machine Translation systems.", "labels": [], "entities": [{"text": "Minimum Error Rate", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.6182250181833903}, {"text": "Statistical Machine Translation", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.7673444549242655}]}, {"text": "In its original formulation, the algorithm uses N-best lists output by the decoder to grow the Translation Pool that shapes the surface on which the actual optimization is performed.", "labels": [], "entities": []}, {"text": "Recent work has been done to extend the algorithm to use the entire translation lattice built by the decoder, instead of N-best lists.", "labels": [], "entities": []}, {"text": "We propose here a third, intermediate way, consisting in growing the translation pool using samples randomly drawn from the translation lattice.", "labels": [], "entities": []}, {"text": "We empirically measure a systematic improvement in the BLEU scores compared to training using N-best lists, without suffering the increase in computational complexity associated with operating with the whole lattice.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9985044002532959}]}], "introductionContent": [{"text": "Most state-of-the-art Statistical Machine Translation (SMT) systems are based on a log-linear model of the conditional probability of generating a certain translation given a specific source sentence.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.7928488353888193}]}, {"text": "More specifically, the conditional probability of a translation e and a word alignment a given a source sentence f is modeled as: * The work behind this paper was done during an internship at the Xerox Research Centre Europe.", "labels": [], "entities": [{"text": "Xerox Research Centre Europe", "start_pos": 196, "end_pos": 224, "type": "DATASET", "confidence": 0.7485563457012177}]}, {"text": "The author was partially supported by NSF through Grant CCF-0643593 and the AFOSR Young Investigator Research Program.", "labels": [], "entities": [{"text": "AFOSR Young Investigator Research Program", "start_pos": 76, "end_pos": 117, "type": "DATASET", "confidence": 0.8619562745094299}]}, {"text": "where the h k (e,a,f) are feature functions providing complementary sources of information on the quality of the produced translation (and alignment).", "labels": [], "entities": []}, {"text": "Once such a model is known: the decoder (i.e. the actual translation program), which builds a translation by searching in the space of all possible translations the one that maximizes the conditional probability: (e * , a * ) = arg max e,a where we have taken into account that the exponential is monotonic.", "labels": [], "entities": []}, {"text": "The parameters \u03bb k determine the relative importance of the different feature functions in the global score.", "labels": [], "entities": []}, {"text": "Best results are typically obtained by searching in the space of all possible parameter vectors \u00af \u03bb for the one that minimizes the error on a held-out development dataset for which one or more reference human translations are available, as measured by some automatic measure.", "labels": [], "entities": []}, {"text": "This procedure is referred to as Minimum Error Rate Training (MERT).", "labels": [], "entities": [{"text": "Minimum Error Rate Training (MERT", "start_pos": 33, "end_pos": 66, "type": "METRIC", "confidence": 0.8823840518792471}]}], "datasetContent": [{"text": "Experiments were conducted on the Europarl corpus with the split used for the WMT-08 shared task (Europarl training and test condition) for the language pairs English-French (En-Fr), English-Spanish (EnEs) and English-German (En-De), each in both directions.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9603469669818878}, {"text": "WMT-08", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.6425700187683105}]}, {"text": "Training corpora contain between 1.2 and 1.3 million sentence pairs each, development and test datasets are of size 2,000.", "labels": [], "entities": []}, {"text": "Detailed token and type statistics can be found in.", "labels": [], "entities": []}, {"text": "The Moses decoder ( was used for generating lattices and n-best lists.", "labels": [], "entities": []}, {"text": "The maximum number of decoding iterations was set to twelve.", "labels": [], "entities": []}, {"text": "Since Moses was run with its lexicalised dis- We assume all phrase pairs cover at least one source word.", "labels": [], "entities": []}, {"text": "tortion model, there were 14 features.", "labels": [], "entities": []}, {"text": "Moses L1-normalises the parameter vector: parameter scaling only marginally affects n-best list construction (via threshold pruning during decoding), while it substantially impacts sampling.", "labels": [], "entities": []}, {"text": "For each of the six configurations, we compared the BLEU score on the test data when optimizing feature weights with MERT using n-best and random samples of size 100 and 200.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9996337890625}, {"text": "MERT", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9508533477783203}]}, {"text": "In all cases we used 20 random restarts for MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.574510395526886}]}, {"text": "We also ran non systematic experiments on some of the configurations with larger samples and n-best lists, with results changing very little from the respective 200 cases: we do not report them here.", "labels": [], "entities": []}, {"text": "Learning curves (BLEU on the development set) are shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9980308413505554}]}, {"text": "Learning curves for the other tested language pairs follow a similar pattern.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set BLEU Scores for six different \"Source- Target\" Pairs", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9718430638313293}]}, {"text": " Table 2: Test set BLEU Scores for the same ''Source- Target\" pairs using a mixed strategy combining a 100 N- best list and a random sample of size 100 after each round  of decoding.", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9741837382316589}]}]}