{"title": [{"text": "Inducing Word Senses to Improve Web Search Result Clustering", "labels": [], "entities": [{"text": "Inducing Word Senses to Improve Web Search Result Clustering", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.801759680112203}]}], "abstractContent": [{"text": "In this paper, we present a novel approach to Web search result clustering based on the automatic discovery of word senses from raw text, a task referred to as Word Sense Induction (WSI).", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6913739740848541}, {"text": "Word Sense Induction (WSI)", "start_pos": 160, "end_pos": 186, "type": "TASK", "confidence": 0.7203957438468933}]}, {"text": "We first acquire the senses (i.e., meanings) of a query by means of a graph-based clustering algorithm that exploits cycles (triangles and squares) in the co-occurrence graph of the query.", "labels": [], "entities": []}, {"text": "Then we cluster the search results based on their semantic similarity to the induced word senses.", "labels": [], "entities": []}, {"text": "Our experiments, conducted on datasets of ambiguous queries, show that our approach improves search result clustering in terms of both clustering quality and degree of diversification.", "labels": [], "entities": [{"text": "search result clustering", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.5770305593808492}]}], "introductionContent": [{"text": "Over recent years increasingly huge amounts of text have been made available on the Web.", "labels": [], "entities": []}, {"text": "Popular search engines such as Yahoo! and Google usually do a good job at retrieving a small number of relevant results from such an enormous collection of Web pages (i.e. retrieving with high precision, low recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9950622916221619}, {"text": "recall", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.9987157583236694}]}, {"text": "However, current search engines are still facing the lexical ambiguity issue ( -i.e. the linguistic property owing to which any particular word may convey different meanings.", "labels": [], "entities": []}, {"text": "Ina recent study -conducted using WordNet () and Wikipedia as sources of ambiguous words -it was reported that around 3% of Web queries and 23% of the most frequent queries are ambiguous.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9329742789268494}]}, {"text": "Examples include: \"buy B-52\" (a cocktail?", "labels": [], "entities": []}, {"text": "tickets fora band?), \"Alexander Smith quotes\" (the novelist? the poet?), \"beagle search\" (dogs?", "labels": [], "entities": [{"text": "beagle search", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.6989109367132187}]}, {"text": "Ambiguity is often the consequence of the low number of query words entered on average by Web users).", "labels": [], "entities": [{"text": "Ambiguity", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9626004099845886}]}, {"text": "While average query length is increasing -it is now estimated at around 3 words per query 1 -many search engines such as Google have already started to tackle the query ambiguity issue by reranking and diversifying their results, so as to prevent Web pages that are similar to each other from ranking too high on the list.", "labels": [], "entities": []}, {"text": "In the past few years, Web clustering engines ( ) have been proposed as a solution to the lexical ambiguity issue in Web Information Retrieval.", "labels": [], "entities": [{"text": "Web clustering", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.6995192170143127}, {"text": "Web Information Retrieval", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.5693165262540182}]}, {"text": "These systems group search results, by providing a cluster for each specific aspect (i.e., meaning) of the input query.", "labels": [], "entities": []}, {"text": "Users can then select the cluster(s) and the pages therein that best answer their information needs.", "labels": [], "entities": []}, {"text": "However, many Web clustering engines group search results on the basis of their lexical similarity.", "labels": [], "entities": []}, {"text": "For instance, consider the following snippets returned for the beagle search query: 1.", "labels": [], "entities": []}, {"text": "Beagle is a search tool that ransacks your...", "labels": [], "entities": [{"text": "Beagle", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9352730512619019}]}, {"text": "2. ...the beagle disappearing in search of game...", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted our experiments on two datasets: \u2022 MORESQUE (MORE Sense-tagged QUEry results), anew dataset of 114 ambiguous queries which we developed as a complement to AMBI-ENT following the guidelines provided by its authors.", "labels": [], "entities": [{"text": "MORESQUE", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.972318172454834}, {"text": "MORE Sense-tagged QUEry results)", "start_pos": 58, "end_pos": 90, "type": "METRIC", "confidence": 0.7963101148605347}, {"text": "AMBI-ENT", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.9239760637283325}]}, {"text": "In fact, our aim was to study the behaviour of Web search algorithms on queries of different lengths, ranging from 1 to 4 words.", "labels": [], "entities": []}, {"text": "However, the AMBIENT dataset is composed mostly of single-word queries.", "labels": [], "entities": [{"text": "AMBIENT dataset", "start_pos": 13, "end_pos": 28, "type": "DATASET", "confidence": 0.9202451109886169}]}, {"text": "MORESQUE provides dozens of queries of length 2, 3 and 4, together with the 100 top results from Yahoo!", "labels": [], "entities": [{"text": "MORESQUE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.6290070414543152}]}, {"text": "for each query annotated as in the AMBIENT dataset (overall, we tagged 11,400 snippets).", "labels": [], "entities": [{"text": "AMBIENT dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9595757126808167}]}, {"text": "We decided to carry on using Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9194921553134918}]}, {"text": "We report the statistics on the composition of the two datasets in.", "labels": [], "entities": []}, {"text": "Given that the snippets could possibly be annotated with more than one Wikipedia subtopic, we also determined the average number of subtopics per snippet.", "labels": [], "entities": []}, {"text": "This amounted to 1.01 for AMBIENT and 1.04 for MORESQUE for snippets with at least one subtopic annotation (51% and 53% of the respective datasets).", "labels": [], "entities": [{"text": "AMBIENT", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.5568179488182068}, {"text": "MORESQUE", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.898248016834259}]}, {"text": "We can thus conclude that multiple subtopic annotations are infrequent.", "labels": [], "entities": []}, {"text": "Our graph-based algorithms have two parameters: the Dice threshold \u03b4 for graph construction (Section 3.1.1) and the threshold \u03c3 for edge removal (Section 3.1.2).", "labels": [], "entities": [{"text": "Dice threshold \u03b4", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.9461683233579}, {"text": "edge removal", "start_pos": 132, "end_pos": 144, "type": "TASK", "confidence": 0.7305397093296051}]}, {"text": "The best parameters, used throughout our experiments, were (\u03b4 = 0.00033, \u03c3 = 0.45) with triangles and (\u03b4 = 0.00033, \u03c3 = 0.33) with squares.", "labels": [], "entities": []}, {"text": "The parameter values were obtained as a result of tuning on a small in-house development dataset.", "labels": [], "entities": []}, {"text": "The dataset was built by automatically identifying monosemous words and creating pseudowords following the scheme proposed by.", "labels": [], "entities": []}, {"text": "We compared Triangles and Squares against the best systems reported by.", "labels": [], "entities": []}, {"text": "Section 2): \u2022 Lingo (): a Web clustering engine implemented in the Carrot 2 opensource framework 5 that clusters the most frequent phrases extracted using suffix arrays.", "labels": [], "entities": [{"text": "Carrot 2 opensource framework 5", "start_pos": 67, "end_pos": 98, "type": "DATASET", "confidence": 0.91823890209198}]}, {"text": "\u2022 Suffix Tree Clustering (STC)): the original Web search clustering approach based on suffix trees.", "labels": [], "entities": [{"text": "Suffix Tree Clustering (STC))", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.6210188815991083}, {"text": "Web search clustering", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.6239019831021627}]}, {"text": "\u2022 KeySRC: a state-of-theart Web clustering engine built on top of STC with part-of-speech pruning and dynamic selection of the cut-off level of the clustering dendrogram.", "labels": [], "entities": []}, {"text": "\u2022 Essential Pages (EP) (Swaminathan et al., 2009): a recent diversification algorithm that selects fundamental pages which maximize the amount of information covered fora given query.", "labels": [], "entities": []}, {"text": "\u2022 Yahoo!: the original search results returned by the Yahoo!", "labels": [], "entities": []}, {"text": "The first three of the above are Web search result clustering approaches, whereas the last two produce lists of possibly diversified results (cf. Section 2).", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.570045530796051}]}, {"text": "While assessing the quality of clustering is a notably hard problem, given a gold standard G we can calculate the Rand index (RI) of a clustering C, a common quality measure in the literature, determined as follows: where Wis the union set of all the words in C and \u03b4(w, w ) = 1 if any two words wand ware in the same cluster both in C and in the gold standard G or they are in two different clusters in both C and G, otherwise \u03b4(w, w ) = 0.", "labels": [], "entities": [{"text": "Rand index (RI)", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.9751063466072083}]}, {"text": "In other words, we calculate the percentage of word pairs that are in the same configuration in both C and G.", "labels": [], "entities": []}, {"text": "For the gold standard G we use the clustering induced by the sense annotations provided in our datasets for each snippet (i.e., each cluster contains the snippets manually associated with a particular Wikipedia subtopic).", "labels": [], "entities": []}, {"text": "Similarly to what was done in Section 3.2, untagged results are grouped together in a special cluster of G.", "labels": [], "entities": []}, {"text": "The results of all systems on the AM-BIENT and MORESQUE datasets according to the average Rand index are shown in 6 . In accordance with previous results in the literature, KeySRC performed generally better than the other search result clustering systems, especially on smaller queries.", "labels": [], "entities": [{"text": "AM-BIENT", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8457790017127991}, {"text": "MORESQUE datasets", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.7313592731952667}, {"text": "Rand index", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9379103779792786}]}, {"text": "Our Word Sense Induction systems, Squares and Triangles, outperformed all other systems by a large margin, thus showing a higher clustering quality (with the exception of KeySRC performing better than Triangles on AMBIENT).", "labels": [], "entities": [{"text": "KeySRC", "start_pos": 171, "end_pos": 177, "type": "DATASET", "confidence": 0.8204762935638428}, {"text": "AMBIENT", "start_pos": 214, "end_pos": 221, "type": "DATASET", "confidence": 0.9048598408699036}]}, {"text": "Interestingly, all clustering systems perform more poorly on longer queries (i.e., on the MORESQUE dataset), however our WSI systems, and especially Triangles, are more robust across query lengths.", "labels": [], "entities": [{"text": "MORESQUE dataset", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.7353359460830688}]}, {"text": "Compared to Triangles, the Squares algorithm performs better, confirming our hunch that Squares is a more solid graph pattern.", "labels": [], "entities": []}, {"text": "Search result clustering can also be used to diversify the top results returned by a search engine.", "labels": [], "entities": [{"text": "Search result clustering", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6003546416759491}]}, {"text": "Thus, for each query q, one natural way of measuring a system's performance is to calculate the subtopic recall-at-K ( given by the number of different subtopics retrieved for q in the top K results returned: where subtopics(r i ) is the set of subtopics manually assigned to the search result r i and M is the number of subtopics for query q (note that in our experiments M is the number of subtopics occurring in the 100 results retrieved for q, so S-recall@100 = 1).", "labels": [], "entities": [{"text": "recall-at-K", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.9238961338996887}]}, {"text": "However, this measure is only suitable for systems returning ranked lists (such as Yahoo! and EP  a clustering C = (C 0 , C 1 , . .", "labels": [], "entities": []}, {"text": ", Cm ), we flatten it to a list as follows: we add to the initially empty list the first element of each cluster C j (j = 1, . .", "labels": [], "entities": []}, {"text": ", m); then we iterate the process by selecting the second element of each cluster C j such that |C j | \u2265 2, and soon.", "labels": [], "entities": []}, {"text": "The remaining elements returned by the search engine, but not included in any cluster of C \\ {C 0 }, are appended to the bottom of the list in their original order.", "labels": [], "entities": []}, {"text": "Note that the elements are selected from each cluster according to their internal ranking (e.g., for our algorithms we use Formula 7 introduced in Section 3.2).", "labels": [], "entities": []}, {"text": "For the sake of clarity and to save space, we selected the best systems from our previous experiment, namely Squares, Triangles and KeySRC, and compared their output with the original snippet list returned by Yahoo! and the output of the EP diversification algorithm (cf. Section 4.1).", "labels": [], "entities": [{"text": "clarity", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9417375326156616}]}, {"text": "The S-recall@K (with K = 3, 5, 10, 15, 20) calculated on AMBIENT+MORESQUE is reported in.", "labels": [], "entities": [{"text": "AMBIENT", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.6247241497039795}, {"text": "MORESQUE", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.6298753619194031}]}, {"text": "Squares and Triangles show the highest degree of diversification, with a subtopic recall greater than all other systems, and with Squares consistently performing better than Triangles.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9614747762680054}]}, {"text": "It is interesting to observe that KeySRC performs worse than Yahoo! with low values of K and generally better with higher values of K.", "labels": [], "entities": [{"text": "KeySRC", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.730150580406189}]}, {"text": "Given that the two datasets complement each other in terms of query lengths (with AMBIENT having queries of length \u2264 2 and MORESQUE with many queries of length \u2265 3), we studied the Srecall@K trend for the two datasets.", "labels": [], "entities": [{"text": "AMBIENT", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.6608471870422363}, {"text": "MORESQUE", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.979105532169342}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "While KeySRC does not show large differences in the presence of short and long ambiguous queries, our graph-based algorithms do.", "labels": [], "entities": []}, {"text": "For instance, as soon as K = 3 the Squares algorithm obtains S-recall values of 37% and 57.5% on AMBIENT and MORESQUE, respectively.", "labels": [], "entities": [{"text": "AMBIENT", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.5087261199951172}, {"text": "MORESQUE", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.8028373122215271}]}, {"text": "The   difference decreases as K increases, but is still significant when K = 10.", "labels": [], "entities": [{"text": "K", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9916720986366272}]}, {"text": "We hypothesize that, because they are less ambiguous, longer queries are easier to diversify with the aid of WSI.", "labels": [], "entities": [{"text": "WSI", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8037257790565491}]}, {"text": "However, we note that, even with low values of K, Squares and Triangles obtain higher S-recall than the other systems (with KeySRC competing on AMBIENT when K \u2264 15).", "labels": [], "entities": [{"text": "S-recall", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9288883805274963}, {"text": "KeySRC", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.8596576452255249}, {"text": "AMBIENT", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.7609772086143494}]}, {"text": "Finally, we observe that -with low values of K -the Squares algorithm performs significantly better than Triangles on shorter queries, and only slightly better on longer ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the datasets of ambiguous queries.", "labels": [], "entities": []}, {"text": " Table 2: Results by Rand index (percentages).", "labels": [], "entities": [{"text": "Rand index", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9811490774154663}]}, {"text": " Table 3: S-recall@K on all queries (percentages).", "labels": [], "entities": []}]}