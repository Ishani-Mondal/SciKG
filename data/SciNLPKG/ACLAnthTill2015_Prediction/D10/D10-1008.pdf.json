{"title": [{"text": "Automatically Producing Plot Unit Representations for Narrative Text", "labels": [], "entities": [{"text": "Producing Plot Unit Representations", "start_pos": 14, "end_pos": 49, "type": "TASK", "confidence": 0.6299156248569489}, {"text": "Narrative Text", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.8941855430603027}]}], "abstractContent": [{"text": "In the 1980s, plot units were proposed as a conceptual knowledge structure for representing and summarizing narrative stories.", "labels": [], "entities": [{"text": "summarizing narrative stories", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.8300533294677734}]}, {"text": "Our research explores whether current NLP technology can be used to automatically produce plot unit representations for narrative text.", "labels": [], "entities": []}, {"text": "We create a system called AESOP that exploits a variety of existing resources to identify affect states and applies \"projection rules\" to map the affect states onto the characters in a story.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.6857532858848572}]}, {"text": "We also use corpus-based techniques to generate anew type of affect knowledge base: verbs that impart positive or negative states onto their patients (e.g., being eaten is an undesirable state, but being fed is a desirable state).", "labels": [], "entities": []}, {"text": "We harvest these \"patient polarity verbs\" from a Web corpus using two techniques: co-occurrence with Evil/Kind Agent patterns, and bootstrapping over conjunctions of verbs.", "labels": [], "entities": []}, {"text": "We evaluate the plot unit representations produced by our system on a small collection of Aesop's fables.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the 1980s, plot units were proposed as a knowledge structure for representing narrative stories and generating summaries.", "labels": [], "entities": []}, {"text": "Plot units are fundamentally different from the story representations that preceded them because they focus on the affect states of characters and the tensions between them as the driving force behind interesting and cohesive stories.", "labels": [], "entities": []}, {"text": "Plot units were used in narrative summarization studies, both in computer science and psychology ( , but previous computational models of plot units relied on tremendous amounts of manual knowledge engineering.", "labels": [], "entities": [{"text": "narrative summarization studies", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.7482970356941223}]}, {"text": "The last few decades have seen tremendous advances in NLP and the emergence of many resources that could be useful for plot unit analysis.", "labels": [], "entities": [{"text": "NLP", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9206916689872742}, {"text": "plot unit analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.781480610370636}]}, {"text": "So we embarked on a project to see whether plot unit representations can be generated automatically using current NLP technology.", "labels": [], "entities": []}, {"text": "We created a system called AESOP that uses a variety of resources to identify words that correspond to positive, negative, and mental affect states.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.6943868398666382}]}, {"text": "AESOP uses affect projection rules to map the affect states onto the characters in the story based on verb argument structure.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.824245274066925}]}, {"text": "Additionally, affect states are inferred based on syntactic properties, and causal and cross-character links are created using simple heuristics.", "labels": [], "entities": []}, {"text": "Affect states often arise from actions that produce good or bad states for the character that is acted upon.", "labels": [], "entities": [{"text": "Affect states", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7811222672462463}]}, {"text": "For example, \"the cat ate the mouse\" produces a negative state for the mouse because being eaten is bad.", "labels": [], "entities": []}, {"text": "Similarly, \"the man fed the dog\" produces a positive state for the dog because being fed is generally good.", "labels": [], "entities": []}, {"text": "Knowledge about the effects of actions (i.e., state changes) on patients is not readily available in existing semantic resources.", "labels": [], "entities": []}, {"text": "We create anew type of lexicon consisting of patient polarity verbs (PPVs) that impart positive or negative states on their patients.", "labels": [], "entities": []}, {"text": "These verbs reflect world knowledge about desirable/undesirable states for animate beings; for example, being fed, paid or adopted are generally desirable states, while being eaten, chased or hospitalized are generally undesirable states.", "labels": [], "entities": []}, {"text": "We automatically generate a lexicon of \"patient polarity verbs\" from a Web corpus using two tech-The Father and His Sons (s1) A father had a family of sons who were perpetually quarreling among themselves.", "labels": [], "entities": []}, {"text": "(s2) When he failed to heal their disputes by his exhortations, he determined to give them a practical illustration of the evils of disunion; and for this purpose he one day told them to bring him a bundle of sticks.", "labels": [], "entities": []}, {"text": "(s3) When they had done so, he placed the faggot into the hands of each of them in succession, and ordered them to break it in pieces.", "labels": [], "entities": []}, {"text": "(s4) They tried with all their strength, and were notable to do it.", "labels": [], "entities": []}, {"text": "(s5) He next opened the faggot, took the sticks separately, one by one, and again put them into his sons' hands, upon which they broke them easily.", "labels": [], "entities": []}, {"text": "(s6) He then addressed them in these words: \"My sons, if you are of one mind, and unite to assist each other, you will be as this faggot, uninjured by all the attempts of your enemies; but if you are divided among yourselves, you will be broken as easily as these sticks.\"", "labels": [], "entities": []}], "datasetContent": [{"text": "Plot unit analysis of narrative text is enormously complex -the idea of creating gold standard plot unit annotations seemed like a monumental task.", "labels": [], "entities": [{"text": "Plot unit analysis of narrative text", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7833978533744812}]}, {"text": "So we began with relatively simple and constrained texts that seemed appropriate: fables.", "labels": [], "entities": []}, {"text": "Fables have two desirable attributes: (1) they have a small cast of characters, and (2) they typically revolve around amoral, which is exemplified by a short and concise plot.", "labels": [], "entities": []}, {"text": "Even so, fables are challenging for NLP due to anthropomorphic characters, flowery language, and sometimes archaic vocabulary.", "labels": [], "entities": []}, {"text": "We collected 34 Aesop's fables from a website 4 , choosing fables that have a true plot (some only contain quotes) and exactly two characters.", "labels": [], "entities": []}, {"text": "We divided them into a development set of 11 stories, a tuning set of 8 stories, and a test set of 15 stories.", "labels": [], "entities": []}, {"text": "Creating a gold standard was itself a substantial undertaking, and training non-experts to produce them did not seem feasible in the short term.", "labels": [], "entities": []}, {"text": "So the authors discussed and iteratively refined manual annotations for the development and tuning sets until we produced similar results and had a common understanding of the task.", "labels": [], "entities": []}, {"text": "Then two authors independently created annotations for the test set, and a third author adjudicated the differences.", "labels": [], "entities": []}, {"text": "For evaluation, we used recall (R), precision (P), and F-measure (F).", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.941495880484581}, {"text": "precision (P)", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9515933990478516}, {"text": "F-measure (F)", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.9591693729162216}]}, {"text": "In our gold standard, each affect state is annotated with the set of clauses that could legitimately produce it.", "labels": [], "entities": []}, {"text": "In most cases (75%), we were able to ascribe the existence of a state to precisely one clause.", "labels": [], "entities": []}, {"text": "During evaluation, the systemproduced affect states must be generated from the correct clause.", "labels": [], "entities": []}, {"text": "However, for affect states that could be ascribed to multiple clauses in a sentence, the evaluation was done at the sentence level.", "labels": [], "entities": []}, {"text": "In this case, the system-produced affect state must come from the sentence that contains one of those clauses.", "labels": [], "entities": []}, {"text": "Coreference resolution is far from perfect, so we created gold standard coreference annotations for our fables and used them for most of our experiments.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8508565425872803}]}, {"text": "This allowed us to evaluate our approach without coreference mistakes factoring in.", "labels": [], "entities": []}, {"text": "In Section 5.5, we re-evaluate our final results using automatic coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9475609660148621}]}, {"text": "Our first set of experiments evaluates the quality of the affect states produced by AESOP using only the external resources.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.7306258678436279}]}, {"text": "The top half of shows the results for each resource independently.", "labels": [], "entities": []}, {"text": "FrameNet produced the best results, yielding much higher recall than any other resource.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8637067675590515}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9993904829025269}]}, {"text": "The bottom half of Ta-  ble 1 shows the results when combining FrameNet with other resources.", "labels": [], "entities": [{"text": "Ta-  ble 1", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.7588336020708084}]}, {"text": "In terms of F score, the only additive benefit came from the Semantic Orientation Lexicon, which produced a better balance of recall and precision and an F score gain of +2.", "labels": [], "entities": [{"text": "F score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9916546642780304}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9988517761230469}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9967795014381409}, {"text": "F score gain", "start_pos": 154, "end_pos": 166, "type": "METRIC", "confidence": 0.9882492820421854}]}, {"text": "Our second set of experiments evaluates the quality of the automatically generated PPV lexicons.", "labels": [], "entities": []}, {"text": "The top portion of shows the results for the negative PPVs.", "labels": [], "entities": []}, {"text": "The PPVs harvested by the Evil Agent patterns produced the best results, yielding recall and precision of .46 for negative states.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9997382760047913}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9980144500732422}]}, {"text": "Note that M and + states are also generated from the negative PPVs because they are inferred during affect projection (Section 4.1.4).", "labels": [], "entities": [{"text": "affect projection", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7096974700689316}]}, {"text": "The polarity of a negative PPV can also be flipped by negation to produce a + state.", "labels": [], "entities": []}, {"text": "Basilisk's negative PPVs achieved similar precision but lower recall.", "labels": [], "entities": [{"text": "Basilisk's negative PPVs", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8679057657718658}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9994868040084839}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9992994070053101}]}, {"text": "We see no additional recall and some precision loss when the Evil Agent and Basilisk PPV lists are combined.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9997020363807678}, {"text": "precision loss", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.985853910446167}, {"text": "Basilisk PPV lists", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.8574395775794983}]}, {"text": "The precision drop is likely due to redundancy, which creates spurious affect states.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991075396537781}]}, {"text": "If two different words have negative polarity but refer to the same event, then only one negative affect state should be generated.", "labels": [], "entities": []}, {"text": "But AE-SOP will generate two affect states, so one will be spurious.", "labels": [], "entities": [{"text": "AE-SOP", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9831172227859497}]}, {"text": "The middle section of shows the results for the positive PPVs.", "labels": [], "entities": []}, {"text": "Both positive PPV lexicons were of dubious quality, so we tried to extract a highquality subset of each list.", "labels": [], "entities": []}, {"text": "For the Kind Agent PPVs, we computed the ratio of the frequency of the verb with Evil Agents versus Kind Agents and only saved verbs with an Evil:Kind ratio (\u03b8) > 1, which yielded 1203 PPVs.", "labels": [], "entities": []}, {"text": "For the positive Basilisk PPVs, we used only the top 100 lexicon and top 100 context verbs, which yielded 164 unique verbs.", "labels": [], "entities": []}, {"text": "The positive PPVs did generate several correct affect states (including a -state when a positive PPV was negated), but also many spurious states.", "labels": [], "entities": []}, {"text": "The bottom section of shows the impact of the learned PPVs when combined with FrameNet and the Semantic Orientation Lexicon (SOLex).", "labels": [], "entities": []}, {"text": "Adding the Evil Agent PPVs improved AESOP's F score from 39% to 44%, mainly due to a +8 recall gain.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.8024764657020569}, {"text": "F score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9848523437976837}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9894931316375732}]}, {"text": "The recall of the -states increased from 22% to 46% with no loss of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9988332390785217}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9993174076080322}]}, {"text": "Interestingly, if we remove SOLex and use only FrameNet with our PPVs, precision increases from 46% to 49% and recall only drops by -1.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9996795654296875}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9996345043182373}]}, {"text": "Finally, the last row of shows that adding Basilisk's positive PPVs produces a small recall boost (+2) with a slight drop in precision (-1).", "labels": [], "entities": [{"text": "Basilisk's positive PPVs", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.8449448198080063}, {"text": "recall boost", "start_pos": 85, "end_pos": 97, "type": "METRIC", "confidence": 0.9868937730789185}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9995054006576538}]}, {"text": "Evaluating the impact of PPVs on plot unit structures is an indirect way of assessing their quality because creating plot units involves many steps.", "labels": [], "entities": []}, {"text": "Also, our test set is small so many verbs will never appear.", "labels": [], "entities": []}, {"text": "To directly measure the quality of our PPVs, we recruited 3 people to manually review them.", "labels": [], "entities": []}, {"text": "We developed annotation guidelines that instructed each annotator to judge whether a verb is generally good or bad for its patient, assuming the patient is animate.", "labels": [], "entities": []}, {"text": "They assigned each verb to one of 6 categories: \u00d7 (not a verb), 2 (always good), 1 (usually good), 0 (neutral, mixed, or requires inanimate patient), -1 (usually bad), -2 (always bad).", "labels": [], "entities": []}, {"text": "Each annotator labeled 250 words: 50 words randomly sampled from each of our 4 PPV lexicons 5 (Evil Agent PPVs, Kind Agent PPVs, Positive Basilisk PPVs, and Negative Basilisk PPVs) plus 50 verbs labeled as neutral in the MPQA lexicon.", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 221, "end_pos": 233, "type": "DATASET", "confidence": 0.9758078157901764}]}, {"text": "First, we measured agreement based on three groupings: negative (-2 and -1), neutral (0), or positive (1 and 2).", "labels": [], "entities": []}, {"text": "We computed \u03ba scores to measure inter-annotator agreement for each pair of annotators.", "labels": [], "entities": []}, {"text": ", but the \u03ba scores were relatively low because the annotators had trouble distinguishing the positive cases from the neutral ones.", "labels": [], "entities": []}, {"text": "So we re-computed agreement using two groupings: negative (-2 and -1) and not-negative (0 through 2), and obtained \u03ba scores of .69, .71, and .74.", "labels": [], "entities": []}, {"text": "We concluded that people largely agree on whether a verb is bad for the patient, but they do not necessarily agree if a verb is good for the patient.", "labels": [], "entities": []}, {"text": "One possible explanation is that many \"bad\" verbs represent physical harm or danger: these verbs are both plentiful and easy to recognize.", "labels": [], "entities": []}, {"text": "In contrast, \"good\" verbs are often more abstract and open to interpretation (e.g., is being \"envied\" or \"feared\" a good thing?).", "labels": [], "entities": []}, {"text": "We used the labels produced by the two annotators with the highest \u03ba score to measure the accuracy of our PPVs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9991316199302673}]}, {"text": "Both the Evil Agent and Negative Basilisk PPVs were judged to be 72.5% accurate, averaged over the judges.", "labels": [], "entities": [{"text": "accurate", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9956091046333313}]}, {"text": "The Kind Agent PPVs were only about 39% accurate, while the Positive Basilisk PPVs were nearly 50% accurate.", "labels": [], "entities": [{"text": "Kind Agent PPVs", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.5562664171059927}, {"text": "accurate", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9852514266967773}]}, {"text": "These results are consistent with our impressions that the negative PPVs are of relatively high quality, while the positive PPVs are mixed.", "labels": [], "entities": []}, {"text": "Some examples of learned PPVs that were not present in our other resources are: -: censor, chase, fire, orphan, paralyze, scare, sue + : accommodate, harbor, nurse, obey, respect, value  We represented each link as a 5-tuple src-clause, src-state, tgt-clause, tgt-state, link-type, where source/target denotes the direction of the link, the source/target-states are the affect state type (+,-,M) and link-type is one of 3 types: actualization (a), motivation (m), or cross-character (xchar).", "labels": [], "entities": []}, {"text": "A system-produced link is considered correct if all 5 elements of the tuple match the human annotation.", "labels": [], "entities": []}, {"text": "The second column of shows the performance of AESOP when using gold standard affect states.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9794156551361084}]}, {"text": "Our simple heuristics for creating links work surprisingly well for xchar and a links when given perfect affect states.", "labels": [], "entities": []}, {"text": "However, these heuristics produce relatively low precision form links, albeit with 100% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9756171703338623}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9988478422164917}]}, {"text": "This reveals that m links primarily do connect adjacent states, but we need to be more discriminating when connecting them.", "labels": [], "entities": []}, {"text": "The third column of shows the results when using systemgenerated affect states.", "labels": [], "entities": []}, {"text": "We see that performance is much lower.", "labels": [], "entities": []}, {"text": "This is not particularly surprising, since AESOP's F-score is 45%, so over half of the individual states are wrong, which means that less than a quarter of the pairs are correct.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.6970293521881104}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9777345061302185}]}, {"text": "From that perspective, the xchar link performance is reasonable, but the causal a and m links need improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results for AESOP using external resources. The # in parentheses is the # of gold affect states.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.5235061645507812}]}, {"text": " Table 2: Evaluation results for AESOP with PPVs. The # in parentheses is the # of gold affect states.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.6170104146003723}]}, {"text": " Table 3: Link results; parentheses show # of gold links.", "labels": [], "entities": []}]}