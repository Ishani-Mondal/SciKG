{"title": [{"text": "Function-based question classification for general QA", "labels": [], "entities": [{"text": "Function-based question classification", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.659042626619339}, {"text": "QA", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.4699399769306183}]}], "abstractContent": [{"text": "In contrast with the booming increase of inter-net data, state-of-art QA (question answering) systems, otherwise, concerned data from specific domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way.", "labels": [], "entities": [{"text": "QA (question answering)", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.6684042632579803}]}, {"text": "Users may welcome a more general QA system for its capability to answer questions of various sources, integrated from existed specialized sub-QA engines.", "labels": [], "entities": []}, {"text": "In this framework, question classification is the primary task.", "labels": [], "entities": [{"text": "question classification", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8728012144565582}]}, {"text": "However, the current paradigms of question classification were focused on some specified type of questions, i.e. factoid questions, which are inappropriate for the general QA.", "labels": [], "entities": [{"text": "question classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8025373816490173}]}, {"text": "In this paper, we propose anew question classification paradigm, which includes a question taxonomy suitable to the general QA and a question classifier based on MLN (Markov logic network), where rule-based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach.", "labels": [], "entities": [{"text": "question classification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7785865366458893}]}, {"text": "Experiments show that our method outperforms traditional question classification approaches.", "labels": [], "entities": [{"text": "question classification", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.7554446756839752}]}], "introductionContent": [{"text": "During along period of time, researches on question answering are mainly focused on finding short and concise answers from plain text for factoid questions driven by annual trackes such as CLEF, TREC and NTCIR.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8725165128707886}, {"text": "CLEF", "start_pos": 189, "end_pos": 193, "type": "DATASET", "confidence": 0.9028629660606384}, {"text": "NTCIR", "start_pos": 204, "end_pos": 209, "type": "DATASET", "confidence": 0.93724125623703}]}, {"text": "However, people usually ask more complex questions in real world which cannot be handled by these QA systems tailored to factoid questions.", "labels": [], "entities": []}, {"text": "During recent years, social collaborative applications begin to flourish, such as Wikipedia, Facebook, Yahoo!", "labels": [], "entities": []}, {"text": "A large amount of semistructured data, which has been accumulated from these services, becomes new sources for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.9331692755222321}]}, {"text": "Previous researches show that different sources are suitable for answering different questions.", "labels": [], "entities": []}, {"text": "For example, the answers for factoid questions can be extracted from webpages with high accuracy, definition questions can be answered by corresponding articles in wikipedia( while community question answering services provide comprehensive answers for complex questions).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.990714430809021}, {"text": "question answering", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.7518795132637024}]}, {"text": "It will greatly enhance the overall performance if we can classify questions into several types, distribute each type of questions to suitable sources and trigger corresponding strategy to summarize returned answers.", "labels": [], "entities": [{"text": "summarize returned answers", "start_pos": 189, "end_pos": 215, "type": "TASK", "confidence": 0.8523967464764913}]}, {"text": "Question classification (QC) in factoid QA is to provide constraints on answer types that allows further processing to pinpoint and verify the answer ().", "labels": [], "entities": [{"text": "Question classification (QC)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8231217086315155}]}, {"text": "Usually, questions are classified into a fine grained content-based taxonomy(e.g. UIUC taxonomy ().", "labels": [], "entities": [{"text": "UIUC taxonomy", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9048866629600525}]}, {"text": "We cannot use these taxonomies directly.", "labels": [], "entities": []}, {"text": "To guide question distribution and answer summarization, questions are classified according to their functions instead of contents.", "labels": [], "entities": [{"text": "question distribution", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7217496335506439}, {"text": "answer summarization", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6772613227367401}]}, {"text": "Motivated by related work on user goal classification) , we propose a function-based question classification category tailored to general QA.", "labels": [], "entities": [{"text": "user goal classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.6935355067253113}, {"text": "function-based question classification category", "start_pos": 70, "end_pos": 117, "type": "TASK", "confidence": 0.7242092043161392}]}, {"text": "The category contain six types, namely Fact, List, Reason, Solution, Definition and Navigation.", "labels": [], "entities": []}, {"text": "We will introduced this category in detail in Section 2.", "labels": [], "entities": []}, {"text": "To classify questions effectively, we unify rulebased methods and statistical methods into a single framework.", "labels": [], "entities": []}, {"text": "Each question is splited into functional words and content words.", "labels": [], "entities": []}, {"text": "We generate strict patterns from functional words and soft patterns from content words.", "labels": [], "entities": []}, {"text": "Each strict pattern is a regular expression while each soft pattern is a bi-gram cluster.", "labels": [], "entities": []}, {"text": "Given a question, we will evaluate its matching degree to each patterns.", "labels": [], "entities": []}, {"text": "The matching degree is either 0 or 1 for strict pattern and between 0 and 1 for soft pattern.", "labels": [], "entities": []}, {"text": "Finally, Markov logic network (MLN) () is used to combine and evaluate all the patterns.", "labels": [], "entities": []}, {"text": "The classical MLN maximize the probability of an assignment of truth values by evaluating the weights of each formula.", "labels": [], "entities": []}, {"text": "However, the real world is full of uncertainty and is unnatural to be represented by a set of boolean values.", "labels": [], "entities": []}, {"text": "In this paper, we propose fuzzy discriminative weight learning of Markov logic network.", "labels": [], "entities": [{"text": "fuzzy discriminative weight learning", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.5977448299527168}]}, {"text": "This method takes degrees of confidence of each evidence predicates into account thus can model the matching degrees between questions and soft patterns.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: In the next section we review related work on question classification, query classification and Markov logic network.", "labels": [], "entities": [{"text": "question classification", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.8235157132148743}, {"text": "query classification", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7804281115531921}]}, {"text": "Section 2 gives a detailed introduction to our new taxonomy for general QA.", "labels": [], "entities": [{"text": "general QA", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.565886527299881}]}, {"text": "Section 4 introduces fuzzy discriminative weight learning of MLN and our methodology to extract strict and soft patterns.", "labels": [], "entities": [{"text": "fuzzy discriminative weight learning", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.5770804733037949}]}, {"text": "In Section 5 we compare our method with previous methods on Chinese question data from Baidu Zhidao and Sina iAsk.", "labels": [], "entities": []}, {"text": "In the last section we conclude this work.", "labels": [], "entities": []}, {"text": "Although we build patterns and do experiments on Chinese questions, our method does not take advantage of the particularity of Chinese language and thus can be easily implemented on other languages.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Experimental results on Chinese cQA data", "labels": [], "entities": [{"text": "Chinese cQA data", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.8809439341227213}]}, {"text": " Table 5: Precision, recall and F-score on each type", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9993945360183716}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9994738698005676}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9993103742599487}]}]}