{"title": [{"text": "Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid", "labels": [], "entities": []}], "abstractContent": [{"text": "Discovering and summarizing opinions from online reviews is an important and challenging task.", "labels": [], "entities": [{"text": "Discovering and summarizing opinions from online reviews", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7857457782541003}]}, {"text": "A commonly-adopted framework generates structured review summaries with aspects and opinions.", "labels": [], "entities": []}, {"text": "Recently topic models have been used to identify meaningful review aspects, but existing topic models do not identify aspect-specific opinion words.", "labels": [], "entities": []}, {"text": "In this paper, we propose a MaxEnt-LDA hybrid model to jointly discover both aspects and aspect-specific opinion words.", "labels": [], "entities": []}, {"text": "We show that with a relatively small amount of training data, our model can effectively identify aspect and opinion words simultaneously.", "labels": [], "entities": []}, {"text": "We also demonstrate the domain adaptability of our model.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the dramatic growth of opinionated usergenerated content, consumers often turn to online product reviews to seek advice while companies see reviews as a valuable source of consumer feedback.", "labels": [], "entities": []}, {"text": "How to automatically understand, extract and summarize the opinions expressed in online reviews has therefore become an important research topic and gained much attention in recent years (.", "labels": [], "entities": [{"text": "automatically understand, extract and summarize the opinions expressed in online reviews", "start_pos": 7, "end_pos": 95, "type": "TASK", "confidence": 0.6849326863884926}]}, {"text": "A wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification () to fine-grained extraction of opinion expressions and their targets ().", "labels": [], "entities": [{"text": "review mining", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.7838433384895325}, {"text": "document-level polarity classification", "start_pos": 92, "end_pos": 130, "type": "TASK", "confidence": 0.6481613715489706}, {"text": "fine-grained extraction of opinion expressions and their targets", "start_pos": 137, "end_pos": 201, "type": "TASK", "confidence": 0.8146654441952705}]}, {"text": "In particular, a general framework of summarizing reviews of a certain product is to first identify different aspects (a.k.a. features) of the given product and then extract specific opinion expressions for each aspect.", "labels": [], "entities": []}, {"text": "For example, aspects of a restaurant may include food, staff, ambience and price, and opinion expressions for staff may include friendly, rude, etc.", "labels": [], "entities": []}, {"text": "Because of the practicality of this structured summary format, it has been adopted in several previous studies ( as well as some commercial systems, e.g. the \"scorecard\" feature at Bing shopping . Different approaches have been proposed to identify aspect words and phrases from reviews.", "labels": [], "entities": []}, {"text": "Previous methods using frequent itemset mining () or supervised learning) have the limitation that they do not group semantically related aspect expressions together.", "labels": [], "entities": []}, {"text": "Supervised learning also suffers from its heavy dependence on training data.", "labels": [], "entities": []}, {"text": "In contrast, unsupervised, knowledge-lean topic modeling approach has been shown to be effective in automatically identifying aspects and their representative words.", "labels": [], "entities": [{"text": "knowledge-lean topic modeling", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6004820366700491}]}, {"text": "For example, words such as waiter, waitress, staff and service are grouped into one aspect.", "labels": [], "entities": []}, {"text": "We follow this promising direction and extend existing topic models to jointly identify both aspect and opinion words, especially aspect-specific opinion words.", "labels": [], "entities": []}, {"text": "Current topic models for opinion mining, which we will review in detail in Section 2, still lack this ability.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.9031304717063904}]}, {"text": "But separating aspect and opinion words can be very useful.", "labels": [], "entities": []}, {"text": "Aspect-specific opinion words can be used to construct a domain-dependent senti-ment lexicon and applied to tasks such as sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.9549926519393921}]}, {"text": "They can also provide more informative descriptions of the product or service being reviewed.", "labels": [], "entities": []}, {"text": "For example, using more specific opinion words such as cozy and romantic to describe the ambience aspect in a review summary is more meaningful than using generic words such as nice and great.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, are the first to study aspect-specific opinion words, but their opinion word detection is performed outside of topic modeling, and they only consider adjectives as possible opinion words.", "labels": [], "entities": [{"text": "opinion word detection", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.6693130433559418}]}, {"text": "In this paper, we propose anew topic modeling approach that can automatically separate aspect and opinion words.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.715519443154335}]}, {"text": "A novelty of this model is the integration of a discriminative maximum entropy (MaxEnt) component with the standard generative component.", "labels": [], "entities": [{"text": "discriminative maximum entropy (MaxEnt)", "start_pos": 48, "end_pos": 87, "type": "METRIC", "confidence": 0.6862952013810476}]}, {"text": "The MaxEnt component allows us to leverage arbitrary features such as POS tags to help separate aspect and opinion words.", "labels": [], "entities": []}, {"text": "Because the supervision relies mostly on non-lexical features, although our model is no longer fully unsupervised, the number of training sentences needed is relatively small.", "labels": [], "entities": []}, {"text": "Moreover, training data can also come from a different domain and yet still remain effective, making our model highly domain adaptive.", "labels": [], "entities": []}, {"text": "Empirical evaluation on large review data sets shows that our model can effectively identify both aspects and aspect-specific opinion words with a small amount of training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our MaxEnt-LDA hybrid model for jointly modeling aspect and opinion words, we used a restaurant review data set previously used in () and a hotel review data set previously used in ().", "labels": [], "entities": []}, {"text": "We removed stop words and used the Stanford POS Tagger 2 to tag the two data sets.", "labels": [], "entities": [{"text": "Stanford POS Tagger 2", "start_pos": 35, "end_pos": 56, "type": "DATASET", "confidence": 0.9216172397136688}]}, {"text": "Only reviews that have no more than 50 sentences were used.", "labels": [], "entities": []}, {"text": "We also kept another version of the data which includes the stop words for the purpose of extracting the contextual features included in x.", "labels": [], "entities": []}, {"text": "Some details of the data sets are given in.", "labels": [], "entities": []}, {"text": "For our hybrid model, we ran 500 iterations of Gibbs sampling.", "labels": [], "entities": []}, {"text": "Following (  50/T , \u03b2 = 0.1 and \u03b3 = 0.5.", "labels": [], "entities": [{"text": "T", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9447130560874939}]}, {"text": "We also experimented with other settings of these priors and did not notice any major difference.", "labels": [], "entities": []}, {"text": "For MaxEnt training, we tried three labeled data sets: one that was taken from the restaurant data set and manually annotated by us 3 , and two from the annotated data set used in (.", "labels": [], "entities": [{"text": "restaurant data set", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.7774945994218191}]}, {"text": "Note that the latter two were used for testing domain adaptation in Section 6.3.", "labels": [], "entities": [{"text": "testing domain adaptation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6068735321362814}]}, {"text": "Some details of the training sets are shown in.", "labels": [], "entities": []}, {"text": "In our preliminary experiments, we also tried two variations of our MaxEnt-LDA hybrid model.", "labels": [], "entities": []}, {"text": "The first is a fully unsupervised model where we used a uniform Dirichlet prior for \u03c0.", "labels": [], "entities": []}, {"text": "We found that this unsupervised model could not separate aspect and opinion words well.", "labels": [], "entities": []}, {"text": "(2) The second is a bootstrapping version of the MaxEnt-LDA model where we used the predicted values of y as pseudo labels and re-trained the MaxEnt model iteratively.", "labels": [], "entities": []}, {"text": "We found that this bootstrapping procedure did not boost the overall performance much and even hurt the performance a little in some cases.", "labels": [], "entities": []}, {"text": "Due to the space limit we do not report these experiments here.", "labels": [], "entities": []}, {"text": "In this section we report the evaluation of our model.", "labels": [], "entities": []}, {"text": "We refer to our MaxEnt-LDA hybrid model as ME-LDA.", "labels": [], "entities": []}, {"text": "We also implemented a local version of the standard LDA method where each sentence is treated as a document.", "labels": [], "entities": []}, {"text": "This is the model used in to identify aspects, and we refer to this model as LocLDA.: Sample aspects of the restaurant domain using LocLDA.", "labels": [], "entities": []}, {"text": "Note that the words in bold are opinion words which are mixed with aspect words.", "labels": [], "entities": []}, {"text": "For each of the two data sets, we show four sample aspects identified by ME-LDA in and Table 5.", "labels": [], "entities": []}, {"text": "Because the hotel domain is somehow similar to the restaurant domain, we used the labeled training data from the restaurant domain also for the hotel data set.", "labels": [], "entities": []}, {"text": "From the tables we can see that generally aspect words are quite coherent and meaningful, and opinion words correspond to aspects very well.", "labels": [], "entities": []}, {"text": "For comparison, we also applied LocLDA to the restaurant data set and present the aspects in.", "labels": [], "entities": [{"text": "restaurant data set", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.7869548896948496}]}, {"text": "We can see that ME-LDA and LocLDA give similar aspect words.", "labels": [], "entities": [{"text": "ME-LDA", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.7178731560707092}]}, {"text": "The major difference between these two models is that ME-LDA can sperate aspect words and opinion words, which can be very useful.", "labels": [], "entities": []}, {"text": "ME-LDA is also able to separate general opinion words from aspect-specific ones, giving more informative opinion expressions for each aspect.", "labels": [], "entities": [{"text": "ME-LDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8952747583389282}]}, {"text": "We also quantitatively evaluated the quality of the automatically identified aspects.", "labels": [], "entities": []}, {"text": "provide a set of annotated sentences from the restaurant data set, in which each sentence has been assigned one or more labels from a gold standard label set S = {Staff, Food, Ambience, Price, Anecdote, Misc}.", "labels": [], "entities": [{"text": "restaurant data set", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.8150325218836466}, {"text": "Anecdote, Misc", "start_pos": 193, "end_pos": 207, "type": "METRIC", "confidence": 0.6881950298945109}]}, {"text": "To evaluate the quality of our aspect identification, we chose from the gold standard labels three major aspects, namely Staff, Food and Ambience.", "labels": [], "entities": [{"text": "aspect identification", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8272017538547516}]}, {"text": "We did not choose the other aspects because (1) Price is often mixed with other aspects such as Food, and (2) Anecdote and Misc do not show clear  Since the major advantage of ME-LDA is its ability to separate aspect and opinion words, we further quantitatively evaluated the quality of the aspectspecific opinion words identified by ME-LDA. has constructed a gold standard set of aspect-specific opinion words for the restaurant data set.", "labels": [], "entities": [{"text": "Anecdote and Misc", "start_pos": 110, "end_pos": 127, "type": "METRIC", "confidence": 0.7927919228871664}, {"text": "restaurant data set", "start_pos": 419, "end_pos": 438, "type": "DATASET", "confidence": 0.801507572333018}]}, {"text": "In this gold standard set, they manually judged eight out of the 14 automatically inferred aspects they had: J = {Ambiance, Staff, Food-Main Dishes, Atmosphere-Physical, FoodBaked Goods, Food-General, Drinks, Service}.", "labels": [], "entities": [{"text": "J", "start_pos": 109, "end_pos": 110, "type": "METRIC", "confidence": 0.9893661737442017}]}, {"text": "Each word is assigned a polarity score ranging from -2.0 to 2.0 in each aspect.", "labels": [], "entities": []}, {"text": "We used their gold standard words whose polarity scores are not equal to zero.", "labels": [], "entities": []}, {"text": "Because their gold standard only includes adjectives, we also manually added more opinion words into the gold standard set.", "labels": [], "entities": []}, {"text": "To do so, we took the top 20 opinion words returned by our method and two baseline methods, pooled them together, and manually judged them.", "labels": [], "entities": []}, {"text": "We use precision at n (P@n), a commonly used metric in information retrieval, for evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9971529245376587}, {"text": "information retrieval", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.8259667158126831}]}, {"text": "Because top words are more important in opinion models, we set n to 5, 10 and 20.", "labels": [], "entities": []}, {"text": "For both ME-LDA and BL-1 below, we again manually mapped each automatically inferred aspect to one of the gold standard aspects.", "labels": [], "entities": [{"text": "BL-1", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9347324967384338}]}, {"text": "Since LocLDA does not identify aspect-specific opinion words, we consider the following two baseline methods that can identify aspect-specific opinion words: BL-1: In this baseline, we start with all adjectives as candidate opinion words, and use mutual information (MI) to rank these candidates.", "labels": [], "entities": [{"text": "LocLDA", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.8701221346855164}, {"text": "BL-1", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9937832355499268}, {"text": "mutual information (MI)", "start_pos": 247, "end_pos": 270, "type": "METRIC", "confidence": 0.6441869735717773}]}, {"text": "Specifically, given an aspect t, we rank the candidate words according to the following scoring function: where Vt is the set of the top-100 frequent aspect words from \u03c6 A,t . BL-2: In this baseline, we first use LocLDA to learn a topic distribution for each sentence.", "labels": [], "entities": [{"text": "BL-2", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9745708703994751}]}, {"text": "We then assign a sentence to the aspect with the largest probability and hence get sentence clusters.", "labels": [], "entities": []}, {"text": "We manually map these clusters to the eight gold standard aspects.", "labels": [], "entities": []}, {"text": "Finally, for each aspect we rank adjectives by their 0.725 0.650 0.563: Average P@n of aspect-specific opinion words on restaurant.", "labels": [], "entities": []}, {"text": "* and indicate that the improvement hypothesis is accepted at confidence level 0.9 respectively for BL-1 and BL-2.", "labels": [], "entities": [{"text": "BL-1", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.8356261849403381}, {"text": "BL-2", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.5458542108535767}]}, {"text": "frequencies in the aspect and treat these as aspectspecific opinion words.", "labels": [], "entities": []}, {"text": "The basic results in terms of the average precision at n over the eight aspects are shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9920638203620911}]}, {"text": "We can see that ME-LDA outperformed the two baselines consistently.", "labels": [], "entities": []}, {"text": "Especially, for P@5, ME-LDA gave more than 100% relative improvement over BL-1.", "labels": [], "entities": [{"text": "ME-LDA", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8828838467597961}, {"text": "BL-1", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.8885510563850403}]}, {"text": "The absolute value of 0.825 for P@5 also indicates that top opinion words discovered by our model are indeed meaningful.", "labels": [], "entities": []}, {"text": "The evaluation in the previous section shows that our model returns good opinion words for each aspect.", "labels": [], "entities": []}, {"text": "It does not, however, directly judge how aspectspecific those opinion words are.", "labels": [], "entities": []}, {"text": "This is because the gold standard created by also includes general opinion words.", "labels": [], "entities": []}, {"text": "E.g. friendly and good may both be judged to be opinion words for the staff aspect, but the former is more specific than the latter.", "labels": [], "entities": []}, {"text": "We suspect that BL-2 has comparable performance with ME-LDA for this reason.", "labels": [], "entities": [{"text": "BL-2", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8722835779190063}]}, {"text": "So we further evaluated the association between opinion words and aspects by directly looking at how easy it is to infer the corresponding aspect by only looking at an aspect-specific opinion word.", "labels": [], "entities": []}, {"text": "We selected four aspects for evaluation: Ambiance, Staff, FoodMain Dishes and Atmosphere-Physical . We chose these four aspects because they are quite different from each other and thus manual judgments on these four aspects can be more objective.", "labels": [], "entities": [{"text": "Ambiance", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9429516196250916}]}, {"text": "For each aspect, similar to the pooling strategy in IR, we pooled the top 20 opinion words identified by BL-1, BL-2 and ME-LDA.", "labels": [], "entities": [{"text": "IR", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9341118931770325}, {"text": "BL-1", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9035909175872803}, {"text": "BL-2", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.7552708983421326}]}, {"text": "We then asked two human assessors to assign an association score to each of these words as follows: If the word is closely associated with an aspect, a score of 2 is given; if it is marginally as-: Average nDCG performance of BL-2 and ME-LDA.", "labels": [], "entities": [{"text": "BL-2", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.9759674668312073}, {"text": "ME-LDA", "start_pos": 235, "end_pos": 241, "type": "METRIC", "confidence": 0.9320310950279236}]}, {"text": "Because only four aspects were used for evaluation, we did not perform statistical significance test.", "labels": [], "entities": [{"text": "statistical significance test", "start_pos": 71, "end_pos": 100, "type": "METRIC", "confidence": 0.6805308361848196}]}, {"text": "We found that in all cases ME-LDA outperformed BL-2 for either all aspects or three out of four aspects.", "labels": [], "entities": [{"text": "BL-2", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.8090969324111938}]}, {"text": "sociated with an aspect, a score of 1 is given; otherwise, 0 is given.", "labels": [], "entities": []}, {"text": "We calculated the Kappa statistics of agreement, and we got a quite high Kappa value of 0.8375 and 0.7875 respectively for the restaurant data set and the hotel data set.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9443829655647278}, {"text": "agreement", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9281911253929138}, {"text": "Kappa", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9760681390762329}, {"text": "restaurant data set", "start_pos": 127, "end_pos": 146, "type": "DATASET", "confidence": 0.7685607572396597}, {"text": "hotel data set", "start_pos": 155, "end_pos": 169, "type": "DATASET", "confidence": 0.8291345238685608}]}, {"text": "Then for each word in an aspect, we took the average of the scores of the two assessors.", "labels": [], "entities": []}, {"text": "We used an nDCG-like metric to compare the performance of our model and of BL-2.", "labels": [], "entities": [{"text": "BL-2", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8716886043548584}]}, {"text": "The metric is defined as follows: where M t,i is the ith aspect-specific opinion word inferred by method M for aspect t, Score(M t,i ) is the association score of this word, and iDCG@k(t) is the score of the ideal DCG measure at k for aspect t, that is, the maximum DCG score assuming an ideal ranking.", "labels": [], "entities": [{"text": "Score", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9498464465141296}]}, {"text": "We chose k = 5 and k = 10.", "labels": [], "entities": []}, {"text": "The average nDCG over the four aspects are presented in.", "labels": [], "entities": []}, {"text": "We can see that ME-LDA outperformed BL-2 quite a lot for the restaurant data set, which conforms to our hypothesis that ME-LDA generates aspect-specific opinion words of stronger association with aspects.", "labels": [], "entities": [{"text": "BL-2", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.7950453758239746}, {"text": "restaurant data set", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.7845219671726227}]}, {"text": "For the hotel data set, ME-LDA outperformed a little.", "labels": [], "entities": [{"text": "hotel data set", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8192578752835592}, {"text": "ME-LDA", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9295842051506042}]}, {"text": "This maybe due to the fact that we used the restaurant training data for the hotel data set.", "labels": [], "entities": [{"text": "hotel data set", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.8271197477976481}]}], "tableCaptions": [{"text": " Table 1: Some statistics of the data sets.", "labels": [], "entities": []}, {"text": " Table 2: Some statistics of the labeled training data.", "labels": [], "entities": []}, {"text": " Table 6: Results of aspects identification on restaurant.", "labels": [], "entities": [{"text": "aspects identification", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.7234286963939667}]}, {"text": " Table 8: Average nDCG performance of BL-2 and ME- LDA. Because only four aspects were used for evaluation,  we did not perform statistical significance test. We found  that in all cases ME-LDA outperformed BL-2 for either  all aspects or three out of four aspects.", "labels": [], "entities": []}, {"text": " Table 10: Comparison of the average P@n using different  feature sets for opinion identification on restaurant.", "labels": [], "entities": []}, {"text": " Table 11: Average F-1 with differen sizes of training data  on restaurant.", "labels": [], "entities": [{"text": "F-1", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.8595119714736938}]}, {"text": " Table 12: Average P@n of aspect-specific opinion words  with differen sizes of training data on restaurant.", "labels": [], "entities": [{"text": "Average P@n", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.8327465802431107}]}, {"text": " Table 13: Average F-1 performance for domain adaption  on restaurant.", "labels": [], "entities": [{"text": "F-1", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9621711373329163}, {"text": "domain adaption", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.6893858015537262}]}, {"text": " Table 14: Average P@n of aspect-specific opinion words  for domain adaption on restaurant.", "labels": [], "entities": [{"text": "Average P@n", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.8157365769147873}, {"text": "domain adaption", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7064100801944733}]}]}