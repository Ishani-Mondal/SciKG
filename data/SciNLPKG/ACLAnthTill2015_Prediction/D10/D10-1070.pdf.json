{"title": [{"text": "A Unified Framework for Scope Learning via Simplified Shallow Seman- tic Parsing", "labels": [], "entities": [{"text": "Scope Learning", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8790118098258972}, {"text": "Simplified Shallow Seman- tic Parsing", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.5581849118073782}]}], "abstractContent": [{"text": "This paper approaches the scope learning problem via simplified shallow semantic parsing.", "labels": [], "entities": [{"text": "scope learning", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9258194863796234}, {"text": "shallow semantic parsing", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.7013887564341227}]}, {"text": "This is done by regarding the cue as the predicate and mapping its scope into several constituents as the arguments of the cue.", "labels": [], "entities": []}, {"text": "Evaluation on the BioScope corpus shows that the structural information plays a critical role in capturing the relationship between a cue and its dominated arguments.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.8761578500270844}]}, {"text": "It also shows that our parsing approach significantly outper-forms the state-of-the-art chunking ones.", "labels": [], "entities": []}, {"text": "Although our parsing approach is only evaluated on negation and speculation scope learning here, it is portable to other kinds of scope learning.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9613397121429443}]}], "introductionContent": [{"text": "Recent years have witnessed an increasing interest in the analysis of linguistic scope in natural language.", "labels": [], "entities": [{"text": "analysis of linguistic scope in natural language", "start_pos": 58, "end_pos": 106, "type": "TASK", "confidence": 0.8477964741843087}]}, {"text": "The task of scope learning deals with the syntactic analysis of what part of a given sentence is under user's special interest.", "labels": [], "entities": [{"text": "scope learning", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8906943798065186}, {"text": "syntactic analysis of what part of a given sentence", "start_pos": 42, "end_pos": 93, "type": "TASK", "confidence": 0.7719057036770715}]}, {"text": "For example, of negation assertion concerned, a negation cue (e.g., not, no) usually dominates a fragment of the given sentence, rather than the whole sentence, especially when the sentence is long.", "labels": [], "entities": [{"text": "negation assertion", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9748536050319672}]}, {"text": "Generally, scope learning involves two subtasks: cue recognition and its scope identification.", "labels": [], "entities": [{"text": "scope learning", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9661940038204193}, {"text": "cue recognition", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7111800760030746}, {"text": "scope identification", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.798419326543808}]}, {"text": "The former decides whether a word or phrase in a sentence is a cue of a special interest, where the semantic information of the word or phrase, rather than the syntactic information, plays a critical role.", "labels": [], "entities": []}, {"text": "The latter determines the sequences of words in the sentence which are dominated by the given cue.", "labels": [], "entities": []}, {"text": "Recognizing the scope of a special interest (e.g., negative assertion and speculative assertion) is essential in information extraction (IE), whose aim is to derive factual knowledge from free text.", "labels": [], "entities": [{"text": "speculative assertion)", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7294672131538391}, {"text": "information extraction (IE)", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.8788647532463074}]}, {"text": "For example,  pointed out that the extracted information within the scope of a negation or speculation cue should either be discarded or presented separately from factual information.", "labels": [], "entities": []}, {"text": "This is especially important in the biomedical and scientific domains, where various linguistic forms are used extensively to express impressions, hypothesized explanations of experimental results or negative findings.", "labels": [], "entities": []}, {"text": "Besides, reported that 13.45% and 17.70% of the sentences in the abstracts subcorpus of the BioScope corpus contain negative and speculative assertions, respectively, while 12.70% and 19.44% of the sentences in the full papers subcorpus contain negative and speculative assertions, respectively.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9154160618782043}]}, {"text": "In addition to the IE tasks in the biomedical domain, negation scope learning has attracted increasing attention in some natural language processing (NLP) tasks, such as sentiment classification.", "labels": [], "entities": [{"text": "IE", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9923628568649292}, {"text": "negation scope learning", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.954837699731191}, {"text": "sentiment classification", "start_pos": 170, "end_pos": 194, "type": "TASK", "confidence": 0.961749404668808}]}, {"text": "For example, in the sentence \"The chair is not comfortable but cheap\", although both the polarities of the words \"comfortable\" and \"cheap\" are positive, the polarity of \"the chair\" regarding the attribute \"cheap\" keeps positive while the polarity of \"the chair\" regarding the attribute \"comfortable\" is reversed due to the negation cue \"not\".", "labels": [], "entities": []}, {"text": "Similarly, seeing the increasing interest in speculation scope learning, the CoNLL'2010 shared task) aims to detect uncertain information in resolving the scopes of speculation cues.", "labels": [], "entities": [{"text": "speculation scope learning", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7144832114378611}]}, {"text": "Most of the initial research in this literature focused on either recognizing negated terms or identifying speculative sentences, using some heuristic rules (, and machine learning methods.", "labels": [], "entities": []}, {"text": "However, scope learning has been largely ignored until the recent release of the BioScope corpus (, where negation/speculation cues and their scopes are annotated explicitly. and pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue.", "labels": [], "entities": [{"text": "scope learning", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.9050791561603546}, {"text": "BioScope corpus", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9358692765235901}, {"text": "scope learning", "start_pos": 205, "end_pos": 219, "type": "TASK", "confidence": 0.8647938668727875}]}, {"text": "Alternatively, and defined heuristic rules for speculation scope learning from constituency and dependency parse tree perspectives, respectively.", "labels": [], "entities": []}, {"text": "Although the chunking approach has been evaluated on negation and speculation scope learning and can be easily ported to other scope learning tasks, it ignores syntactic information and suffers from low performance.", "labels": [], "entities": [{"text": "negation and speculation scope learning", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.673162454366684}]}, {"text": "Alternatively, even if the rule-based methods maybe effective fora special scope learning task (e.g., speculation scope learning), it is not readily adoptable to other scope learning tasks (e.g., negation scope learning).", "labels": [], "entities": [{"text": "negation scope learning", "start_pos": 196, "end_pos": 219, "type": "TASK", "confidence": 0.9325100183486938}]}, {"text": "Instead, this paper explores scope learning from parse tree perspective and formulates it as a simplified shallow semantic parsing problem, which has been extensively studied in the past few years (Carreras and).", "labels": [], "entities": [{"text": "scope learning from parse tree", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.794290292263031}, {"text": "semantic parsing", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7324577271938324}]}, {"text": "In particular, the cue is recast as the predicate and the scope is recast as the arguments of the cue.", "labels": [], "entities": []}, {"text": "The motivation behind is that the structured syntactic information plays a critical role in scope learning and should be paid much more attention, as indicated by previous studies in shallow semantic parsing ().", "labels": [], "entities": [{"text": "scope learning", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.8840114176273346}, {"text": "shallow semantic parsing", "start_pos": 183, "end_pos": 207, "type": "TASK", "confidence": 0.6065927743911743}]}, {"text": "Although our approach is evaluated only on negation and speculation scope learning here, it is portable to other kinds of scope learning.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the Bioscope corpus on which our approach is evaluated.", "labels": [], "entities": [{"text": "Bioscope corpus", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.8078085482120514}]}, {"text": "Section 4 describes our parsing approach by formulating scope learning as a simplified shallow semantic parsing problem.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.974450409412384}, {"text": "formulating scope learning", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.8079252044359843}, {"text": "semantic parsing", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7546015083789825}]}, {"text": "Section 5 presents the experimental results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated our simplified shallow semantic parsing approach to negation and speculation scope learning on the BioScope corpus.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7582264244556427}, {"text": "negation and speculation scope learning", "start_pos": 70, "end_pos": 109, "type": "TASK", "confidence": 0.8738850831985474}, {"text": "BioScope corpus", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.9354015290737152}]}, {"text": "Following the experimental setting in and, the abstracts subcorpus is randomly divided into 10 folds so as to perform 10-fold crossvalidation, while the performance on both the pa-pers and clinical reports subcorpora is evaluated using the system trained on the whole abstracts subcorpus.", "labels": [], "entities": []}, {"text": "In addition, SVMLight is selected as our classifier.", "labels": [], "entities": [{"text": "SVMLight", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8833427429199219}]}, {"text": "In order to select beneficial features from the additional features proposed in Section 4.3, we randomly split the abstracts subcorpus into the training data and the development data with proportion of 4:1.", "labels": [], "entities": []}, {"text": "After performing the greedy feature selection algorithm on the development data, 7 features {CFACCP5, CP2R, CFCP1, AC1P, CP3, CFACCP7, AC4R} are selected consecutively for negation scope identification while 11 features {CFACCP5, AC2W, CFACCP2, CFACCP4, AC5, CFCP1, CFACCP7, CFACCP1, CP4, AC3P, CFAC2} are selected for speculation scope identification.", "labels": [], "entities": [{"text": "negation scope identification", "start_pos": 172, "end_pos": 201, "type": "TASK", "confidence": 0.8986880381902059}, {"text": "speculation scope identification", "start_pos": 319, "end_pos": 351, "type": "TASK", "confidence": 0.648943156003952}]}, {"text": "gives the contribution of additional features on the development data.", "labels": [], "entities": []}, {"text": "It shows that the additional features significantly improve the performance by 11.66% inaccuracy from 74.93% to 86.59% ( ) for negation scope identification and improve the performance by 11.07% inaccuracy from 77.29% to 88.36% ( ) for speculation scope identification.", "labels": [], "entities": [{"text": "negation scope identification", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.9640149474143982}, {"text": "speculation scope identification", "start_pos": 236, "end_pos": 268, "type": "TASK", "confidence": 0.6923888723055521}]}, {"text": "The feature selection experiments suggest that the features (e.g., CFACCP5, AC2W, CFCP1) related to neighboring words of the cue play a critical role for both negation and speculation scope identification.", "labels": [], "entities": [{"text": "CFACCP5", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.76153564453125}, {"text": "negation and speculation scope identification", "start_pos": 159, "end_pos": 204, "type": "TASK", "confidence": 0.6081704318523407}]}, {"text": "This maybe due to the fact that neighboring words usually imply important sentential information.", "labels": [], "entities": []}, {"text": "For example, \"can not be\" indicates a passive clause while \"did not\" indicates an active clause.", "labels": [], "entities": []}, {"text": "Since the additional selected features significantly improve the performance for both negation and speculation scope identification, we will include those additional selected features in all the remaining experiments.", "labels": [], "entities": [{"text": "negation and speculation scope identification", "start_pos": 86, "end_pos": 131, "type": "TASK", "confidence": 0.763748151063919}]}, {"text": "Since all the sentences in the abstracts subcorpus are included in the GTB1.0 corpus while we do not have golden parse trees for the sentences in the full papers and the clinical reports subcorpora, we only evaluate the performance of scope identification on the abstracts subcorpus with golden parse trees.", "labels": [], "entities": [{"text": "GTB1.0 corpus", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9668928980827332}, {"text": "scope identification", "start_pos": 235, "end_pos": 255, "type": "TASK", "confidence": 0.7685805857181549}]}, {"text": "presents the performance on the abstracts subcorpus by performing 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "It shows that given golden parse trees and golden cues, speculation scope identification achieves higher performance (e.g., ~3.3% higher in accuracy) than negation scope identification.", "labels": [], "entities": [{"text": "speculation scope identification", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.8525005380312601}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9989722967147827}, {"text": "negation scope identification", "start_pos": 155, "end_pos": 184, "type": "TASK", "confidence": 0.9263557195663452}]}, {"text": "This is mainly due to the observation on the BioScope corpus that the scope of a speculation cue can be usually characterized by its POS and the syntactic structures of the sentence where it occurs.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.935901015996933}, {"text": "POS", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9925442934036255}]}, {"text": "For example, the scope of a verb in active voice usually starts at the cue itself and ends at its object (e.g., the speculation cue \"indicate that\" in scopes the fragment of \"indicate that corticosteroid resistance cannot be explained by abnormalities\").", "labels": [], "entities": []}, {"text": "Moreover, the statistics on the abstracts subcorpus shows that the number of arguments per speculation cue is smaller than that of arguments per negation cue (e.g., 1.5 vs. 1.8).", "labels": [], "entities": []}, {"text": "The GTB1.0 corpus contains 18,541 sentences in which 11,850 of them (63.91%) overlap with the sentences in the abstracts subcorpus . In order to get automatic parse trees, we train the Berkeley parser with the remaining 6,691 sentences in GTB1.0, which achieves the performance of 85.22 in F1-measure on the remaining 11,850 sentences in GTB1.0.", "labels": [], "entities": [{"text": "GTB1.0 corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9171044230461121}, {"text": "F1-measure", "start_pos": 290, "end_pos": 300, "type": "METRIC", "confidence": 0.9987542629241943}]}, {"text": "shows the performance of scope identification on automatic parse trees and golden cues.", "labels": [], "entities": [{"text": "scope identification", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.9283849000930786}]}, {"text": "In addition, we also report an oracle performance to explore the best possible performance of our system by assuming that our scope finder can always correctly determine whether a candidate is an argument or not.", "labels": [], "entities": []}, {"text": "That is, if an argument candidate falls within the golden scope, then it is a argument.", "labels": [], "entities": []}, {"text": "This is to measure the impact of automatic syntactic parsing itself.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.6324724107980728}]}, {"text": "shows that: 1) For both negation and speculaiton scope identification, automatic syntactic parsing lowers the performance on the abstracts subcorpus (e.g., from 83.10% to 81.84% inaccuracy for negation scope identification and from 86.41% to 83.74% inaccuracy for speculaiton scope identification).", "labels": [], "entities": [{"text": "negation and speculaiton scope identification", "start_pos": 24, "end_pos": 69, "type": "TASK", "confidence": 0.6620838403701782}, {"text": "syntactic parsing", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7068138718605042}, {"text": "negation scope identification", "start_pos": 193, "end_pos": 222, "type": "TASK", "confidence": 0.831943134466807}, {"text": "speculaiton scope identification", "start_pos": 264, "end_pos": 296, "type": "TASK", "confidence": 0.6368192334969839}]}, {"text": "However, the performance drop shows that both negation and speculation scope identification are not as senstive to automatic syntactic parsing as common shallow semantic parsing, whose performance might decrease by about ~10 in F1-measure ().", "labels": [], "entities": [{"text": "negation and speculation scope identification", "start_pos": 46, "end_pos": 91, "type": "TASK", "confidence": 0.6519036829471588}, {"text": "automatic syntactic parsing", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6945008635520935}, {"text": "F1-measure", "start_pos": 228, "end_pos": 238, "type": "METRIC", "confidence": 0.9952219128608704}]}, {"text": "This indicates that scope identification via simplified shallow semantic parsing is robust to some variations in the parse trees.", "labels": [], "entities": [{"text": "scope identification", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8798852860927582}]}, {"text": "2) Although speculation scope identification consistently achieves higher performance than negaiton scope identification when golden parse trees are availabe, speculation scope identification achieves comparable performance with negation scope identification on the abstracts subcorpus and the full papers There area few cases where two sentences in the abstracts subcorpus map into one sentence in GTB1.0.", "labels": [], "entities": [{"text": "speculation scope identification", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.6493262946605682}, {"text": "negaiton scope identification", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.6477105716864268}, {"text": "speculation scope identification", "start_pos": 159, "end_pos": 191, "type": "TASK", "confidence": 0.661491870880127}, {"text": "negation scope identification", "start_pos": 229, "end_pos": 258, "type": "TASK", "confidence": 0.8052308758099874}]}, {"text": "subcorpus while speculation scope identification even performs ~20% lower inaccuracy than negation scope identification on the clinical report subcorpus.", "labels": [], "entities": [{"text": "speculation scope identification", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.8090618848800659}, {"text": "negation scope identification", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.8424501419067383}]}, {"text": "This is largely due to that specuaiton scope identification is more sensitive to syntactic parsing errors than negation scope identification due to the wider scope of a speculation cue while the sentences of the clinical reports come from a different genre, which indicates low performance in syntactic parsing.", "labels": [], "entities": [{"text": "specuaiton scope identification", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.7383559346199036}, {"text": "syntactic parsing", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.6825156807899475}, {"text": "negation scope identification", "start_pos": 111, "end_pos": 140, "type": "TASK", "confidence": 0.8380409081776937}, {"text": "syntactic parsing", "start_pos": 293, "end_pos": 310, "type": "TASK", "confidence": 0.7525919377803802}]}, {"text": "3) Given the performance gap between the performance of our scope finder and the oracle performance, there is still much room for further performance improvement.: Performance comparison of our system with the state-of-the-art ones inaccuracy (%).", "labels": [], "entities": []}, {"text": "Note that all the performances achieved on the full papers subcorpus and the clinical subcorpus are achieved using the whole GTB1.0 corpus of 18,541 sentences while all the performances achieved on the abstract subcorpus are achieved using 6,691 sentences from GTB1.0 due to overlap of the abstract subcorpus with GTB1.0.", "labels": [], "entities": [{"text": "GTB1.0 corpus", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.9126295745372772}]}, {"text": "compares our performance with related ones.", "labels": [], "entities": []}, {"text": "It shows that even our baseline system with the four basic features presented in achieves comparable performance with and.", "labels": [], "entities": []}, {"text": "This further indicates the appropriateness of our simplified shallow semantic parsing approach and the effectiveness of structured syntactic information on scope identification.", "labels": [], "entities": [{"text": "shallow semantic parsing", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.72584468126297}, {"text": "scope identification", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.8435526192188263}]}, {"text": "It also shows that our final system significantly outperforms the state-of-the-art ones using a chunking approach, especially on the abstracts and full papers subcorpora.", "labels": [], "entities": []}, {"text": "However, the improvement on the clinical reports subcorpora for negation scope identification is much less apparent, partly due to the fact that the sentences in this subcorpus are much simpler (with average length of 6.6 words per sentence) and thus a chunking approach can achieve high performance.", "labels": [], "entities": [{"text": "negation scope identification", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.9792179663976034}]}, {"text": "also shows that our parsing approach to speculation scope identification outperforms the rule-based method in, where 10-fold cross-validation is performed on both the abstracts and the full papers subcorpora.", "labels": [], "entities": [{"text": "speculation scope identification", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.8150361378987631}]}, {"text": "So far negation/speculation cues are assumed to be manually annotated and available.", "labels": [], "entities": [{"text": "negation/speculation cues", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.9030638039112091}]}, {"text": "Here we turn to a more realistic scenario in which cues are automatically recognized.", "labels": [], "entities": []}, {"text": "In the following, we first report the results of cue recognition and then the results of scope identification with automatic cues.: Performance of automatic cue recognition with gold parse trees on the abstracts subcorpus using 10-fold cross-validation lists the performance of cue recognition on the abstracts subcorpus, assuming all words in the sentences as candidates.", "labels": [], "entities": [{"text": "cue recognition", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7638350129127502}, {"text": "scope identification", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.7376373410224915}, {"text": "cue recognition", "start_pos": 157, "end_pos": 172, "type": "TASK", "confidence": 0.7461416721343994}, {"text": "cue recognition", "start_pos": 278, "end_pos": 293, "type": "TASK", "confidence": 0.7384196221828461}]}, {"text": "It shows that as a complement to features derived from word/pos information (CC+SW features), structural features (SF features) derived from the syntactic parse tree significantly improve the performance of cue recognition by about 1.52 and 0.78 in F1-measure for negation and speculation cue recognition, respectively, and thus included thereafter.", "labels": [], "entities": [{"text": "cue recognition", "start_pos": 207, "end_pos": 222, "type": "TASK", "confidence": 0.7472489774227142}, {"text": "F1-measure", "start_pos": 249, "end_pos": 259, "type": "METRIC", "confidence": 0.9991635084152222}, {"text": "negation and speculation cue recognition", "start_pos": 264, "end_pos": 304, "type": "TASK", "confidence": 0.6053237020969391}]}, {"text": "In addition, we have also experimented on only these words, which happen to be a cue or inside a cue in the training data as cue candidates.", "labels": [], "entities": []}, {"text": "However, this experimental setting achieves a lower performance than that when all words are considered.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Accuracy (%) of scope identification on the  three subcorpora using automatic parser trained on  6,691 sentences in GTB1.0", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9952009916305542}, {"text": "scope identification", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8740942180156708}, {"text": "GTB1.0", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.882296621799469}]}, {"text": " Table 7: Performance comparison of our system with  the state-of-the-art ones in accuracy (%). Note that all  the performances achieved on the full papers subcorpus  and the clinical subcorpus are achieved using the whole  GTB1.0 corpus of 18,541 sentences while all the per- formances achieved on the abstract subcorpus are  achieved using 6,691 sentences from GTB1.0 due to  overlap of the abstract subcorpus with GTB1.0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.999729573726654}, {"text": "GTB1.0 corpus", "start_pos": 224, "end_pos": 237, "type": "DATASET", "confidence": 0.9069363176822662}]}, {"text": " Table 8: Performance of automatic cue recognition with  gold parse trees on the abstracts subcorpus using 10-fold  cross-validation", "labels": [], "entities": [{"text": "cue recognition", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7203924208879471}]}, {"text": " Table 9: Performance of automatic cue recognition with  automatic parse trees on the three subcorpora", "labels": [], "entities": [{"text": "cue recognition", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7640573084354401}]}, {"text": " Table 10: Performance of both negation and speculation  scope identification with automatic cues and automatic  parse trees", "labels": [], "entities": [{"text": "negation and speculation  scope identification", "start_pos": 31, "end_pos": 77, "type": "TASK", "confidence": 0.7449927210807801}]}]}