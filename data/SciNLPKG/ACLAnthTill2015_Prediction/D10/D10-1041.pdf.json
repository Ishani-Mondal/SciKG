{"title": [{"text": "Facilitating Translation Using Source Language Paraphrase Lattices", "labels": [], "entities": [{"text": "Facilitating Translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9037926197052002}]}], "abstractContent": [{"text": "For resource-limited language pairs, coverage of the test set by the parallel corpus is an important factor that affects translation quality in two respects: 1) out of vocabulary words; 2) the same information in an input sentence can be expressed in different ways, while current phrase-based SMT systems cannot automatically select an alternative way to transfer the same information.", "labels": [], "entities": [{"text": "SMT", "start_pos": 294, "end_pos": 297, "type": "TASK", "confidence": 0.8376780152320862}]}, {"text": "Therefore, given limited data, in order to facilitate translation from the input side, this paper proposes a novel method to reduce the translation difficulty using source-side lattice-based paraphrases.", "labels": [], "entities": []}, {"text": "We utilise the original phrases from the input sentence and the corresponding paraphrases to build a lattice with estimated weights for each edge to improve translation quality.", "labels": [], "entities": []}, {"text": "Compared to the baseline system , our method achieves relative improvements of 7.07%, 6.78% and 3.63% in terms of BLEU score on small, medium and large-scale English-to-Chinese translation tasks respectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.97597935795784}, {"text": "English-to-Chinese translation tasks", "start_pos": 158, "end_pos": 194, "type": "TASK", "confidence": 0.6628548304239908}]}, {"text": "The results show that the proposed method is effective not only for resource-limited language pairs, but also for resource-sufficient pairs to some extent.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, statistical MT systems have been easy to develop due to the rapid explosion in data availability, especially parallel data.", "labels": [], "entities": [{"text": "statistical MT", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.6610307395458221}]}, {"text": "However, in reality there are still many language pairs which lack parallel data, such as Urdu-English, ChineseItalian, where large amounts of speakers exist for both languages; of course, the problem is far worse for pairs such as Catalan-Irish.", "labels": [], "entities": []}, {"text": "For such resourcelimited language pairs, sparse amounts of parallel data would cause the word alignment to be inaccurate, which would in turn lead to an inaccurate phrase alignment, and bad translations would result.", "labels": [], "entities": []}, {"text": "argue that limited amounts of parallel training data can lead to the problem of low coverage in that many phrases encountered at run-time are not observed in the training data and so their translations will not be learned.", "labels": [], "entities": [{"text": "coverage", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9794970750808716}]}, {"text": "Thus, in recent years, research on addressing the problem of unknown words or phrases has become more and more evident for resource-limited language pairs.", "labels": [], "entities": []}, {"text": "Callison- proposed a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence, and then proceeds to use the translation of that paraphrase in the production of the target-language result.", "labels": [], "entities": []}, {"text": "Their experiments showed that by translating paraphrases a marked improvement was achieved in coverage and translation quality, especially in the case of unknown words which previously had been left untranslated.", "labels": [], "entities": []}, {"text": "However, on a large-scale data set, they did not achieve improvements in terms of automatic evaluation.", "labels": [], "entities": []}, {"text": "proposed another way to use paraphrases in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9692464470863342}]}, {"text": "He generates nearly-equivalent syntactic paraphrases of the source-side training sentences, then pairs each paraphrased sentence with the target translation associated with the original sentence in the training data.", "labels": [], "entities": []}, {"text": "Essentially, this method generates new training data using paraphrases to train anew model and obtain more useful phrase pairs.", "labels": [], "entities": []}, {"text": "However, he reported that this method results in bad system performance.", "labels": [], "entities": []}, {"text": "By contrast, real improvements can be achieved by merging the phrase tables of the paraphrase model and the original model, giving priority to the latter.", "labels": [], "entities": []}, {"text": "presented the use of word lattices for multi-source translation, in which the multiple source input texts are compiled into a compact lattice, over which a single decoding pass is then performed.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6865666508674622}]}, {"text": "This lattice-based method achieved positive results across all data conditions.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method using paraphrases to facilitate translation, especially for resource-limited languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.9740995168685913}]}, {"text": "Our method does not distinguish unknown words in the input sentence, but uses paraphrases of all possible words and phrases in the source input sentence to build a source-side lattice to provide a diverse and flexible list of source-side candidates to the SMT decoder so that it can search fora best path and deliver the translation with the highest probability.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 256, "end_pos": 267, "type": "TASK", "confidence": 0.8789158463478088}]}, {"text": "In this case, we neither need to change the phrase table, nor add new features in the log-linear model, nor add new sentences in the training data.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we define the \"translation difficulty\" from the perspective of the source side, and then examine how well the test set is covered by the phrase table and the parallel training data . Section 3 describes our paraphrase lattice method and discusses how to set the weights for the edges in the lattice network.", "labels": [], "entities": []}, {"text": "In Section 4, we report comparative experiments conducted on small, medium and largescale English-to-Chinese data sets.", "labels": [], "entities": []}, {"text": "In Section 5, we analyse the influence of our paraphrase lattice method.", "labels": [], "entities": []}, {"text": "Section 6 concludes and gives avenues for future work.", "labels": [], "entities": []}, {"text": "2 What Makes Translation Difficult?", "labels": [], "entities": [{"text": "Translation Difficult?", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8263689676920573}]}], "datasetContent": [{"text": "The experimental results conducted on small and medium-sized data sets are shown in.", "labels": [], "entities": []}, {"text": "The 95% confidence intervals (CI) for BLEU scores are independently computed on each of three systems, while the \"pair-CI 95%\" are computed relative to the baseline system only for \"Para-Sub\" and \"Lattice\" systems.", "labels": [], "entities": [{"text": "95% confidence intervals (CI)", "start_pos": 4, "end_pos": 33, "type": "METRIC", "confidence": 0.8177067083971841}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9580798745155334}]}, {"text": "All the significance tests use bootstrap and paired-bootstrap resampling normal approximation methods ().", "labels": [], "entities": []}, {"text": "8 Improvements are considered to be significant if the left boundary of the confidence interval is larger than zero in terms of the \"pair-CI 95%\".", "labels": [], "entities": []}, {"text": "It can be seen that 1) our \"Lattice\" system outperforms the baseline by 1.02 and 1.6 absolute (7.07% and 6.78% relative) BLEU points in terms of the 20K and 200K data sets respectively, and our system also decreases the TER scores by 2.24 and 1.19 (2.97% and 1.87% relative) points than the baseline system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9955006241798401}, {"text": "TER scores", "start_pos": 220, "end_pos": 230, "type": "METRIC", "confidence": 0.977806806564331}]}, {"text": "In terms of the \"pair-CI 95%\", the left boundaries for 20K and 200K data are respectively \"+0.74\" and \"+1.19\", which indicate that the \"Lattice\" system is significantly better than the baseline system on these two data sets.", "labels": [], "entities": []}, {"text": "2) The \"Para-Sub\" system performs slightly better (0.36 absolute BLEU points) than the baseline system on the 20K data set, but slightly worse (0.19 absolute BLEU points) than the baseline on the 200K data set, which indicates that the paraphrase substitution method used in) does notwork on resource-sufficient data sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9395970702171326}, {"text": "20K data set", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.8064777652422587}, {"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.910062849521637}, {"text": "200K data set", "start_pos": 196, "end_pos": 209, "type": "DATASET", "confidence": 0.7998542984326681}]}, {"text": "In terms of the \"pair-CI 95%\", the left boundary for 20K data is \"+0.13\", which indicates that it is significantly better than the baseline system, while the left boundary is \"-0.46\" for 200K data, which indicates that the \"Para-Sub\" system is significantly worse than the baseline system.", "labels": [], "entities": []}, {"text": "3) comparing the \"Lattice\" system with the \"Para-Sub\": Comparison between the baseline and our paraphrase lattice method on a large-scale data set.", "labels": [], "entities": []}, {"text": "system, the \"pair-CI 95%\" for 20K and 200K data are respectively [+0.41, +0.92] and [+1.40, +2.17], which indicates that the \"Lattice\" system is significantly better than the \"Para-Sub\" system on these two data sets as well.", "labels": [], "entities": []}, {"text": "4) In terms of the two metrics, our proposed method achieves the best performance, which shows that our method is effective and consistent on different sizes of data.", "labels": [], "entities": []}, {"text": "In order to verify our method on large-scale data, we also perform experiments on 2.1 million sentence-pairs of English-to-Chinese data as described in Section 4.1.", "labels": [], "entities": []}, {"text": "The results are shown in Table 3.", "labels": [], "entities": []}, {"text": "From, it can be seen that the \"Lattice\" system achieves an improvement of 0.51 absolute (3.63% relative) BLEU points and a decrease of 1.6 absolute (2.14% relative) TER points compared to the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9894962906837463}, {"text": "TER", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.9973152279853821}]}, {"text": "In terms of the \"pair-CI 95%\", the left boundary for the \"Lattice\" system is \"+0.15\" which indicates that it is significantly better than the baseline system in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9970278143882751}]}, {"text": "Interestingly, in our experiment, the \"Para-Sub\" system also outperforms the baseline on those three automatic metrics.", "labels": [], "entities": []}, {"text": "However, in terms of the \"pair-CI 95%\", the left boundary for the \"Para-Sub\" system is \"-0.18\" which indicates that it is not significantly better than the baseline system in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9969305396080017}]}, {"text": "The results also show that our proposed method is effective and consistent even on a large-scale data set.", "labels": [], "entities": []}, {"text": "It also can be seen that the improvement on 2.1 million sentence-pairs is less than that of the 20K and 200K data sets.", "labels": [], "entities": []}, {"text": "That is, as the size of the training data increases, the problems of data sparseness decrease, so that the coverage of the test set by the parallel corpus will correspondingly increase.", "labels": [], "entities": []}, {"text": "In this case, the role of paraphrases in decoding becomes a little weaker.", "labels": [], "entities": []}, {"text": "On the other hand, it might become a kind of noise to interfere with the exact translation of the original source-side phrases when decoding.", "labels": [], "entities": []}, {"text": "Therefore, our proposed method maybe more appropriate for language pairs with limited resources.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The coverage of the test set by the phrase table and the parallel corpus based on different amount of the  training data. \"PL\" indicates the Phrase Length N , where {1 <= N <= 10}; \"20K\" and \"200K\" represent the sizes  of the parallel data for model training and phrase extraction; \"Cov.\" indicates the coverage rate; \"Tset\" represents the  number of unique phrases with the length N in the Test Set; \"PT\" represents the number of phrases of the Test Set  occur in the Phrase", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 273, "end_pos": 290, "type": "TASK", "confidence": 0.7566956579685211}, {"text": "PT", "start_pos": 412, "end_pos": 414, "type": "METRIC", "confidence": 0.9918598532676697}]}, {"text": " Table 2: Comparison between the baseline, \"Para-Sub\" and our \"Lattice\" (paraphrase lattice) method.", "labels": [], "entities": []}, {"text": " Table 3: Comparison between the baseline and our paraphrase lattice method on a large-scale data set.", "labels": [], "entities": []}, {"text": " Table 4: The coverage of the paraphrase-added test set by  the phrase table on medium size of the training data.", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9557206034660339}]}, {"text": " Table 5: Comparison on sentence-level scores in terms of  BLEU and TER metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.998712420463562}, {"text": "TER metrics", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9694388210773468}]}]}