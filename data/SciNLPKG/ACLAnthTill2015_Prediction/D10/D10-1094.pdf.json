{"title": [{"text": "Generating Confusion Sets for Context-Sensitive Error Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we consider the problem of generating candidate corrections for the task of correcting errors in text.", "labels": [], "entities": []}, {"text": "We focus on the task of correcting errors in preposition usage made by non-native English speakers, using discriminative classifiers.", "labels": [], "entities": [{"text": "correcting errors in preposition usage made by non-native English speakers", "start_pos": 24, "end_pos": 98, "type": "TASK", "confidence": 0.7718081116676331}]}, {"text": "The standard approach to the problem assumes that the set of candidate corrections fora preposition consists of all preposition choices participating in the task.", "labels": [], "entities": []}, {"text": "We determine likely preposition confusions using an annotated corpus of non-native text and use this knowledge to produce smaller sets of candidates.", "labels": [], "entities": [{"text": "preposition confusions", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.750823438167572}]}, {"text": "We propose several methods of restricting candidate sets.", "labels": [], "entities": []}, {"text": "These methods exclude candidate prepositions that are not observed as valid corrections in the annotated corpus and take into account the likelihood of each preposition confusion in the non-native text.", "labels": [], "entities": []}, {"text": "We find that restricting candidates to those that are observed in the non-native data improves both the precision and the recall compared to the approach that views all prepositions as possible candidates.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9997078776359558}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9997424483299255}]}, {"text": "Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "We address the problem of generating candidate corrections for the task of correcting context-dependent mistakes in text, mistakes that involve confusing valid words in a language.", "labels": [], "entities": []}, {"text": "A well-studied instance of this problem -context-sensitive spelling errorshas received a lot of attention in natural language research).", "labels": [], "entities": []}, {"text": "The context-sensitive spelling correction task addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there or your and you're.", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.5969771941502889}]}, {"text": "In this task, a candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there} or {cite, site, sight}.", "labels": [], "entities": []}, {"text": "Each occurrence of a confusable word in text is represented as a vector of features derived from a small context window around the target.", "labels": [], "entities": []}, {"text": "A classifier is trained on text assumed to be error-free, replacing each target word occurrence (e.g. their) with a confusion set consisting of {their, there}, thus generating both positive and negative examples, respectively, from the same context.", "labels": [], "entities": []}, {"text": "Given a text to correct, for each word in text that belongs to the confusion set the classifier predicts the most likely candidate in the confusion set.", "labels": [], "entities": []}, {"text": "More recently, work in error correction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage.", "labels": [], "entities": [{"text": "error correction", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.6831347793340683}, {"text": "correcting errors made by English as a Second Language (ESL) learners", "start_pos": 85, "end_pos": 154, "type": "TASK", "confidence": 0.7259198014552777}]}, {"text": "These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels.", "labels": [], "entities": []}, {"text": "Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task.", "labels": [], "entities": [{"text": "context-sensitive spelling correction task", "start_pos": 72, "end_pos": 114, "type": "TASK", "confidence": 0.7036251574754715}]}, {"text": "A system is usually trained on wellformed native English text (), but several works incorporate into training error-tagged data or error statistics.", "labels": [], "entities": []}, {"text": "The classifier is then applied to non-native text to predict the correct article/preposition in context.", "labels": [], "entities": []}, {"text": "The possible candidate selections include the set of all articles or all prepositions.", "labels": [], "entities": []}, {"text": "While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes.", "labels": [], "entities": [{"text": "article correction task", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8488308787345886}]}, {"text": "For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction.", "labels": [], "entities": []}, {"text": "This approach, however, does not take into account that writers do not make mistakes randomly: Not all candidates are equally likely given the preposition chosen by the author and errors may depend on the first language (L1) of the writer.", "labels": [], "entities": []}, {"text": "In this paper, we define L1-dependent candidate sets for the preposition correction task (Section 4.1).", "labels": [], "entities": [{"text": "preposition correction task", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.8658232688903809}]}, {"text": "L1-dependent candidate sets reflect preposition confusions observed with the speakers of the first language L1.", "labels": [], "entities": []}, {"text": "We propose methods of enforcing L1-dependent candidate sets in training and testing.", "labels": [], "entities": []}, {"text": "We consider mistakes involving the top ten English prepositions.", "labels": [], "entities": []}, {"text": "As our baseline system, we train a multi-class classifier in one-vs-all approach, which is a standard approach to multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.7796826958656311}]}, {"text": "In this approach, a separate binary classifier for each preposition pi , 1 \u2264 i \u2264 10, is trained, s.t. all pi examples are positive examples for the classifier and all other nine classes act as negative examples.", "labels": [], "entities": []}, {"text": "Thus, for each preposition pi in non-native text there are ten 1 possible prepositions that the classifier can propose as corrections for pi . We contrast this baseline method to two methods that enforce L1-dependent candidate sets in training.", "labels": [], "entities": []}, {"text": "First, we train a separate classifier for each preposition pi on the prepositions that belong to L1-dependent candidate set of pi . In this setting, the negative examples for pi are those that belong to L1-dependent candidate set of pi . The second method of enforcing L1-dependent candidate sets in training is to train on native data with artificial preposition errors in the spirit of, where the errors mimic the error rates and error patterns of the non-native text.", "labels": [], "entities": []}, {"text": "This method requires more knowledge, since it uses a distribution of errors from an error-tagged corpus.", "labels": [], "entities": []}, {"text": "We also propose a method of enforcing L1-dependent candidate sets in testing, through the use of a confidence threshold.", "labels": [], "entities": []}, {"text": "We consider two ways of applying a threshold: (1) the standard way, when a correction is proposed only if the classifier's confidence is sufficiently high and (2) L1-dependent threshold, when a correction is proposed only if it belongs to L1-dependent candidate set.", "labels": [], "entities": []}, {"text": "We show that the methods of restricting candidate sets to L1-dependent confusions improve the preposition correction system.", "labels": [], "entities": [{"text": "preposition correction", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7837339043617249}]}, {"text": "We demonstrate that restricting candidate sets to those prepositions that are confusable in the data by L1 writers is beneficial, when compared to a system that assumes an unrestricted candidate set by considering as valid corrections all prepositions participating in the task.", "labels": [], "entities": []}, {"text": "Furthermore, we find that the most effective method is the one that uses knowledge about the likelihoods of preposition confusions in the non-native text introduced through artificial errors in training.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we describe related work on error correction.", "labels": [], "entities": [{"text": "error correction", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7180779874324799}]}, {"text": "Section 3 presents the ESL data and statistics on preposition errors.", "labels": [], "entities": [{"text": "ESL data", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.7081611901521683}]}, {"text": "Section 4 describes the methods of restricting candidate sets in training and testing.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental setup.", "labels": [], "entities": []}, {"text": "We present and discuss the results in Section 6.", "labels": [], "entities": []}, {"text": "The key findings are summarized in and in Section 6.", "labels": [], "entities": []}, {"text": "We conclude with a brief discussion of directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe experiments with L1-dependent confusion sets.", "labels": [], "entities": []}, {"text": "Combining the three training conditions shown in with the two ways of thresholding described in Section 4.3, we build four systems 7 : 1.", "labels": [], "entities": []}, {"text": "NegAll-Clean-ThreshAll This system assumes both in training and in testing stages that all preposition confusions are possible.", "labels": [], "entities": []}, {"text": "The system is trained as a multi-class 10-way classifier, where for each preposition pi , all other nine prepositions are negative examples.", "labels": [], "entities": []}, {"text": "In testing, when applying the threshold, all prepositions are considered as valid corrections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on prepositions in the ESL data.", "labels": [], "entities": [{"text": "ESL data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9152313470840454}]}, {"text": " Table 4: Performance results for the 4 systems. All sys- tems, except for NegAll-ErrorL1-NoThresh, use a thresh- old, which is optimized for accuracy on the development  set. Baseline denotes the percentage of prepositions used  correctly in the data. The baseline allows us to evaluate  the systems with respect to accuracy, the percentage of  prepositions, on which the prediction of the system is the  same as the label. Averaged results over 2 runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9983178377151489}, {"text": "accuracy", "start_pos": 317, "end_pos": 325, "type": "METRIC", "confidence": 0.998838484287262}]}, {"text": " Table 5: Comparison of the performance of the 4 sys- tems on all data combined. All systems, except for  NegAll-ErrorL1-NoThresh, use a threshold, which is op- timized for accuracy on the development set. The dif- ferences between NegAll-ErrorL1-ThreshL1 and each of  the other three systems are statistically significant (Mc- Nemar's test, p < 0.01).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9984446167945862}]}, {"text": " Table 6: Comparison to other systems. Please note  that a direct comparison is not possible, since the systems  are trained and evaluated on different data sets. Gamon  (2010) also considers missing and extraneous preposition  errors.", "labels": [], "entities": []}]}