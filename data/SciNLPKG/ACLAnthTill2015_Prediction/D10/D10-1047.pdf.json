{"title": [{"text": "Multi-document summarization using A* search and discriminative training", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7042595148086548}]}], "abstractContent": [{"text": "In this paper we address two key challenges for extractive multi-document summarization: the search problem of finding the best scoring summary and the training problem of learning the best model parameters.", "labels": [], "entities": [{"text": "extractive multi-document summarization", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.5928666392962137}]}, {"text": "We propose an A* search algorithm to find the best extractive summary up to a given length, which is both optimal and efficient to run.", "labels": [], "entities": []}, {"text": "Further, we propose a discriminative training algorithm which directly maximises the quality of the best summary , rather than assuming a sentence-level decomposition as in earlier work.", "labels": [], "entities": []}, {"text": "Our approach leads to significantly better results than earlier techniques across a number of evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-document summarization aims to present multiple documents inform of a short summary.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7310050427913666}]}, {"text": "This short summary can be used as a replacement for the original documents to reduce, for instance, the time a reader would spend if she were to read the original documents.", "labels": [], "entities": []}, {"text": "Following dominant trends in summarization research, we focus solely on extractive summarization which simplifies the summarization task to the problem of identifying a subset of units from the document collection (here sentences) which are concatenated to form the summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.979836642742157}, {"text": "summarization", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.9854643940925598}]}, {"text": "Most multi-document summarization systems define a model which assigns a score to a candidate summary based on the features of the sentences included in the summary.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.5477333068847656}]}, {"text": "The research challenges are then twofold: 1) the search problem of finding the best scoring summary fora given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries -typically human authored extractive or abstractive summaries.", "labels": [], "entities": []}, {"text": "Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g.,) or using heuristic strategies based on position in document or lexical clues.", "labels": [], "entities": []}, {"text": "We show in this paper that the search problem can be solved optimally and efficiently using A* search).", "labels": [], "entities": []}, {"text": "Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words.", "labels": [], "entities": []}, {"text": "Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective.", "labels": [], "entities": []}, {"text": "These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.9416696429252625}]}, {"text": "The model is then used during search to find a summary composed of high scoring ('good') sentences (see fora review).", "labels": [], "entities": []}, {"text": "However, there is a disconnect between the model used for training and the model used for prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 90, "end_pos": 100, "type": "TASK", "confidence": 0.9617520570755005}]}, {"text": "In this paper we present a solution to this disconnect in the form of a training algorithm that optimises the full prediction model directly with the search algorithm intact.", "labels": [], "entities": []}, {"text": "The training algorithm learns parameters such that the best scoring whole summary under the model has a high score under the evaluation metric.", "labels": [], "entities": []}, {"text": "We demonstrate that this leads to significantly better test performance than a competitive baseline, to the tune of 3% absolute increase for ROUGE-1, -2 and -SU4.", "labels": [], "entities": [{"text": "absolute", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9735853672027588}, {"text": "ROUGE-1", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9913877844810486}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the summarization model.", "labels": [], "entities": [{"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9667143821716309}]}, {"text": "Next in section 3 we present an A* search algorithm for finding the best scoring (argmax) summary under the model with a constraint on the maximum summary length.", "labels": [], "entities": []}, {"text": "We show that this algorithm performs search efficiently, even for very large document sets composed of many sentences.", "labels": [], "entities": []}, {"text": "The second contribution of the paper is anew training method which directly optimises the summarization system, and is presented in section 4.", "labels": [], "entities": [{"text": "summarization", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9817577004432678}]}, {"text": "This uses the minimum error-rate training (MERT) technique from machine translation to optimise the summariser's output to an arbitrary evaluation metric.", "labels": [], "entities": [{"text": "minimum error-rate training (MERT", "start_pos": 14, "end_pos": 47, "type": "METRIC", "confidence": 0.7832675457000733}, {"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.753831297159195}]}, {"text": "Section 5 describes our experimental setup and section 6 the results.", "labels": [], "entities": []}, {"text": "Finally we conclude in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the features for which we learn weights.", "labels": [], "entities": []}, {"text": "We also describe the input data used in training and testing.", "labels": [], "entities": []}, {"text": "We report results for training and testing.", "labels": [], "entities": []}, {"text": "In both training and testing we distinguish between three different summaries: wordLimit, sentenceLimit and regression.", "labels": [], "entities": []}, {"text": "WordLimit and sentenceLimit summaries are the ones generated using the model trained by MERT.", "labels": [], "entities": [{"text": "WordLimit", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9619926810264587}, {"text": "MERT", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8915274143218994}]}, {"text": "As described in section 4 we trained the summariser using the A* search decoder to maximise the ROUGE score of the best scoring summaries.", "labels": [], "entities": [{"text": "summariser", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.9646089673042297}, {"text": "ROUGE score", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.9834140241146088}]}, {"text": "We used the heuristic function h3 in A* search because it is the best performing heuristic, and 100-best lists.", "labels": [], "entities": []}, {"text": "To experiment with different summary length conditions we differentiate between summaries with a word limit (wordLimit, set to 200 words) and summaries containing N number of sentences (sentenceLimit) as stop condition in A* search.", "labels": [], "entities": []}, {"text": "We set N so that in both wordLimit and sentenceLimit summaries we obtain more or less the same number of words (because our training data contains on average 17 words for each word we set N to 12, 12*17=194).", "labels": [], "entities": []}, {"text": "However, this is only the casein the training.", "labels": [], "entities": []}, {"text": "In the testing for both wordLimit and sentenceLimit we generate summaries with the same word limit constraint which allows us to have a fair comparison between the ROUGE recall scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 164, "end_pos": 169, "type": "METRIC", "confidence": 0.9740335941314697}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.5666627287864685}]}, {"text": "The regression summaries are our baseline.", "labels": [], "entities": []}, {"text": "In these summaries the sentences are ranked based on the weighted features produced by Support Vector Regression (SVR).", "labels": [], "entities": []}, {"text": "9 use multi-document summarization and linear regression methods to rank sentences in the documents.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 6, "end_pos": 34, "type": "TASK", "confidence": 0.5393611490726471}]}, {"text": "As regression model they used SVR and showed We use the term regression to refer to SVR.  that it out-performed classification and Learning To Rank methods on the DUC 2005 to 2007 data.", "labels": [], "entities": [{"text": "DUC 2005 to 2007 data", "start_pos": 163, "end_pos": 184, "type": "DATASET", "confidence": 0.9815627932548523}]}, {"text": "For comparison purpose we use SVR as a baseline system for learning feature weights.", "labels": [], "entities": []}, {"text": "It should be noted that these weights are learned based on single sentences.", "labels": [], "entities": []}, {"text": "However, to have a fair comparison between all our summary types we use these weights to generate summaries using the A* search with the word limit as constraint.", "labels": [], "entities": []}, {"text": "We do this for reporting both for training and testing results.", "labels": [], "entities": []}, {"text": "The results for training are shown in.", "labels": [], "entities": []}, {"text": "The table shows ROUGE recall numbers obtained by comparing model summaries against automatically generated summaries on the training data.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9218316674232483}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.7189027070999146}]}, {"text": "Because in training we used three different metrics (R-1, R-2, R-SU4) to train weights we report results for each of these three different ROUGE metrics.", "labels": [], "entities": []}, {"text": "In we can see that the scores for wordLimit and sentenceLimit type summaries are always at maximum on the metric they were trained on (this can be observed by following the main diagonal of the result matrix).", "labels": [], "entities": []}, {"text": "This confirms that MERT is maximizing the metric for which it was trained.", "labels": [], "entities": [{"text": "MERT", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.5021692514419556}]}, {"text": "However, this is not the case for regression results.", "labels": [], "entities": []}, {"text": "The scores obtained with R-SU4 metric trained weights achieve higher scores on R-1 and R-2 compared to the scores obtained using weights trained on those metrics.", "labels": [], "entities": [{"text": "R-1", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8003292679786682}]}, {"text": "This is most likely due to SVR being trained on sentences rather than over entire summaries, and thereby not adequately optimising the metric used for evaluation.", "labels": [], "entities": []}, {"text": "The results for testing are shown in.", "labels": [], "entities": []}, {"text": "As with the training setting we report ROUGE recall scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9976218342781067}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.8849217891693115}]}, {"text": "We use the testing data described in section 5.2 for this setting.", "labels": [], "entities": []}, {"text": "However, because we have two different input document sets we report separate results for each of these ( shows result for in domain data and shows result for out   of domain data).", "labels": [], "entities": []}, {"text": "Again as with the training setting we report results for the different metrics (R-1, R-2, R-SU4) separately.", "labels": [], "entities": []}, {"text": "From we can see that the wordLimit summaries score highest compared to the other two types of summaries.", "labels": [], "entities": [{"text": "wordLimit summaries", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.8425061106681824}]}, {"text": "This is different from the training results where sentenceLimit summary type summaries are the top scoring ones.", "labels": [], "entities": []}, {"text": "As mentioned earlier the sentenceLimit summaries contain exactly 12 sentences, whereon average each sentence in the training data has 17 words.", "labels": [], "entities": []}, {"text": "We picked 12 sentences to achieve roughly the same word limit constraint (12 \u00d7 17 = 204) so they can be compared to the wordLimit and regression type summaries.", "labels": [], "entities": []}, {"text": "However, these sentenceLimit summaries have an average of 221 words, which explains the higher ROUGE recall scores seen in training compared to testing (where a 200 word limit was imposed).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9929062128067017}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.7100913524627686}]}, {"text": "The wordLimit summaries are significantly better than the scores from the other summary types irrespective of the evaluation metric.", "labels": [], "entities": [{"text": "wordLimit summaries", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7343979775905609}]}, {"text": "It should be noted that these summaries are the only ones where the training and testing had the same condition in A* search concerning the summary word limit constraint.", "labels": [], "entities": []}, {"text": "The scores in sentenceLimit type summaries are significantly lower than wordLimit summaries, despite using MERT to learn the weights.", "labels": [], "entities": [{"text": "sentenceLimit type summaries", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.5921365718046824}, {"text": "wordLimit summaries", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.565884530544281}, {"text": "MERT", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.8859727382659912}]}, {"text": "This shows that training the true model is critical forgetting good accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9984700083732605}]}, {"text": "The regression type summaries achieved the worst ROUGE metric scores.", "labels": [], "entities": [{"text": "ROUGE metric", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9786031544208527}]}, {"text": "The weights used to generate these summaries were trained on single sentences using SVR.", "labels": [], "entities": []}, {"text": "These results indicate that if the goal is to generate high scoring summaries under a length limit in testing, then the same constraint should also be used in training.", "labels": [], "entities": []}, {"text": "From and 6 we can see that the summaries obtained from VirtualTourist captions (in domain data) score roughly the same as the summaries generated using web-documents (out of domain data) as input.", "labels": [], "entities": []}, {"text": "A possible explanation is that in many cases the VirtualTourist original captions contain text from Wikipedia articles, which are also returned as results from the web search.", "labels": [], "entities": []}, {"text": "Therefore the web-document sets included similar content to the VirtualTourist captions.", "labels": [], "entities": [{"text": "VirtualTourist captions", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.7999505996704102}]}, {"text": "We also evaluated our summaries using a readability assessment as in DUC and TAC.", "labels": [], "entities": [{"text": "DUC", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9280068874359131}, {"text": "TAC", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.7693969011306763}]}, {"text": "DUC and TAC manually assess the quality of automatically generated summaries by asking human subjects to score each summary using five criteria -grammaticality, redundancy, clarity, focus and coherence criteria.", "labels": [], "entities": [{"text": "DUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9408770799636841}, {"text": "TAC", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.6107255816459656}, {"text": "clarity", "start_pos": 173, "end_pos": 180, "type": "METRIC", "confidence": 0.9939408302307129}]}, {"text": "Each criterion is scored on a five point scale with high scores indicating a better result.", "labels": [], "entities": []}, {"text": "For this evaluation we used the best scoring summaries from the wordLimit summary type (R-1, R-2 and R-SU4) generated using web-documents (out of domain documents) as input.", "labels": [], "entities": []}, {"text": "We also evaluate the regression summary types generated using the same input documents to investigate the correlation between high and low ROUGE metric scores to manual evaluation ones.", "labels": [], "entities": [{"text": "ROUGE metric scores", "start_pos": 139, "end_pos": 158, "type": "METRIC", "confidence": 0.9567254980405172}]}, {"text": "From the regression summary type we only use summaries under the R2 and RSU4 trained models.", "labels": [], "entities": []}, {"text": "In total we evaluated five different summary types (three from wordLimit and two from regression).", "labels": [], "entities": [{"text": "wordLimit", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.8817214369773865}]}, {"text": "For each type we randomly selected 30 place names and asked three people to assess the summaries for these place names.", "labels": [], "entities": []}, {"text": "Each person was shown all 150  summaries (30 from each summary type) in a random way and was asked to assess them according to the DUC and TAC manual assessment scheme.", "labels": [], "entities": [{"text": "DUC", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.9695595502853394}, {"text": "TAC manual assessment", "start_pos": 139, "end_pos": 160, "type": "DATASET", "confidence": 0.7690609097480774}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "11 From we can see that overall the wordLimit type summaries perform better than the regression ones.", "labels": [], "entities": [{"text": "wordLimit type summaries", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.7566372354825338}]}, {"text": "For each metric in regression summary types (R-2 and R-SU4) we compute the significance of the difference with the same metrics in wordLimit summary types.", "labels": [], "entities": []}, {"text": "The results for the clarity, coherence and focus criteria in wordLimit summaries are significantly better than in regression ones (p<0.001) irrespective of the training metric.", "labels": [], "entities": [{"text": "clarity", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.998615026473999}]}, {"text": "These results concur with the automatic evaluation results as described in section 6.1.", "labels": [], "entities": []}, {"text": "However, this is not the case for the grammaticality and redundancy criteria.", "labels": [], "entities": []}, {"text": "Although in regression type summaries the scores for the grammaticality criterion are lower than those in wordLimit summaries the difference is not significant.", "labels": [], "entities": []}, {"text": "Furthermore, we can see that the redundancy scores for regression summaries are slightly higher than those for wordLimit summaries.", "labels": [], "entities": [{"text": "wordLimit summaries", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.6001881957054138}]}, {"text": "One reason for these differences might be the way we trained feature weights for wordLimit and regression summaries.", "labels": [], "entities": [{"text": "regression summaries", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.5546934306621552}]}, {"text": "As mentioned above, feature weights for wordLimit summaries are trained using summaries with a specific word limit constraint, whereas the weights for the regression summaries are learned using single sentences.", "labels": [], "entities": [{"text": "wordLimit summaries", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.5932752788066864}]}, {"text": "Maximizing the ROUGE metrics using \"final or output We computed the agreement between the users using intra class correlation with Cronbach's Alpha where the correlation coefficient ranges between 0 and 1.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.8401023745536804}]}, {"text": "Numbers close to 1 indicate high correlation and numbers close to 0 indicate low correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9450116753578186}, {"text": "correlation", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.905156672000885}]}, {"text": "For the clarity criterion the assessors' correlation coefficient is 0.547, for coherence 0.687, for focus 0.688, for grammaticality 0.232 and for redundancy 0.453.", "labels": [], "entities": [{"text": "clarity", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9900289177894592}, {"text": "correlation coefficient", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.9135296940803528}]}, {"text": "We compute significance test for the manual evaluation results using \u03c7 square.", "labels": [], "entities": [{"text": "significance", "start_pos": 11, "end_pos": 23, "type": "METRIC", "confidence": 0.985872209072113}]}, {"text": "like summaries\" will lead to a higher content agreement between the training and the model summaries whereas this is not guaranteed with single sentences.", "labels": [], "entities": []}, {"text": "With single sentences we have only a guarantee for high content overlap between single training and model sentences.", "labels": [], "entities": []}, {"text": "However, when these sentences are combined into summaries it is not guaranteed that these summaries will also have high content overlap with the entire model ones.", "labels": [], "entities": []}, {"text": "Therefore we believe if there is a high content agreement between the training and model summaries this could lead to more readable summaries.", "labels": [], "entities": []}, {"text": "However, as we can see from this hypothesis does not hold for all criteria.", "labels": [], "entities": []}, {"text": "In case of the redundancy criterion we have compared to wordLimit summary type high scores in regression summaries although wordLimit summaries are significantly better than regression ones when it concerns the ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 211, "end_pos": 216, "type": "METRIC", "confidence": 0.9564334154129028}]}, {"text": "Thus it is likely that by aggressively optimising the ROUGE metric the model learns to game the metric, which does not penalise redundancy in the summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9278430938720703}]}, {"text": "As such it may no longer possible to extrapolate trends from earlier correlation studies against human judgements.", "labels": [], "entities": []}, {"text": "To minimize redundancy in summaries it is necessary to also take into consideration global features addressing the linguistic aspects of the summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.9603618383407593}]}, {"text": "Furthermore, instead of ROUGE recall scores which do not take the repetition of information into consideration, ROUGE precision scores could be used as a metric in order to minimize the redundant content in the summaries.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.7144307494163513}, {"text": "ROUGE precision scores", "start_pos": 112, "end_pos": 134, "type": "METRIC", "confidence": 0.7712270816167196}]}], "tableCaptions": [{"text": " Table 1: The training input data contains 184 place  names with 42333 sentences in total. The numbers in  the columns give detail about the number of sentences  for each place and the lengths of the sentences.", "labels": [], "entities": []}, {"text": " Table 2: In domain test data. The numbers in the columns  give detail about the number of documents (descriptions)  for each place, number of sentences for each place and  document (description) and the lengths of the sentences.", "labels": [], "entities": []}, {"text": " Table 3: Out of domain test data. The numbers in the  columns give detail about the number of sentences for  each place and document and the lengths of the sentences.", "labels": [], "entities": []}, {"text": " Table 4: ROUGE scores obtained on the training data.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.993225634098053}]}, {"text": " Table 5: ROUGE scores obtained on the testing data. The  automated summaries are generated using the in domain  input documents.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.991851270198822}]}, {"text": " Table 6: ROUGE scores obtained on the testing data. The  automated summaries are generated using the out of do- main input documents.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9945992231369019}]}, {"text": " Table 7: Manual evaluation results for the wordLimit (R1,  R2, RSU4) and regression (R2, RSU4) summary types.  The numbers in the columns are the average scores.", "labels": [], "entities": [{"text": "wordLimit", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.8479355573654175}]}]}