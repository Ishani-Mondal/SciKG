{"title": [], "abstractContent": [{"text": "Strong indications of perspective can often come from collocations of arbitrary length; for example, someone writing get the government out of my X is typically expressing a conservative rather than progressive viewpoint.", "labels": [], "entities": []}, {"text": "However , going beyond unigram or bigram features in perspective classification gives rise to problems of data sparsity.", "labels": [], "entities": [{"text": "perspective classification", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7342774271965027}]}, {"text": "We address this problem using nonparametric Bayesian modeling, specifically adaptor grammars (Johnson et al., 2006).", "labels": [], "entities": []}, {"text": "We demonstrate that an adaptive na\u00a8\u0131vena\u00a8\u0131ve Bayes model captures multiword lexical usages associated with perspective, and establishes anew state-of-the-art for perspective classification results using the Bitter Lemons corpus, a collection of essays about mid-east issues from Israeli and Palestinian points of view.", "labels": [], "entities": [{"text": "perspective classification", "start_pos": 162, "end_pos": 188, "type": "TASK", "confidence": 0.7180490344762802}]}], "introductionContent": [{"text": "Most work on the computational analysis of sentiment and perspective relies on lexical features.", "labels": [], "entities": [{"text": "computational analysis of sentiment and perspective", "start_pos": 17, "end_pos": 68, "type": "TASK", "confidence": 0.8344845871130625}]}, {"text": "This makes sense, since an author's choice of words is often used to express overt opinions (e.g. describing healthcare reform as idiotic or wonderful) or to frame a discussion in order to convey a perspective more implicitly (e.g. using the term death tax instead of estate tax).", "labels": [], "entities": []}, {"text": "Moreover, it is easy and efficient to represent texts as collections of the words they contain, in order to apply a well known arsenal of supervised techniques (;.", "labels": [], "entities": []}, {"text": "At the same time, standard lexical features have their limitations for this kind of analysis.", "labels": [], "entities": []}, {"text": "Such features are usually created by selecting some small n-gram size in advance.", "labels": [], "entities": []}, {"text": "Indeed, it is not uncommon to seethe feature space for sentiment analysis limited to unigrams.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.963604211807251}]}, {"text": "However, important indicators of perspective can also be longer (get the government out of my).", "labels": [], "entities": []}, {"text": "Trying to capture these using standard machine learning approaches creates a problem, since allowing n-grams as features for larger n gives rise to problems of data sparsity.", "labels": [], "entities": []}, {"text": "In this paper, we employ nonparametric Bayesian models) in order to address this limitation.", "labels": [], "entities": []}, {"text": "In contrast to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can \"grow\" to the size best suited to the observed data.", "labels": [], "entities": []}, {"text": "In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure -for example, in topic modeling, when the true number of topics is not known (); in grammatical inference, when the appropriate number of nonterminal symbols is not known (; and in coreference resolution, when the number of entities in a given document is not specified in advance).", "labels": [], "entities": [{"text": "text analysis", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.7218300551176071}, {"text": "topic modeling", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.7997881472110748}, {"text": "coreference resolution", "start_pos": 295, "end_pos": 317, "type": "TASK", "confidence": 0.946750283241272}]}, {"text": "Here we use them for supervised text classification.", "labels": [], "entities": [{"text": "supervised text classification", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.5963525970776876}]}, {"text": "Specifically, we use adaptor grammars), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes, grammar induction, and named entity structure learning, to make supervised na\u00a8\u0131vena\u00a8\u0131ve Bayes classification nonparametric in order to improve perspective modeling.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7370518296957016}, {"text": "named entity structure learning", "start_pos": 181, "end_pos": 212, "type": "TASK", "confidence": 0.6244542673230171}, {"text": "perspective modeling", "start_pos": 301, "end_pos": 321, "type": "TASK", "confidence": 0.7639361023902893}]}, {"text": "Intuitively, na\u00a8\u0131vena\u00a8\u0131ve Bayes associates each class or label with a probability distribution over a fixed vocabulary.", "labels": [], "entities": []}, {"text": "We introduce adaptive na\u00a8\u0131vena\u00a8\u0131ve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined by the properties of the dataset.", "labels": [], "entities": [{"text": "\u0131ve Bayes (ANB)", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.5523536384105683}]}, {"text": "We show that using adaptive na\u00a8\u0131vena\u00a8\u0131ve Bayes improves on state of the art classification using the Bitter Lemons corpus (), a document collection that has been used by a variety of authors to evaluate perspective classification.", "labels": [], "entities": [{"text": "perspective classification", "start_pos": 203, "end_pos": 229, "type": "TASK", "confidence": 0.7448692619800568}]}, {"text": "In Section 2, we review adaptor grammars, show how na\u00a8\u0131vena\u00a8\u0131ve Bayes can be expressed within the formalism, and describe how -and how easily -an adaptive na\u00a8\u0131vena\u00a8\u0131ve Bayes model can be created.", "labels": [], "entities": []}, {"text": "Section 3 validates the approach via experimentation on the Bitter Lemons corpus.", "labels": [], "entities": [{"text": "Bitter Lemons corpus", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.8107263247172037}]}, {"text": "In Section 4, we summarize the contributions of the paper and discuss directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The vocabulary generator determines the vocabulary used by a given experiment by converting the training set to lowercase, stemming with the Porter stemmer, and filtering punctuation.", "labels": [], "entities": []}, {"text": "We remove from the vocabulary any words that appeared in only one document regardless of frequency within that document, words with frequencies lower than a threshold, and stop words.", "labels": [], "entities": []}, {"text": "The vocabulary is then passed to a grammar generator and a corpus filter.", "labels": [], "entities": []}, {"text": "The grammar generator uses the vocabulary to generate the terminating rules of the grammar from the ANB grammar presented in.", "labels": [], "entities": []}, {"text": "The corpus filter takes in a set of documents and replaces all words not in the vocabulary with \"out of vocabulary\" markers.", "labels": [], "entities": []}, {"text": "This process ensures that in all experiments the vocabulary is composed entirely of words from the training set.", "labels": [], "entities": []}, {"text": "After the groups have been filtered, the group used as the test set has its labels removed.", "labels": [], "entities": []}, {"text": "The test and training set are then sent, along with the grammar, into the adaptor grammar inference engine.", "labels": [], "entities": []}, {"text": "Each experiment ran for 3000 iterations.", "labels": [], "entities": []}, {"text": "For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample.", "labels": [], "entities": []}, {"text": "We use the resulting sentence parses for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9560869932174683}]}, {"text": "By design of the grammar, each sentence's words will belong to one and only one distribution.", "labels": [], "entities": []}, {"text": "We identify that distribution from each of the test set sentence parses and use it as the sentence level classification for that particular sentence.", "labels": [], "entities": []}, {"text": "We then use majority rule on the individual sentence classifications in a document to obtain the document classification.", "labels": [], "entities": []}, {"text": "(In most cases the sentence-level assignments are overwhelmingly dominated by one class.) gives the results and compares to prior work.", "labels": [], "entities": []}, {"text": "The support vector machine (SVM), NB-B and LSPM results are taken directly from.", "labels": [], "entities": []}, {"text": "NB-B indicates na\u00a8\u0131vena\u00a8\u0131ve Bayes with full Bayesian inference.", "labels": [], "entities": []}, {"text": "LSPM is the Latent Sentence Perspective Model, also from.", "labels": [], "entities": [{"text": "LSPM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6738659739494324}]}, {"text": "OPUS results are taken from Greene In these experiments, a frequency threshold of 4 was selected prior to testing.  and Resnik.", "labels": [], "entities": [{"text": "OPUS", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.4627342224121094}]}, {"text": "Briefly, OPUS features are generated from observable grammatical relations that come from dependency parses of the corpus.", "labels": [], "entities": []}, {"text": "Use of these features provided the best classification accuracy for this task prior to this work.", "labels": [], "entities": [{"text": "classification", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.9475470781326294}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9761409759521484}]}, {"text": "ANB* refers to the grammar from, but with adaptation disabled.", "labels": [], "entities": [{"text": "ANB", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.57126784324646}]}, {"text": "The reported accuracy values for ANB*, ANB with a common base distribution (see), and ANB with separate base distributions (see) are the mean values from five separate sampling chains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9933933615684509}]}, {"text": "Bold face indicates statistical signficance (p < 0.05) by unpaired t-test between the reported value and ANB*.", "labels": [], "entities": [{"text": "statistical signficance", "start_pos": 20, "end_pos": 43, "type": "METRIC", "confidence": 0.8432053625583649}, {"text": "ANB", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9924423694610596}]}, {"text": "Consistent with all prior work on this corpus we found that the classification accuracy for training on editors and testing on guests was lower than the other direction since the larger number of editors in the guest set allows for greater generalization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9621087312698364}]}, {"text": "The difference between ANB* and ANB with a common base distribution is not statistically significant.", "labels": [], "entities": []}, {"text": "Also of note is that the classification accuracy improves for testing on Guests when the ANB grammar is allowed to adapt and a separate base distribution is used for the two classes (88.28% versus 84.98% without adaptation).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9798275232315063}, {"text": "ANB grammar", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.8078958690166473}]}, {"text": "palestinian territory freedom (of the) press palestinian statehood palestinian violence palestinian refugee learned once inference is complete.", "labels": [], "entities": []}, {"text": "The column labeled unique unigrams cached indicates the number of unique unigrams that appear on the right hand side of the adapted rules.", "labels": [], "entities": []}, {"text": "Similarly, unique n-grams cached indicates the number of unique n-grams that appear on the right hand side of the adapted rules.", "labels": [], "entities": []}, {"text": "The rightmost column indicates the percentage of terms from the group vocabulary that appear on the right hand side of adapted rules as unigrams.", "labels": [], "entities": []}, {"text": "Values less than 100% indicate that the remaining vocabulary terms are cached in n-grams.", "labels": [], "entities": []}, {"text": "As the table shows, a significant number of the rules learned during inference are n-grams of various sizes.", "labels": [], "entities": []}, {"text": "Inspection of the captured bigrams showed that it captured sequences that a human might associate with one perspective over the other.", "labels": [], "entities": []}, {"text": "lists just a few of the more charged bigrams that were captured in the adapted rules.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Counts of cached unigrams and n-grams for the two classes compared to the vocabulary sizes.", "labels": [], "entities": []}, {"text": " Table 7. This  data clearly demonstrates that raw n-gram frequency  alone is not indicative of how many times an n-gram  is used as a cached rule. For example, consider the  bigram people go, which is used as a cached bigram  only three times, yet appears in the corpus 407 times.  Compare that with isra palestinian, which is cached", "labels": [], "entities": []}]}