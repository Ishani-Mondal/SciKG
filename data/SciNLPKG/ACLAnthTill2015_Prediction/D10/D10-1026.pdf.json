{"title": [{"text": "Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval", "labels": [], "entities": []}], "abstractContent": [{"text": "We present three novel methods of compactly storing very large n-gram language models.", "labels": [], "entities": []}, {"text": "These methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved inconstant time, at speeds comparable to modern language modeling toolkits.", "labels": [], "entities": []}, {"text": "Our basic approach generates an explicit minimal perfect hash function, that maps all n-grams in a model to distinct integers to enable storage of associated values.", "labels": [], "entities": []}, {"text": "Extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs, including variable length coding of values and the use of tiered structures that partition the data for more efficient storage.", "labels": [], "entities": []}, {"text": "We apply our approach to storing the full Google Web1T n-gram set and all 1-to-5 grams of the Gigaword newswire corpus.", "labels": [], "entities": [{"text": "Google Web1T n-gram set", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.8505388796329498}, {"text": "Gigaword newswire corpus", "start_pos": 94, "end_pos": 118, "type": "DATASET", "confidence": 0.9614520271619161}]}, {"text": "For the 1.5 billion n-grams of Gigaword, for example, we can store full count information at a cost of 1.66 bytes per n-gram (around 30% of the cost when using the current state-of-the-art approach), or quantized counts for 1.41 bytes per n-gram.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9578049182891846}]}, {"text": "For applications that are tolerant of a certain class of relatively innocuous errors (where unseen n-grams maybe accepted as rare n-grams), we can reduce the latter cost to below 1 byte per n-gram.", "labels": [], "entities": []}], "introductionContent": [{"text": "The availability of very large text collections, such as the Gigaword corpus of newswire, and the Google Web1T 1-5gram corpus), have made it possible to build models incorporating counts of billions of n-grams.", "labels": [], "entities": [{"text": "Gigaword corpus of newswire", "start_pos": 61, "end_pos": 88, "type": "DATASET", "confidence": 0.921503484249115}, {"text": "Google Web1T 1-5gram corpus", "start_pos": 98, "end_pos": 125, "type": "DATASET", "confidence": 0.824222981929779}]}, {"text": "The storage of these language models, however, presents serious problems, given both their size and the need to provide rapid access.", "labels": [], "entities": []}, {"text": "A prevalent approach for language model storage is the use of compact trie structures, but these structures do not scale well and require space proportional to both to the number of n-grams and the vocabulary size.", "labels": [], "entities": [{"text": "language model storage", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.6717038452625275}]}, {"text": "Recent advances) involve the development of Bloom filter based models, which allow a considerable reduction in the space required to store a model, at the cost of allowing some limited extent of false positives when the model is queried with previously unseen n-grams.", "labels": [], "entities": []}, {"text": "The aim is to achieve sufficiently compact representation that even very large language models can be stored totally within memory, avoiding the latencies of disk access.", "labels": [], "entities": []}, {"text": "These Bloom filter based models exploit the idea that it is not actually necessary to store the n-grams of the model, as long as, when queried with an n-gram, the model returns the correct count or probability for it.", "labels": [], "entities": []}, {"text": "These techniques allow the storage of language models that no longer depend on the size of the vocabulary, but only on the number of n-grams.", "labels": [], "entities": []}, {"text": "In this paper we give three different models for the efficient storage of language models.", "labels": [], "entities": []}, {"text": "The first structure makes use of an explicit perfect hash function that is minimal in that it maps n keys to integers in the range 1 ton.", "labels": [], "entities": []}, {"text": "We show that by using a minimal perfect hash function and exploiting the distributional characteristics of the data we produce n-gram models that useless space than all know approaches with no reduction in speed.", "labels": [], "entities": []}, {"text": "Our two further models achieve even more compact storage while maintaining constant time access by using variable length coding to compress the n-grams values and by using tiered hash structures to parti-tion the data into subsets requiring different amounts of storage.", "labels": [], "entities": []}, {"text": "This combination of techniques allows us, for example, to represent the full count information of the Google Web1T corpus) (where count values range up to 95 billion) at a cost of just 2.47 bytes per n-gram (assuming 8-bit fingerprints, to exclude false positives) and just 1.41 bytes per n-gram if we use 8-bit quantization of counts.", "labels": [], "entities": [{"text": "Google Web1T corpus", "start_pos": 102, "end_pos": 121, "type": "DATASET", "confidence": 0.8355231285095215}]}, {"text": "These costs are 36% and 57% respectively of the space required by the Bloomier Filter approach of.", "labels": [], "entities": []}, {"text": "For the Gigaword dataset, we can store full count information at a cost of only 1.66 bytes per n-gram.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9717808663845062}]}, {"text": "We report empirical results showing that our approach allows a look-up rate which is comparable to existing modern language modeling toolkits, and much faster than a competitor approach for space-efficient storage.", "labels": [], "entities": []}, {"text": "Finally, we propose the use of variable length fingerprinting for use in contexts which can tolerate a higher rate of 'less damaging' errors.", "labels": [], "entities": []}, {"text": "This move allows, for example, the cost of storing a quantized model to be reduced to 1 byte per n-gram or less.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: n-gram frequency counts from Gigaword corpus", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9432578682899475}]}, {"text": " Table 2: n-gram frequency counts from Google Web1T  corpus", "labels": [], "entities": [{"text": "Google Web1T  corpus", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.908205529054006}]}, {"text": " Table 3: Space usage in bytes/ngram using 12-bit finger- prints and storing all 1 to 5 grams", "labels": [], "entities": []}, {"text": " Table 4: Space usage in bytes/n-gram using 8-bit finger- prints and storing all 1 to 5 grams", "labels": [], "entities": []}, {"text": " Table 5: Comparison between approaches for storing all  1 to 5 grams of the Gigaword Corpus", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9280340075492859}]}, {"text": " Table 7: Bytes per fingerprint for T-MPHR model using 1  to 6 bit fingerprints for rarest n-grams and 12 bit (in total)  fingerprints for all other n-grams. (All configurations are  as in Footnote 2.)", "labels": [], "entities": [{"text": "Footnote", "start_pos": 189, "end_pos": 197, "type": "DATASET", "confidence": 0.9545512795448303}]}]}