{"title": [{"text": "A Mixture Model with Sharing for Lexical Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning.", "labels": [], "entities": []}, {"text": "Common tasks in lexical semantics such as word relatedness or selec-tional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e.g. the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying com-monalities shared by their typical arguments.", "labels": [], "entities": []}, {"text": "Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded.", "labels": [], "entities": [{"text": "Tiered clustering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6387848407030106}]}, {"text": "We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word meaning can be represented as highdimensional vectors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g.).", "labels": [], "entities": []}, {"text": "Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans.", "labels": [], "entities": []}, {"text": "Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g., often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts.", "labels": [], "entities": []}, {"text": "Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bank institution and bank river . Methods such as Clustering by and multi-prototype representations address this issue by performing word-sense disambiguation across word occurrences, and then building meaning vectors from the disambiguated words.", "labels": [], "entities": []}, {"text": "Such approaches can readily capture the structure of homonymous words with several unrelated meanings (e.g. bat and club), but are not suitable for representing the common metaphor structure found in highly polysemous words such as line or run.", "labels": [], "entities": []}, {"text": "In this paper, we introduce tiered clustering, a novel probabilistic model of the shared structure often neglected in clustering problems.", "labels": [], "entities": []}, {"text": "Tiered clustering performs soft feature selection, allocating features between a Dirichlet Process clustering model and a background model consisting of a single component.", "labels": [], "entities": [{"text": "Tiered clustering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6862259358167648}, {"text": "soft feature selection", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.6837862730026245}]}, {"text": "The background model accounts for features commonly shared by all occurrences (i.e. context-independent feature variation), while the clustering model accounts for variation in word usage (i.e. context-dependent variation, or word senses;).", "labels": [], "entities": []}, {"text": "Using the tiered clustering model, we derive a multi-prototype representation capable of capturing varying degrees of sharing between word senses, and demonstrate its effectiveness in lexical semantic tasks where such sharing is desirable.", "labels": [], "entities": []}, {"text": "In particular we show that tiered clustering outperforms the multi-prototype approach for (1) selectional preference, i.e. predict-ing the typical filler of an argument slot of a verb, and (2) word-relatedness in the presence of highly polysemous words.", "labels": [], "entities": []}, {"text": "The former case exhibits a high degree of explicit structure, especially for more selectionally restrictive verbs (e.g. the set of things that can be eaten or can shoot).", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 gives relevant background on the methods compared, Section 3 outlines the multiprototype model based on the Dirichlet Process mixture model, Section 4 derives the tiered clustering model, Section 5 discusses similarity metrics, Section 6 details the experimental setup and includes a micro-analysis of feature selection, Section 7 presents results applying tiered clustering to word relatedness and selectional preference, Section 8 discusses future work, and Section 9 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the tiered clustering model on two problems from lexical semantics: word relatedness and selectional preference.", "labels": [], "entities": []}, {"text": "WS-353 contains between 13 and 16 human similarity judgements for each of 353 word pairs, rated on a 1-10 integer scale.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8656992316246033}]}, {"text": "WN-Evocation is significantly larger than WS-353, containing over 100k similarity comparisons collected from trained human raters.", "labels": [], "entities": [{"text": "WN-Evocation", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8656286597251892}, {"text": "WS-353", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8393275737762451}]}, {"text": "Comparisons are assigned to only 3-5 human raters on average and contain a significantly higher fraction of zero-and low-similarity items than WS-353), reflecting more accurately real-world lexical semantics settings.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 143, "end_pos": 149, "type": "DATASET", "confidence": 0.842939555644989}]}, {"text": "In our experiments we discard all comparisons with fewer than 5 ratings and then sample 10% of the remaining pairs uniformly at random, resulting in a test set with 1317 comparisons.", "labels": [], "entities": []}, {"text": "For selectional preference, we employ the Pad\u00f3 dataset, which contains 211 verb-noun pairs with human similarity judgements for how plausible the noun is for each argument of the verb (2 arguments per verb, corresponding roughly to subject and object).", "labels": [], "entities": [{"text": "Pad\u00f3 dataset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8271811306476593}]}, {"text": "Results are averaged across 20 raters; typical inter-rater agreement is \u03c1 0.).", "labels": [], "entities": []}, {"text": "In all cases correlation with human judgements is computed using Spearman's nonparametric rank correlation (\u03c1) with average human judgements).", "labels": [], "entities": [{"text": "Spearman's nonparametric rank correlation (\u03c1)", "start_pos": 65, "end_pos": 110, "type": "METRIC", "confidence": 0.6774954609572887}]}], "tableCaptions": [{"text": " Table 2: Examples of highly polysemous pairs from each  data set using sense counts from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9812357425689697}]}, {"text": " Table  3. In general the approaches incorporating multiple", "labels": [], "entities": []}, {"text": " Table 5: Spearman's correlation on the Evocation data  set. The high similarity subset contains the top 20% of  pairs sorted by average rater score.", "labels": [], "entities": [{"text": "Evocation data  set", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.9253145058949789}]}, {"text": " Table 6: Spearman's correlation on the Pad\u00f3 data set.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6546617746353149}, {"text": "Pad\u00f3 data set", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9788707494735718}]}]}