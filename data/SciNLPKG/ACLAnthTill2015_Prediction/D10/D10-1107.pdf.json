{"title": [], "abstractContent": [{"text": "Determining whether two terms in text have an ancestor relation (e.g. Toyota and car) or a sibling relation (e.g. Toyota and Honda) is an essential component of textual inference in NLP applications such as Question Answering , Summarization, and Recognizing Textual Entailment.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.8686977326869965}, {"text": "Summarization", "start_pos": 228, "end_pos": 241, "type": "TASK", "confidence": 0.9779744148254395}, {"text": "Recognizing Textual Entailment", "start_pos": 247, "end_pos": 277, "type": "TASK", "confidence": 0.8385415474573771}]}, {"text": "Significant work has been done on developing stationary knowledge sources that could potentially support these tasks, but these resources often suffer from low coverage , noise, and are inflexible when needed to support terms that are not identical to those placed in them, making their use as general purpose background knowledge resources difficult.", "labels": [], "entities": []}, {"text": "In this paper, rather than building a stationary hierarchical structure of terms and relations , we describe a system that, given two terms, determines the taxonomic relation between them using a machine learning-based approach that makes use of existing resources.", "labels": [], "entities": []}, {"text": "Moreover, we develop a global constraint optimization inference process and use it to leverage an existing knowledge base also to enforce relational constraints among terms and thus improve the classifier predictions.", "labels": [], "entities": []}, {"text": "Our experimental evaluation shows that our approach significantly outperforms other systems built upon existing well-known knowledge sources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Taxonomic relations that are read off of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering, navigating text databases (), Question Answering (QA) () and summarization (.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7287992537021637}, {"text": "Question Answering (QA)", "start_pos": 220, "end_pos": 243, "type": "TASK", "confidence": 0.8820728778839111}, {"text": "summarization", "start_pos": 251, "end_pos": 264, "type": "TASK", "confidence": 0.9911078214645386}]}, {"text": "It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) ().", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 138, "end_pos": 174, "type": "TASK", "confidence": 0.7369924088319143}]}, {"text": "For example, it maybe important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars.", "labels": [], "entities": []}, {"text": "Work in Textual Entailment has argued quite convincingly) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7075644135475159}]}, {"text": "To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the) and the YAGO ontology ().", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.972748875617981}, {"text": "YAGO ontology", "start_pos": 307, "end_pos": 320, "type": "DATASET", "confidence": 0.8247500658035278}]}, {"text": "However, identifying when these relations hold using fixed stationary hierarchical structures maybe impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures.", "labels": [], "entities": []}, {"text": "In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as;, there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re-source with broader coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9987921118736267}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9977952241897583}]}, {"text": "In the current work, we take a different approach, identifying directly whether a pair of terms hold a taxonomic relation.", "labels": [], "entities": []}, {"text": "Fixed resources, as we observe, are inflexible when dealing with targeted terms not being covered.", "labels": [], "entities": []}, {"text": "This often happens when targeted terms have the same meaning, but different surface forms, than the terms used in the resources (e.g. Toyota Camry and Camry).", "labels": [], "entities": []}, {"text": "We argue that it is essential to have a classifier that, given two terms, can build a semantic representation of the terms and determines the taxonomic relations between them.", "labels": [], "entities": []}, {"text": "This classifier will make use of existing knowledge bases in multiple ways, but will provide significantly larger coverage and more precise results.", "labels": [], "entities": [{"text": "coverage", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9557651281356812}]}, {"text": "We make use of a dynamic resource such as Wikipedia to guarantee increased coverage without changing our model and also perform normalization-to-Wikipedia to find appropriate Wikipedia replacements for outside-Wikipedia terms.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.9455084800720215}]}, {"text": "Moreover, stationary resources are usually brittle because of the way most of them are built: using local relational patterns (e.g.).", "labels": [], "entities": []}, {"text": "Infrequent terms are less likely to be covered, and some relations may not be supported well by these methods because their corresponding terms rarely appear in close proximity (e.g., an Israeli tennis player Dudi Sela and Roger Federrer).", "labels": [], "entities": []}, {"text": "Our approach uses search techniques to gather relevant Wikipedia pages of input terms and performs a learning-based classification w.r.t. to the features extracted from these pages as away to get around this brittleness.", "labels": [], "entities": []}, {"text": "Motivated by the needs of NLP applications such as RTE, QA, Summarization, and the compositionality argument alluded to above, we focus on identifying two fundamental types of taxonomic relations -ancestor and sibling.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.956628680229187}]}, {"text": "An ancestor relation and its directionality can help us infer that a statement with respect to the child (e.g. cannabis) holds for an ancestor (e.g. drugs) as in the following example, taken from a textual entailment challenge dataset: T: Nigeria's NDLEA has seized 80 metric tonnes of cannabis in one of its largest ever hauls, officials say.", "labels": [], "entities": [{"text": "T", "start_pos": 236, "end_pos": 237, "type": "METRIC", "confidence": 0.9129719138145447}]}], "datasetContent": [{"text": "In this section, we evaluate TAREC against several systems built upon existing well-known knowledge sources.", "labels": [], "entities": [{"text": "TAREC", "start_pos": 29, "end_pos": 34, "type": "TASK", "confidence": 0.8387041091918945}]}, {"text": "The resources are either hierarchical structures or extracted by using distributional semantic models.", "labels": [], "entities": []}, {"text": "We also perform several experimental analyses to understand TAREC's behavior in details.", "labels": [], "entities": [{"text": "TAREC", "start_pos": 60, "end_pos": 65, "type": "TASK", "confidence": 0.7247874736785889}]}, {"text": "In this section, we discuss some experimental analyses to better understand our systems.", "labels": [], "entities": []}, {"text": "Precision and Recall: We want to study TAREC on individual taxonomic relations using Precision and Recall.", "labels": [], "entities": []}, {"text": "We evaluate all systems that use hierarchical structures as background knowledge on three special data sets derived from Test-I. From 12,000 pairs in Test-I, we created a test set, Wiki, consisting of 10, 456 pairs with all terms in Wikipedia.", "labels": [], "entities": []}, {"text": "We use the rest of 1, 544 pairs with at least one non-Wikipedia term to build a non-Wiki test set.", "labels": [], "entities": []}, {"text": "The third dataset, WordNet, contains 8, 625 pairs with all terms in WordNet and Wikipedia.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9757975935935974}, {"text": "WordNet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9496644139289856}]}, {"text": "Table 6 shows the performance of the systems on these data sets.", "labels": [], "entities": []}, {"text": "Unsurprisingly, Yago07 gets better results on Wiki than on Test-I.", "labels": [], "entities": [{"text": "Wiki", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9602046608924866}]}, {"text": "Snow06, as expected, gives better performance on the WordNet test set.", "labels": [], "entities": [{"text": "Snow06", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9563567638397217}, {"text": "WordNet test set", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9826345443725586}]}, {"text": "TAREC still significantly outperforms these systems.", "labels": [], "entities": [{"text": "TAREC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6290110945701599}]}, {"text": "The improvement of TAREC over TAREC (local) on the Wiki and WordNet test sets shows the contribution of the inference model, whereas the improvement on the non-Wikipedia test set shows the contribution of normalizing input terms to Wikipedia.", "labels": [], "entities": [{"text": "TAREC", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9927383065223694}, {"text": "TAREC", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.986600935459137}, {"text": "WordNet test sets", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.8986998399098715}]}, {"text": "Contribution of Related Terms in Inference: We evaluate TAREC when the inference procedure is fed by related terms that are generated using a \"gold standard\" source instead of YAGO.", "labels": [], "entities": [{"text": "TAREC", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.8427099585533142}, {"text": "YAGO", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.7262238264083862}]}, {"text": "To do this, we use the original data which was used to generate Test-I. For each term in the examples of Test-I, we get its ancestors, siblings, and children, if any, from  the original data and use them as related terms in the inference model.", "labels": [], "entities": []}, {"text": "This system is referred as TAREC (Gold Infer.).", "labels": [], "entities": [{"text": "TAREC", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9841281771659851}]}, {"text": "shows the results of the two systems on different K as the number of levels to go upon the Wikipedia category system.", "labels": [], "entities": []}, {"text": "We see that TAREC gets better results when doing inference with better related terms.", "labels": [], "entities": []}, {"text": "In this experiment, the two systems use the same number of related terms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Performance of TAREC on individual taxo- nomic relation.", "labels": [], "entities": [{"text": "TAREC", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.5818513035774231}, {"text": "taxo- nomic relation", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7447429001331329}]}, {"text": " Table 6: Performance of the systems on special data sets,  in accuracy. On the non-Wikipedia test set, TAREC (lo- cal) simply returns sibling relation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9995220899581909}, {"text": "TAREC", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.993829071521759}]}, {"text": " Table 7: Evaluating TAREC with different sources pro- viding related terms to do inference.", "labels": [], "entities": [{"text": "Evaluating TAREC", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6379528194665909}]}]}