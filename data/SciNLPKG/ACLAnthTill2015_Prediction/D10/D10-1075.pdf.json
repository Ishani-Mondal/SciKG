{"title": [{"text": "The Necessity of Combining Adaptation Methods", "labels": [], "entities": []}], "abstractContent": [{"text": "Problems stemming from domain adaptation continue to plague the statistical natural language processing community.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7284799069166183}, {"text": "statistical natural language processing", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.6394732743501663}]}, {"text": "There has been continuing work trying to find general purpose algorithms to alleviate this problem.", "labels": [], "entities": []}, {"text": "In this paper we argue that existing general purpose approaches usually only focus on one of two issues related to the difficulties faced by adaptation: 1) difference in base feature statistics or 2) task differences that can be detected with labeled data.", "labels": [], "entities": []}, {"text": "We argue that it is necessary to combine these two classes of adaptation algorithms, using evidence collected through theoretical analysis and simulated and real-world data experiments.", "labels": [], "entities": []}, {"text": "We find that the combined approach often outperforms the individual adaptation approaches.", "labels": [], "entities": []}, {"text": "By combining simple approaches from each class of adaptation algorithm, we achieve state-of-the-art results for both Named Entity Recognition adaptation task and the Preposition Sense Disambiguation adaptation task.", "labels": [], "entities": [{"text": "Named Entity Recognition adaptation task", "start_pos": 117, "end_pos": 157, "type": "TASK", "confidence": 0.7173364341259003}, {"text": "Preposition Sense Disambiguation adaptation task", "start_pos": 166, "end_pos": 214, "type": "TASK", "confidence": 0.8649637937545777}]}, {"text": "Second, we also show that applying an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "While recent advances in statistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6615066925684611}, {"text": "domain adaptation", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7382938265800476}]}, {"text": "It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain).", "labels": [], "entities": []}, {"text": "The inability of current statistical models to handle multiple domains is one of the key obstacles hindering the progress of NLP.", "labels": [], "entities": []}, {"text": "Several general purpose algorithms have been proposed to address the domain adaptation problem:.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7472904920578003}]}, {"text": "It is widely believed that the drop in performance of statistical models on new domains is due to the shift of the joint distribution of labels and examples, P (Y, X), from domain to domain, where X represents the input space and Y represents the output space.", "labels": [], "entities": []}, {"text": "In general, we can separate existing adaptation algorithms into two categories: Focuses on P (X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains.", "labels": [], "entities": []}, {"text": "While many different techniques have been proposed, the common goal of these algorithms is to find a better shared representation that brings the source domain and the target domain closer.", "labels": [], "entities": []}, {"text": "Often these algorithms do not use labeled examples in the target domain.", "labels": [], "entities": []}, {"text": "The works) all belong to this category.", "labels": [], "entities": []}, {"text": "Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain.", "labels": [], "entities": []}, {"text": "Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors.", "labels": [], "entities": []}, {"text": "This is often achieved by using a special designed regularization term.", "labels": [], "entities": []}, {"text": "The works ( belong to this category.", "labels": [], "entities": []}, {"text": "It is important to give the definition of an adaptation framework.", "labels": [], "entities": []}, {"text": "An adaptation framework is specified by the data/resources used and a specific learning algorithm.", "labels": [], "entities": []}, {"text": "For example, a framework that used only source labeled examples and one that used both source and target labeled examples should be considered as two different frameworks, even though they might use exactly the same training algorithm.", "labels": [], "entities": []}, {"text": "Note that the goal of a good adaptation framework is to perform well on the target domain and quite often we only need to change the data/resource used to increase the performance without changing the training algorithm.", "labels": [], "entities": []}, {"text": "We refer to frameworks that do not use target labeled data and focus on P (X) as Unlabeled Adaptation Frameworks and refer to frameworks that use algorithms that focus on P (Y |X) as Labeled Adaptation Frameworks.", "labels": [], "entities": []}, {"text": "The major difference between unlabeled adaptation frameworks and labeled adaptation frameworks is the use of target labeled examples.", "labels": [], "entities": []}, {"text": "Unlabeled adaptation frameworks do not use target labeled examples 1 , while the labeled adaptation frameworks make use of target labeled examples.", "labels": [], "entities": []}, {"text": "Under this definition, we consider that a model trained on both source and target labeled examples (later referred as S+T) is a labeled adaptation framework.", "labels": [], "entities": []}, {"text": "It is important to combine the labeled and unlabeled adaptation frameworks for two reasons: \u2022 Mutual Benefit: We analyze these two types of frameworks and find that they address different adaptation issues.", "labels": [], "entities": []}, {"text": "Therefore, it is often beneficial to apply them together.", "labels": [], "entities": []}, {"text": "\u2022 Complex Interaction: Another, probably more important issue, is that these two types of frameworks are not independent.", "labels": [], "entities": []}, {"text": "Different representations will impact how much a labeled adaptation algorithm can transfer information between domains.", "labels": [], "entities": []}, {"text": "Therefore, in order to have a clear picture of what is the best labeled adaptation framework, it is necessary to analyze these two domain adaptation frameworks together.", "labels": [], "entities": []}, {"text": "In this paper, we assume we have both a small amount of target labeled data and a large amount Note that we still use labeled data from source domain in an unlabeled adaptation framework. of unlabeled data so that we can perform both unlabeled and labeled adaptation.", "labels": [], "entities": []}, {"text": "The goal of our paper is to point out the necessity of applying these two adaptation frameworks together.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first paper that both theoretically and empirically analyzes the interdependence between the impact of labeled and unlabeled adaptation frameworks.", "labels": [], "entities": []}, {"text": "The contribution of this paper is as follows: \u2022 Propose a theoretical analysis of the \"Frustratingly Easy\" (FE) framework (Daum\u00e9 III, 2007) (Section 3).", "labels": [], "entities": [{"text": "Frustratingly Easy\" (FE) framework (Daum\u00e9 III, 2007)", "start_pos": 87, "end_pos": 139, "type": "DATASET", "confidence": 0.5854737850335928}]}, {"text": "The theoretical analysis shows that for FE to be effective the domains must already be \"close\".", "labels": [], "entities": [{"text": "FE", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.6877753734588623}]}, {"text": "At some threshold of \"closeness\" it is better to switch from FE to just pool all training together as one domain.", "labels": [], "entities": [{"text": "FE", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.8912585377693176}]}, {"text": "\u2022 Demonstrate the complex interaction between unlabeled and labeled approaches (Section 4) We construct artificial experiments that demonstrate how applying unlabeled adaptation may impact the behavior of two labeled adaptation approaches.", "labels": [], "entities": []}, {"text": "\u2022 Empirically analyze the interaction on real datasets (Section 5).", "labels": [], "entities": []}, {"text": "We show that in general combining both approaches on the tasks of preposition sense disambiguation and named entity recognition works better than either individual method.", "labels": [], "entities": [{"text": "preposition sense disambiguation", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.7124712268511454}, {"text": "named entity recognition", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.6434067289034525}]}, {"text": "Our approach not only achieves state-of-theart results on these two tasks but it also reveals something surprising -finding a better shared representation often makes a simple source+target approach the best adaptation framework in practice.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we will present artificial experiments.", "labels": [], "entities": []}, {"text": "We have two primary goals: 1) verifying the analysis proposed in Section 3, and 2) showing that the representation shift will impact the behavior of the FE algorithm.", "labels": [], "entities": [{"text": "FE algorithm", "start_pos": 153, "end_pos": 165, "type": "TASK", "confidence": 0.5317791402339935}]}, {"text": "The second point will be verified again in the real world experiments in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.8315043747425079}]}, {"text": "Data Generation In the following artificial experiments we experiment with domain adaptation by generating training and test data for two tasks, source and target, where we can control the difference between task definitions.", "labels": [], "entities": [{"text": "Data Generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6601045578718185}, {"text": "domain adaptation", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7297034710645676}]}, {"text": "The general procedure can be divided into two steps: 1) generating weight vectors z 1 and z 2 (for source and target respectively), and 2) randomly generating labeled instances for training and testing using z 1 and z 2 . The different experiments start with the same basic z 1 and z 2 , but then may alter these weights to introduce task dissimilarities or similarities.", "labels": [], "entities": []}, {"text": "The basic z 1 and z 2 are both generated by a multivariate Gaussian distribution with mean z and a diagonal covariance matrix \u03b2I: where N is the normal distribution and z is random vector with zero mean.", "labels": [], "entities": [{"text": "diagonal covariance matrix \u03b2I", "start_pos": 99, "end_pos": 128, "type": "METRIC", "confidence": 0.6721611768007278}]}, {"text": "Note that z is only used to generate z 1 and z 2 . There is one parameter, \u03b2, that controls the variance of the Gaussian distribution.", "labels": [], "entities": []}, {"text": "Hence we use \u03b2 to roughly control the \"angle\" of z 1 and z 2 . When \u03b2 is close to zero, z 1 and z 2 will be very similar.", "labels": [], "entities": []}, {"text": "On the other hand, when \u03b2 is large, z 1 and z 2 can be very different.", "labels": [], "entities": []}, {"text": "In these experiments, we vary \u03b2 between 0.01 and 5 so that we are experimenting only with tasks where the weight the task difference is the \"angle\" or cosine between z 1 and z 2 . Once we obtain the z 1 and z 2 , we normalize them to the unit length.", "labels": [], "entities": []}, {"text": "After selecting z 1 and z 2 , we then generate labeled instances (x, y) for the source task in the following way.", "labels": [], "entities": []}, {"text": "For each example x, we randomly generate n binary features, where each feature has 20% chance to be active.", "labels": [], "entities": []}, {"text": "We then label the example by y = sign(z T 1 x), The data for the target task is generated similarly with z 2 . In these experiments, we fix the number of features n to be 500 and generate 100 source training examples and 40 target training examples, along with 1000 target testing examples.", "labels": [], "entities": []}, {"text": "This matches the reasonable casein NLP where there are more features than training examples and each feature vector is sparse.", "labels": [], "entities": []}, {"text": "In all of the experiments, we report the averaged testing error rate on the target testing data.", "labels": [], "entities": [{"text": "testing error rate", "start_pos": 50, "end_pos": 68, "type": "METRIC", "confidence": 0.6527511278788248}]}, {"text": "Goal The goal here is to verify our theoretical analysis in Section 3.", "labels": [], "entities": []}, {"text": "Note that we do not introduce representation shift in this experiment and assume that both source and target domains use exactly the same features.", "labels": [], "entities": []}, {"text": "Result shows the performance of the three training algorithms as variance decreases and thus cosine between weight vectors (or measure of task similarity) goes to 1.", "labels": [], "entities": []}, {"text": "Note that FE labeled adaptation framework beats TGT once the task cosine passes approximately 0.6.", "labels": [], "entities": [{"text": "FE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.8546932339668274}]}, {"text": "Initially FE slightly outperforms S+T until the tasks are close enough together that it is better to treat all the data as coming from one task.", "labels": [], "entities": [{"text": "FE", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9950520396232605}]}, {"text": "Note that while the experiments are based on the adaptation setting, the results match our analysis based on the multitask setting in Section 3.", "labels": [], "entities": []}, {"text": "Goal So far we have not considered the difference in P (X) between domains.", "labels": [], "entities": []}, {"text": "In the previous experiment, we used only cosine as our task similarity measurement to decide what is the best framework.", "labels": [], "entities": []}, {"text": "However, task similarity should consider the difference in both P (X) and P (Y |X), and the cosine measurement is not sufficient for this.", "labels": [], "entities": []}, {"text": "Here we construct a simple example to show that even a simple representation shift can change the behavior of the labeled adaptation framework.", "labels": [], "entities": []}, {"text": "This case shows that S+T can be better than FE even when the tasks are not similar according to the cosine measurement.", "labels": [], "entities": [{"text": "FE", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.997584342956543}]}, {"text": "Result The second experiment deals with the case where features may appear in only one domain but should be treated like known features in the other domain.", "labels": [], "entities": [{"text": "Result", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9338558912277222}]}, {"text": "An example of this are out of vocabulary words that may not exist in a small target training task, but have synonyms in the source training data.", "labels": [], "entities": []}, {"text": "In this case if we had features grouping words (say byword meanings) then we would recover this cross-domain information.", "labels": [], "entities": []}, {"text": "In this experiment we want to explore which adaptation algorithm performs best before these features are applied.", "labels": [], "entities": []}, {"text": "To simulate this case we start with similar weight vectors z 1 and z 2 (sampled with variance = 0.00001, cos(z 1 ,z 2 ) \u2248 1), but then shift some set of dimensions so that they represent features that appear only in one domain.", "labels": [], "entities": []}, {"text": "By changing the ratio of the size of the dissimilar subset a to the similar subset b we can make the two weight vectors z 1 and z 2 more or less similar.", "labels": [], "entities": []}, {"text": "Using these two new weight vectors we can proceed as above, generating training and testing data.", "labels": [], "entities": []}, {"text": "shows the performance of the three algo- rithms on this data as the number of unrelated features are decreased.", "labels": [], "entities": []}, {"text": "Over the entire range the combined algorithm S+T does better since it more efficiently exploits the shared similar b subset of the feature space.", "labels": [], "entities": []}, {"text": "When the FE algorithm tries to create the shared features, it considers both the similar subset band dissimilar subset a.", "labels": [], "entities": [{"text": "FE", "start_pos": 9, "end_pos": 11, "type": "DATASET", "confidence": 0.5470605492591858}]}, {"text": "However, since a should not be shared, FE algorithm becomes less effective than the S+T algorithm.", "labels": [], "entities": [{"text": "FE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9928463101387024}]}, {"text": "See the bound comparison in Section 3.2 for more intuitions.", "labels": [], "entities": []}, {"text": "With this experiment we have demonstrated that there is a need to consider label and unlabeled adaptation frameworks together.", "labels": [], "entities": []}, {"text": "Goal A good unlabeled adaptation framework should try to find features that \"work\" across domains.", "labels": [], "entities": []}, {"text": "However, it is not clear how these newly added features will impact the behavior of the labeled adaptation frameworks.", "labels": [], "entities": []}, {"text": "In this experiment, we show that the new shared features will bring the domains together, and hence make S+T a very strong adaptation framework.", "labels": [], "entities": []}, {"text": "Result For the third experiment we start with the same setup as in the first experiment, but then augment the initial weight vector with additional shared weights.", "labels": [], "entities": []}, {"text": "These shared weights correspond to the introduction of features that appear in both domains and have the same meaning relative to the tasks, the ideal result of unlabeled adaptation methods.", "labels": [], "entities": []}, {"text": "To generate this case we again start with z 1 and z 2 of varying similarity as in section 4.1, then generate a random weight vector for shared features and append this to both weight vectors.", "labels": [], "entities": []}, {"text": "where \u03b3 is used to put increased importance on the shared weight vectors by increasing the total weight of that section relative to the base z 1 and z 2 subsets.", "labels": [], "entities": []}, {"text": "In our experiments we use 100 shared features to the 500 base features and set \u03b3 to 2.", "labels": [], "entities": []}, {"text": "shows the performance of the labeled adaptation algorithms once shared features had been added.", "labels": [], "entities": []}, {"text": "Here the x-axis is the cosine between the original task weight vectors, demonstrating how the shared features improve performance on potentially dissimilar tasks.", "labels": [], "entities": []}, {"text": "Whereas in the first experiment FE does not improve over just training on target data until the cosine is greater than 0.6, once shared features have been added then both FE and S+T use these features to learn with originally dissimilar tasks.", "labels": [], "entities": [{"text": "FE", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9942858815193176}, {"text": "FE", "start_pos": 171, "end_pos": 173, "type": "METRIC", "confidence": 0.7877651453018188}]}, {"text": "Furthermore the shared features tend to push the tasks 'closer' so that S+T improves over FE earlier.", "labels": [], "entities": [{"text": "FE", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9170092344284058}]}, {"text": "Comparing to, there are regions where before shared features are added it is better to use FE, and after shared features are added it is better to use S+T.", "labels": [], "entities": [{"text": "FE", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.8671188354492188}]}, {"text": "This shows that labeled adaptation and unlabeled are not independent.", "labels": [], "entities": []}, {"text": "Therefore, it is important to combine these two aspects to seethe real contribution of each adaptation framework.", "labels": [], "entities": []}, {"text": "In these three artificial experiments we have demonstrated cases where both FE or S+T are the best algorithm before and after representation changes like those created with unlabeled adaptation are imposed.", "labels": [], "entities": [{"text": "FE", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9846306443214417}]}, {"text": "This fact points to the perhaps obvious conclusion that there is not a single best adaptation algorithm, and the determination of specific best practices depends on task similarity (in both P (X) and P (Y |X)), especially after being brought closer together with other adaptation approaches.", "labels": [], "entities": []}, {"text": "If there is one common trend it is that often once two tasks have been brought close together using a shared representation, then the tasks are now close enough such that the simple S+T algorithm does well.", "labels": [], "entities": []}, {"text": "In Section 4, we have shown through artificial data experiments that labeled and unlabeled adaptation algorithms are not independent.", "labels": [], "entities": []}, {"text": "In this section, we focus on experiments with real datasets.", "labels": [], "entities": []}, {"text": "For the labeled adaptation algorithms, we have the following options: \u2022 TGT: Only uses target labeled training dataset.", "labels": [], "entities": [{"text": "labeled adaptation", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7159985154867172}, {"text": "TGT", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9290509223937988}]}, {"text": "\u2022 FE: Uses both labeled datasets.", "labels": [], "entities": [{"text": "FE", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9974144697189331}]}, {"text": "\u2022 FE + : Uses both labeled datasets.", "labels": [], "entities": [{"text": "FE", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.996641993522644}]}, {"text": "A modification of the FE algorithm, equivalent to multiplying the \"shared\" part of the FE feature vector (Eq.) by 10 ().", "labels": [], "entities": [{"text": "FE", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.4249098598957062}, {"text": "FE feature vector", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.6466878652572632}]}, {"text": "\u2022 S+T: Uses both source and target labeled datasets to train a single model with all labeled data directly.", "labels": [], "entities": []}, {"text": "Throughout all of our experiments, we use SVMs trained with a modified java implementation 8 of LIBLINEAR as our underlying learning classifier (: NER Experiments.", "labels": [], "entities": []}, {"text": "We boldface the best accuracy in a row and underline the runner up.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9990737438201904}]}, {"text": "Both unlabeled adaptation algorithms (adding cluster features) and labeled adaptation algorithm (using source labeled data) help the performance significantly.", "labels": [], "entities": []}, {"text": "Moreover, adding cluster-like features also changes the behavior of the labeled adaptation algorithms.", "labels": [], "entities": []}, {"text": "Note that after adding cluster features, S+T becomes quite competitive with (or slightly better than) the FE + approach.", "labels": [], "entities": [{"text": "FE", "start_pos": 106, "end_pos": 108, "type": "DATASET", "confidence": 0.6006991267204285}]}, {"text": "The size of MUC7 develop set is roughly 20% of the size of the MUC7 training set.", "labels": [], "entities": [{"text": "MUC7 develop set", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.783622940381368}, {"text": "MUC7 training set", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.9243975281715393}]}, {"text": "a local SVM classifier then make our prediction using a greedy approach from left to right.", "labels": [], "entities": []}, {"text": "While we could use a more complex model such as Conditional Random Field (), as we will see later, our simple model generates state-ofthe-art results for many tasks.", "labels": [], "entities": []}, {"text": "Regarding parameter selection, we selected the SVM regularization parameter for the baseline model (TGT) and then fix it for all algorithms 9 . Named Entity Recognition Our first task is Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.6571557323137919}, {"text": "Named Entity Recognition (NER)", "start_pos": 187, "end_pos": 217, "type": "TASK", "confidence": 0.7792909244696299}]}, {"text": "The source domain is from the CoNLL03 shared task) and the target domain is from the MUC7 dataset.", "labels": [], "entities": [{"text": "CoNLL03 shared task", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.8012433648109436}, {"text": "MUC7 dataset", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9764846861362457}]}, {"text": "The goal of this adaptation system is to maximize the performance on the test data of MUC7 dataset with CoNLL training data and (some) MUC7 labeled data.", "labels": [], "entities": [{"text": "MUC7 dataset", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.8975838124752045}, {"text": "CoNLL training data", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.8510816097259521}, {"text": "MUC7 labeled data", "start_pos": 135, "end_pos": 152, "type": "DATASET", "confidence": 0.7222195466359457}]}, {"text": "As an unlabeled adaptation method to address feature sparsity, we add cluster-like features based on the gazetteers and word clustering resources used in) to bridge the source and target domain.", "labels": [], "entities": []}, {"text": "We experiment with both MUC development and training set as our target labeled sets.", "labels": [], "entities": [{"text": "MUC development", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.8994081616401672}]}, {"text": "The experimental results are in.", "labels": [], "entities": []}, {"text": "First, notice that addressing the feature sparsity issue helps the performance significantly.", "labels": [], "entities": []}, {"text": "Adding cluster-like We use L2-hinge loss for all of the experiments, with C = 2 \u22124 for NER experiments and C = 2 \u22125 for the PSD experiments.", "labels": [], "entities": []}, {"text": "features improves the Token-F1 by around 10%.", "labels": [], "entities": [{"text": "Token-F1", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8038066029548645}]}, {"text": "On the other hand, adding target labeled data also helps the results significantly.", "labels": [], "entities": []}, {"text": "Moreover, using both target labeled data and cluster-like shared representation are mutually beneficial in all cases.", "labels": [], "entities": []}, {"text": "Importantly, adding cluster-like features changes the behavior of the labeled adaptation algorithms.", "labels": [], "entities": []}, {"text": "When the cluster-like features are not added, the FE + algorithm is in general the best labeled adaptation framework.", "labels": [], "entities": [{"text": "FE", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.43610456585884094}]}, {"text": "This result agrees with the results showed in (, where the authors show that FE + is the best labeled adaptation framework in their settings.", "labels": [], "entities": [{"text": "FE", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.6771337985992432}]}, {"text": "However, after adding the cluster-like features, the simple S+T approach becomes very competitive to both FE and FE + . This matches our analysis in Section 4: resolving features sparsity will change the behavior of labeled adaptation frameworks.", "labels": [], "entities": [{"text": "FE", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.6237170100212097}, {"text": "FE", "start_pos": 113, "end_pos": 115, "type": "DATASET", "confidence": 0.6502077579498291}]}, {"text": "We compare the simple S+T algorithm with cluster-like features to other published results on adapting from CoNLL dataset to MUC7 dataset in table 3.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.967659592628479}, {"text": "MUC7 dataset", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.890050083398819}]}, {"text": "Past works on this setting often only focus on one class of adaption approach.", "labels": [], "entities": []}, {"text": "For example,) only use the cluster-like features to address the feature sparsity problem, and) only use target labeled data without using gazetteers and word-cluster information.", "labels": [], "entities": []}, {"text": "Notice that because of combining two classes of adaption algorithms, our approach is significantly better than these two systems . Preposition Sense Disambiguation We also test the combination of unlabeled and labeled adaption on the task of Preposition Sense Disambiguation.", "labels": [], "entities": [{"text": "Preposition Sense Disambiguation", "start_pos": 131, "end_pos": 163, "type": "TASK", "confidence": 0.6811189850171407}, {"text": "Preposition Sense Disambiguation", "start_pos": 242, "end_pos": 274, "type": "TASK", "confidence": 0.7758716344833374}]}, {"text": "Here the data contains multiple prepositions where each preposition has many different senses.", "labels": [], "entities": []}, {"text": "The goal is to predict the right sense fora given preposition in the testing data.", "labels": [], "entities": []}, {"text": "The source domain is the SemEval 2007 preposition WSD Task and the target domain is from the dataset annotated in (: Preposition Sense Disambiguation.", "labels": [], "entities": [{"text": "SemEval 2007 preposition WSD Task", "start_pos": 25, "end_pos": 58, "type": "DATASET", "confidence": 0.6886159121990204}]}, {"text": "We mark the best accuracy in a row using the bold font and underline the runner up.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9991297125816345}]}, {"text": "Note that both adding cluster features and adding source labeled data help the performance significantly.", "labels": [], "entities": []}, {"text": "Moreover, adding clusters also changes the behavior of the labeled adaptation algorithms.", "labels": [], "entities": []}, {"text": "labeled adaptation approach we augment all word based features with cluster information from separately generated hierarchical Brown clusters.", "labels": [], "entities": []}, {"text": "The experimental results are in.", "labels": [], "entities": []}, {"text": "Note that we see phenomena similar to what happened in the NER experiments.", "labels": [], "entities": [{"text": "NER", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.6573528051376343}]}, {"text": "First, both labeled and unlabeled adaptation improves the system.", "labels": [], "entities": []}, {"text": "When only 10% of the target labeled data is used, the inclusion of the source labeled data helps significantly.", "labels": [], "entities": []}, {"text": "When there is more labeled data, labeled and unlabeled adaption have similar impact.", "labels": [], "entities": []}, {"text": "Again, using unlabeled adaption changes the behavior of the labeled adaption algorithms.", "labels": [], "entities": []}, {"text": "In, we compare our system to (), who do not use the SemEval data but jointly train their preposition sense disambiguation system with a semantic role labeling system.", "labels": [], "entities": [{"text": "SemEval data", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.7004518508911133}]}, {"text": "With both labeled and unlabeled adaption, our system is significantly better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9965838193893433}, {"text": "MUC7 develop set", "start_pos": 502, "end_pos": 518, "type": "DATASET", "confidence": 0.8378918170928955}, {"text": "MUC7  training set", "start_pos": 553, "end_pos": 571, "type": "DATASET", "confidence": 0.9534270564715067}]}, {"text": " Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use \"Cluster?\" to indicate if cluster features  are used and use \"TGT?\" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.", "labels": [], "entities": [{"text": "token-level F1 score", "start_pos": 99, "end_pos": 119, "type": "METRIC", "confidence": 0.7111107707023621}]}, {"text": " Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.", "labels": [], "entities": [{"text": "Preposition Sense Disambiguation", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.647201806306839}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9958261847496033}]}]}