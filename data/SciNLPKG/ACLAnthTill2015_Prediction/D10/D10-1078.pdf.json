{"title": [{"text": "Latent-Descriptor Clustering for Unsupervised POS Induction", "labels": [], "entities": [{"text": "POS Induction", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.6930657625198364}]}], "abstractContent": [{"text": "We present a novel approach to distributional-only, fully unsupervised, POS tagging, based on an adaptation of the EM algorithm for the estimation of a Gaussian mixture.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.8759030997753143}]}, {"text": "In this approach, which we call Latent-Descriptor Clustering (LDC), word types are clustered using a series of progressively more informative descriptor vectors.", "labels": [], "entities": []}, {"text": "These descriptors, which are computed from the immediate left and right context of each word in the corpus, are updated based on the previous state of the cluster assignments.", "labels": [], "entities": []}, {"text": "The LDC algorithm is simple and intuitive.", "labels": [], "entities": []}, {"text": "Using standard evaluation criteria for unsupervised POS tagging, LDC shows a substantial improvement in performance over state-of-the-art methods, along with a several-fold reduction in computational cost.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8277491629123688}]}], "introductionContent": [{"text": "Part-of-speech (POS) tagging is a fundamental natural-language-processing problem, and POS tags are used as input to many important applications.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.584871768951416}]}, {"text": "While state-of-the-art supervised POS taggers are more than 97% accurate (), unsupervised POS taggers continue to lag far behind.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8478784263134003}, {"text": "accurate", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9850627183914185}, {"text": "POS taggers", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.8429606258869171}]}, {"text": "Several authors addressed this gap using limited supervision, such as a dictionary of tags for each word, or a list of word prototypes for each tag).", "labels": [], "entities": []}, {"text": "Even in light of all these advancements, there is still interest in a completely unsupervised method for POS induction for several reasons.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9794841706752777}]}, {"text": "First, most languages do not have a tag dictionary.", "labels": [], "entities": []}, {"text": "Second, the preparation of such resources is error-prone.", "labels": [], "entities": []}, {"text": "Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language.", "labels": [], "entities": []}, {"text": "Since tags are used as basic features for many important NLP applications (e.g.), exploring new, statistically motivated, tag sets may also be useful.", "labels": [], "entities": []}, {"text": "For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value.", "labels": [], "entities": []}, {"text": "In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.7508425116539001}]}, {"text": "Some of these methods use morphological cues, but all rely heavily on distributional information, i.e., bi-gram statistics.", "labels": [], "entities": []}, {"text": "Two recent papers advocate nondisambiguating models (: these assign the same tag to all tokens of a word type, rather than attempting to disambiguate words in context.", "labels": [], "entities": []}, {"text": "motivate this choice by showing how removing the disambiguation ability from a state-of-the-art disambiguating model results in increasing its accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9974414110183716}]}, {"text": "In this paper, we present a novel approach to non-disambiguating, distributional-only, fully unsupervised, POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.9017868340015411}]}, {"text": "As in all nondisambiguating distributional approaches, the goal, loosely stated, is to assign the same tag to words whose contexts in the corpus are similar.", "labels": [], "entities": []}, {"text": "Our approach, which we call Latent-Descriptor Clustering, or LDC, is an iterative algorithm, in the spirit of the K-means clustering algorithm and of the EM algorithm for the estimation of a mixture of Gaussians.", "labels": [], "entities": []}, {"text": "In conventional K-means clustering, one is given a collection of N objects described as N data points in an r-dimensional Euclidean space, and one seeks a clustering that minimizes the sum of intra-cluster squared distances, i.e., the sum, overall data points, of the squared distance between that point and the centroid of its assigned cluster.", "labels": [], "entities": []}, {"text": "In LDC, we similarly state our goal as one of finding a tagging, i.e., cluster assignment, A, that minimizes the sum of intracluster squared distances.", "labels": [], "entities": []}, {"text": "However, unlike in conventional K-means, the N objects to be clustered are themselves described by vectors-in a suitable manifold-that depend on the clustering A.", "labels": [], "entities": []}, {"text": "We call these vectors latent descriptors.", "labels": [], "entities": []}, {"text": "Specifically, each object to be clustered, i.e., each word type w, is described in terms of its left-tag context and right-tag context.", "labels": [], "entities": []}, {"text": "These context vectors are the counts of the K different tags occurring, under tagging A, to the left and right of tokens of word type win the corpus.", "labels": [], "entities": []}, {"text": "We normalize each of these context vectors to unit length, producing, for each word type w, two points LA (w) and R A (w) on the (K-1)-dimensional unit sphere.", "labels": [], "entities": [{"text": "LA", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9378138780593872}]}, {"text": "The latent descriptor for w consists of the pair (L A (w), R A (w))-more details in Section 2.", "labels": [], "entities": []}, {"text": "A straightforward approach to this latentdescriptor K-means problem is to adapt the classical iterative K-means algorithm so as to handle the latent descriptors.", "labels": [], "entities": []}, {"text": "Specifically, in each iteration, given the assignment A obtained from the previous iteration, one first computes the latent descriptors for all word types as defined above, and then proceeds in the usual way to update cluster centroids and to find anew assignment A to be used in the next iteration.", "labels": [], "entities": []}, {"text": "For reasons to be discussed in Section 5, we instead prefer a soft-assignment strategy, inspired from the EM algorithm for the estimation of a mixture of Gaussians.", "labels": [], "entities": []}, {"text": "Thus, rather than the hard assignment A, we use a soft-assignment matrix P.", "labels": [], "entities": []}, {"text": "P wk , interpreted as the probability of assigning word w to cluster k, is, essentially, proportional to exp{-d wk 2 /2\u03c3 2 }, where d wk is the distance between the latent descriptor for wand the centroid, i.e., Gaussian mean, fork.", "labels": [], "entities": []}, {"text": "Unlike the Gaussian-mixture model however, we use the same mixture coefficient and the same Gaussian width for all k.", "labels": [], "entities": [{"text": "Gaussian width", "start_pos": 92, "end_pos": 106, "type": "METRIC", "confidence": 0.8117812275886536}]}, {"text": "Further, we let the Gaussian width \u03c3\uf020decrease gradually during the iterative process.", "labels": [], "entities": [{"text": "Gaussian width \u03c3", "start_pos": 20, "end_pos": 36, "type": "METRIC", "confidence": 0.9097883701324463}]}, {"text": "As is well-known, the EM algorithm for Gaussian mixtures reduces in the limit of small \u03c3 to the simpler K-means clustering algorithm.", "labels": [], "entities": []}, {"text": "As a result, the last few iterations of LDC effectively implement the hard-assignment K-meansstyle algorithm outlined in the previous paragraph.", "labels": [], "entities": []}, {"text": "The soft assignment used earlier in the process lends robustness to the algorithm.", "labels": [], "entities": []}, {"text": "The LDC approach is shown to yield substantial improvement over state-of-the-art methods for the problem of fully unsupervised, distributional only, POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.7755506336688995}]}, {"text": "The algorithm is conceptually simple and easy to implement, requiring less than 30 lines of Matlab code.", "labels": [], "entities": [{"text": "Matlab code", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9427611827850342}]}, {"text": "It runs in a few seconds of computation time, as opposed to hours or days for the training of HMMs.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Tagging accuracy comparison between  several models for two tagsets and two mapping  criteria. Note that LDC significantly outperforms  all HMMs (", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9142465591430664}]}]}