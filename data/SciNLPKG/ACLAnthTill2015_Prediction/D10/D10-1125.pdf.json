{"title": [{"text": "Dual Decomposition for Parsing with Non-Projective Head Automata", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces algorithms for non-projective parsing based on dual decomposition.", "labels": [], "entities": []}, {"text": "We focus on parsing algorithms for non-projective head automata, a generalization of head-automata models to non-projective structures.", "labels": [], "entities": []}, {"text": "The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms.", "labels": [], "entities": []}, {"text": "They provably solve an LP relaxation of the non-projective parsing problem.", "labels": [], "entities": []}, {"text": "Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences.", "labels": [], "entities": [{"text": "LP relaxation", "start_pos": 16, "end_pos": 29, "type": "METRIC", "confidence": 0.9648815095424652}]}, {"text": "The accuracy of our models is higher than previous work on abroad range of datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996684789657593}]}], "introductionContent": [{"text": "Non-projective dependency parsing is useful for many languages that exhibit non-projective syntactic structures.", "labels": [], "entities": [{"text": "Non-projective dependency parsing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6582688788572947}]}, {"text": "Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models.", "labels": [], "entities": []}, {"text": "There has been along history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation).", "labels": [], "entities": [{"text": "combinatorial optimization", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7553357779979706}, {"text": "dual decomposition", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7816052436828613}]}, {"text": "Thus far, however, these methods are not widely used in NLP.", "labels": [], "entities": []}, {"text": "This paper introduces algorithms for nonprojective parsing based on dual decomposition.", "labels": [], "entities": []}, {"text": "We focus on parsing algorithms for non-projective head automata, a generalization of the head-automata models of and to nonprojective structures.", "labels": [], "entities": []}, {"text": "These models include nonprojective dependency parsing models with higherorder (e.g., sibling and/or grandparent) dependency relations as a special case.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7236064374446869}]}, {"text": "Although decoding of full parse structures with non-projective head automata is intractable, we leverage the observation that key components of the decoding can be efficiently computed using combinatorial algorithms.", "labels": [], "entities": []}, {"text": "In particular, 1. Decoding for individual head-words can be accomplished using dynamic programming.", "labels": [], "entities": []}, {"text": "2. Decoding for arc-factored models can be accomplished using directed minimum-weight spanning tree (MST) algorithms.", "labels": [], "entities": []}, {"text": "The resulting parsing algorithms have the following properties: \u2022 They are efficient and easy to implement, relying on standard dynamic programming and MST algorithms.", "labels": [], "entities": []}, {"text": "\u2022 They provably solve a linear programming (LP) relaxation of the original decoding problem.", "labels": [], "entities": []}, {"text": "\u2022 Empirically the algorithms very often give an exact solution to the decoding problem, in which case they also provide a certificate of optimality.", "labels": [], "entities": []}, {"text": "In this paper we first give the definition for nonprojective head automata, and describe the parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 93, "end_pos": 100, "type": "TASK", "confidence": 0.9723004698753357}]}, {"text": "The algorithm can be viewed as an instance of Lagrangian relaxation; we describe this connection, and give convergence guarantees for the method.", "labels": [], "entities": [{"text": "convergence", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.9910724759101868}]}, {"text": "We describe a generalization to models that include grandparent dependencies.", "labels": [], "entities": []}, {"text": "We then introduce a perceptron-driven training algorithm that makes use of point 1 above.", "labels": [], "entities": []}, {"text": "We describe experiments on non-projective parsing fora number of languages, and in particular compare the dual decomposition algorithm to approaches based on general-purpose linear programming (LP) or integer linear programming (ILP) solvers).", "labels": [], "entities": []}, {"text": "The accuracy of our models is higher than previous work on abroad range of datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996684789657593}]}, {"text": "The method gives exact solutions to the decoding problem, together with a certificate of optimality, on over 98% of test examples for many of the test languages, with parsing times ranging between 0.021 seconds/sentence for the most simple languages/models, to 0.295 seconds/sentence for the most complex settings.", "labels": [], "entities": []}, {"text": "The method compares favorably to previous work using LP/ILP formulations, both in terms of efficiency, and also in terms of the percentage of exact solutions returned.", "labels": [], "entities": []}, {"text": "While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG), CCG, and projective head automata.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7334962487220764}]}], "datasetContent": [{"text": "We report results on a number of data sets.", "labels": [], "entities": []}, {"text": "For comparison to, we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task), and English data from the CoNLL-2008 shared task.", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 116, "end_pos": 135, "type": "DATASET", "confidence": 0.8509181340535482}, {"text": "CoNLL-2008 shared task", "start_pos": 164, "end_pos": 186, "type": "DATASET", "confidence": 0.8758331934611002}]}, {"text": "We use the official training/test splits for these data sets, and the same evaluation methodology as.", "labels": [], "entities": []}, {"text": "For comparison to, we also report results on Danish and Dutch using their alternate training/test split.", "labels": [], "entities": []}, {"text": "Finally, we report results on the English WSJ treebank, and the Prague treebank.", "labels": [], "entities": [{"text": "English WSJ treebank", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8229897816975912}, {"text": "Prague treebank", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9931420087814331}]}, {"text": "We use feature sets that are very similar to those described in.", "labels": [], "entities": []}, {"text": "We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem.", "labels": [], "entities": []}, {"text": "In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in, to be 5,000.", "labels": [], "entities": []}, {"text": "If the algorithm does not terminate-i.e., it does not return (y (k) , z (k) ) within 5,000 iterations-we simply take the parse y (k) with the maximum value off (y (k) ) as the output from the algorithm.", "labels": [], "entities": []}, {"text": "At first sight 5,000 might appear to be a large number, but decoding is still fast-see Sections 7.3 and 7.4 for discussion.", "labels": [], "entities": []}, {"text": "2 Note also that the feature vectors \u03c6 and inner products w \u00b7\u03c6 The strategy for choosing step sizes \u03b1 k is described in Appendix A, along with other details.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.8619280457496643}]}, {"text": "We first discuss performance in terms of accuracy, success in recovering an exact solution, and parsing speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9991405010223389}, {"text": "parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9712845683097839}]}, {"text": "We then describe additional experiments examining various aspects of the algorithm.", "labels": [], "entities": []}, {"text": "shows results for previous work on the various data sets, and results for an arc-factored model with pure MST decoding with our features.", "labels": [], "entities": []}, {"text": "(We use the acronym UAS (unlabeled attachment score) for dependency accuracy.)", "labels": [], "entities": [{"text": "UAS (unlabeled attachment score)", "start_pos": 20, "end_pos": 52, "type": "METRIC", "confidence": 0.7746142248312632}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9707328081130981}]}, {"text": "We also show results for the bigram-sibling and grandparent/sibling (G+S) models under dual decomposition.", "labels": [], "entities": []}, {"text": "Both the bigramsibling and G+S models show large improvements over the arc-factored approach; they also compare favorably to previous work-for example the G+S model gives better results than all results reported in the CoNLL-X shared task, on all languages.", "labels": [], "entities": []}, {"text": "Note that we use different feature sets from both and.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our first- order baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via  dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08:  The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald  and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task  (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald  and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples  for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for  test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing,  test decoding was carried out on identical machines with zero additional load; however, training was conducted on  machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng 1 : UAS when testing on  the CoNLL-08 validation set, following Martins et al. (2009). Eng 2 : UAS when testing on the CoNLL-08 test set.", "labels": [], "entities": [{"text": "consistency", "start_pos": 1004, "end_pos": 1015, "type": "METRIC", "confidence": 0.9813554883003235}, {"text": "CoNLL-08 corpus", "start_pos": 1208, "end_pos": 1223, "type": "DATASET", "confidence": 0.9647268354892731}, {"text": "UAS", "start_pos": 1233, "end_pos": 1236, "type": "METRIC", "confidence": 0.9954724907875061}, {"text": "CoNLL-08 validation set", "start_pos": 1258, "end_pos": 1281, "type": "DATASET", "confidence": 0.8904229402542114}, {"text": "UAS", "start_pos": 1324, "end_pos": 1327, "type": "METRIC", "confidence": 0.9925041198730469}]}, {"text": " Table 3: UAS of projective and non-projective decoding  for the English (PTB) and Czech (PDT) validation sets.  Sib/G+S: as in", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6498514413833618}]}, {"text": " Table 1. P-Sib/P-G+S: Projective versions  of Sib/G+S, where the MST component has been re- placed with the Eisner (2000) first-order projective parser.", "labels": [], "entities": []}]}