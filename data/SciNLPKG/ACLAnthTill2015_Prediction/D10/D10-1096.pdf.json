{"title": [{"text": "Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks", "labels": [], "entities": [{"text": "Solving Event Extraction Tasks", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.8350252658128738}]}], "abstractContent": [{"text": "In state-of-the-art approaches to information extraction (IE), dependency graphs constitute the fundamental data structure for syntactic structuring and subsequent knowledge elicita-tion from natural language documents.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.8871176838874817}]}, {"text": "The top-performing systems in the BioNLP 2009 Shared Task on Event Extraction all shared the idea to use dependency structures generated by a variety of parsers-either directly or in some converted manner-and optionally modified their output to fit the special needs of IE.", "labels": [], "entities": [{"text": "BioNLP 2009 Shared Task on Event Extraction", "start_pos": 34, "end_pos": 77, "type": "TASK", "confidence": 0.7693107724189758}]}, {"text": "As there are systematic differences between various dependency representations being used in this competition, we scrutinize on different encoding styles for dependency information and their possible impact on solving several IE tasks.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 226, "end_pos": 234, "type": "TASK", "confidence": 0.9367958903312683}]}, {"text": "After assessing more or less established dependency representations such as the Stanford and CoNLL-X dependencies , we will then focus on trimming operations that pave the way to more effective IE.", "labels": [], "entities": [{"text": "Stanford and CoNLL-X dependencies", "start_pos": 80, "end_pos": 113, "type": "DATASET", "confidence": 0.7494162768125534}, {"text": "trimming", "start_pos": 138, "end_pos": 146, "type": "TASK", "confidence": 0.9783583879470825}, {"text": "IE", "start_pos": 194, "end_pos": 196, "type": "TASK", "confidence": 0.9839611053466797}]}, {"text": "Our evaluation study covers data from a number of constituency-and dependency-based parsers and provides experimental evidence which dependency representations are particularly beneficial for the event extraction task.", "labels": [], "entities": [{"text": "event extraction task", "start_pos": 196, "end_pos": 217, "type": "TASK", "confidence": 0.8059012095133463}]}, {"text": "Based on empirical findings from our study we were able to achieve the performance of 57.2% F-score on the development data set of the BioNLP Shared Task 2009.", "labels": [], "entities": [{"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9994113445281982}, {"text": "development data set of the BioNLP Shared Task 2009", "start_pos": 107, "end_pos": 158, "type": "DATASET", "confidence": 0.835921068986257}]}], "introductionContent": [{"text": "Relation and event extraction are among the most demanding semantics-oriented NLP challenge tasks (both in the newspaper domain such as for ACE , as well as in the biological domain such as for BioCreative or the BioNLP Shared Task 3 ), comparable in terms of analytical complexity with recent efforts directed at opinion mining (e.g., NTCIR-7 or TREC Blog tracks ) or the recognition of textual entailment.", "labels": [], "entities": [{"text": "Relation and event extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6348651200532913}, {"text": "opinion mining", "start_pos": 314, "end_pos": 328, "type": "TASK", "confidence": 0.7178981602191925}, {"text": "recognition of textual entailment", "start_pos": 373, "end_pos": 406, "type": "TASK", "confidence": 0.8394798189401627}]}, {"text": "The most recent BioNLP 2009 Shared Task on Event Extraction () required, fora sample of 260 MEDLINE abstracts, to determine all mentioned events -to be chosen from a given set of nine event types, including \"Localization\", \"Binding\", \"Gene Expression\", \"Transcription\", \"Protein Catabolism\", \"Phosphorylation\", \"Positive Regulation\", \"Negative Regulation\", and (unspecified) \"Regulation\" -and link them appropriately with a priori supplied protein annotations.", "labels": [], "entities": [{"text": "BioNLP 2009 Shared Task on Event Extraction", "start_pos": 16, "end_pos": 59, "type": "TASK", "confidence": 0.5739743879863194}]}, {"text": "The demands on text analytics to deal with the complexity of this Shared Task in terms of relation diversity and specificity are unmatched by former challenges.", "labels": [], "entities": []}, {"text": "For relation extraction in the biomedical domain (the focus of our work), a stunning convergence towards dependency-based syntactic representation structures is witnessed by the performance results of the top-performing systems in the BioNLP'09 Shared Task on Event Extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9085366725921631}, {"text": "BioNLP'09 Shared Task on Event Extraction", "start_pos": 235, "end_pos": 276, "type": "TASK", "confidence": 0.5842402428388596}]}, {"text": "Regarding the fact that dependency representations were always viewed as a vehicle to represent fundamental semantic relationships already at the syntactic level, this is not a great surprise.", "labels": [], "entities": []}, {"text": "Yet, dependency grammar is not a monolithic, consensually shaped and welldefined linguistic theory.", "labels": [], "entities": [{"text": "dependency grammar", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.8317253589630127}]}, {"text": "Accordingly, associated parsers tend to vary in terms of dependency pairing or structuring (which pairs of words join in a dependency relation?) and dependency typing (how are dependency relations fora particular pair labelled?).", "labels": [], "entities": [{"text": "dependency typing", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.7784310877323151}]}, {"text": "Depending on the type of dependency theory or parser being used, various representations emerge (.", "labels": [], "entities": []}, {"text": "In this paper, we explore these different representations of the dependency graphs and try, first, to pinpoint their effects on solving the overall event extraction task and, second, to further enhance the potential of JREX, a high-performance relation and event extractor developed at the JULIE Lab ().", "labels": [], "entities": [{"text": "event extraction task", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.8063190778096517}, {"text": "event extractor", "start_pos": 257, "end_pos": 272, "type": "TASK", "confidence": 0.7350115478038788}, {"text": "JULIE Lab", "start_pos": 290, "end_pos": 299, "type": "DATASET", "confidence": 0.9286438524723053}]}], "datasetContent": [{"text": "In this section, we describe the experiments and results related to event extraction tasks based on alternative dependency graph representations.", "labels": [], "entities": [{"text": "event extraction tasks", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.8119982282320658}]}, {"text": "For our experiments, we selected the following topperforming parsers -the first three phrase structure based and thus the origin of derivative dependency structures, the last three fully dependency based for making native dependency structures available: \u2022 C+J, Charniak and Johnson's reranking parser, with the WSJtrained parsing model.", "labels": [], "entities": [{"text": "WSJtrained parsing", "start_pos": 312, "end_pos": 330, "type": "TASK", "confidence": 0.6568424254655838}]}, {"text": "\u2022 M+C, Charniak and Johnson's reranking parser, with the selftrained biomedical parsing model from McClosky (2010).", "labels": [], "entities": [{"text": "biomedical parsing", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6728213876485825}]}, {"text": "\u2022 Bikel, Bikel's parser) with the WSJ-trained parsing model.", "labels": [], "entities": [{"text": "WSJ-trained parsing", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.656847208738327}]}, {"text": "\u2022 GDep , a native dependency parser.", "labels": [], "entities": []}, {"text": "\u2022 MST), another native dependency parser.", "labels": [], "entities": []}, {"text": "\u2022 MALT), yet another native dependency parser.", "labels": [], "entities": [{"text": "MALT", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9296417832374573}]}, {"text": "The native dependency parsers were re-trained on the GENIA Treebank () conversions.", "labels": [], "entities": [{"text": "GENIA Treebank", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9696639776229858}]}, {"text": "These conversions, i.e., Stanford basic, CoNLL'07 and CoNLL'08 were produced with the currently available conversion scripts.", "labels": [], "entities": [{"text": "Stanford basic", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.890679270029068}, {"text": "CoNLL'07", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.8974018096923828}, {"text": "CoNLL'08", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9258487224578857}]}, {"text": "For the Stanford dependency conversion, we used the Stanford parser tool, for CoNLL'07 and CoNLL'08 we used the treebank-to-CoNLL conversion scripts 17 available from the CoNLL'X Shared Task organizers.", "labels": [], "entities": [{"text": "dependency conversion", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7082597762346268}, {"text": "CoNLL'07", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9259815812110901}, {"text": "CoNLL'08", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.8005303144454956}, {"text": "CoNLL'X Shared Task organizers", "start_pos": 171, "end_pos": 201, "type": "DATASET", "confidence": 0.8017547577619553}]}, {"text": "The phrase structure based parsers were applied with already available models, i.e., the Bikel and C+J parsers as trained on the WSJ corpus, and M+C as trained on the GENIA Treebank corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.9767699241638184}, {"text": "GENIA Treebank corpus", "start_pos": 167, "end_pos": 188, "type": "DATASET", "confidence": 0.9828328688939413}]}, {"text": "For our experiments, we converted the prediction results of the phrase structure based parsers into five dependency graph representations, viz.", "labels": [], "entities": []}, {"text": "Stanford basic, Stanford collapsed, Stanford ccprocessed, CoNLL'07 and CoNLL'08, using the same scripts as for the conversion of the GENIA Treebank.", "labels": [], "entities": [{"text": "Stanford ccprocessed", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.7905131578445435}, {"text": "GENIA Treebank", "start_pos": 133, "end_pos": 147, "type": "DATASET", "confidence": 0.9742650091648102}]}, {"text": "The JULIELab event extraction system was retrained on the Shared Task data enriched with different outputs of syntactic parsers as described above.", "labels": [], "entities": [{"text": "JULIELab event extraction", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7079106370608012}, {"text": "Shared Task data", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.6613533596197764}]}, {"text": "The results for the event extraction task are represented in under SVT-TOTAL; regulatory events are summarized under REG-TOTAL; the overall extraction results are listed in ALL-TOTAL (see).", "labels": [], "entities": [{"text": "event extraction task", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7666339874267578}, {"text": "SVT-TOTAL", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.8918188810348511}, {"text": "REG-TOTAL", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.6473538279533386}]}, {"text": "Obviously, the event extraction system trained on various dependency representations indeed produces truly different results.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8174343407154083}]}, {"text": "The differences in terms of Fscore come up to 2.4 percentage points for the SVT-TOTAL events (cf. the MALT parser, difference between SD basic (75.6% F-score) and CoNLL'07 (78.0% F-score)), up to 3.6 points for REG-TOTAL (cf. the M+C parser, difference between SD ccprocessed (40.9% F-score) and CoNLL'07 (44.5% Fscore)) and up to 2.5 points for ALL-TOTAL (cf. the M+C parser, difference between SD ccprocessed (52.8% F-score) and CoNLL'07 (55.3% F-score)).", "labels": [], "entities": [{"text": "Fscore", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9988299012184143}, {"text": "SVT-TOTAL", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.7636656165122986}, {"text": "F-score", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.9629741907119751}, {"text": "Fscore", "start_pos": 312, "end_pos": 318, "type": "METRIC", "confidence": 0.9018471837043762}]}, {"text": "The top three event extraction results on the development data based on different syntactic parsers results are achieved with M+C parser -CoNLL'07 representation (55.3% F-score), MST parserCoNLL'08 representation (54.6% F-score) and MALT parser -CoNLL'08 representation (53.8% F-score) (see, ALL-TOTAL).", "labels": [], "entities": [{"text": "F-score", "start_pos": 169, "end_pos": 176, "type": "METRIC", "confidence": 0.9878072142601013}, {"text": "F-score", "start_pos": 220, "end_pos": 227, "type": "METRIC", "confidence": 0.9497404098510742}, {"text": "F-score", "start_pos": 277, "end_pos": 284, "type": "METRIC", "confidence": 0.9601663947105408}]}, {"text": "Surprisingly, both the CoNLL'08 and CoNLL'07 formats clearly outperform Stanford representations on all event extraction tasks.", "labels": [], "entities": [{"text": "CoNLL'08", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.9277299046516418}, {"text": "event extraction tasks", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.7848434944947561}]}, {"text": "Stanford dependencies seem to be useful here only in the basic mode.", "labels": [], "entities": []}, {"text": "The collapsed and ccprocessed modes produce even worse results for the event extraction tasks.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7254937887191772}]}, {"text": "Our second experiment focused on trimming operations on CoNLL'X dependency graphs.", "labels": [], "entities": [{"text": "trimming", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.9921899437904358}, {"text": "CoNLL'X dependency graphs", "start_pos": 56, "end_pos": 81, "type": "DATASET", "confidence": 0.834091067314148}]}, {"text": "Here we performed event extraction after the trimming of the dependency trees as described in Section 4.2 in different modes: coords -re-structuring coordinations; preps -collapsing of prepositions; auxiliaries -propagating dependency relations of auxiliars and modals to main verbs; noun phrase -re-structuring noun phrases containing action adjectives.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.742151290178299}]}, {"text": "Our second experiment showed that the extraction of selected events can profit in particular from the trimming procedures coords and auxiliaries, but there is no evidence fora general trimming configuration for the overall event extraction task.", "labels": [], "entities": [{"text": "trimming", "start_pos": 102, "end_pos": 110, "type": "TASK", "confidence": 0.9680485129356384}, {"text": "event extraction task", "start_pos": 223, "end_pos": 244, "type": "TASK", "confidence": 0.7982053160667419}]}, {"text": "In we summarize the best configurations we found for the events in focus.", "labels": [], "entities": []}, {"text": "It is quite evident that the CoNLL'08 and CoNLL'07 dependencies modified for auxiliaries and coordinations are the best configurations for four events (out of nine).", "labels": [], "entities": [{"text": "CoNLL'08", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9130313396453857}]}, {"text": "For three events no modifications are necessary and      only one event profits from trimming of prepositions.", "labels": [], "entities": []}, {"text": "Only the Binding event profits significantly from noun phrase modifications (see).", "labels": [], "entities": [{"text": "Binding event", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.8785791099071503}, {"text": "noun phrase modifications", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6169955829779307}]}, {"text": "The increase in F-score for trimming procedures is 4.1 percentage points for Binding events.", "labels": [], "entities": [{"text": "F-score", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9996904134750366}, {"text": "trimming", "start_pos": 28, "end_pos": 36, "type": "TASK", "confidence": 0.9492748379707336}, {"text": "Binding", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9814328551292419}]}, {"text": "In our final experiment we connected the best configurations for each of the BioNLP'09 events as presented in.", "labels": [], "entities": []}, {"text": "The overall event extraction results of this final configuration are presented in Tables 4 and 5.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.6928909718990326}]}, {"text": "We achieved an increase of 1.9 percentage points F-score in the overall event extraction compared to the best-performing single parser configuration (M+C, CoNLL'07) (see, ALL-TOTAL).", "labels": [], "entities": [{"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.999433696269989}, {"text": "event extraction", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.6411374062299728}]}, {"text": "The reported results on the development data outperform the results of the TOKYO system by 2.6 percentage points F-score for all basic events including Binding events (see, EVT-TOTAL) and by 0.9 percentage points in the overall event extraction task (see, ALL-TOTAL).", "labels": [], "entities": [{"text": "TOKYO", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.5562040209770203}, {"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9992759823799133}, {"text": "event extraction", "start_pos": 228, "end_pos": 244, "type": "TASK", "confidence": 0.7203660309314728}, {"text": "ALL-TOTAL", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9158766865730286}]}, {"text": "On the test data we achieved an F-score similar to the current JULIELab system trained on modified CoNLL'07 dependencies from the MST parser (see, ALL-TOTAL).", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9989534616470337}]}, {"text": "The results on the official test data reveal that the performance differences between various parsers may play a much smaller role than the proper choice of dependency representations.", "labels": [], "entities": []}, {"text": "Our empirical findings that the best performance results could only be achieved by eventspecific dependency graph configurations reveal that the syntactic representations of different semantic events vary considerably at the level of dependency graph complexity and that the automatic prediction of such syntactic structures can vary from one dependency parser to the other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the Shared Task development data for Event Extraction Task. Approximate Span Match- ing/Approximate Recursive Matching.", "labels": [], "entities": [{"text": "Event Extraction Task", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.6641347110271454}, {"text": "Approximate Span Match- ing", "start_pos": 81, "end_pos": 108, "type": "METRIC", "confidence": 0.8987627148628234}, {"text": "Approximate", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9585334062576294}]}, {"text": " Table 2: Best Configurations for Dependency Representations for Event Extraction Task on the development data.", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.677635133266449}]}, {"text": " Table 3: Effects of trimming of CoNLL dependencies on the Shared Task development data for Binding events. Ap- proximate Span Matching/Approximate Recursive Matching. The data was processed by the MST parser.", "labels": [], "entities": [{"text": "Binding events", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.875219076871872}, {"text": "Ap- proximate", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.9301662643750509}, {"text": "Span Matching", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.594582200050354}, {"text": "MST", "start_pos": 198, "end_pos": 201, "type": "DATASET", "confidence": 0.8675053715705872}]}, {"text": " Table 4: Results on the Shared Task development data. Approximate Span Matching/Approximate Recursive Match- ing.", "labels": [], "entities": [{"text": "Shared Task development data", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.7094496637582779}, {"text": "Approximate", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9948280453681946}, {"text": "Approximate Recursive Match- ing", "start_pos": 81, "end_pos": 113, "type": "METRIC", "confidence": 0.9099098086357117}]}]}