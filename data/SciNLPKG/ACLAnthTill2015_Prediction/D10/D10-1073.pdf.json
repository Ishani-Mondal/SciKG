{"title": [{"text": "Word Sense Induction & Disambiguation Using Hierarchical Random Graphs", "labels": [], "entities": [{"text": "Word Sense Induction & Disambiguation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6829361021518707}]}], "abstractContent": [{"text": "Graph-based methods have gained attention in many areas of Natural Language Processing (NLP) including Word Sense Disambiguation (WSD), text summarization, keyword extraction and others.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.759744867682457}, {"text": "text summarization", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7469605803489685}, {"text": "keyword extraction", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.7862198948860168}]}, {"text": "Most of the work in these areas formulate their problem in a graph-based setting and apply unsupervised graph clustering to obtain a set of clusters.", "labels": [], "entities": []}, {"text": "Recent studies suggest that graphs often exhibit a hierarchical structure that goes beyond simple flat clustering.", "labels": [], "entities": []}, {"text": "This paper presents an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word.", "labels": [], "entities": []}, {"text": "The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs significantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7587498823801676}]}], "introductionContent": [{"text": "A number of NLP problems can be cast into a graphbased framework, in which entities are represented as vertices in a graph and relations between them are depicted by weighted or unweighted edges.", "labels": [], "entities": []}, {"text": "For instance, in unsupervised WSD a number of methods () have constructed word co-occurrence graphs fora target polysemous word and applied graph-clustering to obtain the clusters (senses) of that word.", "labels": [], "entities": [{"text": "WSD", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8536175489425659}]}, {"text": "Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6686108410358429}]}, {"text": "Graph-based ranking algorithms, such as PageRank (, were then applied in order to determine the significance of sentences.", "labels": [], "entities": []}, {"text": "In the same vein, graph-based methods have been applied to other problems such as determining semantic similarity of text ().", "labels": [], "entities": [{"text": "determining semantic similarity of text", "start_pos": 82, "end_pos": 121, "type": "TASK", "confidence": 0.7758341073989868}]}, {"text": "Recent studies suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and soon, until we reach the leaves.", "labels": [], "entities": []}, {"text": "This hierarchical structure provides additional information as opposed to flat clustering by explicitly including organisation at all scales of a graph.", "labels": [], "entities": []}, {"text": "In this paper, we present an unsupervised method for inferring the hierarchical structure (binary tree) of a graph, in which vertices are the contexts of a polysemous word and edges represent the similarity between contexts.", "labels": [], "entities": []}, {"text": "The method that we use to infer that hierarchical structure is the Hierarchical Random Graphs (HRGs) algorithm due to.", "labels": [], "entities": []}, {"text": "The binary tree produced by our method groups the contexts of a polysemous word at different heights of the tree.", "labels": [], "entities": []}, {"text": "Thus, it induces the senses of that word at different levels of sense granularity.", "labels": [], "entities": []}, {"text": "To evaluate our method, we apply it to the problem of noun sense disambiguation showing that inferring the hierarchical structure using HRGs provides additional information from the observed graph leading to improved WSD performance compared to: (1) simple flat clustering, and (2) traditional agglomerative clustering.", "labels": [], "entities": [{"text": "noun sense disambiguation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.795585572719574}, {"text": "WSD", "start_pos": 217, "end_pos": 220, "type": "TASK", "confidence": 0.8613455295562744}]}, {"text": "Finally, we compare our results with state-of-the-art sense induction systems and show that our method yields improvements.", "labels": [], "entities": []}, {"text": "shows the different stages of the proposed method that we describe in the following sections.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on the nouns of the SemEval-2007 word sense induction task under the second evaluation setting of that task, i.e. supervised evaluation.", "labels": [], "entities": [{"text": "SemEval-2007 word sense induction task", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.7550897300243378}]}, {"text": "Specifically, we use the standard WSD measures of precision and recall in order to produce their harmonic mean (FScore).", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.6587210893630981}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9995254278182983}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9984723925590515}, {"text": "FScore)", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.8792911767959595}]}, {"text": "The official scoring software of that task has been used in our evaluation.", "labels": [], "entities": []}, {"text": "Note that the unsupervised measures of that task are not directly applicable to our induced hierarchies, since they focus on assessing flat clustering methods.", "labels": [], "entities": []}, {"text": "The first aim of our evaluation is to test whether inferring the hierarchical structure of the constructed graphs improves WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.9729352593421936}]}, {"text": "For that reason our first baseline, Chinese Whispers Unweighted version (CWU), takes as input the same unweighted graph of contexts as HRGs in order to produce a flat clustering.", "labels": [], "entities": [{"text": "Chinese Whispers Unweighted version (CWU)", "start_pos": 36, "end_pos": 77, "type": "DATASET", "confidence": 0.7763121979577201}]}, {"text": "The set of produced clusters is then mapped to GS senses using the training dataset and performance is then measured on the testing dataset.", "labels": [], "entities": []}, {"text": "We followed the same sense mapping method as in the SemEval-2007 sense induction task.", "labels": [], "entities": [{"text": "sense mapping", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7601393461227417}, {"text": "SemEval-2007 sense induction task", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.8138664215803146}]}, {"text": "Our second baseline, Chinese Whispers Weighted version (CWW), is similar to the previous one, with the difference that the edges of the input graph are weighted using Equation 1.", "labels": [], "entities": [{"text": "Chinese Whispers Weighted version (CWW)", "start_pos": 21, "end_pos": 60, "type": "DATASET", "confidence": 0.6651373505592346}]}, {"text": "For clustering the graphs of CWU and CWW we employ, Chinese Whispers 4 ().", "labels": [], "entities": [{"text": "Chinese Whispers 4", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.8706802129745483}]}, {"text": "The second aim of our evaluation is to assess whether the hierarchical structure inferred by HRGs is more informative than the hierarchical structure inferred by traditional Hierarchical Clustering (HAC).", "labels": [], "entities": []}, {"text": "Hence, our third baseline, takes as input a similarity matrix of the graph vertices and performs bottom-up clustering with average-linkage, which has already been used in WSI in (Pantel and Lin, The number of iterations for CW was set to and was shown to have superior or similar performance to single-linkage and complete-linkage in the related problem of learning a taxonomy of senses ( . To calculate the similarity matrix of vertices we follow a process similar to the one used in Section 4.2 for calculating the probability of an internal node.", "labels": [], "entities": [{"text": "WSI", "start_pos": 171, "end_pos": 174, "type": "DATASET", "confidence": 0.6917276382446289}]}, {"text": "The similarity between two vertices is calculated according to the degree of connectedness among their direct neighbours.", "labels": [], "entities": []}, {"text": "Specifically, we would like to assign high similarity to pairs of vertices, whose neighbours are close to forming a clique.", "labels": [], "entities": []}, {"text": "Given two vertices (contexts) c i and c j , let N (c i , c j ) be the set of their neighbours and K(c i , c j ) be the set of edges between the vertices in N (c i , c j ).", "labels": [], "entities": []}, {"text": "The maximum number of edges that could exist be- . Thus, the similarity of c i , c j is set equal to the number of edges that actually exist in that neighbourhood divided by the total number of edges that could exist ( ).", "labels": [], "entities": []}, {"text": "The disambiguation process using the HAC tree is identical to the one presented in Section 5.2 with the only difference that the internal probability, \u03b8 i , in Equation 6 does not exist for HAC.", "labels": [], "entities": [{"text": "HAC tree", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.7973673939704895}]}, {"text": "Hence, we replaced it with the factor 1 |H(D i )| , where H(D i ) is the set of children of internal node Di . This factor provides lower weights for nodes high in the tree, since their discriminating ability will possibly be lower.", "labels": [], "entities": []}, {"text": "shows the parameter values used in the evaluation.", "labels": [], "entities": []}, {"text": "shows the performance of the proposed method against the baselines for p 3 = 0.05 and different p 1 and p 2 values.(B) illustrates the results of the same experiment using p 3 = 0.09.", "labels": [], "entities": []}, {"text": "In both figures, we observe that HRGs outperform the CWU baseline under all parameter combinations.", "labels": [], "entities": [{"text": "CWU baseline", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.8768716752529144}]}, {"text": "In particular, all of the 12 performance differences for p 3 = 0.09 are statistically significant using McNemar's test at 95% confidence level, while for p 3 = 0.05 only 2 out of the 12 performance differences were not judged as significant from the test.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.8445635636647543}]}], "tableCaptions": [{"text": " Table 3: HRGs against recent methods & baselines.", "labels": [], "entities": [{"text": "HRGs", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.6829919219017029}]}]}