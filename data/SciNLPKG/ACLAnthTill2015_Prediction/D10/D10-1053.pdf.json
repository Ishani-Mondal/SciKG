{"title": [{"text": "Hierarchical Phrase-based Translation Grammars Extracted from Alignment Posterior Probabilities", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation Grammars Extracted from Alignment Posterior Probabilities", "start_pos": 0, "end_pos": 95, "type": "TASK", "confidence": 0.7982567184501224}]}], "abstractContent": [{"text": "We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text.", "labels": [], "entities": [{"text": "phrase-based translation grammars", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.666585773229599}]}, {"text": "Rather than restrict rule extraction to a single alignment , such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignment model.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7472528517246246}, {"text": "Viterbi", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9465130567550659}, {"text": "HMM word-to-word alignment", "start_pos": 152, "end_pos": 178, "type": "TASK", "confidence": 0.5312060912450155}]}, {"text": "We define translation grammars progressively by adding classes of rules to a basic phrase-based system.", "labels": [], "entities": [{"text": "translation grammars", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9466742873191833}]}, {"text": "We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield.", "labels": [], "entities": []}, {"text": "In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6843473017215729}]}, {"text": "We also find that grammars with rules with only one nonterminal, when extracted from posteriors , can outperform more complex grammars extracted from Viterbi alignments.", "labels": [], "entities": []}, {"text": "Finally, we show that the best way to exploit source-to-target and target-to-source alignment models is to build two separate systems and combine their output translation lattices.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current practice in hierarchical phrase-based translation extracts regular phrases and hierarchical rules from word-aligned parallel text.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.6089064180850983}]}, {"text": "Alignment models estimated over the parallel text are used to generate these alignments, but these models are then typically used no further in rule extraction.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 144, "end_pos": 159, "type": "TASK", "confidence": 0.8529472947120667}]}, {"text": "This is less than ideal because these alignment models, even if they are not suitable for direct use in translation, can still provide a great deal of useful information beyond a single best estimate of the alignment of the parallel text.", "labels": [], "entities": []}, {"text": "Our aim is to use alignment models to generate the statistics needed to build translation grammars.", "labels": [], "entities": [{"text": "translation grammars", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8961781859397888}]}, {"text": "The challenge in doing so is to extend the current procedures, which are geared towards the use of a single alignment, to make more of what can be provided by alignment models.", "labels": [], "entities": []}, {"text": "The goal is to extract a richer and more robust set of translation rules.", "labels": [], "entities": []}, {"text": "There are two aspects to hierarchical phrase-based translation grammars which concern us.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation grammars", "start_pos": 25, "end_pos": 71, "type": "TASK", "confidence": 0.6508913487195969}]}, {"text": "The first is expressive power, which we take as the ability to generate known reference translations from sentences in the source language.", "labels": [], "entities": []}, {"text": "This is determined by the degree of phrase movements and the translations allowed by the rules of the grammar.", "labels": [], "entities": []}, {"text": "For a grammar with given types of rules, larger rule sets will yield greater expressive power.", "labels": [], "entities": []}, {"text": "This motivates studies of grammars based on the rules which are extracted and the movement the grammar allows.", "labels": [], "entities": []}, {"text": "The second aspect is of course translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9732003211975098}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9356300234794617}]}, {"text": "If the expressive power is adequate, then the desire is that the grammar assigns a high score to a correct translation.", "labels": [], "entities": []}, {"text": "We use posterior probabilities over parallel data to address both of these aspects.", "labels": [], "entities": []}, {"text": "These posteriors allow us to build larger rule sets with improved translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.7929221391677856}]}, {"text": "Ideally, fora sentence pair we wish to consider all possible alignments between all possible source and target phrases within these sentences.", "labels": [], "entities": []}, {"text": "Given a grammar allowing certain types of movement, we would then extract all possible parses that are consistent with any alignments of these phrases.", "labels": [], "entities": []}, {"text": "To make this approach feasible, we consider only phrase-to-phrase alignments with a high posterior probability under the alignment models.", "labels": [], "entities": []}, {"text": "In this way, the alignment model probabilities guide rule extraction.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.747735470533371}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work on using posteriors to extract phrases, as well as other approaches that tightly integrate word alignment and rule extraction.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7697711288928986}, {"text": "rule extraction", "start_pos": 141, "end_pos": 156, "type": "TASK", "confidence": 0.7283650040626526}]}, {"text": "Section 3 describes rule extraction based on word and phrase posterior distributions provided by the HMM word-to-word alignment model.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8401978015899658}, {"text": "HMM word-to-word alignment", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.6270282367865244}]}, {"text": "In Section 4 we define translation grammars progressively by adding classes of rules to a basic phrase-based system, motivating each rule type by the phrase movement it is intended to achieve.", "labels": [], "entities": [{"text": "translation grammars", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.944210022687912}]}, {"text": "In Section 5 we assess these grammars in terms of their expressive power and the quality of the translations they yield in Chinese-toEnglish, showing that rule extraction from posteriors gives translation improvements.", "labels": [], "entities": []}, {"text": "We also find that the best way to exploit source-to-target and targetto-source alignment models is to build two separate systems and combine their output translation lattices.", "labels": [], "entities": []}, {"text": "Section 6 presents the main conclusions of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experiments in Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6376429796218872}]}, {"text": "Our system is trained on a subset of the GALE 2008 evaluation parallel text; 2 this is approximately 50M words per language.", "labels": [], "entities": [{"text": "GALE 2008 evaluation parallel text", "start_pos": 41, "end_pos": 75, "type": "DATASET", "confidence": 0.9091389894485473}]}, {"text": "We report translation results on a development set tune-nw and a test set test-nw1.", "labels": [], "entities": []}, {"text": "These contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT06.", "labels": [], "entities": [{"text": "GALE program", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8197606205940247}, {"text": "MT02", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9180764555931091}, {"text": "MT06", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.766120970249176}]}, {"text": "out test set test-nw2, containing 60% of the NIST newswire portion of MT06, that is, 369 sentences.", "labels": [], "entities": [{"text": "NIST newswire portion of MT06", "start_pos": 45, "end_pos": 74, "type": "DATASET", "confidence": 0.9020900845527648}]}, {"text": "The parallel texts for both language pairs are aligned using MTTK).", "labels": [], "entities": [{"text": "MTTK", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.7732077240943909}]}, {"text": "For decoding we use HiFST, a lattice-based decoder implemented with Weighted Finite State Transducers (de.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8904687166213989}]}, {"text": "Likelihood-based search pruning is applied if the number of states in the lattice associated with each CYK grid cell exceeds 10,000, otherwise the entire search space is explored.", "labels": [], "entities": []}, {"text": "The language model is a 4-gram language model estimated over the English side of the parallel text and the AFP and Xinhua portions of the English Gigaword Fourth Edition (LDC2009T13), interpolated with a zero-cutoff stupid-backoff () 5-gram estimated using 6.6B words of English newswire text.", "labels": [], "entities": [{"text": "AFP and Xinhua portions of the English Gigaword Fourth Edition (LDC2009T13)", "start_pos": 107, "end_pos": 182, "type": "DATASET", "confidence": 0.9184545049300561}]}, {"text": "In tuning the systems, standard MERT iterative parameter estimation under IBM BLEU 3 is performed on the development sets.", "labels": [], "entities": [{"text": "MERT iterative parameter estimation", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.6100104674696922}, {"text": "IBM", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.6754037737846375}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9206342101097107}]}], "tableCaptions": [{"text": " Table 2: Chinese-to-English translation results with alternative grammars and extraction methods (lower-cased BLEU  shown). Time (secs/word) and prune (times/word) measurements done on tune-nw set.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.5051926523447037}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9980916380882263}, {"text": "Time", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9944815635681152}]}, {"text": " Table 3: Translation results under grammar G 2 with indi- vidual rule sets, merged rule sets, and rescoring and sys- tem combination with lattice-based MBR (lower-cased  BLEU shown)", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9790494441986084}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.997247040271759}]}]}