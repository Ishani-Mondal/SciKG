{"title": [{"text": "Positional Language Models for Clinical Information Retrieval", "labels": [], "entities": [{"text": "Clinical Information Retrieval", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.583095779021581}]}], "abstractContent": [{"text": "The PECO framework is a knowledge representation for formulating clinical questions.", "labels": [], "entities": [{"text": "PECO framework", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7185157984495163}, {"text": "formulating clinical questions", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.8977751731872559}]}, {"text": "Queries are decomposed into four aspects, which are Patient-Problem (P), Exposure (E), Comparison (C) and Outcome (O).", "labels": [], "entities": [{"text": "Outcome (O)", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9354631751775742}]}, {"text": "However, no test collection is available to evaluate such framework in information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.8102991282939911}]}, {"text": "In this work, we first present the construction of a large test collection extracted from systematic literature reviews.", "labels": [], "entities": []}, {"text": "We then describe an analysis of the distribution of PECO elements throughout the relevant documents and propose a language modeling approach that uses these distributions as a weighting strategy.", "labels": [], "entities": []}, {"text": "In our experiments carried out on a collection of 1.5 million documents and 423 queries, our method was found to lead to an improvement of 28% in MAP and 50% in P@5, as compared to the state-of-the-art method.", "labels": [], "entities": [{"text": "MAP", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9119887948036194}, {"text": "P@5", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9266945123672485}]}], "introductionContent": [{"text": "In recent years, the volume of health and biomedical literature available in electronic form has grown exponentially.", "labels": [], "entities": []}, {"text": "MEDLINE, the authoritative repository of citations from the medical and bio-medical domain, contains more than 18 million citations.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9305897951126099}]}, {"text": "Searching for clinically relevant information within this large amount of data is a difficult task that medical professionals are often unable to complete in a timely manner.", "labels": [], "entities": []}, {"text": "A better access to clinical evidence represents a high impact application for physicians.", "labels": [], "entities": []}, {"text": "Evidence-Based Medicine (EBM) is a widely accepted paradigm for medical practice.", "labels": [], "entities": [{"text": "Evidence-Based Medicine (EBM)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7398394525051117}]}, {"text": "EBM is defined as the conscientious, explicit and judicious use of current best evidence in making decisions about patient care.", "labels": [], "entities": []}, {"text": "Practice EBM means integrating individual clinical expertise with the best available external clinical evidence from systematic research.", "labels": [], "entities": []}, {"text": "It involves tracking down the best evidence from randomized trials or meta-analyses with which to answer clinical questions.", "labels": [], "entities": []}, {"text": "identified the following four aspects as the key elements of a well-built clinical question: \u2022 Patient-problem: what are the patient characteristics (e.g. age range, gender, etc.)?", "labels": [], "entities": []}, {"text": "What is the primary condition or disease?", "labels": [], "entities": []}, {"text": "\u2022 Exposure-intervention: what is the main intervention (e.g. drug, treatment, duration, etc.)?", "labels": [], "entities": [{"text": "duration", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.993242084980011}]}, {"text": "\u2022 Comparison: what is the exposure compared to (e.g. placebo, another drug, etc.)?", "labels": [], "entities": []}, {"text": "\u2022 Outcome: what are the clinical outcomes (e.g. healing, morbidity, side effects, etc.)?", "labels": [], "entities": [{"text": "Outcome", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.8394650220870972}]}, {"text": "These elements are known as the PECO elements.", "labels": [], "entities": []}, {"text": "Physicians are educated to formulate their clinical questions in respect to this structure.", "labels": [], "entities": []}, {"text": "For example, in the following question: \"In patients of all ages with Parkinson's disease, does a Treadmill training compared to no training allows to increase the walking distance?\" one can identify the following elements: \u2022 P: Patients of all ages with Parkinson's disease \u2022 E: Treadmill training \u2022 C: No treadmill training \u2022 O: Walking distance In spite of this well-defined question structure, physicians still use keyword-based queries when they search for clinical evidence.", "labels": [], "entities": [{"text": "O", "start_pos": 328, "end_pos": 329, "type": "METRIC", "confidence": 0.9820288419723511}]}, {"text": "An explanation of that is the almost total absence of PECO search interfaces.", "labels": [], "entities": []}, {"text": "PubMed 1 , the most used search interface, does not allow users to formulate PECO queries yet.", "labels": [], "entities": [{"text": "PubMed 1", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.92305126786232}]}, {"text": "For the previously mentioned clinical question, a physician would use the query \"Treadmill AND Parkinson's disease\".", "labels": [], "entities": [{"text": "Treadmill", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.7351755499839783}]}, {"text": "There is intuitively much to gain by using a PECO structured query in the retrieval process.", "labels": [], "entities": []}, {"text": "This structure specifies the role of each concept in the desired documents, which is a clear advantage over a keyword-based approach.", "labels": [], "entities": []}, {"text": "One can for example differentiate two queries in which a disease would be a patient condition or a clinical outcome.", "labels": [], "entities": []}, {"text": "This conceptual decomposition of queries is also particularly useful in a sense that it can be used to balance the importance of each element in the search process.", "labels": [], "entities": []}, {"text": "Another important factor that prevented researchers from testing approaches to clinical information retrieval (IR) based on PECO elements is the lack of a test collection, which contains a set of documents, a set of queries and the relevance judgments.", "labels": [], "entities": [{"text": "clinical information retrieval (IR)", "start_pos": 79, "end_pos": 114, "type": "TASK", "confidence": 0.828753262758255}]}, {"text": "The construction of such a test collection is costly in manpower.", "labels": [], "entities": []}, {"text": "In this paper, we take advantage of the systematic reviews about clinical questions from Cochrane.", "labels": [], "entities": []}, {"text": "Each Cochrane review examines in depth a clinical question and survey all the available relevant publications.", "labels": [], "entities": []}, {"text": "The reviews are written for medical professionals.", "labels": [], "entities": []}, {"text": "We transformed them into a TREC-like test collection, which contains 423 queries and 8926 relevant documents extracted from MEDLINE.", "labels": [], "entities": [{"text": "TREC-like test collection", "start_pos": 27, "end_pos": 52, "type": "DATASET", "confidence": 0.7562009692192078}, {"text": "MEDLINE", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9247133135795593}]}, {"text": "Ina second part of this paper, we present a model integrating the PECO framework in a language modeling approach to IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9842723608016968}]}, {"text": "An intuitive method would try to annotate the concepts in documents into PECO categories.", "labels": [], "entities": []}, {"text": "One can then match the PECO elements in the query to the elements detected in documents.", "labels": [], "entities": []}, {"text": "However, as previous studies have shown, it is very difficult to automatically annotate accurately PECO elements in documents.", "labels": [], "entities": []}, {"text": "To by-pass this issue, we propose an alternative that relies on the observed positional distribution of these elements in documents.", "labels": [], "entities": []}, {"text": "We will see that different types of element have different distributions.", "labels": [], "entities": []}, {"text": "By weighting words according to their positions, we can indirectly weigh the importance of different types of element in search.", "labels": [], "entities": []}, {"text": "As we will show 1 www.pubmed.gov in this paper, this approach turns out to be highly effective.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first briefly review the previous work, followed by a description of the test collection we have constructed.", "labels": [], "entities": []}, {"text": "Next, we give the details of the method we propose and present our experiments and results.", "labels": [], "entities": []}, {"text": "Lastly, we conclude with a discussion and directions for further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a collection of documents, we gathered 1.5 millions of citations from PubMed.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.8545371890068054}]}, {"text": "We used the following constraints: citations with an abstract, human subjects, and belonging to one of the following publication types: randomized control trials, reviews, clinical trials, letters, editorials and metaanalyses.", "labels": [], "entities": []}, {"text": "The set of queries and relevance judgments described in Section 3 is used to evaluate our model.", "labels": [], "entities": []}, {"text": "Relevant documents were, if not already included, added to the collection.", "labels": [], "entities": []}, {"text": "Because each query is generated from a systematic literature review completed at a time t, we placed an additional restriction on the publication date of the retrieved documents: only documents published before time tare considered.", "labels": [], "entities": []}, {"text": "Before indexing, each citation is pre-processed to extract its title and abstract text and then converted into a TREC-like document format.", "labels": [], "entities": []}, {"text": "Abstracts are divided into 10 parts of equal length (the ones containing less than 10 words are discarded).", "labels": [], "entities": []}, {"text": "The following fields are marked in each document: title, P1, P2 \u00b7 \u00b7 \u00b7 P10.", "labels": [], "entities": [{"text": "title", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7913318276405334}]}, {"text": "The following evaluation measures are used: \u2022 Precision at rank n (P@n): precision computed on then topmost retrieved documents.", "labels": [], "entities": [{"text": "Precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9922066330909729}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9992247819900513}]}, {"text": "\u2022 Mean Average Precision (MAP): average of precision measures computed at the point of each relevant document in the ranked list.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 2, "end_pos": 30, "type": "METRIC", "confidence": 0.9664265910784403}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9911824464797974}]}, {"text": "\u2022 Number of relevant documents retrieved All retrieval tasks are performed using an \"outof-the-shelf\" version of the Lemur toolkit . We use the embedded tokenization algorithm along with the standard Porter stemmer.", "labels": [], "entities": []}, {"text": "The number of retrieved documents is set to 1000 and the Dirichlet prior smoothing parameter to \u00b5 = 2000.", "labels": [], "entities": []}, {"text": "In all our experiments, we use the KL divergence scoring function (equation 1) as baseline.", "labels": [], "entities": [{"text": "KL divergence scoring function", "start_pos": 35, "end_pos": 65, "type": "METRIC", "confidence": 0.6661553308367729}]}, {"text": "Statistical significance is computed using the well-known Student's t-test.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.722642719745636}]}, {"text": "To determine reasonable weights and avoid overtuning the parameters, we use a 10-fold cross-validation optimizing the MAP values.", "labels": [], "entities": [{"text": "MAP", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.7857667803764343}]}, {"text": "We first investigated the impact of using PECOstructured queries on the retrieval performance.", "labels": [], "entities": []}, {"text": "As far as we know, no quantitative evaluation of the increase or decrease of performance in comparison with a keyword-based search strategy has been reported.", "labels": [], "entities": []}, {"text": "presented a comparison between PubMed and a PECO search interface but failed to demonstrate any significant difference between the two search protocols.", "labels": [], "entities": []}, {"text": "The larger number of words in PECO-structured queries, on average 18.8 words per query compared to 4.3 words for keyword queries, should capture more aspects of the information need.", "labels": [], "entities": []}, {"text": "But, it may also be a disadvantage due to the fact that more noise can be brought in, causing query-drift issues.", "labels": [], "entities": []}, {"text": "We propose two baselines using the keywordbased queries.", "labels": [], "entities": []}, {"text": "The first baseline (named Baseline-1) uses keyword queries with the traditional language modeling approach.", "labels": [], "entities": []}, {"text": "This is one of the stateof-the-art approaches in current IR research.", "labels": [], "entities": [{"text": "IR", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9925830364227295}]}, {"text": "This retrieval model considers each word in a query as an equal, independent source of information.", "labels": [], "entities": []}, {"text": "In the second baseline (named Baseline-2), we consider multiword phrases.", "labels": [], "entities": []}, {"text": "In our test collection, queries are often composed of multiword phrases such as \"low back pain\" or \"early pregnancy\".", "labels": [], "entities": []}, {"text": "It is clear that finding the exact phrase \"heart failure\" is a much stronger indicator of relevance than just finding \"heart\" and \"failure\" scattered within a document.", "labels": [], "entities": []}, {"text": "The Indri operator #1 is used to perform phrase-based retrieval.", "labels": [], "entities": [{"text": "Indri operator #1", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.8945021629333496}, {"text": "phrase-based retrieval", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7914586067199707}]}, {"text": "Phrases are already indicated in queries by the conjunction and (e.g. vaccine and hepatitis B).", "labels": [], "entities": []}, {"text": "A simple regular expression is used to recognize the phrases.", "labels": [], "entities": []}, {"text": "As expected, phrase-based retrieval leads to some increase in retrieval precision (P@5).", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.8200667500495911}, {"text": "P@5)", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9572988003492355}]}, {"text": "However, the number of relevant documents retrieved is decreased.", "labels": [], "entities": []}, {"text": "This is due to the fact that we use exact phrase matching that can reduce query coverage.", "labels": [], "entities": [{"text": "phrase matching", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.6915973573923111}]}, {"text": "One solution would be to use unordered window features (Indri operator #uwn) that would require words to be close together but not necessarily in an exact sequence order).", "labels": [], "entities": []}, {"text": "The PECO queries use PECO-structured queries as a bag of words.", "labels": [], "entities": []}, {"text": "We observe that PECO queries do not enhance the average precision but increase the P@5 significantly.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9941893815994263}, {"text": "P@5", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9834329684575399}]}, {"text": "The number of relevant documents retrieved is also larger.", "labels": [], "entities": []}, {"text": "These results indicate that formulating clinical queries according to the PECO framework enhance the retrieval effectiveness.", "labels": [], "entities": [{"text": "PECO framework", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.8471145331859589}]}], "tableCaptions": [{"text": " Table 1: Comparing the performance measures of  keyword-based and PECO-structured queries in terms of  MAP, precision at 5 and number of relevant documents  retrieved (#rel. ret.). ( * : t.test < 0.05)", "labels": [], "entities": [{"text": "MAP", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.8676272630691528}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9993183612823486}]}, {"text": " Table 2: 10-fold cross validation scores for the Baseline-2, Baseline-3 and the two variants of our proposed model  (Model-1 and Model-2). Relative increase over the Baseline-2 is given, #rel. ret. is the number of relevant documents  retrieved. ( \u2020: t.test < 0.01,  * : t.test < 0.05)", "labels": [], "entities": [{"text": "Relative", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9901386499404907}]}]}