{"title": [{"text": "Evaluating Models of Latent Document Semantics in the Presence of OCR Errors", "labels": [], "entities": []}], "abstractContent": [{"text": "Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text.", "labels": [], "entities": []}, {"text": "In an effort to apply such models to noisy optical character recognition (OCR) text output , we endeavor to understand the effect that character-level noise can have on unsu-pervised topic modeling.", "labels": [], "entities": [{"text": "optical character recognition (OCR) text output", "start_pos": 43, "end_pos": 90, "type": "TASK", "confidence": 0.8374839276075363}]}, {"text": "We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data.", "labels": [], "entities": [{"text": "document-level topic analysis (document clustering)", "start_pos": 30, "end_pos": 81, "type": "TASK", "confidence": 0.6182475686073303}, {"text": "word-level topic analysis (LDA)", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.6710404207309087}]}, {"text": "As expected, experimental results show that performance declines as word error rates increase.", "labels": [], "entities": []}, {"text": "Common techniques for alleviating these problems, such as filtering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unpro-cessed OCR output in the case of LDA.", "labels": [], "entities": []}, {"text": "To our knowledge, this study is the first of its kind.", "labels": [], "entities": []}], "introductionContent": [{"text": "As text data becomes available in massive quantities, it becomes increasingly difficult for human curators to manually catalog and index modern document collections.", "labels": [], "entities": []}, {"text": "To aid in the automation of such tasks, algorithms can be used to create models of the latent semantics present in a given corpus.", "labels": [], "entities": []}, {"text": "One example of this type of analysis is document clustering, in which documents are grouped into clusters by topic.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7213897556066513}]}, {"text": "Another type of topic analysis attempts to discover finer-grained topics-labeling individual words in a document as belonging to a particular topic.", "labels": [], "entities": [{"text": "topic analysis", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7997236549854279}]}, {"text": "This type of analysis has grown in popularity recently as inference on models containing large numbers of latent variables has become feasible.", "labels": [], "entities": []}, {"text": "The modern explosion of data includes vast amounts of historical documents, made available by means of Optical Character Recognition (OCR), which can introduce significant numbers of errors.", "labels": [], "entities": [{"text": "Optical Character Recognition (OCR)", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.7324021955331167}]}, {"text": "Undertakings to produce such data include the Google Books, Internet Archive, and HathiTrust projects.", "labels": [], "entities": [{"text": "Google Books, Internet Archive", "start_pos": 46, "end_pos": 76, "type": "DATASET", "confidence": 0.7440022647380828}]}, {"text": "In addition, researchers are having increasing levels of success in digitizing hand-written manuscripts, though error rates remain much higher than for OCR.", "labels": [], "entities": [{"text": "error", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9842188954353333}]}, {"text": "Due to their nature, these collections often lack helpful meta-data or labels.", "labels": [], "entities": []}, {"text": "In the absence of such labels, unsupervised machine learning methods can reveal patterns in the data.", "labels": [], "entities": []}, {"text": "Finding good estimates for the parameters of models such as the mixture of multinomials document model and the Latent Dirichlet Allocation (LDA) model ( requires accurate counts of the occurrences and co-occurrences of words.", "labels": [], "entities": []}, {"text": "Depending on the age of a document and the way in which it was created, the OCR process results in text containing many types of noise, including character-level errors, which distort these counts.", "labels": [], "entities": []}, {"text": "It is obvious, therefore, that model quality must suffer, especially since unsupervised methods are typically much more sensitive to noise than supervised methods.", "labels": [], "entities": []}, {"text": "Good supervised learning algorithms are substantially more immune to spurious patterns in the data created by noise for the following reason: under the mostly reasonable assumption that the process contributing the noise operates independently from the class labels, the noise in the features will not correlate well with the class labels, and the algorithm will learn to ignore those features arising from noise.", "labels": [], "entities": []}, {"text": "Unsupervised models, in contrast, have no grounding in labels to prevent them from confusing patterns that emerge by chance in the noise with the \"true\" patterns of potential interest.", "labels": [], "entities": []}, {"text": "For example, even on clean data, LDA will often do poorly if the very simple feature selection step of removing stop-words is not performed first.", "labels": [], "entities": []}, {"text": "Though we expect model quality to decrease, it is not well understood how sensitive these models are to OCR errors, or how quality deteriorates as the level of OCR noise increases.", "labels": [], "entities": []}, {"text": "In this work we show how the performance of unsupervised topic modeling algorithms degrades as character-level noise is introduced.", "labels": [], "entities": []}, {"text": "We demonstrate the effect using both artificially corrupted data and an existing real-world OCR corpus.", "labels": [], "entities": [{"text": "OCR corpus", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.7111875265836716}]}, {"text": "The results are promising, especially in the case of relatively low word error rates (e.g. less than 20%).", "labels": [], "entities": [{"text": "word error rates", "start_pos": 68, "end_pos": 84, "type": "METRIC", "confidence": 0.652747263511022}]}, {"text": "Though model quality declines as errors increase, simple feature selection techniques enable the learning of relatively high quality models even as word error rates approach 50%.", "labels": [], "entities": []}, {"text": "This result is particularly interesting in that even humans find it difficult to make sense of documents with error rates of that magnitude ().", "labels": [], "entities": []}, {"text": "Because of the difficulties in evaluating topic models, even on clean data, these results should not be interpreted as definitive answers, but they do offer insight into prominent trends.", "labels": [], "entities": []}, {"text": "For example, properties of the OCR data suggest measures that can betaken to improve performance in future work.", "labels": [], "entities": [{"text": "OCR data", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.8317070603370667}]}, {"text": "It is our hope that this work will lead to an increase in the usefulness of collections of OCRed texts, as document clustering and topic modeling expose useful patterns to historians and other interested parties.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.6918892860412598}, {"text": "topic modeling", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.7308617234230042}]}, {"text": "The remainder of the paper is outlined as follows.", "labels": [], "entities": []}, {"text": "After an overview of related work in Section 2, Section 3 introduces the data used in our experiments, including an explanation of how the synthetic data were created and of some of their properties.", "labels": [], "entities": []}, {"text": "Section 4 describes how the experiments were designed and carried out, and gives an analysis of the results both empirically and qualitatively.", "labels": [], "entities": []}, {"text": "Finally, conclusions and future work are presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran experiments on both the real and synthetic OCR data.", "labels": [], "entities": [{"text": "OCR data", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7771436274051666}]}, {"text": "In this section we explain our experi- mental methodology and present both empirical and qualitative analyses of the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of test dataset characteristics. |D| is  the number of documents in the dataset. K is the number  of human-labeled classes provided with the dataset.", "labels": [], "entities": []}, {"text": " Table 2: Top words for the five topics with the highest \u03b1  prior values found using MALLET for one run of LDA  on the uncorrupted Reuters data.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9688341617584229}, {"text": "Reuters data", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.9654941856861115}]}, {"text": " Table 3: Top words for the five topics with the highest \u03b1  prior values found using MALLET for one run of LDA  on the Reuters data corrupted with the data-derived noise  model to a WER of 45%.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9774633646011353}, {"text": "Reuters data corrupted", "start_pos": 119, "end_pos": 141, "type": "DATASET", "confidence": 0.9722666343053182}, {"text": "WER", "start_pos": 182, "end_pos": 185, "type": "METRIC", "confidence": 0.9991713762283325}]}]}