{"title": [{"text": "NLP on Spoken Documents without ASR", "labels": [], "entities": [{"text": "Spoken Documents without ASR", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.6861687898635864}]}], "abstractContent": [{"text": "There is considerable interest in interdis-ciplinary combinations of automatic speech recognition (ASR), machine learning, natural language processing, text classification and information retrieval.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.8335320949554443}, {"text": "text classification", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7988052070140839}, {"text": "information retrieval", "start_pos": 176, "end_pos": 197, "type": "TASK", "confidence": 0.8220616579055786}]}, {"text": "Many of these boxes, especially ASR, are often based on considerable linguistic resources.", "labels": [], "entities": [{"text": "ASR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9474956393241882}]}, {"text": "We would like to be able to process spoken documents with few (if any) resources.", "labels": [], "entities": []}, {"text": "Moreover, connecting black boxes in series tends to multiply errors , especially when the key terms are out-of-vocabulary (OOV).", "labels": [], "entities": []}, {"text": "The proposed alternative applies text processing directly to the speech without a dependency on ASR.", "labels": [], "entities": []}, {"text": "The method finds long (\u223c 1 sec) repetitions in speech, and clusters them into pseudo-terms (roughly phrases).", "labels": [], "entities": []}, {"text": "Document clustering and classification work surprisingly well on pseudo-terms; performance on a Switchboard task approaches a baseline using gold standard manual transcriptions.", "labels": [], "entities": [{"text": "Document clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8978637158870697}]}], "introductionContent": [{"text": "Can we do IR-like tasks without ASR?", "labels": [], "entities": []}, {"text": "Information retrieval (IR) typically makes use of simple features that count terms within/across documents such as term frequency (tf) and inverse document frequency (IDF).", "labels": [], "entities": [{"text": "Information retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8655362904071808}, {"text": "term frequency (tf)", "start_pos": 115, "end_pos": 134, "type": "METRIC", "confidence": 0.8590715885162353}, {"text": "inverse document frequency (IDF)", "start_pos": 139, "end_pos": 171, "type": "METRIC", "confidence": 0.8723249733448029}]}, {"text": "Crucially, to compute these features, it is sufficient to count repetitions of a term.", "labels": [], "entities": []}, {"text": "In particular, for many IR-like tasks, there is no need for an automatic speech recognition (ASR) system to label terms with phonemes and/or words.", "labels": [], "entities": [{"text": "IR-like tasks", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.918260931968689}, {"text": "automatic speech recognition (ASR)", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.7769880493481954}]}, {"text": "This paper builds on, a method for discovering terms with zero resources.", "labels": [], "entities": []}, {"text": "This approach identifies long, faithfully repeated patterns in the acoustic signal.", "labels": [], "entities": []}, {"text": "These acoustic repetitions often correspond to terms useful for information retrieval tasks.", "labels": [], "entities": [{"text": "information retrieval tasks", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.8358529408772787}]}, {"text": "Critically, this method does not require a phonetically interpretable acoustic model or knowledge of the target language.", "labels": [], "entities": []}, {"text": "By analyzing a large untranscribed corpus of speech, this discovery procedure identifies avast number of repeated regions that are subsequently grouped using a simple graph-based clustering method.", "labels": [], "entities": []}, {"text": "We call the resulting groups pseudo-terms since they typically represent a single word or phrase spoken at multiple points throughout the corpus.", "labels": [], "entities": []}, {"text": "Each pseudo-term takes the place of a word or phrase in bag of terms vector space model of a text document, allowing us to apply standard NLP algorithms.", "labels": [], "entities": []}, {"text": "We show that despite the fully automated and noisy method by which the pseudo-terms are created, we can still successfully apply NLP algorithms with performance approaching that achieved with the gold standard manual transcription.", "labels": [], "entities": []}, {"text": "Natural language processing tools can play a key role in understanding text document collections.", "labels": [], "entities": [{"text": "understanding text document collections", "start_pos": 57, "end_pos": 96, "type": "TASK", "confidence": 0.8334211111068726}]}, {"text": "Given a large collection of text, NLP tools can classify documents by category (classification) and organize documents into similar groups fora high level view of the collection (clustering).", "labels": [], "entities": []}, {"text": "For example, given a collection of news articles, these tools can be applied so that the user can quickly seethe topics covered in the news articles, and organize the collection to find all articles on a given topic.", "labels": [], "entities": []}, {"text": "These tools require little or no human input (annotation) and work across languages.", "labels": [], "entities": []}, {"text": "Given a large collection of speech, we would like tools that perform many of the same tasks, allowing the user to understand the contents of the collection while listening to only small portions of the audio.", "labels": [], "entities": []}, {"text": "Previous work has applied these NLP tools to speech corpora with similar results (see and the references therein.)", "labels": [], "entities": []}, {"text": "However, unlike text, which requires little or no preprocessing, audio files are typically first transcribed into text before applying standard NLP tools.", "labels": [], "entities": []}, {"text": "Automatic speech recognition (ASR) solutions, such as large vocabulary continuous speech recognition (LVCSR) systems, can produce an automatic transcript from speech, but they require significant development efforts and training resources, typically hundreds of hours of manually transcribed speech.", "labels": [], "entities": [{"text": "Automatic speech recognition (ASR)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7968904475371043}, {"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 54, "end_pos": 108, "type": "TASK", "confidence": 0.7654782012104988}]}, {"text": "Moreover, the terms that maybe most distinctive in particular spoken documents often lie outside the predefined vocabulary of an off-the-shelf LVCSR system.", "labels": [], "entities": []}, {"text": "This means that unlike with text, where many tools can be applied to new languages and domains with minimal effort, the equivalent tools for speech corpora often require a significant investment.", "labels": [], "entities": []}, {"text": "This greatly raises the entry threshold for constructing even a minimal tool set for speech corpora analysis.", "labels": [], "entities": [{"text": "entry", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9823703765869141}, {"text": "speech corpora analysis", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.6723419825236002}]}, {"text": "The paper proceeds as follows.", "labels": [], "entities": []}, {"text": "After a review of related work, we describe, a method for finding repetitions in speech.", "labels": [], "entities": []}, {"text": "We then explain how these repetitions are grouped into pseudo-terms.", "labels": [], "entities": []}, {"text": "Document clustering and classification work surprisingly well on pseudo-terms; performance on a Switchboard task approaches a baseline based on gold standard manual transcriptions.", "labels": [], "entities": [{"text": "Document clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8951251804828644}]}], "datasetContent": [{"text": "There are numerous approaches to evaluating clustering algorithms.", "labels": [], "entities": []}, {"text": "We consider several methods: Purity, Entropy and B-Cubed.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9385582208633423}]}, {"text": "For a full treatment of these metrics, see.", "labels": [], "entities": []}, {"text": "Purity measures the precision of each cluster, i.e., how many examples in each cluster belong to the same true topic.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9721011519432068}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9986153841018677}]}, {"text": "Purity ranges between zero and one, with one being optimal.", "labels": [], "entities": []}, {"text": "While optimal purity can be obtained by putting each document in its own cluster, we fix the number of clusters in all experiments so purity numbers are comparable.", "labels": [], "entities": [{"text": "purity", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9638124108314514}]}, {"text": "The purity of a cluster is defined as the largest percentage of examples in a cluster that have the same topic label.", "labels": [], "entities": [{"text": "purity", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9856352806091309}]}, {"text": "Purity of the entire clustering is the average purity of each cluster: where C is the clustering, L is the reference labeling, and N are the number of examples.", "labels": [], "entities": []}, {"text": "Following this notation, c i is a specific cluster and l j is a specific true label.", "labels": [], "entities": []}, {"text": "Entropy measures how the members of a cluster are distributed amongst the true labels.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8136303424835205}]}, {"text": "The global metric is computed by taking the weighted average of the entropy of the members of each cluster.", "labels": [], "entities": []}, {"text": "Specifically, entropy(C, L) is given by: where Ni is the number of instances in cluster i, P (c i , l j ) is the probability of seeing label l j in cluster c i and the other variables are defined as above.", "labels": [], "entities": []}, {"text": "B-Cubed measures clustering effectiveness from the perspective of a user's inspecting the clustering results ().", "labels": [], "entities": [{"text": "B-Cubed", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8513849377632141}]}, {"text": "B-Cubed precision can be defined as an algorithm as follows: suppose a user randomly selects a single example.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.5402163863182068}]}, {"text": "She then proceeds to inspect every other example that occurs in the same cluster.", "labels": [], "entities": []}, {"text": "How many of these items will have the same true label as the selected example (precision)?", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9989017248153687}]}, {"text": "B-Cubed recall operates in a similar fashion, but it measures what percentage of all examples that share the same label as the selected example will appear in the selected cluster.", "labels": [], "entities": [{"text": "B-Cubed", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7714391946792603}, {"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.7324670553207397}]}, {"text": "Since BCubed averages its evaluation over each document and not each cluster, it is less sensitive to small errors in large clusters as opposed to many small errors in small clusters.", "labels": [], "entities": []}, {"text": "We include results for B-Cubed F1, the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "B-Cubed F1", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.8092910051345825}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9993798732757568}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9987378716468811}]}], "tableCaptions": [{"text": " Table 2: Statistics on the number of features (pseudo- terms) generated for different settings of the match dura- tion \u03ba and the overlap threshold \u03c4 .", "labels": [], "entities": []}, {"text": " Table 3: Clustering results on development data using  globally optimal repeated bisection and I 2 criteria. The  best results over the manual word transcript baselines  and for each match duration (\u03ba) are highlighted in bold.  Pseudo-term results are better than the phonetic baseline  and almost as good as the transcript baseline.", "labels": [], "entities": [{"text": "match duration (\u03ba)", "start_pos": 184, "end_pos": 202, "type": "METRIC", "confidence": 0.830115556716919}]}, {"text": " Table 4: Results on held out tuning data. The parameters  (globally optimal repeated bisection clustering with I 2  criteria, \u03ba = 0.70 seconds and \u03c4 = 0.98) were selected  using the development data and validated on tuning data.  Note that the clusters produced by each manual transcript  test were identical in this case.", "labels": [], "entities": []}, {"text": " Table 5: Results on evaluation data. The parameters  (globally optimal repeated bisection clustering with I 2  criteria, \u03ba = 0.7 seconds and \u03c4 = 0.98) were selected  using the development data and validated on tuning data.", "labels": [], "entities": []}, {"text": " Table 6: The top 15 results (measured as average accu- racy across the 4 algorithms) for pseudo-terms on de- velopment data. The best pseudo-term and manual tran- script results for each algorithm are bolded. All results  are based on 10-fold cross validation. Pseudo-term re- sults are better than the phonetic baseline and almost as  good as the transcript baseline.", "labels": [], "entities": []}, {"text": " Table 7: Results on held out tuning data. The parameters  (\u03ba = 0.75 seconds and \u03c4 = 0.97) were selected using the  development data and validated on tuning data. All re- sults are based on 10-fold cross validation. Pseudo-term  results are very close to the transcript baseline and often  better than the phonetic baseline.", "labels": [], "entities": []}, {"text": " Table 8: Results on evaluation data. The parameters  (\u03ba = 0.75 seconds and \u03c4 = 0.97) were selected using the  development data and validated on tuning data. All re- sults are based on 10-fold cross validation. Pseudo-term  results are very close to the transcript baseline and often  better than the phonetic baseline.", "labels": [], "entities": []}]}