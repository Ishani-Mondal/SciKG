{"title": [{"text": "A Simple Domain-Independent Probabilistic Approach to Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework.", "labels": [], "entities": [{"text": "content selection and surface realization", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.7524255871772766}]}, {"text": "In our approach, we breakup the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively.", "labels": [], "entities": []}, {"text": "We deployed our system in three different domains-Robocup sportscasting, technical weather forecasts, and common weather forecasts , obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 223, "end_pos": 234, "type": "METRIC", "confidence": 0.9730073511600494}]}], "introductionContent": [{"text": "In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records.", "labels": [], "entities": []}, {"text": "While existing generation systems can be engineered to obtain good performance on particular domains (e.g.,,,,, inter alia), it is often difficult to adapt them across different domains.", "labels": [], "entities": []}, {"text": "Furthermore, content selection (what to say: see,, inter alia) and surface realization (how to say it: see,,,, etc.) are typically handled separately.", "labels": [], "entities": [{"text": "content selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7290311455726624}, {"text": "surface realization", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7646914422512054}]}, {"text": "Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework.", "labels": [], "entities": [{"text": "content selection", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.701774537563324}]}, {"text": "We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output).", "labels": [], "entities": []}, {"text": "We use the model of to automatically induce the correspondences between words in the text and the actual database records mentioned.", "labels": [], "entities": []}, {"text": "We breakup the full generation process into a sequence of local decisions, training a log-linear classifier for each type of decision.", "labels": [], "entities": []}, {"text": "We use a simple but expressive set of domain-independent features, where each decision is allowed to depend on the entire history of previous decisions, as in the model of.", "labels": [], "entities": []}, {"text": "These long-range contextual dependencies turnout to be critical for accurate generation.", "labels": [], "entities": []}, {"text": "More specifically, our model is defined in terms of three types of decisions.", "labels": [], "entities": []}, {"text": "The first type chooses records from the database (macro content selection)-for example, wind speed, in the case of generating weather forecasts.", "labels": [], "entities": []}, {"text": "The second type chooses a subset of fields from a record (micro content selection)-e.g., the minimum and maximum temperature.", "labels": [], "entities": []}, {"text": "The third type chooses a suitable template to render the content (surface realization)-e.g., winds between and mph; templates are automatically extracted from training data.", "labels": [], "entities": []}, {"text": "We tested our approach in three domains: ROBOCUP, for sportscasting; SUMTIME, for technical weather forecast generation (); and WEATHERGOV, for common weather forecast generation ().", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9749396443367004}, {"text": "SUMTIME", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.8303788304328918}, {"text": "technical weather forecast generation", "start_pos": 82, "end_pos": 119, "type": "TASK", "confidence": 0.6401271745562553}, {"text": "common weather forecast generation", "start_pos": 144, "end_pos": 178, "type": "TASK", "confidence": 0.7704652696847916}]}, {"text": "We performed both automatic (BLEU) and human evaluation.", "labels": [], "entities": [{"text": "automatic", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9813434481620789}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.6830130219459534}]}, {"text": "On WEATHERGOV, we s: pass(arg1=purple6, arg2=purple3) kick(arg1=purple3) badPass(arg1=purple3,arg2=pink9) turnover(arg1=purple3,arg2=pink9) w: purple3 made a bad pass that was picked off by pink9 (a) Robocup s: temperature(time=5pm-6am,min=48,mean=53,max=61) windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10) windDir(time=5pm-6am,mode=SSW) gust(time=5pm-6am,min=0,mean=0,max=0) skyCover(time=5pm-9pm,mode=0-25) skyCover(time=2am-6am,mode=75-100) precipPotential(time=5pm-6am,min=2,mean=14,max=20) rainChance(time=5pm-6am,mode=someChance) w: a 20 percent chance of showers after midnight . increasing clouds , with a low around 48 southwest wind between 5 and 10 mph (b) WeatherGov s: wind10m(time=6am,dir=SW,min=16,max=20,gust min=0,gust max=-) wind10m(time=9pm,dir=SSW,min=28,max=32,gust min=40,gust max=-) wind10m(time=12am,dir=-,min=24,max=28,gust min=36,gust max=-) w: sw 16 -20 backing ssw 28 -32 gusts 40 by mid evening easing 24 -28 gusts 36 late evening (c) SumTime: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains.", "labels": [], "entities": [{"text": "WEATHERGOV", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.9018899202346802}]}, {"text": "Each row in the world state denotes a record.", "labels": [], "entities": []}, {"text": "Our generation task is to map a world state s (input) to a text w (output).", "labels": [], "entities": []}, {"text": "Note that this mapping involves both content selection and surface realization.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.796277791261673}]}, {"text": "achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9992721676826477}, {"text": "content selection and generation", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.7026353627443314}]}, {"text": "On ROBOCUP and SUMTIME, we achieved results comparable to the state-of-the-art.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.8497275114059448}]}, {"text": "most importantly, we obtained these results with a general-purpose approach that we believe is simpler than current state-of-the-art systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present an empirical evaluation of our system on our three domains-ROBOCUP, SUMTIME, and WEATHERGOV.", "labels": [], "entities": [{"text": "SUMTIME", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.4522654712200165}]}, {"text": "Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score () (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7678927183151245}, {"text": "surface realization", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7827910482883453}, {"text": "BLEU score", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9894867539405823}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9964080452919006}]}, {"text": "To evaluate macro content selection, we measured the F 1 score (the harmonic mean of precision and recall) of the set of records chosen with respect to the human-annotated set of records.", "labels": [], "entities": [{"text": "macro content selection", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7178148229916891}, {"text": "F 1 score", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9905041257540385}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9421592354774475}, {"text": "recall)", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9780713617801666}]}, {"text": "Human Evaluation We conducted a human evaluation using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.9647814432779948}]}, {"text": "For each domain, we chose 100 scenarios randomly from the test set.", "labels": [], "entities": []}, {"text": "We ran each system under consideration on each of these scenarios, and presented each resulting output to 10 evaluators.", "labels": [], "entities": []}, {"text": "Evaluators were given instructions to rank an output on the basis of English fluency and semantic correctness on the following scale: 2 To minimize bias, we evaluated all the systems at once, randomly shuffling the outputs of the systems.", "labels": [], "entities": []}, {"text": "The evaluators were not necessarily the same 10 evaluators.", "labels": [], "entities": []}, {"text": "Evaluators were also given additional domainspecific information: (1) the background of the domain (e.g., that SUMTIME reports are technical weather reports); (2) general properties of the desired output (e.g., that SUMTIME texts should mention every record whereas WEATHERGOV texts need not); and (3) peculiarities of the text (e.g., the suffix ly in SUMTIME should exist as a separate token from its stem, or that pink goalie and pink1 have the same meaning in ROBOCUP).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROBOCUP results. WASPER-GEN is described in", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.941391110420227}, {"text": "WASPER-GEN", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.4523024260997772}]}, {"text": " Table 2: SUMTIME results. The SUMTIME-Hybrid system  is described in", "labels": [], "entities": []}, {"text": " Table 3: WEATHERGOV results. The BLEU score is on joint  content selection and surface realization and is modified to not  penalize numeric deviations of at most 5.", "labels": [], "entities": [{"text": "WEATHERGOV", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.5513410568237305}, {"text": "BLEU score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.970909833908081}, {"text": "surface realization", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7393480539321899}]}]}