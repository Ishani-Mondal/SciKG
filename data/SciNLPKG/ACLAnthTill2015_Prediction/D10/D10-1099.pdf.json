{"title": [{"text": "Collective Cross-Document Relation Extraction Without Labelled Data", "labels": [], "entities": [{"text": "Cross-Document Relation Extraction", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.7602587540944418}]}], "abstractContent": [{"text": "We present a novel approach to relation extraction that integrates information across documents , performs global inference and requires no labelled text.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9240296483039856}]}, {"text": "In particular, we tackle relation extraction and entity identification jointly.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.9062348008155823}, {"text": "entity identification", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8238515257835388}]}, {"text": "We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.9143057763576508}, {"text": "Freebase", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9458402395248413}]}, {"text": "For inference we run an efficient Gibbs sam-pler that leads to linear time joint inference.", "labels": [], "entities": []}, {"text": "We evaluate our approach both for an in-domain (Wikipedia) and a more realistic out-of-domain (New York Times Corpus) setting.", "labels": [], "entities": [{"text": "New York Times Corpus)", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.8524598121643067}]}, {"text": "For the in-domain setting, our joint model leads to 4% higher precision than an isolated local approach, but has no advantage over a pipeline.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.998786985874176}]}, {"text": "For the out-of-domain data, we benefit strongly from joint modelling, and observe improvements in precision of 13% over the pipeline, and 15% over the isolated baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.999476969242096}]}], "introductionContent": [{"text": "Relation Extraction is the task of predicting semantic relations over entities expressed in structured or semi-structured text.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.940685898065567}, {"text": "predicting semantic relations over entities expressed in structured or semi-structured text", "start_pos": 35, "end_pos": 126, "type": "TASK", "confidence": 0.8342555978081443}]}, {"text": "This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers.", "labels": [], "entities": [{"text": "extraction of employer-employee relations", "start_pos": 32, "end_pos": 73, "type": "TASK", "confidence": 0.8506063520908356}]}, {"text": "It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations.", "labels": [], "entities": [{"text": "prediction of entity types", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8540996462106705}]}, {"text": "A particularly attractive approach to relation extraction is based on distant supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.9611079692840576}]}, {"text": "1 Here in Also called self training, or weak supervision.", "labels": [], "entities": []}, {"text": "place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor ().", "labels": [], "entities": []}, {"text": "The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor.", "labels": [], "entities": []}, {"text": "Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one.", "labels": [], "entities": []}, {"text": "While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning.", "labels": [], "entities": []}, {"text": "When applied to unseen text, these patterns will produce noisy facts.", "labels": [], "entities": []}, {"text": "Indeed, we find that extraction precision still leaves much room for improvement.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9602303504943848}]}, {"text": "This room is not as large as in previous work) where target text and training KB are closely related.", "labels": [], "entities": []}, {"text": "However, when we use the knowledge base) and the New York Times corpus, we observe very low precision.", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.9054744094610214}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.997017502784729}]}, {"text": "For example, the precision of the top-ranked 50 nationality relation instances is only 28%.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9994526505470276}]}, {"text": "On inspection, it turns out that many of the errors can be easily identified: they amount to violations of basic compatibility constraints between facts.", "labels": [], "entities": []}, {"text": "In particular, we observe unsatisfied selectional preferences of relations towards particular entity types as types of their arguments.", "labels": [], "entities": []}, {"text": "An example is the fact that the first argument of nationality is always a person while the second is a country.", "labels": [], "entities": []}, {"text": "A simple way to address this is a pipeline: first predict entity types, and then condition on these when predicting relations.", "labels": [], "entities": []}, {"text": "However, this neglects the fact that relations could as well be used to help entity type prediction.", "labels": [], "entities": [{"text": "entity type prediction", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.716005822022756}]}, {"text": "While there is some existing work on enforcing such constraints in a joint fashion), they are not directly applicable here.", "labels": [], "entities": []}, {"text": "The difference is the amount of facts they take into account at the same time.", "labels": [], "entities": []}, {"text": "They focus on single sentence extractions, and only consider very few interacting facts.", "labels": [], "entities": [{"text": "single sentence extractions", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.6865310271581014}]}, {"text": "This allows them to work with exact optimization techniques such as (Integer) Linear Programs and still remain efficient.", "labels": [], "entities": []}, {"text": "However, when working on a sentence level they fail to exploit the redundancy present in a corpus.", "labels": [], "entities": []}, {"text": "Moreover, the fewer facts they consider at the same time, the lower the chance that some of these will be incompatible, and that modelling compatibility will make a difference.", "labels": [], "entities": []}, {"text": "In this work we present a novel approach that performs relation extraction across documents, enforces selectional preferences, and needs no labelled data.", "labels": [], "entities": [{"text": "relation extraction across documents", "start_pos": 55, "end_pos": 91, "type": "TASK", "confidence": 0.8608458042144775}]}, {"text": "It is based on an undirected graphical model in which variables correspond to facts, and factors between them measure compatibility.", "labels": [], "entities": []}, {"text": "In order to scale up, we run an efficient Gibbs-Sampler at inference time, and train our model using SampleRank ().", "labels": [], "entities": []}, {"text": "In practice this leads to a runtime behaviour that is linear in the size of the corpus.", "labels": [], "entities": []}, {"text": "For example, 200,000 documents take less than three hours for training and testing.", "labels": [], "entities": []}, {"text": "For evaluation we consider two scenarios.", "labels": [], "entities": []}, {"text": "First we follow, use Freebase as source of distant supervision, and employ Wikipedia as source of unlabelled text-we will call this an in-domain setting.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.9293527007102966}]}, {"text": "This scenario is somewhat artificial in that Freebase itself is partially derived from Wikipedia, and in practice we cannot expect text and training knowledge base to be so close.", "labels": [], "entities": []}, {"text": "Hence we also evaluate our approach on the New York Times corpus (out-of-domain setting).", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.900217279791832}]}, {"text": "For in-domain data we make the following finding.", "labels": [], "entities": []}, {"text": "When we compare to an isolated baseline that makes no use of entity types, our joint model improves average precision by 4%.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9703448414802551}]}, {"text": "However, it does not outperform a pipelined system.", "labels": [], "entities": []}, {"text": "In the out-ofdomain setting, our collective model substantially outperforms both other approaches.", "labels": [], "entities": []}, {"text": "Compared to the isolated baseline, we achieve a 15% increase in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9995822310447693}]}, {"text": "With respect to the pipeline approach, the increase is 13%.", "labels": [], "entities": []}, {"text": "In the following we will first give some background information on relation extraction with distant supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.9023423492908478}]}, {"text": "Then we will present our graphical model as well as the inference and learning techniques we apply.", "labels": [], "entities": []}, {"text": "After discussing related work, we present our empirical results and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "We setup experiments to answer the following questions: (i) Does the explicit modelling of selectional preferences improve accuracy?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9978864789009094}]}, {"text": "(ii) Can we also perform joint entity and relation extraction in a pipeline and achieve similar results?", "labels": [], "entities": [{"text": "joint entity and relation extraction", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.6493836522102356}]}, {"text": "(iii) How does our cross-document approach scale?", "labels": [], "entities": []}, {"text": "To answer these questions we carryout experiments on two data sets, Wikipedia and New York Times articles, and use Freebase as distant supervision source for both.", "labels": [], "entities": []}, {"text": "We follow and perform two types of evaluation: held-out and manual.", "labels": [], "entities": []}, {"text": "In both cases we have a training and a test corpus of documents, and training and test sets of entities.", "labels": [], "entities": []}, {"text": "For held-out evaluation we split the set of entities in Freebase into training and test sets.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9801554083824158}]}, {"text": "For manual evaluation we use all Freebase entities during training.", "labels": [], "entities": []}, {"text": "For testing we use all entities that appear in the test document corpus.", "labels": [], "entities": []}, {"text": "For both training and testing we then choose the candidate tuples C that mayor may not be relation instances.", "labels": [], "entities": []}, {"text": "To pick the entities C 1 we want to predict entity types for, we choose all entities that are mentioned at least once in the train/test corpus.", "labels": [], "entities": []}, {"text": "To pick the entity pairs C 2 that we want to predict the relations of, we choose those that appear at least once together in a sentence.", "labels": [], "entities": []}, {"text": "The set of candidates C will contain many tuples which are not related in any Freebase relations.", "labels": [], "entities": []}, {"text": "For efficiency, we filter out a large fraction of these negative candidates for training.", "labels": [], "entities": []}, {"text": "The number of negative examples we keep is chosen to be about 10 times the number of positive candidates.", "labels": [], "entities": []}, {"text": "This number stems from trading-off the accuracy it leads to and the increased training time it requires.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9994714856147766}]}, {"text": "For both manual and held-out evaluation we rank extracted test relation instances in the MAP state of the network.", "labels": [], "entities": []}, {"text": "This state is found by sampling 20 iterations with a low temperature of 0.00001.", "labels": [], "entities": []}, {"text": "The ranking is done according to the log linear score that the assigned relation fora candidate tuple gets from the factors in its Markov Blanket.", "labels": [], "entities": []}, {"text": "For optimal performance, the score is normalized by the number of relation mentions.", "labels": [], "entities": []}, {"text": "For manual evaluation we pick the top ranked 50 relation instances for the most frequent relations.", "labels": [], "entities": []}, {"text": "We ask three annotators to inspect the mentions of these relation instances to decide whether they are correct.", "labels": [], "entities": []}, {"text": "Upon disagreement, we use majority vote.", "labels": [], "entities": []}, {"text": "To summarize precisions across relations, we take their average, and their average weighted by the proportion of predicted instances for the given relation.", "labels": [], "entities": [{"text": "precisions", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9145522713661194}]}, {"text": "We split 1,300,000 Wikipedia articles into training and test sets.", "labels": [], "entities": []}, {"text": "shows the statistics for this split.", "labels": [], "entities": []}, {"text": "The last row provides the number of negative relation instances (candidates which are not related according to Freebase) associated with each data set.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.9553971886634827}]}, {"text": "shows the precision-recall curves of relation extraction for held-out data of various configurations.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9960924983024597}, {"text": "relation extraction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7800162732601166}]}, {"text": "We notice a slight advantage of the joint approach in the low recall area.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9979802966117859}]}, {"text": "Moreover, the joint model predicts more relation instances, as can be seen by its longer line in the graph.", "labels": [], "entities": []}, {"text": "For higher recall, the joint model performs slightly worse.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9997610449790955}]}, {"text": "On closer inspection, we find that Wikipedia NYT 219K 590K 64K 94K  this observation is somewhat misleading.", "labels": [], "entities": [{"text": "Wikipedia NYT 219K 590K 64K 94K", "start_pos": 35, "end_pos": 66, "type": "DATASET", "confidence": 0.8701449235280355}]}, {"text": "Many of the predictions of the joint model are not in the held-out test set derived from Freebase, but nevertheless correct.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9748300909996033}]}, {"text": "Hence, to understand if one system really outperforms another, we need to rely on manual evaluation.", "labels": [], "entities": []}, {"text": "Note that the figure only considers binary relations-for entity types all configurations perform similarly.", "labels": [], "entities": []}, {"text": "Manually evaluated precision for New York Times data can be seen in table 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9994427561759949}, {"text": "New York Times data", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.7821984589099884}]}, {"text": "In contrast to the Wiki setting, here modelling entity types and relations jointly makes a substantial difference.", "labels": [], "entities": []}, {"text": "For average precision, our joint model improves over the isolated baseline by 15%, and over the pipeline by 13%.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994400143623352}]}, {"text": "Similar improvements can be observed for weighted average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.8054752945899963}]}, {"text": "Let us look at a break-down of precisions with respect to different relations shown in table 3.", "labels": [], "entities": [{"text": "precisions", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9947356581687927}]}, {"text": "We see dramatic improvements for nationality and founded when applying the joint model.", "labels": [], "entities": [{"text": "founded", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9770535826683044}]}, {"text": "Note that the nationality relation takes a larger part in the predicted relation instances of the joint model and hence contributes significantly to the weighted average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.5448909997940063}]}, {"text": "We choose all articles of the New York times during 2005 and 2006 as training corpus.", "labels": [], "entities": [{"text": "New York times during 2005", "start_pos": 30, "end_pos": 56, "type": "DATASET", "confidence": 0.8589413046836853}]}, {"text": "As test corpus we use the first 6 months of 2007.", "labels": [], "entities": []}, {"text": "shows precision-recall curves for our various setups.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 6, "end_pos": 22, "type": "METRIC", "confidence": 0.9985617995262146}]}, {"text": "We see that jointly modelling entity types and relations helps to improve precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.997643768787384}]}, {"text": "Due to the smaller overlap between Freebase and NYT data, figure 3 also has to betaken with more caution.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9655988216400146}, {"text": "NYT data", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.8623564839363098}]}, {"text": "The systems may predict correct relation instances that just do not appear in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9480097889900208}]}, {"text": "Hence manual evaluation is even more important.", "labels": [], "entities": []}, {"text": "When evaluating entity precision we find that for both models it is about 84%.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.959801435470581}]}, {"text": "This raises the question why the joint entity type and relation extraction model outperforms the pipeline on relations.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7258603423833847}]}, {"text": "We take a close look at the entities which participate in relations and find that joint model performs better on most entity types, for example, country and citytown.", "labels": [], "entities": []}, {"text": "We also look at the relation instances which are predicted by both systems and find that the joint model does predict correct entity types when the pipeline mis-predicts.", "labels": [], "entities": []}, {"text": "And exactly these mis-predictions lead the pipeline astray.", "labels": [], "entities": []}, {"text": "Considering binary relation instances where the pipeline fails but the joint model does not, we observe an entity precision of 76% for the pipeline and 86% for our joint approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.7934480905532837}]}, {"text": "The joint model fails to correctly predict some entity types that the pipeline gets right, but these tend to appear in contexts where relation instances are easy to extract without considering en-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of held-out evaluation on  Wikipedia and New York Times.", "labels": [], "entities": [{"text": "New York Times", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.7539992729822794}]}, {"text": " Table 2: Average and weighted (w) average preci- sion over frequent relations for New York Times and  Wikipedia data, based on manual evaluation.", "labels": [], "entities": [{"text": "weighted (w) average preci- sion", "start_pos": 22, "end_pos": 54, "type": "METRIC", "confidence": 0.6937342584133148}, {"text": "New York Times and  Wikipedia data", "start_pos": 83, "end_pos": 117, "type": "DATASET", "confidence": 0.7708147068818411}]}, {"text": " Table 3: Precision at 50 for the most frequent rela- tions on New York Times", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.99391770362854}, {"text": "New York Times", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.7676559686660767}]}]}