{"title": [{"text": "Assessing Phrase-Based Translation Models with Oracle Decoding 1 Phrase-Based Machine Translation 1.1 Principle", "labels": [], "entities": [{"text": "Assessing Phrase-Based Translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.648333340883255}, {"text": "Phrase-Based Machine Translation", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.6119810938835144}]}], "abstractContent": [{"text": "Extant Statistical Machine Translation (SMT) systems are very complex softwares, which embed multiple layers of heuristics and embark very large numbers of numerical parameters.", "labels": [], "entities": [{"text": "Extant Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7828730472496578}]}, {"text": "As a result, it is difficult to analyze output translations and there is areal need for tools that could help developers to better understand the various causes of errors.", "labels": [], "entities": []}, {"text": "In this study, we make a step in that direction and present an attempt to evaluate the quality of the phrase-based translation model.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.7185926735401154}]}, {"text": "In order to identify those translation errors that stem from deficiencies in the phrase table (PT), we propose to compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 133, "end_pos": 145, "type": "METRIC", "confidence": 0.9800091981887817}]}, {"text": "By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks.", "labels": [], "entities": [{"text": "BLEU-1", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9725270867347717}]}, {"text": "Various other applications of these oracle decoding techniques are also reported and discussed.", "labels": [], "entities": []}, {"text": "A Phrase-Based Translation System (PBTS) consists of a ruleset and a scoring function (Lopez, 2009).", "labels": [], "entities": [{"text": "Phrase-Based Translation System (PBTS)", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.7589748253424963}]}, {"text": "The ruleset, represented in the phrase table, is a set of phrase 1 pairs {(f, e)}, each pair expressing that the source phrase f can be rewritten (translated) into a target phrase e.", "labels": [], "entities": []}, {"text": "Translation hypotheses are generated by iteratively rewriting portions of the source sentence as prescribed by the rule-set, until each source word has been consumed by exactly one rule.", "labels": [], "entities": [{"text": "Translation hypotheses", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8859785795211792}]}, {"text": "The order of target words in an hypothesis is uniquely determined by the order in which the rewrite operation are performed.", "labels": [], "entities": []}, {"text": "The search space of the translation model corresponds to the set of all possible sequences of 1 Following the usage in statistical machine translation literature, we use \"phrase\" to denote a subsequence of consecutive words.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.6170512437820435}]}, {"text": "The scoring function aims to rank all possible translation hypotheses in such away that the best one has the highest score.", "labels": [], "entities": []}, {"text": "A PBTS is learned from a parallel corpus in two independent steps.", "labels": [], "entities": []}, {"text": "Ina first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuris-tics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights.", "labels": [], "entities": []}, {"text": "In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003).", "labels": [], "entities": [{"text": "Minimum Error Rate training", "start_pos": 92, "end_pos": 119, "type": "METRIC", "confidence": 0.8832016587257385}]}, {"text": "Translating a sentence amounts to finding the best scoring translation hypothesis in the search space.", "labels": [], "entities": []}, {"text": "Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004).", "labels": [], "entities": [{"text": "translation", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9886077046394348}]}, {"text": "Moreover , to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics.", "labels": [], "entities": []}, {"text": "For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence 2 and enforces a distortion limit 3 over which phrases can be reordered.", "labels": [], "entities": []}, {"text": "As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score.", "labels": [], "entities": []}, {"text": "1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes.", "labels": [], "entities": [{"text": "Typology", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.93541020154953}, {"text": "PBTS Errors Analyzing the", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.8040013611316681}, {"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9469017386436462}]}, {"text": "For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et al., 2009): search errors, induction errors and model errors.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.992618978023529}]}, {"text": "The former corresponds to cases where the hypothesis with the best score is missed by the search procedure, either because of the use of an ap-2 the ttl option of Moses, defaulting to 20. 3 the dl option of Moses, whose default value is 7. 933", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We propose to use our oracle decoder to study the ability of a PBTS to translate from English to French and from German to English.", "labels": [], "entities": []}, {"text": "These two languages pairs present different challenges: English to French translation is considered a relatively easy pair, notwithstanding the difficulties of generating the right inflection marks in French.", "labels": [], "entities": [{"text": "English to French translation", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.6479218751192093}]}, {"text": "Translating from German into English is more difficult, notably due to the productivity of inflectional and compounding processes in German, and also to significant differences in word ordering between these languages.", "labels": [], "entities": [{"text": "Translating from German", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9174558321634928}]}, {"text": "Our experiments are based on the corpora distributed for the WMT'09 constrained tasks).", "labels": [], "entities": [{"text": "WMT'09", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.7120275497436523}]}, {"text": "All data are tokenized, cleaned and converted to lowercase letters using the tools provided by the organizers.", "labels": [], "entities": []}, {"text": "We then used a standard training pipeline to construct the translation model: the bitexts were aligned using Giza++ , symmetrized using the grow-diag-final-and heuristic; the phrase table was extracted and scored using the tools distributed with Moses.", "labels": [], "entities": []}, {"text": "Finally, baseline systems were optimized using WMT'08 test set as development using MERT.", "labels": [], "entities": [{"text": "WMT'08 test set", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9794140060742696}, {"text": "MERT", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.7685588002204895}]}, {"text": "Note that, for all these steps, we used the default value of the various parameters.", "labels": [], "entities": []}, {"text": "The extracted phrase table is then used to find the oracle alignment on the task test set.", "labels": [], "entities": []}, {"text": "Recall that oracle decoding do not use the scores estimated by Moses in anyway.", "labels": [], "entities": []}, {"text": "In the experiments reported below, two settings are considered.", "labels": [], "entities": []}, {"text": "In the first one, denoted NEWSCO, Moses was trained only on a small data set taken from the News Commentary corpus.", "labels": [], "entities": [{"text": "NEWSCO", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.7198134064674377}, {"text": "News Commentary corpus", "start_pos": 92, "end_pos": 114, "type": "DATASET", "confidence": 0.8517634272575378}]}, {"text": "Using a small sized corpus reduces both training time and decoding time, which allows us to quickly test different configurations of the decoder.", "labels": [], "entities": []}, {"text": "Ina second setting, denoted EUROPARL, Moses was trained on a larger corpora containing the entirety of the Europarl Corpus, but no in-domain data, to provide results on more realistic conditions.", "labels": [], "entities": [{"text": "EUROPARL", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8974261283874512}, {"text": "Europarl Corpus", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.9956057667732239}]}, {"text": "Statistics regarding the different corpora used are reported in Finding the oracle alignment amounts to solving the ILP problems introduced above.", "labels": [], "entities": []}, {"text": "Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible.", "labels": [], "entities": [{"text": "ILP solvers", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.6558514982461929}]}, {"text": "In our experiments, we used the free solver SCIP).", "labels": [], "entities": [{"text": "solver SCIP", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8098066449165344}]}, {"text": "An optimal solution was found for all problems we considered.", "labels": [], "entities": []}, {"text": "Decoding the 3, 027 sentences of WMT'09 test set takes about 10 minutes (wall time) for the NEWSCO setting, and several hours for the EUROPARL setting 13 . reports, for all considered settings, the BLEU-4 scores 14 achieved by our oracle decoder, as well as the number of source words used to generate the oracle hypothesis and the number of target words that are reachable.", "labels": [], "entities": [{"text": "WMT'09 test set", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9677321910858154}, {"text": "NEWSCO setting", "start_pos": 92, "end_pos": 106, "type": "DATASET", "confidence": 0.909015953540802}, {"text": "EUROPARL setting", "start_pos": 134, "end_pos": 150, "type": "DATASET", "confidence": 0.8955170512199402}, {"text": "BLEU-4", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9991177916526794}]}, {"text": "In these experiments, two objective functions were considered: first, we only consider the objective function corresponding to the relaxed problem defined by Eq.", "labels": [], "entities": []}, {"text": "(6); second, we introduced an extra term in the objective to penalize distortion, as described by Eq.", "labels": [], "entities": [{"text": "penalize distortion", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.5359671115875244}]}, {"text": "Unless explicitly stated otherwise, we always used the exact match strategy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size and  percentage of test words not appearing in the train set (OOV).", "labels": [], "entities": [{"text": "OOV", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.8551580309867859}]}, {"text": " Table 2: Translation score of the ILP oracle decoder for the various settings described in Section 3.1", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9158610701560974}]}, {"text": " Table 3: Output examples of our oracle decoder on the English to French task. Words in bold are non-aligned words and words in  italic are non-aligned out-of-vocabulary words. For clarity the examples have been detokenized and recased.", "labels": [], "entities": []}, {"text": " Table 4: Average distortion and percentage of phrases with a  distortion greater that Moses default distortion limit.", "labels": [], "entities": [{"text": "Average distortion", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9105682373046875}, {"text": "default distortion limit", "start_pos": 93, "end_pos": 117, "type": "METRIC", "confidence": 0.7234947681427002}]}]}