{"title": [{"text": "Example-based Paraphrasing for Improved Phrase-Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation", "start_pos": 40, "end_pos": 84, "type": "TASK", "confidence": 0.5850912407040596}]}], "abstractContent": [{"text": "In this article, an original view on how to improve phrase translation estimates is proposed.", "labels": [], "entities": [{"text": "phrase translation estimates", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.8794121742248535}]}, {"text": "This proposal is grounded on two main ideas: first, that appropriate examples of a given phrase should participate more in building its translation distribution; second, that paraphrases can be used to better estimate this distribution.", "labels": [], "entities": []}, {"text": "Initial experiments provide evidence of the potential of our approach and its implementation for effectively improving translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.9657812714576721}]}], "introductionContent": [{"text": "Phrase translation estimation in Statistical Phrasebased Translation () is hampered by the availability of both too many and too few training instances.", "labels": [], "entities": [{"text": "Phrase translation estimation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9762832323710123}, {"text": "Statistical Phrasebased Translation", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.6486229399840037}]}, {"text": "Recent results on tera-scale SMT show that access to many training examples 1 can lead to significant improvements in translation quality.", "labels": [], "entities": [{"text": "tera-scale SMT", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.5860602557659149}, {"text": "translation", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.9598422050476074}]}, {"text": "Also, providing indirect training instances via synonyms or paraphrases for previously unseen phrases can result in gains in translation quality, which are more apparent when little training data is originally available.", "labels": [], "entities": []}, {"text": "Although there is a consensus on the importance of using more parallel data in SMT, it has never been formally shown that all additional training instances are actually useful in predicting contextually appropriate translation hypotheses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9937600493431091}, {"text": "predicting contextually appropriate translation hypotheses", "start_pos": 179, "end_pos": 237, "type": "TASK", "confidence": 0.7132898032665252}]}, {"text": "Attempts at limiting training parallel sentences to those resembling test data through thematic adaptation () indeed confirm that large quantities of training data cannot compensate for the requirement for contextually appropriate training instances.", "labels": [], "entities": []}, {"text": "In fact, it is important that phrase translation models adequatly reflect contextual preferences for each phrase occurrence in a text.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7605056762695312}]}, {"text": "A variety of recent works have used dynamically adapted translation models, where each phrase occurrence has its own translation distribution) derived from local contextual information in the training examples.", "labels": [], "entities": []}, {"text": "These approaches are supported by the study of () which shows that phrase-based SMT systems are expressive enough to achieve very high translation performance and therefore suggests a better scoring of phrases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.783423125743866}]}, {"text": "The apparent tradeoff between the number of training examples and their appropriateness in each indivual context naturally asks for means of increasing the number of appropriate examples.", "labels": [], "entities": []}, {"text": "Exploiting comparable corpora for acquiring translation equivalents () offers interesting prospects to this issue, but so far focus has not been so much on context appropriateness as on globally increasing the number of biphrase examples.", "labels": [], "entities": []}, {"text": "The approach we take in this article is motivated by the fact that natural language allows for multiple text views on a given content, and that if two phrases are good paraphrases in context, then considering appropriate training examples of one of the phrases could provide larger quantities of training data for translating the other.", "labels": [], "entities": []}, {"text": "In other words, we hypothesize that there maybe more training data to learn a phrase's translations in a bilingual corpus than what SMT approaches typically use.", "labels": [], "entities": [{"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9828110933303833}]}, {"text": "In contrast to previous attempts at using paraphrases to improve Statistical Machine Translation, which require external data in the form of additional parallel bilingual corpora), monolingual corpora (, lexico-semantic resources (, or sub-sentential ( or sentential paraphrases of the input (), the approach we take here can be endogenous with respect to the original training data.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.8609049121538798}]}, {"text": "It also significantly departs from previous work in that paraphrasing is not simply considered as away of finding alternative wordings that can be translated given the original training data for out-of-vocabulary phrases only), but as a means to better estimate translations for any possible phrase.", "labels": [], "entities": []}, {"text": "Also, as opposed to the work by), we do not encode paraphrases into input lattices to have them compete against each other to belong to the source sentential paraphrase that will lead to the highest scoring output sentence 3 . Instead, we make use of all contextually appropriate paraphrases of a source phrase, which collectively evaluate the quality of each translation for that phrase.", "labels": [], "entities": []}, {"text": "This work can thus be seen as a contribution towards shifting from global phrase translation distributions to contextual translation distributions for contextually equivalent source units.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.6905821710824966}]}, {"text": "The remainder of this paper is organized as followed.", "labels": [], "entities": []}, {"text": "In section 2 we review relevant previous works and discuss how they differ from our approach.", "labels": [], "entities": []}, {"text": "Section 3 provides a description of the details of our approach.", "labels": [], "entities": []}, {"text": "We describe an experimental setup in section 4 and com-3 This highly depends on how well estimated translations for each independent paraphrase are.", "labels": [], "entities": []}, {"text": "Finally, we discuss our future work in section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}