{"title": [{"text": "A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages *", "labels": [], "entities": [{"text": "Morpheme-Word Representation", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.7207203209400177}, {"text": "Machine Translation of Morphologically Rich Languages", "start_pos": 42, "end_pos": 95, "type": "TASK", "confidence": 0.8529699941476186}]}], "abstractContent": [{"text": "We propose a language-independent approach for improving statistical machine translation for morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.7073821624120077}]}, {"text": "Our model extends the classic phrase-based model by means of (1) word boundary-aware morpheme-level phrase extraction, (2) minimum error-rate training fora morpheme-level translation model using word-level BLEU, and (3) joint scoring with morpheme-and word-level language models.", "labels": [], "entities": [{"text": "word boundary-aware morpheme-level phrase extraction", "start_pos": 65, "end_pos": 117, "type": "TASK", "confidence": 0.6376827776432037}, {"text": "BLEU", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.8986092209815979}]}, {"text": "Further improvements are achieved by combining our model with the classic one.", "labels": [], "entities": []}, {"text": "The evaluation on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.96787428855896}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.995826780796051}]}], "introductionContent": [{"text": "The fast progress of statistical machine translation (SMT) has boosted translation quality significantly.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.8229620903730392}, {"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9596141576766968}]}, {"text": "While research keeps diversifying, the word remains the atomic token-unit of translation.", "labels": [], "entities": []}, {"text": "This is fine for languages with limited morphology like English and French, or no morphology at all like Chinese, but it is inadequate for morphologically rich languages like Arabic, Czech or Finnish).", "labels": [], "entities": []}, {"text": "* This research was sponsored in part by CSIDM and by a National Research Foundation grant entitled \"Interactive Media Search\" (grant # R-252-000-325-279).", "labels": [], "entities": []}, {"text": "There has been a line of recent SMT research that incorporates morphological analysis as part of the translation process, thus providing access to the information within the individual words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9950650334358215}]}, {"text": "Unfortunately, most of this work either relies on languagespecific tools, or only works for very small datasets.", "labels": [], "entities": []}, {"text": "Below we propose a language-independent approach to SMT of morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9973879456520081}]}, {"text": "We use unsupervised morphological analysis and we incorporate its output into the process of translation, as opposed to relying on pre-processing and post-processing only as has been done in previous work.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 present our morphological and phrase merging enhancements.", "labels": [], "entities": [{"text": "phrase merging enhancements", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.7857237259546915}]}, {"text": "Section 5 describes our experiments, and Section 6 analyzes the results.", "labels": [], "entities": []}, {"text": "Finally, Section 7 concludes and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use the English-Finnish data from the 2005 shared task (), which is split into training, development, and test portions; see for details.", "labels": [], "entities": []}, {"text": "We further split the training dataset into four subsets T 1 , T 2 , T 3 , and T 4 of sizes 40K, 80K, 160K, and 320K parallel sentence pairs, which we use for studying the impact of training data size on translation performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics. Shown are the number of  parallel sentences, and the average number of words and  Morfessor morphemes on the English and Finnish sides  of the training, development and test datasets.", "labels": [], "entities": []}, {"text": " Table 2: Baseline system performance (on the test  dataset). Shown are word BLEU and morpheme m- BLEU scores for the w-system and m-system.", "labels": [], "entities": [{"text": "word", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9248988628387451}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.6451202034950256}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.6861028075218201}]}, {"text": " Table 3: Impact of the morphological enhancements  (on test dataset). Shown are BLEU scores (in %) for  training on T 1 and on the full dataset for (i) baselines,  (ii) enhancements individually, and (iii) combined. Su- perscripts indicate absolute improvements w.r.t m-system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9995818734169006}]}, {"text": " Table 4: Effect of selection of primary phrase table for  add-1 (on dev dataset): P T w\u2192m , derived from a word- token input, vs. P T m , from a morpheme-token input.  Shown is BLEU (in %) on T 1 and the full training dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9993113279342651}]}, {"text": " Table 5: Trying different values for interpolate (on dev  dataset). BLEU (in %) is for the full training dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.999450147151947}]}, {"text": " Table 6. As we can see,  add-1 and add-2 make little difference compared to  the m-system baseline. In contrast, interpolation and  ourMethod yield sizable absolute improvements of  0.55 and 0.74 BLEU points, respectively, over the  m-system; moreover, they outperform the w-system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9990026354789734}]}, {"text": " Table 6: Merging m+phr+lm and w-system (on test  dataset). BLEU (in %) is for the full training dataset. Su- perscripts indicate performance gain/loss w.r.t m-system.", "labels": [], "entities": [{"text": "Merging", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9827976226806641}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9994820952415466}]}]}