{"title": [{"text": "Enhancing Mention Detection using Projection via Aligned Corpora", "labels": [], "entities": [{"text": "Enhancing Mention Detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.844339112440745}]}], "abstractContent": [{"text": "The research question treated in this paper is centered on the idea of exploiting rich resources of one language to enhance the performance of a mention detection system of another one.", "labels": [], "entities": [{"text": "mention detection system", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.7969330151875814}]}, {"text": "We successfully achieve this goal by projecting information from one language to another via a parallel corpus.", "labels": [], "entities": []}, {"text": "We examine the potential improvement using various degrees of linguistic information in a statistical framework and we show that the proposed technique is effective even when the target language model has access to a significantly rich feature set.", "labels": [], "entities": []}, {"text": "Experimental results show up to 2.4F improvement in performance when the system has access to information obtained by projecting mentions from a resource-rich-language mention detection system via a parallel corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of identifying and classifying entity textual references in open-domain texts, i.e. the Mention Detection (MD) task, has become one of the most important subtasks of Information Extraction (IE).", "labels": [], "entities": [{"text": "classifying entity textual references in open-domain texts", "start_pos": 28, "end_pos": 86, "type": "TASK", "confidence": 0.7875371319907052}, {"text": "Mention Detection (MD) task", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.8239811162153879}, {"text": "Information Extraction (IE)", "start_pos": 175, "end_pos": 202, "type": "TASK", "confidence": 0.8401632905006409}]}, {"text": "It might intervene both as one step to structure natural language texts or as a text enrichment preprocessing step to help other Natural Language Processing (NLP) applications reach higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9673404693603516}]}, {"text": "Similarly to the Automatic Content Extraction (ACE) nomenclature, we consider that a mention can be either named (e.g., John, Chicago), nominal (e.g., president, activist) or pronominal (e.g., he, she).", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE) nomenclature", "start_pos": 17, "end_pos": 64, "type": "TASK", "confidence": 0.8070519055638995}]}, {"text": "It has also a specific class which describes the type of the entity it refers to.", "labels": [], "entities": []}, {"text": "For instance, in the sentence: Michael Bloomberg, the Mayor of NYC, declared his war on tobacco and sugary drinks in the city.", "labels": [], "entities": []}, {"text": "we find the mentions 'Michael Bloomberg',his' of the same person entity.", "labels": [], "entities": []}, {"text": "Their types are named, nominal and pronominal, respectively.", "labels": [], "entities": []}, {"text": "'NYC' and 'city', on the other hand, are mentions of the same geopolitical (GPE) entity of type named and nominal, respectively.", "labels": [], "entities": []}, {"text": "Consequently, MD is a more general and complex task than the well known Named Entity Recognition (NER) task which aims solely at the identification and classification of the named mentions.", "labels": [], "entities": [{"text": "MD", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.8464755415916443}, {"text": "Named Entity Recognition (NER) task", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.8083501969064985}, {"text": "identification and classification of the named mentions", "start_pos": 133, "end_pos": 188, "type": "TASK", "confidence": 0.823535463639668}]}, {"text": "The difficulty of the MD task is directly related to the nature of the language and the linguistic resources available, i.e. it is easier to build accurate MD systems for languages with a simple morphology and a high amount of linguistic resources.", "labels": [], "entities": []}, {"text": "For this reason, we explore the idea of using an MD system, which has been designed and built fora resource-rich language (RRL), to help enhance the performance of an MD system in a target language (TL).", "labels": [], "entities": []}, {"text": "More specifically, the goal of the research work we present in this paper is to employ the richness of English, in terms of natural language resources, to raise the accuracy of MD systems in other languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9986860156059265}]}, {"text": "For instance, an English MD system might achieve a performance of F \u03b2=1 -measure=82.7 ( when it resorts to a rich set of features extracted from diverse resources, namely: part-of-speech, chunk information, syntactic parse trees, word sense information, WordNet information and information from the output of other mention detection classifiers.", "labels": [], "entities": [{"text": "F \u03b2=1 -measure", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.8645034730434418}]}, {"text": "In this paper, our research question revolves around investigating an adequate approach to use such a system to the benefit of other languages such as Arabic, Chinese, French or Spanish MD systems, which also have annotated resources but not of the same quantity and/or quality as English.", "labels": [], "entities": []}, {"text": "In this paper, we have targeted English and Arabic as the RRL and TL, respectively, because: 1.", "labels": [], "entities": [{"text": "RRL", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.536014974117279}, {"text": "TL", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.726055383682251}]}, {"text": "We have a very competitive English MD system; 2.", "labels": [], "entities": [{"text": "English MD", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.589519739151001}]}, {"text": "The linguistic resources available for the Arabic language allow a simulation of different TL richness levels; and 3.", "labels": [], "entities": [{"text": "TL richness", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.8904180824756622}]}, {"text": "The use of two languages of an utterly different nature makes the extrapolation of the results to other languages possible.", "labels": [], "entities": []}, {"text": "Our hypothesis might be expressed as follows: using an MD system resorting to a rich feature set (i.e. the RRL MD system) to boost a MD system performance in a TL can be very beneficial if the \"donor\" system surpasses its TL counterpart in terms of resources.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we have projected MD tags from RRL to TL via a parallel corpus, and then extracted several linguistic features about the automatically tagged words.", "labels": [], "entities": []}, {"text": "Thereafter, we have conducted experiments adding these new features to the TL baseline MD system.", "labels": [], "entities": [{"text": "TL baseline MD system", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.8326554745435715}]}, {"text": "In order to have a complete picture on the impact of these new features, we have used TL baseline systems resorting to a varied amount of features, starting with a case employing only lexical information to a case where we use all the resources we could gather for the TL.", "labels": [], "entities": []}, {"text": "Experiments show that the gain is always statistically significant and it reaches its maximum when only very basic features are used in the baseline TL MD system.", "labels": [], "entities": [{"text": "TL MD", "start_pos": 149, "end_pos": 154, "type": "TASK", "confidence": 0.6952310502529144}]}], "datasetContent": [{"text": "Experiments are conducted on the Arabic ACE 2007 data.", "labels": [], "entities": [{"text": "Arabic ACE 2007 data", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.9726382791996002}]}, {"text": "There are 379 Arabic documents and almost 98, 000 words.", "labels": [], "entities": []}, {"text": "We find seven classes of mentions: Person (PER), Organization (ORG), GeoPolitical Entity (GPE), Location (LOC), Facility (FAC), Vehicle (VEH) and Weapon (WEA).", "labels": [], "entities": []}, {"text": "Since the evaluation test sets are not publicly available, we have split the publicly available training corpus into an 85%/15% data split.", "labels": [], "entities": []}, {"text": "We use 323 documents (80, 000 words) for training and 56 documents (18, 000 words) as a test set.", "labels": [], "entities": []}, {"text": "This results in 17, 634 mentions (7, 816 named, 8, 831 nominal and 987 pronominal) for training and 3, 566 for test (1, 673 named, 1, 682 nominal and 211 pronominal).", "labels": [], "entities": []}, {"text": "To facilitate future comparisons with work presented here, and to simulate a realistic scenario, the splits are created based on article dates: the test data is selected as the latest 15% of the data in chronological order, in each of the covered genres (newswire and webblog).", "labels": [], "entities": []}, {"text": "Performance on the ACE data is usually evaluated using a special-purpose measure, i.e. the ACE value metric.", "labels": [], "entities": [{"text": "ACE data", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9052833616733551}, {"text": "ACE value metric", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.555344820022583}]}, {"text": "However, given that we are interested in the mention detection task only, we decided to use the more intuitive and popular (un-weighted) F-measure, the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "mention detection task", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8881316582361857}, {"text": "F-measure", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9732803702354431}, {"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9937803149223328}, {"text": "recall", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9949927926063538}]}, {"text": "As we have stated earlier, our main goal is to investigate how an MD model of a TL might benefit from additional information about the mentions obtained by propagation from an RRL.", "labels": [], "entities": []}, {"text": "In our research study we have chosen Arabic as the TL and English as the RRL.", "labels": [], "entities": [{"text": "RRL", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.45227500796318054}]}, {"text": "The English MD system we use has access to a large set of information (  3-n \u2212 Head: Base.", "labels": [], "entities": []}, {"text": "+ head-word based features; 4-n \u2212 P ars.: Base.", "labels": [], "entities": []}, {"text": "+ parser-related features; 5-Gaz.: Base.", "labels": [], "entities": []}, {"text": "+ automatically extracted gazetteers from the parallel corpus; 6-M odel: Base.", "labels": [], "entities": []}, {"text": "+ output of model trained on the Arabic part of the parallel corpus; 7-Comb.: combination of all the above.", "labels": [], "entities": []}, {"text": "In the rest of the paper, to measure whether the improvement in performance of a system using features from parallel data over baseline is statistically significant or not, we use the stratified bootstrap resampling significance test used in the NER shared task of . We consider results as statistically significant when p < 0.02.", "labels": [], "entities": [{"text": "NER shared task", "start_pos": 246, "end_pos": 261, "type": "TASK", "confidence": 0.5996790329615275}]}], "tableCaptions": [{"text": " Table 1: Obtained results when the features were ex- tracted from a hand-aligned parallel corpus", "labels": [], "entities": [{"text": "Obtained", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9904770851135254}]}, {"text": " Table 3: Distribution over the classes of the blind test  mentions", "labels": [], "entities": []}, {"text": " Table 4: Obtained results on blind test", "labels": [], "entities": [{"text": "Obtained", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980230331420898}]}, {"text": " Table 5: Obtained results when the features were  extracted from both hand-aligned and automatically- aligned parallel corpora", "labels": [], "entities": [{"text": "Obtained", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9926369190216064}]}]}