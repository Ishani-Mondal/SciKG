{"title": [{"text": "Multi-level Structured Models for Document-level Sentiment Classification", "labels": [], "entities": [{"text": "Document-level Sentiment Classification", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.7956384221712748}]}], "abstractContent": [{"text": "In this paper, we investigate structured models for document-level sentiment classification.", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 52, "end_pos": 91, "type": "TASK", "confidence": 0.72886989514033}]}, {"text": "When predicting the sentiment of a subjective document (e.g., as positive or negative), it is well known that not all sentences are equally discriminative or informative.", "labels": [], "entities": [{"text": "predicting the sentiment of a subjective document", "start_pos": 5, "end_pos": 54, "type": "TASK", "confidence": 0.8537949664252145}]}, {"text": "But identifying the useful sentences automatically is itself a difficult learning problem.", "labels": [], "entities": []}, {"text": "This paper proposes a joint two-level approach for document-level sentiment classification that simultaneously extracts useful (i.e., subjective) sentences and predicts document-level sentiment based on the extracted sentences.", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.7727119823296865}]}, {"text": "Unlike previous joint learning methods for the task, our approach (1) does not rely on gold standard sentence-level subjectivity annotations (which maybe expensive to obtain), and (2) optimizes directly for document-level performance.", "labels": [], "entities": []}, {"text": "Empirical evaluations on movie reviews and U.S. Congressional floor debates show improved performance over previous approaches .", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment classification is a well-studied and active research area.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9742016792297363}]}, {"text": "One of the main challenges for document-level sentiment categorization is that not every part of the document is equally informative for inferring the sentiment of the whole document.", "labels": [], "entities": [{"text": "document-level sentiment categorization", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.6177556117375692}]}, {"text": "Objective statements interleaved with the subjective statements can be confusing for learning methods, and subjective statements with conflicting sentiment further complicate the document categorization task.", "labels": [], "entities": []}, {"text": "For example, authors of movie reviews often devote large sections to (largely objective) descriptions of the plot ().", "labels": [], "entities": []}, {"text": "In addition, an overall positive review might still include some negative opinions about an actor or the plot.", "labels": [], "entities": []}, {"text": "Early research on document-level sentiment classification employed conventional machine learning techniques for text categorization).", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 18, "end_pos": 57, "type": "TASK", "confidence": 0.8205357392628988}]}, {"text": "These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords).", "labels": [], "entities": []}, {"text": "As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited.", "labels": [], "entities": []}, {"text": "And although researchers subsequently proposed methods for incorporating sentence-level subjectivity information, existing techniques have some undesirable properties.", "labels": [], "entities": []}, {"text": "First, they typically require gold standard sentence-level annotations,).", "labels": [], "entities": []}, {"text": "But the cost of acquiring such labels can be prohibitive.", "labels": [], "entities": []}, {"text": "Second, some solutions for incorporating sentencelevel information lack mechanisms for controlling how errors propagate from the subjective sentence identification subtask to the main document classification task ().", "labels": [], "entities": [{"text": "sentence identification subtask", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.7944667339324951}, {"text": "document classification task", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.7930536071459452}]}, {"text": "Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document-and sentence-level classification accuracy.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7326391041278839}, {"text": "document-and sentence-level classification", "start_pos": 138, "end_pos": 180, "type": "TASK", "confidence": 0.5348960558573405}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.69976806640625}]}, {"text": "Optimizing for this compromise, when the real goal is to maximize only the document-level accuracy, can potentially hurt document-level performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8903977274894714}]}, {"text": "In this paper, we propose a joint two-level model to address the aforementioned concerns.", "labels": [], "entities": []}, {"text": "We formulate our training objective to directly optimize for document-level accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9594820737838745}]}, {"text": "Further, we do not require gold standard sentence-level labels for training.", "labels": [], "entities": []}, {"text": "Instead, our training method treats sentence-level labels as hidden variables and jointly learns to predict the document label and those (subjective) sentences that best \"explain\" it, thus controlling the propagation of incorrect sentence labels.", "labels": [], "entities": []}, {"text": "And by directly optimizing for document-level accuracy, our model learns to solve the sentence extraction subtask only to the extent required for accurately classifying document sentiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9719163179397583}, {"text": "sentence extraction subtask", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.7943605383237203}, {"text": "classifying document sentiment", "start_pos": 157, "end_pos": 187, "type": "TASK", "confidence": 0.8462353348731995}]}, {"text": "A software implementation of our method is also publicly available.", "labels": [], "entities": []}, {"text": "For the rest of the paper, we will discuss related work, motivate and describe our model, present an empirical evaluation on movie reviews and U.S. Congressional floor debates datasets and close with discussion and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our methods using the Movie Reviews and U.S. Congressional Floor Debates datasets, following the setup used in previous work for comparison purposes.", "labels": [], "entities": [{"text": "U.S. Congressional Floor Debates datasets", "start_pos": 52, "end_pos": 93, "type": "DATASET", "confidence": 0.7019163846969605}]}, {"text": "We use the movie reviews dataset from that was originally released by.", "labels": [], "entities": [{"text": "movie reviews dataset", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.6678833564122518}]}, {"text": "This version contains annotated rationales for each review, which we use to generate an additional initialization during training (described below).", "labels": [], "entities": []}, {"text": "We follow exactly the experimental setup used in.", "labels": [], "entities": []}, {"text": "We also use the U.S. Congressional floor debates transcripts from.", "labels": [], "entities": [{"text": "U.S. Congressional floor debates transcripts", "start_pos": 16, "end_pos": 60, "type": "DATASET", "confidence": 0.7302132368087768}]}, {"text": "The data was extracted from GovTrack (http://govtrack.us), which has all available transcripts of U.S. floor debates in the House of Representatives in 2005.", "labels": [], "entities": [{"text": "GovTrack", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9450860023498535}]}, {"text": "As in previous work, only debates with discussions of \"controversial\" bills were considered (where the losing side had at least 20% of the speeches).", "labels": [], "entities": []}, {"text": "The goal is to predict the vote (\"yea\" or \"nay\") for the speaker of each speech segment.", "labels": [], "entities": [{"text": "predict the vote (\"yea", "start_pos": 15, "end_pos": 37, "type": "METRIC", "confidence": 0.7873538732528687}]}, {"text": "For our experiments, we evaluate our methods using the speakerbased speech-segment classification setting as described in Since our training procedure solves a non-convex optimization problem, it requires an initial guess of the explanatory sentences.", "labels": [], "entities": [{"text": "speakerbased speech-segment classification", "start_pos": 55, "end_pos": 97, "type": "TASK", "confidence": 0.6424402197202047}]}, {"text": "We use an explanatory set size (5) of 30% of the number of sentences in each document, L = 0.3 \u00b7 |x||, with a lower cap of 1.", "labels": [], "entities": []}, {"text": "We generate initializations using OpinionFinder (), which were shown to be a reasonable substitute for human annotations in the Movie Reviews dataset (.", "labels": [], "entities": [{"text": "Movie Reviews dataset", "start_pos": 128, "end_pos": 149, "type": "DATASET", "confidence": 0.8797044356664022}]}, {"text": "We consider two additional (baseline) methods for initialization: using a random set of sentences, and using the last 30% of sentence in the document.", "labels": [], "entities": []}, {"text": "In the Movie Reviews dataset, we also use sentences containing human-annotator rationales as a final initialization option.", "labels": [], "entities": [{"text": "Movie Reviews dataset", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.8411479194959005}]}, {"text": "No such manual annotations are available for the Congressional Debates.", "labels": [], "entities": [{"text": "Congressional Debates", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.8792683482170105}]}, {"text": "We evaluate three versions of our model: the initial model (3) which we call SVM sle (SVMs for Sentiment classification with Latent Explanations), SVM sle regularized relative to a prior as described in the documents in the whole dataset contain only 1-3 sentences, making it an uninteresting setting to analyze with our model.", "labels": [], "entities": [{"text": "Sentiment classification with Latent Explanations)", "start_pos": 95, "end_pos": 145, "type": "TASK", "confidence": 0.770847017566363}]}, {"text": "We select all sentences whose majority vote of OpinionFinder word-level polarities matches the document's sentiment.", "labels": [], "entities": []}, {"text": "If there are fewer than L sentences, we add sentences starting from the end of the document.", "labels": [], "entities": []}, {"text": "If there are more, we remove sentences starting from the beginning of the document.", "labels": [], "entities": []}, {"text": "Section 4.5.1 which we refer to as SVM sle w/ Prior, and the feature smoothing model described in Section 4.5.2 which we call SVM sle f s . Due to the difficulty of selecting a good prior, we expect SVM sle f s to exhibit the most robust performance.", "labels": [], "entities": []}, {"text": "shows a comparison of our proposed methods on the two datasets.", "labels": [], "entities": []}, {"text": "We observe that SVM sle f s provides both strong and robust performance.", "labels": [], "entities": []}, {"text": "The performance of SVM sle is generally better when trained using a prior than not in the Movie Reviews dataset.", "labels": [], "entities": [{"text": "Movie Reviews dataset", "start_pos": 90, "end_pos": 111, "type": "DATASET", "confidence": 0.9189934730529785}]}, {"text": "Both extensions appear to hurt performance in the U.S. Congressional Floor Debates dataset.", "labels": [], "entities": [{"text": "U.S. Congressional Floor Debates dataset", "start_pos": 50, "end_pos": 90, "type": "DATASET", "confidence": 0.6384937405586243}]}, {"text": "Using OpinionFinder to initialize our training procedure offers good performance across both datasets, whereas the baseline initializations exhibit more erratic performance behavior.", "labels": [], "entities": []}, {"text": "8 Unsurprisingly, initializing using human annotations (in the Movie Reviews dataset) can offer further improvement.", "labels": [], "entities": [{"text": "Movie Reviews dataset", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.9109618465105692}]}, {"text": "Adding proximity features (as described in Section 4.4) in general seems to improve performance when using a good initialization, and hurts performance otherwise.", "labels": [], "entities": []}, {"text": "show a comparison of SVM sle f s with previous work on the Movie Reviews and U.S. Congressional Floor Debates datasets, respectively.", "labels": [], "entities": [{"text": "U.S. Congressional Floor Debates datasets", "start_pos": 77, "end_pos": 118, "type": "DATASET", "confidence": 0.6022377669811249}]}, {"text": "For the Movie Reviews dataset, we considered two settings: when human annotations are available, and when they are not (in which case we initialized using OpinionFinder).", "labels": [], "entities": [{"text": "Movie Reviews dataset", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.7844897707303365}]}, {"text": "For the U.S. Congressional Floor Debates dataset we used only the latter setting, since there are no annotations available for this dataset.", "labels": [], "entities": [{"text": "U.S. Congressional Floor Debates dataset", "start_pos": 8, "end_pos": 48, "type": "DATASET", "confidence": 0.6610615253448486}]}, {"text": "In all cases we observe SVM sle f s showing improved performance compared to previous results.", "labels": [], "entities": []}, {"text": "We tried around 10 different values for C parameter, and selected the final model based on the validation set.", "labels": [], "entities": []}, {"text": "The training procedure alternates between training a standard structural SVM model and using the subsequent model to re-label the latent variables.", "labels": [], "entities": []}, {"text": "We selected the halting iteration of the training procedure using the validation set.", "labels": [], "entities": []}, {"text": "When initializing using human annotations for the Movie Reviews dataset, the halting iteration is typically the first iteration, whereas the halting iteration is typically chosen from a later iteration  when initializing using OpinionFinder.", "labels": [], "entities": [{"text": "Movie Reviews dataset", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.9099682569503784}]}, {"text": "shows the per-iteration overlap of extracted sentences from SVM sle f s models initialized using OpinionFinder and human annotations on the Movie Reviews training set.", "labels": [], "entities": [{"text": "Movie Reviews training set", "start_pos": 140, "end_pos": 166, "type": "DATASET", "confidence": 0.8447349071502686}]}, {"text": "We can see that training has approximately converged after about 10 iterations.", "labels": [], "entities": []}, {"text": "We can also see that both models iteratively learn to extract sentences that are more similar to each other than their respective initializations (the overlap between the two initializations is 57%).", "labels": [], "entities": []}, {"text": "This is an indicator that our learning problem, despite being non-convex and having multiple local optima, has a reasonably large \"good\" region that can be approached using different initialization methods.", "labels": [], "entities": []}, {"text": "shows how accuracy on the test set of SVM sle f s changes on the Movie Reviews dataset as a function of varying the extraction size f (|x|) from (5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999386191368103}, {"text": "Movie Reviews dataset", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.8859809041023254}]}, {"text": "We can see that performance changes smoothly 10 (and so is robust), and that one might see further improvement from more The number of iterations required to converge is an upper bound on the number of iterations from which to choose the halting iteration (based on a validation set).", "labels": [], "entities": []}, {"text": "The smoothness will depend on the initialization.: Example \"yea\" speech with Latent Explanations from the U.S. Congressional Floor Debates dataset predicted by SVM sle f s with OpinionFinder initialization.", "labels": [], "entities": [{"text": "U.S. Congressional Floor Debates dataset", "start_pos": 106, "end_pos": 146, "type": "DATASET", "confidence": 0.5363491892814636}]}, {"text": "Latent Explanations are preceded by solid circles with numbers denoting their preference order (1 being most preferred by SVM sle f s ).", "labels": [], "entities": []}, {"text": "The five least subjective sentences are preceded by circles with numbers denoting the subjectivity order (1 being least subjective according to SVM sle f s ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates  (bottom) datasets using SVM sle , SVM sle w/ Prior and SVM sle  f s with and without proximity features.", "labels": [], "entities": []}]}