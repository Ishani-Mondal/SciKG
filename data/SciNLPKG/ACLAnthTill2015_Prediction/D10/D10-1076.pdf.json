{"title": [{"text": "Training continuous space language models: some practical issues", "labels": [], "entities": []}], "abstractContent": [{"text": "Using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling, with applications in speech recognition and statistical machine translation.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.8058722217877706}, {"text": "speech recognition", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7511349022388458}, {"text": "statistical machine translation", "start_pos": 189, "end_pos": 220, "type": "TASK", "confidence": 0.7425157229105631}]}, {"text": "However, training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available.", "labels": [], "entities": []}, {"text": "In this work, we study the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms.", "labels": [], "entities": []}, {"text": "The induced word embeddings for extreme cases are also analysed, thus providing insight into the convergence issues.", "labels": [], "entities": []}, {"text": "A new initialization scheme and new training techniques are then introduced.", "labels": [], "entities": []}, {"text": "These methods are shown to greatly reduce the training time and to significantly improve performance, both in terms of perplexity and on a large-scale translation task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical language models play an important role in many practical applications, such as machine translation and automatic speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.8356324434280396}, {"text": "automatic speech recognition", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.6184164782365164}]}, {"text": "Let V be a finite vocabulary, statistical language models define distributions over sequences of words w L 1 in V usually factorized as: Modeling the joint distribution of several discrete random variables (such as words in a sentence) is difficult, especially in real-world Natural Language Processing applications where V typically contains dozens of thousands words.", "labels": [], "entities": []}, {"text": "Many approaches to this problem have been proposed over the last decades, the most widely used being back-off n-gram language models.", "labels": [], "entities": []}, {"text": "n-gram models rely on a Markovian assumption, and despite this simplification, the maximum likelihood estimate (MLE) remains unreliable and tends to underestimate the probability of very rare n-grams, which are hardly observed even in huge corpora.", "labels": [], "entities": [{"text": "maximum likelihood estimate (MLE)", "start_pos": 83, "end_pos": 116, "type": "METRIC", "confidence": 0.9079508086045583}]}, {"text": "Conventional smoothing techniques, such as KneserNey and Witten-Bell back-off schemes (see) for an empirical overview, and) fora Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events.", "labels": [], "entities": []}, {"text": "n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index.", "labels": [], "entities": []}, {"text": "In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model.", "labels": [], "entities": []}, {"text": "Various approaches have proposed to overcome this limitation, notably the use of word-classes (, of generalized back-off strategies () or the explicit integration of morphological information in the random-forest model ().", "labels": [], "entities": []}, {"text": "One of the most successful alternative to date is to use distributed word representations (, where distributionally similar words are represented as neighbors in a continuous space.", "labels": [], "entities": []}, {"text": "This turns n-grams distributions into smooth functions of the word representations.", "labels": [], "entities": []}, {"text": "These representations and the associated probability estimates are jointly computed in a multi-layer neural network architecture.", "labels": [], "entities": []}, {"text": "This approach has showed significant and consistent improvements when applied to automatic speech recognition and machine translation tasks ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.6879318654537201}, {"text": "machine translation tasks", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.8039825757344564}]}, {"text": "Hence, continuous space language models are becoming increasingly used.", "labels": [], "entities": []}, {"text": "These successes have revitalized the research on neuronal architectures for language models, and given rise to several new proposals (see, for instance,).", "labels": [], "entities": []}, {"text": "A major difficulty with these approaches remains the complexity of training, which does not scale well to the massive corpora that are nowadays available.", "labels": [], "entities": []}, {"text": "Practical solutions to this problem are discussed in, which introduces a number of optimization and tricks to make training doable.", "labels": [], "entities": []}, {"text": "Even then, training a neuronal language model typically takes days.", "labels": [], "entities": []}, {"text": "In this paper, we empirically study the convergence behavior of two multi-layer neural networks for statistical language modeling, comparing the standard model of () with the logbilinear (LBL) model of.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 100, "end_pos": 129, "type": "TASK", "confidence": 0.784151017665863}]}, {"text": "Our contributions are the following: we first propose a reformulation of Mnih and Hinton's model, which reveals its similarity with extant models, and allows a direct and fair comparison with the standard model.", "labels": [], "entities": [{"text": "Mnih and Hinton's model", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.8254886269569397}]}, {"text": "For the standard model, these results highlight the impact of parameter initialization.", "labels": [], "entities": []}, {"text": "We first investigate a re-initialization method which allows to escape from the local extremum the standard model converges to.", "labels": [], "entities": []}, {"text": "While this method yields a significative improvement, the underlying assumption about the structure of the model does not meet the requirement of very large-scale tasks.", "labels": [], "entities": []}, {"text": "We therefore introduce a different initialization strategy, called one vector initialization.", "labels": [], "entities": []}, {"text": "Experimental results show that these novel training strategies drastically reduce the total training time, while delivering significant improvements both in terms of perplexity and in a large-scale translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 198, "end_pos": 214, "type": "TASK", "confidence": 0.8782838881015778}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first describe, in Section 2, the standard and the LBL language models.", "labels": [], "entities": []}, {"text": "By reformulating the latter, we show that both models are very similar and emphasize the remaining differences.", "labels": [], "entities": []}, {"text": "Section 2.4 discusses complexity issues and possible solutions to reduce the training time.", "labels": [], "entities": []}, {"text": "We then report, in Section 3, preliminary experimental results that enlighten some caveats of the standard approach.", "labels": [], "entities": []}, {"text": "Based on these observations, we introduce in Section 4 novel and more efficient training schemes, yielding improved performance and a reduced training time both on small and large scale experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "To validate this approach, we compare the convergence of a standard model trained (with the standard learning regime) with the one vector initialization regime.", "labels": [], "entities": []}, {"text": "The context vocabulary is defined by the 532, 557 words occurring in the training data and the prediction vocabulary by the 10, 000 most frequent words 6 . All other parameters are the same as in the previous experiments.", "labels": [], "entities": []}, {"text": "Based on the curves displayed on, we can observe that the model obtained with the one vector initialization regime outperforms the model trained with a completely random initialization.", "labels": [], "entities": []}, {"text": "Moreover, the latter reaches convergence in only 14 epochs, while the learning regime we propose only needs 9 epochs.", "labels": [], "entities": []}, {"text": "Convergence is even faster than when we used the standard training regime and a small context vocabulary.", "labels": [], "entities": []}, {"text": "To illustrate the impact of our initialization scheme, we also used a principal component analysis to represent the induced word representations in a two dimensional space.", "labels": [], "entities": []}, {"text": "represents the vectors associated with numbers 7 in red, while all other words are represented in blue.", "labels": [], "entities": []}, {"text": "Two different models are used: the standard model on the left, and the one vector initialization model on the right.", "labels": [], "entities": []}, {"text": "We can observe that, for the standard model, most of the red points are scattered allover a large portion of the representation space.", "labels": [], "entities": []}, {"text": "On the opposite, for the one vector initialization model, points associated with numbers are much more concentrated: this is simply because all the points are originally identical, and the training aim to spread the point around this starting point.", "labels": [], "entities": []}, {"text": "We also created the closest word list reported in, in a manner similar to Table 1.", "labels": [], "entities": []}, {"text": "Clearly, the new method seems to yield more  meaningful neighborhoods in the context space.", "labels": [], "entities": []}, {"text": "It is finally noteworthy to mention that when used with a small context vocabulary (as in the experimental setting of Section 4.1) this initialization strategy underperforms the standard initialization.", "labels": [], "entities": []}, {"text": "This is simply due to the much greater data sparsity in the large context vocabulary experiments, where the rarer word types are really rare (they typically occur once or twice).", "labels": [], "entities": []}, {"text": "By contrast, the rarer words in the small vocabulary tasks occurred more than several hundreds times in the training corpus, which was more than sufficient to guide the model towards satisfactory projection matrices.", "labels": [], "entities": []}, {"text": "This finally suggests that there still exists room for improvement if we can find more efficient initialization strategies than starting from one or several random points.", "labels": [], "entities": []}, {"text": "As a last experiment, we compare the various models on a large scale machine translation task.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.7825035254160563}]}, {"text": "Statistical language models are key component of current statistical machine translation systems, where they both help disambiguate lexical choices in the target language and influence the choice of the right word ordering.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6266849140326182}]}, {"text": "The integration of a neural network language model in such a system is far from easy, given the computational cost of computing word probabilities, a task that is performed repeatedly during the search of the best translation.", "labels": [], "entities": []}, {"text": "We then had to resort to a two pass decoding approach: the first pass uses a conventional back-off language model to produce a n-best list (the n most likely translations and their associated scores); in the second pass, the probability of the neural language model is computed for each hypothesis and the n- best list is accordingly reordered to produce the final translations.", "labels": [], "entities": []}, {"text": "The different language models discussed in this article are evaluated on the Arabic to English NIST 2009 constrained task.", "labels": [], "entities": [{"text": "NIST 2009 constrained task", "start_pos": 95, "end_pos": 121, "type": "DATASET", "confidence": 0.8605155199766159}]}, {"text": "For the continuous space language model, the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1).", "labels": [], "entities": []}, {"text": "The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set.", "labels": [], "entities": [{"text": "NIST test set", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9768063624699911}, {"text": "NIST test set", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9615193009376526}]}, {"text": "Our system is built using the open-source Moses toolkit () with default settings.", "labels": [], "entities": []}, {"text": "To setup our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in).", "labels": [], "entities": []}, {"text": "The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 58, "end_pos": 85, "type": "METRIC", "confidence": 0.8610461801290512}]}, {"text": "Performance is measured based on the BLEU () scores, which are reported in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9988221526145935}]}, {"text": "All the experimented neural language models yield to a significant BLEU increase.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9992976188659668}]}, {"text": "The best result is obtained by the one vector initialization standard model which achieves a 0.9 BLEU improvement.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9993465542793274}]}, {"text": "While this results is similar to the one obtained with the standard model, the training time is reduced hereby a third.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The 5 closest words in the representation spaces of the standard and LBL language models.  word (frequency)  model  space  5 most closest words  is  standard  context  was are were be been  900, 350  standard prediction  was has would had will  LBL  both  was reveals proves are ON  are  standard  context  were is was be been  478, 440  standard prediction  were could will have can  LBL  both  were is was FOR ON  have  standard  context  had has of also the  465, 417  standard prediction  are were provide remain will  LBL  both  had has Have were embrace  meeting  standard  context  meetings conference them 10 talks  150, 317  standard prediction undertaking seminar meetings gathering project  LBL  both  meetings summit gathering festival hearing  Imam  standard  context  PCN rebellion 116. Cuba 49  787  standard prediction  Castro Sen Nacional Al-Ross  LBL  both  Salah Khaled Al-Muhammad Khalid  1947  standard  context  36 Mercosur definite 2002-2003 era  774  standard prediction", "labels": [], "entities": [{"text": "summit gathering festival hearing  Imam  standard  context  PCN rebellion 116", "start_pos": 732, "end_pos": 809, "type": "DATASET", "confidence": 0.5645260035991668}]}, {"text": " Table 2: Summary of the perplexity (PPX) results mea- sured on the same development set with the different con- tinuous space language models. For all of them, the prob- abilities are combined with the back-off n-gram model", "labels": [], "entities": []}, {"text": " Table 3: The 5 closest words in the context space of the standard and one vector initialization language models  word (freq.)  model  5 closest words  is  standard  was are were been remains  900, 350  1 vector init.  was are be were been  conducted  standard  undertaken launched $270,900 Mufamadi 6.44-km-long  18, 388  1 vector init.  pursued conducts commissioned initiated executed  Cambodian  standard  Shyorongi $3,192,700 Zairian depreciations teachers  2, 381  1 vector init.  Danish Latvian Estonian Belarussian Bangladeshi  automatically  standard  MSSD Sarvodaya $676,603,059 Kissana 2,652,627  1, 528  1 vector init.  routinely occasionally invariably inadvertently seldom  Tosevski  standard  $12.3 Action,3 Kassouma 3536 Applique  34  1 vector init.  Shafei Garvalov Dostiev Bourloyannis-Vrailas Grandi  October-12  standard  39,572 anti-Hutu $12,852,200 non-contracting Party's  8  1 vector init.  March-26 April-11 October-1 June-30 August4  3727th  standard  Raqu Tatsei Ayatallah Mesyats Langlois  1  1 vector init.  4160th 3651st 3487th 3378th 3558th", "labels": [], "entities": [{"text": "Danish Latvian Estonian Belarussian Bangladeshi  automatically  standard  MSSD Sarvodaya", "start_pos": 487, "end_pos": 575, "type": "DATASET", "confidence": 0.7457940909597609}]}, {"text": " Table 4: BLEU scores on the NIST MT08 test set with  different language models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989563226699829}, {"text": "NIST MT08 test set", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.9266111999750137}]}]}