{"title": [{"text": "PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts", "labels": [], "entities": [{"text": "Paraphrase Evaluation Metric Exploiting Parallel Texts", "start_pos": 7, "end_pos": 61, "type": "TASK", "confidence": 0.8459737797578176}]}], "abstractContent": [{"text": "We present PEM, the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems.", "labels": [], "entities": [{"text": "PEM", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9022385478019714}, {"text": "paraphrase generation", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.8542400896549225}]}, {"text": "Our metric is based on three criteria: adequacy, fluency, and lexical dissimilar-ity.", "labels": [], "entities": []}, {"text": "The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity.", "labels": [], "entities": []}, {"text": "Human evaluation shows that PEM achieves high correlation with human judgments.", "labels": [], "entities": [{"text": "PEM", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9427719116210938}]}], "introductionContent": [{"text": "In recent years, there has been an increasing interest in the task of paraphrase generation (PG).", "labels": [], "entities": [{"text": "paraphrase generation (PG)", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.9033964157104493}]}, {"text": "At the same time, the task has seen applications such as machine translation (MT), MT evaluation (;), summary evaluation (), and question answering).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8046143770217895}, {"text": "MT evaluation", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.9490346610546112}, {"text": "question answering", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.8950801491737366}]}, {"text": "Despite the research activities, we see two major problems in the field.", "labels": [], "entities": []}, {"text": "First, there is currently no consensus on what attributes characterize a good paraphrase.", "labels": [], "entities": []}, {"text": "As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system.", "labels": [], "entities": []}, {"text": "Second, and as a consequence, no automatic evaluation metric exists for paraphrases.", "labels": [], "entities": []}, {"text": "Most works in this area resort toad hoc manual evaluations, such as the percentage of \"yes\" judgments to the question of \"is the meaning preserved\".", "labels": [], "entities": []}, {"text": "This type of evaluation is incomprehensive, expensive, and non-comparable between different studies, making progress hard to judge.", "labels": [], "entities": []}, {"text": "In this work we address both problems.", "labels": [], "entities": []}, {"text": "We propose a set of three criteria for good paraphrases: adequacy, fluency, and lexical dissimilarity.", "labels": [], "entities": []}, {"text": "Considering that paraphrase evaluation is a very subjective task with no rigid definition, we conduct experiments with human judges to show that humans generally have a consistent intuition for good paraphrases, and that the three criteria are good indicators.", "labels": [], "entities": [{"text": "paraphrase evaluation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.955036848783493}]}, {"text": "Based on these criteria, we construct PEM (Paraphrase Evaluation Metric), a fully automatic evaluation metric for PG systems.", "labels": [], "entities": [{"text": "PEM", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9166675209999084}]}, {"text": "PEM takes as input the original sentence Rand its paraphrase candidate P , and outputs a single numeric score b estimating the quality of P as a paraphrase of R.", "labels": [], "entities": []}, {"text": "PG systems can be compared based on the average scores of their output paraphrases.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first automatic metric that gives an objective and unambiguous ranking of different PG systems, which serves as a benchmark of progress in the field of PG.", "labels": [], "entities": [{"text": "PG", "start_pos": 194, "end_pos": 196, "type": "TASK", "confidence": 0.9537755846977234}]}, {"text": "The main difficulty of deriving PEM is to measure semantic closeness without relying on lexical level similarity.", "labels": [], "entities": []}, {"text": "To this end, we propose bag of pivot language N-grams (BPNG) as a robust, broad-coverage, and knowledge-lean semantic representation for natural language sentences.", "labels": [], "entities": []}, {"text": "Most importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation.", "labels": [], "entities": [{"text": "BPNG", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.4759373366832733}]}, {"text": "The only linguistic re-source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language.", "labels": [], "entities": []}, {"text": "We highlight that paraphrase evaluation and paraphrase recognition are related yet distinct tasks.", "labels": [], "entities": [{"text": "paraphrase evaluation", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.9302574396133423}, {"text": "paraphrase recognition", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.915518045425415}]}, {"text": "Consider two sentences S 1 and S 2 that are the same except for the substitution of a single synonym.", "labels": [], "entities": []}, {"text": "A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.8534665405750275}]}, {"text": "Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.7589391469955444}]}, {"text": "We define the task of paraphrase evaluation in Section 3 and develop our metric in Section 4.", "labels": [], "entities": [{"text": "paraphrase evaluation", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.9365323483943939}]}, {"text": "We conduct a human evaluation and analyze the results in Section 5.", "labels": [], "entities": []}, {"text": "The correlation of PEM with human judgments is studied in Section 6.", "labels": [], "entities": [{"text": "PEM", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9695742130279541}, {"text": "Section 6", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.930436909198761}]}, {"text": "Finally, we discuss our findings and future work in Section 7 and conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we devise our metric according to the three proposed evaluation criteria, namely adequacy, fluency, and dissimilarity.", "labels": [], "entities": []}, {"text": "The main challenge is to measure the adequacy, or semantic similarity, completely independent of any lexical similarity.", "labels": [], "entities": []}, {"text": "We address this problem in Sections 4.1 to 4.3.", "labels": [], "entities": []}, {"text": "The remaining two criteria are addressed in Section 4.4, and we describe the final combined metric PEM in Section 4.5.", "labels": [], "entities": []}, {"text": "To validate our definition of paraphrase evaluation and the PEM method, we conduct an experiment to evaluate paraphrase qualities manually, which allows us to judge whether paraphrase evaluation according to our definition is an inherently coherent and well-defined problem.", "labels": [], "entities": [{"text": "paraphrase evaluation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.8658264577388763}]}, {"text": "The evaluation also allows us to establish an upper bound for the paraphrase evaluation task, and to validate the contribution of the three proposed criteria to the overall paraphrase score.", "labels": [], "entities": [{"text": "paraphrase evaluation task", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.8726462324460348}]}, {"text": "We use the Multiple-Translation Chinese Corpus (MTC) 2 as a source of paraphrases.", "labels": [], "entities": [{"text": "Multiple-Translation Chinese Corpus (MTC) 2", "start_pos": 11, "end_pos": 54, "type": "DATASET", "confidence": 0.819479546376637}]}, {"text": "The MTC corpus consists of Chinese news articles (993 sentences in total) and multiple sentence-aligned English translations.", "labels": [], "entities": [{"text": "MTC corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7815241813659668}]}, {"text": "We select one human translation as the original text.", "labels": [], "entities": []}, {"text": "Two other human translations and two automatic machine translations serve as paraphrases of the original sentences.", "labels": [], "entities": []}, {"text": "We refer to the two human translations and the two MT systems as paraphrase systems human1, human2, machine1, and machine2.", "labels": [], "entities": []}, {"text": "We employ three human judges to manually assess the quality of 300 original sentences paired with each of the four paraphrases.", "labels": [], "entities": []}, {"text": "Therefore, each judge assesses 1,200 paraphrase pairs in total.", "labels": [], "entities": []}, {"text": "The judgment for each paraphrase pair consists of four scores, each given on a five-point scale: \u2022 Adequacy (Is the meaning preserved adequately?)", "labels": [], "entities": []}, {"text": "\u2022 Fluency (Is the paraphrase fluent English?)", "labels": [], "entities": [{"text": "Fluency", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9537832736968994}]}, {"text": "\u2022 Lexical Dissimilarity (How much has the paraphrase changed the original sentence?)", "labels": [], "entities": [{"text": "Lexical Dissimilarity", "start_pos": 2, "end_pos": 23, "type": "TASK", "confidence": 0.806170254945755}]}, {"text": "\u2022 Overall score The instructions given to the judges for the overall score were as follows.", "labels": [], "entities": []}, {"text": "A good paraphrase should convey the same meaning as the original sentence, while being as different as possible on the surface form and being fluent and grammatical English.", "labels": [], "entities": []}, {"text": "With respect to this definition, give an overall score from 5 (perfect) to 1 (unacceptable) for this paraphrase.", "labels": [], "entities": []}, {"text": "The paraphrases are presented to the judges in a random order and without any information as to which paraphrase system produced the paraphrase.", "labels": [], "entities": []}, {"text": "In addition to the four paraphrase systems mentioned above, for each original English sentence, we add three more artificially constructed paraphrases with pre-determined \"human\" judgment scores: (1) the original sentence itself, with adequacy 5, fluency 5, dissimilarity 1, and overall score 2; (2) a random sentence drawn from the same domain, with adequacy 1, fluency 5, dissimilarity 5, and overall score 1; and (3) a random sentence generated by a unigram language model, with adequacy 1, fluency 1, dissimilarity 5, and overall score 1.", "labels": [], "entities": []}, {"text": "These artificial paraphrases serve as controls in our evaluation.", "labels": [], "entities": []}, {"text": "Our final data set therefore consists of 2,100 paraphrase pairs with judgments on 4 different criteria.", "labels": [], "entities": []}, {"text": "We build the phrase table used to evaluate the pivot language F 1 from the FBIS Chinese-English corpus, consisting of about 250,000 Chinese sentences, each with a single English translation.", "labels": [], "entities": [{"text": "FBIS Chinese-English corpus", "start_pos": 75, "end_pos": 102, "type": "DATASET", "confidence": 0.9339166482289633}]}, {"text": "The paraphrases are taken from the MTC corpus in the same way as the human experiment described in Section 5.1.", "labels": [], "entities": [{"text": "MTC corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.8112488090991974}]}, {"text": "Both FBIS and MTC are in the Chinese newswire domain.", "labels": [], "entities": [{"text": "MTC", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8431967496871948}]}, {"text": "We stem all English words in both data sets with the Porter stemmer.", "labels": [], "entities": []}, {"text": "We use the maximum entropy segmenter of () to segment the Chinese part of the FBIS corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8082908987998962}]}, {"text": "Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner () with five iterations of training.", "labels": [], "entities": []}, {"text": "Phrases are then extracted with the widelyused heuristic in.", "labels": [], "entities": []}, {"text": "We extract phrases of up to four words in length.", "labels": [], "entities": []}, {"text": "Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3.", "labels": [], "entities": []}, {"text": "For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probabilities higher than 0.01 in the bag of N-grams.", "labels": [], "entities": []}, {"text": "We collect N-grams up to length four.", "labels": [], "entities": []}, {"text": "The language model used to judge fluency is trained on the English side of the FBIS parallel text.", "labels": [], "entities": [{"text": "FBIS parallel text", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.7517291704813639}]}, {"text": "We use SRILM) to build a 4-gram model with the default parameters.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.5683066844940186}]}, {"text": "The PEM SVM regression is trained on the paraphrase pairs for the first 200 original English sentences and tested on the paraphrase pairs of the remaining 100 original English sentences.", "labels": [], "entities": [{"text": "PEM SVM", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.5894952714443207}]}, {"text": "Thus, there are 1,400 instances for training and 700 instances for testing.", "labels": [], "entities": []}, {"text": "For each instance, we calculate the values", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-judge correlation for overall paraphrase  score", "labels": [], "entities": []}, {"text": " Table 2: Correlation of paraphrase criteria with overall  score", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9204251766204834}]}, {"text": " Table 3: Correlation of PEM with human judgment (over- all score)", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9268686175346375}, {"text": "PEM", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8361227512359619}]}]}