{"title": [{"text": "A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7822548151016235}, {"text": "MCMC", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.861585795879364}]}], "abstractContent": [{"text": "A word in one language can be translated to zero, one, or several words in other languages.", "labels": [], "entities": []}, {"text": "Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.7454133927822113}, {"text": "statistical machine translation", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.7264845371246338}]}, {"text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.", "labels": [], "entities": []}, {"text": "This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 35, "end_pos": 55, "type": "METRIC", "confidence": 0.8312065601348877}]}, {"text": "It is similar in some ways to IBM Model 4, but is much easier to understand.", "labels": [], "entities": []}, {"text": "We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "labels": [], "entities": []}], "introductionContent": [{"text": "IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.8320741653442383}, {"text": "word alignment", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.6829072833061218}]}, {"text": "There are three kinds of important information for word alignment models: lexicality, locality and fertility.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7897772490978241}]}, {"text": "IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago.", "labels": [], "entities": []}, {"text": "Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.", "labels": [], "entities": []}, {"text": "Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the target language.", "labels": [], "entities": []}, {"text": "However, IBM Model 4 is so complex that most researches use the GIZA++ software package, and IBM Model 4 itself is treated as a black box.", "labels": [], "entities": [{"text": "GIZA++ software package", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.8763371258974075}]}, {"text": "The complexity in IBM Model 4 makes it hard to understand and to improve.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.8721365133921305}]}, {"text": "Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.", "labels": [], "entities": []}, {"text": "We also want it to be accurate and computationally efficient.", "labels": [], "entities": []}, {"text": "There have been many years of research on word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.8144262433052063}]}, {"text": "Our work is different from others in essential ways.", "labels": [], "entities": []}, {"text": "Most other researchers take either the HMM alignments () or IBM Model 4 alignments as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.", "labels": [], "entities": []}, {"text": "Directly modeling fertility makes our model fundamentally different from others.", "labels": [], "entities": []}, {"text": "Most models have limited ability to model fertility.", "labels": [], "entities": []}, {"text": "learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1.", "labels": [], "entities": [{"text": "fertility", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9523653388023376}]}, {"text": "ITG models () assume the fertility to be either zero or one.", "labels": [], "entities": [{"text": "ITG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8944945335388184}]}, {"text": "It can model phrases, but the phrase has to be contiguous.", "labels": [], "entities": []}, {"text": "There have been works that try to simulate fertility using the hidden Markov model (), but we prefer to model fertility directly.", "labels": [], "entities": []}, {"text": "Our model is a coherent generative model that combines the HMM and IBM Model 4.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.9144935806592306}]}, {"text": "It is easier to understand than IBM Model 4 (see Section 3).", "labels": [], "entities": []}, {"text": "Our model also removes several undesired properties in IBM Model 4.", "labels": [], "entities": []}, {"text": "We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter estimation.", "labels": [], "entities": []}, {"text": "Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (.", "labels": [], "entities": [{"text": "distortion", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9527702927589417}, {"text": "inverse distortion", "start_pos": 89, "end_pos": 107, "type": "METRIC", "confidence": 0.9164468348026276}]}, {"text": "Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable.", "labels": [], "entities": []}, {"text": "Our model is much faster than IBM Model 4.", "labels": [], "entities": []}, {"text": "In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 73, "end_pos": 93, "type": "METRIC", "confidence": 0.9435664812723795}]}, {"text": "Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. and first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility.", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8464436233043671}, {"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7256444692611694}]}, {"text": "If the optimal alignment is not in those neighbors, this method will not be able find the optimal alignment.", "labels": [], "entities": []}, {"text": "We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, which has nice probabilistic guarantees.", "labels": [], "entities": []}, {"text": "applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.8178948760032654}, {"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7904199063777924}]}], "datasetContent": [{"text": "We evaluated our model by computing the word alignment and machine translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7628498673439026}, {"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6531917452812195}]}, {"text": "We use the alignment error rate (AER) as the word alignment evaluation criterion.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 11, "end_pos": 37, "type": "METRIC", "confidence": 0.9461603959401449}, {"text": "word alignment evaluation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6484269996484121}]}, {"text": "Let Abe the alignments output byword alignment system, P be a set of possible alignments, and S be a set of sure alignments both labeled by human beings.", "labels": [], "entities": [{"text": "alignments output byword alignment", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.615366019308567}]}, {"text": "S is a subset of P . Precision, recall, and AER are defined as follows: AER is an extension to F-score.", "labels": [], "entities": [{"text": "Precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9968543648719788}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9968200922012329}, {"text": "AER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9990323781967163}, {"text": "AER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9978160858154297}, {"text": "F-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9429848194122314}]}, {"text": "We evaluate our fertility models on a ChineseEnglish corpus.", "labels": [], "entities": [{"text": "ChineseEnglish corpus", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.9861269593238831}]}, {"text": "The Chinese-English data taken from FBIS newswire data, and has 380K sentence pairs, and we use the first 100K sentence pairs as our training data.", "labels": [], "entities": [{"text": "FBIS newswire data", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.959986130396525}]}, {"text": "We used hand-aligned data as reference.", "labels": [], "entities": []}, {"text": "The Chinese-English data has 491 sentence pairs.", "labels": [], "entities": []}, {"text": "We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.9515811403592428}, {"text": "IBM Model 1", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9707436164220175}]}, {"text": "We smooth all parameters (\u03bb(e) and P (f |e)) by adding a small value (10 \u22128 ), so they never become too small.", "labels": [], "entities": []}, {"text": "We run both models for 5 iterations.", "labels": [], "entities": []}, {"text": "AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.", "labels": [], "entities": [{"text": "AER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7590996623039246}, {"text": "IBM Model 1 Viterbi alignments", "start_pos": 35, "end_pos": 65, "type": "DATASET", "confidence": 0.9032356262207031}]}, {"text": "We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.9273104071617126}]}, {"text": "We smooth all parameters (\u03bb(e), P (a|a \u2032 ) and P (f |e)) by adding a small value (10 \u22128 ).", "labels": [], "entities": []}, {"text": "We run both models for 5 iterations.", "labels": [], "entities": []}, {"text": "AER results are computed using traditional HMM Viterbi decoding for both models.", "labels": [], "entities": [{"text": "AER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9631761312484741}]}, {"text": "It is always difficult to determine how many samples are enough for sampling algorithms.", "labels": [], "entities": []}, {"text": "However, both fertility models achieve better results than their baseline models using a small amount of samples.", "labels": [], "entities": []}, {"text": "For the fertility IBM Model 1, we sample 10 times for each a j , and restart 3 times in the training stage; we sample 100 times and restart 12 times in the testing stage.", "labels": [], "entities": []}, {"text": "For the fertility HMM, we sample 30 times for each a j with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.", "labels": [], "entities": []}, {"text": "More samples give no further improvement.", "labels": [], "entities": []}, {"text": "Initially, the fertility IBM Model 1 and fertility HMM did not perform well.", "labels": [], "entities": [{"text": "fertility HMM", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.6625905632972717}]}, {"text": "If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter \u03bb(e).", "labels": [], "entities": []}, {"text": "Hence, smoothing is needed.", "labels": [], "entities": []}, {"text": "One may try to solve it by forcing all these words to share a same parameter \u03bb(e infrequent ).", "labels": [], "entities": []}, {"text": "Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.", "labels": [], "entities": []}, {"text": "We solve the problem in the following way: estimate the parameter \u03bb(e nonempty ) for all non-empty words, all infrequent words share this parameter.", "labels": [], "entities": []}, {"text": "We consider words that appear less than 10 times as infrequent words., and shows the AER results for different models.", "labels": [], "entities": [{"text": "AER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9989795088768005}]}, {"text": "We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.9288638035456339}]}, {"text": "The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.", "labels": [], "entities": [{"text": "AER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9993964433670044}]}, {"text": "show the training time for different models.", "labels": [], "entities": []}, {"text": "In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM.", "labels": [], "entities": [{"text": "AER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9996613264083862}]}, {"text": "It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase).", "labels": [], "entities": [{"text": "AER", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.999509334564209}]}, {"text": "We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model.", "labels": [], "entities": [{"text": "AER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9994650483131409}]}, {"text": "We also evaluate our model by computing the machine translation BLEU score () using the Moses system (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9562084078788757}]}, {"text": "The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic.", "labels": [], "entities": [{"text": "word alignment evaluation bitexts", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.7588599547743797}]}, {"text": "Our testis 633 sentences of up to length 50, with four references.", "labels": [], "entities": []}, {"text": "Results are shown in Table 2; we see that better word alignment results do not lead to better translations.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.717966765165329}]}], "tableCaptions": [{"text": " Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1,  5, and 30 for the fertility HMM.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9807157516479492}, {"text": "IBM1F", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.8544093370437622}]}]}