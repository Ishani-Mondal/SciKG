{"title": [{"text": "What a Parser can Learn from a Semantic Role Labeler and Vice Versa", "labels": [], "entities": [{"text": "Semantic Role Labeler", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.6942204236984253}]}], "abstractContent": [{"text": "In many NLP systems, there is a unidirectional flow of information in which a parser supplies input to a semantic role labeler.", "labels": [], "entities": []}, {"text": "In this paper, we build a system that allows information to flow in both directions.", "labels": [], "entities": []}, {"text": "We make use of semantic role predictions in choosing a single-best parse.", "labels": [], "entities": []}, {"text": "This process relies on an averaged perceptron model to distinguish likely semantic roles from erroneous ones.", "labels": [], "entities": []}, {"text": "Our system penalizes parses that give rise to low-scoring semantic roles.", "labels": [], "entities": []}, {"text": "To explore the consequences of this we perform two experiments.", "labels": [], "entities": []}, {"text": "First, we use a baseline gen-erative model to produce n-best parses, which are then reordered by our semantic model.", "labels": [], "entities": []}, {"text": "Second, we use a modified version of our semantic role labeler to predict semantic roles at parse time.", "labels": [], "entities": []}, {"text": "The performance of this modified labeler is weaker than that of our best full SRL, because it is restricted to features that can be computed directly from the parser's packed chart.", "labels": [], "entities": []}, {"text": "For both experiments, the resulting semantic predictions are then used to select parses.", "labels": [], "entities": []}, {"text": "Finally , we feed the selected parses produced by each experiment to the full version of our semantic role labeler.", "labels": [], "entities": []}, {"text": "We find that SRL performance can be improved over this baseline by selecting parses with likely semantic roles.", "labels": [], "entities": [{"text": "SRL", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9674917459487915}]}], "introductionContent": [{"text": "In the semantic role labeling task, words or groups of words are described in terms of their relations to a predicate.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.7436282932758331}]}, {"text": "For example, the sentence Robin admires Leslie has two semantic role-bearing words: Robin is the agent or experiencer of the admire predicate, and Leslie is the patient.", "labels": [], "entities": []}, {"text": "These semantic relations are distinct from syntactic relations like subject and object -the proper nouns in the sentence Leslie is admired by Robin have the same semantic relationships as Robin admires Leslie, even though the syntax differs.", "labels": [], "entities": []}, {"text": "Although syntax and semantics do not always align with each other, they are correlated.", "labels": [], "entities": []}, {"text": "Almost all automatic semantic role labeling systems take a syntactic representation of a sentence (taken from an automatic parser or a human annotator), and use the syntactic information to predict semantic roles.", "labels": [], "entities": [{"text": "automatic semantic role labeling", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.6671288162469864}]}, {"text": "When a semantic role labeler predicts an incorrect role, it is often due to an error in the parse tree.", "labels": [], "entities": []}, {"text": "Consider the erroneously annotated sentence from the Penn Treebank corpus shown in.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.9953999320665995}]}, {"text": "If a semantic role labeling system relies heavily upon syntactic attachment decisions, then it will likely predict that in 1956 describes the time that asbestos was used, rather than when it ceased to be used.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.6312894721825918}]}, {"text": "Errors of this kind are common in treebanks and in automatic parses.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9456937313079834}]}, {"text": "It is telling, though, that while the handannotated Penn Treebank (, the Charniak parser, and the C&C parser () all produce the erroneous parse from, the hand-annotated Propbank corpus of verbal semantic roles () correctly identifies in 1956 as a temporal modifier of stopped, rather than using.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9631428122520447}, {"text": "C&C parser", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.8776217103004456}, {"text": "Propbank corpus of verbal semantic roles", "start_pos": 169, "end_pos": 209, "type": "DATASET", "confidence": 0.9316374262173971}]}, {"text": "This demonstrates that while syntactic attachment decisions like these are difficult for humans and for automatic parsers, a human reader has little difficulty identifying the correct semantic relationship between the temporal modifier and the verbs.", "labels": [], "entities": []}, {"text": "This is likely due to the fact that the meaning suggested by the parse in is unlikelythe reader instinctively feels that a temporal modifier fits better with the verb stop than with the verb use.", "labels": [], "entities": []}, {"text": "In this paper, we will use the idea that semantic roles predicted by correct parses are more natural than semantic roles predicted by erroneous parses.", "labels": [], "entities": []}, {"text": "By modifying a state-of-the-art CCG semantic role labeler to predict semantic roles at parse time, or by using it to select from an n-best list, we can prefer analyses that yield likely semantic roles.", "labels": [], "entities": []}, {"text": "Syntactic analysis is treated not as an autonomous task, but rather as a contributor to the final goal of semantic role labeling.", "labels": [], "entities": [{"text": "Syntactic analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9404435455799103}, {"text": "semantic role labeling", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.6518047054608663}]}, {"text": "Figure 1: A parse tree based on the treebank parse of wsj 0003.3.", "labels": [], "entities": []}, {"text": "Notice that the temporal adjunct is erroneously attached low.", "labels": [], "entities": []}, {"text": "Ina syntax-based SRL system, this will likely lead to a role prediction error.", "labels": [], "entities": [{"text": "SRL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9431998133659363}, {"text": "role prediction", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8792679607868195}]}], "datasetContent": [{"text": "Our first experiment demonstrates our model's performance in a ranking task.", "labels": [], "entities": []}, {"text": "In this task, a list of candidate parses are generated by our baseline model.", "labels": [], "entities": []}, {"text": "This baseline model treats rule applications as a PCFG -each rule application (say, np + s\\np = s) is given a probability in the standard way.", "labels": [], "entities": []}, {"text": "The rule probabilities are unsmoothed maximum likelihood estimates derived from rule counts in the training portion of CCGbank.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9776656627655029}]}, {"text": "After n-best derivations are produced by the baseline model, we use the Brutus semantic role labeler to assign roles to each candidate derivation.", "labels": [], "entities": []}, {"text": "We vary the size of the n-best list from 1 to 10 (note that an n-best list of size 1 is equivalent to the single-best baseline parse).", "labels": [], "entities": []}, {"text": "We then use the semantic model to re-rank the candidate parses and produce a single-best parse.", "labels": [], "entities": []}, {"text": "The outcomes are shown in  The availability of even two candidate parses yields a 2.1% boost to the balanced F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.873095691204071}]}, {"text": "This is because the semantic role labeler is very sensitive to syntactic attachment decisions, and in many cases the set of rule applications used in the derivation are very similar or even the same.", "labels": [], "entities": []}, {"text": "Consider the simplified version of a phenomenon found in wsj 0001.1 shown in  common in the treebank, the baseline model identifies it as the single-best parse, and identifies the derivation in as the second-best parse.", "labels": [], "entities": []}, {"text": "The semantic model, however, correctly recognizes that the semantic roles predicted by the derivation in are superior to those predicted by the derivation in.", "labels": [], "entities": []}, {"text": "This demonstrates how a second or third-best parse according to the baseline model can be greatly superior to the single-best in terms of semantics.", "labels": [], "entities": []}, {"text": "One potential weakness with the n-best list approach described in Section 7 is choosing the size of the n-best list.", "labels": [], "entities": []}, {"text": "As the length of the sentence grows, the number of candidate analyses grows.", "labels": [], "entities": []}, {"text": "Because sentences in the treebank and in real-world applications are of varying length and complexity, restricting ourselves to an n-best list of a particular size opens us to considering some badly mangled derivations on short, simple sentences, and not enough derivations on long, complicated ones.", "labels": [], "entities": []}, {"text": "One possible solution to this is to simply choose a single best derivation directly from the packed chart using the semantic model, eschewing the baseline model entirely except for breaking ties.", "labels": [], "entities": []}, {"text": "In this approach, we use the local SRL model described in section 6 to predict semantic roles at parse time, inside the packed chart.", "labels": [], "entities": []}, {"text": "This frees us from the need to have a complete derivation (as in the n-best list approach in Section 7).", "labels": [], "entities": []}, {"text": "We use the semantic model to choose a single-best parse from the packed chart, then we pass this complete parse through the global SRL model to give it all the benefits afforded to the parses in the n-best approach.", "labels": [], "entities": []}, {"text": "The results for the semantic model compared to the baseline model are shown in  the semantic model performs considerably worse than the baseline model.", "labels": [], "entities": []}, {"text": "To understand why, it is necessary to remember that the semantic model uses only semantic features -probabilities of rule applications are not considered.", "labels": [], "entities": []}, {"text": "Therefore, the semantic model is perfectly happy to predict derivations with sequences of highly unlikely rule applications so long as they predict a role that the model has been trained to prefer.", "labels": [], "entities": []}, {"text": "Apparently, the reckless pursuit of appealing semantic roles can ultimately harm semantic role labeling accuracy as well as parse accuracy.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.6069042483965555}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.8736081123352051}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.7858653664588928}]}, {"text": "Consider the analysis shown in.", "labels": [], "entities": []}, {"text": "Because the averaged perceptron semantic model is not sensitive to the relationships between different semantic roles, and because Arg1 of name is a \"good\" semantic role, the semantic model predicts as many of them as it can.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9844638705253601}]}, {"text": "The very common np-appositive construction is particularly vulnerable to this kind of error, as it can be easily mistaken fora three-way coordination (like carrots, peas and watermelon).", "labels": [], "entities": []}, {"text": "Many of the precision errors generated by the local model are of this nature, and the global model is unlikely to remove them, given the presence of strong dependencies between each of the \"subjects\" and the predicate.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9963120818138123}]}, {"text": "Coordination errors are also common when dealing with relative clause attachment.", "labels": [], "entities": [{"text": "relative clause attachment", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.7127461234728495}]}, {"text": "To a PCFG model, there is little difference between attaching the relative clause to the researchers or Lorillard nor the researchers.", "labels": [], "entities": [{"text": "Lorillard", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.8736481666564941}]}, {"text": "The semantic model, however, would rather predict two semantic roles than just one (because study:Arg0 is a highly appealing semantic role).", "labels": [], "entities": [{"text": "Arg0", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9786102771759033}]}, {"text": "Once again, the pursuit of appealing semantic roles has led the system astray.", "labels": [], "entities": []}, {"text": "We have shown in Section 7 that the semantic model can improve SRL performance when it is constrained to the most likely PCFG derivations, but enumerating n-best lists is costly and cumbersome.", "labels": [], "entities": [{"text": "SRL", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9882733821868896}]}, {"text": "We can, however, combine the semantic model with the baseline PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9326230883598328}]}, {"text": "Our method for doing this is designed to avoid the kinds of error described above.", "labels": [], "entities": []}, {"text": "We first identify the highest-scoring parse according to the PCFG model.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9019004702568054}]}, {"text": "This parse will be used in later processing unless we are able to identify another parse that satisfies the following criteria: 1.", "labels": [], "entities": []}, {"text": "It must be closely related to the parse that has the best score according to the semantic model.", "labels": [], "entities": []}, {"text": "To identify such parses, we ask the chart unpacking algorithm to generate all the parses that can be reached by making up to five attachment changes to this semantically preferred parse -no more.", "labels": [], "entities": []}, {"text": "2. It must have a PCFG score that is not much less than that of the single-best PCFG parse.", "labels": [], "entities": [{"text": "PCFG score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.6572243273258209}]}, {"text": "We do this by requiring that it has a score that is within a factor of \u03b1 of the best available.", "labels": [], "entities": []}, {"text": "That is, the single-best parse from the semantic model must satisfy log P (sem) > log P (baseline) + log(\u03b1) where the \u03b1 value is tuned on the development set.", "labels": [], "entities": []}, {"text": "If no semantically preferred parse meets the above criteria, the single-best PCFG parse is used.", "labels": [], "entities": []}, {"text": "We find that the PCFG-preferred parse is used about 35% of the time and an alternative used instead about 65% of the time.", "labels": [], "entities": [{"text": "PCFG-preferred", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.8422836065292358}]}, {"text": "The SRL performance for this regime, using a range of cut-off factors, is shown in table 4.", "labels": [], "entities": [{"text": "SRL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.501535177230835}]}, {"text": "On this basis we select a cut-off of 0.5 as suitable for use for final testing.", "labels": [], "entities": []}, {"text": "On the development set this method gives the best precision in extracting dependencies, but is slightly inferior to the method using a 2-best list on recall and balanced F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9965470433235168}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9968475699424744}, {"text": "F-measure", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9917455315589905}]}, {"text": "Factor: A parse produced by the unrestricted semantic model.", "labels": [], "entities": []}, {"text": "Notice that Rudolph Agnew, 61 and the former chairman is erroneously treated as a three-way conjunction, assigning semantic roles to all three heads.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SRL results for treebank parses, using the local model  described in Section 5 and the full global model.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7956762909889221}, {"text": "treebank parses", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.5075707137584686}]}, {"text": " Table 2: SRL performance on the development set (section 00)  for various values of n. The final row indicates SRL perfor- mance on section 00 parses from the Clark and Curran CCG  parser.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8777123093605042}, {"text": "SRL perfor- mance", "start_pos": 112, "end_pos": 129, "type": "METRIC", "confidence": 0.770188495516777}, {"text": "Clark and Curran CCG  parser", "start_pos": 160, "end_pos": 188, "type": "DATASET", "confidence": 0.7097020328044892}]}, {"text": " Table 3: A comparison of the performance of the baseline model  and the semantic model on semantic role labeling. The seman- tic model, when unrestrained by the baseline model, performs  substantially worse.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.6829054156939188}]}, {"text": " Table 4: SRL accuracy when the semantic model is constrained  by the baseline model", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9068073034286499}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.962932288646698}]}, {"text": " Table 5: The full system results on the test set of the WSJ  corpus (Section 23). Included are the baseline parser, the n- best reranking model from Section 7, the single-best chart- unpacking model from Section 8, and the state-of-the-art C&C  parser. The final row shows the SRL performance obtained by", "labels": [], "entities": [{"text": "WSJ  corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9511469006538391}, {"text": "SRL", "start_pos": 278, "end_pos": 281, "type": "TASK", "confidence": 0.6250965595245361}]}]}