{"title": [{"text": "A Probabilistic Morphological Analyzer for Syriac", "labels": [], "entities": []}], "abstractContent": [{"text": "We define a probabilistic morphological ana-lyzer using a data-driven approach for Syriac in order to facilitate the creation of an annotated corpus.", "labels": [], "entities": []}, {"text": "Syriac is an under-resourced Semitic language for which there are no available language tools such as morphological analyzers.", "labels": [], "entities": []}, {"text": "We introduce novel probabilistic models for segmentation, dictionary linkage, and morphological tagging and connect them in a pipeline to create a probabilistic morphological analyzer requiring only labeled data.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.6955144107341766}]}, {"text": "We explore the performance of models with varying amounts of training data and find that with about 34,500 labeled tokens, we can outperform a reasonable baseline trained on over 99,000 tokens and achieve an accuracy of just over 80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9992380142211914}]}, {"text": "When trained on all available training data, our joint model achieves 86.47% accuracy, a 29.7% reduction in error rate over the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9987851977348328}, {"text": "error rate", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9861619174480438}]}], "introductionContent": [{"text": "Our objective is to facilitate the annotation of a large corpus of classical Syriac (referred to simply as \"Syriac\" throughout the remainder of this work).", "labels": [], "entities": []}, {"text": "Syriac is an under-resourced Western Semitic language of the Christian Near East and a dialect of Aramaic.", "labels": [], "entities": []}, {"text": "It is currently employed almost entirely as a liturgical language but was a true spoken language up until the eighth century, during which time many prolific authors wrote in Syriac.", "labels": [], "entities": []}, {"text": "Even today there are texts still being composed in or translated into Syriac.", "labels": [], "entities": []}, {"text": "By automatically annotating these texts with linguistically useful information, we will facilitate systematic study by scholars of Syriac, the Near East, and Eastern Christianity.", "labels": [], "entities": []}, {"text": "Furthermore, languages that are linguistically similar to Syriac (e.g., Arabic and Hebrew) may benefit from the methodology presented here.", "labels": [], "entities": []}, {"text": "Our desired annotations include morphological segmentation, links to dictionary entries, and morphological attributes.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7069135308265686}]}, {"text": "Typically, annotations of this kind are made with the assistance of language tools, such as morphological analyzers, segmenters, or part-of-speech (POS) taggers.", "labels": [], "entities": []}, {"text": "Such tools do not exist for Syriac, but some labeled data does exist: compiled an annotated version of the Peshitta New Testament (1920) and a concordance thereof.", "labels": [], "entities": [{"text": "Peshitta New Testament (1920)", "start_pos": 107, "end_pos": 136, "type": "DATASET", "confidence": 0.8717026015122732}]}, {"text": "We aim to replicate this kind of annotation on a much larger scale with more modern tools, building up from the labeled New Testament data, our only resource.", "labels": [], "entities": [{"text": "New Testament data", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.7049342195192972}]}, {"text": "Motivated by this state of affairs, our learning and annotation framework requires only labeled data.", "labels": [], "entities": []}, {"text": "We approach the problem of Syriac morphological annotation by creating five probabilistic sub-models that can be trained in a supervised fashion and combined in a joint model of morphological annotation.", "labels": [], "entities": [{"text": "Syriac morphological annotation", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.874589721361796}]}, {"text": "We introduce novel algorithms for segmentation, dictionary linkage, and morphological tagging.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9691987633705139}, {"text": "morphological tagging", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.7091793715953827}]}, {"text": "We then combine these sub-models into a joint nbest pipeline.", "labels": [], "entities": []}, {"text": "This joint model outperforms a strong, though na\u00efve, baseline for all amounts of training data over about 9,900 word tokens.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are using the Syriac Peshitta New Testament in the form compiled by.", "labels": [], "entities": [{"text": "Syriac Peshitta New Testament", "start_pos": 17, "end_pos": 46, "type": "DATASET", "confidence": 0.8232639282941818}]}, {"text": "This data is segmented, annotated with baseform and root, and labeled with morphological attributes.", "labels": [], "entities": []}, {"text": "Kiraz and others in the Syriac community refined and corrected the original annotation while preparing a digital and print concordance of the New Testament.", "labels": [], "entities": []}, {"text": "We augmented Kiraz's version of the data by segmenting suffixes and by streamlining the tagset.", "labels": [], "entities": []}, {"text": "The dataset consists of 109,640 word tokens.", "labels": [], "entities": []}, {"text": "shows part of a tagged Syriac sentence using this tagset.", "labels": [], "entities": []}, {"text": "The suffix and stem tags consist of indices representing morphological attributes.", "labels": [], "entities": []}, {"text": "In the example sentence, the suffix tag 1011 represents the values \"masculine\", \"N/A\", \"plural\", \"normal suffix\" for the suffix attributes of gender, person, number, and contraction.", "labels": [], "entities": []}, {"text": "Each value of 0 for each stem and suffix attribute represents a value of \"N/A\", except for that of grammatical category, which always must have a value other than \"N/A\".", "labels": [], "entities": []}, {"text": "Therefore, the suffix tag 0000 means there is no suffix.", "labels": [], "entities": []}, {"text": "For the stem tags, the attribute order is the same as that shown in from top to bottom.", "labels": [], "entities": []}, {"text": "The following describes the interpretation of the stem values represented in.", "labels": [], "entities": []}, {"text": "Grammatical category values 0, 2, and 3 represent \"verb\", \"noun\", and \"pronoun\", respectively.", "labels": [], "entities": []}, {"text": "(Grammatical category has no \"N/A\" value.)", "labels": [], "entities": []}, {"text": "The verb conjugation value 1 represents \"peal conjugation\".", "labels": [], "entities": []}, {"text": "Aspect value 1 represents \"perfect\".", "labels": [], "entities": []}, {"text": "State value 3 represents \"emphatic\".", "labels": [], "entities": [{"text": "emphatic", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9652608633041382}]}, {"text": "Number values 1 and 2 represent \"singular\" and \"plural\".", "labels": [], "entities": []}, {"text": "Person values 2 and 3 represent \"sec- The Way International, a Biblical research ministry, annotated this version of the New Testament by hand and required 15 years to do so. ond\" and \"third\" person.", "labels": [], "entities": []}, {"text": "Gender values 2 and 3 represent \"masculine\" and \"feminine\".", "labels": [], "entities": []}, {"text": "Pronoun type value 2 represents \"demonstrative\".", "labels": [], "entities": []}, {"text": "Demonstrative category value 2 represents \"far\".", "labels": [], "entities": []}, {"text": "Finally, noun type 2 represents \"common\".", "labels": [], "entities": []}, {"text": "The last two columns of 0 represent \"N/A\" for numeral type and particle type.", "labels": [], "entities": []}, {"text": "We implement five sub-tasks: segmentation, baseform linkage, root linkage, suffix tagging, and stem tagging.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9617449641227722}, {"text": "suffix tagging", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.722197875380516}, {"text": "stem tagging", "start_pos": 95, "end_pos": 107, "type": "TASK", "confidence": 0.7098569869995117}]}, {"text": "We compare each sub-task to a na\u00efve approach as a baseline.", "labels": [], "entities": []}, {"text": "In addition to desiring good sub-models, we also want a joint pipeline model that significantly outperforms the na\u00efve joint approach, which is formed by using each of the following baselines in the pipeline framework.", "labels": [], "entities": []}, {"text": "The baseline implementation of segmentation is to choose the most-frequent label: fora given word, the baseline predicts the segmentation with which that word appeared most frequently during training.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9690232872962952}]}, {"text": "For unknown words, it chooses the largest prefix and largest suffix that is possible for that word from the list of prefixes and suffixes seen during training.", "labels": [], "entities": []}, {"text": "(This na\u00efve baseline for unknown words does not take into account the fact that the stem is often at least three characters in length.)", "labels": [], "entities": []}, {"text": "For dictionary linkage, the baseline is similar: both baseform linkage and root linkage use the mostfrequent label approach.", "labels": [], "entities": []}, {"text": "Given a stem, the baseline baseform linker predicts the baseform with which the stem was seen most frequently during training; likewise, the baseline root linker predicts the root from the baseform in a similar manner.", "labels": [], "entities": []}, {"text": "For the unknown stem case, the baseline baseform linker predicts the baseform to be identical to the stem.", "labels": [], "entities": []}, {"text": "For the unknown baseform case, the baseline root linker predicts a root identical to the first three consonants of the baseform, since for Syriac the root is exactly three consonants in a large majority of the cases.", "labels": [], "entities": []}, {"text": "The baselines for stem and suffix tagging are the most-frequent label approaches.", "labels": [], "entities": [{"text": "stem and suffix tagging", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.6075765937566757}]}, {"text": "These baselines are similar to maxent-mono and maxent-ind, using the monolithic and independent approaches used by maxent-mono and maxent-ind.", "labels": [], "entities": []}, {"text": "The difference is that instead of using maximum entropy, the na\u00efve most-frequent approach is used in its place.", "labels": [], "entities": []}, {"text": "The joint baseline tagger uses each of the component baselines in the n-best joint pipeline framework.", "labels": [], "entities": []}, {"text": "Because this framework is modular, we can trivially swap in and out different models for each of the subtasks.", "labels": [], "entities": []}, {"text": "Since we are focusing on under-resourced circumstances, we sweep the amount of training data and produce learning curves to better understand how our models perform in such circumstances.", "labels": [], "entities": []}, {"text": "For each point in our learning curves and for all other evaluations, we employ ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "The learning curves use the chosen percentage of the data for training and a fixed-size test set from each fold and report the average accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9984453320503235}]}, {"text": "The reported task accuracy requires the entire output for that task to be correct in order to be counted as correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9677929878234863}]}, {"text": "For example, during stem tagging, if one of the sub-tags is incorrect, then the entire tag is said to be incorrect.", "labels": [], "entities": [{"text": "stem tagging", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.839779794216156}]}, {"text": "Furthermore, for syromorph, the outputs of every sub-task must be correct in order for the word token to be counted as correct.", "labels": [], "entities": []}, {"text": "Moving beyond token-level metrics, in order to understand performance of the system at the level of individual decisions (including N/A decisions), we compute decision-level accuracy: we call this metric total-decisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9685444235801697}]}, {"text": "For the syromorph method reported here, there area total of 20 decisions: 2 for segmentation (prefix and suffix boundaries), 1 for baseform linkage, 1 for root linkage, 4 for suffix tagging, and 12 for stem tagging.", "labels": [], "entities": [{"text": "suffix tagging", "start_pos": 175, "end_pos": 189, "type": "TASK", "confidence": 0.7122533470392227}, {"text": "stem tagging", "start_pos": 202, "end_pos": 214, "type": "TASK", "confidence": 0.7033355087041855}]}, {"text": "This accuracy helps us to assess the number of decisions a human annotator would need to correct, if data were preannotated by a given model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9994490742683411}]}, {"text": "Excluding N/A decisions, we compute per-decision coverage and accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.847146213054657}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.998290479183197}]}, {"text": "These metrics are called applicable-coverage and applicable-accuracy.", "labels": [], "entities": []}, {"text": "We show results on both the individual sub-tasks and the entire joint task.", "labels": [], "entities": []}, {"text": "Since previous subtasks can adversely affect tasks further down in the pipeline, we evaluate the sub-models by placing them in the pipeline with other (simulated) submodels that correctly predict every instance.", "labels": [], "entities": []}, {"text": "For example, when testing a root linker, we place the root linker to be evaluated in the pipeline with a segmenter, baseform linker, and taggers that return the correct label for every prediction.", "labels": [], "entities": []}, {"text": "This gives an upper-bound for the individual model, removes the possibility of error propagation, and shows how well that model performs without the effects of the other models in the pipeline.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7210359871387482}]}, {"text": "For our results, unknown accuracy is the accuracy of unknown instances, specific to the task, at training time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9585540890693665}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9987196922302246}]}, {"text": "In the case of baseform linkage, for example, a stem is considered unknown if that stem was not seen during training.", "labels": [], "entities": []}, {"text": "It is therefore possible to have a known word with an unknown stem and vice versa.", "labels": [], "entities": []}, {"text": "As in other NLP problems, unknown instances area manifestation of training data sparsity. is grouped by sub-task and reports the results of each of the baseline sub-tasks in the first row of each group.", "labels": [], "entities": []}, {"text": "Each of the baselines performs surprisingly well.", "labels": [], "entities": []}, {"text": "The accuracies of the baselines for most of the tasks are high because the ambiguity of the labels given the instance is quite low: the average ambiguity across word types for segmentation, baseform linkage, root linkage, suffix tagging, and stem tagging are 1.01, 1.05, 1.02, 1.35, and 1.47, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.970520555973053}, {"text": "suffix tagging", "start_pos": 222, "end_pos": 236, "type": "TASK", "confidence": 0.697064682841301}, {"text": "stem tagging", "start_pos": 242, "end_pos": 254, "type": "TASK", "confidence": 0.6865356117486954}]}], "tableCaptions": [{"text": " Table 3: Part of a labeled Syriac sentence \u0308  \u202b\u0718\u202c \u0308  \u202b\u0718\u202c \u202b\u072c\u072c\u072c\u072c\u072c\u202c \u202b\u0722\u0722\u0722\u0722\u202c \u202b\u0710\u0722\u0722\u0722\u202c \u202b,\u0718\u072c\u072c\u072c\u072c\u202c \"And you have made them a kingdom and  priests and kings for our God.\" (Revelation 5:10)", "labels": [], "entities": []}, {"text": " Table 4: Word-level accuracies for the individual sub- models used in the syromorph approach.", "labels": [], "entities": []}, {"text": " Table 5: Word-level accuracies for various joint syro- morph models.", "labels": [], "entities": []}]}