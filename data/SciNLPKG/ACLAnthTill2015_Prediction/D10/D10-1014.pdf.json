{"title": [{"text": "Soft Syntactic Constraints for Hierarchical Phrase-based Translation Using Latent Syntactic Distributions", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7043602267901102}]}], "abstractContent": [{"text": "In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.6500983834266663}]}, {"text": "Rather than directly using treebank categories as in previous studies , we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed, word-aligned parallel corpus , based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side.", "labels": [], "entities": []}, {"text": "In our model, each X non-terminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories.", "labels": [], "entities": []}, {"text": "These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations.", "labels": [], "entities": []}, {"text": "Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.6591817736625671}]}], "introductionContent": [{"text": "In recent years, syntax-based translation models) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models () to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering.", "labels": [], "entities": []}, {"text": "Approaches to syntaxbased translation models can be largely categorized into two classes based on their dependency on annotated corpus.", "labels": [], "entities": []}, {"text": "Linguistically syntaxbased models (e.g.,)) utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank) and guide the derivation of SCFG rules with explicit parsing on at least one side of the parallel corpus.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 116, "end_pos": 129, "type": "DATASET", "confidence": 0.9920755326747894}]}, {"text": "Formally syntax-based models (e.g.,) extract synchronous grammars from parallel corpora based on the hierarchical structure of natural language pairs without any explicit linguistic knowledge or annotations.", "labels": [], "entities": []}, {"text": "In this work, we focus on the hierarchical phrase-based models of, which is formally syntax-based, and always refer the term SCFG, from now on, to the grammars of this model class.", "labels": [], "entities": []}, {"text": "On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models.", "labels": [], "entities": []}, {"text": "Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models.", "labels": [], "entities": []}, {"text": "As shown in , hierarchical phrase-based models significantly outperform tree-to-string models (), even when attempts are made to alleviate parsing errors using either forest-based decoding (  or forest-based rule extraction ).", "labels": [], "entities": []}, {"text": "On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.9610700011253357}]}, {"text": "The tree-to-string models of  can actually signif-icantly outperform hierarchical phrase-based models when using forest-based rule extraction together with forest-based decoding.", "labels": [], "entities": []}, {"text": "also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions.", "labels": [], "entities": []}, {"text": "In this paper, we augment rules in hierarchical phrase-based translation systems with novel syntactic features.", "labels": [], "entities": []}, {"text": "Unlike previous studies (e.g.,) that directly use explicit treebank categories such as NP, NP/PP (NP missing PP from the right) to annotate phrase pairs, we induce a set of latent categories to capture the syntactic dependencies inherent in the hierarchical structure of phrase pairs, and derive a real-valued feature vector for each X nonterminal of a SCFG rule based on the distribution of the latent categories.", "labels": [], "entities": []}, {"text": "Moreover, we convert the equality test of two sequences of syntactic categories, which are either identical or different, into the computation of a similarity score between their corresponding feature vectors.", "labels": [], "entities": [{"text": "equality test", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9432966113090515}]}, {"text": "In our model, two symbolically different sequences of syntactic categories could have a high similarity score in the feature vector representation if they are syntactically similar, and a low score otherwise.", "labels": [], "entities": []}, {"text": "In decoding, these feature vectors are utilized to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations.", "labels": [], "entities": []}, {"text": "Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.6591817736625671}]}, {"text": "To the best of our knowledge, this is the first work that applies real-valued syntactic feature vectors to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7941790223121643}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reviews hierarchical phrase-based translation models.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.5966874659061432}]}, {"text": "Section 3 presents an overview of our approach, followed by Section 4 describing the hierarchical structure of aligned phrase pairs and Section 5 describing how to induce latent syntactic categories.", "labels": [], "entities": []}, {"text": "Experimental results are reported in Section 6, followed by discussions in Section 7.", "labels": [], "entities": []}, {"text": "Section 8 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on two tasks, English-toGerman and English-to-Chinese, both aimed for speech-to-speech translation.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6896230131387711}]}, {"text": "The training data for the English-to-German task is a filtered subset of the Europarl corpus (, containing \u223c300k parallel bitext with \u223c4.5M tokens on each side.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.9936833381652832}]}, {"text": "The dev and test sets both contain 1k sentences with one reference for each.", "labels": [], "entities": []}, {"text": "The training data for the Englishto-Chinese task is collected from transcription and human translation of conversations in travel domain.", "labels": [], "entities": []}, {"text": "It consists of \u223c500k parallel bitext with \u223c3M tokens 14 on each side.", "labels": [], "entities": []}, {"text": "Both dev and test sets contain \u223c1.3k sentences, each with two references.", "labels": [], "entities": []}, {"text": "Both corpora are also preprocessed with punctuation removed and words down-cased to make them suitable for speech translation.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7589564323425293}]}, {"text": "The baseline system is our implementation of the hierarchical phrase-based model of, and it includes basic features such as rule and lexicalized rule translation probabilities, language model scores, rule counts, etc.", "labels": [], "entities": [{"text": "rule and lexicalized rule translation probabilities", "start_pos": 124, "end_pos": 175, "type": "TASK", "confidence": 0.6557861616214117}]}, {"text": "We use 4-gram language models in both tasks, and conduct minimumerror-rate training to optimize feature weights on the dev set.", "labels": [], "entities": []}, {"text": "Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively.", "labels": [], "entities": []}, {"text": "The English side of the parallel data is parsed by our implementation of the Berkeley parser ( ) trained on the combination of Broadcast News treebank from Ontonotes () and a speechified version of the WSJ treebank () to achieve higher parsing accuracy.", "labels": [], "entities": [{"text": "Broadcast News treebank", "start_pos": 127, "end_pos": 150, "type": "DATASET", "confidence": 0.9662896394729614}, {"text": "Ontonotes", "start_pos": 156, "end_pos": 165, "type": "DATASET", "confidence": 0.497289776802063}, {"text": "WSJ treebank", "start_pos": 202, "end_pos": 214, "type": "DATASET", "confidence": 0.987766832113266}, {"text": "accuracy", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.8618268370628357}]}, {"text": "Our approach introduces anew syntactic feature and its feature weight is tuned in the same way together with the features in the baseline model.", "labels": [], "entities": []}, {"text": "In this study, we induce 16 latent categories for both X and B nonterminals.", "labels": [], "entities": []}, {"text": "Our approach identifies \u223c180k unique tag sequences for the English side of phrase pairs in both tasks.", "labels": [], "entities": []}, {"text": "As shown by the examples in, the syntactic feature vector representation is able to identify similar and dissimilar tag sequences.", "labels": [], "entities": []}, {"text": "For instance, it determines that the sequence of \"DT JJ NN\" is syntactically very similar to \"DT ADJP NN\" while very dissimilar to \"NN CD VP\".", "labels": [], "entities": [{"text": "DT JJ NN", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7797127564748129}, {"text": "DT ADJP NN", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8312289714813232}]}, {"text": "Notice that our latent categories are learned automatically to maximize the likelihood of the training forests extracted based on alignment and are not explicitly instructed to discriminate between syntactically different tag sequences.", "labels": [], "entities": []}, {"text": "Our approach is not guaranteed to always assign similar feature vectors to syntactically similar tag sequences.", "labels": [], "entities": []}, {"text": "However, as the experimental results show below, the latent categories are able to capture some similarities among tag sequences that are beneficial for translation. and 4 report the experimental results on the English-to-German and English-to-Chinese tasks, respectively.", "labels": [], "entities": [{"text": "translation.", "start_pos": 153, "end_pos": 165, "type": "TASK", "confidence": 0.9739276170730591}]}, {"text": "The addition of the syntax feature achieves a statistically significant improvement (p \u2264 0.01) of 0.6 in BLEU on the test set of the: BLEU scores of the English-to-Chinese task (two references).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9987949132919312}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9982156753540039}]}, {"text": "This improvement is substantial given that only one reference is used for each test sentence.", "labels": [], "entities": []}, {"text": "On the English-to-Chinese task, the syntax feature achieves a smaller improvement of 0.41 BLEU on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9992513060569763}]}, {"text": "One potential explanation for the smaller improvement is that the sentences on the English-to-Chinese task are much shorter, with an average of only 6 words per sentence, compared to 15 words in the English-to-German task.", "labels": [], "entities": []}, {"text": "The hypothesis space of translating a longer sentence is much larger than that of a shorter sentence.", "labels": [], "entities": []}, {"text": "Therefore, there is more potential gain from using syntax features to rule out unlikely derivations of longer sentences, while phrasal rules might be adequate for shorter sentences, leaving less room for syntax to help as in the case of the English-to-Chinese task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The distribution of tag sequences for X 1 in X \u2192  I am reading X 1 , \u00b7 \u00b7 \u00b7 .", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores of the English-to-German task  (one reference).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991186261177063}]}, {"text": " Table 4: BLEU scores of the English-to-Chinese task  (two references).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993434548377991}]}]}