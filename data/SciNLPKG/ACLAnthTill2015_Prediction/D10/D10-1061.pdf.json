{"title": [{"text": "Discriminative Sample Selection for Statistical Machine Translation *", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.8325189153353373}]}], "abstractContent": [{"text": "Production of parallel training corpora for the development of statistical machine translation (SMT) systems for resource-poor languages usually requires extensive manual effort.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.784778947631518}]}, {"text": "Active sample selection aims to reduce the labor , time, and expense incurred in producing such resources, attaining a given performance benchmark with the smallest possible training corpus by choosing informative, non-redundant source sentences from an available candidate pool for manual translation.", "labels": [], "entities": [{"text": "manual translation", "start_pos": 283, "end_pos": 301, "type": "TASK", "confidence": 0.7070876955986023}]}, {"text": "We present a novel, discriminative sample selection strategy that preferentially selects batches of candidate sentences with constructs that lead to erroneous translations on a held-out development set.", "labels": [], "entities": []}, {"text": "The proposed strategy supports a built-in diversity mechanism that reduces redundancy in the selected batches.", "labels": [], "entities": []}, {"text": "Simulation experiments on English-to-Pashto and Spanish-to-English translation tasks demonstrate the superiority of the proposed approach to a number of competing techniques, such as random selection, dissimilarity-based selection , as well as a recently proposed semi-supervised active learning strategy.", "labels": [], "entities": [{"text": "Spanish-to-English translation tasks", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.7824261784553528}]}], "introductionContent": [{"text": "Resource-poor language pairs present a significant challenge to the development of statistical machine translation (SMT) systems due to the latter's dependence on large parallel texts for training.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.8000072240829468}]}, {"text": "Bilingual human experts capable of producing the requisite * Distribution Statement \"A\" (Approved for Public Release, Distribution Unlimited) data resources are often in short supply, and the task of preparing high-quality parallel corpora is laborious and expensive.", "labels": [], "entities": []}, {"text": "In light of these constraints, an attractive strategy is to construct the smallest possible parallel training corpus with which a desired performance benchmark maybe achieved.", "labels": [], "entities": []}, {"text": "Such a corpus maybe constructed by selecting the most informative instances from a large collection of source sentences for translation by a human expert, a technique often referred to as active learning.", "labels": [], "entities": []}, {"text": "A SMT system trained with sentence pairs thus generated is expected to perform significantly better than if the source sentences were chosen using, say, a na\u00a8\u0131vena\u00a8\u0131ve random sampling strategy.", "labels": [], "entities": [{"text": "SMT", "start_pos": 2, "end_pos": 5, "type": "TASK", "confidence": 0.9927773475646973}]}, {"text": "Previously, described a selection strategy that attempts to maximize coverage by choosing sentences with the highest proportion of previously unseen n-grams.", "labels": [], "entities": []}, {"text": "Depending on the composition of the candidate pool with respect to the domain, this strategy may select irrelevant outliers.", "labels": [], "entities": []}, {"text": "They also described a technique based on TF-IDF to de-emphasize sentences similar to those that have already been selected, thereby encouraging diversity.", "labels": [], "entities": []}, {"text": "However, this strategy is bootstrapped by random initial choices that do not necessarily favor sentences that are difficult to translate.", "labels": [], "entities": []}, {"text": "Finally, they worked exclusively with the source language and did not use any SMT-derived features to guide selection.", "labels": [], "entities": [{"text": "SMT-derived", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9486575722694397}]}, {"text": "proposed a number of features, such as similarity to the seed corpus, translation probability, n-gram and phrase coverage, etc., that drive data selection.", "labels": [], "entities": []}, {"text": "They also proposed a model in which these features combine linearly to predict a rank for each candidate sentence.", "labels": [], "entities": []}, {"text": "The top-ranked sentences are chosen for manual translation.", "labels": [], "entities": [{"text": "manual translation", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7411163747310638}]}, {"text": "However, this approach requires that the pool have the same distributional characteristics as the development sets used to train the ranking model.", "labels": [], "entities": []}, {"text": "Additionally, batches are chosen atomically.", "labels": [], "entities": []}, {"text": "Since similar or identical sentences in the pool will typically meet the selection criteria simultaneously, this can have the undesired effect of choosing redundant batches with low diversity.", "labels": [], "entities": []}, {"text": "The semi-supervised active learning strategy proposed by uses multilayer perceptrons (MLPs) to rank candidate sentences based on various features, including domain representativeness, translation difficulty, and batch diversity.", "labels": [], "entities": []}, {"text": "A greedy, incremental batch construction technique encourages diversity.", "labels": [], "entities": []}, {"text": "While this strategy was shown to be superior to random as well as n-gram based dissimilarity selection, its coarse granularity (reducing a candidate sentence to a lowdimensional feature vector for ranking) makes it unsuitable for many situations.", "labels": [], "entities": []}, {"text": "In particular, it is seen to have little or no benefit over random selection when there is no logical separation of the candidate pool into \"in-domain\" and \"out-of-domain\" subsets.", "labels": [], "entities": []}, {"text": "This paper introduces a novel, active sample selection technique that identifies translation errors on a held-out development set, and preferentially selects candidate sentences with constructs that are incorrectly translated in the former.", "labels": [], "entities": []}, {"text": "A discriminative pairwise comparator function, trained on the ranked development set, is used to order candidate sentences and pick sentences that provide maximum potential reduction in translation error.", "labels": [], "entities": []}, {"text": "The feature functions that power the comparator are updated after each selection to encourage batch diversity.", "labels": [], "entities": []}, {"text": "In the following sections, we provide details of the proposed sample selection approach, and describe simulation experiments that demonstrate its superiority over a number of competing strategies.", "labels": [], "entities": [{"text": "sample selection", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.719538077712059}]}], "datasetContent": [{"text": "We conduct a variety of simulation experiments with multiple language pairs (English-Pashto and Spanish-English) and different data configurations in order to demonstrate the utility of discriminative sample selection in the context of resource-poor SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 250, "end_pos": 253, "type": "TASK", "confidence": 0.9778123497962952}]}, {"text": "We also compare the performance of the proposed strategy to numerous competing active and passive selection methods as follows: \u2022 Random: Source sentences are uniformly sampled from the candidate pool P.", "labels": [], "entities": []}, {"text": "\u2022 Similarity: Choose sentences from P with the highest fraction of n-gram overlap with the seed corpus S.", "labels": [], "entities": []}, {"text": "\u2022 Dissimilarity: Select sentences from P with the highest proportion of n-grams not seen in the seed corpus S ().", "labels": [], "entities": []}, {"text": "\u2022 Longest: Pick the longest sentences from the candidate pool P.", "labels": [], "entities": [{"text": "Longest", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9865019917488098}]}, {"text": "\u2022 Semi-supervised: Semi-supervised active learning with greedy incremental selection (Ananthakrishnan et al., 2010).", "labels": [], "entities": []}, {"text": "\u2022 Discriminative: Choose sentences that potentially minimize translation error using a maximum-entropy pairwise comparator (proposed method).", "labels": [], "entities": []}, {"text": "Identical low-resource initial conditions are applied to each selection strategy so that they maybe objectively compared.", "labels": [], "entities": []}, {"text": "Avery small seed corpus S is sampled from the available parallel training data; the remainder serves as the candidate pool.", "labels": [], "entities": [{"text": "Avery small seed corpus", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.7496133297681808}]}, {"text": "Following the literature on active learning for SMT, our simulation experiments are iterative.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9946160912513733}]}, {"text": "A fixed-size batch of source sentences is constructed from the candidate pool using one of the above selection strategies.", "labels": [], "entities": []}, {"text": "We then lookup the corresponding translations from the candidate targets (simulating an expert human translator), augment the seed corpus with the selected data, and update the SMT system with the expanded training corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 177, "end_pos": 180, "type": "TASK", "confidence": 0.9879472851753235}]}, {"text": "The selected data are removed from the candidate pool.", "labels": [], "entities": []}, {"text": "This select-update cycle is then repeated for either a fixed number of iterations or until a specified performance benchmark is attained.", "labels": [], "entities": []}, {"text": "At each iteration, we decode the unseen test set T with the most current SMT configuration and evaluate translation performance in terms of BLEU as well as coverage (defined as the fraction of untranslatable source words in the target hypotheses).", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9783350825309753}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9988251328468323}, {"text": "coverage", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9910567998886108}]}, {"text": "We use a phrase-based SMT framework similar to for all experiments.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8749120831489563}]}], "tableCaptions": [{"text": " Table 2: Source corpus size (in words) and BLEU area after 20 sample selection iterations.", "labels": [], "entities": [{"text": "BLEU area", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9395957589149475}]}]}