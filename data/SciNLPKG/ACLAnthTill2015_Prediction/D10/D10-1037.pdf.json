{"title": [{"text": "Incorporating Content Structure into Text Analysis Applications", "labels": [], "entities": [{"text": "Incorporating Content Structure", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8609681526819865}, {"text": "Text Analysis", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.6999943107366562}]}], "abstractContent": [{"text": "In this paper, we investigate how modeling content structure can benefit text analysis applications such as extractive summarization and sentiment analysis.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7216658294200897}, {"text": "sentiment analysis", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.9499709904193878}]}, {"text": "This follows the linguistic intuition that rich contextual information should be useful in these tasks.", "labels": [], "entities": []}, {"text": "We present a framework which combines a supervised text analysis application with the induction of latent content structure.", "labels": [], "entities": []}, {"text": "Both of these elements are learned jointly using the EM algorithm.", "labels": [], "entities": []}, {"text": "The induced content structure is learned from a large unannotated corpus and biased by the underlying text analysis task.", "labels": [], "entities": []}, {"text": "We demonstrate that exploiting content structure yields significant improvements over approaches that rely only on local context.", "labels": [], "entities": [{"text": "exploiting content structure", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8314170440038046}]}], "introductionContent": [{"text": "In this paper, we demonstrate that leveraging document structure significantly benefits text analysis applications.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.8311252593994141}]}, {"text": "As a motivating example, consider the excerpt from a DVD review shown in.", "labels": [], "entities": []}, {"text": "This review discusses multiple aspects of a product, such as audio and video properties.", "labels": [], "entities": []}, {"text": "While the word \"pleased\" is a strong indicator of positive sentiment, the sentence in which it appears does not specify the aspect to which it relates.", "labels": [], "entities": []}, {"text": "Resolving this ambiguity requires information about global document structure.", "labels": [], "entities": []}, {"text": "A central challenge in utilizing such information lies in finding a relevant representation of content structure fora specific text analysis task.", "labels": [], "entities": [{"text": "text analysis task", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7747178475062052}]}, {"text": "For Extras This single-disc DVD comes packed in a black amaray case with a glossy slipcover.", "labels": [], "entities": []}, {"text": "Cover art has clearly been designed to appeal the Twilight crowd ...", "labels": [], "entities": []}, {"text": "Finally, we've got a deleted scenes reel.", "labels": [], "entities": []}, {"text": "Most of the excised scenes are actually pretty interesting.", "labels": [], "entities": []}, {"text": "instance, when performing single-aspect sentiment analysis, the most relevant aspect of content structure is whether a given sentence is objective or subjective ().", "labels": [], "entities": [{"text": "single-aspect sentiment analysis", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.8010083138942719}]}, {"text": "Ina multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates).", "labels": [], "entities": []}, {"text": "As we can see from even these closely related applications, the content structure representation should be intimately tied to a specific text analysis task.", "labels": [], "entities": [{"text": "content structure representation", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.668252577384313}, {"text": "text analysis task", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7693514625231425}]}, {"text": "In this work, we present an approach in which a content model is learned jointly with a text analysis task.", "labels": [], "entities": []}, {"text": "We assume complete annotations for the task itself, but we learn the content model from raw, unannotated text.", "labels": [], "entities": []}, {"text": "Our approach is implemented in a discriminative framework using latent variables to represent facets of content structure.", "labels": [], "entities": []}, {"text": "In this framework, the original task features (e.g., lexical ones) are conjoined with latent variables to enrich the features with global contextual information.", "labels": [], "entities": []}, {"text": "For example, in, the feature associated with the word \"pleased\" should contribute most strongly to the sentiment of the audio aspect when it is augmented with a relevant topic indicator.", "labels": [], "entities": []}, {"text": "The coupling of the content model and the taskspecific model allows the two components to mutually influence each other during learning.", "labels": [], "entities": []}, {"text": "The content model leverages unannotated data to improve the performance of the task-specific model, while the task-specific model provides feedback to improve the relevance of the content model.", "labels": [], "entities": []}, {"text": "The combined model can be learned effectively using a novel EM-based method for joint training.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two complementary text analysis tasks.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.7183434814214706}]}, {"text": "Our first task is a multi-aspect sentiment analysis task, where a system predicts the aspect-specific sentiment ratings).", "labels": [], "entities": [{"text": "multi-aspect sentiment analysis task", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.7547616511583328}]}, {"text": "Second, we consider a multi-aspect extractive summarization task in which a system extracts key properties fora pre-specified set of aspects.", "labels": [], "entities": [{"text": "multi-aspect extractive summarization task", "start_pos": 22, "end_pos": 64, "type": "TASK", "confidence": 0.7571837157011032}]}, {"text": "On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts.", "labels": [], "entities": []}, {"text": "Moreover, jointly learning content and task parameters yields additional gains over independently learned models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply our approach to two text analysis tasks that stand to benefit from modeling content structure: multi-aspect sentiment analysis and multi-aspect review summarization.", "labels": [], "entities": [{"text": "multi-aspect sentiment analysis", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.6884771287441254}, {"text": "multi-aspect review summarization", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.6287226776281992}]}, {"text": "Baselines For all the models, we obtain a baseline system by eliminating content features and only using a task model with the set of features described above.", "labels": [], "entities": []}, {"text": "We also compare against a simplified variant of our method wherein a content model is induced in isolation rather than learned jointly in the context of the underlying task.", "labels": [], "entities": []}, {"text": "In our experiments, we refer to the two methods as the No Content Model (NoCM) and Independent Content Model (IndepCM) settings, respectively.", "labels": [], "entities": []}, {"text": "The Joint Content M = Movie V = Video A = Audio E = Extras M This collection certainly offers some nostalgic fun, but at the end of the day, the shows themselves, for the most part, just don't holdup.", "labels": [], "entities": []}, {"text": "(5) V Regardless, this is a fairly solid presentation, but it's obvious there was room for improvement.", "labels": [], "entities": []}, {"text": "(c) Sample labeled text from the Yelp multi-aspect summarization corpus: Excerpts from the three corpora with the corresponding labels.", "labels": [], "entities": [{"text": "Yelp multi-aspect summarization corpus", "start_pos": 33, "end_pos": 71, "type": "DATASET", "confidence": 0.8654719293117523}]}, {"text": "Note that sentences from the multi-aspect summarization corpora generally focus on only one or two aspects.", "labels": [], "entities": []}, {"text": "The multi-aspect sentiment corpus has labels per paragraph rather than per sentence.", "labels": [], "entities": [{"text": "multi-aspect sentiment", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7152463793754578}]}, {"text": "Model (JointCM) setting refers to our full model described in Section 3, where content and task components are learned jointly.", "labels": [], "entities": []}, {"text": "Evaluation Metrics For multi-aspect sentiment ranking, we report the average L 2 (squared difference) and L 1 (absolute difference) between system prediction and true 1-10 sentiment rating across test documents and aspects.", "labels": [], "entities": [{"text": "multi-aspect sentiment ranking", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.744128942489624}, {"text": "L 2 (squared difference) and L 1 (absolute difference)", "start_pos": 77, "end_pos": 131, "type": "METRIC", "confidence": 0.8598098433934726}]}, {"text": "For the multi-aspect summarization task, we measure average token precision and recall of the label assignments (Multi-label).", "labels": [], "entities": [{"text": "multi-aspect summarization task", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.679315984249115}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9153911471366882}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9991050362586975}]}, {"text": "For the Amazon corpus, we also report a coarser metric which measures extraction precision and recall while ignoring labels (Binary labels) as well as ROUGE: Results for multi-aspect summarization on the Yelp corpus.", "labels": [], "entities": [{"text": "Amazon corpus", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9333372414112091}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.8977562785148621}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9986028075218201}, {"text": "ROUGE", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.9979742169380188}, {"text": "Yelp corpus", "start_pos": 204, "end_pos": 215, "type": "DATASET", "confidence": 0.9821167290210724}]}, {"text": "Marked precision and recall are statistically significant with p < 0.05: * over the previous model and \u2020 over NoCM.", "labels": [], "entities": [{"text": "Marked", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9068238139152527}, {"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.87294602394104}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996404647827148}, {"text": "NoCM", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.8787074685096741}]}, {"text": "each system to predict the same number of tokens as the original labeled document.", "labels": [], "entities": []}, {"text": "Our metrics of statistical significance vary by task.", "labels": [], "entities": []}, {"text": "For the sentiment task, we use Student's ttest.", "labels": [], "entities": [{"text": "sentiment task", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.9138225018978119}]}, {"text": "For the multi-aspect summarization task, we perform chi-square analysis on the ROUGE scores as well as on precision and recall separately, as is commonly done in information extraction.", "labels": [], "entities": [{"text": "multi-aspect summarization", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.5895406603813171}, {"text": "ROUGE", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9913246631622314}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9991851449012756}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9941121935844421}, {"text": "information extraction", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.7855536341667175}]}], "tableCaptions": [{"text": " Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and  unlabeled documents are used for training the content model, while only the labeled training corpus is used  to train the task model. Note that the entire data set for the multi-aspect sentiment analysis task is labeled.", "labels": [], "entities": [{"text": "multi-aspect sentiment analysis task", "start_pos": 284, "end_pos": 320, "type": "TASK", "confidence": 0.7761076241731644}]}, {"text": " Table 3: The error rate on the multi-aspect sentiment  ranking. We report mean L 1 and L 2 between system  prediction and true values over all aspects. Marked  results are statistically significant with p < 0.05: *  over the previous model and  \u2020 over NoCM.", "labels": [], "entities": [{"text": "error", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9736449122428894}, {"text": "NoCM", "start_pos": 253, "end_pos": 257, "type": "DATASET", "confidence": 0.8944808840751648}]}, {"text": " Table 4: Results for multi-aspect summarization on  the Yelp corpus. Marked precision and recall are  statistically significant with p < 0.05: * over the  previous model and  \u2020 over NoCM.", "labels": [], "entities": [{"text": "Yelp corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9772954285144806}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9146347045898438}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9995619654655457}, {"text": "NoCM", "start_pos": 183, "end_pos": 187, "type": "DATASET", "confidence": 0.8918144106864929}]}, {"text": " Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  \u2020 over NoCM.", "labels": [], "entities": [{"text": "Amazon corpus", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9521260261535645}, {"text": "ROUGE", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9167007803916931}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995313882827759}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9996991157531738}, {"text": "NoCM", "start_pos": 191, "end_pos": 195, "type": "DATASET", "confidence": 0.911135733127594}]}]}