{"title": [{"text": "Automatic Evaluation of Translation Quality for Distant Language Pairs", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic evaluation of Machine Translation (MT) quality is essential to developing high-quality MT systems.", "labels": [], "entities": [{"text": "evaluation of Machine Translation (MT)", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.7501751184463501}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9828322529792786}]}, {"text": "Various evaluation met-rics have been proposed, and BLEU is now used as the de facto standard metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9981366395950317}]}, {"text": "However , when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do notwork well.", "labels": [], "entities": [{"text": "translation between distant language", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.8355196863412857}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9989099502563477}, {"text": "NIST", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.5317849516868591}, {"text": "PER", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9906402826309204}, {"text": "TER", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9971718192100525}]}, {"text": "It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation.", "labels": [], "entities": []}, {"text": "Otherwise, translations with wrong word order often lead to misunderstanding and in-comprehensibility.", "labels": [], "entities": []}, {"text": "For instance, SMT-based Japanese-to-English translators tend to translate 'A because B' as 'B because A.'", "labels": [], "entities": [{"text": "SMT-based Japanese-to-English translators", "start_pos": 14, "end_pos": 55, "type": "TASK", "confidence": 0.9046337207158407}]}, {"text": "Thus, word order is the most important problem for distant language translation.", "labels": [], "entities": [{"text": "word order", "start_pos": 6, "end_pos": 16, "type": "TASK", "confidence": 0.7794456481933594}, {"text": "distant language translation", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.7025958895683289}]}, {"text": "However, conventional evaluation metrics do not significantly penalize such word order mistakes.", "labels": [], "entities": []}, {"text": "Therefore, locally optimizing these metrics leads to inadequate translations.", "labels": [], "entities": []}, {"text": "In this paper , we propose an automatic evaluation metric based on rank correlation coefficients modified with precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9962286949157715}]}, {"text": "Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics.", "labels": [], "entities": [{"text": "NTCIR-7 PATMT JE task data", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.7822856545448303}]}], "introductionContent": [{"text": "Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible.", "labels": [], "entities": [{"text": "Automatic evaluation of machine translation (MT)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7489850074052811}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7441302537918091}]}, {"text": "If we have a perfect automatic evaluation metric, we can tune our translation system for the metric.", "labels": [], "entities": []}, {"text": "BLEU () showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9830625057220459}]}, {"text": "However, argued that the MT community is overly reliant on BLEU by showing examples of poor performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9752568006515503}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9880053400993347}]}, {"text": "For Japanese-to-English (JE) translation, showed that the popular BLEU and NIST do notwork well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task ().", "labels": [], "entities": [{"text": "Japanese-to-English (JE) translation", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.6550462365150451}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9932055473327637}, {"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.9588920474052429}, {"text": "NTCIR-7 PATMT (patent translation) JE task", "start_pos": 131, "end_pos": 173, "type": "DATASET", "confidence": 0.8495422303676605}]}, {"text": "On the other hand, ROUGE-L (, Word Error Rate (WER), and IMPACT worked better.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9969865679740906}, {"text": "Word Error Rate (WER)", "start_pos": 30, "end_pos": 51, "type": "METRIC", "confidence": 0.8731204072634379}, {"text": "IMPACT", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9387000203132629}]}, {"text": "In these studies, Pearson's correlation coefficient and Spearman's rank correlation \u03c1 with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 18, "end_pos": 51, "type": "METRIC", "confidence": 0.8776959478855133}, {"text": "Spearman's rank correlation \u03c1", "start_pos": 56, "end_pos": 85, "type": "METRIC", "confidence": 0.6704963862895965}]}, {"text": "This evaluation of automatic evaluation methods is called meta-evaluation.", "labels": [], "entities": []}, {"text": "In human evaluation, people judge the adequacy and the fluency of each translation.", "labels": [], "entities": []}, {"text": "Denoual and pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9888626337051392}]}, {"text": "Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means he was interested in world history because he read the book into an English sentence such as (H0) he read the book because he was interested in world history in which the cause and the effect are swapped.", "labels": [], "entities": [{"text": "JE translation", "start_pos": 148, "end_pos": 162, "type": "TASK", "confidence": 0.8390389978885651}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 169, "end_pos": 206, "type": "TASK", "confidence": 0.7849892427523931}]}, {"text": "The former half of (J0) means \"He read the book,\" and the latter half means \"(he) was interested in world history.\"", "labels": [], "entities": []}, {"text": "The middle word \"node\" between them corresponds to \"because.\"", "labels": [], "entities": []}, {"text": "Therefore, SMT systems output sentences like (H0).", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9819409847259521}]}, {"text": "On the other hand, Rule-based Machine Translation (RBMT) systems correctly give (R0).", "labels": [], "entities": [{"text": "Rule-based Machine Translation (RBMT)", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.7441472709178925}, {"text": "R0", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9665097594261169}]}, {"text": "In order to find (R0), SMT systems have to search a very large space because we cannot restrict its search space with a small distortion limit.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9927234053611755}]}, {"text": "Most SMT systems thus fail to find (R0).", "labels": [], "entities": [{"text": "SMT", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9875434637069702}]}, {"text": "Consequently, the global word order is essential for translation between distant language pairs, and wrong word order can easily lead to misunderstanding or incomprehensibility.", "labels": [], "entities": [{"text": "translation between distant language pairs", "start_pos": 53, "end_pos": 95, "type": "TASK", "confidence": 0.8706156373023987}]}, {"text": "Perhaps, some readers do not understand why we emphasize word order from this example alone.", "labels": [], "entities": []}, {"text": "A few more examples will clarify what happens when SMT is applied to Japanese-to-English translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9951625466346741}]}, {"text": "Even the most famous SMT service available on the web failed to translate the following very simple sentence at the time of writing this paper.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9779114127159119}]}, {"text": "Japanese: meari wa jon wo koroshita.", "labels": [], "entities": []}, {"text": "Reference: Mary killed John.", "labels": [], "entities": []}, {"text": "SMT output: John killed Mary.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.5332669466733932}]}, {"text": "Since it cannot translate such a simple sentence, it obviously cannot translate more complex sentences correctly.", "labels": [], "entities": []}, {"text": "Japanese: bobu ga katta hon wo jon wa yonda.", "labels": [], "entities": []}, {"text": "Reference: John read a book that Bob bought.", "labels": [], "entities": [{"text": "John read a book", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.8879313319921494}]}, {"text": "SMT output: Bob read the book John bought.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7243715524673462}, {"text": "Bob read the book John bought", "start_pos": 12, "end_pos": 41, "type": "DATASET", "confidence": 0.6617936243613561}]}, {"text": "Another example is: Japanese: bobu wa meari ni yubiwa wo kau tameni, jon no mise ni itta.", "labels": [], "entities": []}, {"text": "Reference: Bob went to John's store to buy a ring for Mary.", "labels": [], "entities": []}, {"text": "SMT output: Bob Mary to buy the ring, John went to the store.", "labels": [], "entities": []}, {"text": "In this way, this SMT service usually gives incomprehensible or misleading translations, and thus people prefer RBMT services.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.974702775478363}]}, {"text": "Other SMT systems also tend to make similar word order mistakes, and special care should be paid to the translation between distant language pairs such as Japanese and English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9873881936073303}]}, {"text": "Even Japanese people cannot solve this word order problem easily: It is well known that Japanese people are not good at speaking English.", "labels": [], "entities": []}, {"text": "From this point of view, conventional automatic evaluation metrics of translation quality disregard word order mistakes too much.", "labels": [], "entities": []}, {"text": "Single-reference BLEU is defined by a geometrical mean of n-gram precisions p n and is modified by Brevity Penalty (BP) min(1, exp(1 \u2212 r/h)), where r is the length of the reference and h is the length of the hypothesis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9893364906311035}, {"text": "Brevity Penalty (BP) min", "start_pos": 99, "end_pos": 123, "type": "METRIC", "confidence": 0.9775035381317139}]}, {"text": "The BLEU score of (H0) with reference (R0) is 1.0\u00d7(11/11\u00d79/10\u00d76/9\u00d74/8) 1/4 = 0.740.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9790175259113312}, {"text": "H0", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9726952910423279}]}, {"text": "Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9932571649551392}]}, {"text": "Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs.", "labels": [], "entities": [{"text": "translation between distant language pairs", "start_pos": 101, "end_pos": 143, "type": "TASK", "confidence": 0.823244047164917}]}, {"text": "Similarly, other popular scores such as NIST, PER, and TER () also give relatively good scores to this translation.", "labels": [], "entities": [{"text": "NIST", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8216198682785034}, {"text": "PER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9959761500358582}, {"text": "TER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9981179237365723}]}, {"text": "NIST also considers only local word orders (n-grams).", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9634763598442078}]}, {"text": "PER (Position-Independent Word Error Rate) was designed to disregard word order completely.", "labels": [], "entities": [{"text": "PER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9803459048271179}]}, {"text": "TER) was designed to allow phrase movements without large penalties.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8080518245697021}, {"text": "phrase movements", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.8345558345317841}]}, {"text": "Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs.", "labels": [], "entities": [{"text": "translation between distant language pairs", "start_pos": 65, "end_pos": 107, "type": "TASK", "confidence": 0.838908064365387}]}, {"text": "In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs.", "labels": [], "entities": []}, {"text": "Our method is based on rank correlation coefficients.", "labels": [], "entities": []}, {"text": "We use them to compare the word ranks in the reference with those in the hypothesis.", "labels": [], "entities": []}, {"text": "There are two popular rank correlation coefficients: Spearman's \u03c1 and Kendall's \u03c4.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.7738080819447836}, {"text": "Kendall's \u03c4", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.7094792524973551}]}, {"text": "In, we used Kendall's \u03c4 to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.6679948717355728}]}, {"text": "It is not clear how well \u03c4 works as an automatic evaluation metric of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9554764032363892}]}, {"text": "Moreover, Spearman's \u03c1 might work better than Kendall's \u03c4 . As we discuss later, \u03c4 considers only the direction of the rank change, whereas \u03c1 considers the distance of the change.", "labels": [], "entities": []}, {"text": "The first objective of this paper is to examine which is the better metric for distant language pairs.", "labels": [], "entities": []}, {"text": "The second objective is to find improvements of these rank correlation-metrics.", "labels": [], "entities": []}, {"text": "Spearman's \u03c1 is based on Pearson's correlation coefficients.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.5690666238466898}, {"text": "Pearson's correlation", "start_pos": 25, "end_pos": 46, "type": "METRIC", "confidence": 0.48116037249565125}]}, {"text": "Suppose we have two lists of numbers Then, Spearman's \u03c1 between x and y is given by Pearson's coefficients between x and y . This \u03c1 can be rewritten as follows when there is no tie: Here, d i indicates the difference in the ranks of the i-th element.", "labels": [], "entities": []}, {"text": "Rank distances are squared in this formula.", "labels": [], "entities": []}, {"text": "Because of this square, we expect that \u03c1 decreases drastically when there is an element that significantly changes in rank.", "labels": [], "entities": []}, {"text": "But we are also afraid that \u03c1 maybe too severe for alternative good translations.", "labels": [], "entities": []}, {"text": "Since Pearson's correlation metric assumes linearity, nonlinear monotonic functions can change its score.", "labels": [], "entities": []}, {"text": "On the other hand, Spearman's \u03c1 and Kendall's \u03c4 uses ranks instead of raw evaluation scores, and simple application of monotonic functions cannot change them (use of other operations such as averaging sentence scores can change them).", "labels": [], "entities": []}], "datasetContent": [{"text": "These rank correlation metrics sometimes have negative values.", "labels": [], "entities": []}, {"text": "In order to make them just like other automatic evaluation metrics, we normalize them as follows.", "labels": [], "entities": []}, {"text": "\u2022 Normalized Kendall's \u03c4 : NKT = (\u03c4 + 1)/2.", "labels": [], "entities": []}, {"text": "\u2022 Normalized Spearman's \u03c1: NSR = (\u03c1 + 1)/2.", "labels": [], "entities": []}, {"text": "Accordingly, NKT is 0.382 and NSR is 0.205.", "labels": [], "entities": [{"text": "NKT", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.5693404674530029}, {"text": "NSR", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8650022745132446}]}, {"text": "These metrics are defined only when the number of aligned words is two or more.", "labels": [], "entities": []}, {"text": "We define both NKT and NSR as zero when the number is one or less.", "labels": [], "entities": [{"text": "NKT", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.8102639317512512}]}, {"text": "Consequently, these normalized metrics have the same range.", "labels": [], "entities": []}, {"text": "In order to avoid confusion, we use these abbreviations (NKT and NSR) when we use rank correlations as word order metrics, because these correlation metrics are also used in the machine translation community for meta-evaluation.", "labels": [], "entities": [{"text": "machine translation community", "start_pos": 178, "end_pos": 207, "type": "TASK", "confidence": 0.7851275404294332}]}, {"text": "For metaevaluation, we use Spearman's \u03c1 and Pearson's correlation coefficient and call them \"Spearman\" and \"Pearson,\" respectively.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 44, "end_pos": 77, "type": "METRIC", "confidence": 0.9116805344820023}, {"text": "Pearson", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.8142142295837402}]}, {"text": "Both NSR-based metrics and NKT-based metrics perform better than conventional metrics for this NT-CIR PATMT JE translation data.", "labels": [], "entities": [{"text": "NT-CIR PATMT JE translation", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.5776464119553566}]}, {"text": "As we expected, \u00d7BP and \u00d7P (1/1) performed badly.", "labels": [], "entities": [{"text": "BP", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9904334545135498}]}, {"text": "Spearman of BP itself is zero.", "labels": [], "entities": [{"text": "Spearman", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9951767921447754}, {"text": "BP", "start_pos": 12, "end_pos": 14, "type": "DATASET", "confidence": 0.7828637957572937}]}, {"text": "NKT performed slightly better than NSR.", "labels": [], "entities": [{"text": "NKT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9659339785575867}, {"text": "NSR", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.7895470857620239}]}, {"text": "Perhaps, NSR penalized alternative good translations too much.", "labels": [], "entities": [{"text": "NSR", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7349632382392883}]}, {"text": "However, one of the NSR-based metrics, NSRP 1/4 , gave the best Spearman score of 0.947, and the difference between NSRP \u03b1 and NKTP \u03b1 was small.", "labels": [], "entities": []}, {"text": "Modification with P led to this improvement.", "labels": [], "entities": [{"text": "P", "start_pos": 18, "end_pos": 19, "type": "METRIC", "confidence": 0.9534971714019775}]}, {"text": "NKT gave the best Pearson score of 0.922.", "labels": [], "entities": [{"text": "NKT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9845795631408691}, {"text": "Pearson", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9977366924285889}]}, {"text": "However, Pearson measures linearity and we can change its score through a nonlinear monotonic function without changing Spearman very much.", "labels": [], "entities": []}, {"text": "For instance, (NSRP 1/4 ) 1.5 also has Spearman of 0.947 but its Pearson is 0.931, which is better than NKT's 0.922.", "labels": [], "entities": [{"text": "NSRP 1/4 ) 1.5", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.9226516981919607}, {"text": "Spearman", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9992684721946716}, {"text": "Pearson", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9964315891265869}, {"text": "NKT", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.9613440036773682}]}, {"text": "Thus, we think Spearman is a better metaevaluation metric than Pearson.", "labels": [], "entities": [{"text": "Spearman", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9217196106910706}]}, {"text": "The right part of shows correlation with fluency, but adequacy is more important, because our motivation is to provide a metric that is useful to reduce incomprehensible or misunderstanding outputs of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 201, "end_pos": 203, "type": "TASK", "confidence": 0.9834156036376953}]}, {"text": "Again, the correlation-based metrics gave better scores than conventional metrics, and BP performed badly.", "labels": [], "entities": [{"text": "BP", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9224272966384888}]}, {"text": "NSR-based metrics proved to be as good as NKT-based metrics.", "labels": [], "entities": []}, {"text": "Meta-evaluation scores of the de facto standard BLEU is much lower than those of other metrics.", "labels": [], "entities": [{"text": "Meta-evaluation", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.8955945372581482}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9575538039207458}]}, {"text": "reported that IMPACT performed very well for sentence-level evaluation of NTCIR-7 PATMT JE data.", "labels": [], "entities": [{"text": "NTCIR-7 PATMT JE data", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.6859373599290848}]}, {"text": "This corpus-level result also shows that IMPACT works better than BLEU, but ROUGE-L, WER, and our methods give better scores than IMPACT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9962324500083923}, {"text": "ROUGE-L", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9547386765480042}, {"text": "WER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9670401215553284}]}, {"text": "shows inter-annotator agreements for the human evaluation methods.", "labels": [], "entities": []}, {"text": "According to their table, the \"sentence ranking\" (or \"rank\") method obtained better agreement than \"adequacy.\"", "labels": [], "entities": []}, {"text": "Therefore, we show Spearman's \u03c1 for \"rank.\"", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.5943125089009603}]}, {"text": "We used the scores given in their, and 11.", "labels": [], "entities": []}, {"text": "(The \"constituent\" methods obtained the best inter-annotator agreement, but these methods focus on local translation quality and have nothing to do with global word order, which we are discussing here.) shows that our metrics designed for distant language pairs are comparable to conventional methods even for similar language pairs, but ROUGE-L and ROUGE-S performed better than ours for French News Corpus and German Europarl.", "labels": [], "entities": [{"text": "French News Corpus", "start_pos": 389, "end_pos": 407, "type": "DATASET", "confidence": 0.9393342932065328}, {"text": "German Europarl", "start_pos": 412, "end_pos": 427, "type": "DATASET", "confidence": 0.7545212507247925}]}, {"text": "BLEU scores in this table agree with those in of within rounding errors.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9892455339431763}]}, {"text": "After some experiments, we noticed that the use of R instead of P often gives better scores for WMT-07, but it degrades NTCIR-7 scores.", "labels": [], "entities": [{"text": "R", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9498271942138672}, {"text": "WMT-07", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.652482807636261}, {"text": "NTCIR-7", "start_pos": 120, "end_pos": 127, "type": "METRIC", "confidence": 0.8575150966644287}]}, {"text": "We can extend our metric by F \u03b2 , weighted harmonic mean of P and R, or any other interpolation, but the introduction of new parameters into our metric makes it difficult to control.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9251441061496735}]}, {"text": "Improvement without new parameters is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs.", "labels": [], "entities": []}, {"text": "Therefore, we also use the WMT-07 data shows the main results of this paper.", "labels": [], "entities": [{"text": "WMT-07 data", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.930636078119278}]}, {"text": "The left part has corpus-level meta-evaluation with adequacy.", "labels": [], "entities": []}, {"text": "Error metrics, WER, PER, and TER, have negative correlation coefficients, but we did not show their minus signs here.", "labels": [], "entities": [{"text": "WER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9896700978279114}, {"text": "PER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9754151105880737}, {"text": "TER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.998399555683136}]}], "tableCaptions": [{"text": " Table 1: NTCIR-7 Meta-evaluation: correlation with hu- man judgments (Spm = Spearman, Prs = Pearson)", "labels": [], "entities": [{"text": "NTCIR-7", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.8092802166938782}]}, {"text": " Table 3: NTCIR-7 meta-evaluation: Effects of square  root (b(x) = 1 \u2212  \u221a  1 \u2212 x)", "labels": [], "entities": [{"text": "NTCIR-7", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.7784884572029114}]}]}