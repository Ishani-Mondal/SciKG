{"title": [], "abstractContent": [{"text": "Automated essay scoring is one of the most important educational applications of natural language processing.", "labels": [], "entities": [{"text": "Automated essay scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7207070589065552}, {"text": "natural language processing", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6554456055164337}]}, {"text": "Recently, researchers have begun exploring methods of scoring essays with respect to particular dimensions of quality such as coherence, technical errors, and relevance to prompt, but there is relatively little work on modeling organization.", "labels": [], "entities": []}, {"text": "We present anew annotated corpus and propose heuristic-based and learning-based approaches to scoring essays along the organization dimension, utilizing techniques that involve sequence alignment, alignment kernels, and string kernels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated essay scoring, the task of employing computer technology to evaluate and score written text, is one of the most important educational applications of natural language processing (NLP) (see and for an overview of the state of the art in this task).", "labels": [], "entities": [{"text": "Automated essay scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6846198836962382}, {"text": "natural language processing (NLP)", "start_pos": 160, "end_pos": 193, "type": "TASK", "confidence": 0.7868438263734182}]}, {"text": "Recent years have seen a surge of interest in this and other educational applications in the NLP community, as evidenced by the panel discussion on \"Emerging Application Areas in Computational Linguistics\" at NAACL 2009, as well as increased participation in the series of workshops on \"Innovative Use of NLP for Building Educational Applications\".", "labels": [], "entities": [{"text": "NAACL 2009", "start_pos": 209, "end_pos": 219, "type": "DATASET", "confidence": 0.7490023374557495}]}, {"text": "Besides its potential commercial value, automated essay scoring brings about a number of relatively less-studied but arguably rather challenging discourse-level problems that involve the computational modeling of different facets of text structure, such as content, coherence, and organization.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.717404767870903}]}, {"text": "A major weakness of many existing essay scoring engines such as IntelliMetric and Intelligent Essay Assessor ( is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer.", "labels": [], "entities": []}, {"text": "In particular, it is not clear which dimension of an essay (e.g., coherence, relevance) a score should be attributed to.", "labels": [], "entities": []}, {"text": "Recent work addresses this problem by scoring a particular dimension of essay quality such as coherence (), technical errors, and relevance to prompt).", "labels": [], "entities": []}, {"text": "Automated systems that provide instructional feedback along multiple dimensions of essay quality such as Criterion ( ) have also begun to emerge.", "labels": [], "entities": []}, {"text": "Nevertheless, there is an essay scoring dimension for which few computational models have been developed -organization.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.7815747261047363}]}, {"text": "Organization refers to the structure of an essay.", "labels": [], "entities": []}, {"text": "A high score on organization means that writers introduce a topic, state their position on that topic, support their position, and conclude, often by restating their position.", "labels": [], "entities": []}, {"text": "A well-organized essay is structured in away that logically develops an argument.", "labels": [], "entities": []}, {"text": "Note that organization is a different facet of text structure than coherence, which is concerned with the transition of ideas at both the global (e.g., paragraph) and local (e.g., sentence) levels.", "labels": [], "entities": []}, {"text": "While organization is an important dimension of essay quality, state-of-the-art essay scoring software such as e-rater V.) employs rather simple heuristicbased methods for computing the score of an essay along this particular dimension.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to develop a computational model for the organization of student es-says.", "labels": [], "entities": []}, {"text": "While many models of text coherence have been developed in recent years (e.g.,,,,), the same is not true for text organization.", "labels": [], "entities": [{"text": "text organization", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7458121180534363}]}, {"text": "One reason is the availability of training and test data for coherence modeling.", "labels": [], "entities": [{"text": "coherence modeling", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8024373650550842}]}, {"text": "Coherence models are typically evaluated on the sentence ordering task, and hence training and test data can be generated simply by scrambling the order of the sentences in a text.", "labels": [], "entities": [{"text": "sentence ordering task", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.766359676917394}]}, {"text": "On the other hand, it is not particularly easy to find poorly organized texts for training and evaluating organization models.", "labels": [], "entities": []}, {"text": "We believe that student essays are an ideal source of well-and poorly-organized texts.", "labels": [], "entities": []}, {"text": "We evaluate our organization model on a data set of 1003 essays annotated with organization scores.", "labels": [], "entities": []}, {"text": "In sum, our contributions in this paper are twofold.", "labels": [], "entities": []}, {"text": "First, we address a less-studied discourse-level task -predicting the organization score of an essay -by developing a computational model of organization, thus establishing a baseline against which future work on this task can be compared.", "labels": [], "entities": []}, {"text": "Second, we annotate a subset of our student essay corpus with organization scores and make this data set publicly available.", "labels": [], "entities": []}, {"text": "Since progress in organization modeling is hindered in part by the lack of a publicly annotated corpus, we believe that our data set will be a valuable resource to the NLP community.", "labels": [], "entities": [{"text": "organization modeling", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7606548964977264}]}], "datasetContent": [{"text": "We designed three evaluation metrics to measure the error of our organization scoring system.", "labels": [], "entities": []}, {"text": "The simplest metric, S1, is perhaps the most intuitive.", "labels": [], "entities": []}, {"text": "It measures the frequency at which a system predicts the wrong score out of the seven possible scores.", "labels": [], "entities": []}, {"text": "Hence, a system that predicts the right score only 25% of the time would receive an S1 score of 0.75.", "labels": [], "entities": [{"text": "S1 score", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9594348967075348}]}, {"text": "The S2 metric is slightly less intuitive than S1, but no less reasonable.", "labels": [], "entities": []}, {"text": "It measures the average distance between the system's score and the actual score.", "labels": [], "entities": []}, {"text": "This metric reflects the idea that a system that estimates scores close to the annotator-assigned scores should be preferred over a system whose estimations are further off, even if both systems estimate the correct score at the same frequency.", "labels": [], "entities": []}, {"text": "Finally, the S3 evaluation metric measures the average square of the distance between a system's organization score estimations and the annotatorassigned scores.", "labels": [], "entities": []}, {"text": "The intuition behind this system is that not only should we prefer a system whose estimations are close to the annotator scores, but we should also prefer one whose estimations are not too frequently very faraway from the annotator scores.", "labels": [], "entities": []}, {"text": "These three scores are given by: where A i and E i are the annotator assigned and system estimated scores respectively for essay i, and N is the number of essays.", "labels": [], "entities": []}, {"text": "Since many of the systems we have described assign test essays real-valued organization scores, to obtain E i for system S1 we round the outputs of each system to the nearest of the seven scores the human annotators were permitted to assign (1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0).", "labels": [], "entities": []}, {"text": "To test our system, we performed 5-fold cross validation on our 1003 essay set, micro-averaging our results into three scores corresponding to the three scoring metrics described above.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Distribution of organization scores.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9660301804542542}]}, {"text": " Table 6. Though simple, this baseline is by no  means easy-to-beat, since 41% of the essays have a  score of 3, and 96% of the essays have a score that  is within one point of 3.", "labels": [], "entities": []}, {"text": " Table 6. It is clear from the results that  the H p systems yielded the best baseline predictions  under all three scoring metrics, performing signif- icantly better than both the Avg and H s systems", "labels": [], "entities": [{"text": "Avg", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.8857001662254333}]}]}