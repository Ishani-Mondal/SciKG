{"title": [{"text": "Exploiting Conversation Structure in Unsupervised Topic Segmentation for Emails", "labels": [], "entities": [{"text": "Topic Segmentation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7031375169754028}, {"text": "Emails", "start_pos": 73, "end_pos": 79, "type": "TASK", "confidence": 0.45076754689216614}]}], "abstractContent": [{"text": "This work concerns automatic topic segmen-tation of email conversations.", "labels": [], "entities": []}, {"text": "We present a corpus of email threads manually annotated with topics, and evaluate annotator reliability.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first such email corpus.", "labels": [], "entities": []}, {"text": "We show how the existing topic segmentation models (i.e., Lexical Chain Seg-menter (LCSeg) and Latent Dirichlet Allocation (LDA)) which are solely based on lexical information, can be applied to emails.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA))", "start_pos": 95, "end_pos": 129, "type": "METRIC", "confidence": 0.8221461574236552}]}, {"text": "By pointing out where these methods fail and what any desired model should consider, we propose two novel extensions of the models that not only use lexical information but also exploit finer level conversation structure in a principled way.", "labels": [], "entities": []}, {"text": "Empirical evaluation shows that LCSeg is a better model than LDA for segmenting an email thread into topical clusters and incorporating conversation structure into these models improves the performance significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the ever increasing popularity of emails and web technologies, it is very common for people to discuss issues, events, agendas or tasks by email.", "labels": [], "entities": []}, {"text": "Effective processing of the email contents can be of great strategic value.", "labels": [], "entities": []}, {"text": "In this paper, we study the problem of topic segmentation for emails, i.e., grouping the sentences of an email thread into a set of coherent topical clusters.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.730229839682579}, {"text": "grouping the sentences of an email thread into a set of coherent topical clusters", "start_pos": 76, "end_pos": 157, "type": "TASK", "confidence": 0.6532397759812218}]}, {"text": "Adapting the standard definition of topic () to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or express their opinions.", "labels": [], "entities": []}, {"text": "For example, in the email thread shown in, according to the majority of our annotators, participants discuss three topics (e.g., 'telecon cancellation', 'TAG document', and 'responding to I18N').", "labels": [], "entities": [{"text": "telecon cancellation", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.6474826782941818}]}, {"text": "Multiple topics seem to occur naturally in social interactions, whether synchronous (e.g., chats, meetings) or asynchronous (e.g., emails, blogs) conversations.", "labels": [], "entities": []}, {"text": "In multi-party chat) report an average of 2.75 discussions active at a time.", "labels": [], "entities": []}, {"text": "In our email corpus, we found an average of 2.5 topics per thread.", "labels": [], "entities": []}, {"text": "Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and), information extraction and ordering), information retrieval (), and intelligent user interfaces (.", "labels": [], "entities": [{"text": "Topic segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7923161685466766}, {"text": "summarization", "start_pos": 168, "end_pos": 181, "type": "TASK", "confidence": 0.9493734836578369}, {"text": "information extraction and ordering", "start_pos": 199, "end_pos": 234, "type": "TASK", "confidence": 0.8389949947595596}, {"text": "information retrieval", "start_pos": 237, "end_pos": 258, "type": "TASK", "confidence": 0.8241830766201019}]}, {"text": "While extensive research has been conducted in topic segmentation for monologues (e.g.,), () and synchronous dialogs (e.g., (,)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email).", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7562552392482758}]}, {"text": "Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available.", "labels": [], "entities": []}, {"text": "Also, it is our key hypothesis that, because of its asynchronous nature, and the use of quotation, topics in an email thread often do not change in a sequential way.", "labels": [], "entities": []}, {"text": "As a result, we do not expect models which have proved successful in monologue or dialog to be as effective when they are applied to email conversations.", "labels": [], "entities": []}, {"text": "Our contributions in this paper aim to remedy these problems.", "labels": [], "entities": []}, {"text": "First, we present an email corpus annotated with topics and evaluate annotator agreement.", "labels": [], "entities": []}, {"text": "Second, we adopt a set of metrics to measure the local and global structural similarity between two annotations from the work on multi-party chat disentanglement.", "labels": [], "entities": [{"text": "multi-party chat disentanglement", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.6942994594573975}]}, {"text": "Third, we show how the two state-of-the-art topic segmentation methods (i.e., LCSeg and LDA) which are solely based on lexical information and make strong assumptions on the resulting topic models, can be effectively applied to emails, by having them to consider, in a principled way, a finer level structure of the underlying conversations.", "labels": [], "entities": []}, {"text": "Experimental results show that both LCSeg and LDA benefit when they are extended to consider the conversational structure.", "labels": [], "entities": [{"text": "LDA", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.7602522969245911}]}, {"text": "When comparing the two methods, we found that LCSeg is better than LDA and this advantage is preserved when they are extended to incorporate conversational structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are no publicly available email corpora annotated with topics.", "labels": [], "entities": []}, {"text": "Therefore, the first step was to develop our own corpus.", "labels": [], "entities": []}, {"text": "We have annotated the BC3 email corpus () with topics . The BC3 corpus, previously annotated with sentence level speech acts, meta sentence, subjectivity, extractive and abstractive summaries, is one of a growing number of corpora being used for email research.", "labels": [], "entities": [{"text": "BC3 email corpus", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.8404761155446371}, {"text": "BC3 corpus", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.8246796429157257}]}, {"text": "The corpus contains 40 email threads from the W3C corpus 2 . It has 3222 sentences and an average of 5 emails per thread.", "labels": [], "entities": [{"text": "W3C corpus 2", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9331822594006857}]}, {"text": "In this section we describe the metrics used to compare different human annotations and system's output.", "labels": [], "entities": []}, {"text": "As different annotations (or system's output) can group sentences in different number of clusters, metrics widely used in classification, such as the \u03ba statistic, are not applicable.", "labels": [], "entities": []}, {"text": "Again, our problem of topic segmentation for emails is not sequential in nature.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7393657863140106}]}, {"text": "Therefore, the standard metrics widely used in sequential topic segmentation for monologues and dialogs, such as P k and W indowDif f (W D), are also not applicable.", "labels": [], "entities": [{"text": "sequential topic segmentation", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.7066536049048106}]}, {"text": "We adopt the more appropriate metrics 1-to-1, lock and m-to-1, introduced recently by.", "labels": [], "entities": [{"text": "lock", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9537392854690552}]}, {"text": "The 1-to-1 metric measures the global similarity between two annotations.", "labels": [], "entities": []}, {"text": "It pairs up the clusters from the two annotations in away that maximizes (globally) the total overlap and then reports the percentage of overlap.", "labels": [], "entities": []}, {"text": "lock measures the local agreement within a con-TRO' and 'END'.", "labels": [], "entities": [{"text": "END", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9943304061889648}]}, {"text": "In all our computation (i.e., statistics, agreement, system's input) we excluded the sentences marked as either 'INTRO' or 'END' text of k sentences.", "labels": [], "entities": [{"text": "INTRO", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9868211150169373}, {"text": "END", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.7201752662658691}]}, {"text": "To compute the loc 3 metric for the m-th sentence in the two annotations, we consider the previous 3 sentences: m-1, m-2 and m-3, and mark them as either 'same' or 'different' depending on their topic assignment.", "labels": [], "entities": []}, {"text": "The loc 3 score between two annotations is the mean agreement on these 'same' or 'different' judgments, averaged overall sentences.", "labels": [], "entities": [{"text": "agreement", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9438078999519348}]}, {"text": "We report the agreement found in 1-to-1 and lock in.", "labels": [], "entities": []}, {"text": "In both of the metrics we get high agreement, though the local agreement (average of 83%) is little higher than the global agreement (average of 80%).", "labels": [], "entities": [{"text": "agreement", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9956759810447693}]}, {"text": "If we consider the topic of a randomly picked sentence as a random variable then its entropy measures the level of detail in an annotation.", "labels": [], "entities": [{"text": "entropy", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9757173657417297}]}, {"text": "If the topics are evenly distributed then the uncertainty (i.e., entropy) is higher.", "labels": [], "entities": [{"text": "uncertainty", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9730747938156128}, {"text": "entropy)", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9789382517337799}]}, {"text": "It also increases with the increase of the number of topics.", "labels": [], "entities": []}, {"text": "Therefore, it is a measure of how specific an annotator is and in our dataset it varies from 0 6 to 2.7.", "labels": [], "entities": []}, {"text": "To measure how much the annotators agree on the general structure we use the m-to-1 metric.", "labels": [], "entities": []}, {"text": "It maps each of the source clusters to the single target cluster with which it gets the highest overlap, then computes the total percentage of overlap.", "labels": [], "entities": []}, {"text": "This metric is asymmetrical and not a measure to be optimized 7 , but it gives us some intuition about specificity.", "labels": [], "entities": []}, {"text": "If one annotator divides a cluster into two clusters then, the m-to-1 metric from fine to coarse is 1.", "labels": [], "entities": []}, {"text": "In our corpus by mapping from fine to coarse we get an m-to-1 average of 0.949.", "labels": [], "entities": []}, {"text": "We ran our four systems LDA, LDA+FQG, LCSeg, and LCSeg+FQG on the dataset 11 . The statistics of these four annotations and two best performing baselines (i.e., 'Speaker' and 'Block 5' as described below) are shown in.", "labels": [], "entities": []}, {"text": "For brevity we just mention the average measures.", "labels": [], "entities": []}, {"text": "Comparing with Table 1, we see that these fall within the bounds of the human annotations.", "labels": [], "entities": []}, {"text": "We compare our results in, where we also provide the results of some simple baseline systems.", "labels": [], "entities": []}, {"text": "We evaluated the following baselines and report the best two in.", "labels": [], "entities": []}, {"text": "All different: Each sentence is a separate topic.", "labels": [], "entities": []}, {"text": "All same: The whole thread is a single topic.", "labels": [], "entities": []}, {"text": "Speaker: The sentences from each participant constitute a separate topic.", "labels": [], "entities": []}, {"text": "Blocks of k(= 5, 10, 15): Each consecutive group of k sentences is a topic.", "labels": [], "entities": []}, {"text": "Most of these baselines perform rather poorly.", "labels": [], "entities": []}, {"text": "All different is the worst baseline with mean 1-to-1 score of 0.10 (max: 0.33, min: 0.03) and mean loc 3 score of 0.245 (max: 0.67, min: 0).", "labels": [], "entities": [{"text": "mean loc 3 score", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.8626370579004288}]}, {"text": "Block 10 has mean 1-to-1 score of 0.35 (max: 0.71, min: 0.13) and mean loc 3 score of 0.584 (max: 0.76, min: 0.31).", "labels": [], "entities": [{"text": "mean 1-to-1 score", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.8878212173779806}, {"text": "mean loc 3 score", "start_pos": 66, "end_pos": 82, "type": "METRIC", "confidence": 0.8841699659824371}]}, {"text": "Block 15 has mean 1-to-1 score of 0.32 (max: 0.77, min: 0.16) and mean loc 3 score of 0.56 (max: 0.82, min: 0.38).", "labels": [], "entities": [{"text": "mean 1-to-1 score", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.8773537874221802}, {"text": "mean loc 3 score", "start_pos": 66, "end_pos": 82, "type": "METRIC", "confidence": 0.8734697997570038}]}, {"text": "All same is optimal for threads containing only one topic, but its performance rapidly degrades as the number of topics in a thread increases.", "labels": [], "entities": []}, {"text": "It has mean 1-to-1 score of 0.28 (max: 1 12 , min: 0.11) and mean loc 3 score of 0.54 For a fair comparison of the systems we set the same topic number per thread for all of them.", "labels": [], "entities": [{"text": "mean loc 3 score", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.8353362828493118}]}, {"text": "If at least two of the annotators agree on the topic number we set that number, otherwise we set the floor value of the average topic number.", "labels": [], "entities": []}, {"text": "\u03bb is set to 20 in LDA+FQG.", "labels": [], "entities": [{"text": "LDA+FQG", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.5723546544710795}]}, {"text": "The maximum value of 1 is due to the fact that for some threads some annotators found only one topic (max: 1, min: 0.34).", "labels": [], "entities": []}, {"text": "As shown in, Speaker and Blocks of 5 are two strong baselines especially for the loc 3 . In general, our systems perform better than the baselines, but worse than the gold standard.", "labels": [], "entities": []}, {"text": "Of all the systems, the basic LDA model performs very disappointingly.", "labels": [], "entities": []}, {"text": "In the local agreement it even fails to beat the baselines.", "labels": [], "entities": []}, {"text": "A likely explanation is that the independence assumption made by LDA when computing the distribution over topics fora sentence from the distribution over topics for the words causes sentences in a local context to be excessively distributed over topics.", "labels": [], "entities": []}, {"text": "Another possible explanation for LDA's disappointing performance is the limited amount of data available for training.", "labels": [], "entities": [{"text": "LDA", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8738673329353333}]}, {"text": "In our corpus, the average number of sentences per thread is 26.3 (see) which might not be sufficient for the LDA models.", "labels": [], "entities": []}, {"text": "If we compare the performance of the regularized LDA (in the table LDA+FQG) with the basic LDA we get a significant (p=0.0002 (1-to-1), p=9.8e-07 (loc 3 )) improvement in both of the measures . This supports our claim that sentences connected by referential relations in the FQG usually refer to the same topic.", "labels": [], "entities": [{"text": "FQG", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8046897053718567}, {"text": "FQG", "start_pos": 275, "end_pos": 278, "type": "DATASET", "confidence": 0.9613260626792908}]}, {"text": "The regularization also prevents the local context from being overly distributed over topics.", "labels": [], "entities": []}, {"text": "A comparison of the basic LCSeg with the basic LDA reveals that LCSeg is a better model for email topic segmentation (p=0.00017 (1-to-1), p<2.2e-16 (loc 3 )).", "labels": [], "entities": [{"text": "email topic segmentation", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.6645534733931223}]}, {"text": "One possible reason is that LCSeg extracts the topics keeping the local context intact.", "labels": [], "entities": []}, {"text": "Another reason could be the term weighting scheme employed by LCSeg.", "labels": [], "entities": [{"text": "LCSeg", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9394152164459229}]}, {"text": "Unlike LDA, which considers only 'repetition', LCSeg also considers how tightly the 'repetition' happens.", "labels": [], "entities": []}, {"text": "When we incorporate the conversation structure (i.e., FQG) into LCSeg (in the table LCSeg+FQG), we get a significant improvement in the 1-to-1 measure over the basic LCSeg (p=0.0014).", "labels": [], "entities": [{"text": "FQG", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.8363451361656189}, {"text": "FQG", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.606084406375885}]}, {"text": "Though the local context (i.e., loc 3 ) suf-  fers a bit, the decrease in performance is minimal and it is not significant.", "labels": [], "entities": []}, {"text": "The fact that LCSeg is a better model than LDA is also preserved when we incorporate FQG into them (p=2.140e-05 (1-to-1), p=1.3e-09 (loc 3 )).", "labels": [], "entities": [{"text": "FQG", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.6477282643318176}]}, {"text": "Overall, LCSeg+FQG is the best model for this data.", "labels": [], "entities": [{"text": "FQG", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.6209204792976379}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics of human annotations", "labels": [], "entities": []}, {"text": " Table 2: Annotator agreement in the scale of 0 to 1", "labels": [], "entities": [{"text": "Annotator agreement", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.6519463062286377}]}, {"text": " Table 3: Corpus statistics of different system's annotation", "labels": [], "entities": []}, {"text": " Table 4: Comparison of Human, System and best Baseline annotations", "labels": [], "entities": []}]}