{"title": [{"text": "Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Exophoric References in Situated Dialogue", "labels": [], "entities": [{"text": "Fusing Eye Gaze", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.763135552406311}, {"text": "Speech Recognition", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.6993816941976547}, {"text": "Resolve Exophoric References in Situated Dialogue", "start_pos": 54, "end_pos": 103, "type": "TASK", "confidence": 0.6500475605328878}]}], "abstractContent": [{"text": "In situated dialogue humans often utter linguistic expressions that refer to extralinguistic entities in the environment.", "labels": [], "entities": []}, {"text": "Correctly resolving these references is critical yet challenging for artificial agents partly due to their limited speech recognition and language understanding capabilities.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7352940142154694}]}, {"text": "Motivated by psycholin-guistic studies demonstrating a tight link between language production and human eye gaze, we have developed approaches that integrate naturally occurring human eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue in a virtual world.", "labels": [], "entities": []}, {"text": "In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks.", "labels": [], "entities": [{"text": "word confusion networks", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.753894587357839}]}, {"text": "Our empirical results demonstrate that incorporating eye gaze with recognition hypotheses consistently out-performs the results obtained from processing recognition hypotheses alone.", "labels": [], "entities": []}, {"text": "Incorporating eye gaze with word confusion networks further improves performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a rapid growth in virtual world applications for tutoring and training, video games and simulations, and assistive technology, enabling situated dialogue in virtual worlds has become increasingly important.", "labels": [], "entities": []}, {"text": "Situated dialogue allows human users to navigate in a spatially rich environment and carry a conversation with artificial agents to achieve specific tasks pertinent to the environment.", "labels": [], "entities": []}, {"text": "Different from traditional telephony-based spoken dialogue systems and multimodal conversational interfaces, situated dialogue supports immersion and mobility in a visually rich environment and encourages social and collaborative language use ().", "labels": [], "entities": []}, {"text": "In situated dialogue, human users often need to make linguistic references, known as exophoric referring expressions (e.g., the book to the right), to extralinguistic entities in the environment.", "labels": [], "entities": []}, {"text": "Reliably resolving these references is critical for dialogue success.", "labels": [], "entities": []}, {"text": "However, reference resolution remains a challenging problem, partly due to limited speech and language processing capabilities caused by poor speech recognition (ASR), ambiguous language, and insufficient pragmatic knowledge.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.9851735234260559}, {"text": "speech recognition (ASR)", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.8244892597198487}]}, {"text": "To address this problem, motivated by psycholinguistic studies demonstrating a close relationship between language production and eye gaze, our previous work has incorporated naturally occurring eye gaze in reference resolution.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 207, "end_pos": 227, "type": "TASK", "confidence": 0.804913729429245}]}, {"text": "Our findings have shown that eye gaze can partially compensate for limited language processing and domain modeling.", "labels": [], "entities": [{"text": "domain modeling", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.6994617134332657}]}, {"text": "However, this work was conducted in a setting where users only spoke to a static visual interface.", "labels": [], "entities": []}, {"text": "In situated dialogue, human speech and eye gaze patterns are much more complex.", "labels": [], "entities": []}, {"text": "The dynamic nature of the environment and the complexity of spatially rich tasks have a massive influence on what the user will look at and say.", "labels": [], "entities": []}, {"text": "It is not clear to what degree prior findings can generalize to situated dialogue.", "labels": [], "entities": []}, {"text": "Therefore, this paper explores new studies on incorporating eye gaze for exophoric reference resolution in a fully situated virtual envi-ronment -a more realistic approximation of real world interaction.", "labels": [], "entities": [{"text": "exophoric reference resolution", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.6632292171319326}]}, {"text": "In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks.", "labels": [], "entities": [{"text": "word confusion networks", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.753894587357839}]}, {"text": "Our empirical results have demonstrated the utility of eye gaze for reference resolution in situated dialogue.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.8055846393108368}]}, {"text": "Although eye gaze is much more noisy given the mobility of the user, our results have shown that incorporating eye gaze with recognition hypotheses consistently outperform the results obtained from processing recognition hypotheses alone.", "labels": [], "entities": []}, {"text": "In addition, incorporating eye gaze with word confusion networks further improves performance.", "labels": [], "entities": [{"text": "word confusion networks", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7639260093371073}]}, {"text": "Our analysis also indicates that, although a word confusion network appears to be more complicated, the time complexity of its integration with eye gaze is well within the acceptable range for real-time applications.", "labels": [], "entities": [{"text": "word confusion network", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8030654589335123}]}], "datasetContent": [{"text": "Using our data, described in Section 3, we applied the multimodal reference resolution algorithm described in Section 5.", "labels": [], "entities": []}, {"text": "All of the data is used to report the experimental results.", "labels": [], "entities": []}, {"text": "Reference resolution model parameters are set based on our prior work in a different domain.", "labels": [], "entities": [{"text": "Reference resolution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7961254417896271}]}, {"text": "For each utterance we compare the reference resolution performance with and without the integration of eye gaze information.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.6170465350151062}]}, {"text": "We also evaluate using a word confusion network compared to a 1-best list to model speech recognition hypotheses.", "labels": [], "entities": [{"text": "speech recognition hypotheses", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.8128000299135844}]}, {"text": "For perspective, reference resolution with recognized speech input is compared with transcribed speech.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8275392055511475}]}, {"text": "The reference resolution algorithm outputs a list of (referring expression, referent object set) pairs for each utterance.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7634789347648621}]}, {"text": "We evaluate the algorithm by comparing the generated pairs to the annotated \"gold standard\" pairs using F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.951644241809845}]}, {"text": "We perform the following two types of evaluation: \u2022 Lenient Evaluation: Due to speech recognition errors, there are many cases in which the algorithm may not return a referring expression that exactly matches the gold standard referring expression.", "labels": [], "entities": [{"text": "Lenient Evaluation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8850495219230652}]}, {"text": "It may only match based on the object type.", "labels": [], "entities": []}, {"text": "For example, the expressions one long sword and sword are different, but they match in terms of the intended object type.", "labels": [], "entities": []}, {"text": "For applications in which it is critical to identify the objects referred to by the user, precisely identifying uttered referring expressions maybe unnecessary.", "labels": [], "entities": []}, {"text": "Thus, we evaluate the reference resolution algorithm with a lenient comparison of (referring expression, referent object set) pairs.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7981404066085815}]}, {"text": "In this case, two pairs are considered a match if at least the object types specified via the referring expressions match each other and the referent object sets are identical.", "labels": [], "entities": []}, {"text": "\u2022 Strict Evaluation: For some applications it maybe important to identify exact referring expressions in addition to the objects they refer to.", "labels": [], "entities": [{"text": "Strict Evaluation", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.8008870184421539}]}, {"text": "This is important for applications that attempt to learn a relationship between referring expressions and referenced objects.", "labels": [], "entities": []}, {"text": "For example, in automated vocabulary acquisition, words other than object types must be identified so the system can learn to associate these words with referenced objects.", "labels": [], "entities": [{"text": "automated vocabulary acquisition", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.6245127816994985}]}, {"text": "Similarly, in systems that apply priming for language generation, identification of the exact referring expressions from human users could be important.", "labels": [], "entities": [{"text": "language generation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7334868162870407}, {"text": "identification of the exact referring expressions from human users", "start_pos": 66, "end_pos": 132, "type": "TASK", "confidence": 0.6726959149042765}]}, {"text": "Thus, we also evaluate the reference resolution algorithm with a strict comparison of (referring expression, referent object set) pairs.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7883970141410828}]}, {"text": "In this case, a referring expression from the system output needs to exactly match the corresponding expression from the gold standard.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Lenient F-measure Evaluation", "labels": [], "entities": [{"text": "Lenient F-measure Evaluation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7488365868727366}]}, {"text": " Table 5: Strict F-measure Evaluation", "labels": [], "entities": [{"text": "Strict F-measure Evaluation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8206974466641744}]}]}