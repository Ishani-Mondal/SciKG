{"title": [{"text": "Uptraining for Accurate Deterministic Question Parsing", "labels": [], "entities": [{"text": "Accurate Deterministic Question Parsing", "start_pos": 15, "end_pos": 54, "type": "TASK", "confidence": 0.7110646069049835}]}], "abstractContent": [{"text": "It is well known that parsing accuracies drop significantly on out-of-domain data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9776721596717834}, {"text": "accuracies", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.7582511305809021}]}, {"text": "What is less known is that some parsers suffer more from domain shifts than others.", "labels": [], "entities": []}, {"text": "We show that dependency parsers have more difficulty parsing questions than constituency parsers.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8027995228767395}, {"text": "parsing questions", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.9030340313911438}]}, {"text": "In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set.", "labels": [], "entities": [{"text": "deterministic shift-reduce dependency parsers", "start_pos": 15, "end_pos": 60, "type": "TASK", "confidence": 0.6037787944078445}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.8305686712265015}]}, {"text": "We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate , but slower, latent variable constituency parser (converted to dependencies).", "labels": [], "entities": []}, {"text": "Uptrain-ing with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training.", "labels": [], "entities": []}, {"text": "With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9725171327590942}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9585862159729004}]}], "introductionContent": [{"text": "Parsing accuracies on the popular Section 23 of the Wall Street Journal (WSJ) portion of the Penn Treebank have been steadily improving over the past decade.", "labels": [], "entities": [{"text": "Parsing accuracies", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.7698700428009033}, {"text": "Section 23 of the Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 34, "end_pos": 106, "type": "DATASET", "confidence": 0.9164154012997945}]}, {"text": "At this point, we have many different parsing models that reach and even surpass 90% dependency or constituency accuracy on this test set (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.7874364852905273}]}, {"text": "Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see and references therein for more details).", "labels": [], "entities": []}, {"text": "Their linear running time makes them ideal candidates for large scale text processing, and our model of choice for this paper.", "labels": [], "entities": [{"text": "text processing", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7336043417453766}]}, {"text": "Unfortunately, the parsing accuracies of all models have been reported to drop significantly on outof-domain test sets, due to shifts in vocabulary and grammar usage.", "labels": [], "entities": []}, {"text": "In this paper, we focus our attention on the task of parsing questions.", "labels": [], "entities": [{"text": "parsing questions", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.9295800626277924}]}, {"text": "Questions pose interesting challenges for WSJ-trained parsers because they are heavily underrepresented in the training data (there are only 334 questions among the 39,832 training sentences).", "labels": [], "entities": [{"text": "WSJ-trained parsers", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.5761747360229492}]}, {"text": "At the same time, questions are of particular interest for user facing applications like question answering or web search, which necessitate parsers that can process questions in a fast and accurate manner.", "labels": [], "entities": [{"text": "question answering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8248178958892822}]}, {"text": "We start our investigation in Section 3 by training several state-of-the-art (dependency and constituency) parsers on the standard WSJ training set.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.9213389754295349}]}, {"text": "When evaluated on a question corpus, we observe dramatic accuracy drops exceeding 20% for the deterministic shift-reduce parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9988399147987366}]}, {"text": "In general, dependency parsers , seem to suffer more from this domain change than constituency parsers).", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8469951748847961}]}, {"text": "Overall, the latent variable approach of appears to generalize best to this new domain, losing only about 5%.", "labels": [], "entities": []}, {"text": "Unfortunately, the parsers that generalize better to this new domain have time complexities that are cubic in the sentence length (or even higher), rendering them impractical for web-scale text processing.", "labels": [], "entities": []}, {"text": "We therefore propose an uptraining method, in which a deterministic shift-reduce parser is trained on the output of a more accurate, but slower parser (Section 4).", "labels": [], "entities": []}, {"text": "This type of domain adaptation is reminiscent of self-training () and co-training), except that the goal here is not to further improve the performance of the very best model.", "labels": [], "entities": []}, {"text": "Instead, our aim is to train a computationally cheaper model (a linear time dependency parser) to match the performance of the best model (a cubic time constituency parser), resulting in a computationally efficient, yet highly accurate model.", "labels": [], "entities": []}, {"text": "In practice, we parse a large amount of unlabeled data from the target domain with the constituency parser of and then train a deterministic dependency parser on this noisy, automatically parsed data.", "labels": [], "entities": []}, {"text": "The accuracy of the linear time parser on a question test set goes up from 60.06% (LAS) to 76.94% after uptraining, which is comparable to adding 2,000 labeled questions to the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999619722366333}, {"text": "LAS)", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9818673729896545}]}, {"text": "Combining uptraining with 2,000 labeled questions further improves the accuracy to 84.14%, fully recovering the drop between in-domain and out-of-domain accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9994630217552185}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.978624165058136}]}, {"text": "We also present a detailed error analysis in Section 5, showing that the errors of the WSJ-trained model are primarily caused by sharp changes in syntactic configurations and only secondarily due to lexical shifts.", "labels": [], "entities": []}, {"text": "Uptraining leads to large improvements across all error metrics and especially on important dependencies like subjects (nsubj).", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the following experimental protocol throughout the paper.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.", "labels": [], "entities": []}, {"text": " Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.", "labels": [], "entities": []}, {"text": " Table 3: Uptraining substantially improves parsing accuracies, while self-training gives only minor improvements.", "labels": [], "entities": [{"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9787102937698364}, {"text": "accuracies", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.6793241500854492}]}, {"text": " Table 4: Parsing accuracies of uptrained parsers with and  without part-of-speech tags and word cluster features.", "labels": [], "entities": []}, {"text": " Table 5: F 1 scores for the most frequent labels in the  QuestionBank development set. Uptraining leads to huge  improvements compared to training only on the WSJ.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9898676872253418}, {"text": "QuestionBank development set", "start_pos": 58, "end_pos": 86, "type": "DATASET", "confidence": 0.8903173406918844}, {"text": "WSJ", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.9766560196876526}]}]}