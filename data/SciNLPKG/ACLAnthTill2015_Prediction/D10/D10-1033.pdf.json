{"title": [{"text": "Improving Mention Detection Robustness to Noisy Input", "labels": [], "entities": [{"text": "Improving Mention Detection Robustness", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8773912489414215}]}], "abstractContent": [{"text": "Information-extraction (IE) research typically focuses on clean-text inputs.", "labels": [], "entities": [{"text": "Information-extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6728747859597206}]}, {"text": "However, an IE engine serving real applications yields many false alarms due to less-well-formed input.", "labels": [], "entities": []}, {"text": "For example, IE in a multilingual broadcast processing system has to deal with inaccurate automatic transcription and translation.", "labels": [], "entities": [{"text": "IE", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9929085969924927}]}, {"text": "The resulting presence of non-target-language text in this case, and non-language material interspersed in data from other applications , raise the research problem of making IE robust to such noisy input text.", "labels": [], "entities": [{"text": "IE", "start_pos": 175, "end_pos": 177, "type": "TASK", "confidence": 0.9935688972473145}]}, {"text": "We address one such IE task: entity-mention detection.", "labels": [], "entities": [{"text": "IE", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9643122553825378}, {"text": "entity-mention detection", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7496617138385773}]}, {"text": "We describe augmenting a statistical mention-detection system in order to reduce false alarms from spurious passages.", "labels": [], "entities": []}, {"text": "The diverse nature of input noise leads us to pursue a multi-faceted approach to robustness.", "labels": [], "entities": []}, {"text": "For our English-language system, at various miss rates we eliminate 97% of false alarms on inputs from other Latin-alphabet languages.", "labels": [], "entities": [{"text": "miss", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9708332419395447}]}, {"text": "In another experiment, representing scenarios in which genre-specific training is infeasible, we process real financial-transactions text containing mixed languages and data-set codes.", "labels": [], "entities": []}, {"text": "On these data, because we do not train on data like it, we achieve a smaller but significant improvement.", "labels": [], "entities": []}, {"text": "These gains come with virtually no loss inaccuracy on clean English text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information-extraction (IE) research is typically performed on clean text in a predetermined language.", "labels": [], "entities": [{"text": "Information-extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6689870059490204}]}, {"text": "Lately, IE has improved to the point of being usable for some real-world tasks whose accuracy requirements are reachable with current technology.", "labels": [], "entities": [{"text": "IE", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9800711274147034}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9962193369865417}]}, {"text": "These uses include media monitoring, topic alerts, summarization, population of databases for advanced search, etc.", "labels": [], "entities": [{"text": "topic alerts", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.8191120028495789}, {"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9828553795814514}]}, {"text": "These uses often combine IE with technologies such as speech recognition, machine translation, topic clustering, and information retrieval.", "labels": [], "entities": [{"text": "IE", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9672440886497498}, {"text": "speech recognition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7677453756332397}, {"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7517278790473938}, {"text": "topic clustering", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7730362415313721}, {"text": "information retrieval", "start_pos": 117, "end_pos": 138, "type": "TASK", "confidence": 0.8043128848075867}]}, {"text": "The propagation of IE technology from isolated use to aggregates with such other technologies, from NLP experts to other types of computer scientists, and from researchers to users, feeds back to the IE research community the need for additional investigation which we loosely refer to as \"informationextraction robustness\" research.", "labels": [], "entities": []}, {"text": "Broadcast monitoring demands that IE handle as input not only clean text, but also the transcripts output by speech recognizers.", "labels": [], "entities": [{"text": "IE", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.873950183391571}]}, {"text": "2. Multilingual applications, and the imperfection of translation technology, require IE to contend with non-target-language text input ().", "labels": [], "entities": []}, {"text": "3. Naive users at times input to IE other material which deviates from clean text, such as a PDF file that \"looks\" like plain text.", "labels": [], "entities": []}, {"text": "4. Search applications require IE to deal with databases which not only possess clean text but at times exhibit other complications like markup codes particular to narrow, applicationspecific data-format standards, for example, the excerpt from a financial-transactions data set shown in.", "labels": [], "entities": [{"text": "IE", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.8772499561309814}]}, {"text": "Legacy industry-specific standards, such as illustrated in this example, are part of longestablished processes which are cumbersome to convert to a more-modern database format.", "labels": [], "entities": []}, {"text": "Transaction data sets typically buildup over a period of years, and as seen here, can exhibit  peculiar mark-up interspersed with meaningful text.", "labels": [], "entities": []}, {"text": "They also suffer complications arising from limited-size entry fields and a diversity of data-entry personnel, leading to effects like haphazard abbreviation and improper spacing, as shown.", "labels": [], "entities": []}, {"text": "These issues greatly complicate the IE problem, particularly considering that adapting IE to such formats is hampered by the existence of a multitude of such \"standards\" and by lack of sufficient annotated data in each one.", "labels": [], "entities": [{"text": "IE", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9909036755561829}]}, {"text": "A typical state-of-the-art statistical IE engine will happily process such \"noisy\" inputs, and will typically provide garbage-in/garbage-out performance, embarrassingly reporting spurious \"information\" no human would ever mistake.", "labels": [], "entities": []}, {"text": "Yet it is also inappropriate to discard such documents wholesale: even poor-quality inputs may have relevant information interspersed.", "labels": [], "entities": []}, {"text": "This information can include accurate speech-recognition output, names which are recognizable even in wrong-language material, and clean target-language passages interleaved with the markup.", "labels": [], "entities": []}, {"text": "Thus, here we address methods to make IE robust to such varied-quality inputs.", "labels": [], "entities": [{"text": "IE", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9932871460914612}]}, {"text": "Specifically, our overall goals are \u2022 to skip processing non-language material such as standard or database-specific mark-up, \u2022 to process all non-target-language text cautiously, catching interspersed target-language text as well as text which is compatible with the target language, e.g. person names which are the same in the target-and non-target language, and \u2022 to degrade gracefully when processing anomalous target-language material, while minimizing any disruption of the processing of clean, target-language text, and avoiding any necessity for explicit pre-classification of the genre of material being input to the system.", "labels": [], "entities": []}, {"text": "Such explicit classification would be impractical in the presence of the interleaving and the unconstrained data formats from unpredetermined sources.", "labels": [], "entities": []}, {"text": "We begin our robustness work by addressing an important and basic IE task: mention detection (MD).", "labels": [], "entities": [{"text": "IE", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9859551787376404}, {"text": "mention detection (MD)", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.8450744390487671}]}, {"text": "MD is the task of identifying and classifying textual references to entities in open-domain texts.", "labels": [], "entities": [{"text": "identifying and classifying textual references to entities in open-domain texts", "start_pos": 18, "end_pos": 97, "type": "TASK", "confidence": 0.5437701612710952}]}, {"text": "Mentions maybe of type \"named\" (e.g. John, Las Vegas), \"nominal\" (e.g. engineer, dentist) or \"pronominal\" (e.g. they, he).", "labels": [], "entities": []}, {"text": "A mention also has a specific class which describes the type of entity it refers to.", "labels": [], "entities": []}, {"text": "For instance, consider the following sentence: Julia Gillard, prime minister of Australia, declared she will enhance the country's economy.", "labels": [], "entities": []}, {"text": "Here we see three mentions of one person entity: Julia Gillard, prime minister, and she; these mentions are of type named, nominal, and pronominal, respectively.", "labels": [], "entities": []}, {"text": "Australia and country are mentions of type named and nominal, respectively, of a single geopolitical entity.", "labels": [], "entities": []}, {"text": "Thus, the MD task is a more general and complex task than named-entity recognition, which aims at identifying and classifying only named mentions.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7510935962200165}]}, {"text": "Our approach to IE has been to use languageindependent algorithms, in order to facilitate reuse across languages, but we train them with languagespecific data, for the sake of accuracy.", "labels": [], "entities": [{"text": "IE", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9896404147148132}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9956644177436829}]}, {"text": "Therefore, input is expected to be predominantly in a target language.", "labels": [], "entities": []}, {"text": "However, real-world data genres inevitably include some mixed-language/non-linguistic input.", "labels": [], "entities": []}, {"text": "Genre-specific training is typically infeasible due to such application-specific data sets being unannotated, motivating this line of research.", "labels": [], "entities": []}, {"text": "Therefore, the goal of this study is to investigate schemes to make a language-specific MD engine robust to the types of interspersed non-target material described above.", "labels": [], "entities": []}, {"text": "In these initial experiments, we work with English as the target language, though we aim to make our approach to robustness as target-language-independent as possible.", "labels": [], "entities": []}, {"text": "While our ultimate goal is a languageindependent approach to robustness, in these initial experiments, English is the target language.", "labels": [], "entities": []}, {"text": "However, we process mixed-language material including real-world data with its own peculiar mark-up, text conventions including abbreviations, and mix of languages, with the goal of English MD.", "labels": [], "entities": [{"text": "English MD", "start_pos": 182, "end_pos": 192, "type": "TASK", "confidence": 0.6320417821407318}]}, {"text": "We approach robust MD using a multi-stage strategy.", "labels": [], "entities": []}, {"text": "First, non-target-character-set passages (here, non-Latin-alphabet) are identified and marked for non-processing.", "labels": [], "entities": []}, {"text": "Then, following word-tokenization, we apply a language classifier to a sliding variablelength set of windows in order to generate features for each word indicative of how much the text around that word resembles good English, primarily in comparison to other Latin-alphabet languages.", "labels": [], "entities": []}, {"text": "These features are used in a separate maximumentropy classifier whose output is a single feature to add to the MD classifier.", "labels": [], "entities": []}, {"text": "Additional features, primarily to distinguish English from non-language input, are added to MD as well.", "labels": [], "entities": []}, {"text": "An example is the minimum of the number of letters and the number of digits in the \"word\", which when greater than zero often indicates database detritus.", "labels": [], "entities": []}, {"text": "Then we run the MD classifier enhanced with these new robustnessoriented features.", "labels": [], "entities": []}, {"text": "We evaluate using a detectionerror-trade-off (DET) () analysis, in addition to traditional precision/recall/Fmeasure.", "labels": [], "entities": [{"text": "detectionerror-trade-off (DET)", "start_pos": 20, "end_pos": 50, "type": "METRIC", "confidence": 0.8595250248908997}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9987101554870605}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.6894211173057556}, {"text": "Fmeasure", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.5284377932548523}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses previous work.", "labels": [], "entities": []}, {"text": "Section 3 describes the baseline maximum-entropy-based MD system.", "labels": [], "entities": []}, {"text": "Section 4 introduces enhancements to the system to achieve robustness.", "labels": [], "entities": []}, {"text": "Section 5 describes databases used for experiments, which are discussed in Section 6, and Section 7 draws conclusions and plots future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "MD systems were trained to recognize the 116 entity-mention types shown in, annotated as described previously.", "labels": [], "entities": []}, {"text": "The clean-data classifier was trained on the English training data using the feature set described in Section 3.1.", "labels": [], "entities": [{"text": "English training data", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.6347223023573557}]}, {"text": "The classifier for \"mixed\"-quality data and the \"gazetteer\" model were each trained on that set plus the \"Latin\" training set and the supplemental set.", "labels": [], "entities": []}, {"text": "In addition, \"mixed\" training included the additional features described in Section 4.5.", "labels": [], "entities": []}, {"text": "The framework used to build the baseline MD system is similar to the one we used in the ACE evaluation 2 . This system has achieved competitive results with an F -measure of 82.7 when trained on the seven main types of ACE data with access to wordnet and part-of-speech-tag information as well as output of other MD and named-entity recognizers (.", "labels": [], "entities": [{"text": "F -measure", "start_pos": 160, "end_pos": 170, "type": "METRIC", "confidence": 0.9955456058184305}]}, {"text": "It is instructive to evaluate on the individual component systems as well as the combination, despite the fact that the individual components are not wellsuited to all the data sets, for example, the mixed and gazetteer systems being a poorer fit to the English task than the baseline, and vice versa for the age event: Performance of clean, mixed, and gazetteer-based mention detection systems as well as their combination.", "labels": [], "entities": [{"text": "gazetteer-based mention detection", "start_pos": 353, "end_pos": 386, "type": "TASK", "confidence": 0.6503795087337494}]}, {"text": "Performance is presented in terms of Precision (P), Recall (R), and F -measure (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9369484037160873}, {"text": "Recall (R)", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9588016271591187}, {"text": "F -measure (F)", "start_pos": 68, "end_pos": 82, "type": "METRIC", "confidence": 0.9779277344544729}]}, {"text": "Precision/recall/F -measure results are shown in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9885884523391724}, {"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8154153227806091}, {"text": "F -measure", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.980551520983378}]}, {"text": "Not surprisingly, the baseline system, intended for clean data, performs poorly on noisy data.", "labels": [], "entities": []}, {"text": "The mixed and gazetteer systems, having a variety of noisy data in their training set, perform much better on the noisy conditions, particularly on Latin-alphabet-non-English data because that is one of the conditions included in its training, while Transactions remains a condition not covered in the training set and so shows less improvement.", "labels": [], "entities": []}, {"text": "However, because the mixed classifier, and moreso the gazetteer classifier, are oriented to noisy data, on clean data they suffer in performance by 2.5 and 5 F -measure points, respectively.", "labels": [], "entities": [{"text": "F -measure", "start_pos": 158, "end_pos": 168, "type": "METRIC", "confidence": 0.9775485197703043}]}, {"text": "But system combination serves us well: it recovers all but 0.5 F -measure point of this loss, while also actually performing better on the noisy data sets than the two classifiers specifically targeted toward them, as can be seen in.", "labels": [], "entities": [{"text": "F -measure", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.98581463098526}]}, {"text": "It is important to note that the major advantage of using the combination model is the fact that we do not have to know the data source in order to select the appropriate MD model to use.", "labels": [], "entities": []}, {"text": "We assume that the data source is unknown, which is our claim in this work, and we show that we obtain better performance than using source-specific MD models.", "labels": [], "entities": []}, {"text": "This reflects the fact that a noisy data set will in fact have portions with varying degrees of \"noise\", so the combination outperforms any single model targeted to a single particular level of noise, enabling the system to contend with such variability without the need for presegregating sub-types of data for noise level.", "labels": [], "entities": []}, {"text": "The obtained improvement from the system combination overall other models is statistically significant based on the stratified bootstrap re-sampling significance test.", "labels": [], "entities": []}, {"text": "We consider results statistically significant when p < 0.05, which is the casein this paper.", "labels": [], "entities": []}, {"text": "This approach was used in the named-entityrecognition shared task of . It should be noted that some completely-nontarget types of data, such as non-target-character set data, have been omitted from analysis here.", "labels": [], "entities": []}, {"text": "Including them would make our system look comparatively stronger, as they would have only spurious mentions and so generate false alarms but no correct mentions in the baseline system, while our system deterministically removes them.", "labels": [], "entities": []}, {"text": "As mentioned above, we view MD robustness primarily as an effort to eliminate, relative to a baseline system, large volumes of spurious \"mentions\" detected in non-target input content, while minimiz-(a) DET plot for clean (baseline), mixed, gazetteer, and combination MD systems on the Latin-alphabetnon-English text.", "labels": [], "entities": [{"text": "DET", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.8006113767623901}]}, {"text": "The clean system (upper curve) performs far worse than the other three systems designed to provide robustness; these systems in turn perform nearly indistinguishably.", "labels": [], "entities": []}, {"text": "(b) DET plot for clean (baseline), mixed, gazetteer, and combination MD systems on the Transactions data set.", "labels": [], "entities": [{"text": "DET", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7573598623275757}, {"text": "Transactions data set", "start_pos": 87, "end_pos": 108, "type": "DATASET", "confidence": 0.9614894588788351}]}, {"text": "The clean system (upper/longer curve) reaches far higher false-alarm rates, while never approaching the lower miss rates achievable by any of the other three systems, which in turn perform comparably to each other.", "labels": [], "entities": [{"text": "miss rates", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9527336359024048}]}, {"text": "ing disruption of detection in target input.", "labels": [], "entities": []}, {"text": "A secondary goal is recall in the event of occasional valid mentions in such non-target material.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9922441840171814}]}, {"text": "Thus, as input material degrades, precision increases in importance relative to recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9995361566543579}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9971119165420532}]}, {"text": "As such, we view precision and recall asymmetrically on this task, and so rather than evaluating purely in terms of F -measure, we perform a detection-error-trade-off (DET) () analysis, in which we plot a curve of miss rate on valid mentions vs. false-alarm rate, with the curve traced by varying a confidence threshold across its range.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9986862540245056}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9963688850402832}, {"text": "F -measure", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9913946588834127}, {"text": "detection-error-trade-off (DET)", "start_pos": 141, "end_pos": 172, "type": "METRIC", "confidence": 0.8039962947368622}, {"text": "miss rate", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9378344118595123}]}, {"text": "We measure false-alarm and miss rates relative to the number of actual mentions annotated in the data set: FA rate = # false alarms # annotated mentions (1) where false alarms are \"mentions\" output by the system but not appearing in annotation, while misses are mentions which are annotated but do not appear in the system output.", "labels": [], "entities": [{"text": "FA rate", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9883992969989777}]}, {"text": "Each mention is treated equally in this analysis, so frequently-recurring entity/mention types weigh on the results accordingly.", "labels": [], "entities": []}, {"text": "shows a DET plot for the clean, mixed, gazetteer, and combination systems on the \"Latin\" data set, while shows the analogous plot for the \"Transactions\" data set.", "labels": [], "entities": [{"text": "Latin\" data set", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.7121214494109154}, {"text": "Transactions\" data set", "start_pos": 139, "end_pos": 161, "type": "DATASET", "confidence": 0.7327161058783531}]}, {"text": "The drastic gains made over the baseline system by the three experimental systems are evident in the plots.", "labels": [], "entities": []}, {"text": "For example, on Latin, choosing an operating point of amiss rate of 0.6 (nearly the best achievable by the clean system), we find that the robustness-oriented systems eliminate 97% of the false alarms of the clean baseline system, as the plot shows false-alarm rates near 0.07 compared to the baseline's of 2.08.", "labels": [], "entities": []}, {"text": "Gains on Transaction data are more modest, owing to this case representing a data genre not included in training.", "labels": [], "entities": [{"text": "Gains", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9828028082847595}]}, {"text": "It should be noted that the jaggedness of the Transaction curves traces to the repetitive nature of some of the terms in this data set.", "labels": [], "entities": []}, {"text": "In making a system more oriented toward robustness in the face of non-target inputs, it is important to quantify the effect of these systems being lessoriented toward clean, target-language text.", "labels": [], "entities": []}, {"text": "shows the analogous DET plot for the English test set, showing that achieving robustness through the combination system comes at a small cost to accuracy on the text the original system is trained to process.", "labels": [], "entities": [{"text": "DET", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.7814366817474365}, {"text": "English test set", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.8356555302937826}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9990423321723938}]}], "tableCaptions": [{"text": " Table 1: Entity-type categories used in these experiments. The eight in the right-most column are not  further distinguished by mention type, while the remaining 36 are further classified as named, nominal or  pronominal, for a total of 36 \u00d7 3 + 8 = 116 mention labels.", "labels": [], "entities": []}, {"text": " Table 2: Performance of clean, mixed, and gazetteer-based mention detection systems as well as their com- bination. Performance is presented in terms of Precision (P), Recall (R), and F -measure (F).", "labels": [], "entities": [{"text": "gazetteer-based mention detection", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6074824333190918}, {"text": "Precision (P)", "start_pos": 154, "end_pos": 167, "type": "METRIC", "confidence": 0.9574665725231171}, {"text": "Recall (R)", "start_pos": 169, "end_pos": 179, "type": "METRIC", "confidence": 0.9467190951108932}, {"text": "F -measure (F)", "start_pos": 185, "end_pos": 199, "type": "METRIC", "confidence": 0.9774195551872253}]}]}