{"title": [{"text": "We're Not in Kansas Anymore: Detecting Domain Changes in Streams", "labels": [], "entities": [{"text": "Detecting Domain Changes in Streams", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.9022323608398437}]}], "abstractContent": [{"text": "Domain adaptation, the problem of adapting a natural language processing system trained in one domain to perform well in a different domain, has received significant attention.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8170965909957886}]}, {"text": "This paper addresses an important problem for deployed systems that has received little attention detecting when such adaptation is needed by a system operating in the wild, i.e., performing classification over a stream of unlabeled examples.", "labels": [], "entities": []}, {"text": "Our method uses A-distance, a metric for detecting shifts in data streams, combined with classification margins to detect domain shifts.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9897538423538208}]}, {"text": "We empirically show effective domain shift detection on a variety of data sets and shift conditions.", "labels": [], "entities": [{"text": "domain shift detection", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7606245080629984}]}], "introductionContent": [{"text": "Consider a named entity recognition system trained on newswire stories.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.6808788379033407}]}, {"text": "Given annotated documents containing sentences like \"Tony Hayward has faced fresh criticism for taking time off to go sailing . .", "labels": [], "entities": []}, {"text": ".\" we would like to learn a model that will allow us to recognize that \"Obama\" and \"BP\" are named entities in a sentence like \"Obama summoned BP executives . .", "labels": [], "entities": []}, {"text": "When all of the documents come from one data distribution, like newswire articles, this tends to work well.", "labels": [], "entities": []}, {"text": "However, the sentence \"OBAMA SUMMONED BP EXECUTIVES . .", "labels": [], "entities": [{"text": "OBAMA SUMMONED BP EXECUTIVES", "start_pos": 23, "end_pos": 51, "type": "METRIC", "confidence": 0.7965501844882965}]}, {"text": ".\" from transcribed broadcast news, and others like it, will probably lead to poor results because the features it relies on have changed.", "labels": [], "entities": []}, {"text": "For example, capitalization patterns are no longer a good indicator of the presence of a named entity and appositives are not indicated by punctuation.", "labels": [], "entities": []}, {"text": "This problem of domain shift is a pervasive problem in NLP in which any kind of model -a parser, a POS tagger, a sentiment classifier -is tested on data that do not match the training data.", "labels": [], "entities": [{"text": "domain shift", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.7091632783412933}]}, {"text": "Given a model and a stream of unlabeled instances, we are interested in automatically detecting changes in the feature distribution that negatively impact classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.8249980807304382}]}, {"text": "For example, a sentiment classification model trained on book reviews may heavily weight n-grams features like \"uplifting\" and \"page turner\".", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.9665361940860748}]}, {"text": "Those features may never occur in reviews of kitchen appliances that get mixed in attest time, and useful features in this new domain like \"efficient\" and \"noisy compressor\" will have never been seen during training and therefore not be in the model.", "labels": [], "entities": []}, {"text": "Furthermore, we do not assume labeled instances are available to help detect these harmful changes.", "labels": [], "entities": []}, {"text": "Other tasks related to changes in data distributions, like detecting concept drift in which the labeling function changes, may require labeled instances, but that is not the focus of this paper.", "labels": [], "entities": [{"text": "detecting concept drift", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8456522623697916}]}, {"text": "There is significant work on the related problem of adapting a classifier fora known domain shift.", "labels": [], "entities": []}, {"text": "Versions of this problem include adapting using only unlabeled target domain data, adapting using a limited amount of target domain labeled data, and learning across multiple domains simultaneously in an online setting).", "labels": [], "entities": []}, {"text": "However, in practical settings, we do not know if the data distribution will change, and certainly not when.", "labels": [], "entities": []}, {"text": "Additionally, we will not know to what do-main the shift will happen.", "labels": [], "entities": []}, {"text": "A discussion forum devoted to science fiction books may changeover time to focus more on fantasy and then narrow to discussions of vampire fiction.", "labels": [], "entities": []}, {"text": "Maybe this shift is harmless and it is possible to identify the sentiment of the discussants with the original model with no loss inaccuracy.", "labels": [], "entities": []}, {"text": "If not, we seek methods that detect this shift and trigger the use of an adaptation method.", "labels": [], "entities": []}, {"text": "Our domain shift detection problem can be decomposed into two subproblems: detecting distributional changes in streams of real numbers, and representing a stream of examples as a stream of real numbers informative for distribution change detection.", "labels": [], "entities": [{"text": "domain shift detection", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.759861965974172}, {"text": "detecting distributional changes in streams of real numbers", "start_pos": 75, "end_pos": 134, "type": "TASK", "confidence": 0.8036335781216621}, {"text": "distribution change detection", "start_pos": 218, "end_pos": 247, "type": "TASK", "confidence": 0.708843876918157}]}, {"text": "We select the A-distance metric () to solve the first subproblem since it has been previously used in other domain adaptation work.", "labels": [], "entities": [{"text": "A-distance metric", "start_pos": 14, "end_pos": 31, "type": "METRIC", "confidence": 0.9677099883556366}]}, {"text": "Our main contribution is towards the second problem, representing examples as real numbers for this task.", "labels": [], "entities": []}, {"text": "We demonstrate that classification margins, which incorporate information about features that most impact system accuracy, can effectively solve the second subproblem.", "labels": [], "entities": [{"text": "classification", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.9649637937545776}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9970645308494568}]}, {"text": "Furthermore, we show that the previously proposed Confidence Weighted learning algorithm can provide a more informative measure than a simple margin for this task.", "labels": [], "entities": []}, {"text": "Our experiments include evaluations on commonly used domain adaptation data and false change scenarios, as well as comparisons to supervised detection methods that observe label values, or have knowledge of the target domain.", "labels": [], "entities": []}, {"text": "We begin with a description of our task and previous applications to language data.", "labels": [], "entities": []}, {"text": "After describing the data used in this paper, we discuss the Adistance metric and how it has previously been used for adaptation.", "labels": [], "entities": []}, {"text": "We then show that margin based methods effectively capture information to detect domain shifts, and propose an alternate way of generating informative margin values.", "labels": [], "entities": []}, {"text": "Finally, we compare our results to settings with supervised knowledge, and close with a survey of related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We begin the presentation of our methods by describing the data used in our experiments.", "labels": [], "entities": []}, {"text": "We selected three data sets commonly used in domain adaptation: spam, ACE 2005 named entity recognition, and sentiment.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7380878180265427}, {"text": "ACE 2005 named entity recognition", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.8449187755584717}]}, {"text": "Sentiment and spam are binary and ACE is multi-class.", "labels": [], "entities": []}, {"text": "Note that in all experiments, a shift in the domain yields a decrease in system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9947975873947144}]}, {"text": "The goal of the spam data is to classify an email (bag-of-words) as either spam or ham (not-spam).", "labels": [], "entities": []}, {"text": "Each email user may have different preferences and features.", "labels": [], "entities": []}, {"text": "We used unigram and bigram features, following Dredze and Crammer (2008b) for feature extraction, and used the three task A users as three domains.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7135425508022308}]}, {"text": "The ACE 2005 named entity recognition dataset includes 7 named entity class labels (person, organization, location, geopolitical entity, facility, vehicle, weapon) for 5 text genres (newswire, broadcast news, broadcast conversations, conversational telephone speech, weblogs).", "labels": [], "entities": [{"text": "ACE 2005 named entity recognition dataset", "start_pos": 4, "end_pos": 45, "type": "DATASET", "confidence": 0.8688448369503021}]}, {"text": "We use 4000 examples from each genre and used Jiang and Zhai's featureextracted data.", "labels": [], "entities": []}, {"text": "The sentiment data contains reviews from Amazon for four product types: books, dvds, electronics, and kitchen.", "labels": [], "entities": []}, {"text": "We include an additional two types (music and video from Dredze and Crammer) in our false shift experiments and use unigram and bigram features, following Blitzer et al.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: False positives (FPs) observed in true domain  shift and false domain shift experiments for methods in  corresponding sections. Each setting was run 10 times,  resulting in 380 true domain shifts and 110 false shifts.", "labels": [], "entities": [{"text": "False positives (FPs)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9647398591041565}]}]}