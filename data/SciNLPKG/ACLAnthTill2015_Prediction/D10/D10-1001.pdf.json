{"title": [{"text": "On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing", "labels": [], "entities": [{"text": "Linear Programming Relaxations", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.644331713517507}]}], "abstractContent": [{"text": "This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8578589856624603}]}, {"text": "The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles.", "labels": [], "entities": []}, {"text": "The approach provably solves a linear programming (LP) relaxation of the global inference problem.", "labels": [], "entities": []}, {"text": "It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation.", "labels": [], "entities": []}, {"text": "We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dynamic programming algorithms have been remarkably useful for inference in many NLP problems.", "labels": [], "entities": []}, {"text": "Unfortunately, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.", "labels": [], "entities": []}, {"text": "As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP.", "labels": [], "entities": []}, {"text": "This paper introduces dual decomposition ( as a framework for deriving inference algorithms in NLP.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.820781946182251}]}, {"text": "Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems.", "labels": [], "entities": []}, {"text": "The approach leads to inference algorithms with the following properties: \u2022 The resulting algorithms are simple and efficient, building on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a method for forcing agreement between the oracles.", "labels": [], "entities": []}, {"text": "\u2022 The algorithms provably solve a linear programming (LP) relaxation of the original inference problem.", "labels": [], "entities": []}, {"text": "\u2022 Empirically, the LP relaxation often leads to an exact solution to the original problem.", "labels": [], "entities": [{"text": "LP relaxation", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9347312748432159}]}, {"text": "The approach is very general, and should be applicable to a wide range of problems in NLP.", "labels": [], "entities": []}, {"text": "The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact).", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows.", "labels": [], "entities": []}, {"text": "We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and dependency parsing.", "labels": [], "entities": [{"text": "trigram part-ofspeech (POS) tagging", "start_pos": 89, "end_pos": 124, "type": "TASK", "confidence": 0.5747410804033279}, {"text": "dependency parsing", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7465550899505615}]}, {"text": "In both settings, it is possible to solve the integrated problem through an \"intersected\" dynamic program (e.g., for integration of parsing and tagging, the construction from Bar- can be used).", "labels": [], "entities": []}, {"text": "However, these methods, although polynomial time, are substantially less efficient than our algorithms, and are considerably more complex to implement.", "labels": [], "entities": []}, {"text": "Next, we describe exact polyhedral formulations for the two problems, building on connections between dynamic programming algorithms and marginal polytopes, as described in.", "labels": [], "entities": []}, {"text": "These allow us to precisely characterize the relationship between the exact formulations and the LP relaxations that we solve.", "labels": [], "entities": []}, {"text": "We then give guarantees of convergence for our algorithms by showing that they are instantiations of Lagrangian relaxation, a general method for solving linear programs of a particular form.", "labels": [], "entities": [{"text": "convergence", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9524424076080322}]}, {"text": "Finally, we describe experiments that demonstrate the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "First, we consider the integration of the generative model for phrase-structure parsing of, with the second-order discriminative dependency parser of . This is an interesting problem in its own right: the goal is to inject the high performance of discriminative dependency models into phrase-structure parsing.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.798446923494339}, {"text": "phrase-structure parsing", "start_pos": 285, "end_pos": 309, "type": "TASK", "confidence": 0.8091468811035156}]}, {"text": "The method uses off-theshelf decoders for the two models.", "labels": [], "entities": []}, {"text": "We find three main results: 1) in spite of solving an LP relaxation, empirically the method finds an exact solution on over 99% of the examples; 2) the method converges quickly, typically requiring fewer than 10 iterations of decoding; 3) the method gives gains over a baseline method that forces the phrase-structure parser to produce the same dependencies as the firstbest output from the dependency parser model has an F1 score of 88.1%; the baseline method has an F1 score of 89.7%; and the dual decomposition method has an F1 score of 90.7%).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 422, "end_pos": 430, "type": "METRIC", "confidence": 0.9827156662940979}, {"text": "F1 score", "start_pos": 468, "end_pos": 476, "type": "METRIC", "confidence": 0.9826745092868805}, {"text": "F1 score", "start_pos": 528, "end_pos": 536, "type": "METRIC", "confidence": 0.9843437075614929}]}, {"text": "Ina second set of experiments, we use dual decomposition to integrate the trigram POS tagger of with the parser of.", "labels": [], "entities": []}, {"text": "We again find that the method finds an exact solution in almost all cases, with convergence in just a few iterations of decoding.", "labels": [], "entities": []}, {"text": "Although the focus of this paper is on dynamic programming algorithms-both in the experiments, and also in the formal results concerning marginal polytopes-it is straightforward to use other combinatorial algorithms within the approach.", "labels": [], "entities": []}, {"text": "For example, describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7768087387084961}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Convergence results for Section 23 of the WSJ  Treebank for the dependency parsing and POS experi- ments. Each column gives the percentage of sentences  whose exact solutions were found in a given range of sub- gradient iterations. ** is the percentage of sentences that  did not converge by the iteration limit (K=50).", "labels": [], "entities": [{"text": "WSJ  Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9804722368717194}, {"text": "dependency parsing", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7802368998527527}]}, {"text": " Table 2: Performance results for Section 23 of the WSJ  Treebank. Model 1: a reimplementation of the genera- tive parser of", "labels": [], "entities": [{"text": "WSJ  Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.975999653339386}]}, {"text": " Table 3: Performance results for Section 23 of the WSJ.  Model 1 (Fixed", "labels": [], "entities": [{"text": "Section 23 of the WSJ", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.7101322889328003}]}]}