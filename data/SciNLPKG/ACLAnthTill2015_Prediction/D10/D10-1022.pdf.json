{"title": [{"text": "Negative Training Data can be Harmful to Text Classification", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper studies the effects of training data on binary text classification and postulates that negative training data is not needed and may even be harmful for the task.", "labels": [], "entities": [{"text": "binary text classification", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.6660273174444834}]}, {"text": "Traditional binary classification involves building a clas-sifier using labeled positive and negative training examples.", "labels": [], "entities": []}, {"text": "The classifier is then applied to classify test instances into positive and negative classes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text classification is a well-studied problem in machine learning, natural language processing, and information retrieval.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8447627127170563}, {"text": "natural language processing", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6672671139240265}, {"text": "information retrieval", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7847808301448822}]}, {"text": "To build a text classifier, a set of training documents is first labeled with predefined classes.", "labels": [], "entities": []}, {"text": "Then, a supervised machine learning algorithm (e.g., Support Vector Machines (SVM), na\u00efve Bayesian classifier (NB)) is applied to the training examples to build a classifier that is subsequently employed to assign class labels to the instances in the test set.", "labels": [], "entities": []}, {"text": "In this paper, we focus on binary text classification with two classes (i.e. positive and negative classes).", "labels": [], "entities": [{"text": "binary text classification", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.6464063227176666}]}, {"text": "Most learning methods assume that the training and test data have identical distributions.", "labels": [], "entities": []}, {"text": "However, this assumption may not hold in practice, i.e., the training and the test distributions can be different.", "labels": [], "entities": []}, {"text": "The problem is called covariate shift or sample selection bias).", "labels": [], "entities": []}, {"text": "In general, this problem is not solvable because the two distributions can be arbitrarily far apart from each other.", "labels": [], "entities": []}, {"text": "Various assumptions were made to solve special cases of the problem.", "labels": [], "entities": []}, {"text": "One main assumption was that the conditional distribution of the class given an instance is the same over the training and test sets.", "labels": [], "entities": []}, {"text": "In this paper, we study another special case of the problem in which the positive training and test samples have identical distributions, but the negative training and test samples may have different distributions.", "labels": [], "entities": []}, {"text": "We believe this scenario is more applicable for binary text classification.", "labels": [], "entities": [{"text": "binary text classification", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.6860758463541666}]}, {"text": "As the focus in many applications is on identifying positive instances correctly, it is important that the positive training and the positive test data have the same distribution.", "labels": [], "entities": []}, {"text": "The distributions of the negative training and negative test data can be different.", "labels": [], "entities": []}, {"text": "We believe that this special case of the sample selection bias problem is also more applicable for machine learning.", "labels": [], "entities": []}, {"text": "We will show that a partially supervised learning model, called PU learning (learning from Positive and Unlabeled examples) fits this special case quite well (.", "labels": [], "entities": []}, {"text": "Following the notations in (), our special case of the sample selection bias problem can be formulated as follows: We are given a training sample matrix XL with row vectors x 1 , \u2026, x k . The positive and negative training instances are governed by different unknown distributions p(x|\u03bb) and p(x|\u03b4) respectively.", "labels": [], "entities": []}, {"text": "The element y i of vector y = (y 1 , y 2 , \u2026, y k ) is the class label for training instance xi (y i \u2208{+1, -1}, where +1 and -1 denote positive and negative classes respectively) and is drawn based on an unknown target concept p(y|x).", "labels": [], "entities": []}, {"text": "In addition, we are also given an unlabeled test set in matrix X T with rows x k+1 , \u2026, x k+m . The (hidden) positive test instances in X T are also governed by the unknown distribution p(x|\u03bb), but the (hidden) negative test instances in X T are governed by an unknown distribution, p(x|\u03b8), where \u03b8 mayor may not be the same as \u03b4. p(x|\u03b8) and p(x|\u03b4) can differ arbitrarily, but there is only one unknown target conditional class distribution p(y|x).", "labels": [], "entities": []}, {"text": "This problem setting is common in many applications, especially in those applications where the user is interested in identifying a particular type of documents (i.e. binary text classification).", "labels": [], "entities": [{"text": "binary text classification)", "start_pos": 167, "end_pos": 194, "type": "TASK", "confidence": 0.7410919815301895}]}, {"text": "For example, we want to find sentiment analysis papers in the literature.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9616090357303619}]}, {"text": "For training a text classifier, we may label the papers in some EMNLP proceedings as sentiment analysis (positive) and non-sentiment analysis (negative) papers.", "labels": [], "entities": [{"text": "EMNLP proceedings", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.8684379458427429}]}, {"text": "A classifier can then be built to find sentiment analysis papers from ACL and other EMNLP proceedings.", "labels": [], "entities": [{"text": "sentiment analysis papers from ACL", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.745384007692337}, {"text": "EMNLP proceedings", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.7425163984298706}]}, {"text": "However, this labeled training set will not be appropriate for identifying sentiment analysis papers from the WWW, KDD and SIGIR conference proceedings.", "labels": [], "entities": [{"text": "identifying sentiment analysis", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.7307217319806417}, {"text": "WWW, KDD and SIGIR conference proceedings", "start_pos": 110, "end_pos": 151, "type": "DATASET", "confidence": 0.7103243895939418}]}, {"text": "This is because although the sentiment analysis papers in these proceedings are similar to those in the training data, the non-sentiment analysis papers in these conferences can be quite different.", "labels": [], "entities": []}, {"text": "Another example is email spam detection.", "labels": [], "entities": [{"text": "email spam detection", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.761670450369517}]}, {"text": "A spam classification system built using the training data of spam and non-spam emails from a university may not perform well in a company.", "labels": [], "entities": [{"text": "spam classification", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.854797899723053}]}, {"text": "The reason is that although the spam emails (e.g., unsolicited commercial ads) are similar in both environments, the non-spam emails in them can be quite different.", "labels": [], "entities": []}, {"text": "One can consider labeling the negative data in each environment individually so that only the negative instances relevant to the testing environment are used to train the classifier.", "labels": [], "entities": []}, {"text": "However, it is often impractical (if not impossible) to do so.", "labels": [], "entities": []}, {"text": "For example, given a large blog hosting site, we want to classify its blogs into those that discuss stock markets (positive), and those that do not (negative).", "labels": [], "entities": []}, {"text": "In this case, the negative data covers an arbitrary range of topics.", "labels": [], "entities": []}, {"text": "It is clearly impractical to label all the negative data.", "labels": [], "entities": []}, {"text": "Most existing methods for addressing the sample selection bias problem work as follows.", "labels": [], "entities": [{"text": "sample selection bias", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7678685486316681}]}, {"text": "First, they estimate the bias of the training data based on the given test data using statistical methods.", "labels": [], "entities": []}, {"text": "Then, a classifier is trained on a weighted version of the original training set based on the estimated bias.", "labels": [], "entities": []}, {"text": "In this paper, we show that our special case of the sample selection bias problem can be solved in a much simpler and somewhat radical manner-by simply discarding the negative training data altogether.", "labels": [], "entities": [{"text": "sample selection bias", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.6863810817400614}]}, {"text": "We can use the positive training data and the unlabeled test data to build the classifier using the PU learning model (.", "labels": [], "entities": []}, {"text": "PU learning was originally proposed to solve the learning problem where no labeled negative training data exist.", "labels": [], "entities": [{"text": "PU learning", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7832905054092407}]}, {"text": "Several algorithms have been developed in the past few years that can learn from a set of labeled positive examples augmented with a set of unlabeled examples.", "labels": [], "entities": []}, {"text": "That is, given a set P of positive examples of a particular class (called the positive class) and a set U of unlabeled examples (which contains both hidden positive and hidden negative examples), a classifier is built using P and U to classify the data in U as well as future test data into two classes, i.e., those belonging to P (positive) and those not belonging to P (negative).", "labels": [], "entities": []}, {"text": "In this paper, we also propose anew PU learning method which gives more consistently accurate results than the current methods.", "labels": [], "entities": []}, {"text": "Our experimental evaluation shows that when the distributions of the negative training and test samples are different, PU learning is much more accurate than traditional supervised learning from the positive and negative training samples.", "labels": [], "entities": []}, {"text": "This means that the negative training data actually harms classification in this case.", "labels": [], "entities": [{"text": "classification", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.8587961792945862}]}, {"text": "In addition, when the distributions of the negative training and test samples are identical, PU learning is shown to perform equally well as supervised learning, which means that the negative training data is not needed.", "labels": [], "entities": []}, {"text": "This paper thus makes three contributions.", "labels": [], "entities": []}, {"text": "First, it formulates anew special case of the sample selection bias problem, and proposes to solve the problem using PU learning by discarding the negative training data.", "labels": [], "entities": []}, {"text": "Second, it proposes anew PU learning method which is more accurate than the existing methods.", "labels": [], "entities": []}, {"text": "Third, it experimentally demonstrates the effectiveness of the proposed method and shows that negative training data is not needed and can even be harmful.", "labels": [], "entities": []}, {"text": "This result is important as it may fundamentally change the way that many practical classification problems should be solved.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present the experimental results to support our claim that negative training data is not needed and can even harm text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.85649773478508}]}, {"text": "We also show the effectiveness of the proposed PU learning methods CR-SVM and CR-EM.", "labels": [], "entities": []}, {"text": "The following methods are compared: (1) traditional supervised learning methods SVM and NB which use both positive and negative training data; (2) PU learning methods, including two existing methods S-EM and Roc-SVM and two new methods CR-SVM and CR-EM, and (3) one-class SVM () where only positive training data is used in learning (the unlabeled set is not used at all).", "labels": [], "entities": []}, {"text": "We used LIBSVM 1 for SVM and one-class SVM, and two publicly available 2 PU learning techniques S-EM and Roc-SVM.", "labels": [], "entities": []}, {"text": "Note that we do not compare with some other PU learning methods such as those in ( as the purpose of this paper is not to find the best PU learning method but to show that PU learning can address our special sample selection bias problem.", "labels": [], "entities": []}, {"text": "Our current methods already do very well for this purpose.", "labels": [], "entities": []}, {"text": "We used two well-known benchmark data collections for text classification, the Reuters-21578 collection and the 20 Newsgroup collection 4 . Reuters-21578 contains 21578 documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8259947597980499}, {"text": "Reuters-21578 collection", "start_pos": 79, "end_pos": 103, "type": "DATASET", "confidence": 0.9864828288555145}, {"text": "20 Newsgroup collection", "start_pos": 112, "end_pos": 135, "type": "DATASET", "confidence": 0.8984416723251343}]}, {"text": "We used the most populous 10 out of the 135 categories following the common practice of other researchers.", "labels": [], "entities": []}, {"text": "20 Newsgroup has 11997 documents from 20 discussion groups.", "labels": [], "entities": [{"text": "20 Newsgroup has 11997 documents", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.9384177327156067}]}, {"text": "The 20 groups were also categorized into 4 main categories.", "labels": [], "entities": []}, {"text": "We have performed two sets of experiments, and just used bag-of-words as features since our objective in this paper is not feature engineering.", "labels": [], "entities": []}, {"text": "(1) Test set has other topic documents.", "labels": [], "entities": []}, {"text": "This set of experiments simulates the scenario in which the negative training and test samples have different distributions.", "labels": [], "entities": []}, {"text": "We select positive, negative and other topic documents for Reuters and 20 Newsgroup, and produce various data sets.", "labels": [], "entities": [{"text": "Reuters and 20 Newsgroup", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.8397079408168793}]}, {"text": "Using these data sets, we want to show that PU learning can do bet- For the Reuters collection, each of the 10 categories is used as a positive class.", "labels": [], "entities": [{"text": "Reuters collection", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.9767268002033234}]}, {"text": "We randomly select one or two of the remaining categories as the negative class (denoted by Neg 1 or Neg 2), and then we randomly choose some documents from the rest of the categories as other topic documents.", "labels": [], "entities": []}, {"text": "These other topic documents are regarded as negatives and added to the test set but not to the negative training data.", "labels": [], "entities": []}, {"text": "They thus introduce a different distribution to the negative test data.", "labels": [], "entities": []}, {"text": "We generated 20 data sets (10*2) for our experiments this way.", "labels": [], "entities": []}, {"text": "The 20 Newsgroup collection has 4 main categories with sub-categories ; the sub-categories in the same main category are relatively similar to each other.", "labels": [], "entities": [{"text": "20 Newsgroup collection", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.8800147771835327}]}, {"text": "We are able to simulate two scenarios: (1) the other topic documents are similar to the negative class documents (similar case), and (2) the other topic documents are quite different from the negative class documents (different case).", "labels": [], "entities": []}, {"text": "This allows us to investigate whether the classification results will be affected when the other topic documents are somewhat similar or vastly different from the negative training set.", "labels": [], "entities": []}, {"text": "To create the training and test data for our experiments, we randomly select one sub-category from a main category (cat 1) as the positive class, and one (or two) subcategory from another category (cat 2) as the negative class (again denoted by Neg 1 or Neg 2).", "labels": [], "entities": []}, {"text": "For the other topics, we randomly choose some docu- The four main categories and their corresponding subcategories are: computer (graphics, os, ibmpc.hardware, mac.hardware, windows.x), recreation (autos, motorcycles, baseball, hockey), science (crypt, electronics, med, space), and talk (politics.misc, politics.guns, politics.mideast, religion).", "labels": [], "entities": []}, {"text": "1. Every document in P is assigned the class label +1; 2.", "labels": [], "entities": []}, {"text": "Every document in RN is assigned the label -1; 3.", "labels": [], "entities": []}, {"text": "Use P and RN to train a SVM classifier Si , with i = 1 initially and i = i+1 with each iteration (line 3-7); 4.", "labels": [], "entities": [{"text": "SVM classifier Si", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.813835601011912}]}, {"text": "Classify Q using Si . Let the set of documents in Q that are classified as negative be W; 5.", "labels": [], "entities": []}, {"text": "If (W = \u2205) then stop; 6.", "labels": [], "entities": []}, {"text": "else Q = Q -W; 7.", "labels": [], "entities": [{"text": "Q -W", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.7527032295862833}]}, {"text": "RN = RN \u222aW 8.", "labels": [], "entities": [{"text": "RN", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.4677756726741791}]}, {"text": "goto (3); 9.", "labels": [], "entities": []}, {"text": "Use the last SVM classifier S last to classify P; 10.", "labels": [], "entities": []}, {"text": "If more than 5% positives are classified as negative 11.", "labels": [], "entities": []}, {"text": "then use S 1 as the final classifier; 12.", "labels": [], "entities": []}, {"text": "else use S last as the final classifier;.", "labels": [], "entities": []}, {"text": "Constructing the final classifier using SVM ments from the remaining sub-categories of cat 2 for the similar case, and some documents from a randomly chosen different category (cat 3) (as the other topic documents) for the different case.", "labels": [], "entities": []}, {"text": "We generated 8 data sets (4*2) for the similar case, and 8 data sets (4*2) for the different case.", "labels": [], "entities": []}, {"text": "The training and test sets are then constructed as follows: we partition the positive (and similarly for the negative) class documents into two standard subsets: 70% for training and 30% for testing.", "labels": [], "entities": []}, {"text": "In order to create different experimental settings, we vary the number of the other topic documents that are added to the test set as negatives, controlled by a parameter \u03b1, which is a percentage of |TN|, where |TN| is the size of the negative test set without the other topic documents.", "labels": [], "entities": []}, {"text": "That is, the number of other topic documents added is \u03b1 \u00d7 |TN|.", "labels": [], "entities": []}, {"text": "(2) Test set has no other topic documents.", "labels": [], "entities": []}, {"text": "This set of experiments is the traditional classification in which the training and test data have the same distribution.", "labels": [], "entities": []}, {"text": "We employ the same data sets as in (1) but without having any other topic documents in the test set.", "labels": [], "entities": []}, {"text": "Here we want to show that PU learning can do equally well without using the negative training data even in the traditional setting.", "labels": [], "entities": [{"text": "PU learning", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9203282594680786}]}], "tableCaptions": [{"text": " Table 1. Comparison of methods without other docu- ments in test set", "labels": [], "entities": []}]}