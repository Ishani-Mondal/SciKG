{"title": [{"text": "A Semi-Supervised Approach to Improve Classification of Infrequent Discourse Relations using Feature Vector Extension", "labels": [], "entities": [{"text": "Improve Classification of Infrequent Discourse Relations", "start_pos": 30, "end_pos": 86, "type": "TASK", "confidence": 0.9081764419873556}]}], "abstractContent": [{"text": "Several recent discourse parsers have employed fully-supervised machine learning approaches.", "labels": [], "entities": [{"text": "discourse parsers", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7268591225147247}]}, {"text": "These methods require human an-notators to beforehand create an extensive training corpus, which is a time-consuming and costly process.", "labels": [], "entities": []}, {"text": "On the other hand, un-labeled data is abundant and cheap to collect.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel semi-supervised method for discourse relation classification based on the analysis of co-occurring features in unlabeled data, which is then taken into account for extending the feature vectors given to a classifier.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.6898959974447886}]}, {"text": "Our experimental results on the RST Discourse Tree-bank corpus and Penn Discourse Treebank indicate that the proposed method brings a significant improvement in classification accuracy and macro-average F-score when small training datasets are used.", "labels": [], "entities": [{"text": "RST Discourse Tree-bank corpus", "start_pos": 32, "end_pos": 62, "type": "DATASET", "confidence": 0.8635666966438293}, {"text": "Penn Discourse Treebank", "start_pos": 67, "end_pos": 90, "type": "DATASET", "confidence": 0.9754202961921692}, {"text": "classification", "start_pos": 161, "end_pos": 175, "type": "TASK", "confidence": 0.9320980310440063}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9413481950759888}, {"text": "F-score", "start_pos": 203, "end_pos": 210, "type": "METRIC", "confidence": 0.9702784419059753}]}, {"text": "For instance, with training sets of c.a.", "labels": [], "entities": []}, {"text": "1000 labeled instances, the proposed method brings improvements inaccuracy and macro-average F-score up to 50% compared to a baseline classifier.", "labels": [], "entities": [{"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9346532225608826}]}, {"text": "We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with alack of annotated data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis), text summarization () and dialogue generation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.9050720036029816}, {"text": "text summarization", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7665592133998871}, {"text": "dialogue generation", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.8510076701641083}]}, {"text": "However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches.", "labels": [], "entities": [{"text": "discourse relation classifiers", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6700011591116587}]}, {"text": "Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT)) and the Penn Discourse Treebank (PDTB) (), which are both based on the Wall Street Journal (WSJ) corpus.", "labels": [], "entities": [{"text": "RST Discourse Treebank (RSTDT))", "start_pos": 59, "end_pos": 90, "type": "DATASET", "confidence": 0.8002515882253647}, {"text": "Penn Discourse Treebank (PDTB)", "start_pos": 99, "end_pos": 129, "type": "DATASET", "confidence": 0.945772796869278}, {"text": "Wall Street Journal (WSJ) corpus", "start_pos": 162, "end_pos": 194, "type": "DATASET", "confidence": 0.9112382446016584}]}, {"text": "In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations.", "labels": [], "entities": []}, {"text": "Each of these relations has furthermore several possible configurations for its arguments-its 'nuclearity'.", "labels": [], "entities": []}, {"text": "In practice, a classifier trained on these coarse-grained relations must solve a 41-class classification problem.", "labels": [], "entities": [{"text": "41-class classification", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.6143109053373337}]}, {"text": "Some of the relations corresponding to these classes are relatively more frequent in the corpus, such as the ELAB-ORATION relation (4441 instances), or the ATTRIBUTION[N] relation (1612 instances).", "labels": [], "entities": [{"text": "ELAB-ORATION", "start_pos": 109, "end_pos": 121, "type": "METRIC", "confidence": 0.9180822968482971}, {"text": "ATTRIBUTION", "start_pos": 156, "end_pos": 167, "type": "METRIC", "confidence": 0.957077145576477}]}, {"text": "However, other relation types occur very rarely, such as TOPIC-COMMENT[N] (2 instances), or EVALUATION[N] (3 instances).", "labels": [], "entities": [{"text": "TOPIC-COMMENT", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.6142294406890869}]}, {"text": "A similar phenomenon can be observed in PDTB, in which 15 level-two relations are employed: Some, such as EXPANSION.CONJUNCTION, occur as often as 8759 times throughout the corpus, whereas the remainder of the relations, such as EXPANSION.EXCEPTION and COMPARISON.PRAGMATIC CONCESSION, can appear as rarely as 17 and 12 times respectively.", "labels": [], "entities": []}, {"text": "Although supervised approaches to discourse relation learning achieve good results on frequent relations, performance is poor on rare relation types.", "labels": [], "entities": [{"text": "discourse relation learning", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7303792734940847}]}, {"text": "Nonetheless, certain infrequent relation types might be important for specific tasks.", "labels": [], "entities": []}, {"text": "For instance, capturing the RST TOPIC-COMMENT[N] and EVALUATION[N] relations can be useful for sentiment analysis.", "labels": [], "entities": [{"text": "RST TOPIC-COMMENT[N", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7464187145233154}, {"text": "EVALUATION", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.893953263759613}, {"text": "sentiment analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9714609980583191}]}, {"text": "Another situation where detection of lowoccurring relations is desirable is the case where we have only a small training set at our disposal, for instance when there is not enough annotated data for all the relation types described in a discourse theory.", "labels": [], "entities": []}, {"text": "In this case, all the dataset's relations can be considered rare, and being able to build an efficient classifier depends on the capacity to deal with this lack of annotated data.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose a semi-supervised method that exploits the abundant, freely-available unlabeled data, which is harvested for feature cooccurrence information, and used as a basis to extend feature vectors to help classification for cases where unknown features are found in test vectors.", "labels": [], "entities": []}, {"text": "\u2022 The proposed method is evaluated on the RSTDT and PDTB corpus, where it significantly improves accuracy and macro-average F-score when small training sets are used.", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.7809053063392639}, {"text": "PDTB corpus", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.8911869823932648}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9992877840995789}, {"text": "F-score", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9493927955627441}]}, {"text": "For instance, when trained on moderately small datasets with ca.", "labels": [], "entities": []}, {"text": "1000 instances, the proposed method increases the macro-average F-score and accuracy up to 50%, compared to a baseline classifier.", "labels": [], "entities": [{"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9201807379722595}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9996147155761719}]}], "datasetContent": [{"text": "The proposed method is independent of any particular classification algorithm.", "labels": [], "entities": []}, {"text": "Because our goal is strictly to evaluate the relative benefit of employing the proposed method, and not the absolute performance when used with a specific classification algorithm, we select a logistic regression classifier, for its simplicity.", "labels": [], "entities": []}, {"text": "We use the multi-class logistic regression (maximum entropy model) implemented in the Classias toolkit.", "labels": [], "entities": []}, {"text": "Regularization parameters are set to their default value of one and are fixed throughout the experiments described in the paper.", "labels": [], "entities": []}, {"text": "To create our unlabeled dataset, we use sentences extracted from the English Wikipedia 2 , as they are freely available and relatively easy to collect.", "labels": [], "entities": [{"text": "English Wikipedia 2", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.868613084157308}]}, {"text": "For further extraction of syntactic features, these sentences are automatically parsed using the Stanford parser (.", "labels": [], "entities": []}, {"text": "Then, they are segmented into elementary discourse units (EDUs) using our sequential discourse segmenter).", "labels": [], "entities": []}, {"text": "The relatively high performance of this RST segmenter, which has an F-score of 0.95 compared to that of 0.98 between human annotators, is acceptable for this task.", "labels": [], "entities": [{"text": "RST segmenter", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9489863514900208}, {"text": "F-score", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9993959665298462}]}, {"text": "We collect and parse 100000 sentences from random Wikipedia articles.", "labels": [], "entities": []}, {"text": "As there is no segmentation tool for the PDTB framework, we assume that co-occurrence information taken from EDUs created using a RST segmenter is also useful for extending feature vectors of PDTB relations.", "labels": [], "entities": []}, {"text": "Unless otherwise noted, the experiments presented in the rest of this paper are done using those 100000 unlabeled instances.", "labels": [], "entities": []}, {"text": "In the unlabeled data, any two consecutive discourse units might not always be connected by a discourse relation.", "labels": [], "entities": []}, {"text": "Therefore, we introduce an artificial NONE relation in the training set, in order to facilitate this.", "labels": [], "entities": []}, {"text": "Instances of the NONE relation are generated randomly by pairing consecutive discourse units which are not connected by a discourse relation in the training data.", "labels": [], "entities": []}, {"text": "NONE is also learnt as a separate discourse relation class by the multi-class classification algorithm.", "labels": [], "entities": [{"text": "NONE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6250818967819214}]}, {"text": "This enables us to detect discourse units between which there exist no discourse relation, thereby improving the classification accuracy for other relation types.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9863165020942688}]}, {"text": "We follow the common practice in discourse research for partitioning the discourse corpora into training and test set.", "labels": [], "entities": []}, {"text": "For the RST classifier, the dedicated training and test sets of the RSTDT are Because in the PDTB an instance can be annotated with several discourse relations simultaneously-called 'senses' in-for each instance with n senses in the corpus, we create n identical feature vectors, each being labeled by one of the instance's senses.", "labels": [], "entities": [{"text": "RST classifier", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.82123664021492}]}, {"text": "However, in the RST framework, only one relation is allowed to hold between two EDUs.", "labels": [], "entities": []}, {"text": "Consequently, each instance from the RSTDT is labeled with a single discourse relation, from which a single feature vector is created.", "labels": [], "entities": []}, {"text": "For RSTDT, we extract 25078 training vectors and 1633 test vectors.", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.7287955284118652}]}, {"text": "For PDTB we extract 49748 training vectors and 1688 test vectors.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8392053842544556}]}, {"text": "There are 41 classes (relation types) in the RSTDT relation classification task, and 29 classes in the PDTB task.", "labels": [], "entities": [{"text": "RSTDT relation classification task", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.8015984445810318}]}, {"text": "For the PDTB, we selected level-two relations, because they have better expressivity and are not too fine-grained.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.7702630162239075}]}, {"text": "We experimentally set the entropy-based feature selection parameter to N = 5000.", "labels": [], "entities": [{"text": "entropy-based feature selection", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.5892003377278646}]}, {"text": "With large N values, we must store and process large feature co-occurrence matrices.", "labels": [], "entities": []}, {"text": "For example, doubling the number of selected features, N to 10000 did not improve the classification accuracy, although it required 4GB of memory to store the feature co-occurrence matrix.", "labels": [], "entities": [{"text": "classification", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.9349514842033386}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9657222032546997}]}, {"text": "shows the number of features that occur in test data but not in labeled training data, against the number of training instances.", "labels": [], "entities": []}, {"text": "It can be seen from that, with less training data available to the classifier, we can potentially obtain more information regarding features by looking at unlabeled data.", "labels": [], "entities": []}, {"text": "However, when the training dataset's size increases, the number of features that only appear in test data decreases rapidly.", "labels": [], "entities": []}, {"text": "This inverse relation between the training dataset size and the number of features that only appear in test data can be observed in both RSTDT and PDTB datasets.", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.6656437516212463}, {"text": "PDTB datasets", "start_pos": 147, "end_pos": 160, "type": "DATASET", "confidence": 0.828793078660965}]}, {"text": "For a training set of 100 instances, there are 23580 unseen features in the case of RSTDT, and 27757 in the case of PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.9470480680465698}]}, {"text": "The number of unseen features is halved fora training set of 1800 instances in the case of RSTDT, and fora training set of 1300 instances in the case of PDTB.", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8306816816329956}, {"text": "PDTB", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.9569258093833923}]}, {"text": "Finally, when selecting all available training data, we count only 1365 unseen test features in the case of RSTDT, and 87 in the case of PDTB.", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.6459694504737854}, {"text": "PDTB", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.9437604546546936}]}, {"text": "In the following experiments, we use macroaveraged F-scores to evaluate the performance of the proposed discourse relation classifier on test data.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8068513870239258}]}, {"text": "Macro-averaged F-score is not influenced by the number of instances that exist in each relation type.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9701812267303467}]}, {"text": "It equally weights the performance on both frequent relation types and infrequent relation types.", "labels": [], "entities": []}, {"text": "Because we are interested in measuring the overall performance of a discourse relation classifier across all re- lation types we use macro-averaged F-score as the preferred evaluation metric for this task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.8983451724052429}]}, {"text": "We train a multi-class logistic regression model without extending the feature vectors as a baseline method.", "labels": [], "entities": []}, {"text": "This baseline is expected to show the effect of using the proposed feature vector extension approach for the task of discourse relation learning.", "labels": [], "entities": [{"text": "discourse relation learning", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.7403273979822794}]}, {"text": "Experimental results on RSTDT and PDTB datasets are depicted in.", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.7083249092102051}, {"text": "PDTB datasets", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9225498735904694}]}, {"text": "From these figures, we see that the proposed feature extension method outperforms the baseline for both RSTDT and PDTB datasets for the full range of training dataset sizes.", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.634975254535675}, {"text": "PDTB datasets", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.8063591420650482}]}, {"text": "However, whereas the difference of scores between the two methods is obvious for small amounts of training data, this difference progressively decreases as we increase the amount of training data.", "labels": [], "entities": []}, {"text": "Specifically, with 100 training instances, the difference between baseline and proposed method is the largest: For RSTDT, the baseline has a macro-averaged F-score of 0.084, whereas the the proposed method has a macro-averaged Fscore of 0.189 (ca. 119% increase in F-score).", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 115, "end_pos": 120, "type": "TASK", "confidence": 0.7103444337844849}, {"text": "F-score", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9628269672393799}, {"text": "Fscore", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9550671577453613}, {"text": "F-score", "start_pos": 265, "end_pos": 272, "type": "METRIC", "confidence": 0.9980254173278809}]}, {"text": "For PDTB, the baseline has an F-score of 0.016, while the proposed method has an F-score of 0.089 (459% increase).", "labels": [], "entities": [{"text": "PDTB", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7462137341499329}, {"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9996732473373413}, {"text": "F-score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9995390176773071}]}, {"text": "The difference of scores between the two methods then progressively diminishes as the number of training instances is increased, and fades beyond 10000 training instances.", "labels": [], "entities": []}, {"text": "The reason for this behavior is given by: For a small number of training instances, the number of unseen features in training data is large.", "labels": [], "entities": []}, {"text": "In this case, the feature vector extension process is comprehensive, and score can be increased by the use of unlabeled data.", "labels": [], "entities": [{"text": "score", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9869600534439087}]}, {"text": "When more training data is progressively used, the number of unseen test features sharply diminishes, which means feature vector extension becomes more limited, and the performance of the proposed method gets progressively closer to the baseline.", "labels": [], "entities": []}, {"text": "Note that we plotted PDTB performance up to 25000 training instances, as the number of unseen test features becomes so small past this point that the performances of the proposed method and baseline are identical.", "labels": [], "entities": []}, {"text": "Using all PDTB training data (49748 instances), both baseline and proposed method reach a macro-average F-score of 0.308.: F-scores for RSTDT relations, using a training set containing #Tr instances of each relation.", "labels": [], "entities": [{"text": "PDTB training data", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.8434937198956808}, {"text": "F-score", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9666795134544373}, {"text": "F-scores", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9775285124778748}, {"text": "RSTDT relations", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7728980183601379}]}, {"text": "B. indicates F-score for baseline, P.M. for the proposed method.", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9985629916191101}]}, {"text": "A boldface indicates the best classifier for each relation.", "labels": [], "entities": []}, {"text": "Although the distribution of discourse relations in RSTDT and PDTB is not uniform, it is possible to study the performance of the proposed method when all relations are made equally rare.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.8471975922584534}]}, {"text": "We evaluate performance on artificially-created training sets containing an equal amount of each discourse relation.", "labels": [], "entities": []}, {"text": "contains the F-score for each RSTDT relation, using training sets containing respectively one, two, three, five and seven instances of each relation.", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9964045286178589}, {"text": "RSTDT relation", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7323179543018341}]}, {"text": "For space considerations, only relations with significant results are shown.", "labels": [], "entities": []}, {"text": "We observe that, when using respectively one and two instances of each relation, the baseline classifier is unable to detect any relation, and has a macro-average F-score of zero.", "labels": [], "entities": [{"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9318013787269592}]}, {"text": "Contrastingly, the classifier built with feature vector extension reaches in those cases an Fscore of 0.06.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9990507960319519}]}, {"text": "Furthermore, when employing the proposed method, certain relations have relatively high F-scores even with very little labeled data: With one training instance, ATTRIBUTION[N] has an F-score of 0.597, while SUMMARY[N] has an Fscore of 0.429.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9951979517936707}, {"text": "ATTRIBUTION", "start_pos": 161, "end_pos": 172, "type": "METRIC", "confidence": 0.9949941039085388}, {"text": "F-score", "start_pos": 183, "end_pos": 190, "type": "METRIC", "confidence": 0.995519757270813}, {"text": "Fscore", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.9986675977706909}]}, {"text": "With three training instances, EN-ABLEMENT[N] has an F-score of 0.579.", "labels": [], "entities": [{"text": "EN-ABLEMENT", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.7991456389427185}, {"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9996563196182251}]}, {"text": "When the amount of each relation is increased, the baseline classifier starts detecting more relations.", "labels": [], "entities": []}, {"text": "In all cases, the proposed method performs better in terms of accuracy and macro-average F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9996507167816162}, {"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9381507039070129}]}, {"text": "With a training set containing seven instances of each relation, the baseline's macro-average F-score is starting to get closer to the extended classifier's, with superior performances for several relations, such as COM-, and TEMPO-RAL[N].", "labels": [], "entities": [{"text": "F-score", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9268202185630798}, {"text": "TEMPO-RAL", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.908805251121521}]}, {"text": "Still, in this case, the extended classifier's accuracy is higher than the baseline (0.213 versus 0.122).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9994271993637085}]}, {"text": "summarizes the outcome of the same experiments performed on the PDTB dataset.", "labels": [], "entities": [{"text": "PDTB dataset", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9846610724925995}]}, {"text": "The results exhibit a similar trend, despite the baseline classifier having a relatively high accuracy for each case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9986624717712402}]}, {"text": "Using the data from, it is possible to calculate the relative score change occurring when using the proposed method, as a function of the number of unseen features found in test data.", "labels": [], "entities": [{"text": "relative score change occurring", "start_pos": 53, "end_pos": 84, "type": "METRIC", "confidence": 0.857454925775528}]}, {"text": "This graph is plotted in.", "labels": [], "entities": []}, {"text": "Besides macro-average F-score, we additionally plot accuracy change.", "labels": [], "entities": [{"text": "F-score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.977816104888916}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9808745384216309}]}, {"text": "In the top subfigure, representing the case of RSTDT, we see that, for the lowest amount of unseen test features, the proposed method does #Tr = 1 #Tr = 2 #Tr = 3 #Tr = 5 #Tr = 7 Relation name -0.056   not bring any change in F-score or accuracy.", "labels": [], "entities": [{"text": "Tr", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9668456315994263}, {"text": "Relation name -0.056", "start_pos": 179, "end_pos": 199, "type": "METRIC", "confidence": 0.9366575628519058}, {"text": "F-score", "start_pos": 226, "end_pos": 233, "type": "METRIC", "confidence": 0.9949560761451721}, {"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9793177843093872}]}, {"text": "Indeed, as the number of unknown features is low, feature vector extension is very limited, and does not improve the performance compared to the baseline.", "labels": [], "entities": []}, {"text": "Then, a progressive increase of both accuracy and macro-average F-score is observed, as the number of unseen test features is incremented.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995239973068237}, {"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9609411954879761}]}, {"text": "For instance, for 8500 unseen test features, the macroaverage F-score increase (resp. accuracy increase) is 25% (resp. 2.5%), while it is 20% (resp. 1%) for 11000 unseen test instances.", "labels": [], "entities": [{"text": "macroaverage F-score increase (resp. accuracy increase)", "start_pos": 49, "end_pos": 104, "type": "METRIC", "confidence": 0.7458201125264168}]}, {"text": "These values reach a maximum of 119% macro-average F-score increase, and 66% accuracy increase, when 23500 features unseen during training are present in test data.", "labels": [], "entities": [{"text": "F-score increase", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.971542626619339}, {"text": "accuracy increase", "start_pos": 77, "end_pos": 94, "type": "METRIC", "confidence": 0.9927856028079987}]}, {"text": "This situation corresponds in to the case of very small training sets.", "labels": [], "entities": []}, {"text": "The bottom subfigure of, for the case of PDTB, reveals a similar tendency.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.810658872127533}]}, {"text": "The macro-average F-score increase (resp. accuracy increase) is negligible for 1000 unseen test features, while this increase is 21% for both macro-average F-score and accuracy in the case of 9700 unseen test features, and 459% (resp.", "labels": [], "entities": [{"text": "F-score increase", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.9583444595336914}, {"text": "accuracy increase)", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.9629802505175272}, {"text": "F-score", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9111603498458862}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.991696298122406}]}, {"text": "630% for accuracy) when 28000 unseen features are found in test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9995126724243164}]}, {"text": "This shows that the proposed method is useful when large numbers of features are missing from the training set, which corresponds in practice to small training sets, with few training instances for each relation type.", "labels": [], "entities": []}, {"text": "For large training sets, most features are encountered by the classifier during training, and feature vector extension does not bring useful information.", "labels": [], "entities": []}, {"text": "We empirically evaluate the effect of using different amounts of unlabeled data on the performance of the proposed method.", "labels": [], "entities": []}, {"text": "We use respectively 100 and 10000 labeled training instances, create feature cooccurrence matrices with different amounts of unlabeled data, and evaluate the performance in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 173, "end_pos": 196, "type": "TASK", "confidence": 0.9204655885696411}]}, {"text": "Experimental results for RSTDT are illustrated in (top).", "labels": [], "entities": [{"text": "RSTDT", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.9165927171707153}]}, {"text": "From it appears clearly that macro-average F-scores improve with increased number of unlabeled instances.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9298586249351501}]}, {"text": "However, the benefit of using larger amounts of unlabeled data is more pronounced when only a small number of labeled training instances are employed (ca. 100).", "labels": [], "entities": []}, {"text": "In fact, with 100 labeled training instances, the maximum improvement in F-score is 119% (corresponds to using all our 100000 unlabeled instances).", "labels": [], "entities": [{"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9992188215255737}]}, {"text": "However, the maximum improvement in F-score with 10000 labeled training instances is small, only 2.5% (corresponds to 10000 unlabeled instances).", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9970767498016357}]}, {"text": "The effect of using unlabeled data on PDTB relation classification is illustrated in (bottom).", "labels": [], "entities": [{"text": "PDTB relation classification", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.709037979443868}]}, {"text": "Similarly, we consecutively set the labeled training dataset size to 100 and 10000 instances, and plot the macro-average F-score against the unlabeled dataset size.", "labels": [], "entities": [{"text": "F-score", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9425413012504578}]}, {"text": "As in the RSTDT experiment, the benefit of us- ing unlabeled data is more obvious when the number of labeled training instances is small.", "labels": [], "entities": []}, {"text": "In particular, with 100 training instances, the maximum improvement in F-score is 459% (corresponds to 100000 unlabeled instances).", "labels": [], "entities": [{"text": "F-score", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9994505047798157}]}, {"text": "However, with 10000 labeled training instances the maximum improvement in F-score is 15% (corresponds to 100 unlabeled instances).", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.999305248260498}]}, {"text": "These results confirm that, on the one hand performance improvement is more prominent for smaller training sets, and that on the other hand, performance is increased when using larger amounts of unlabeled data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F-scores for RSTDT relations, using a training set containing #Tr instances of each relation. B. indicates  F-score for baseline, P.M. for the proposed method. A boldface indicates the best classifier for each relation.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9643623232841492}, {"text": "RSTDT relations", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.8868997097015381}, {"text": "F-score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9823240041732788}]}, {"text": " Table 2: F-scores for PDTB relations.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9885727167129517}, {"text": "PDTB relations", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.49527569115161896}]}]}