{"title": [{"text": "Fast Easy Unsupervised Domain Adaptation with Marginalized Structured Dropout", "labels": [], "entities": [{"text": "Unsupervised Domain Adaptation", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.6707558631896973}]}], "abstractContent": [{"text": "Unsupervised domain adaptation often relies on transforming the instance representation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7756046056747437}]}, {"text": "However, most such approaches are designed for bag-of-words models, and ignore the structured features present in many problems in NLP.", "labels": [], "entities": []}, {"text": "We propose anew technique called marginalized struc-tured dropout, which exploits feature structure to obtain a remarkably simple and efficient feature projection.", "labels": [], "entities": []}, {"text": "Applied to the task of fine-grained part-of-speech tagging on a dataset of historical Por-tuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-of-magnitude over previous work.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6904933154582977}, {"text": "Por-tuguese", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.9105125069618225}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9985326528549194}, {"text": "speed", "start_pos": 181, "end_pos": 186, "type": "METRIC", "confidence": 0.9879961609840393}]}], "introductionContent": [{"text": "Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations.", "labels": [], "entities": [{"text": "Unsupervised domain adaptation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.761786142985026}, {"text": "natural language processing", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.674531489610672}]}, {"text": "This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles, and as there is increasing interest in natural language processing for historical texts).", "labels": [], "entities": []}, {"text": "While a number of different approaches for domain adaptation have been proposed, they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7322830557823181}, {"text": "sentiment analysis", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9014462828636169}]}, {"text": "Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.6099950969219208}]}, {"text": "As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces.", "labels": [], "entities": []}, {"text": "We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances ().", "labels": [], "entities": []}, {"text": "By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains.", "labels": [], "entities": []}, {"text": "showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (.", "labels": [], "entities": []}, {"text": "While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 10 5 or more features.", "labels": [], "entities": []}, {"text": "In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP.", "labels": [], "entities": []}, {"text": "For example, in part-of-speech tagging, define several feature \"templates\": the current word, the previous word, the suffix of the current word, and soon.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7097671926021576}]}, {"text": "For each feature template, there are thousands of binary features.", "labels": [], "entities": []}, {"text": "To exploit this structure, we propose two alternative noising techniques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template.", "labels": [], "entities": []}, {"text": "We show how it is possible to marginalize over both types of noise, and find that the solution for structured dropout is substantially simpler and more efficient than the mDA approach of, which does not consider feature structure.", "labels": [], "entities": []}, {"text": "We apply these ideas to fine-grained part-ofspeech tagging on a dataset of Portuguese texts from the years 1502 to 1836, training on recent texts and evaluating on older documents.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7266567796468735}]}, {"text": "Both structure-aware domain adaptation algorithms perform as well as standard dropout -and better than the wellknown structural correspondence learning (SCL) algorithm -but structured dropout is more than an order-of-magnitude faster.", "labels": [], "entities": [{"text": "structure-aware domain adaptation", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.6433129807313284}, {"text": "structural correspondence learning (SCL)", "start_pos": 117, "end_pos": 157, "type": "TASK", "confidence": 0.6645142287015915}]}, {"text": "As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts.", "labels": [], "entities": [{"text": "syntactic analysis of historical texts", "start_pos": 117, "end_pos": 155, "type": "TASK", "confidence": 0.8258600950241088}]}], "datasetContent": [{"text": "We compare these methods on historical Portuguese part-of-speech tagging, creating domains over historical epochs.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7015531659126282}]}, {"text": "Datasets We use the Tycho Brahe corpus to evaluate our methods.", "labels": [], "entities": [{"text": "Tycho Brahe corpus", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.8993425965309143}]}, {"text": "The corpus contains a total of 1,480,528 manually tagged words.", "labels": [], "entities": []}, {"text": "It uses a set of 383 tags and is composed of various texts from historical Portuguese, from 1502 to 1836.", "labels": [], "entities": []}, {"text": "We divide the texts into fifty-year periods to create different domains.", "labels": [], "entities": []}, {"text": "presents some statistics of the datasets.", "labels": [], "entities": []}, {"text": "We holdout 5% of data as development data to tune parameters.", "labels": [], "entities": []}, {"text": "The two most recent domains (1800-1849 and 1750-1849) are treated as source domains, and the other domains are target domains.", "labels": [], "entities": []}, {"text": "This scenario is motivated by training a tagger on a modern newstext corpus and applying it to historical documents.", "labels": [], "entities": []}, {"text": ", we consider pivot features that appear more than 50 times in all the domains.", "labels": [], "entities": []}, {"text": "This leads to a total of 1572 pivot features in our experiments.", "labels": [], "entities": []}, {"text": "Methods We compare mDA with three alternative approaches.", "labels": [], "entities": []}, {"text": "We refer to baseline as training a CRF tagger on the source domain and testing on the target domain with only base features.", "labels": [], "entities": []}, {"text": "We also include PCA to project the entire dataset onto a low-dimensional sub-space (while still including the original features).", "labels": [], "entities": []}, {"text": "Finally, we compare against Structural Correspondence Learning (SCL;), another feature learning algorithm.", "labels": [], "entities": []}, {"text": "In all cases, we include the entire dataset to compute the feature projections; we also conducted experiments using only the test and training data for feature projections, with very similar results.", "labels": [], "entities": []}, {"text": "Parameters All the hyper-parameters are decided with our development data on the training set.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9803169965744019}]}, {"text": "We try different low dimension K from 10 to 2000 for PCA.", "labels": [], "entities": []}, {"text": "Following Blitzer (2008) we perform feature centering/normalization, as well as rescaling for SCL.", "labels": [], "entities": [{"text": "feature centering/normalization", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.674558162689209}]}, {"text": "The best parameters for SCL are dimensionality K = 25 and rescale factor \u03b1 = 5, which are the same as in the original paper.", "labels": [], "entities": [{"text": "SCL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9856215119361877}, {"text": "dimensionality K", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.9034033715724945}, {"text": "rescale factor \u03b1", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.9856330752372742}]}, {"text": "For mDA, the best corruption level is p = 0.9 for dropout noise, and p = 0.1 for scrambling noise.", "labels": [], "entities": []}, {"text": "Structured dropout noise has no free hyperparameters.", "labels": [], "entities": []}, {"text": "presents results for different domain adaptation tasks.", "labels": [], "entities": [{"text": "domain adaptation tasks", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7972701986630758}]}, {"text": "We also compute the transfer ratio, which is defined as adaptation accuracy baseline accuracy , shown in.", "labels": [], "entities": [{"text": "adaptation accuracy baseline accuracy", "start_pos": 56, "end_pos": 93, "type": "METRIC", "confidence": 0.7515303194522858}]}, {"text": "The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the Tycho Brahe Corpus", "labels": [], "entities": [{"text": "Tycho Brahe", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9075330197811127}]}, {"text": " Table 3. The scram- bling noise is most time-consuming, with cost  dominated by a matrix multiplication.", "labels": [], "entities": []}, {"text": " Table 3: Time, in seconds, to compute the feature  transformation", "labels": [], "entities": []}, {"text": " Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981076717376709}]}]}