{"title": [{"text": "Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation", "labels": [], "entities": [{"text": "Scoring Coreference Partitions of Predicted Mentions", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8034919997056326}]}], "abstractContent": [{"text": "The definitions of two coreference scoring metrics-B 3 and CEAF-are underspeci-fied with respect to predicted, as opposed to key (or gold) mentions.", "labels": [], "entities": []}, {"text": "Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping.", "labels": [], "entities": []}, {"text": "On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions.", "labels": [], "entities": [{"text": "BLANC", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.6914666891098022}]}, {"text": "In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially harmful as it could produce unin-tuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation.", "labels": [], "entities": [{"text": "CoNLL-2011/2012 shared task", "start_pos": 413, "end_pos": 440, "type": "DATASET", "confidence": 0.865058696269989}]}, {"text": "This will help the community accurately measure and compare new end-to-end corefer-ence resolution algorithms.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7186020463705063}]}], "introductionContent": [{"text": "Coreference resolution is a key task in natural language processing aiming to detect the referential expressions (mentions) in a text that point to the same entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.920245885848999}]}, {"text": "Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora-MUC) and ACE ().", "labels": [], "entities": [{"text": "coreference", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9645622372627258}, {"text": "ACE", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.6018094420433044}]}, {"text": "Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used.", "labels": [], "entities": []}, {"text": "Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference.", "labels": [], "entities": []}, {"text": "This has been expounded in.", "labels": [], "entities": []}, {"text": "The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference-The OntoNotes corpus) in the general domain, as well as the i2b2 () and THYME () corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative.", "labels": [], "entities": [{"text": "coreference-The", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.8850050568580627}, {"text": "OntoNotes corpus", "start_pos": 132, "end_pos": 148, "type": "DATASET", "confidence": 0.7544032335281372}]}, {"text": "The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.9712871015071869}, {"text": "OntoNotes corpus", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.8928247690200806}]}, {"text": "Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures:).", "labels": [], "entities": []}, {"text": "The arithmetic mean of the first three was the task's final score.", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9500863254070282}]}, {"text": "An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics-B 3 and CEAFused by the community to handle predicted mentions; (ii) a buggy implementation of the proposal that tried to reconcile these variations; and (iii) the erroneous computation of the BLANC metric for partitions of predicted mentions.", "labels": [], "entities": [{"text": "BLANC", "start_pos": 325, "end_pos": 330, "type": "METRIC", "confidence": 0.8519800901412964}]}, {"text": "Different interpretations as to how to compute B and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions-which is usually the caseled to variations of these metrics that manipulate the gold standard and system output in order to get a one-to-one mention mapping (.", "labels": [], "entities": [{"text": "B", "start_pos": 47, "end_pos": 48, "type": "METRIC", "confidence": 0.9911324381828308}, {"text": "CEAF", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7619558572769165}]}, {"text": "Some of these variations arguably produce rather unintuitive results, while others are not faithful to the original measures.", "labels": [], "entities": []}, {"text": "In this paper, we address the issues in scoring coreference partitions of predicted mentions.", "labels": [], "entities": []}, {"text": "Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is unnecessary and could in fact produce unintuitive results.", "labels": [], "entities": []}, {"text": "We demonstrate the use of our recent extension of BLANC that can seamlessly handle predicted mentions ().", "labels": [], "entities": [{"text": "BLANC", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7904366850852966}]}, {"text": "We make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures that do not involve mention manipulation and is faithful to the original intentions of the proposers of these metrics.", "labels": [], "entities": []}, {"text": "We republish the CoNLL-2011/2012 results based on this scorer, so that future systems can use it for evaluation and have the CoNLL results available for comparison.", "labels": [], "entities": [{"text": "CoNLL-2011/2012 results", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.9239102602005005}, {"text": "CoNLL", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.8754037618637085}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of the variations of the existing measures.", "labels": [], "entities": []}, {"text": "We present our newly updated coreference scoring package in Section 3 together with the rescored CoNLL-2011/2012 outputs.", "labels": [], "entities": [{"text": "coreference scoring", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7527836561203003}, {"text": "CoNLL-2011/2012 outputs", "start_pos": 97, "end_pos": 120, "type": "DATASET", "confidence": 0.866017535328865}]}, {"text": "Section 4 walks through a scoring example for all the measures, and we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance on the official, closed track  in percentages using all predicted information for  the CoNLL-2011 and 2012 shared tasks.", "labels": [], "entities": [{"text": "CoNLL-2011", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.8807898163795471}]}]}