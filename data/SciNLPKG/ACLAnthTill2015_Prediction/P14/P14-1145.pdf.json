{"title": [{"text": "ConnotationWordNet: Learning Connotation over the Word+Sense Network", "labels": [], "entities": [{"text": "Word+Sense Network", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.886785477399826}]}], "abstractContent": [{"text": "We introduce ConnotationWordNet, a connotation lexicon over the network of words in conjunction with senses.", "labels": [], "entities": []}, {"text": "We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields, and present a loopy belief propagation algorithm for inference.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7478675842285156}]}, {"text": "The key aspect of our method is that it is the first unified approach that assigns the polarity of both word-and sense-level connotations, exploiting the innate bipar-tite graph structure encoded in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 199, "end_pos": 206, "type": "DATASET", "confidence": 0.9509526491165161}]}, {"text": "We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons.", "labels": [], "entities": []}], "introductionContent": [{"text": "We introduce ConnotationWordNet, a connotation lexicon over the network of words in conjunction with senses, as defined in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9653246998786926}]}, {"text": "A connotation lexicon, as introduced first by, aims to encompass subtle shades of sentiment a word may conjure, even for seemingly objective words such as \"sculpture\", \"Ph.D.\", \"rosettes\".", "labels": [], "entities": []}, {"text": "Understanding the rich and complex layers of connotation remains to be a challenging task.", "labels": [], "entities": []}, {"text": "As a starting point, we study a more feasible task of learning the polarity of connotation.", "labels": [], "entities": []}, {"text": "For non-polysemous words, which constitute a significant portion of English vocabulary, learning the general connotation at the word-level (rather than at the sense-level) would be a natural operational choice.", "labels": [], "entities": []}, {"text": "However, for polysemous words, which correspond to most frequently used words, it would bean overly crude assumption that the same connotative polarity should be assigned for all senses of a given word.", "labels": [], "entities": []}, {"text": "For example, consider \"abound\", for which lexicographers of WordNet prescribe two different senses: \u2022 (v) abound: (be abundant of plentiful; exist in large quantities) \u2022 (v) abound, burst, bristle: (be in a state of movement or action) \"The room abounded with screaming children\"; \"The garden bristled with toddlers\" For the first sense, which is the most commonly used sense for \"abound\", the general overtone of the connotation would seem positive.", "labels": [], "entities": []}, {"text": "That is, although one can use this sense in both positive and negative contexts, this sense of \"abound\" seems to collocate more often with items that are good to be abundant (e.g., \"resources\"), than unfortunate items being abundant (e.g., \"complaints\").", "labels": [], "entities": []}, {"text": "However, as for the second sense, for which \"burst\" and \"bristle\" can be used interchangeably with respect to this particular sense, 1 the general overtone is slightly more negative with a touch of unpleasantness, or at least not as positive as that of the first sense.", "labels": [], "entities": []}, {"text": "Especially if we lookup the WordNet entry for \"bristle\", there are noticeably more negatively connotative words involved in its gloss and examples.", "labels": [], "entities": [{"text": "WordNet entry", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9478127062320709}]}, {"text": "This word sense issue has been a universal challenge fora range of Natural Language Processing applications, including sentiment analysis.", "labels": [], "entities": [{"text": "word sense", "start_pos": 5, "end_pos": 15, "type": "TASK", "confidence": 0.7000649720430374}, {"text": "sentiment analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.9702353179454803}]}, {"text": "Recent studies have shown that it is fruitful to tease out subjectivity and objectivity corresponding to different senses of the same word, in order to improve computational approaches to sentiment analysis (e.g.,).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.9545973837375641}]}, {"text": "Encouraged by these recent successes, in this study, we investigate if we can attain similar gains if we model the connotative polarity of senses separately.", "labels": [], "entities": []}, {"text": "There is one potential practical issue we would like to point out in building a sense-level lexical resource, however.", "labels": [], "entities": []}, {"text": "End-users of such a lexicon may not wish to deal with Word Sense Disam-biguation (WSD), which is known to be often too noisy to be incorporated into the pipeline with respect to other NLP tasks.", "labels": [], "entities": [{"text": "Word Sense Disam-biguation (WSD)", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.49598586062590283}]}, {"text": "As a result, researchers often would need to aggregate labels across different senses to derive the word-level label.", "labels": [], "entities": []}, {"text": "Although such aggregation is not entirely unreasonable, it does not seem to be the most optimal and principled way of integrating available resources.", "labels": [], "entities": []}, {"text": "Therefore, in this work, we present the first unified approach that learns both sense-and wordlevel connotations simultaneously.", "labels": [], "entities": []}, {"text": "This way, endusers will have access to more accurate sense-level connotation labels if needed, while also having access to more general word-level connotation labels.", "labels": [], "entities": []}, {"text": "We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields (pairwise-MRF) and derive a loopy belief propagation algorithm for inference.", "labels": [], "entities": []}, {"text": "The key aspect of our approach is that we exploit the innate bipartite graph structure between words and senses encoded in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9419145584106445}]}, {"text": "Although our approach seems conceptually natural, previous approaches, to our best knowledge, have not directly exploited these relations between words and senses for the purpose of deriving lexical knowledge over words and senses collectively.", "labels": [], "entities": []}, {"text": "In addition, previous studies (for both sentiment and connotation lexicons) aimed to produce only either of the two aspects of the polarity: word-level or sense-level, while we address both.", "labels": [], "entities": []}, {"text": "Another contribution of our work is the introduction of loopy belief propagation (loopy-BP) as a lexicon induction algorithm.", "labels": [], "entities": [{"text": "loopy belief propagation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6910899678866068}]}, {"text": "Loopy-BP in our study achieves statistically significantly better performance over the constraint optimization approaches previously explored.", "labels": [], "entities": []}, {"text": "In addition, it runs much faster and it is considerably easier to implement.", "labels": [], "entities": []}, {"text": "Last but not least, by using probabilistic representation of pairwise-MRF in conjunction with Loopy-BP as inference, the resulting solution has the natural interpretation as the intensity of connotation.", "labels": [], "entities": []}, {"text": "This contrasts to approaches that seek discrete solutions such as Integer Linear Programming(.", "labels": [], "entities": []}, {"text": "ConnotationWordNet, the final outcome of our study, is anew lexical resource that has connotation labels over both words and senses following the structure of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 159, "end_pos": 166, "type": "DATASET", "confidence": 0.9655640721321106}]}, {"text": "The lexicon is publicly available at: http://www.cs.sunysb.", "labels": [], "entities": []}, {"text": "edu/ \u02dc junkang/connotation_wordnet.)", "labels": [], "entities": []}, {"text": "In what follows, we will first describe the net- work of words and senses (Section 2), then introduce the representation of the network structure as pairwise Markov Random Fields, and a loopy belief propagation algorithm as collective inference (Section 3).", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.7270003855228424}]}, {"text": "We then present comprehensive evaluation (Section 4 & 5 & 6), followed by related work (Section 7) and conclusion (Section 8).", "labels": [], "entities": [{"text": "conclusion", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9610621333122253}]}], "datasetContent": [{"text": "ConnotationWordNet is expected to be the superset of a sentiment lexicon, as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity.", "labels": [], "entities": []}, {"text": "Thus, we use two conventional sentiment lexicons, General Inquirer (GENINQ) () and MPQA (), as surrogates to measure the performance of our inference algorithm.", "labels": [], "entities": [{"text": "General Inquirer (GENINQ)", "start_pos": 50, "end_pos": 75, "type": "METRIC", "confidence": 0.6166121244430542}]}, {"text": "In this section, we present the result of human evaluation we executed using Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.9164686799049377}]}, {"text": "We collect two separate sets of labels: a set of labels at the word-level, and another set at the sense-level.", "labels": [], "entities": []}, {"text": "We first describe the labeling process of sense-level connotation: We selected 350 polysemous words and one of their senses, and each Turker was asked to rate the connotative polarity of a given word (or of a given sense), from -5 to 5, 0 being the neutral.", "labels": [], "entities": []}, {"text": "For each word, we asked 5 Turkers to rate and we took the average of the 5 ratings as the connotative intensity score of the word.", "labels": [], "entities": []}, {"text": "We labeled a word as negative if its intensity score is less than 0 and positive otherwise.", "labels": [], "entities": [{"text": "intensity score", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.9562568366527557}]}, {"text": "For word-level labels we apply similar procedure as above.", "labels": [], "entities": []}, {"text": "Note that for > 0.25, compatibilities of \u03c8 t 2 in are reversed, hence the maximum of 0.24.", "labels": [], "entities": []}, {"text": "Because senses in WordNet can be tricky to understand, care should betaken in designing the task so that the Turkers will focus only on the corresponding sense of a word.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9493541121482849}]}, {"text": "Therefore, we provided the part of speech tag, the WordNet gloss of the selected sense, and a few examples as given in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9367276430130005}, {"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9786665439605713}]}, {"text": "As an incentive, each Turker was rewarded $0.07 per hit which consists of 10 words to label.: Word-/Sense-level evaluation results  We first evaluate the word-level assignment of connotation, as shown in.", "labels": [], "entities": []}, {"text": "The agreement between the new lexicon and human judges varies between 84% and 86.98%.", "labels": [], "entities": []}, {"text": "Sentiment lexicons such as SentiWordNet () and OpinionFinder () show low agreement rate with human, which is somewhat as expected: human judges in this study are labeling for subtle connotation, not for more explicit sentiment.", "labels": [], "entities": []}, {"text": "OpinionFinder's low agreement rate was mainly due to the low hit rate of the words (successful look-up rate, 33.43%).", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.7895086109638214}, {"text": "successful look-up rate", "start_pos": 84, "end_pos": 107, "type": "METRIC", "confidence": 0.8367671569188436}]}, {"text": "Feng2013 is the lexicon presented in) and it showed a relatively higher 72.13% hit rate.", "labels": [], "entities": [{"text": "Feng2013", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9581318497657776}, {"text": "72.13% hit rate", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.9104146361351013}]}, {"text": "Note that belief propagation was run until 95% and 99% of the nodes were converged in their beliefs.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7506859004497528}]}, {"text": "In addition, the seed words with known connotation labels originally consist of 20 positive and 20 negative predicates.", "labels": [], "entities": []}, {"text": "We also extended the seed set with the sentiment lexicon words and denote these runs with E-for 'Extended'.", "labels": [], "entities": []}, {"text": "We also examined the agreement rates on the sense-level.", "labels": [], "entities": []}, {"text": "Since OpinionFinder and Feng2013 do not provide the polarity scores at the senselevel, we excluded them from this evaluation.", "labels": [], "entities": [{"text": "Feng2013", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9300605058670044}]}, {"text": "Because sense-level polarity assignment is a harder (more subtle) task, the performance of all lexicons decreased to some degree in comparison to that of word-level evaluations.", "labels": [], "entities": [{"text": "sense-level polarity assignment", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6622650325298309}]}, {"text": "Finally, to show the utility of the resulting lexicon in the context of a concrete sentiment analysis task, we perform lexicon-based sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.7968040307362875}, {"text": "lexicon-based sentiment analysis", "start_pos": 119, "end_pos": 151, "type": "TASK", "confidence": 0.6555374463399252}]}, {"text": "We experiment with SemEval dataset) that includes the human labeled dataset for predicting whether a news headline is a good news or a bad news, which we expect to have a correlation with the use of connotative words that we focus on in this paper.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.662124365568161}]}, {"text": "The good/bad news are annotated with scores (ranging from -100 to 87).", "labels": [], "entities": []}, {"text": "We construct several data sets by applying different thresholds on scores.", "labels": [], "entities": []}, {"text": "For example, with the threshold set to 60, we discard the instances whose scores lie between -60 and 60.", "labels": [], "entities": []}, {"text": "For comparison, we also test the connotation lexicon from) and the combined sentiment lexicon GENINQ+MPQA.", "labels": [], "entities": [{"text": "GENINQ", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.7791641354560852}]}, {"text": "Note that there is a difference in how humans judge the orientation and the degree of connotation fora given word out of context, and how the use of such words in context can be perceived as good/bad news.", "labels": [], "entities": []}, {"text": "In particular, we conjecture that humans may have a bias toward the use of positive words, which in turn requires calibration from the readers' minds.", "labels": [], "entities": []}, {"text": "That is, we might need to tone down the level of positiveness in order to correctly measure the actual intended positiveness of the message.", "labels": [], "entities": []}, {"text": "With this in mind, we tune the appropriate calibration from a small training data, by using 1 fold from N fold cross validation, and using the remaining N \u2212 1 folds as testing.", "labels": [], "entities": []}, {"text": "We simply learn the mixture coefficient \u03bb to scale the contribution of positive and negative connotation values.", "labels": [], "entities": []}, {"text": "We tune this parameter \u03bb 8 for other lexicons we compare against as well.", "labels": [], "entities": []}, {"text": "Note that due to this parameter learning, we are able to report better performance for the connotation lexicon of than what the authors have reported in their paper (labeled with *) in. shows the results for N =15, where the new lexicon consistently outperforms other competitive lexicons.", "labels": [], "entities": []}, {"text": "In addition, shows that the performance does not change much based on the size of training data used for parameter tuning (N ={5, 10, 15, 20}).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Connotation inference performance on  various graphs. '-W' indicates weighted versions  (see  \u00a74.1). P: precision, R: recall, F: F1-score (%).", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9832268357276917}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.966183066368103}, {"text": "F1-score", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9414678812026978}]}, {"text": " Table 3: Word-/Sense-level evaluation results", "labels": [], "entities": []}, {"text": " Table 4: Results of pair-wise intensity evaluation,  for intensity difference threshold = 2.0", "labels": [], "entities": [{"text": "intensity difference threshold", "start_pos": 58, "end_pos": 88, "type": "METRIC", "confidence": 0.9647913773854574}]}, {"text": " Table 4.  Despite that intensity is generally a harder prop- erty to measure (than the coarser binary catego- rization of polarities), our connotation lexicons  perform surprisingly well, reaching up to 74.83%  accuracy. Further study on the incorrect cases re- veals that SentiWordNet has many pair of words  with the same polarity score (23.34%). Such cases  seems to be due to the limited score patterns of  SentiWordNet. The ratio of such cases are ac- counted as Undecided in", "labels": [], "entities": [{"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9985238909721375}, {"text": "ac- counted", "start_pos": 454, "end_pos": 465, "type": "METRIC", "confidence": 0.9586319724718729}]}, {"text": " Table 5: SemEval evaluation results, for N =15", "labels": [], "entities": [{"text": "SemEval evaluation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8870136737823486}]}]}