{"title": [{"text": "Learning to Automatically Solve Algebra Word Problems", "labels": [], "entities": [{"text": "Learning to Automatically Solve Algebra Word Problems", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7273930396352496}]}], "abstractContent": [{"text": "We present an approach for automatically learning to solve algebra word problems.", "labels": [], "entities": []}, {"text": "Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text.", "labels": [], "entities": []}, {"text": "The learning algorithm uses varied supervision, including either full equations or just the final answers.", "labels": [], "entities": []}, {"text": "We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset.", "labels": [], "entities": []}, {"text": "This is, to our knowledge, the first learning result for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Algebra word problems concisely describe a world state and pose questions about it.", "labels": [], "entities": []}, {"text": "The described state can be modeled with a system of equations whose solution specifies the questions' answers.", "labels": [], "entities": []}, {"text": "For example, shows one such problem.", "labels": [], "entities": []}, {"text": "The reader is asked to infer how many children and adults were admitted to an amusement park, based on constraints provided by ticket prices and overall sales.", "labels": [], "entities": []}, {"text": "This paper studies the task of learning to automatically solve such problems given only the natural language.", "labels": [], "entities": []}, {"text": "Solving these problems requires reasoning across sentence boundaries to find a system of equations that concisely models the described semantic relationships.", "labels": [], "entities": []}, {"text": "For example, in, the total ticket revenue computation in the second equation summarizes facts about ticket prices and total sales described in the second, third, and fifth The code and data for this work are available at http://groups.csail.mit.edu/rbg/code/ wordprobs/.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset We collected anew dataset of algebra word problems from Algebra.com, a crowdsourced tutoring website.", "labels": [], "entities": []}, {"text": "The questions were posted by students for members of the community to respond with solutions.", "labels": [], "entities": []}, {"text": "Therefore, the problems are highly varied, and are taken from real problems given to students.", "labels": [], "entities": []}, {"text": "We heuristically filtered the data to get only linear algebra questions which did not require any explicit background knowledge.", "labels": [], "entities": []}, {"text": "From these we randomly chose a set of 1024 questions.", "labels": [], "entities": []}, {"text": "As the questions are posted to a web forum, the posts often contained additional comments which were not part of the word problems and the solutions are embedded in long freeform natural language descriptions.", "labels": [], "entities": []}, {"text": "To clean the data we asked Amazon Mechanical Turk workers to extract from the text: the algebra word problem itself, the solution equations, and the numeric answer.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.9304398695627848}]}, {"text": "We manually verified both the equations and the numbers to ensure they were correct.", "labels": [], "entities": []}, {"text": "To ensure each problem type is seen at least a few times in the training data, we removed the infrequent problem types.", "labels": [], "entities": []}, {"text": "Specifically, we induced the system template from each equation system, as described in Section 4.1, and removed all problems for which the associated system template appeared less than 6 times in the dataset.", "labels": [], "entities": []}, {"text": "This left us with 514 problems.", "labels": [], "entities": []}, {"text": "Forms of Supervision We consider both semisupervised and supervised learning.", "labels": [], "entities": [{"text": "Supervision", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9636038541793823}]}, {"text": "In the semisupervised scenario, we assume access to the numerical answers of all problems in the training corpus and to a small number of problems paired with full equation systems.", "labels": [], "entities": []}, {"text": "To select which problems to annotate with equations, we identified the five most common types of questions in the data and annotated a randomly sampled question of each type.", "labels": [], "entities": []}, {"text": "5EQ+ANS uses this form of weak supervision.", "labels": [], "entities": [{"text": "ANS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7591184973716736}]}, {"text": "To show the benefit of using the weakly supervised data, we also provide results fora baseline scenario 5EQ, where the training data includes only the five seed questions annotated with equation systems.", "labels": [], "entities": []}, {"text": "In the fully supervised scenario ALLEQ, we assume access to full equation systems for the entire training set.", "labels": [], "entities": [{"text": "ALLEQ", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9622171521186829}]}, {"text": "Evaluation Protocol We run all our experiments using 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "Since our model generates a solution for every problem, we report only accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9990646243095398}]}, {"text": "We report two metrics: equation accuracy to measure how often the system generates the correct equation system, and answer accuracy to evaluate how often the generated numerical answer is correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.967473566532135}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.5922850370407104}]}, {"text": "When comparing equations, we avoid spurious differences by canonicalizing the equation system, as described in Section 6.", "labels": [], "entities": []}, {"text": "To compare answer tuples we disregard the ordering and require each number appearing in the reference answer to appear in the generated answer.", "labels": [], "entities": []}, {"text": "Parameters and Solver In our experiments we set kin our beam search algorithm (Section 5) to 200, and l to 20.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9178889989852905}]}, {"text": "We run the L-BFGS computation for 50 iterations.", "labels": [], "entities": []}, {"text": "We regularize our learning objective using the L 2 -norm and a \u03bb value of 0.1.", "labels": [], "entities": []}, {"text": "The set of mathematical relations supported by our implementation is {+, \u2212, \u00d7, /}.Our implementation uses the Gaussian Elimination function in the Efficient Java Matrix Library (EJML) to generate answers given a set of equations.", "labels": [], "entities": []}, {"text": "8 Results   results in performance which is almost 70% of ALLEQ, demonstrating the value of weakly supervised data.", "labels": [], "entities": [{"text": "ALLEQ", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9895469546318054}]}, {"text": "In contrast, 5EQ, which cannot use this weak supervision, performs much worse.", "labels": [], "entities": [{"text": "5EQ", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.7845327854156494}]}], "tableCaptions": [{"text": " Table 3: Cross-validation accuracy results for var- ious forms of supervision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9861425757408142}]}, {"text": " Table 4: Performance on different template fre- quencies for ALLEQ.", "labels": [], "entities": []}, {"text": " Table 5: Cross-validation accuracy results with  different feature groups ablated for ALLEQ. Re- sults are for answer accuracy which is 68.7% with- out any features ablated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9835400581359863}, {"text": "ALLEQ", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.905704140663147}, {"text": "Re- sults", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9731348156929016}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8058871030807495}]}]}