{"title": [{"text": "Weakly Supervised User Profile Extraction from Twitter", "labels": [], "entities": []}], "abstractContent": [{"text": "While user attribute extraction on social media has received considerable attention, existing approaches, mostly supervised, encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates (e.g., gender).", "labels": [], "entities": [{"text": "user attribute extraction", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.703690747419993}, {"text": "predicting unary predicates", "start_pos": 213, "end_pos": 240, "type": "TASK", "confidence": 0.8450392882029215}]}, {"text": "In this paper, we present a weakly-supervised approach to user profile extraction from Twitter.", "labels": [], "entities": [{"text": "user profile extraction", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.6333893239498138}]}, {"text": "Users' profiles from social media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their attributes from user-generated text.", "labels": [], "entities": [{"text": "extraction of their attributes from user-generated text", "start_pos": 123, "end_pos": 178, "type": "TASK", "confidence": 0.7782172134944371}]}, {"text": "In addition to traditional linguistic features used in distant supervision for information extraction , our approach also takes into account network information, a unique opportunity offered by social media.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8193528652191162}]}, {"text": "We test our algorithm on three attribute domains: spouse, education and job; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The overwhelming popularity of online social media creates an opportunity to display given aspects of oneself.", "labels": [], "entities": []}, {"text": "Users' profile information in social networking websites such as Facebook 2 or Google Plus 3 provides a rich repository personal information in a structured data format, making it amenable to automatic processing.", "labels": [], "entities": []}, {"text": "This includes, for example, users' jobs and education, and provides a useful source of information for applications such as search , friend recommendation, on-@ has taken all the kids today so I can go shopping-CHILD FREE!", "labels": [], "entities": [{"text": "FREE", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9945758581161499}]}, {"text": "#iloveyoushano #iloveyoucreditcard Tamworth promo day with my handsome classy husband @ Spouse: shanenicholson I got accepted to be part of the UofM engineering safety pilot program in Here in class.", "labels": [], "entities": [{"text": "Tamworth promo day", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.9510671297709147}]}, {"text": "(@ [Florida State University] -Williams Building) Don't worry , guys ! Our beloved will always continue to rise \" to the top ! Education: Florida State University (FSU) first day of work at, a sports bar woo come visit me yo..", "labels": [], "entities": [{"text": "Florida State University] -Williams Building", "start_pos": 4, "end_pos": 48, "type": "DATASET", "confidence": 0.9226213353020805}, {"text": "Florida State University (FSU", "start_pos": 138, "end_pos": 167, "type": "DATASET", "confidence": 0.8016481041908264}]}, {"text": "start to think we should just add a couple desks to the newsroom for Business Insider writers just back from, what a hell ! Job: HuffPo: Examples of Twitter message clues for user profile inference.", "labels": [], "entities": [{"text": "HuffPo", "start_pos": 129, "end_pos": 135, "type": "DATASET", "confidence": 0.9368346929550171}, {"text": "user profile inference", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.6342017153898875}]}, {"text": "line advertising, computational social science and more.", "labels": [], "entities": [{"text": "line advertising", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6549791991710663}]}, {"text": "Although profiles exist in an easy-to-use, structured data format, they are often sparsely populated; users rarely fully complete their online profiles.", "labels": [], "entities": []}, {"text": "Additionally, some social networking services such as Twitter don't support this type of structured profile data.", "labels": [], "entities": []}, {"text": "It is therefore difficult to obtain a reasonably comprehensive profile of a user, or a reasonably complete facet of information (say, education level) fora class of users.", "labels": [], "entities": []}, {"text": "While many users do not explicitly list all their personal information in their online profile, their user generated content often contains strong evidence to suggest many types of user attributes, for example education, spouse, and employment (See).", "labels": [], "entities": []}, {"text": "Can one use such information to infer more details?", "labels": [], "entities": []}, {"text": "In particular, can one exploit indirect clues from an unstructured data source like Twitter to obtain rich, structured user profiles?", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that it is feasible to automatically extract Facebook-style pro-files directly from users' tweets, thus making user profile data available in a structured format for upstream applications.", "labels": [], "entities": []}, {"text": "We view user profile inference as a structured prediction task where both text and network information are incorporated.", "labels": [], "entities": [{"text": "user profile inference", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.6547970771789551}]}, {"text": "Concretely, we cast user profile prediction as binary relation extraction), e.g., SPOUSE(User i , User j ), EDUCATION(User i , Entity j ) and EMPLOYER(User i , Entity j ).", "labels": [], "entities": [{"text": "user profile prediction", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.6070523460706075}, {"text": "SPOUSE", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9838601350784302}, {"text": "EDUCATION", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9039673209190369}, {"text": "EMPLOYER", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.7556912302970886}]}, {"text": "Inspired by the concept of distant supervision, we collect training tweets by matching attribute ground truth from an outside \"knowledge base\" such as Facebook or Google Plus.", "labels": [], "entities": []}, {"text": "One contribution of the work presented here is the creation of the first large-scale dataset on three general Twitter user profile domains (i.e., EDUCA-TION, JOB, SPOUSE).", "labels": [], "entities": [{"text": "EDUCA-TION", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.7874250411987305}, {"text": "JOB", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.5532593727111816}]}, {"text": "Experiments demonstrate that by simultaneously harnessing both text features and network information, our approach is able to make accurate user profile predictions.", "labels": [], "entities": []}, {"text": "We are optimistic that our approach can easily be applied to further user attributes such as HOBBIES and INTERESTS (MOVIES, BOOKS, SPORTS or STARS), RELIGION, HOMETOWN, LIVING LOCA-TION, FAMILY MEMBERS and soon, where training data can be obtained by matching ground truth retrieved from multiple types of online social media such as Facebook, Google Plus, or LinkedIn.", "labels": [], "entities": [{"text": "HOBBIES", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.924818217754364}, {"text": "INTERESTS", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9629954099655151}, {"text": "BOOKS", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9323199987411499}, {"text": "STARS", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.8456701636314392}, {"text": "RELIGION", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.997443675994873}, {"text": "HOMETOWN", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.98847895860672}, {"text": "LIVING LOCA-TION", "start_pos": 169, "end_pos": 185, "type": "METRIC", "confidence": 0.8972650170326233}, {"text": "FAMILY", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9833797812461853}, {"text": "MEMBERS", "start_pos": 194, "end_pos": 201, "type": "METRIC", "confidence": 0.5799385905265808}]}, {"text": "Our contributions are as follows: \u2022 We cast user profile prediction as an information extraction task.", "labels": [], "entities": [{"text": "user profile prediction", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.6671399374802908}, {"text": "information extraction task", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7836459378401438}]}, {"text": "\u2022 We present a large-scale dataset for this task gathered from various structured and unstructured social media sources.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate the benefit of jointly reasoning about users' social network structure when extracting their profiles from text.", "labels": [], "entities": []}, {"text": "\u2022 We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, JOB and EDUCATION.", "labels": [], "entities": [{"text": "SPOUSE", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9579588174819946}, {"text": "JOB", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9480772018432617}, {"text": "EDUCATION", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9934279322624207}]}, {"text": "The remainder of this paper is organized as follows: We summarize related work in Section 2.", "labels": [], "entities": []}, {"text": "The creation of our dataset is described in Section 3.", "labels": [], "entities": []}, {"text": "The details of our model are presented in Section 4.", "labels": [], "entities": []}, {"text": "We present experimental results in Section 5 and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe the generation of our distantly supervised training dataset in detail.", "labels": [], "entities": []}, {"text": "We make use of Google Plus and Freebase to obtain ground facts and extract positive/negative bags of postings from users' twitter streams according to the ground facts.", "labels": [], "entities": []}, {"text": "Education/Job We first used the Google Plus API 5 (shown in) to obtain a seed set of users whose profiles contain both their education/job status and a link to their twitter account.", "labels": [], "entities": []}, {"text": "Then, we fetched tweets containing the mention of the education/job entity from each correspondent user's twitter stream using Twitter's search API 7 (shown in) and used them to construct positive bags of tweets expressing the associated attribute, namely EDUCATION(User i , Entity j ), or EMPLOYER(User i , Entity j ).", "labels": [], "entities": [{"text": "EDUCATION", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9394518733024597}, {"text": "EMPLOYER", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.8530013561248779}]}, {"text": "The Freebase API 8 is employed for alias recognition, to match terms such as \"Harvard University\", \"Harvard\", \"Harvard U\" to a single The remainder of each corresponding user's entire Twitter feed is used as negative training data.", "labels": [], "entities": [{"text": "Freebase API 8", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9143657883008321}, {"text": "alias recognition", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7757042646408081}, {"text": "Harvard University\"", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.9432269334793091}, {"text": "Harvard\"", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8499429821968079}, {"text": "Harvard U\"", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.8746932943662008}]}, {"text": "We expanded our dataset from the seed users according to network information provided by Google Plus and Twitter.", "labels": [], "entities": []}, {"text": "Concretely, we crawled circle information of users in the seed set from both their Twitter and Google Plus accounts and performed a matching to pick out shared users between one's Twitter follower list and Google Plus Circle.", "labels": [], "entities": [{"text": "Google Plus Circle", "start_pos": 206, "end_pos": 224, "type": "DATASET", "confidence": 0.8936487634976705}]}, {"text": "This process assures friend identity and avoids the problem of name ambiguity when matching accounts across websites.", "labels": [], "entities": []}, {"text": "Among candidate users, those who explicitly display Job or Education information on Google Plus are preserved.", "labels": [], "entities": []}, {"text": "We then gathered positive and negative data as described above.", "labels": [], "entities": []}, {"text": "Dataset statistics are presented in SPOUSE is an exception to the \"homophily\" effect.", "labels": [], "entities": []}, {"text": "But it exhibits another unique property, known as, REFLEXIVITY: fact IsSpouseOf (e 1 , e 2 ) and IsSpouseOf (e 2 , e 1 ) will hold or not hold at the same time.", "labels": [], "entities": [{"text": "REFLEXIVITY", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9977389574050903}]}, {"text": "Given training data expressing the tuple IsSpouseOf (e 1 , e 2 ) from user e 1 's twitter stream, we also gather user e 2 's tweet collection, and fetch tweets with the mention of e 1 . We augment negative training data from e 2 as in the case of Education and Job.", "labels": [], "entities": [{"text": "IsSpouseOf", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9471889138221741}]}, {"text": "Our Spouse dataset contains 1,636 users, where there are 554 couples (1108 users).", "labels": [], "entities": [{"text": "Spouse dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8334129154682159}]}, {"text": "Note that the number of positive entities) is greater than the number of users as (1) one user can have multiple spouses at different periods of time multiple entities may point to the same individual, e.g., BarackObama, Barack Obama and Barack.", "labels": [], "entities": []}, {"text": "In this Section, we present our experimental results in detail.: Affinity values for Education and Job.", "labels": [], "entities": [{"text": "Affinity", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9887579679489136}]}, {"text": "Each tweet posting is tokenized using Twitter NLP tool introduced by Noah's Ark 14 with # and @ separated following tokens.", "labels": [], "entities": []}, {"text": "We assume that attribute values should be either name entities or terms following @ and #.", "labels": [], "entities": []}, {"text": "Name entities are extracted using.", "labels": [], "entities": []}, {"text": "Consecutive tokens with the same named entity tag are chunked (.", "labels": [], "entities": []}, {"text": "Part-ofspeech tags are assigned based on Owoputi et al's tweet POS system (.", "labels": [], "entities": []}, {"text": "Data is divided in halves.", "labels": [], "entities": []}, {"text": "The first is used as training data and the other as testing data.", "labels": [], "entities": []}, {"text": "We evaluate settings described in Section 4.2 i.e., GLOBAL setting, where user-level attribute is predicted directly from jointly feature space and LO-CAL setting where user-level prediction is made based on tweet-level prediction along with different inference approaches described in Section 4.4, i.e. NEIGH-OBSERVED and NEIGH-LATENT, regarding whether neighbor information is observed or latent.", "labels": [], "entities": []}, {"text": "Baselines We implement the following baselines for comparison and use identical processing techniques for each approach for fairness.", "labels": [], "entities": []}, {"text": "\u2022 Only-Text: A simplified version of our algorithm where network/neighbor influence is ignored.", "labels": [], "entities": []}, {"text": "Classifier is trained and tested only based on text features.", "labels": [], "entities": []}, {"text": "\u2022 NELL: For Job and Education, candidate is selected as attribute value once it matches bag of words in the list of universities or companies borrowed from NELL.", "labels": [], "entities": []}, {"text": "For Education, the list is extended by alias identification based on Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9823141098022461}]}, {"text": "For Job, we also fetch the name abbreviations . NELL is only implemented for Education and Job attribute.", "labels": [], "entities": [{"text": "NELL", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.5440489053726196}]}, {"text": "For each setting from each approach, we report the (P)recision, (R)ecall and (F)1-score.", "labels": [], "entities": [{"text": "recision", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.704753577709198}, {"text": "R)ecall", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.6577513813972473}, {"text": "F)1-score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9176835815111796}]}, {"text": "For LO-CAL setting, we report the performance for both entity-level prediction (Entity) and posting-level prediction (Tweet).", "labels": [], "entities": [{"text": "entity-level prediction", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.659673810005188}, {"text": "posting-level prediction", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.6516918987035751}]}, {"text": "Results for Education, Job and Spouse from different approaches appear in, 5 and 6 respectively.", "labels": [], "entities": []}, {"text": "Local or Global For horizontal comparison, we observe that GLOBAL obtains a higher Precision score but a lower Recall than LOCAL(ENTITY).", "labels": [], "entities": [{"text": "GLOBAL", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.5717303156852722}, {"text": "Precision score", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.988642156124115}, {"text": "Recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9993114471435547}, {"text": "LOCAL", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9949644804000854}, {"text": "ENTITY", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.5278207659721375}]}, {"text": "This can be explained by the fact that LOCAL(U) sets z k i,e = 1 once one posting x \u2208 Le i is identified as attribute related, while GLOBAL tend to be more meticulous by considering the conjunctive feature space from all postings.", "labels": [], "entities": []}, {"text": "Homophile effect In agreement with our expectation, NEIGH-OBSERVED performs better than NEIGH-LATENT since erroneous predictions in http://www.abbreviations.com/ NEIGH-LATENT setting will have negative influence on further prediction during the greedy search process.", "labels": [], "entities": []}, {"text": "Both NEIGH-OBSERVED and NEIGH-LATENT where network information is harnessed, perform better than Only-Text, which the prediction is made independently on user's text features.", "labels": [], "entities": [{"text": "NEIGH-OBSERVED", "start_pos": 5, "end_pos": 19, "type": "DATASET", "confidence": 0.6580466032028198}]}, {"text": "The improvement of NEIGH-OBSERVED over Only-Text is 22.7% and 6.4% regarding F-1 score for Education and Job respectively, which further illustrate the usefulness of making use of Homophile effect for attribute inference on online social media.", "labels": [], "entities": [{"text": "NEIGH-OBSERVED", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.908018946647644}, {"text": "F-1 score", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.983112245798111}]}, {"text": "It is also interesting to note the improvement much more significant in Education inference than Job inference.", "labels": [], "entities": []}, {"text": "This is in accord with what we find in Section 5.2, where education network exhibits stronger HOMOPHILE property than Job network, enabling a significant benefit for education inference, but limited for job inference.", "labels": [], "entities": [{"text": "HOMOPHILE", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9857257604598999}]}, {"text": "Spouse prediction also benefits from neighboring effect and the improvement is about 12% for LOCAL(ENTITY) setting.", "labels": [], "entities": [{"text": "Spouse prediction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8836731016635895}, {"text": "LOCAL", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9527668356895447}]}, {"text": "Unlike Education and Job prediction, for which in NEIGH-OBSERVED setting all neighboring variables are observed, network variables are hidden during spouse prediction.", "labels": [], "entities": [{"text": "Education and Job prediction", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.5273636355996132}, {"text": "spouse prediction", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.6816929429769516}]}, {"text": "By considering network information, the model benefits from evident clues offered by tweet corpus of user e's spouse when making prediction fore, but also suffers when erroneous decision are made and then used for downstream predictions.", "labels": [], "entities": []}, {"text": "NELL Baseline Notably, NELL achieves highest Recall score for Education inference.", "labels": [], "entities": [{"text": "Recall score", "start_pos": 45, "end_pos": 57, "type": "METRIC", "confidence": 0.9722883403301239}]}, {"text": "It is also worth noting that most of education mentions that NELL fails to retrieve are those involve irregular spellings, such as HarvardUniv and Cornell U, which means Recall score for NELL baseline would be even higher if these irregular spellings are recognized in a more sophisticated system.", "labels": [], "entities": [{"text": "NELL", "start_pos": 61, "end_pos": 65, "type": "TASK", "confidence": 0.8878379464149475}, {"text": "HarvardUniv", "start_pos": 131, "end_pos": 142, "type": "DATASET", "confidence": 0.9882197380065918}, {"text": "Cornell U", "start_pos": 147, "end_pos": 156, "type": "DATASET", "confidence": 0.7993469536304474}, {"text": "Recall score", "start_pos": 170, "end_pos": 182, "type": "METRIC", "confidence": 0.9757283329963684}]}, {"text": "The reason for such high recall is that as our ground truths are obtained from Google plus, the users from which are mostly affiliated with decent schools found in NELL dictionary.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9993970394134521}, {"text": "NELL dictionary", "start_pos": 164, "end_pos": 179, "type": "DATASET", "confidence": 0.9078276455402374}]}, {"text": "However, the high recall from NELL is sacrificed at precision, as users can mention school entities in many of situations, such as paying a visitor reporting some relevant news.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.999455988407135}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9986218214035034}]}, {"text": "NELL will erroneously classify these cases as attribute mentions.", "labels": [], "entities": [{"text": "NELL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7884348034858704}]}, {"text": "NELL does notwork out for Job, with a fairly poor 0.0156 F1 score for LOCAL(ENTITY) and 0.163 for LOCAL(TWEET).", "labels": [], "entities": [{"text": "NELL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.464633584022522}, {"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9872247576713562}, {"text": "LOCAL(ENTITY)", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.8070474416017532}]}, {"text": "Poor precision is expected for as users can mention firm entity in a great many of situations.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.999183714389801}]}, {"text": "The recall score for: Results for Spouse Prediction NELL in job inference is also quite low as job related entities exhibit a greater diversity of mentions, many of which are not covered by the NELL dictionary.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9995087385177612}, {"text": "Spouse Prediction NELL", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8552396893501282}, {"text": "job inference", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.7022783160209656}]}, {"text": "Vertical Comparison: Education, Job and Spouse Job prediction turned out to be much more difficult than Education, as shown in Tables 4 and 5.", "labels": [], "entities": [{"text": "Spouse Job prediction", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7255363066991171}]}, {"text": "Explanations are as follows: (1) Job contains a much greater diversity of mentions than Education.", "labels": [], "entities": []}, {"text": "Education inference can benefit a lot from the dictionary relevant feature which Job may not.", "labels": [], "entities": [{"text": "Education inference", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6694570928812027}]}, {"text": "(2) Education mentions are usually associated with clear evidence such as homework, exams, studies, cafeteria or books, while situations are much more complicated for job as vocabularies are usually specific for different types of jobs.", "labels": [], "entities": []}, {"text": "The boundary between a user working in and a fun fora specific operation is usually ambiguous.", "labels": [], "entities": []}, {"text": "For example, a Google engineer may constantly update information about outcome products of Google, so does a big fun.", "labels": [], "entities": []}, {"text": "If the aforementioned engineer barely tweets about working conditions or colleagues (which might still be ambiguous), his tweet collection, which contains many of mentions about outcomes of Google product, will be significantly similar to tweets published by a Google fun.", "labels": [], "entities": []}, {"text": "Such nuisance can be partly solved by the consideration of network information, but not totally.", "labels": [], "entities": []}, {"text": "The relatively high F1 score for spouse prediction is largely caused by the great many of nonindividual related entities in the dataset, the identification of which would be relatively simpler.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9891810715198517}, {"text": "spouse prediction", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8103405833244324}]}, {"text": "A deeper look at the result shows that the classifier frequently makes wrong decisions for entities such as userID and name entities.", "labels": [], "entities": []}, {"text": "Significant as some spouse relevant features are, such as love, husband, child, inmost circumstances, spouse mentions are extremely hard to recognize.", "labels": [], "entities": []}, {"text": "For example, in tweets \"Check this out, @alancross, it's awesome bit.ly/1bnjYHh.\" or \"Happy Birthday @alancross !\".", "labels": [], "entities": []}, {"text": "alancross can reasonably be any option among current user's friend, colleague, parents, child or spouse.", "labels": [], "entities": []}, {"text": "Repeated mentions add no confidence.", "labels": [], "entities": []}, {"text": "Although we can identify alancross as spouse attribute once it jointly appear with other strong spouse indicators, they are still many cases where they never co-appear.", "labels": [], "entities": []}, {"text": "How to integrate more useful side information for spouse recognition constitutes our future work.", "labels": [], "entities": [{"text": "spouse recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9010654091835022}]}], "tableCaptions": [{"text": " Table 2: Statistics for our Dataset", "labels": [], "entities": []}, {"text": " Table 4: Results for Education Prediction", "labels": [], "entities": []}, {"text": " Table 5: Results for Job Prediction", "labels": [], "entities": [{"text": "Job Prediction", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7583873271942139}]}]}