{"title": [{"text": "Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.8345539768536886}]}], "abstractContent": [{"text": "In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep auto-encoder (DAE) paradigm for phrase-based translation model.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 190, "end_pos": 214, "type": "TASK", "confidence": 0.7877196967601776}]}, {"text": "Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsuper-vised DBN features.", "labels": [], "entities": []}, {"text": "Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning.", "labels": [], "entities": []}, {"text": "On two Chinese-English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the base-line features, respectively.", "labels": [], "entities": [{"text": "IWSLT)", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9320366680622101}, {"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.7060121297836304}]}], "introductionContent": [{"text": "Recently, many new features have been explored for SMT and significant performance have been obtained in terms of translation quality, such as syntactic features, sparse features, and reordering features.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9956474900245667}]}, {"text": "However, most of these features are manually designed on linguistic phenomena that are related to bilingual language pairs, thus they are very difficult to devise and estimate.", "labels": [], "entities": []}, {"text": "Instead of designing new features based on intuition, linguistic knowledge and domain, for the first time, explored the possibility of inducing new features in an unsupervised fashion using deep belief net (DBN) ( ) for hierarchical phrase-based translation model.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 233, "end_pos": 257, "type": "TASK", "confidence": 0.5913363695144653}]}, {"text": "Using the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence, and generated new unsupervised DBN features using forward computation.", "labels": [], "entities": []}, {"text": "These new features are appended as extra features to the phrase table for the translation decoder.", "labels": [], "entities": []}, {"text": "However, the above approach has two major shortcomings.", "labels": [], "entities": []}, {"text": "First, the input original features for the DBN feature learning are too simple, the limited 4 phrase features of each phrase pair, such as bidirectional phrase translation probability and bidirectional lexical weighting (, which area bottleneck for learning effective feature representation.", "labels": [], "entities": [{"text": "DBN feature learning", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.6551594138145447}, {"text": "phrase translation", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.6993836164474487}]}, {"text": "Second, it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines (RBM), does not have a training objective, so its performance relies on the empirical parameters.", "labels": [], "entities": []}, {"text": "Thus, this approach is unstable and the improvement is limited.", "labels": [], "entities": []}, {"text": "In this paper, we strive to effectively address the above two shortcomings, and systematically explore the possibility of learning new features using deep (multilayer) neural networks (DNN, which is usually referred under the name Deep Learning) for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 250, "end_pos": 253, "type": "TASK", "confidence": 0.9933444261550903}]}, {"text": "To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (), phrase frequency, phrase length (, and phrase generative probability (, which also show further improvement for new phrase feature learning in our experiments.", "labels": [], "entities": [{"text": "SMT", "start_pos": 211, "end_pos": 214, "type": "TASK", "confidence": 0.9934276342391968}, {"text": "phrase generative", "start_pos": 291, "end_pos": 308, "type": "TASK", "confidence": 0.7084332704544067}]}, {"text": "To address the second shortcoming, inspired by the successful use of DAEs for handwritten digits recognition (), information retrieval (, and speech spectrograms, we propose new feature learning using semi-supervised DAE for phrase-based translation model.", "labels": [], "entities": [{"text": "handwritten digits recognition", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6452506283919016}, {"text": "information retrieval", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.8062202036380768}, {"text": "phrase-based translation", "start_pos": 225, "end_pos": 249, "type": "TASK", "confidence": 0.7638667821884155}]}, {"text": "By using the input data as the teacher, the \"semi-supervised\" fine-tuning process of DAE addresses the problem of \"back-propagation without a teacher\", which makes the DAE learn more powerful and abstract features).", "labels": [], "entities": []}, {"text": "For our semisupervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE's parameters and use the input original phrase features as the \"teacher\" for semi-supervised backpropagation.", "labels": [], "entities": []}, {"text": "Compared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable.", "labels": [], "entities": []}, {"text": "Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition for DAEs (HCDAE) that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs, which shows further improvement compared with single DAE in our experiments.", "labels": [], "entities": []}, {"text": "It is encouraging that, non-parametric feature expansion using gaussian mixture model (GMM), which guarantees invariance to the specific embodiment of the original features, has been proved as a feasible feature generation approach for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 236, "end_pos": 239, "type": "TASK", "confidence": 0.9958450198173523}]}, {"text": "Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM.", "labels": [], "entities": [{"text": "feature learning", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.728173017501831}]}, {"text": "Thus, instead of GMM, we use DNN (DBN, DAE and HCDAE) to learn new non-parametric features, which has the similar evolution in speech recognition (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7279210239648819}]}, {"text": "DNN features are learned from the non-linear combination of the input original features, they strong capture highorder correlations between the activities of the original features, and we believe this deep learning paradigm induces the original features to further reach their potential for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 291, "end_pos": 294, "type": "TASK", "confidence": 0.9964703321456909}]}, {"text": "Finally, we conduct large-scale experiments on IWSLT and NIST Chinese-English translation tasks, respectively, and the results demonstrate that our solutions solve the two aforementioned shortcomings successfully.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.7858810424804688}, {"text": "NIST Chinese-English translation tasks", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.7693082839250565}]}, {"text": "Our semi-supervised DAE features significantly outperform the unsupervised DBN features and the baseline features, and our introduced input phrase features significantly improve the performance of DAE feature learning.", "labels": [], "entities": [{"text": "DAE feature learning", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.8518750468889872}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly summarizes the recent related work about the applications of DNN for SMT tasks.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.9561072289943695}]}, {"text": "Section 3 presents our introduced input features for DNN feature learning.", "labels": [], "entities": [{"text": "DNN feature learning", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8652512232462565}]}, {"text": "Section 4 describes how to learn our semi-supervised DAE features for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.993565022945404}]}, {"text": "Section 5 describes and discusses the large-scale experimental results.", "labels": [], "entities": []}, {"text": "Finally, we end with conclusions in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now test our DAE features on the following two Chinese-English translation tasks.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.6865764359633127}]}, {"text": "The bilingual corpus is the ChineseEnglish part of Basic Traveling Expression corpus (BTEC) and China-Japan-Korea (CJK) corpus (0.38M sentence pairs with 3.5/3.8M Chinese/English words).", "labels": [], "entities": [{"text": "Basic Traveling Expression corpus (BTEC)", "start_pos": 51, "end_pos": 91, "type": "DATASET", "confidence": 0.6882057700838361}]}, {"text": "The LM corpus is the English side of the parallel data (BTEC, CJK and CWMT08 3 ) (1.34M sentences).", "labels": [], "entities": [{"text": "LM corpus", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7109329104423523}, {"text": "BTEC", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9509428143501282}, {"text": "CJK", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8219186663627625}]}, {"text": "Our development set is IWSLT 2005 test set (506 sentences), and our test set is IWSLT 2007 test set (489 sentences).", "labels": [], "entities": [{"text": "IWSLT 2005 test set", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.9651055037975311}, {"text": "IWSLT 2007 test set", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.969620019197464}]}, {"text": "The bilingual corpus is LDC 4 (3.4M sentence pairs with 64/70M Chinese/English words).", "labels": [], "entities": []}, {"text": "The LM corpus is the English side of the parallel data as well as the English Gigaword corpus (LDC2007T07) (11.3M sentences).", "labels": [], "entities": [{"text": "LM corpus", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8328031301498413}, {"text": "English Gigaword corpus", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.7472465435663859}]}, {"text": "Our development set is NIST 2005 MT evaluation set (1084 sentences), and our test set is NIST 2006 MT evaluation set (1664 sentences).", "labels": [], "entities": [{"text": "NIST 2005 MT evaluation set", "start_pos": 23, "end_pos": 50, "type": "DATASET", "confidence": 0.8993458390235901}, {"text": "NIST 2006 MT evaluation set", "start_pos": 89, "end_pos": 116, "type": "DATASET", "confidence": 0.9176323652267456}]}, {"text": "We choose the Moses () framework to implement our phrase-based machine system.", "labels": [], "entities": []}, {"text": "The 4-gram LMs are estimated by the SRILM toolkit with modified Kneser-Ney  xf\", \"DBN X xf\", \"DAE X 1 xf\" and \"DAE X xf\" represent that we use DBN and DAE, input features X 1 and X, to learn x-dimensional features, respectively.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8305046558380127}]}, {"text": "\"HCDAE X x+xf\" represents horizontally combining two DAEs and each DAE has the same x-dimensional learned features.", "labels": [], "entities": []}, {"text": "All improvements on two test sets are statistically significant by the bootstrap resampling).", "labels": [], "entities": []}, {"text": "*: significantly better than the baseline (p < 0.05), **: significantly better than \"DBN X 1 xf\" or \"DBN X xf\" (p < 0.01), ***: significantly better than \"DAE X 1 xf\" or \"DAE X xf\" (p < 0.01), ****: significantly better than \"HCDAE X x+xf\" (p < 0.01), +: significantly better than \"X 2 +X 3 +X 4 +X 5 \" (p < 0.01). discounting.", "labels": [], "entities": [{"text": "discounting", "start_pos": 315, "end_pos": 326, "type": "METRIC", "confidence": 0.9161124229431152}]}, {"text": "We perform pairwise ranking optimization () to tune feature weights.", "labels": [], "entities": [{"text": "pairwise ranking optimization", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.6363538901011149}]}, {"text": "The translation quality is evaluated by case-insensitive IBM BLEU-4 metric.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9632872939109802}, {"text": "BLEU-4", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9453491568565369}]}, {"text": "The baseline translation models are generated by Moses with default parameter settings.", "labels": [], "entities": []}, {"text": "In the contrast experiments, our DAE and HCDAE features are appended as extra features to the phrase table.", "labels": [], "entities": []}, {"text": "The details of the used network structure in our experiments are shown in.: The effectiveness of our introduced input features.", "labels": [], "entities": []}, {"text": "\"DAE X 1 +X i 4f\" represents that we use DAE, input features X 1 + X i , to learn 4-dimensional features.", "labels": [], "entities": []}, {"text": "*: significantly better than \"DAE X 1 4f\" (p < 0.05).", "labels": [], "entities": [{"text": "DAE X 1 4f", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.7734168916940689}]}], "tableCaptions": [{"text": " Table 2: The translation results by adding new DNN features (DBN feature (Maskey and Zhou, 2012),  our proposed DAE and HCDAE feature) as extra features to the phrase table on two tasks. \"DBN X 1", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9674083590507507}]}, {"text": " Table 3: The variance distributions of each dimensional learned DBN feature and DAE feature on the  two tasks.", "labels": [], "entities": [{"text": "DAE", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.973173201084137}]}, {"text": " Table 4: The effectiveness of our introduced in- put features. \"DAE X 1 +X i 4f\" represents that  we use DAE, input features X 1 + X i , to learn 4- dimensional features. *: significantly better than  \"DAE X 1 4f\" (p < 0.05).", "labels": [], "entities": []}]}