{"title": [{"text": "Representation Learning for Text-level Discourse Parsing", "labels": [], "entities": [{"text": "Representation Learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9000518023967743}, {"text": "Text-level Discourse Parsing", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7060756683349609}]}], "abstractContent": [{"text": "Text-level discourse parsing is notoriously difficult, as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features.", "labels": [], "entities": [{"text": "Text-level discourse parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5652109682559967}]}, {"text": "In this paper, we present a representation learning approach, in which we transform surface features into a latent space that facilitates RST discourse parsing.", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 138, "end_pos": 159, "type": "TASK", "confidence": 0.9351935982704163}]}, {"text": "By combining the machinery of large-margin transition-based struc-tured prediction with representation learning , our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features.", "labels": [], "entities": [{"text": "large-margin transition-based struc-tured prediction", "start_pos": 30, "end_pos": 82, "type": "TASK", "confidence": 0.6362108141183853}]}, {"text": "The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the RST Treebank.", "labels": [], "entities": [{"text": "predicting", "start_pos": 115, "end_pos": 125, "type": "TASK", "confidence": 0.9609655141830444}, {"text": "nuclearity", "start_pos": 140, "end_pos": 150, "type": "TASK", "confidence": 0.7953662872314453}, {"text": "RST Treebank", "start_pos": 158, "end_pos": 170, "type": "DATASET", "confidence": 0.9671363532543182}]}], "introductionContent": [{"text": "Discourse structure describes the high-level organization of text or speech.", "labels": [], "entities": [{"text": "Discourse structure", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7004666924476624}]}, {"text": "It is central to a number of high-impact applications, such as text summarization, sentiment analysis), question answering, and automatic evaluation of student writing (.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7682653963565826}, {"text": "sentiment analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.9729896187782288}, {"text": "question answering", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.9332861304283142}]}, {"text": "Hierarchical discourse representations such as Rhetorical Structure Theory (RST) are particularly useful because of the computational applicability of tree-shaped discourse structures (), as shown in.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.799087663491567}]}, {"text": "Unfortunately, the performance of discourse parsing is still relatively weak: the state-of-the-art F-measure for text-level relation detection in the RST Treebank is only slightly above 55% (Joty when profit was $107.8 million on sales of $435.5 million.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.6934099197387695}, {"text": "F-measure", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9980100989341736}, {"text": "text-level relation detection", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.6570487916469574}, {"text": "RST Treebank", "start_pos": 150, "end_pos": 162, "type": "DATASET", "confidence": 0.8098664581775665}]}, {"text": "The projections are in the neighborhood of 50 cents a share to 75 cents, et al., 2013).", "labels": [], "entities": []}, {"text": "While recent work has introduced increasingly powerful features) and inference techniques (, discourse relations remain hard to detect, due in part to along tail of \"alternative lexicalizations\" that can be used to realize each relation ().", "labels": [], "entities": []}, {"text": "Surface and syntactic features are not capable of capturing what are fundamentally semantic distinctions, particularly in the face of relatively small annotated training sets.", "labels": [], "entities": []}, {"text": "In this paper, we present a representation learning approach to discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6995416134595871}]}, {"text": "The core idea of our work is to learn a transformation from a bag-of-words surface representation into a latent space in which discourse relations are easily identifiable.", "labels": [], "entities": []}, {"text": "The latent representation for each discourse unit can be viewed as a discriminativelytrained vector-space representation of its meaning.", "labels": [], "entities": []}, {"text": "Alternatively, our approach can be seen as a nonlinear learning algorithm for incremental structure prediction, which overcomes feature sparsity through effective parameter tying.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7907456755638123}]}, {"text": "We consider several alternative methods for transforming the original features, corresponding to different ideas of the meaning and role of the latent representation.", "labels": [], "entities": []}, {"text": "Our method is implemented as a shift-reduce discourse parser.", "labels": [], "entities": []}, {"text": "Learning is performed as large-margin transitionbased structure prediction (), while at the same time jointly learning to project the surface representation into latent space.", "labels": [], "entities": [{"text": "large-margin transitionbased structure prediction", "start_pos": 25, "end_pos": 74, "type": "TASK", "confidence": 0.6355243250727654}]}, {"text": "The resulting system strongly outperforms the prior state-of-the-art at labeled F-measure, obtaining raw improvements of roughly 6% on relation labels and 2.5% on nuclearity.", "labels": [], "entities": []}, {"text": "In addition, we show that the latent representation coheres well with the characterization of discourse connectives in the Penn Discourse Treebank ().", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.9846011598904928}]}], "datasetContent": [{"text": "We evaluate DPLP on the RST Discourse Treebank (), comparing against state-of-the-art results.", "labels": [], "entities": [{"text": "RST Discourse Treebank", "start_pos": 24, "end_pos": 46, "type": "DATASET", "confidence": 0.8296200831731161}]}, {"text": "We also investigate the information encoded by the projection matrix.", "labels": [], "entities": []}, {"text": "Dataset The RST Discourse Treebank (RST-DT) consists of 385 documents, with 347 for train-  On span detection, DPLP performs slightly worse than the prior state-of-the-art.", "labels": [], "entities": [{"text": "RST Discourse Treebank (RST-DT)", "start_pos": 12, "end_pos": 43, "type": "DATASET", "confidence": 0.808195561170578}, {"text": "train-  On span detection", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.5753914713859558}]}, {"text": "These systems employ richer syntactic and contextual features, which might be especially helpful for span identification.", "labels": [], "entities": [{"text": "span identification", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.9412544667720795}]}, {"text": "As shown byline 4 of the results table, the basic features from provide most of the predictive power for spans; however, these features are inadequate at the more semantically-oriented tasks of nuclearity and relation prediction, which benefit substantially from the projected features.", "labels": [], "entities": [{"text": "nuclearity", "start_pos": 194, "end_pos": 204, "type": "TASK", "confidence": 0.9844411611557007}, {"text": "relation prediction", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.8597374558448792}]}, {"text": "Since correctly identifying spans is a precondition for nuclearity and relation prediction, we might obtain still better results by combining features from HILDA and TSP with the representation learning approach described here.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.748855322599411}]}, {"text": "Lines 5 and 6 show that discriminative learning of the projection matrix is crucial, as fixed projections obtained from NMF or neural word embeddings perform substantially worse.", "labels": [], "entities": []}, {"text": "Line 7 shows that the original bag-of-words representation together with basic features could give us some benefit on discourse parsing, but still not as good as results from DPLP.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.6897887587547302}]}, {"text": "From lines 8 and 9, we see that the concatenation construction is superior to the difference construction, but the comparison between lines 10 and 11 is inconclusive on the merits of the general form of A.", "labels": [], "entities": [{"text": "A", "start_pos": 203, "end_pos": 204, "type": "METRIC", "confidence": 0.9280146956443787}]}, {"text": "This suggests that using the projection matrix to model interrelationships between EDUs does not substantially improve performance, and the simpler concatenation construction maybe preferred.", "labels": [], "entities": []}, {"text": "shows how performance changes for different latent dimensions K.", "labels": [], "entities": []}, {"text": "At each value of K, we employ grid search over a development set to identify the optimal regularizers \u03bb and \u03c4 . For the concatenation construction, performance is not overly sensitive to K.", "labels": [], "entities": []}, {"text": "For the general form of A, performance decreases with large K.", "labels": [], "entities": []}, {"text": "Recall from Section 2.3 that this construction has nine times as many parameters as the concatenation form; with large values of K, it is likely to overfit.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Parsing results of different models on the RST-DT test set. The results of TSP and HILDA are  reprinted from prior work (Joty et al., 2013; Hernault et al., 2010).", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8561100165049235}, {"text": "TSP", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.6913201808929443}, {"text": "HILDA", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9887096285820007}]}]}