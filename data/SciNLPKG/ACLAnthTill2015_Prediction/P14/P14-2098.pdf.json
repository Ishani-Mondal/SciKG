{"title": [{"text": "Improved Correction Detection in Revised ESL Sentences", "labels": [], "entities": [{"text": "Improved Correction Detection", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7096181511878967}, {"text": "Revised ESL Sentences", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.6226605176925659}]}], "abstractContent": [{"text": "This work explores methods of automatically detecting corrections of individual mistakes in sentence revisions for ESL students.", "labels": [], "entities": [{"text": "automatically detecting corrections of individual mistakes in sentence revisions", "start_pos": 30, "end_pos": 110, "type": "TASK", "confidence": 0.8107787768046061}]}, {"text": "We have trained a classifier that specializes in determining whether consecutive basic-edits (word insertions, deletions, substitutions) address the same mistake.", "labels": [], "entities": [{"text": "word insertions, deletions, substitutions", "start_pos": 94, "end_pos": 135, "type": "TASK", "confidence": 0.7872252116600672}]}, {"text": "Experimental result shows that the proposed system achieves an F 1-score of 81% on correction detection and 66% for the overall system, out-performing the baseline by a large margin.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9951221346855164}, {"text": "correction detection", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7663910984992981}]}], "introductionContent": [{"text": "Quality feedback from language tutors can help English-as-a-Second-Language (ESL) students improve their writing skills.", "labels": [], "entities": []}, {"text": "One of the tutors' tasks is to isolate writing mistakes within sentences, and point out (1) why each case is considered a mistake, and (2) how each mistake should be corrected.", "labels": [], "entities": [{"text": "isolate writing mistakes within sentences", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.8108137488365174}]}, {"text": "Because this is time consuming, tutors often just rewrite the sentences without giving any explanations).", "labels": [], "entities": []}, {"text": "Due to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions.", "labels": [], "entities": []}, {"text": "Computer aided language learning tools offer a solution for providing more detailed feedback.", "labels": [], "entities": []}, {"text": "Programs can be developed to compare the student's original sentences with the tutor-revised sentences.", "labels": [], "entities": []}, {"text": "have proposed a promising framework for this purpose.", "labels": [], "entities": []}, {"text": "Their approach has two components: one to detect individual corrections within a revision, which they termed correction detection; another to determine what the correction fixes, which they termed error type selection.", "labels": [], "entities": [{"text": "correction detection", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.7695282995700836}]}, {"text": "Although they reported a high accuracy for the error type selection classifier alone, the bottleneck of their system is the other component -correction detection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992626309394836}, {"text": "component -correction detection", "start_pos": 130, "end_pos": 161, "type": "TASK", "confidence": 0.7341323792934418}]}, {"text": "An analysis of their system shows that approximately 70% of the system's mistakes are caused by mis-detections in the first place.", "labels": [], "entities": []}, {"text": "Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus).", "labels": [], "entities": [{"text": "correction detection", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.9664570391178131}, {"text": "FCE corpus", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.9647850394248962}]}, {"text": "When determining whether a set of basic-edits (word insertions, deletions, substitutions) contributes to the same correction, these heuristics lack the flexibility to adapt to a specific context.", "labels": [], "entities": [{"text": "word insertions, deletions, substitutions", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.8252428074677786}]}, {"text": "Furthermore, it is not clear if the heuristics will work as well for tutors trained to markup revisions under different guidelines.", "labels": [], "entities": []}, {"text": "We propose to improve upon the correction detection component by training a classifier that determines which edits in a revised sentence address the same error in the original sentence.", "labels": [], "entities": [{"text": "correction detection", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9410143792629242}]}, {"text": "The classifier can make more accurate decisions adjusted to contexts.", "labels": [], "entities": []}, {"text": "Because the classifier were trained on revisions where corrections are explicitly marked by English experts, it is also possible to build systems adjusted to different annotation standards.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: (1) We show empirically that a major challenge in correction detection is to determine the number of edits that address the same error.", "labels": [], "entities": [{"text": "correction detection", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.9739114046096802}]}, {"text": "(2) We have developed a merging model that reduces mis-detection by 1/3, leading to significant improvement in the accuracies of combined correction detection and error type selection.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9762334227561951}, {"text": "correction detection", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.6499027162790298}]}, {"text": "(3) We have conducted experiments across multiple corpora, indicating that the proposed merging model is generalizable.", "labels": [], "entities": []}], "datasetContent": [{"text": "We combine Levenshtein algorithm with different merging algorithms for correction detection.", "labels": [], "entities": [{"text": "correction detection", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.9418932795524597}]}, {"text": "An ideal data resource would be a real-world collection of student essays and their revisions ().", "labels": [], "entities": []}, {"text": "However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard.", "labels": [], "entities": []}, {"text": "We instead use error annotated data, in which the corrections were provided by human experts.", "labels": [], "entities": []}, {"text": "We simulate the revisions by applying corrections onto the original sentence.", "labels": [], "entities": []}, {"text": "The teachers' annotations are treated as gold standard for the detailed corrections.", "labels": [], "entities": []}, {"text": "We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NU-CLE corpus), UIUC corpus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus ().", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8484779000282288}, {"text": "NU-CLE corpus", "start_pos": 135, "end_pos": 148, "type": "DATASET", "confidence": 0.8763661086559296}, {"text": "UIUC corpus 2", "start_pos": 151, "end_pos": 164, "type": "DATASET", "confidence": 0.7959967652956644}, {"text": "HOO2011 corpus", "start_pos": 197, "end_pos": 211, "type": "DATASET", "confidence": 0.8948569595813751}]}, {"text": "These corpora all provide experts' corrections along with error Type name description A gap-between-edits Gap between the two edits.", "labels": [], "entities": [{"text": "error", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9721199870109558}]}, {"text": "In particular, we use the number of words between the two edits' original words, as well as the revised words.", "labels": [], "entities": []}, {"text": "Note that Swanson and Yamangil's approach is a special case that only considers if the basic-edits have zero gap in both sentences.", "labels": [], "entities": []}, {"text": "tense-change We detect patterns such as: if the original-revision pair matches the pattern \"V-ing\u21d2to V\".", "labels": [], "entities": []}, {"text": "word-order-error Whether the basic-edits' original word set and the revised word set are the same (one or zero).", "labels": [], "entities": []}, {"text": "same-word-set If the original sentence and the revised sentence have the same word set, then it's likely that all the edits are fixing the word order error.", "labels": [], "entities": []}, {"text": "revised-to The phrase comprised of the two revised words.", "labels": [], "entities": []}, {"text": "In addition to evaluating the merging algorithms on the stand-alone task of correction detection, we have also plugged in the merging algorithms into an end-to-end system in which every automatically detected correction is further classified into an error type.", "labels": [], "entities": [{"text": "correction detection", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.9197206497192383}]}, {"text": "We replicated the error type selector described in.", "labels": [], "entities": []}, {"text": "The error type selector's accuracies are shown in 3 . We compare two merging algorithms, combined with Levenshtein algorithm: S&Y The merging heuristic proposed by Swanson and Yamangil, which merges the adjacent basic edits into single corrections.", "labels": [], "entities": []}, {"text": "MaxEntMerger We use the Maximum Entropy classifier to predict whether we should merge the two edits, as described in Section 3 4 . We evaluate extrinsically the merging components' effect on overall system performance by Our replication has a slightly lower error type selection accuracy on FCE (80.02%) than the figure reported by Swanson and Yamangil (82.5%).", "labels": [], "entities": [{"text": "error type selection accuracy", "start_pos": 258, "end_pos": 287, "type": "METRIC", "confidence": 0.6606242209672928}, {"text": "FCE", "start_pos": 291, "end_pos": 294, "type": "METRIC", "confidence": 0.4589722454547882}]}, {"text": "This small difference on error type selection does not affect our conclusions about correc-  We design experiments to answer two questions: 1.", "labels": [], "entities": []}, {"text": "Do the additional contextual information about correction patterns help guide the merging decisions?", "labels": [], "entities": []}, {"text": "How much does a classifier trained for this task improve the system's overall accuracy?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.997292697429657}]}, {"text": "2. How well does our method generalize over revisions from different sources?", "labels": [], "entities": []}, {"text": "Our major experimental results are presented in. compares the overall educational system's accuracies with different merging algorithms.", "labels": [], "entities": []}, {"text": "shows the system's F 1 score when trained and tested on different corpora.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9923344055811564}]}, {"text": "We make the following observations: First, shows that by incorporating correction patterns into the merging algorithm, the tion detection.errors in correction detection step were reduced.", "labels": [], "entities": []}, {"text": "This led to a significant improvement on the overall system's F 1 -score on all corpora.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9895986765623093}]}, {"text": "The improvement is most noticeable on FCE corpus, where the error in correction detection step was reduced by 9%.", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.95212122797966}, {"text": "error", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9695125222206116}, {"text": "correction detection", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.6828663945198059}]}, {"text": "That is, one third of the correction misdetections were eliminated.", "labels": [], "entities": [{"text": "correction misdetections", "start_pos": 26, "end_pos": 50, "type": "METRIC", "confidence": 0.9427460134029388}]}, {"text": "shows that the number of merging errors are significantly reduced by the new merging algorithm.", "labels": [], "entities": []}, {"text": "In particular, the number of false positives (system proposes merges when it should not) is significantly reduced.", "labels": [], "entities": []}, {"text": "Second, our proposed model is able to generalize over different corpora.", "labels": [], "entities": []}, {"text": "The models built on corpora can generally improve the correction detection accuracy 5 . Models built on the same corpus generally perform the best.", "labels": [], "entities": [{"text": "correction detection", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8155011832714081}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.7627376914024353}]}, {"text": "Also, as suggested by the experimental result, among the four corpora, FCE corpus is a comparably good resource for training correction detection models with our current feature set.", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.8649661540985107}, {"text": "correction detection", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.6944443583488464}]}, {"text": "One reason is that FCE corpus has many more training instances, which benefits model training.", "labels": [], "entities": [{"text": "FCE corpus", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8275206983089447}]}, {"text": "We tried varying the training dataset size, and test it on different corpora.", "labels": [], "entities": []}, {"text": "suggests that the model's accuracies increase with the training corpus size.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9884170889854431}]}], "tableCaptions": [{"text": " Table 2: Basic statistics of the corpora that we consider.", "labels": [], "entities": []}, {"text": " Table 3: Error type selection accuracies on different cor-", "labels": [], "entities": [{"text": "Error type selection accuracies", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.8113587200641632}]}, {"text": " Table  6. The models built on corpora can generally im- prove the correction detection accuracy 5 . Mod- els built on the same corpus generally perform  the best. Also, as suggested by the experimental  result, among the four corpora, FCE corpus is a  comparably good resource for training correction  detection models with our current feature set. One  reason is that FCE corpus has many more training  instances, which benefits model training. We tried  varying the training dataset size, and test it on dif- ferent corpora.", "labels": [], "entities": [{"text": "correction detection", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.7907110154628754}, {"text": "FCE corpus", "start_pos": 236, "end_pos": 246, "type": "DATASET", "confidence": 0.9058986008167267}, {"text": "correction  detection", "start_pos": 291, "end_pos": 312, "type": "TASK", "confidence": 0.7153554409742355}, {"text": "FCE corpus", "start_pos": 370, "end_pos": 380, "type": "DATASET", "confidence": 0.9135698974132538}]}, {"text": " Table 4: Extrinsic evaluation, where we plugged the two", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.848704993724823}]}, {"text": " Table 5: Intrinsic evaluation, where we evaluate the pro-", "labels": [], "entities": []}, {"text": " Table 6: Correction detection experiments by building the", "labels": [], "entities": [{"text": "Correction detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8728585243225098}]}]}