{"title": [{"text": "Predicting Grammaticality on an Ordinal Scale", "labels": [], "entities": [{"text": "Predicting Grammaticality", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9040162861347198}]}], "abstractContent": [{"text": "Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning).", "labels": [], "entities": [{"text": "identifying whether sentences are grammatical", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.7166839838027954}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7597608864307404}, {"text": "essay scoring", "start_pos": 142, "end_pos": 155, "type": "TASK", "confidence": 0.667179062962532}]}, {"text": "In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores).", "labels": [], "entities": []}, {"text": "We also present anew publicly available dataset of learner sentences judged for grammaticality on an ordinal scale.", "labels": [], "entities": []}, {"text": "In evaluations, we compare our system to the one from Post (2011) and find that our approach yields state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we develop a system for the task of predicting the grammaticality of sentences, and present a dataset of learner sentences rated for grammaticality.", "labels": [], "entities": [{"text": "predicting the grammaticality of sentences", "start_pos": 51, "end_pos": 93, "type": "TASK", "confidence": 0.844520914554596}]}, {"text": "Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7639931440353394}, {"text": "natural language generation", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.6809389988581339}, {"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.8019536435604095}]}, {"text": "It could also be used in educational applications such as essay scoring.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.8854678869247437}]}, {"text": "Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions), articles (), and collocations ().", "labels": [], "entities": [{"text": "predicting grammaticality", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.932229220867157}, {"text": "identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions), articles (), and collocations", "start_pos": 74, "end_pos": 255, "type": "Description", "confidence": 0.7316679507493973}]}, {"text": "While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation).", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9258586466312408}, {"text": "machine translation evaluation", "start_pos": 171, "end_pos": 201, "type": "TASK", "confidence": 0.845003346602122}]}, {"text": "Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (), such as the MT Quality Estimation Shared Tasks (, but relatively little on evaluating the grammaticality of naturally occurring text.", "labels": [], "entities": [{"text": "MT Quality Estimation Shared Tasks", "start_pos": 142, "end_pos": 176, "type": "TASK", "confidence": 0.7788021087646484}]}, {"text": "Also, most other research on evaluating grammaticality involves artificial tasks or datasets (.", "labels": [], "entities": []}, {"text": "Here, we make the following contributions.", "labels": [], "entities": []}, {"text": "\u2022 We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above.", "labels": [], "entities": [{"text": "predicting the grammaticality of sentences", "start_pos": 45, "end_pos": 87, "type": "TASK", "confidence": 0.8793232917785645}]}, {"text": "\u2022 We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal scale for grammaticality.", "labels": [], "entities": []}, {"text": "With this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality.", "labels": [], "entities": [{"text": "predicting sentence-level grammaticality", "start_pos": 132, "end_pos": 172, "type": "TASK", "confidence": 0.8377986152966818}]}], "datasetContent": [{"text": "We created a dataset consisting of 3,129 sentences randomly selected from essays written by nonnative speakers of English as part of a test of English language proficiency.", "labels": [], "entities": []}, {"text": "We oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences.", "labels": [], "entities": []}, {"text": "Two of the authors of this paper, both native speakers of English with linguistic training, annotated the data.", "labels": [], "entities": []}, {"text": "We refer to these annotators as expert judges.", "labels": [], "entities": []}, {"text": "When making judgments of the sentences, they saw the previous sentence from the same essay as context.", "labels": [], "entities": []}, {"text": "These two authors were not directly involved in development of the system in \u00a73.", "labels": [], "entities": []}, {"text": "Each sentence was annotated on a scale from 1 to 4 as described below, with 4 being the most grammatical.", "labels": [], "entities": []}, {"text": "We use an ordinal rather than binary scale, following previous work such as that of and who argue that the distinction between grammatical and ungrammatical is not simply binary.", "labels": [], "entities": []}, {"text": "Also, for practical applications, we believe that it is useful to distinguish sentences with minor errors from those with major errors that may disrupt communication.", "labels": [], "entities": []}, {"text": "Our annotation scheme was influenced by a translation rating scheme by.", "labels": [], "entities": []}, {"text": "Every sentence judged on the 1-4 scale must be a clause.", "labels": [], "entities": []}, {"text": "There is an extra category (\"Other\") for sentences that do not fit this criterion.", "labels": [], "entities": []}, {"text": "We exclude instances of \"Other\" in our experiments (see \u00a74).", "labels": [], "entities": []}, {"text": "Next, we present evaluations on the GUG dataset.", "labels": [], "entities": [{"text": "GUG dataset", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9536927044391632}]}], "tableCaptions": [{"text": " Table 1: Pearson's r on the development set, for  our full system and variations excluding each fea- ture type. \"\u2212 X\" indicates the full model without  the \"X\" features.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.937070369720459}]}, {"text": " Table 2: Human-machine agreement statistics for our system, the system from Post (2011), and simple  baselines, computed from the averages of human ratings in the testing set ( \u00a72). \"*\" in a Sig. column  indicates a statistically significant difference from \"our system\" (p < .05, see text for details). A majority  baseline for the binary task achieves 74.8% accuracy. The best results for each metric are in bold.", "labels": [], "entities": [{"text": "Post (2011)", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9219082593917847}, {"text": "accuracy", "start_pos": 361, "end_pos": 369, "type": "METRIC", "confidence": 0.998862624168396}]}]}