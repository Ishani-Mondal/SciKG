{"title": [{"text": "kLogNLP: Graph Kernel-based Relational Learning of Natural Language", "labels": [], "entities": [{"text": "Graph Kernel-based Relational Learning of Natural Language", "start_pos": 9, "end_pos": 67, "type": "TASK", "confidence": 0.7422295681067875}]}], "abstractContent": [{"text": "kLog is a framework for kernel-based learning that has already proven successful in solving a number of relational tasks in natural language processing.", "labels": [], "entities": []}, {"text": "In this paper , we present kLogNLP, a natural language processing module for kLog.", "labels": [], "entities": []}, {"text": "This module enriches kLog with NLP-specific preprocessors, enabling the use of existing libraries and toolkits within an elegant and powerful declarative machine learning framework.", "labels": [], "entities": []}, {"text": "The resulting relational model of the domain can be extended by specifying additional relational features in a declarative way using a logic programming language.", "labels": [], "entities": []}, {"text": "This declarative approach offers a flexible way of experimentation and away to insert domain knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "kLog ( ) is a logical and relational language for kernel-based learning.", "labels": [], "entities": []}, {"text": "It has already proven successful for several tasks in computer vision () and natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.6398664712905884}]}, {"text": "For example, in the case of binary sentence classification, we have shown an increase of 1.2 percent in F1-score on the best performing system in the CoNLL 2010 Shared Task on hedge cue detection (Wikipedia dataset) ().", "labels": [], "entities": [{"text": "binary sentence classification", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6320209205150604}, {"text": "F1-score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9995248317718506}, {"text": "CoNLL 2010 Shared Task on hedge cue detection", "start_pos": 150, "end_pos": 195, "type": "TASK", "confidence": 0.7966521233320236}, {"text": "Wikipedia dataset)", "start_pos": 197, "end_pos": 215, "type": "DATASET", "confidence": 0.8914577762285868}]}, {"text": "On a sentence labeling task for evidence-based medicine, a multi-class multi-label classification problem, kLog showed improved results over both the state-of-the-art CRF-based system of and a memory-based benchmark).", "labels": [], "entities": [{"text": "sentence labeling task", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.7863452633221945}, {"text": "multi-label classification", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7020061910152435}]}, {"text": "Also for spatial relation extraction from natural language, kLog has shown to provide a flexible relational representation to model the task domain (.", "labels": [], "entities": [{"text": "spatial relation extraction", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.6727697749932607}]}, {"text": "kLog has two distinguishing features.", "labels": [], "entities": []}, {"text": "First, it is able to transform relational into graph-based representations, which allows to incorporate structural features into the learning process.", "labels": [], "entities": []}, {"text": "Subsequently, kernel methods are used to work in an extended high-dimensional feature space, which is much richer than most of the direct propositionalisation approaches.", "labels": [], "entities": []}, {"text": "Second, it uses the logic programming language Prolog for defining and using (additional) background knowledge, which renders the model very interpretable and provides more insights into the importance of individual (structural) features.", "labels": [], "entities": []}, {"text": "These properties prove especially advantageous in the case of NLP.", "labels": [], "entities": []}, {"text": "The graphical approach of kLog is able to exploit the full relational representation that is often a natural way to express language structures, and in this way allows to fully exploit contextual features.", "labels": [], "entities": []}, {"text": "On top of this relational learning approach, the declarative feature specification allows to include additional background knowledge, which is often essential for solving NLP problems.", "labels": [], "entities": []}, {"text": "In this paper, we present kLogNLP 1 , an NLP module for kLog.", "labels": [], "entities": []}, {"text": "Starting from a dataset and a declaratively specified model of the domain (based on entity-relationship modeling from database theory), it transforms the dataset into a graph-based relational format.", "labels": [], "entities": []}, {"text": "We propose a general model that fits most tasks in NLP, which can be extended by specifying additional relational features in a declarative way.", "labels": [], "entities": []}, {"text": "The resulting relational representation then serves as input for kLog, and thus results in a full relational learning pipeline for NLP.", "labels": [], "entities": []}, {"text": "kLogNLP is most related to Learning-Based Java (LBJ) ( in that it offers a declarative pipeline for modeling and learning tasks in NLP.", "labels": [], "entities": []}, {"text": "The aims are similar, namely abstracting away the technical details from the programmer, and leaving him to reason about the modeling.", "labels": [], "entities": []}, {"text": "However, whereas LBJ focuses more on the learning side (by the specification of constraints on features which are reconciled at inference time, using the constrained conditional: General kLog workflow extended with the kLogNLP module model framework), due to its embedding in kLog, kLogNLP focuses on the relational modeling, in addition to declarative feature construction and feature generation using graph kernels.", "labels": [], "entities": [{"text": "declarative feature construction", "start_pos": 341, "end_pos": 373, "type": "TASK", "confidence": 0.7894508838653564}, {"text": "feature generation", "start_pos": 378, "end_pos": 396, "type": "TASK", "confidence": 0.7222184389829636}]}, {"text": "kLog in itself is related to several frameworks for relational learning, for which we refer the reader to.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized according to the general kLog workflow, preceded with the kLogNLP module, as outlined in.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss the modeling of the data, and present a general relational data model for NLP tasks.", "labels": [], "entities": []}, {"text": "Also the option to declaratively construct new features using logic programming is outlined.", "labels": [], "entities": []}, {"text": "In the subsequent parts, we will illustrate the remaining steps in the kLog pipeline, namely graphicalization and feature generation (Section 3), and learning (Section 4) in an NLP setting.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.6963562667369843}]}, {"text": "The last section draws conclusions and presents ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each interpretation can be regarded as a small relational database.", "labels": [], "entities": []}, {"text": "We will illustrate the extensional feature extraction step on the CoNLL-2010 dataset on hedge cue detection, a binary classification task where the goal is to detect uncertainty in sentences.", "labels": [], "entities": [{"text": "extensional feature extraction", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.6332239409287771}, {"text": "CoNLL-2010 dataset", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.9750555157661438}, {"text": "hedge cue detection", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.6202238400777181}]}, {"text": "This task is situated at the sentence level, so we left out the sentence and nextS signatures, as no context from other sentences was taken into account.", "labels": [], "entities": []}, {"text": "A part of a resulting interpretation is shown in Listing 3.", "labels": [], "entities": [{"text": "Listing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9562699794769287}]}], "tableCaptions": []}