{"title": [{"text": "Robust Entity Clustering via Phylogenetic Inference", "labels": [], "entities": [{"text": "Robust Entity Clustering", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9062554041544596}, {"text": "Phylogenetic Inference", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.5844190120697021}]}], "abstractContent": [{"text": "Entity clustering must determine when two named-entity mentions refer to the same entity.", "labels": [], "entities": [{"text": "Entity clustering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8550003170967102}]}, {"text": "Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data.", "labels": [], "entities": [{"text": "cross-document coreference resolution", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.7986369729042053}]}, {"text": "The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context.", "labels": [], "entities": [{"text": "generative process", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8950228989124298}]}, {"text": "Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process.", "labels": [], "entities": []}, {"text": "We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Variation poses a serious challenge for determining who or what a name refers to.", "labels": [], "entities": [{"text": "Variation", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9058630466461182}]}, {"text": "For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. President article, including: President Obama Barack H. Obama, Jr.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe experiments on three different datasets.", "labels": [], "entities": []}, {"text": "Our main results are described first: Twitter features many instances of name variation that we would like our model to be able to learn.", "labels": [], "entities": []}, {"text": "We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits.", "labels": [], "entities": []}, {"text": "We report additional experiments on the ACE 2008 corpus, and on apolitical blog corpus, to demonstrate that our approach is applicable in different settings.", "labels": [], "entities": [{"text": "ACE 2008 corpus", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9722850720087687}, {"text": "apolitical blog corpus", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.6634650627772013}]}, {"text": "For Twitter and ACE 2008, we report the standard B 3 metric ().", "labels": [], "entities": [{"text": "ACE 2008", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.9073963463306427}, {"text": "B 3 metric", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9379815856615702}]}, {"text": "For the political blog dataset, the reference does not consist of entity annotations, and so we follow the evaluation procedure of.", "labels": [], "entities": [{"text": "political blog dataset", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.671327531337738}]}], "tableCaptions": [{"text": " Table 1: Results for the Twitter dataset. Higher B 3 scores  are better. Note that each number is averaged over four  different test splits. In three out of four experiments,  PHYLO+TOPIC+MBR achieved the highest F1 score; in one  case PHYLO+TOPIC won by a small margin.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.8856774270534515}, {"text": "B 3", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9473192989826202}, {"text": "MBR", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.7742787599563599}, {"text": "F1 score", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9865038990974426}]}, {"text": " Table 2: Results for the ACE 2008 newswire dataset.", "labels": [], "entities": [{"text": "ACE 2008 newswire dataset", "start_pos": 26, "end_pos": 51, "type": "DATASET", "confidence": 0.958103358745575}]}]}