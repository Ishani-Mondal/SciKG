{"title": [{"text": "Bilingually-constrained Phrase Embeddings for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7485183775424957}]}], "abstractContent": [{"text": "We propose Bilingually-constrained Re-cursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings.", "labels": [], "entities": [{"text": "BRAE", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.7122359275817871}]}, {"text": "The BRAE is trained in away that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously.", "labels": [], "entities": [{"text": "BRAE", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9693986773490906}]}, {"text": "After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other.", "labels": [], "entities": []}, {"text": "We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.991629421710968}]}, {"text": "Extensive experiments show that the BRAE is remarkably effective in these two tasks.", "labels": [], "entities": [{"text": "BRAE", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9365593194961548}]}], "introductionContent": [{"text": "Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (.", "labels": [], "entities": []}, {"text": "Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment ( , translation confidence estimation (, phrase reordering prediction ( , translation modelling ( and language modelling ().", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.804282416899999}, {"text": "word alignment", "start_pos": 137, "end_pos": 151, "type": "TASK", "confidence": 0.7789431214332581}, {"text": "translation confidence estimation", "start_pos": 156, "end_pos": 189, "type": "TASK", "confidence": 0.6812044084072113}, {"text": "phrase reordering prediction", "start_pos": 193, "end_pos": 221, "type": "TASK", "confidence": 0.8515772620836893}, {"text": "translation modelling", "start_pos": 226, "end_pos": 247, "type": "TASK", "confidence": 0.977980762720108}, {"text": "language modelling", "start_pos": 254, "end_pos": 272, "type": "TASK", "confidence": 0.767726331949234}]}, {"text": "Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9958099126815796}]}, {"text": "However, in the conventional (phrase-based) SMT, phrases are the basic translation units.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.6619606018066406}]}, {"text": "The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules.", "labels": [], "entities": []}, {"text": "Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work.", "labels": [], "entities": []}, {"text": "In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector.", "labels": [], "entities": []}, {"text": "In some previous works, phrase embedding has been discussed from different views.", "labels": [], "entities": [{"text": "phrase embedding", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7462373077869415}]}, {"text": "(2011) make the phrase embeddings capture the sentiment information.", "labels": [], "entities": []}, {"text": "enable the phrase embeddings to mainly capture the syntactic knowledge.", "labels": [], "entities": []}, {"text": "attempt to encode the reordering pattern in the phrase embeddings.", "labels": [], "entities": []}, {"text": "utilize a simple convolution model to generate phrase embeddings from word embeddings.", "labels": [], "entities": []}, {"text": "consider a phrase as an indivisible n-gram.", "labels": [], "entities": []}, {"text": "Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram).", "labels": [], "entities": []}, {"text": "Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9807430505752563}]}, {"text": "Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that our phrase embedding can fully represent the phrase and best fit the phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.8138499855995178}]}, {"text": "Assuming the phrase is a meaningful composition of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings.", "labels": [], "entities": [{"text": "Bilinguallyconstrained Recursive Auto-encoders (BRAE", "start_pos": 82, "end_pos": 134, "type": "METRIC", "confidence": 0.53833988904953}]}, {"text": "The core idea behind is that a phrase and its correct translation should share the same semantic meaning.", "labels": [], "entities": []}, {"text": "Thus, they can supervise each other to learn their semantic phrase embeddings.", "labels": [], "entities": []}, {"text": "Similarly, non-translation pairs should have different semantic meanings, and this information can also be used to guide learning semantic phrase embeddings.", "labels": [], "entities": []}, {"text": "In our method, the standard recursive autoencoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error, while the bilingually-constrained model learns to finetune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs.", "labels": [], "entities": []}, {"text": "We use an example to explain our model.", "labels": [], "entities": []}, {"text": "As illustrated in, the Chinese phrase on the left and the English phrase on the right are translations with each other.", "labels": [], "entities": []}, {"text": "If we learn the embedding of the Chinese phrase correctly, we can regard it as the gold representation for the English phrase and use it to guide the process of learning English phrase embedding.", "labels": [], "entities": []}, {"text": "In the other direction, the Chinese phrase embedding can be learned in the same way.", "labels": [], "entities": []}, {"text": "This procedure can be performed with an co-training style algorithm so as to minimize the semantic distance between the translation equivalents . In this way, the result Chinese and English phrase embeddings will capture the semantics as much as possible.", "labels": [], "entities": []}, {"text": "Furthermore, a transformation function between the Chinese and English semantic spaces can be learned as well.", "labels": [], "entities": []}, {"text": "With the learned model, we can accurately measure the semantic similarity between a source phrase and a translation candidate.", "labels": [], "entities": []}, {"text": "Accordingly, we evaluate the BRAE model on two end-toend SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to check whether a translation candidate and the source phrase are in the same meaning.", "labels": [], "entities": [{"text": "BRAE", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9119796752929688}, {"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9807119369506836}]}, {"text": "In phrase table pruning, we discard the phrasal translation rules with low semantic similarity.", "labels": [], "entities": []}, {"text": "In decoding with phrasal semantic similarities, we apply the semantic similarities of the phrase pairs as new features during decoding to guide translation can-1 For simplicity, we do not show non-translation pairs here.", "labels": [], "entities": []}, {"text": "The experiments show that up to 72% of the phrase table can be discarded without significant decrease on the translation quality, and in decoding with phrasal semantic similarities up to 1.7 BLEU score improvement over the state-ofthe-art baseline can be achieved.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9990853071212769}]}, {"text": "In addition, our semantic phrase embeddings have many other potential applications.", "labels": [], "entities": []}, {"text": "For instance, the semantic phrase embeddings can be directly fed to DNN to model the decoding process.", "labels": [], "entities": []}, {"text": "Besides SMT, the semantic phrase embeddings can be used in other cross-lingual tasks (e.g. cross-lingual question answering) and monolingual applications such as textual entailment, question answering and paraphrase detection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9910333752632141}, {"text": "cross-lingual question answering)", "start_pos": 91, "end_pos": 124, "type": "TASK", "confidence": 0.6969731524586678}, {"text": "question answering", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.9244604110717773}, {"text": "paraphrase detection", "start_pos": 205, "end_pos": 225, "type": "TASK", "confidence": 0.9195119440555573}]}], "datasetContent": [{"text": "With the semantic phrase embeddings and the vector space transformation function, we apply the BRAE to measure the semantic similarity between a source phrase and its translation candidates in the phrase-based SMT.", "labels": [], "entities": [{"text": "BRAE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9966996312141418}, {"text": "SMT", "start_pos": 210, "end_pos": 213, "type": "TASK", "confidence": 0.7701141834259033}]}, {"text": "Two tasks are involved in the experiments: phrase table pruning that discards entries whose semantic similarity is very low and decoding with the phrasal semantic similarities as additional new features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison between BRAE-based pruning and Significance pruning of phrase table. Threshold  means similarity in BRAE and negative-log-p-value in Significance. \"ALL\" combines the development  and test sets. Bold numbers denote that the result is better than or comparable to that of baseline. n = 50  is used for embedding dimensionality.", "labels": [], "entities": [{"text": "Threshold", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9978487491607666}, {"text": "BRAE", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.7789198756217957}]}, {"text": " Table 2: Experimental results of decoding with phrasal semantic similarities. n is the embedding dimen- sionality. \"+\" means that the model significantly outperforms the baseline with p < 0.01.", "labels": [], "entities": []}]}