{"title": [{"text": "Improving Lexical Embeddings with Semantic Knowledge", "labels": [], "entities": [{"text": "Improving Lexical Embeddings", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.923115611076355}]}], "abstractContent": [{"text": "Word embeddings learned on unlabeled data area popular tool in semantics, but may not capture the desired semantics.", "labels": [], "entities": []}, {"text": "We propose anew learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embed-dings.", "labels": [], "entities": []}, {"text": "We demonstrate that our embed-dings improve over those learned solely on raw text in three settings: language mod-eling, measuring semantic similarity, and predicting human judgements.", "labels": [], "entities": [{"text": "predicting human judgements", "start_pos": 156, "end_pos": 183, "type": "TASK", "confidence": 0.8988038698832194}]}], "introductionContent": [{"text": "Word embeddings are popular representations for syntax), semantics, morphology () and other areas.", "labels": [], "entities": []}, {"text": "A long line of embeddings work, such as LSA and randomized embeddings, has recently turned to neural language models (.", "labels": [], "entities": []}, {"text": "Unsupervised learning can take advantage of large corpora, which can produce impressive results.", "labels": [], "entities": []}, {"text": "However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest.", "labels": [], "entities": []}, {"text": "Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms another.", "labels": [], "entities": []}, {"text": "Controlling this behavior is challenging with an unsupervised objective.", "labels": [], "entities": []}, {"text": "However, rich prior knowledge exists for many tasks, and there are numerous such semantic resources.", "labels": [], "entities": []}, {"text": "We propose anew training objective for learning word embeddings that incorporates prior * This work was done while the author was visiting JHU. knowledge.", "labels": [], "entities": [{"text": "JHU. knowledge", "start_pos": 139, "end_pos": 153, "type": "DATASET", "confidence": 0.9830414454142252}]}, {"text": "Our model builds on word2vec (), a neural network based language model that learns word embeddings by maximizing the probability of raw text.", "labels": [], "entities": []}, {"text": "We extend the objective to include prior knowledge about synonyms from semantic resources; we consider both the Paraphrase Database ( and WordNet, which annotate semantic relatedness between words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9318423867225647}]}, {"text": "The latter was also used in () for training a network for predicting synset relation.", "labels": [], "entities": [{"text": "predicting synset relation", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.8535537521044413}]}, {"text": "The combined objective maximizes both the probability of the raw corpus and encourages embeddings to capture semantic relations from the resources.", "labels": [], "entities": []}, {"text": "We demonstrate improvements in our embeddings on three tasks: language modeling, measuring word similarity, and predicting human judgements on word pairs.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7778568863868713}, {"text": "predicting human judgements on word pairs", "start_pos": 112, "end_pos": 153, "type": "TASK", "confidence": 0.8703141311804453}]}], "datasetContent": [{"text": "For training cbow we use the New York Times (NYT) 1994-97 subset from Gigaword v5.0 (Parker et al., 2011).", "labels": [], "entities": [{"text": "New York Times (NYT) 1994-97 subset", "start_pos": 29, "end_pos": 64, "type": "DATASET", "confidence": 0.7507188320159912}]}, {"text": "We select 1,000 paragraphs each for dev and test data from the December 2010 portion of the NYT.", "labels": [], "entities": [{"text": "dev and test data from the December 2010 portion of the NYT", "start_pos": 36, "end_pos": 95, "type": "DATASET", "confidence": 0.7039574384689331}]}, {"text": "Sentences are tokenized using OpenNLP 1 , yielding 518,103,942 tokens for training, 42,953 tokens for dev and 41,344 for test.", "labels": [], "entities": [{"text": "OpenNLP 1", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.9436310231685638}]}, {"text": "We consider two resources for training the RCM term: the Paraphrase Database (PPDB) () and WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9384876489639282}]}, {"text": "For each semantic pair extracted from these resources, we add a relation to the RCM objective.", "labels": [], "entities": []}, {"text": "Since we use both resources for evaluation, we divide each into train, dev and test.", "labels": [], "entities": []}, {"text": "PPDB is an automatically extracted dataset containing tens of millions of paraphrase pairs, including words and phrases.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8727023005485535}]}, {"text": "We used the \"lexical\" version of PPDB (no phrases) and filtered to include pairs that contained words found in the 200,000 most frequent words in the NYT corpus, which ensures each word in the relations had support in the text corpus.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 150, "end_pos": 160, "type": "DATASET", "confidence": 0.9603507816791534}]}, {"text": "Next, we removed duplicate pairs: if <A,B> occurred in PPDB, we removed relations of <B,A>.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.879660964012146}]}, {"text": "PPDB is organized 1 https://opennlp.apache.org/  into 6 parts, ranging from S (small) to XXXL.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8151015043258667}]}, {"text": "Division into these sets is based on an automatically derived accuracy metric.", "labels": [], "entities": [{"text": "Division", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9592154622077942}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9972054362297058}]}, {"text": "Since S contains the most accurate paraphrases, we used these for evaluation.", "labels": [], "entities": []}, {"text": "We divided S into a dev set (1582 pairs) and test set (1583 pairs).", "labels": [], "entities": []}, {"text": "Training was based on one of the other sets minus relations from S.", "labels": [], "entities": []}, {"text": "We created similar splits using WordNet, extracting synonyms using the 100,000 most frequent NYT words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9714018702507019}, {"text": "NYT", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.7962708473205566}]}, {"text": "We divide the vocabulary into three sets: the most frequent 10,000 words, words with ranks between 10,001-30,000 and 30,001-100,000.", "labels": [], "entities": []}, {"text": "We sample 500 words from each set to construct a dev and test set.", "labels": [], "entities": []}, {"text": "For each word we sample one synonym to form a pair.", "labels": [], "entities": []}, {"text": "The remaining words and their synonyms are used for training.", "labels": [], "entities": []}, {"text": "However we did not use the training data because it is too small to affect the results.", "labels": [], "entities": []}, {"text": "The goal of our experiments is to demonstrate the value of learning semantic embeddings with information from semantic resources.", "labels": [], "entities": []}, {"text": "In each setting, we will compare the word2vec baseline embedding trained with cbow against RCM alone, the joint model and Joint\u2192RCM.", "labels": [], "entities": []}, {"text": "We consider three evaluation tasks: language modeling, measuring semantic similarity, and predicting human judgements on semantic relatedness.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7953713238239288}, {"text": "predicting human judgements on semantic relatedness", "start_pos": 90, "end_pos": 141, "type": "TASK", "confidence": 0.8618757824103037}]}, {"text": "In all of our experiments, we conducted model development and tuned model parameters (C, \u03b1 cbow , \u03b1 RCM , PPDB dataset, etc.) on development data, and evaluate the best performing model on test data.", "labels": [], "entities": [{"text": "PPDB dataset", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.8730186223983765}]}, {"text": "The models are notated as follows: word2vec for the baseline objective (cbow or skip-gram), RCM-r/p and Joint-r/p for random and pre-trained initializations of the RCM and Joint objectives, and Joint\u2192RCM for pre-training RCM with Joint embeddings.", "labels": [], "entities": []}, {"text": "Unless otherwise notes, we train using PPDB XXL.", "labels": [], "entities": [{"text": "PPDB XXL", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8411550521850586}]}, {"text": "We initially created WordNet training data, but found it too small to affect results.", "labels": [], "entities": [{"text": "WordNet training data", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.9183748563130697}]}, {"text": "Therefore, we include only RCM results trained on PPDB, but show evaluations on both PPDB and WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9689653515815735}]}, {"text": "We trained 200-dimensional embeddings and used output embeddings for measuring similarity.", "labels": [], "entities": []}, {"text": "During the training of cbow objectives we remove all words with frequencies less than 5, which is the default setting of word2vec.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: LM evaluation on held out NYT data.", "labels": [], "entities": [{"text": "LM", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.920950174331665}, {"text": "NYT data", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8490791618824005}]}, {"text": " Table 3: MRR for semantic similarity on PPDB and WordNet dev and test data. Higher is better. All  RCM objectives are trained with PPDB XXL. To preserve test data integrity, only the best performing  setting of each model is evaluated on the test data.", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9965991377830505}, {"text": "WordNet dev", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.885624349117279}]}, {"text": " Table 4: Results for ranking the quality of PPDB  pairs as compared to human judgements.", "labels": [], "entities": []}, {"text": " Table 5: MRR on PPDB dev data for training on  an increasing number of relations.", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.4634639322757721}, {"text": "PPDB dev data", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.8614313999811808}]}, {"text": " Table 6: Effect of learning rate \u03b1 RCM on MRR for  the RCM objective in Joint models.", "labels": [], "entities": [{"text": "MRR", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.5335438251495361}]}]}