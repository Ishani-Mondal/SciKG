{"title": [{"text": "Exponential Reservoir Sampling for Streaming Language Models", "labels": [], "entities": [{"text": "Exponential Reservoir Sampling", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6503402690092722}]}], "abstractContent": [{"text": "We show how rapidly changing textual streams such as Twitter can be modelled in fixed space.", "labels": [], "entities": []}, {"text": "Our approach is based upon a randomised algorithm called Exponential Reservoir Sampling, unexplored by this community until now.", "labels": [], "entities": [{"text": "Exponential Reservoir Sampling", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.5307014187177023}]}, {"text": "Using language models over Twitter and Newswire as a testbed, our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present.", "labels": [], "entities": []}], "introductionContent": [{"text": "Work by, Van Durme and Lall (2009) and considered the problem of building very large language models via the use of randomized data structures known as sketches.", "labels": [], "entities": []}, {"text": "1 While efficient, these structures still scale linearly in the number of items stored, and do not handle deletions well: if processing an unbounded stream of text, with new words and phrases being regularly added to the model, then with a fixed amount of space, errors will only increase overtime.", "labels": [], "entities": []}, {"text": "This was pointed out by, who investigated an alternate approach employing perfecthashing to allow for deletions overtime.", "labels": [], "entities": []}, {"text": "Their deletion criterion was task-specific and based on how a machine translation system queried a language model.", "labels": [], "entities": [{"text": "machine translation system queried a language model", "start_pos": 62, "end_pos": 113, "type": "TASK", "confidence": 0.819352422441755}]}, {"text": "Here we ask what the appropriate selection criterion is for streaming data based on a nonstationary process, when concerned with an intrinsic measure such as perplexity.", "labels": [], "entities": []}, {"text": "Using Twitter and newswire, we pursue this via a sampling strategy: we construct models over sentences based on a sample of previously observed sentences, then measure perplexity of incoming sentences, all on a day by day, rolling basis.", "labels": [], "entities": []}, {"text": "Three sampling approaches are considered: A fixed-width sliding window of most recent content, uniformly at random over the stream and a biased sample that prefers recent history over the past.", "labels": [], "entities": []}, {"text": "We show experimentally that a moving window is better than uniform sampling, and further that exponential (biased) sampling is best of all.", "labels": [], "entities": []}, {"text": "For streaming data, recently encountered data is valuable, but there is also signal in the previous stream.", "labels": [], "entities": []}, {"text": "Our sampling methods are based on reservoir sampling, a popularly known method in some areas of computer science, but which has seen little use within computational linguistics.", "labels": [], "entities": []}, {"text": "Standard reservoir sampling is a method for maintaining a uniform sample over a dynamic stream of elements, using constant space.", "labels": [], "entities": []}, {"text": "Novel to this community, we consider a variant owing to which provides for an exponential bias towards recently observed elements.", "labels": [], "entities": []}, {"text": "This exponential reservoir sampling has all of the guarantees of standard reservoir sampling, but as we show, is a better fit for streaming textual data.", "labels": [], "entities": []}, {"text": "Our approach is fully general and can be applied to any streaming task where we need to model the present and can only use fixed space.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments use two streams of data to illustrate exponential sampling: Twitter and a more conventional newswire stream.", "labels": [], "entities": []}, {"text": "The Twitter data is interesting as it is very multilingual, bursty (for example, it talks about memes, breaking news, gossip etc) and written by literally millions of different people.", "labels": [], "entities": []}, {"text": "The newswire stream is a lot more well behaved and serves as a control.", "labels": [], "entities": [{"text": "newswire stream", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9191098213195801}]}, {"text": "We used one month of chronologically ordered Twitter data and divided it into 31 equal sized blocks (roughly corresponding with days).", "labels": [], "entities": []}, {"text": "We also used the AFP portion of the Giga Word corpus as another source of data that evolves at a slower pace.", "labels": [], "entities": [{"text": "AFP portion of the Giga Word corpus", "start_pos": 17, "end_pos": 52, "type": "DATASET", "confidence": 0.8873521941048759}]}, {"text": "This data was divided into 50 equal sized blocks.", "labels": [], "entities": []}, {"text": "gives statistics about the data.", "labels": [], "entities": []}, {"text": "As can be seen, the Twitter data is vastly larger than newswire and arrives at a much faster rate.", "labels": [], "entities": []}, {"text": "We considered the following models.", "labels": [], "entities": []}, {"text": "Each one (apart from the exact model) was trained using the same amount of data: \u2022 Static.", "labels": [], "entities": []}, {"text": "This model was trained using data from the start of the duration and never varied.", "labels": [], "entities": []}, {"text": "This model was trained using all available data from the start of the stream and acts as an upper bound on performance.", "labels": [], "entities": []}, {"text": "This model used all data in a fixed-sized window immediately before the given test point.", "labels": [], "entities": []}, {"text": "Here, we use uniform reservoir sampling to select the data.", "labels": [], "entities": []}, {"text": "Lastly, we use exponential reservoir sampling to select the data.", "labels": [], "entities": []}, {"text": "This model is parameterised, indicating how strongly biased towards the present the sample will be.", "labels": [], "entities": []}, {"text": "The \u03b2 parameter is a multiplier over the reservoir length.", "labels": [], "entities": []}, {"text": "For example, a \u03b2 value of 1.1 with a sample size of 10 means the value is 11.", "labels": [], "entities": []}, {"text": "In general, \u03b2 always needs to be bigger than the reservoir size.", "labels": [], "entities": []}, {"text": "We sample over whole sentences (or Tweets) and not ngrams.", "labels": [], "entities": []}, {"text": "Using ngrams instead would give us a finer-grained control over results, but would come at the expense of greatly complicating the analysis.", "labels": [], "entities": []}, {"text": "This is because we would need to reason about not just a set of items but a multiset of items.", "labels": [], "entities": []}, {"text": "Note that because the samples are large   We test the model on unseen data from all of the next day (or block).", "labels": [], "entities": []}, {"text": "Afterwards, we advance to the next day (block) and repeat, potentially incorporating the previously seen test data into the current training data.", "labels": [], "entities": []}, {"text": "Evaluation is in terms of perplexity (which is standard for language modelling).", "labels": [], "entities": []}, {"text": "We used KenLM for building models and evaluating them.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.7650552988052368}]}, {"text": "Each model was an unpruned trigram, with Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "Increasing the language model order would not change the results.", "labels": [], "entities": []}, {"text": "Here the focus is upon which data is used in a model (that is, which data is added and which data is removed) and not upon making it compactor making retraining efficient.", "labels": [], "entities": []}, {"text": "shows the effect of varying the \u03b2 parameter (using Twitter).", "labels": [], "entities": [{"text": "\u03b2", "start_pos": 32, "end_pos": 33, "type": "METRIC", "confidence": 0.9467400312423706}]}, {"text": "The higher the \u03b2 value, the more uniform the sampling.", "labels": [], "entities": [{"text": "\u03b2", "start_pos": 15, "end_pos": 16, "type": "METRIC", "confidence": 0.9575132727622986}]}, {"text": "As can be seen, performance improves when sampling becomes more biased.", "labels": [], "entities": []}, {"text": "Not shown here, but for Twitter, even smaller \u03b2 values produce better results and for newswire, results degrade.", "labels": [], "entities": []}, {"text": "These differences are small and do not affect any conclusions made here.", "labels": [], "entities": []}, {"text": "In practise, this value would beset using a development set and to simplify the rest of the paper, all other experiments use the same \u03b2 value (1.1).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Perplexities for different sample sizes  over Twitter. Lower is better.", "labels": [], "entities": []}, {"text": " Table 4: Perplexities for differently selected sam- ples over Twitter (sample size = five days, \u03b2 =  1.1). Results in bold are the best sampling results.  Lower is better.", "labels": [], "entities": []}, {"text": " Table 5: Perplexities for differently selected sam- ples over Gigaword (sample size = 10 blocks, \u03b2 =  1.1). Lower is better.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.879035472869873}]}]}