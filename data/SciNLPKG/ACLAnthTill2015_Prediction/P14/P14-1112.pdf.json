{"title": [{"text": "Joint Syntactic and Semantic Parsing with Combinatory Categorial Grammar", "labels": [], "entities": [{"text": "Syntactic and Semantic Parsing", "start_pos": 6, "end_pos": 36, "type": "TASK", "confidence": 0.5897897481918335}]}], "abstractContent": [{"text": "We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision.", "labels": [], "entities": []}, {"text": "The trained parser produces a full syntactic parse of any sentence, while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary.", "labels": [], "entities": []}, {"text": "We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology.", "labels": [], "entities": []}, {"text": "A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach.", "labels": [], "entities": []}, {"text": "A syntactic evaluation on CCGbank demonstrates that the parser's dependency F-score is within 2.5% of state-of-the-art.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8196313381195068}]}], "introductionContent": [{"text": "Integrating syntactic parsing with semantics has long been a goal of natural language processing and is expected to improve both syntactic and semantic processing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7391647398471832}]}, {"text": "For example, semantics could help predict the differing prepositional phrase attachments in \"I caught the butterfly with the net\" and \"I caught the butterfly with the spots.\"", "labels": [], "entities": []}, {"text": "A joint analysis could also avoid propagating syntactic parsing errors into semantic processing, thereby improving performance.", "labels": [], "entities": []}, {"text": "We suggest that a large populated knowledge base should play a key role in syntactic and semantic parsing: in training the parser, in resolving syntactic ambiguities when the trained parser is applied to new text, and in its output semantic representation.", "labels": [], "entities": [{"text": "syntactic and semantic parsing", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.6450141668319702}]}, {"text": "Using semantic information from the knowledge base at training and test time will ideally improve the parser's ability to solve difficult syntactic parsing problems, as in the examples above.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.6911213099956512}]}, {"text": "A semantic representation tied to a knowledge base allows for powerful inference operations -such as identifying the possible entity referents of a noun phrase -that cannot be performed with shallower representations (e.g., frame semantics () or a direct conversion of syntax to logic).", "labels": [], "entities": []}, {"text": "This paper presents an approach to training a joint syntactic and semantic parser using a large background knowledge base.", "labels": [], "entities": []}, {"text": "Our parser produces a full syntactic parse of every sentence, and furthermore produces logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary.", "labels": [], "entities": []}, {"text": "For example, given a phrase like \"my favorite town in California,\" our parser will assign a logical form like \u03bbx.CITY(x) \u2227 LOCATEDIN(x, CALIFORNIA) to the \"town in California\" portion.", "labels": [], "entities": [{"text": "LOCATEDIN", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.988175630569458}, {"text": "CALIFORNIA", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.7394258975982666}]}, {"text": "Additionally, the parser uses predicate and entity type information during parsing to select a syntactic parse.", "labels": [], "entities": []}, {"text": "Our parser is trained by combining a syntactic parsing task with a distantly-supervised relation extraction task.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7749963204065958}]}, {"text": "Syntactic information is provided by CCGbank, a conversion of the Penn Treebank into the CCG formalism).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9958683550357819}]}, {"text": "Semantics are learned by training the parser to extract knowledge base relation instances from a corpus of unlabeled sentences, in a distantly-supervised training regime.", "labels": [], "entities": []}, {"text": "This approach uses the knowledge base to avoid expensive manual labeling of individual sentence semantics.", "labels": [], "entities": []}, {"text": "By optimizing the parser to perform both tasks simultaneously, we train a parser that produces accurate syntactic and semantic analyses.", "labels": [], "entities": []}, {"text": "We demonstrate our approach by training a joint syntactic and semantic parser, which we call ASP.", "labels": [], "entities": []}, {"text": "ASP produces a full syntactic analysis of every sentence while simultaneously producing logical forms containing any of 61 category and 69 re-lation predicates from NELL.", "labels": [], "entities": []}, {"text": "Experiments with ASP demonstrate that jointly analyzing syntax and semantics improves semantic parsing performance over comparable prior work and a pipelined syntax-then-semantics approach.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7483934164047241}]}, {"text": "ASP's syntactic parsing performance is within 2.5% of state-ofthe-art; however, we also find that incorporating semantic information reduces syntactic parsing accuracy by \u223c 0.5%.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.5466681569814682}, {"text": "syntactic parsing", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.661423534154892}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.8678805828094482}]}], "datasetContent": [{"text": "The experiments below evaluate ASP's syntactic and semantic parsing ability.", "labels": [], "entities": [{"text": "ASP", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9792977571487427}, {"text": "semantic parsing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.6560045033693314}]}, {"text": "The parser is trained on CCGbank and a corpus of Wikipedia sentences, using NELL's predicate vocabulary.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9507908225059509}]}, {"text": "The syntactic analyses of the trained parser are evaluated against CCGbank, and its logical forms are evaluated on an information extraction task and against an annotated test set of Wikipedia sentences.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9511712789535522}, {"text": "information extraction task", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.7868374983469645}]}, {"text": "The syntactic evaluation measures ASP's ability to reproduce the predicate-argument dependencies in CCGbank.", "labels": [], "entities": [{"text": "ASP", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9335484504699707}]}, {"text": "As in previous work, our evaluation uses labeled and unlabeled dependencies.", "labels": [], "entities": []}, {"text": "Labeled dependencies are dependency structures with both words and semantic types removed, leaving two word indexes, a syntactic category, and an argument number.", "labels": [], "entities": []}, {"text": "Unlabeled dependencies further eliminate the syntactic category and argument number, leaving a pair of word indexes.", "labels": [], "entities": []}, {"text": "Performance is measured using precision, recall, and F-measure against the annotated dependency structures in CCGbank.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9995895028114319}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9994698166847229}, {"text": "F-measure", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9993742108345032}, {"text": "CCGbank", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9487388730049133}]}, {"text": "Precision is the fraction of predicted dependencies which are in CCGbank, recall is the fraction of CCGbank dependencies produced by the parser, and F-measure is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9909257888793945}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.999213457107544}, {"text": "F-measure", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9944145679473877}, {"text": "precision", "start_pos": 183, "end_pos": 192, "type": "METRIC", "confidence": 0.9989795088768005}, {"text": "recall", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.993374764919281}]}, {"text": "For comparison, we also trained a syntactic version of our parser, ASP-SYN, using only the CCGbank lexicon and grammar.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9603785276412964}]}, {"text": "Comparing against this parser lets us measure the effect of the relation extraction task on syntactic parsing.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7760035693645477}, {"text": "syntactic parsing", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7504227757453918}]}, {"text": "shows the results of our evaluation.", "labels": [], "entities": []}, {"text": "For comparison, we include results for two existing syntactic CCG parsers: C&C, the current state-of-the-art CCG parser, and the next best system.", "labels": [], "entities": []}, {"text": "Both ASP and ASP-SYN perform reasonably well, within 2.5% of the performance of C&C at the same coverage level.", "labels": [], "entities": []}, {"text": "However, ASP-: Logical form accuracy and extraction precision/recall on the annotated test set.", "labels": [], "entities": [{"text": "ASP", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8405888676643372}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9372174739837646}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.7182573080062866}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9894188046455383}]}, {"text": "The high extraction recall for ASP shows that it produces more complete logical forms than either baseline.", "labels": [], "entities": [{"text": "extraction recall", "start_pos": 9, "end_pos": 26, "type": "METRIC", "confidence": 0.8362480103969574}, {"text": "ASP", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8912904262542725}]}, {"text": "SYN outperforms ASP by around 0.5%, suggesting that ASP's additional semantic knowledge slightly hurts syntactic parsing performance.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.6467068642377853}]}, {"text": "This performance loss appears to be largely due to poor entity mention detection, as we found that not using entity mention lexicon entries attest time improves ASP's labeled and unlabeled F-scores by 0.3% on Section 00.", "labels": [], "entities": [{"text": "entity mention detection", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7556577523549398}, {"text": "F-scores", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.822430431842804}, {"text": "Section 00", "start_pos": 209, "end_pos": 219, "type": "DATASET", "confidence": 0.9142177104949951}]}, {"text": "The knowledge base contains many infrequently-mentioned entities with common names; these entities contribute incorrect semantic type information that confuses the parser.", "labels": [], "entities": []}, {"text": "We performed two semantic evaluations to better understand ASP's ability to construct logical forms.", "labels": [], "entities": [{"text": "ASP", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9418447613716125}]}, {"text": "The first evaluation emphasizes precision over recall, and the second evaluation accurately measures recall using a manually labeled test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9993367791175842}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9981270432472229}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.998468816280365}]}, {"text": "The information extraction evaluation uses each system to extract logical forms from a large corpus of sentences, then measures the fraction of extracted logical forms that are correct.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8302054703235626}]}, {"text": "The test set consists of 8.5k sentences sampled from the held-out Wikipedia sentences.", "labels": [], "entities": []}, {"text": "Each system was run on this data set, extracting all logical forms from each sentence that entailed at least one category or relation instance.", "labels": [], "entities": []}, {"text": "We ranked these extractions using the parser's inside chart score, then manually annotated a sample of 250 logical forms from each system for correctness.", "labels": [], "entities": []}, {"text": "Logical forms were marked correct if all category and relation instances entailed by the logical form were expressed by the sentence.", "labels": [], "entities": []}, {"text": "Note that a correct logical form need not entail all of the relations expressed by the sentence, reflecting an emphasis on precision over recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9983060359954834}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9887714982032776}]}, {"text": "shows some example logical forms produced by ASP in the evaluation.", "labels": [], "entities": []}, {"text": "The annotated sample of logical forms allows us to estimate precision for each system as a function of the number of correct extractions ().", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9985010623931885}]}, {"text": "The number of correct extractions is directly proportional to recall, and was estimated from the total number of extractions and precision at each rank in the sample.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9993531107902527}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9992352724075317}]}, {"text": "All three systems initially have high precision, implying that their extracted logical forms express facts found in the sentence.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9976121187210083}]}, {"text": "However, ASP produces 3 times more correct logical forms than either baseline because it jointly analyzes syntax and semantics.", "labels": [], "entities": [{"text": "ASP", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9081600308418274}]}, {"text": "The baselines suffer from reduced recall because they depend on receiving an accurate syntactic parse as input; syntactic parsing errors cause these systems to fail.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9993201494216919}]}, {"text": "Examining the incorrect logical forms produced by ASP reveals that incorrect mention detection is by far the most common source of mistakes.", "labels": [], "entities": [{"text": "ASP", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9337118268013}, {"text": "incorrect mention detection", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6050793826580048}]}, {"text": "Approximately 50% of errors are caused by marking common nouns as entity mentions (e.g., marking \"coin\" as a COMPANY).", "labels": [], "entities": []}, {"text": "These errors occur because the knowledge base contains many infrequently mentioned entities with relatively common names.", "labels": [], "entities": []}, {"text": "Another 30% of errors are caused by assigning an incorrect type to a common proper noun (e.g, marking \"Bolivia\" as a CITY).", "labels": [], "entities": [{"text": "errors", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9789230823516846}]}, {"text": "This analysis suggests that performing entity linking before parsing could significantly reduce errors.", "labels": [], "entities": [{"text": "entity linking before parsing", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7075367271900177}]}, {"text": "A limitation of the previous evaluation is that it does not measure the completeness of predicted logical forms, nor estimate what portion of sentences are left unanalyzed.", "labels": [], "entities": []}, {"text": "We conducted a second evaluation to measure these quantities.", "labels": [], "entities": []}, {"text": "The data for this evaluation consists of sentences annotated with logical forms for subspans.", "labels": [], "entities": []}, {"text": "We manually annotated Wikipedia sentences from the held-out set with logical forms for the largest subspans for which a logical form existed.", "labels": [], "entities": []}, {"text": "To avoid trivial cases, we only annotated logical forms containing at least one category or relation predicate and at least one mention.", "labels": [], "entities": []}, {"text": "We also chose not to annotate mentions of entities that are not in the knowledge base, as no system would be able to correctly identify them.", "labels": [], "entities": []}, {"text": "The corpus contains 97 sentences with 100 annotated logical forms.", "labels": [], "entities": []}, {"text": "We measured performance using two metrics: logical form accuracy, and extraction precision/recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.7225126028060913}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.6272299885749817}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.835709810256958}]}, {"text": "Logical form accuracy examines the predicted logical form for the smallest subspan of the sentence containing the annotated span, and marks this prediction correct if it exactly matches the annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9389755129814148}]}, {"text": "A limitation of this metric is that it does not assign partial credit to logical forms that are close to, but do not exactly match, the annotation.", "labels": [], "entities": []}, {"text": "The extraction metric assigns partial credit by computing the precision and recall of the category and relation instances entailed by the predicted logical form, using those entailed by the annotated logical form as the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.999321460723877}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9910634756088257}]}, {"text": "shows the computation of both error metrics on two examples from the test corpus.", "labels": [], "entities": []}, {"text": "shows the results of the annotated sentence evaluation.", "labels": [], "entities": []}, {"text": "ASP outperforms both baselines in logical form accuracy and extraction recall, suggesting that it produces more complete analyses than either baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9596620798110962}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9618803858757019}]}, {"text": "The extraction precision of 90% suggests that ASP rarely extracts incorrect information.", "labels": [], "entities": [{"text": "extraction", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9702187180519104}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.7528096437454224}, {"text": "ASP", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9888650178909302}]}, {"text": "Precision is higher in this evaluation because every sentence in the data set has at least one correct extraction.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9931911826133728}]}], "tableCaptions": [{"text": " Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using  precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7384761869907379}, {"text": "Section 23 of CCGbank", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.6283048391342163}, {"text": "precision (P)", "start_pos": 101, "end_pos": 114, "type": "METRIC", "confidence": 0.9553036689758301}, {"text": "recall (R)", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9549068063497543}, {"text": "F-measure (F)", "start_pos": 131, "end_pos": 144, "type": "METRIC", "confidence": 0.9652957171201706}]}, {"text": " Table 3: Logical form accuracy and extraction pre- cision/recall on the annotated test set. The high  extraction recall for ASP shows that it produces  more complete logical forms than either baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9717487096786499}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9578356742858887}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.49700790643692017}]}]}