{"title": [{"text": "XMEANT: Better semantic MT evaluation without reference translations", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9165005087852478}]}], "abstractContent": [{"text": "We introduce XMEANT-a new cross-lingual version of the semantic frame based MT evaluation metric MEANT-which can correlate even more closely with human adequacy judgments than monolingual MEANT and eliminates the need for expensive human references.", "labels": [], "entities": [{"text": "MT evaluation metric MEANT-which", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.7508362978696823}]}, {"text": "Previous work established that MEANT reflects translation adequacy with state-of-the-art accuracy, and optimizing MT systems against MEANT robustly improves translation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9983211159706116}, {"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9574648141860962}]}, {"text": "However, to go beyond tuning weights in the loglinear SMT model, a cross-lingual objective function that can deeply integrate semantic frame criteria into the MT training pipeline is needed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9807054400444031}, {"text": "MT training", "start_pos": 159, "end_pos": 170, "type": "TASK", "confidence": 0.9088355600833893}]}, {"text": "We show that cross-lingual XMEANT out-performs monolingual MEANT by (1) replacing the monolingual context vector model in MEANT with simple translation probabilities, and (2) incorporating bracketing ITG constraints .", "labels": [], "entities": []}], "introductionContent": [{"text": "We show that XMEANT, anew cross-lingual version of MEANT (, correlates with human judgment even more closely than MEANT for evaluating MT adequacy via semantic frames, despite discarding the need for expensive human reference translations.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.6068224906921387}, {"text": "MT adequacy via semantic frames", "start_pos": 135, "end_pos": 166, "type": "TASK", "confidence": 0.8231185793876648}]}, {"text": "XMEANT is obtained by (1) using simple lexical translation probabilities, instead of the monolingual context vector model used in MEANT for computing the semantic role fillers similarities, and (2) incorporating bracketing ITG constrains for word alignment within the semantic role fillers.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 242, "end_pos": 256, "type": "TASK", "confidence": 0.7467615604400635}]}, {"text": "We conjecture that the reason that XMEANT correlates more closely with human adequacy judgement than MEANT is that on the one hand, the semantic structure of the MT output is closer to that of the input sentence than that of the reference translation, and on the other hand, the BITG constraints the word alignment more accurately than the heuristic bag-ofword aggregation used in MEANT.", "labels": [], "entities": [{"text": "BITG", "start_pos": 279, "end_pos": 283, "type": "METRIC", "confidence": 0.9506164193153381}, {"text": "word alignment", "start_pos": 300, "end_pos": 314, "type": "TASK", "confidence": 0.69754658639431}]}, {"text": "Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations.", "labels": [], "entities": [{"text": "MT translation", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.9622193276882172}]}, {"text": "The MEANT family of metrics () adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: \"who did what to whom, when, where and why\").", "labels": [], "entities": [{"text": "MEANT", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.6533098220825195}]}, {"text": "MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7686992883682251}, {"text": "MT output", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.8358940184116364}]}, {"text": "It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9434854984283447}, {"text": "MT evaluation", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.9066897034645081}]}, {"text": "In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9576012492179871}, {"text": "BLEU", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.998006284236908}, {"text": "TER", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.9618428945541382}, {"text": "MEANT", "start_pos": 217, "end_pos": 222, "type": "METRIC", "confidence": 0.9818598628044128}]}, {"text": "In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9820924997329712}, {"text": "MT training", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.8926850855350494}]}, {"text": "We therefore propose XMEANT, a cross-lingual MT evaluation metric, that modifies MEANT using (1) simple translation probabilities (in our experiments, from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.8759442567825317}, {"text": "BITGs", "start_pos": 262, "end_pos": 267, "type": "METRIC", "confidence": 0.8726731538772583}]}, {"text": "We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language.", "labels": [], "entities": [{"text": "MT adequacy", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.825195848941803}, {"text": "MEANT", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.991436779499054}]}], "datasetContent": [{"text": "Surface-form oriented metrics such as BLEU), NIST), METEOR (Banerjee and), CDER (), WER (), and TER () do not correctly reflect the meaning similarities of the input sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9984523057937622}, {"text": "NIST", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7558842897415161}, {"text": "METEOR", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9949111342430115}, {"text": "WER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9686098694801331}, {"text": "TER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9953610301017761}]}, {"text": "In fact, a number of large scale meta-evaluations) report cases where BLEU strongly disagrees with human judgments of translation adequacy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9912324547767639}]}, {"text": "This has caused a recent surge of work to develop better ways to automatically measure MT adequacy.", "labels": [], "entities": [{"text": "MT adequacy", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.8439719974994659}]}, {"text": "improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by, but did not achieve higher correlation with human adequacy judgments than metrics like METEOR.", "labels": [], "entities": []}, {"text": "TINE () is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments.", "labels": [], "entities": [{"text": "TINE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9576906561851501}, {"text": "recall-oriented", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9960185885429382}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9991391897201538}, {"text": "METEOR", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9887797832489014}]}, {"text": "ULC ( incorporates several semantic features and shows improved correlation with human judgement on translation quality but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time.", "labels": [], "entities": [{"text": "SMT", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.9913956522941589}]}, {"text": "Similarly, SPEDE () predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model.", "labels": [], "entities": []}, {"text": "Sagan) is a semantic textual similarity metric based on a complex textual entailment pipeline.", "labels": [], "entities": [{"text": "Sagan)", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8779313564300537}]}, {"text": "These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9399036169052124}, {"text": "MT development", "start_pos": 288, "end_pos": 302, "type": "TASK", "confidence": 0.9105962812900543}]}], "tableCaptions": [{"text": " Table 1: Sentence-level correlation with HAJ  (GALE phase 2.5 evaluation data)", "labels": [], "entities": [{"text": "HAJ", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.4057856500148773}, {"text": "GALE phase 2.5 evaluation data", "start_pos": 48, "end_pos": 78, "type": "DATASET", "confidence": 0.786117672920227}]}]}