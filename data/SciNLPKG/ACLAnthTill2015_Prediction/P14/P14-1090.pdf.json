{"title": [{"text": "Information Extraction over Structured Data: Question Answering with Freebase Center for Language and Speech Processing", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7610864341259003}, {"text": "Question Answering", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8276487588882446}]}], "abstractContent": [{"text": "Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing.", "labels": [], "entities": [{"text": "Answering natural language questions", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8585850447416306}, {"text": "Freebase knowledge base", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.9160822431246439}, {"text": "open domain semantic parsing", "start_pos": 150, "end_pos": 178, "type": "TASK", "confidence": 0.6533979400992393}]}, {"text": "Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base.", "labels": [], "entities": []}, {"text": "Here we show that relatively modest information extraction techniques, when paired with a web-scale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7829035222530365}]}], "introductionContent": [{"text": "Question answering (QA) from a knowledge base (KB) has along history within natural language processing, going back to the 1960s and 1970s, with systems such as Baseball and.", "labels": [], "entities": [{"text": "Question answering (QA) from a knowledge base (KB)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.7982932776212692}]}, {"text": "These systems were limited to closed-domains due to alack of knowledge resources, computing power, and ability to robustly understand natural language.", "labels": [], "entities": []}, {"text": "With the recent growth in KBs such as, and), it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now, based on Google's Knowledge Graph, and Facebook Graph Search, based on social network connections.", "labels": [], "entities": []}, {"text": "The AI community has tended to approach this problem with a focus on first understanding the intent of the question, via shallow or deep forms of semantic parsing (c.f. \u00a73 fora discussion).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.7577108442783356}]}, {"text": "Typically questions are converted into some meaning representation (e.g., the lambda calculus), then mapped to database queries.", "labels": [], "entities": []}, {"text": "Performance is thus bounded by the accuracy of the original semantic parsing, and the well-formedness of resultant database queries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999126136302948}, {"text": "semantic parsing", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7156429588794708}]}, {"text": "The Information Extraction (IE) community approaches QA differently: first performing relatively coarse information retrieval as away to triage the set of possible answer candidates, and only then attempting to perform deeper analysis.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.800546395778656}, {"text": "QA", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9442140460014343}]}, {"text": "Researchers in semantic parsing have recently explored QA over Freebase as away of moving beyond closed domains such as GeoQuery ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7736045122146606}]}, {"text": "While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared: we show that \"traditional\" IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of 34% F 1 as compared to.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7571929395198822}, {"text": "semantic parsing", "start_pos": 267, "end_pos": 283, "type": "TASK", "confidence": 0.7411223649978638}, {"text": "F 1", "start_pos": 324, "end_pos": 327, "type": "METRIC", "confidence": 0.9970361292362213}]}], "datasetContent": [{"text": "Both ClueWeb and its Freebase annotation has a bias.", "labels": [], "entities": []}, {"text": "Thus we were firstly interested in the coverage of mined relation mappings.", "labels": [], "entities": [{"text": "coverage of mined relation mappings", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.7198002636432648}]}, {"text": "As a comparison, we used a dataset of relation mapping contributed by and.", "labels": [], "entities": []}, {"text": "The idea is very similar: they intersected Freebase relations with predicates in (arg1, predicate, arg2) triples extracted from ReVerb to learn the mapping between Freebase relations and triple predicates.", "labels": [], "entities": []}, {"text": "Note the scale difference: although ReVerb was also extracted from ClueWeb09, there were only 15 million triples to intersect with the relations, while we had 1.2 billion alignment pairs.", "labels": [], "entities": []}, {"text": "We call this dataset ReverbMapping and ours CluewebMapping.", "labels": [], "entities": [{"text": "ReverbMapping", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.8574236631393433}]}, {"text": "The evaluation dataset, WEBQUESTIONS, was also contributed by.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.7239965200424194}]}, {"text": "It contains 3778 training and 2032 test questions collected from the Google Suggest service.", "labels": [], "entities": []}, {"text": "All questions were annotated with answers from Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.990281343460083}]}, {"text": "Some questions have more than one answer, such as what to see near sedona arizona?.", "labels": [], "entities": []}, {"text": "We evaluated on the training set in two aspects: coverage and prediction performance.", "labels": [], "entities": [{"text": "coverage", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9048146605491638}, {"text": "prediction", "start_pos": 62, "end_pos": 72, "type": "TASK", "confidence": 0.9459453225135803}]}, {"text": "We define answer node as the node that is the answer and answer relation as the relation from the answer node to its direct parent.", "labels": [], "entities": []}, {"text": "Then we computed how much and how well the answer relation was triggered by ReverbMapping and CluewebMapping.", "labels": [], "entities": [{"text": "ReverbMapping", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.8565468788146973}]}, {"text": "Thus for the question, who is the father of King George VI, we ask two questions: does the mapping, 1.", "labels": [], "entities": [{"text": "King George VI", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.7498703400293986}]}, {"text": "(coverage) contain the answer relation people.person.parents?", "labels": [], "entities": []}, {"text": "2. (precision) predict the answer relation from the question?", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9983426332473755}]}, {"text": "shows the coverage of CluewebMapping, which covers 93.0% of all answer relations.", "labels": [], "entities": [{"text": "CluewebMapping", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8366040587425232}]}, {"text": "Among them, we were able to learn the rule mapping using more than 10 thousand relationsentence pairs for each of the 89.5% of all answer relations.", "labels": [], "entities": [{"text": "rule mapping", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.7926689982414246}]}, {"text": "In contrast, ReverbMapping covers 89.7% of the answer relations.", "labels": [], "entities": []}, {"text": "Next we evaluated the prediction performance, using the evaluation metrics of information retrieval.", "labels": [], "entities": []}, {"text": "For each question, we extracted all relations in its corresponding topic graph, and ranked each relation with whether it is the answer relation.", "labels": [], "entities": []}, {"text": "For instance, for the previous example question, we want to rank the relation people.person.parents as number 1.", "labels": [], "entities": []}, {"text": "We computed standard MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank), shown in Table 2(a).", "labels": [], "entities": [{"text": "MAP", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9937441945075989}, {"text": "Mean Average Precision)", "start_pos": 26, "end_pos": 49, "type": "METRIC", "confidence": 0.9024444222450256}, {"text": "MRR", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.997062623500824}, {"text": "Mean Reciprocal Rank)", "start_pos": 59, "end_pos": 80, "type": "METRIC", "confidence": 0.8900531679391861}]}, {"text": "As a simple baseline, \"word overlap\" counts the overlap between relations and the question.", "labels": [], "entities": []}, {"text": "CluewebMapping ranks each relation by\u02dcP by\u02dc by\u02dcP (R | Q).", "labels": [], "entities": []}, {"text": "ReverbMapping does the same, except that we took a uniform distribution o\u00f1 P (w | R) and\u02dcPand\u02dc and\u02dcP (R) since the contributed dataset did not include co-occurrence counts to estimate these probabilities.", "labels": [], "entities": [{"text": "ReverbMapping", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9033825397491455}]}, {"text": "Note that the median rank from CluewebMapping is only 12, indicating that half of all answer relations are ranked in the top 12.(b) further shows the percentage of answer relations with respect to their ranking.", "labels": [], "entities": [{"text": "CluewebMapping", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9025188088417053}]}, {"text": "CluewebMapping successfully ranked 19% of answer relations as top 1.", "labels": [], "entities": [{"text": "CluewebMapping", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9284624457359314}]}, {"text": "A sample of these includes person.place of birth, location.containedby, country.currency used, regular tv appearance.actor, etc.", "labels": [], "entities": []}, {"text": "These percentage numbers are good clue for feature design: for instance, we maybe confident in a relation if it is ranked top 5 or 10 by CluewebMapping.", "labels": [], "entities": [{"text": "feature design", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7528358697891235}, {"text": "CluewebMapping", "start_pos": 137, "end_pos": 151, "type": "DATASET", "confidence": 0.9353755712509155}]}, {"text": "To conclude, we found that CluewebMapping provides satisfying coverage on the 3778 training questions: only 7% were missing, despite the biased nature of web data.", "labels": [], "entities": []}, {"text": "Also, CluewebMapping gives reasonably good precision on its prediction, despite the noisy nature of web data.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994263648986816}]}, {"text": "We move onto fully evaluate the final QA F 1 .  We evaluate the final F 1 in this section.", "labels": [], "entities": [{"text": "QA F 1", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.7481791575749716}, {"text": "F 1", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9648464918136597}]}, {"text": "The system of comparison is that of.", "labels": [], "entities": []}, {"text": "Data We re-used WEBQUESTIONS, a dataset collected by.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.8100939393043518}]}, {"text": "It contains 5810 questions crawled from the Google Suggest service, with answers annotated on Amazon Mechanical Turk.", "labels": [], "entities": []}, {"text": "All questions contain at least one answer from Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9875882267951965}]}, {"text": "This dataset has been split by 65%/35% into.All named entities 8 in a question were sent to this API, which returned a ranked list of relevant topics.", "labels": [], "entities": []}, {"text": "We also evaluated how well the search API served the IR purpose.", "labels": [], "entities": [{"text": "IR", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9847584962844849}]}, {"text": "WEBQUESTIONS not only has answers annotated, but also which Freebase topic nodes the answers come from.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9406241178512573}]}, {"text": "Thus we evaluated the ranking of retrieval with the gold standard annotation on TRAIN-ALL, shown in Table 3.", "labels": [], "entities": [{"text": "TRAIN-ALL", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9833788275718689}]}, {"text": "The top 2 results of the Search API contain gold standard topics for more than 90% of the questions and the top 10 results contain more than 95%.", "labels": [], "entities": []}, {"text": "We took this as a \"good enough\" IR frontend and used it on TEST.", "labels": [], "entities": [{"text": "TEST", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8756194710731506}]}, {"text": "Once a topic is obtained we query the Freebase Topic API to retrieve all relevant information, resulting in a topic graph.", "labels": [], "entities": []}, {"text": "The API returns almost identical information as displayed via a web browser to a user viewing this topic.", "labels": [], "entities": []}, {"text": "Given that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.6997188180685043}]}, {"text": "Model Tuning We treat QA on Freebase as a binary classification task: for each node in the topic graph, we extract features and judge whether it is the answer node.", "labels": [], "entities": []}, {"text": "Every question was processed by the Stanford CoreNLP suite with the caseless model.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.882777750492096}]}, {"text": "Then the question features ( \u00a74.1) and node features ( \u00a74.2) were combined ( \u00a74.3) for each node.", "labels": [], "entities": []}, {"text": "The learning problem is challenging: for about 3000 questions in TRAIN, there are 3 million nodes (1000 nodes per topic graph), and 7 million feature types.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.719111442565918}]}, {"text": "We employed a high-performance machine learning tool, Classias.", "labels": [], "entities": []}, {"text": "Training usually took around 4 hours.", "labels": [], "entities": []}, {"text": "We experimented with various discriminative learners on DEV, including logistic regression, perceptron and SVM, and found L1 regularized logistic regression to give the best result.", "labels": [], "entities": [{"text": "DEV", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8778870105743408}]}, {"text": "The L1 regularization encourages sparse features by driving feature weights towards zero, which was ideal for the over-generated feature space.", "labels": [], "entities": []}, {"text": "After training, we had around 30 thousand features with non-zero weights, a 200 fold reduction from the original features.", "labels": [], "entities": []}, {"text": "Also, we did an ablation test on DEV about how additional features on the mapping between Freebase relations and the original questions help, with three feature settings: 1) \"basic\" features include feature productions read off from the feature graph (); 2) \"+ word overlap\" adds additional features on whether sub-relations have overlap with the question; and 3) \"+ CluewebMapping\" adds the ranking of relation prediction given the question according to CluewebMapping.", "labels": [], "entities": [{"text": "DEV", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.971922755241394}]}, {"text": "actually reported accuracy on this dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.99954754114151}]}, {"text": "However, since their system predicted answers for almost every question (p.c.), it is roughly that precision=recall=F 1 =accuracy for them.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9994671940803528}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9898832440376282}, {"text": "F 1", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9180749952793121}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9961591958999634}]}, {"text": "features improved overall F 1 by 5%, a 13% relative improvement: a remarkable gain given that the model already learned a strong correlation between question types and answer types (explained more in discussion and later).", "labels": [], "entities": [{"text": "F 1", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9936456680297852}]}, {"text": "Finally, the ratio of positive vs. negative examples affect final F 1 : the more positive examples, the lower the precision and the higher the recall.", "labels": [], "entities": [{"text": "F 1", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.8774664103984833}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9996650218963623}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.999344527721405}]}, {"text": "Under the original setting, this ratio was about 1 : 275.", "labels": [], "entities": []}, {"text": "This produced precision around 60% and recall around 35% (c.f.).", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.999693751335144}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9997835755348206}]}, {"text": "To optimize for F 1 , we down-sampled the negative examples to 20%, i.e., anew ratio of 1 : 55.", "labels": [], "entities": [{"text": "F 1", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.953999400138855}]}, {"text": "This boosted the final F 1 on DEV to 48%.", "labels": [], "entities": [{"text": "the final F 1", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.7548408061265945}, {"text": "DEV", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.943494439125061}]}, {"text": "We report the final TEST result under this down-sampled training.", "labels": [], "entities": [{"text": "TEST", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9944581985473633}]}, {"text": "In practice the precision/recall balance can be adjusted by the positive/negative ratio.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9996248483657837}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9587129354476929}]}, {"text": "Test Results gives the final F 1 on TEST.", "labels": [], "entities": [{"text": "F 1", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9780604541301727}, {"text": "TEST", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.8212685585021973}]}, {"text": "\"Gold Retrieval\" always ranked the correct topic node top 1, a perfect IR front-end assumption.", "labels": [], "entities": [{"text": "Gold Retrieval\"", "start_pos": 1, "end_pos": 16, "type": "TASK", "confidence": 0.6346853971481323}]}, {"text": "Ina more realistic scenario, we had already evaluated that the Freebase Search API returned the correct topic node 95% of the time in its top 10 results (c.f.), thus we also tested on the top 10 results returned by the Search API.", "labels": [], "entities": [{"text": "Freebase Search API", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9385306040445963}]}, {"text": "To keep things simple, we did not perform answer voting, but simply extracted answers from the first (ranked by the Search API) topic node with predicted answer(s) found.", "labels": [], "entities": [{"text": "answer voting", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.8691136240959167}]}, {"text": "The final F 1 of 42.0% gives a relative improvement over previous best result) of 31.4% by one third.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9728573262691498}]}, {"text": "One question of interest is whether our system, aided by the massive web data, can be fairly compared to the semantic parsing approaches (note that also used ClueWeb indirectly through ReVerb).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.7380215525627136}]}, {"text": "Thus we took out the word overlapping and CluewebMapping based features, and the new F 1 on TEST was 36.9%.", "labels": [], "entities": [{"text": "word overlapping", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.6189415901899338}, {"text": "F 1", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9860015511512756}, {"text": "TEST", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.7030332684516907}]}, {"text": "The other question of interest is that whether our system has acquired some level of \"machine wgt.", "labels": [], "entities": []}, {"text": "feature: A sample of the top 50 most positive/negative features.", "labels": [], "entities": []}, {"text": "Features are production between question and node features (c.f.).", "labels": [], "entities": []}, {"text": "intelligence\": how much does it know what the question inquires?", "labels": [], "entities": []}, {"text": "We discuss it below through feature and error analysis.", "labels": [], "entities": []}, {"text": "Discussion The combination between questions and Freebase nodes captures some real gist of QA pattern typing, shown in with sampled features and weights.", "labels": [], "entities": [{"text": "QA pattern typing", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.6632569829622904}]}, {"text": "Our system learned, for instance, when the question asks for geographic adjacency information (qverb=border), the correct answer relation to look for is location.adjoins.", "labels": [], "entities": []}, {"text": "Detailed comparison with the output from Berant et al. is a work in progress and will be presented in a follow-up report.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation on answer relation ranking  prediction on 3778 training questions.", "labels": [], "entities": [{"text": "answer relation ranking  prediction", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.8763818889856339}]}, {"text": " Table 3: Evaluation on the Freebase Search API:  how many questions' top n retrieved results con- tain the gold standard topic. Total number of ques- tions is 3778 (size of TRAIN-ALL). There were  only 5 questions with no retrieved results.", "labels": [], "entities": [{"text": "Total number of ques- tions", "start_pos": 129, "end_pos": 156, "type": "METRIC", "confidence": 0.8568601012229919}, {"text": "TRAIN-ALL", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9407160878181458}]}, {"text": " Table 4: F 1 on DEV with different feature settings.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.95807284116745}, {"text": "DEV", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.9340946674346924}]}, {"text": " Table 5: F 1 on TEST with Gold Retrieval and  Freebase Search API as the IR front end. Berant  et al.", "labels": [], "entities": []}, {"text": " Table 6: A sample of the top 50 most positive/neg- ative features. Features are production between  question and node features (c.f.", "labels": [], "entities": []}]}