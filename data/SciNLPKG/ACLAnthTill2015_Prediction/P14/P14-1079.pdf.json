{"title": [{"text": "Distant Supervision for Relation Extraction with Matrix Completion", "labels": [], "entities": [{"text": "Distant Supervision", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9641863107681274}, {"text": "Relation Extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.9843364655971527}]}], "abstractContent": [{"text": "The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7717583179473877}, {"text": "multi-label classification", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.726658046245575}]}, {"text": "To tackle the s-parsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank.", "labels": [], "entities": []}, {"text": "We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8216306567192078}]}, {"text": "Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low.", "labels": [], "entities": []}, {"text": "We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix.", "labels": [], "entities": []}, {"text": "The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum.", "labels": [], "entities": [{"text": "matrix completion", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7605699598789215}, {"text": "fixed point continuation (FPC)", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6228385517994562}]}, {"text": "Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation Extraction (RE) is the process of generating structured relation knowledge from unstructured natural language texts.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9053095817565918}]}, {"text": "Traditional supervised methods () on small hand-labeled corpora, such as MUC and ACE 2 , can achieve high precision and recall.", "labels": [], "entities": [{"text": "MUC", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.7765207886695862}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9988769888877869}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.997711181640625}]}, {"text": "However, as producing handlabeled corpora is laborius and expensive, the supervised approach cannot satisfy the increasing demand of building large-scale knowledge repositories with the explosion of Web texts.", "labels": [], "entities": []}, {"text": "To address the lacking training data issue, we consider the distant () or weak () supervision paradigm attractive, and we improve the effectiveness of the paradigm in this paper.", "labels": [], "entities": []}, {"text": "The intuition of the paradigm is that one can take advantage of several knowledge bases, such as WordNet , Freebase and YAGO 5 , to automatically label free texts, like Wikipedia and New York Times corpora , based on some heuristic alignment assumptions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9733207821846008}]}, {"text": "An example accounting for the basic but practical assumption is illustrated in, in which we know that the two entities (<Barack Obama, U.S.>) are not only involved in the relation instances 8 coming from knowledge bases (President-of(Barack Obama, U.S.) and Born-in(Barack Obama, U.S.)),: The procedure of noise-tolerant low-rank matrix completion.", "labels": [], "entities": [{"text": "noise-tolerant low-rank matrix completion", "start_pos": 306, "end_pos": 347, "type": "TASK", "confidence": 0.7013791650533676}]}, {"text": "In this scenario, distantly supervised relation extraction task is transformed into completing the labels for testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7199778705835342}]}, {"text": "We seek to recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously.", "labels": [], "entities": []}, {"text": "but also co-occur in several relation mentions 9 appearing in free texts (Barack Obama is the 44th and current President of the U.S. and Barack Obama was born in Honolulu, Hawaii, U.S., etc.).", "labels": [], "entities": []}, {"text": "We extract diverse textual features from all those relation mentions and combine them into a rich feature vector labeled by the relation names to produce a weak training corpus for relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.9177613854408264}]}, {"text": "This paradigm is promising to generate largescale training corpora automatically.", "labels": [], "entities": []}, {"text": "However, it comes up against three technical challeges: \u2022 Sparse features.", "labels": [], "entities": [{"text": "Sparse", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9879748821258545}]}, {"text": "As we cannot tell what kinds of features are effective in advance, we have to use NLP toolkits, such as Stanford CoreNLP , to extract a variety of textual features, e.g., named entity tags, part-of-speech tags and lexicalized dependency paths.", "labels": [], "entities": []}, {"text": "Unfortunately, most of them appear only once in the training corpus, and hence leading to very sparse features.", "labels": [], "entities": []}, {"text": "Not all relation mentions express the corresponding relation instances.", "labels": [], "entities": []}, {"text": "For example, the second relation mention in does not explicitly describe any relation instance, so features extracted from this sentence can be noisy.", "labels": [], "entities": []}, {"text": "Such analogous cases commonly exist in feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8066093921661377}]}, {"text": "Similar to noisy fea- The sentences that contain the given entity pair are called relation mentions.", "labels": [], "entities": []}, {"text": "10 http://nlp.stanford.edu/downloads/corenlp.shtml tures, the generated labels can be incomplete.", "labels": [], "entities": []}, {"text": "For example, the fourth relation mention in should have been labeled by the relation Senate-of.", "labels": [], "entities": []}, {"text": "However, the incomplete knowledge base does not contain the corresponding relation instance (Senate-of(Barack Obama, U.S.)).", "labels": [], "entities": []}, {"text": "Therefore, the distant supervision paradigm may generate incomplete labeling corpora.", "labels": [], "entities": []}, {"text": "In essence, distantly supervised relation extraction is an incomplete multi-label classification task with sparse and noisy features.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7880889475345612}, {"text": "multi-label classification task", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7821817994117737}]}, {"text": "In this paper, we formulate the relationextraction task from a novel perspective of using matrix completion with low rank criterion.", "labels": [], "entities": [{"text": "matrix completion", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7098478823900223}]}, {"text": "To the best of our knowledge, we are the first to apply this technique on relation extraction with distant supervision.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.9480979442596436}]}, {"text": "More specifically, as shown in, we model the task with a sparse matrix whose rows present items (entity pairs) and columns contain noisy textual features and incomplete relation labels.", "labels": [], "entities": []}, {"text": "In such away, relation classification is transformed into a problem of completing the unknown labels for testing items in the sparse matrix that concatenates training and testing textual features with training labels, based on the assumption that the item-by-feature and item-by-label joint matrix is of low rank.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.9242148399353027}]}, {"text": "The rationale of this assumption is that noisy features and incomplete labels are semantically correlated.", "labels": [], "entities": []}, {"text": "The low-rank factorization of the sparse feature-label matrix delivers the low-dimensional representation of de-correlation for features and labels.", "labels": [], "entities": []}, {"text": "We contribute two optimization models, DRM-C 11 -b and DRMC-1, aiming at exploiting the sparsity to recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously.", "labels": [], "entities": [{"text": "DRM-C 11 -b", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7945063263177872}, {"text": "DRMC-1", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.9246407151222229}]}, {"text": "Moreover, the logistic cost function is integrated in our models to reduce the influence of noisy features and incomplete labels, due to that it is suitable for binary variables.", "labels": [], "entities": []}, {"text": "We also modify the fixed point continuation (FPC) algorithm ) to find the global optimum.", "labels": [], "entities": []}, {"text": "Experiments on two widely used datasets demonstrate that our noise-tolerant approaches outperform the baseline and the state-of-the-art methods.", "labels": [], "entities": []}, {"text": "Furthermore, we discuss the influence of feature sparsity, and our approaches consistently achieve better performance than compared methods under different sparsity degrees.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods) on two public datasets.", "labels": [], "entities": [{"text": "DRMC-b", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.9304113388061523}, {"text": "DRMC-1", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.8980439901351929}]}, {"text": "The two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora.", "labels": [], "entities": []}, {"text": "The first dataset 12 , NYT'10, was developed by, and also used by.", "labels": [], "entities": [{"text": "NYT'10", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9470561742782593}]}, {"text": "Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions.", "labels": [], "entities": []}, {"text": "The second dataset 13 , NYT'13, was also released by, in which they only regarded the lexicalized dependency path between two entities as features.", "labels": [], "entities": [{"text": "NYT'13", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.9426432847976685}]}, {"text": "shows that the two datasets differ in some main attributes.", "labels": [], "entities": []}, {"text": "More specifically, NYT'10 contains much higher dimensional features than NYT'13, whereas fewer training and testing items.", "labels": [], "entities": [{"text": "NYT'10", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.8601756691932678}, {"text": "NYT'13", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9134498238563538}]}], "tableCaptions": [{"text": " Table 1: Statistics about the two widely used datasets.", "labels": [], "entities": []}, {"text": " Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The  threshold \u03b8 means filtering the features that appear less than \u03b8 times. The values in brackets pertaining to  DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing  sets.", "labels": [], "entities": [{"text": "DRMC-b", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.9439199566841125}, {"text": "DRMC-1", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9009459018707275}, {"text": "DRMC-b", "start_pos": 210, "end_pos": 216, "type": "DATASET", "confidence": 0.9570579528808594}, {"text": "DRMC-1", "start_pos": 221, "end_pos": 227, "type": "DATASET", "confidence": 0.8620117902755737}]}, {"text": " Table 3: Precision of NFE-13, DRMC-b and  DRMC-1 on Top-100, Top-200 and Top-500 pre- dicted relation instances.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9774351119995117}, {"text": "NFE-13", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8598489165306091}, {"text": "DRMC-b", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.8510072827339172}]}]}