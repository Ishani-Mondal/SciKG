{"title": [{"text": "Sparser, Better, Faster GPU Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.6790336966514587}]}], "abstractContent": [{"text": "Due to their origin in computer graphics , graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points.", "labels": [], "entities": []}, {"text": "Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6131241222222646}]}, {"text": "Recently, Canny et al.", "labels": [], "entities": []}, {"text": "(2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses fora high-quality grammar at about 164 sentences per second on a mid-range GPU.", "labels": [], "entities": [{"text": "GPU parsing", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.6959154307842255}]}, {"text": "In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU.", "labels": [], "entities": [{"text": "GPU parsing", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.7377533614635468}]}, {"text": "The resulting system is capable of computing over 404 Viterbi parses per second-more than a 2x speedup-on the same hardware.", "labels": [], "entities": []}, {"text": "Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning-nearly a 6x speedup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Because NLP models typically treat sentences independently, NLP problems have long been seen as \"embarrassingly parallel\" -large corpora can be processed arbitrarily fast by simply sending different sentences to different machines.", "labels": [], "entities": []}, {"text": "However, recent trends in computer architecture, particularly the development of powerful \"general purpose\" GPUs, have changed the landscape even for problems that parallelize at the sentence level.", "labels": [], "entities": []}, {"text": "First, classic single-core processors and main memory architectures are no longer getting substantially faster overtime, so speed gains must now come from parallelism within a single machine.", "labels": [], "entities": []}, {"text": "Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic.", "labels": [], "entities": []}, {"text": "Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9707324504852295}]}, {"text": "The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on.", "labels": [], "entities": []}, {"text": "Recently, proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide.", "labels": [], "entities": [{"text": "constituency parser", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7056376338005066}]}, {"text": "Their system uses a grammar based on the Berkeley parser) (which is particularly amenable to GPU processing), \"compiling\" the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart.", "labels": [], "entities": []}, {"text": "Together these kernels implement the Viterbi inside algorithm.", "labels": [], "entities": []}, {"text": "On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below).", "labels": [], "entities": []}, {"text": "In this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-tofine pruning to a GPU setting.", "labels": [], "entities": []}, {"text": "On a CPU, pruning methods can give speedups of up to 100x.", "labels": [], "entities": []}, {"text": "Such extreme speedups over a dense GPU baseline currently seem unlikely because fine-grained sparsity appears to be directly at odds with dense parallelism.", "labels": [], "entities": []}, {"text": "However, in this paper, we present a system that finds a middle ground, where some level of sparsity can be maintained without losing the parallelism of the GPU.", "labels": [], "entities": [{"text": "GPU", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.9730998277664185}]}, {"text": "We use a coarse-to-fine approach as in, but with only one coarse pass.", "labels": [], "entities": []}, {"text": "shows an overview of the approach: we first parse densely with a coarse grammar and then parse sparsely with the fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely.", "labels": [], "entities": []}, {"text": "Using this approach, we see again of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second.", "labels": [], "entities": []}, {"text": "For comparison, the publicly available CPU implementation of parses approximately 7 sentences per second per core on a modern CPU.", "labels": [], "entities": [{"text": "parses", "start_pos": 61, "end_pos": 67, "type": "TASK", "confidence": 0.9605379104614258}]}, {"text": "A further drawback of the dense approach in is that it only computes Viterbi parses.", "labels": [], "entities": []}, {"text": "As with other grammars with a parse/derivation distinction, the grammars of only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9984654188156128}, {"text": "F1", "start_pos": 173, "end_pos": 175, "type": "METRIC", "confidence": 0.9663506746292114}, {"text": "Penn Treebank", "start_pos": 220, "end_pos": 233, "type": "DATASET", "confidence": 0.9961428940296173}]}, {"text": "To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing anew way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass.", "labels": [], "entities": []}, {"text": "We then implement minimumBayes-risk parsing via the max recall algorithm of.", "labels": [], "entities": [{"text": "minimumBayes-risk parsing", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.5901907086372375}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.7320721745491028}]}, {"text": "Without the coarse pass, the dense marginal computation is not efficient on a GPU, processing only 32 sentences per second.", "labels": [], "entities": []}, {"text": "However, our approach allows us to process over 190 sentences per second, almost a 6x speedup.", "labels": [], "entities": []}], "datasetContent": [{"text": "We buildup our approach incrementally, with experiments interspersed throughout the paper, and summarized in.", "labels": [], "entities": []}, {"text": "In this paper, we focus our attention on current-generation NVIDIA GPUs.", "labels": [], "entities": []}, {"text": "Many of the ideas described here apply to other GPUs (such as those from AMD), but some specifics will differ.", "labels": [], "entities": []}, {"text": "All experiments are run with an NVIDIA GeForce GTX 680, a mid-range GPU that costs around $500 at time of writing.", "labels": [], "entities": []}, {"text": "Unless otherwise noted, all experiments are conducted on sentences of length \u2264 40 words, and we estimate times based on batches of 20K sentences.", "labels": [], "entities": []}, {"text": "We should note that our experimental condition differs from that of: they evaluate on sentences of length \u2264 30.", "labels": [], "entities": []}, {"text": "Furthermore, they The implementation of cannot handle batches so large, and so we tested it on batches of 1200 sentences.", "labels": [], "entities": []}, {"text": "Our reimplementation is approximately the same speed for the same batch sizes.", "labels": [], "entities": []}, {"text": "For batches of 20K sentences, we used sentences from the training set.", "labels": [], "entities": []}, {"text": "We verified that there was no significant difference in speed for sentences from the training set and from the test set.", "labels": [], "entities": [{"text": "speed", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9820333123207092}]}, {"text": "use two NVIDIA GeForce GTX 690s-each of which is essentially a repackaging of two 680s-meaning that our system and experiments would run approximately four times faster on their hardware.", "labels": [], "entities": []}, {"text": "(This expected 4x factor is empirically consistent with the result of running their system on our hardware.)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance numbers for computing  Viterbi inside charts on 20,000 sentences of length  \u226440 from the Penn Treebank. All times are  measured on an NVIDIA GeForce GTX 680.  'Reimpl' is our reimplementation of their ap- proach. Speedups are measured in reference to this  reimplementation. See Section 7 for discussion of  the clustering algorithms and Section 6 for a de- scription of the pruning methods. The", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.9965082108974457}]}, {"text": " Table 2: Performance numbers for computing max  constituent", "labels": [], "entities": []}, {"text": " Table 3: Time spent in the passes of our differ- ent systems, in seconds per 1000 sentences. Prun- ing refers to using a 1-split grammar for the coarse  pass.", "labels": [], "entities": [{"text": "Prun- ing", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.8872895439465841}]}, {"text": " Table 4: Breakdown of time spent in our different systems, in seconds per 1000 sentences. Binary and  Unary refer to spent processing binary rules. Queueing refers to the amount of time used to move memory  around within the GPU for processing. Overhead includes all other time, which includes communication  between the GPU and the CPU.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.84344083070755}]}]}