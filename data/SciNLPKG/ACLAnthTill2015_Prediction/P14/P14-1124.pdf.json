{"title": [{"text": "Can You Repeat That? Using Word Repetition to Improve Spoken Term Detection", "labels": [], "entities": [{"text": "Spoken Term Detection", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6949166357517242}]}], "abstractContent": [{"text": "We aim to improve spoken term detection performance by incorporating con-textual information beyond traditional N-gram language models.", "labels": [], "entities": [{"text": "spoken term detection", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7353472113609314}]}, {"text": "Instead of taking abroad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents.", "labels": [], "entities": [{"text": "word repetition within single documents", "start_pos": 176, "end_pos": 215, "type": "TASK", "confidence": 0.8237569689750671}]}, {"text": "We show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document.", "labels": [], "entities": []}, {"text": "We leverage this bursti-ness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits.", "labels": [], "entities": []}, {"text": "We then develop a principled approach to select interpolation weights using only the ASR training data.", "labels": [], "entities": [{"text": "ASR training data", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.8138885299364725}]}, {"text": "Using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program.", "labels": [], "entities": [{"text": "term detection", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7664686739444733}, {"text": "BABEL program", "start_pos": 138, "end_pos": 151, "type": "DATASET", "confidence": 0.8423730731010437}]}], "introductionContent": [{"text": "The spoken term detection task arises as a key subtask in applying NLP applications to spoken content.", "labels": [], "entities": [{"text": "spoken term detection task", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7718346565961838}]}, {"text": "Tasks like topic identification and namedentity detection require transforming a continuous acoustic signal into a stream of discrete tokens which can then be handled by NLP and other statistical machine learning techniques.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9401728212833405}, {"text": "namedentity detection", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7557842433452606}]}, {"text": "Given a small vocabulary of interest (1000-2000 words or multi-word terms) the aim of the term detection task is to enumerate occurrences of the keywords within a target corpus.", "labels": [], "entities": [{"text": "term detection", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7207008302211761}]}, {"text": "Spoken term detection converts the raw acoustics into time-marked keyword occurrences, which may subsequently be fed (e.g. as a bag-of-terms) to standard NLP algorithms.", "labels": [], "entities": [{"text": "Spoken term detection", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7673582037289938}]}, {"text": "Although spoken term detection does not require the use of word-based automatic speech recognition (ASR), it is closely related.", "labels": [], "entities": [{"text": "spoken term detection", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7665969530741373}, {"text": "word-based automatic speech recognition (ASR)", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.7225361210959298}]}, {"text": "If we had perfectly accurate ASR in the language of the corpus, term detection is reduced to an exact string matching task.", "labels": [], "entities": [{"text": "ASR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9814576506614685}, {"text": "term detection", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.8252764940261841}]}, {"text": "The word error rate (WER) and term detection performance are clearly correlated.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.8360030899445215}, {"text": "term detection", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.6680447906255722}]}, {"text": "Given resource constraints, domain, channel, and vocabulary limitations, particularly for languages other than English, the errorful token stream makes term detection a non-trivial task.", "labels": [], "entities": [{"text": "term detection", "start_pos": 152, "end_pos": 166, "type": "TASK", "confidence": 0.8358674943447113}]}, {"text": "In order to improve detection performance, and restricting ourselves to an existing ASR system or systems at our disposal, we focus on leveraging broad document context around detection hypotheses.", "labels": [], "entities": [{"text": "ASR", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9446418881416321}]}, {"text": "ASR systems traditionally use N-gram language models to incorporate prior knowledge of word occurrence patterns into prediction of the next word in the token stream.", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9730305075645447}]}, {"text": "N-gram models cannot, however, capture complex linguistic or topical phenomena that occur outside the typical 3-5 word scope of the model.", "labels": [], "entities": []}, {"text": "Yet, though many language models more sophisticated than N-grams have been proposed, N-grams are empirically hard to beat in terms of WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.7526004910469055}]}, {"text": "We consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence.", "labels": [], "entities": [{"text": "term detection", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7996248006820679}]}, {"text": "Confidence scores from an ASR system (which incorporate N-gram probabilities) are optimized in order to produce the most likely sequence of words rather than the accuracy of individual word detections.", "labels": [], "entities": [{"text": "ASR", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9213727712631226}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9966727495193481}]}, {"text": "Looking at broader document context within a more limited task might allow us to escape the limits of N-gram performance.", "labels": [], "entities": []}, {"text": "We will show that by focusing on contextual information in the form of word repetition within documents, we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program.", "labels": [], "entities": [{"text": "word repetition within documents", "start_pos": 71, "end_pos": 103, "type": "TASK", "confidence": 0.804305300116539}, {"text": "IARPA BABEL", "start_pos": 195, "end_pos": 206, "type": "TASK", "confidence": 0.3229802995920181}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Term detection scores for swept \u03b1 values  on Tagalog development data", "labels": [], "entities": [{"text": "Term detection", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6839993000030518}, {"text": "Tagalog development data", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.6172576248645782}]}, {"text": " Table 2: Term detection performance using vari- ous interpolation weight strategies on Tagalog dev  data", "labels": [], "entities": [{"text": "Term detection", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.971454918384552}]}, {"text": " Table 3: Word-repetition re-scored results for available CTS term detection corpora", "labels": [], "entities": [{"text": "CTS term detection corpora", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.8075524419546127}]}]}