{"title": [{"text": "A chance-corrected measure of inter-annotator agreement for syntax", "labels": [], "entities": []}], "abstractContent": [{"text": "Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.", "labels": [], "entities": []}, {"text": "With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncor-rected parser evaluation metrics such as bracket F 1 (for phrase structure) and accuracy scores (for dependencies).", "labels": [], "entities": [{"text": "bracket F 1", "start_pos": 148, "end_pos": 159, "type": "METRIC", "confidence": 0.8117131590843201}, {"text": "accuracy scores", "start_pos": 187, "end_pos": 202, "type": "METRIC", "confidence": 0.982513964176178}]}, {"text": "In this work we present a chance-corrected metric based on Krippendorff's \u03b1, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications.", "labels": [], "entities": []}, {"text": "To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses, before finally contrasting the behaviour of our chance-corrected metric with that of un-corrected parser evaluation metrics on real corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is a truth universally acknowledged that an annotation task in good standing be in possession of a measure of inter-annotator agreement (IAA).", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 113, "end_pos": 144, "type": "METRIC", "confidence": 0.8032739758491516}]}, {"text": "However, no such measure is in widespread use for the task of syntactic annotation.", "labels": [], "entities": [{"text": "syntactic annotation", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.8844519257545471}]}, {"text": "This is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation.", "labels": [], "entities": []}, {"text": "For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fallback to simple accuracy measures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9969401359558105}]}, {"text": "As shown in, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent.", "labels": [], "entities": [{"text": "agreement", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9194433689117432}]}, {"text": "In this article we propose a family of chancecorrected measures of agreement, applicable to both dependency-and constituency-based syntactic annotation, based on Krippendorff's \u03b1 and tree edit distance.", "labels": [], "entities": []}, {"text": "First we give an overview of traditional agreement measures and why they are insufficient for syntax, before presenting our proposed metrics.", "labels": [], "entities": []}, {"text": "Next, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9959210157394409}]}], "datasetContent": [{"text": "In the previous section, we proposed three different agreement metrics \u03b1 plain , \u03b1 diff and \u03b1 norm , each involving different trade-offs.", "labels": [], "entities": []}, {"text": "Deciding which of these metrics is the best one for our purposes of judging the consistency of syntactic annotation poses a bit of a conundrum.", "labels": [], "entities": [{"text": "consistency", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9741133451461792}]}, {"text": "We could at this point apply our metrics to various real corpora and compare the results, but since the consistency of the corpora is unknown, it's impossible to say whether the best metric is the one resulting in the highest scores, the lowest scores or somewhere in the middle.", "labels": [], "entities": []}, {"text": "To properly settle this question, we first performed a number of synthetic experiments to gauge how the different metrics respond to disagreement.", "labels": [], "entities": []}, {"text": "The general approach we take is based on that used by, adapted to dependency trees.", "labels": [], "entities": []}, {"text": "An already annotated corpus, in our case 100 randomly selected sentences from the Norwegian Dependency Treebank (), are taken as correct and then permuted to produce \"annotations\" of different quality.", "labels": [], "entities": [{"text": "Norwegian Dependency Treebank", "start_pos": 82, "end_pos": 111, "type": "DATASET", "confidence": 0.9282305041948954}]}, {"text": "For dependency trees, the input corpus is permuted as follows: 1.", "labels": [], "entities": []}, {"text": "Each token has a probability p relabel of being assigned a different label uniformly at random from the set of labels used in the corpus.", "labels": [], "entities": []}, {"text": "Each token has a probability p reattach of being assigned anew head uniformly at random from the set of tokens not dominated by the token.", "labels": [], "entities": []}, {"text": "The second permutation process is dependent on the order the tokens are processed, and we consider the tokens in the post-order 5 as dictated by the original tree.", "labels": [], "entities": []}, {"text": "This way tokens close to the root have a fair chance of having candidate heads if they are selected.", "labels": [], "entities": []}, {"text": "A pre-order traversal would result in tokens close to the root having few options, and in particular if the root has a single child, that node has no possible new heads unless one of its children has been assigned the root as its new headfirst.", "labels": [], "entities": []}, {"text": "For example in the trees in, assigning any other head than the root to the PRED nodes directly dominated by the root will result in invalid (cyclic and unconnected) dependency trees.", "labels": [], "entities": []}, {"text": "Traversing the tokens in the linear order dictated by the sentence has similar issues for tokens close to the root and close to the start of the sentence.", "labels": [], "entities": []}, {"text": "For our first set of experiments, we set p relabel = p reattach and evaluated the different agreement metrics for 10 evenly spaced p-values between 0.1 and 1.0.", "labels": [], "entities": []}, {"text": "Initial exploration of the data showed that the mean follows the median very closely regardless of metric and perturbation level, and therefore we only report the mean scores across runs in this paper.", "labels": [], "entities": []}, {"text": "The results of these experiments are shown in, with the labelled attachment score 6 (LAS) for comparison.", "labels": [], "entities": [{"text": "labelled attachment score 6 (LAS)", "start_pos": 56, "end_pos": 89, "type": "METRIC", "confidence": 0.8822252580097744}]}, {"text": "That is, the child nodes of anode are all processed before the node itself.", "labels": [], "entities": []}, {"text": "Nodes on the same level are traversed from left to right.", "labels": [], "entities": []}, {"text": "The de facto standard parser evaluation metric in depen- The \u03b1 diff metric is clearly extremely sensitive to noise, with p = 0.1 yielding mean \u03b1 diff = 15.8%, while \u03b1 norm is more lenient than both LAS and \u03b1 plain , with mean \u03b1 norm = 14.5% at p = 1, quite high compared to LAS = 0.9%, \u03b1 plain = \u22126.8% and \u03b1 diff = \u2212246%.", "labels": [], "entities": [{"text": "LAS", "start_pos": 274, "end_pos": 277, "type": "METRIC", "confidence": 0.9534000158309937}]}, {"text": "To further study the sensitivity of the metrics to the two kinds of noise, we performed an additional set of experiments, setting one p = 0 while varying the other over the same range as in the previous experiment, the results of which are shown in Figures 4 and 5.", "labels": [], "entities": []}, {"text": "The LAS curves are mostly unremarkable, with one exception: Mean LAS at p reattach = 1 of Figure 5 is 23.9%, clearly much higher than we would expect if the trees were completely random.", "labels": [], "entities": [{"text": "LAS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8104708194732666}, {"text": "LAS at p reattach = 1", "start_pos": 65, "end_pos": 86, "type": "METRIC", "confidence": 0.7775861769914627}]}, {"text": "In comparison, mean LAS when only labels are perturbed is 4.1%, and since the sample space of trees of size n is clearly much larger than that of relabellings, a uniform random selection of tree would yield a LAS much closer to 0.", "labels": [], "entities": [{"text": "LAS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.5552996397018433}]}, {"text": "This shows that our tree shuffling algorithm has a non-uniform distribution over the sample space.", "labels": [], "entities": []}, {"text": "While the behaviour of our alphas and LAS are relatively similar in show that they do in fact have important differences.", "labels": [], "entities": [{"text": "LAS", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.7222923636436462}]}, {"text": "Whereas LAS responds linearly to perturbation of both labels and structure, with its parabolic behaviour in being simply the product of these two linear responses, the \u03b1 metrics respond differently to structural noise and label noise, with label disagreements being penalised less harshly dency parsing: the percentage of tokens that receive the correct head and dependency relation.", "labels": [], "entities": [{"text": "dency parsing", "start_pos": 289, "end_pos": 302, "type": "TASK", "confidence": 0.821567177772522}]}], "tableCaptions": [{"text": " Table 1: Sizes of the different IAA corpora", "labels": [], "entities": [{"text": "Sizes", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.976502001285553}]}, {"text": " Table 2: Agreement scores on real-world corpora", "labels": [], "entities": [{"text": "Agreement scores", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9691886007785797}]}]}