{"title": [{"text": "That's Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text", "labels": [], "entities": []}], "abstractContent": [{"text": "two realization models and calculated precision, recall and F 1 of the dependencies for each realization by comparing them with the gold dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9996358156204224}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9994994401931763}, {"text": "F 1", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9927331805229187}]}, {"text": "We then ranked the realizations by their F 1 score of parse accuracy, keeping the original ranking in case of ties.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9574980139732361}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.6869877576828003}]}, {"text": "We also tried using unlabeled (and unordered) dependencies, in order to possibly make better use of parses that were close to being correct.", "labels": [], "entities": []}, {"text": "In this setting, as long as the right pair of tokens occur in a dependency relation, it was counted as a correctly recovered dependency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rajkumar & have recently shown that some rather egregious surface realization errors-in the sense that the reader would likely end up with the wrong interpretation-can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model , as reviewed in the next section.", "labels": [], "entities": []}, {"text": "However, one is apt to wonder: could one use a parser to check whether the intended interpretation is easy to recover, either as an alternative or to catch additional mistakes?", "labels": [], "entities": []}, {"text": "Doing so would be tantamount to selfmonitoring in model of language production.", "labels": [], "entities": []}, {"text": "pursued the idea of self-monitoring for generation in early work with reversible grammars.", "labels": [], "entities": []}, {"text": "As Noord observed, a simple, brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form, then to parse each realization to see how many interpretations it has, keeping only those that have a single reading; they then went onto devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences, targeted to the ambiguous portion of the output.", "labels": [], "entities": []}, {"text": "We might question, however, whether it is really possible to avoid ambiguity entirely in the general case, since and others have argued that nearly every sentence is potentially ambiguous, though we (as human comprehenders) may not notice the ambiguities if they are unlikely.", "labels": [], "entities": []}, {"text": "Taking up this issue,-building on approach to identifying \"innocuous\" ambiguities-conducted several experiments to test whether ambiguity could be balanced against length or fluency in the context of generating referring expressions involving coordinate structures.", "labels": [], "entities": []}, {"text": "Though Khan et al.'s study was limited to this one kind of structural ambiguity, they do observe that generating the brief variants when the intended interpretation is clear instantiates Van Deemter's (2004) general strategy of only avoiding vicious ambiguities-that is, ambiguities where the intended interpretation fails to be considerably more likely than any other distractor interpretations-rather than trying to avoid all ambiguities.", "labels": [], "entities": []}, {"text": "In this paper, we investigate whether Neumann & van Noord's brute-force strategy for avoid-ing ambiguities in surface realization can be updated to only avoid vicious ambiguities, extending (and revising) Van Deemter's general strategy to all kinds of structural ambiguity, not just the one investigated by Khan et al.", "labels": [], "entities": []}, {"text": "To do so-in a nutshell-we enumerate an n-best list of realizations and rerank them if necessary to avoid vicious ambiguities, as determined by one or more automatic parsers.", "labels": [], "entities": []}, {"text": "A potential obstacle, of course, is that automatic parsers may not be sufficiently representative of human readers, insofar as errors that a parser makes may not be problematic for human comprehension; moreover, parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length, even with carefully edited news text.", "labels": [], "entities": []}, {"text": "Consequently, we examine two reranking strategies, one a simple baseline approach and the other using an SVM reranker).", "labels": [], "entities": []}, {"text": "Our simple reranking strategy for selfmonitoring is to rerank the realizer's n-best list by parse accuracy, preserving the original order in case of ties.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9511921405792236}]}, {"text": "In this way, if there is a realization in the n-best list that can be parsed more accurately than the top-ranked realization-even if the intended interpretation cannot be recovered with 100% accuracy-it will become the preferred output of the combined realization-with-selfmonitoring system.", "labels": [], "entities": [{"text": "accuracy-it", "start_pos": 191, "end_pos": 202, "type": "METRIC", "confidence": 0.9824753999710083}]}, {"text": "With this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with baseline generative model, but not with their averaged perceptron model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9986854195594788}, {"text": "Penn Treebank development data", "start_pos": 136, "end_pos": 166, "type": "DATASET", "confidence": 0.9609190374612808}]}, {"text": "In inspecting the results of reranking with this strategy, we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities, common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers.", "labels": [], "entities": []}, {"text": "Therefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n-best parsing results, and per-label precision and recall for each type of dependency, together with the realizer's normalized perceptron model score as a feature.", "labels": [], "entities": [{"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9690840840339661}, {"text": "recall", "start_pos": 253, "end_pos": 259, "type": "METRIC", "confidence": 0.9843199849128723}]}, {"text": "With the SVM reranker, we obtain a significant improvement in BLEU scores over White & Rajkumar's averaged perceptron model on both development and test data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9995243549346924}]}, {"text": "Additionally, in a targeted manual analysis, we find that in cases where the SVM reranker improves the BLEU score, improvements to fluency and adequacy are roughly balanced, while in cases where the BLEU score goes down, it is mostly fluency that is made worse (with reranking yielding an acceptable paraphrase roughly one third of the time in both cases).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9822621047496796}, {"text": "BLEU score", "start_pos": 199, "end_pos": 209, "type": "METRIC", "confidence": 0.9788678288459778}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the realization ranking models that serve as a starting point for the paper.", "labels": [], "entities": []}, {"text": "In Section 3, we report on our experiments with the simple reranking strategy, including a discussion of the ways in which this method typically fails.", "labels": [], "entities": []}, {"text": "In Section 4, we describe how we trained an SVM reranker and report our results using BLEU scores ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.999323844909668}]}, {"text": "In Section 5, we present a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, discussing both successes and errors.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9783923327922821}]}, {"text": "In Section 6, we briefly review related work on broad coverage surface realization.", "labels": [], "entities": [{"text": "broad coverage surface realization", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.7432992309331894}]}, {"text": "Finally, in Section 7, we sum up and discuss opportunities for future work in this direction.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Devset BLEU scores for simple ranking  on top of n-best perceptron model realizations", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9847174286842346}]}, {"text": " Table 3: Devset results of SVM ranking on top  of perceptron model. Significance codes:  *  *  for  p < 0.05,  *  for p < 0.1.", "labels": [], "entities": []}]}