{"title": [{"text": "Active Learning with Efficient Feature Weighting Methods for Improving Data Quality and Classification Accuracy", "labels": [], "entities": [{"text": "Improving Data Quality", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7797513405481974}]}], "abstractContent": [{"text": "Many machine learning datasets are noisy with a substantial number of mislabeled instances.", "labels": [], "entities": []}, {"text": "This noise yields sub-optimal classification performance.", "labels": [], "entities": []}, {"text": "In this paper we study a large, low quality annotated dataset, created quickly and cheaply using Amazon Mechanical Turk to crowd-source annotations.", "labels": [], "entities": []}, {"text": "We describe compu-tationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mis-labeled instances to significantly improve annotation quality at low cost.", "labels": [], "entities": []}, {"text": "Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computation-ally expensive techniques.", "labels": [], "entities": [{"text": "emotion extraction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7459554374217987}]}, {"text": "Our techniques save a considerable amount of time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised classification algorithms require annotated data to teach the machine, by example, how to perform a specific task.", "labels": [], "entities": []}, {"text": "There are generally two ways to collect annotations of a dataset: through a few expert annotators, or through crowdsourcing services (e.g., Amazon's Mechanical Turk).", "labels": [], "entities": []}, {"text": "High-quality annotations can be produced by expert annotators, but the process is usually slow and costly.", "labels": [], "entities": []}, {"text": "The latter option is appealing since it creates a large annotated dataset at low cost.", "labels": [], "entities": []}, {"text": "In recent years, there have been an increasing number of studies () using crowdsourcing for data annotation.", "labels": [], "entities": []}, {"text": "However, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and * This author's research was done during an internship with Samsung Research America.", "labels": [], "entities": [{"text": "Samsung Research America", "start_pos": 193, "end_pos": 217, "type": "DATASET", "confidence": 0.7201179762681326}]}, {"text": "unreliable, which significantly reduces the performance of the classification model.", "labels": [], "entities": [{"text": "classification", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.9664714932441711}]}, {"text": "This is a challenge faced by many real world applicationsgiven a large, quickly and cheaply created, low quality annotated dataset, how can one improve its quality and learn an accurate classifier from it?", "labels": [], "entities": []}, {"text": "Re-annotating the whole dataset is too expensive.", "labels": [], "entities": []}, {"text": "To reduce the annotation effort, it is desirable to have an algorithm that selects the most likely mislabeled examples first for re-labeling.", "labels": [], "entities": []}, {"text": "The process of selecting and re-labeling data points can be conducted with multiple rounds to iteratively improve the data quality.", "labels": [], "entities": []}, {"text": "This is similar to the strategy of active learning.", "labels": [], "entities": []}, {"text": "The basic idea of active learning is to learn an accurate classifier using less training data.", "labels": [], "entities": []}, {"text": "An active learner uses a small set of labeled data to iteratively select the most informative instances from a large pool of unlabeled data for human annotators to label.", "labels": [], "entities": []}, {"text": "In this work, we borrow the idea of active learning to interactively and iteratively correct labeling errors.", "labels": [], "entities": []}, {"text": "The crucial step is to effectively and efficiently select the most likely mislabeled instances.", "labels": [], "entities": []}, {"text": "An intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels.", "labels": [], "entities": []}, {"text": "The data points with the highest confidence scores but conflicting preliminary labels are most likely mislabeled.", "labels": [], "entities": []}, {"text": "The algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that require frequent iterations on large datasets.", "labels": [], "entities": []}, {"text": "Specifically, we propose a novel non-linear distribution spreading algorithm, which first uses Delta IDF technique () to weight features, and then leverages the distribution of Delta IDF scores of a feature across different classes to efficiently recognize discriminative features for the classification task in the presence of mislabeled data.", "labels": [], "entities": [{"text": "distribution spreading", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7959023416042328}]}, {"text": "The idea is that some effective fea-tures maybe subdued due to label noise, and the proposed techniques are capable of counteracting such effect, so that the performance of classification algorithms could be less affected by the noise.", "labels": [], "entities": []}, {"text": "With the proposed algorithm, the active learner becomes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified.", "labels": [], "entities": []}, {"text": "We consider emotion analysis as an interesting and challenging problem domain of this study, and conduct comprehensive experiments on Twitter data.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7702877819538116}]}, {"text": "We employ Amazon's Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers.", "labels": [], "entities": [{"text": "AMT dataset", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.8320244550704956}]}, {"text": "Extensive experiments show that, the proposed techniques are as effective as more computational expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on a Twitter dataset that contains tweets about TV shows and movies.", "labels": [], "entities": []}, {"text": "The goal is to extract consumers' emotional reactions to multimedia content, which has broad commercial applications including targeted advertising, intelligent search, and recommendation.", "labels": [], "entities": [{"text": "extract consumers' emotional reactions to multimedia content", "start_pos": 15, "end_pos": 75, "type": "TASK", "confidence": 0.7963464089802333}]}, {"text": "To create the dataset, we collected 2 billion unique tweets using Twitter API queries fora list of known TV shows and movies on IMDB.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9711190462112427}]}, {"text": "Spam tweets were filtered out using a set of heuristics and manually crafted rules.", "labels": [], "entities": []}, {"text": "From the set of 2 billion tweets we randomly selected a small subset of 100K tweets about the 60 most highly mentioned TV shows and movies in the dataset.", "labels": [], "entities": []}, {"text": "Tweets were randomly sampled for each show using the round robin algorithm.", "labels": [], "entities": []}, {"text": "This samples an equal number of tweets for each show.", "labels": [], "entities": []}, {"text": "We then sent these tweets to Amazon Mechanical Turk for annotation.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9146970311800638}]}, {"text": "We defined our own set of emotions to annotate.", "labels": [], "entities": []}, {"text": "The widely accepted emotion taxonomies, including Ekmans Basic Emotions), Russells Circumplex model (, and Plutchiks emotion wheel, did not fit well for TV shows and Movies.", "labels": [], "entities": []}, {"text": "For example, the emotion expressed by laughter is a very important emotion for TV shows and movies, but this emotion is not covered by the taxonomies listed above.", "labels": [], "entities": []}, {"text": "After browsing through the raw dataset, reviewing the literature on emotion analysis, and considering the TV and movie problem domain, we decided to focus on eight emotions: funny, happy, sad, exciting, boring, angry, fear, and heartwarming.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7265779972076416}]}, {"text": "Emotion annotation is a non-trivial task that is typically time-consuming, expensive and errorprone.", "labels": [], "entities": [{"text": "Emotion annotation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.882707267999649}]}, {"text": "This task is difficult because: (1) There are multiple emotions to annotate.", "labels": [], "entities": []}, {"text": "In this work, we annotate eight different emotions.", "labels": [], "entities": []}, {"text": "(2) Emotion expressions could be subtle and ambiguous and thus are easy to miss when labeling quickly.", "labels": [], "entities": []}, {"text": "The dataset is very imbalanced, which increases the problem of confirmation bias.", "labels": [], "entities": [{"text": "confirmation bias", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.8682858049869537}]}, {"text": "As minority classes, emotional tweets can be easily missed because the last X tweets are all not emotional, and the annota-  tors do not expect the next one to be either.", "labels": [], "entities": []}, {"text": "Due to these reasons, there is alack of sufficient and high quality labeled data for emotion research.", "labels": [], "entities": []}, {"text": "Some researchers have studied harnessing Twitter hashtags to automatically create an emotion annotated dataset ().", "labels": [], "entities": []}, {"text": "In order to evaluate our approach in real world scenarios, instead of creating a high quality annotated dataset and then introducing artificial noise, we followed the common practice of crowdsoucing, and collected emotion annotations through Amazon Mechanical Turk (AMT).", "labels": [], "entities": []}, {"text": "This AMT annotated dataset was used as the low quality dataset\u02c6D dataset\u02c6 dataset\u02c6D in our evaluation.", "labels": [], "entities": [{"text": "AMT annotated dataset", "start_pos": 5, "end_pos": 26, "type": "DATASET", "confidence": 0.7721155285835266}]}, {"text": "After that, the same dataset was annotated independently by a group of expert annotators to create the ground truth.", "labels": [], "entities": []}, {"text": "We evaluate the proposed approach on two factors, the effectiveness of the models for emotion classification, and the improvement of annotation quality provided by the active learning procedure.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7786388993263245}]}, {"text": "We first describe the AMT annotation and ground truth annotation, and then discuss the baselines and experimental results.", "labels": [], "entities": [{"text": "AMT annotation", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.5985748767852783}]}, {"text": "Amazon Mechanical Turk Annotation: we posted the set of 100K tweets to the workers on AMT for emotion annotation.", "labels": [], "entities": [{"text": "AMT", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9057260155677795}, {"text": "emotion annotation", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7865748405456543}]}, {"text": "We defined a set of annotation guidelines, which specified rules and examples to help annotators determine when to tag a tweet with an emotion.", "labels": [], "entities": []}, {"text": "We applied substantial quality control to our AMT workers to improve the initial quality of annotation following the common practice of crowdsourcing.", "labels": [], "entities": [{"text": "AMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8761619925498962}]}, {"text": "Each tweet was annotated by at least two workers.", "labels": [], "entities": []}, {"text": "We used a series of tests to identify bad workers.", "labels": [], "entities": []}, {"text": "These tests include (1) identifying workers with poor pairwise agreement, (2) identifying workers with poor performance on English language annotation, (3) identifying workers that were annotating at unrealistic speeds, (4) identifying workers with near random annotation distributions, and (5) identifying workers that annotate each tweet fora given TV show the same (or nearly the same) way.", "labels": [], "entities": []}, {"text": "We manually inspected any worker with low performance on any of these tests before we made a final decision about using any of their annotations.", "labels": [], "entities": []}, {"text": "For further quality control, we also gathered additional annotations from additional workers for tweets where only one out of two workers identified an emotion.", "labels": [], "entities": []}, {"text": "After these quality control steps we defined minimum emotion annotation thresholds to determine and assign preliminary emotion labels to tweets.", "labels": [], "entities": []}, {"text": "Note that some tweets were discarded as mixed examples for each emotion based upon thresholds for how many times they were tagged, and it resulted in different number of tweets in each emotion dataset.", "labels": [], "entities": []}, {"text": "See for the statistics of the annotations collected from AMT.", "labels": [], "entities": [{"text": "AMT", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8020933866500854}]}, {"text": "Ground Truth Annotation: After we obtained the annotated dataset from AMT, we posted the same dataset (without the labels) to a group of expert annotators.", "labels": [], "entities": [{"text": "AMT", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8246886134147644}]}, {"text": "The experts followed the same annotation guidelines, and each tweet was labeled by at least two experts.", "labels": [], "entities": []}, {"text": "When there was a disagreement between two experts, they discussed to reach an agreement or gathered additional opinion from another expert to decide the label of a tweet.", "labels": [], "entities": []}, {"text": "We used this annotated dataset as ground truth.", "labels": [], "entities": []}, {"text": "See for the statistics of the ground truth annotations.", "labels": [], "entities": []}, {"text": "Compared with the ground truth, many emotion bearing tweets were missed by the AMT annotators, despite the quality control we applied.", "labels": [], "entities": [{"text": "emotion bearing tweets", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7789526581764221}, {"text": "AMT annotators", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9040848314762115}]}, {"text": "aggravates the confirmation bias -the minority class examples are especially easy to miss when labeling quickly due to their rare presence in the dataset.", "labels": [], "entities": []}, {"text": "Evaluation Metric: We evaluated the results with both Mean Average Precision (MAP) and F1 Score.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 54, "end_pos": 82, "type": "METRIC", "confidence": 0.9684410293896993}, {"text": "F1 Score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9877511262893677}]}, {"text": "Average Precision (AP) is the average of the algorithm's precision at every position in the confidence ranked list of results where a true emotional document has been identified.", "labels": [], "entities": [{"text": "Average Precision (AP)", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.9329674243927002}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9988971948623657}]}, {"text": "Thus, AP places extra emphasis on getting the front of the list correct.", "labels": [], "entities": [{"text": "AP", "start_pos": 6, "end_pos": 8, "type": "DATASET", "confidence": 0.5530967116355896}]}, {"text": "MAP is the mean of the average precision scores for each ranked list.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9565725922584534}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9888527393341064}]}, {"text": "This is highly desirable for many practical application such as intelligent search, recommendation, and target advertising where users almost never see results that are not at the top of the list.", "labels": [], "entities": []}, {"text": "F1 is a widely-used measure of classification accuracy.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9949960708618164}, {"text": "classification", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.9531220197677612}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8533716201782227}]}, {"text": "Methods: We evaluated the overall performance relative to the common SVM bag of words approach that can be ubiquitously found in text mining literature.", "labels": [], "entities": [{"text": "text mining", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.7480158805847168}]}, {"text": "We implemented the following four classification methods: \u2022 Delta-IDF: Takes the dot product of the Delta IDF weight vector (Formula 1) with the document's term frequency vector.", "labels": [], "entities": []}, {"text": "\u2022 Spread: Takes the dot product of the distribution spread weight vector (Formula 3) with the document's term frequency vector.", "labels": [], "entities": [{"text": "Spread", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9245911836624146}]}, {"text": "For all the experiments, we used spread parameter s = 2.", "labels": [], "entities": [{"text": "spread parameter s", "start_pos": 33, "end_pos": 51, "type": "METRIC", "confidence": 0.9605361421902975}]}, {"text": "\u2022 SVM-TF: Uses a bag of words SVM with term frequency weights.", "labels": [], "entities": []}, {"text": "\u2022 SVM-Delta-IDF: Uses a bag of words SVM classification with TF.Delta-IDF weights (Formula 2) in the feature vectors before training or testing an SVM.", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7897903621196747}]}, {"text": "We employed each method to build the active learner C described in Algorithm 1.", "labels": [], "entities": []}, {"text": "We used standard bag of unigram and bigram words representation and topic-based fold cross validation.", "labels": [], "entities": []}, {"text": "Since in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data, we defined a test fold for each TV show or movie in our labeled data set.", "labels": [], "entities": []}, {"text": "Each test fold corresponded to a training fold containing all the labeled data from all the other TV shows and movies.", "labels": [], "entities": []}, {"text": "We call it topic-based fold cross validation.", "labels": [], "entities": [{"text": "topic-based fold cross validation", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.5787998288869858}]}, {"text": "We built the SVM classifiers using LIB-LINEAR and applied its L2-regularized support vector regression model.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.829023540019989}]}, {"text": "Based on the dot product or SVM regression scores, we ranked the tweets by how strongly they express the emotion.", "labels": [], "entities": []}, {"text": "We selected the top m tweets with the highest dot product or regression scores but conflicting preliminary AMT labels as the suspected mislabeled instances for re-annotation, just as described in Algorithm 1.", "labels": [], "entities": []}, {"text": "For the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances.", "labels": [], "entities": []}, {"text": "Since the dataset is highly imbalanced, we applied the under-sampling strategy when training the classifiers.", "labels": [], "entities": []}, {"text": "compares the performance of different approaches in each iteration after a certain number of potentially mislabeled instances are re-annotated.", "labels": [], "entities": []}, {"text": "The X axis shows the total number of data points that have been examined for each emotion so far till the current iteration (i.e., 300, 900, 1800, 3000, 4500, 6900, 10500, 16500, and 26100).", "labels": [], "entities": [{"text": "X axis", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9597106873989105}]}, {"text": "We reported both the macro-averaged MAP) and the macro-averaged F1 Score on eight emotions as the overall performance of three competitive methodsSpread, SVM-Delta-IDF and SVM-TF.", "labels": [], "entities": [{"text": "MAP", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.7642972469329834}, {"text": "F1 Score", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9799648225307465}, {"text": "SVM-TF", "start_pos": 172, "end_pos": 178, "type": "DATASET", "confidence": 0.9108334183692932}]}, {"text": "We have also conducted experiments using Delta-IDF, but its performance is low and not comparable with the other three methods.", "labels": [], "entities": []}, {"text": "Generally, shows consistent performance gains as more labels are corrected during active learning.", "labels": [], "entities": []}, {"text": "In comparison, SVM-Delta-IDF significantly outperforms SVM-TF with respect to both MAP and F1 Score.", "labels": [], "entities": [{"text": "MAP", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9755154848098755}, {"text": "F1 Score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9807437658309937}]}, {"text": "SVM-TF achieves higher MAP and F1 Score than Spread at the first few iterations, but then it is beat by Spread after 16,500 tweets had been selected and re-annotated till the eighth iteration.", "labels": [], "entities": [{"text": "SVM-TF", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.716252326965332}, {"text": "MAP", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9938795566558838}, {"text": "F1 Score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.876290887594223}]}, {"text": "Overall, at the end of the active learning process, Spread outperforms SVM-TF by 3.03% the MAP score (and by 4.29% the F1 score), and SVM-Delta-IDF outperforms SVM-TF by 8.59% the MAP score (and by 5.26% the F1 score).", "labels": [], "entities": [{"text": "MAP score", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9073770046234131}, {"text": "F1 score", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9851740896701813}, {"text": "MAP score", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.7658658623695374}, {"text": "F1 score", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9834276139736176}]}, {"text": "Spread achieves a F1 Score of 58.84%, which is quite competitive compared to 59.82% achieved by SVM-Delta-IDF, though SVM-Delta-IDF outperforms Spread with respect to MAP.", "labels": [], "entities": [{"text": "F1 Score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9556666612625122}]}, {"text": "Spread and Delta-IDF are superior with respect to the time efficiency.", "labels": [], "entities": [{"text": "Spread", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8938889503479004}]}, {"text": "shows the average training time of the four methods on eight emotions.", "labels": [], "entities": []}, {"text": "The time spent training SVM-TF classifiers is twice that of SVM-Delta-IDF classifiers, 12 times that of Spread classifiers, and 31 times that of Delta-IDF classifiers.", "labels": [], "entities": [{"text": "SVM-TF classifiers", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.5937843918800354}]}, {"text": "In our experiments, on average, it took 258.8 seconds to train a SVM-TF classifier for one emotion.", "labels": [], "entities": []}, {"text": "In comparison, the average training time of a Spread classifier was only 21.4 seconds, and it required almost no parameter tuning.", "labels": [], "entities": []}, {"text": "In total, our method Spread saved up to (258.8 \u2212 21.4) * 9 * 8 = 17092.8 seconds (4.75 hours) over nine iterations of active learning for all the eight emotions.", "labels": [], "entities": []}, {"text": "This is enough time to re-annotate thousands of data points.", "labels": [], "entities": []}, {"text": "The other important quantity to measure is annotation quality.", "labels": [], "entities": []}, {"text": "One measure of improvement for annotation quality is the number of mislabeled instances that can be fixed after a certain number of active learning iterations.", "labels": [], "entities": []}, {"text": "Better methods can fix more labels with fewer iterations.", "labels": [], "entities": []}, {"text": "Besides the four methods, we also implemented a random baseline (Random) which randomly selected the specified number of instances for reannotation in each round.", "labels": [], "entities": []}, {"text": "We compared the improved dataset with the final ground truth at the end of each round to monitor the progress.", "labels": [], "entities": []}, {"text": "reports the accumulated average percentage of corrected labels on all emotions in each iteration of the active learning process.", "labels": [], "entities": [{"text": "accumulated average percentage of corrected labels", "start_pos": 12, "end_pos": 62, "type": "METRIC", "confidence": 0.6366635908683141}]}, {"text": "According to the figure, SVM-Delta-IDF and SVM-TF are the most advantageous methods, followed by Spread and Delta-IDF.", "labels": [], "entities": [{"text": "SVM-TF", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8724097609519958}]}, {"text": "After the last iteration, SVM-Delta-IDF, SVM-TF, Spread and Delta-IDF has fixed 85.23%, 85.85%, 81.05% and 58.66% of the labels, respectively, all of which significantly outperform the Random baseline (29.74%).", "labels": [], "entities": [{"text": "Random baseline", "start_pos": 185, "end_pos": 200, "type": "METRIC", "confidence": 0.9107852280139923}]}], "tableCaptions": [{"text": " Table 1: Amazon Mechanical Turk annotation label counts.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk annotation label counts", "start_pos": 10, "end_pos": 56, "type": "DATASET", "confidence": 0.9031675259272257}]}, {"text": " Table 2: Ground truth annotation label counts for each emotion. 2", "labels": [], "entities": []}]}