{"title": [{"text": "Going beyond sentences when applying tree kernels", "labels": [], "entities": []}], "abstractContent": [{"text": "We go beyond the level of individual sentences applying parse tree kernels to paragraphs.", "labels": [], "entities": []}, {"text": "We build a set of extended trees fora paragraph of text from the individual parse trees for sentences and learn short texts such as search results and social profile postings to take advantage of additional discourse-related information.", "labels": [], "entities": []}, {"text": "Extension is based on coref-erences and rhetoric structure relations between the phrases in different sentences.", "labels": [], "entities": []}, {"text": "We evaluate our approach, tracking relevance classification improvement for multi-sentence search task.", "labels": [], "entities": [{"text": "relevance classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6214123964309692}]}, {"text": "The search problem is formulated as classification of search results into the classes of relevant and irrelevant, learning from the Bing search results.", "labels": [], "entities": []}, {"text": "We compare performances of individual sentence kernels with the ones for extended parse trees and show that adding discourse information to learning data helps to improve classification results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In spite of substantial efforts to formulate a complete linking theory between syntax and semantics, it is not available yet.", "labels": [], "entities": []}, {"text": "Hence the design of syntactic features for automated learning of syntactic structures is still an art.", "labels": [], "entities": []}, {"text": "One of the solutions to systematically treat these syntactic features -tree kernels built over syntactic parse trees.", "labels": [], "entities": []}, {"text": "Convolution tree kernel) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees.", "labels": [], "entities": []}, {"text": "They have found a number of applications in several natural language tasks, e.g. syntactic parsing re-ranking, relation extraction (, named entity recognition and Semantic Role Labeling), pronoun resolution (), question classification ( and machine translation.", "labels": [], "entities": [{"text": "syntactic parsing re-ranking", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.8050374190012614}, {"text": "relation extraction", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.8676344752311707}, {"text": "named entity recognition", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.6036723752816519}, {"text": "Semantic Role Labeling", "start_pos": 163, "end_pos": 185, "type": "TASK", "confidence": 0.7210883299509684}, {"text": "pronoun resolution", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.7738450765609741}, {"text": "question classification", "start_pos": 211, "end_pos": 234, "type": "TASK", "confidence": 0.9202947318553925}, {"text": "machine translation", "start_pos": 241, "end_pos": 260, "type": "TASK", "confidence": 0.7926752269268036}]}, {"text": "The kernel ability to generate large feature sets is useful to quickly model new and not well understood linguistic phenomena in learning machines.", "labels": [], "entities": []}, {"text": "However, it is often possible to manually design features for linear kernels that produce high accuracy and fast computation time whereas the complexity of tree kernels may prevent their application in real scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9925827980041504}]}, {"text": "Many learning algorithms, such as SVM ( can work directly with kernels by replacing the dot product with a particular kernel function.", "labels": [], "entities": []}, {"text": "This useful property of kernel methods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects such as sentences, has made kernel methods an effective solution to modeling structured objects in NLP.", "labels": [], "entities": []}, {"text": "A number of NL tasks require computing of semantic features over paragraphs of text containing multiple sentences.", "labels": [], "entities": []}, {"text": "Doing it in a sentence pair-wise manner is not always accurate, since it is strongly dependent on how information (phrases) is distributed through sentences.", "labels": [], "entities": []}, {"text": "An approach to build a kernel based on more than a single parse tree has been proposed, however without any relations between parse trees or fora different purpose than treating multi-sentence portions of text.", "labels": [], "entities": []}, {"text": "To compensate for parsing errors), a convolution kernel over packed parse forest) is used to mine syntactic features from it directly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9727975130081177}]}, {"text": "A packed forest compactly encodes exponential number of n-best parse trees, and thus containing much more rich structured features than a single parse tree.", "labels": [], "entities": []}, {"text": "This advantage enables the forest kernel not only to be more robust against parsing errors, but also to be able to learn more reliable feature values and help to solve the data sparseness issue that exists in the traditional tree kernel.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9644012451171875}]}, {"text": "On the contrary, in this study we form a tree forest of sequence of sentences in a paragraph of text.", "labels": [], "entities": []}, {"text": "Currently, kernel methods tackle individual sentences.", "labels": [], "entities": []}, {"text": "However, in learning settings where texts include multiple sentences, structures which include paragraph-level information need to be employed.", "labels": [], "entities": []}, {"text": "We demonstrate that in certain domains and certain cases discourse structure is essential for proper classification of texts.", "labels": [], "entities": []}], "datasetContent": [{"text": "To estimate whether additional high-level semantic and discourse information contributes to classical kernel based approach, we compare two sources for trees: \uf0b7 Regular parse trees \uf0b7 Extended parse trees To perform this estimation, we need a corpus including a high number of short texts similar to our example in Introduction.", "labels": [], "entities": []}, {"text": "These texts should have high similarity (otherwise keyword approach would do well), certain discourse structure, and describe some objects (products) in a meaningful application domain.", "labels": [], "entities": []}, {"text": "Unfortunately, to the best of our knowledge such corpus is not available.", "labels": [], "entities": []}, {"text": "Therefore, for comparison of tree kernel performances we decided to use search results, given the query which is a short text.", "labels": [], "entities": []}, {"text": "We rely on search engine APIs following the evaluation settings in the studies on answering complex questions ( ).", "labels": [], "entities": []}, {"text": "Search results typically include texts of fairly high similarity, which is leveraged in our evaluation.", "labels": [], "entities": []}, {"text": "To formulate classification problem on the set of texts obtained as search results, we need to form positive and negative sets.", "labels": [], "entities": [{"text": "formulate classification", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6775734126567841}]}, {"text": "To do that, we select the first n search results as relevant (positive) and also n results towards to tail of search results lists as irrelevant (negative).", "labels": [], "entities": []}, {"text": "In this case each search session yields an individual training (and evaluation) dataset.", "labels": [], "entities": []}, {"text": "The same nature of such data allows averaging of precision and recall, having individual training dataset of a limited size.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9996825456619263}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9995478987693787}]}, {"text": "Hence reliability of our results is achieved not via the size of individual dataset, but instead by the increased number of search sessions.", "labels": [], "entities": [{"text": "reliability", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9835772514343262}]}, {"text": "To assure an abrupt change in relevance proceeding from the head to the tail of search results lists, we use complicated queries including multiple sentences, which are not handled by modern search engines well.", "labels": [], "entities": []}, {"text": "The preparation of search queries (which include multiple sentences) is based on the following steps: 1.", "labels": [], "entities": []}, {"text": "Forming the names of products and their short descriptions 2.", "labels": [], "entities": []}, {"text": "Given (1), find a text including an extended review or opinion about this product.", "labels": [], "entities": []}, {"text": "3. Texts (2) cannot be used as queries as they are.", "labels": [], "entities": []}, {"text": "To form the queries from (2), we need to extract most significant phrases from them; otherwise, search engines are confused which keywords to choose and give either duplicate, or irrelevant results.", "labels": [], "entities": []}, {"text": "These were the longest noun and selected verb phrases from (2).", "labels": [], "entities": []}, {"text": "The analogous steps were conducted for Yahoo Answers data.", "labels": [], "entities": [{"text": "Yahoo Answers data", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.8404287298520406}]}, {"text": "We manually select a 100 most interesting search queries for each domain.", "labels": [], "entities": []}, {"text": "The training/evaluation datasets is formed from search results in the following way.", "labels": [], "entities": []}, {"text": "We obtain a first hundred search results (or less if hundred is not available).", "labels": [], "entities": []}, {"text": "We select 1..20 (or first 20%) of search results as a positive set, and 81..100 as a negative set.", "labels": [], "entities": []}, {"text": "Search results 21..80 form the basis of evaluation dataset, from which we randomly select 10 texts to be classified into the classes of positive or negative.", "labels": [], "entities": []}, {"text": "Hence we have the ratio 4:1 between the training and evaluation datasets.", "labels": [], "entities": []}, {"text": "To motivate our evaluation setting, we rely on the following observations.", "labels": [], "entities": []}, {"text": "In case of searching for complex multi-sentence queries, relevance indeed drops abruptly with proceeding from the first 10-20 search results, as search evaluation results demonstrated ( ).", "labels": [], "entities": [{"text": "relevance", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9943873286247253}]}, {"text": "The order of search results in first 20% and last 20% does not affect our evaluation.", "labels": [], "entities": []}, {"text": "Although the last 20% of search results is not really a \"gold standard\", it is nevertheless a set that can be reasonably separated from the positive set.", "labels": [], "entities": []}, {"text": "If such separation is too easy or too difficult, it would be hard to adequately evaluate the difference between regular parse trees and extended trees for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.8312251269817352}]}, {"text": "Search-based approach to collect texts for evaluation of classification allows reaching maximum degree of experiment automation.", "labels": [], "entities": [{"text": "evaluation of classification", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6706949671109518}, {"text": "automation", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9498859643936157}]}, {"text": "It turned out that the use of tail search results as negative set helps to leverage the high level semantic and discourse information.", "labels": [], "entities": []}, {"text": "Negative examples, as well as positive ones, include most keywords from the queries.", "labels": [], "entities": []}, {"text": "However, the main difference between the positive and negative search results is that the former include much more coreferences and rhetoric structures similar to the query, than the latter set.", "labels": [], "entities": []}, {"text": "The use of the extended trees was beneficial in the cases where phrases from queries are distributed through multiple sentences in search results.", "labels": [], "entities": []}, {"text": "We conducted two independent experiments for each search session, classifying search result snippets and also original texts, extracted from webpages.", "labels": [], "entities": []}, {"text": "For the snippets, we split them into sentence fragments and built extended trees for these fragments of sentences.", "labels": [], "entities": []}, {"text": "For original texts, we extracted all sentences related to the snippet fragments and built extended trees for these sentences.", "labels": [], "entities": []}, {"text": "Training and classification occurs in the automated mode, and the classification assessment is conducted by the members of research group guided by the authors.", "labels": [], "entities": [{"text": "classification", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9569962620735168}, {"text": "classification assessment", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.8789819777011871}]}, {"text": "The assessors only consulted the query and answer snippets.", "labels": [], "entities": []}, {"text": "We used the standard parameters of tree sequence kernels from http://disi.unitn.it/moschitti/Tree-Kernel.htm).", "labels": [], "entities": []}, {"text": "Tree kernel is applied to all tree pairs from two forests.", "labels": [], "entities": []}, {"text": "The latest version of tree kernel learner was obtained from the author.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results for products domain", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results for popular answers do- main", "labels": [], "entities": []}]}