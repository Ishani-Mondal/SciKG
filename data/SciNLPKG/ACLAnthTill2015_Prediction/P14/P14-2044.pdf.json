{"title": [{"text": "POS induction with distributional and morphological information using a distance-dependent Chinese restaurant process", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew approach to inducing the syntactic categories of words, combining their distributional and morphological properties in a joint nonparametric Bayesian model based on the distance-dependent Chinese Restaurant Process.", "labels": [], "entities": []}, {"text": "The prior distribution over word clusterings uses a log-linear model of morphological similarity ; the likelihood function is the probability of generating vector word embeddings.", "labels": [], "entities": [{"text": "word clusterings", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7375574707984924}]}, {"text": "The weights of the morphology model are learned jointly while inducing part-of-speech clusters, encouraging them to co-here with the distributional features.", "labels": [], "entities": []}, {"text": "The resulting algorithm outperforms competitive alternatives on English POS induction.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.44014178216457367}]}], "introductionContent": [{"text": "The morphosyntactic function of words is reflected in two ways: their distributional properties, and their morphological structure.", "labels": [], "entities": []}, {"text": "Each information source has its own advantages and disadvantages.", "labels": [], "entities": []}, {"text": "Distributional similarity varies smoothly with syntactic function, so that words with similar syntactic functions should have similar distributional properties.", "labels": [], "entities": []}, {"text": "In contrast, there can be multiple paradigms fora single morphological inflection (such as past tense in English).", "labels": [], "entities": []}, {"text": "But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears.", "labels": [], "entities": []}, {"text": "These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features.", "labels": [], "entities": [{"text": "induction of syntactic categories", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.8214997500181198}]}, {"text": "But these features are difficult to combine because of their disparate representations.", "labels": [], "entities": []}, {"text": "Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or \"embeddings\" (.", "labels": [], "entities": []}, {"text": "In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance.", "labels": [], "entities": []}, {"text": "Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity.", "labels": [], "entities": []}, {"text": "In this paper we present anew approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP;).", "labels": [], "entities": []}, {"text": "In the dd-CRP, each data point (word type) selects another point to \"follow\"; this chain of following links corresponds to a partition of the data points into clusters.", "labels": [], "entities": []}, {"text": "The probability of word w 1 following w 2 depends on two factors: 1) the distributional similarity between all words in the proposed partition containing w 1 and w 2 , which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w 1 and w 2 , which acts as a prior distribution on the induced clustering.", "labels": [], "entities": []}, {"text": "We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning.", "labels": [], "entities": []}, {"text": "We apply our model to the English section of the the Multext-East corpus) in order to evaluate both against the coarse-grained and fine-grained tags, where the fine-grained tags encode detailed morphological classes.", "labels": [], "entities": [{"text": "Multext-East corpus", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.9297482073307037}]}, {"text": "We find that our model effectively combines morphological features with distributional similarity, outperforming comparable alternative approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data For our experiments we used the English word embeddings from the Polyglot project (AlRfou' et al., 2013) 2 , which provides embeddings trained on Wikipedia texts for 100,000 of the most frequent words in many languages.", "labels": [], "entities": [{"text": "Polyglot project (AlRfou' et al., 2013) 2", "start_pos": 70, "end_pos": 111, "type": "DATASET", "confidence": 0.920374100858515}]}, {"text": "We evaluate on the English part of the MultextEast (MTE) corpus, which provides both coarse-grained and fine-grained POS labels for the text of Orwell's \"1984\".", "labels": [], "entities": [{"text": "MultextEast (MTE) corpus", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.8850402355194091}, {"text": "Orwell's \"1984\"", "start_pos": 144, "end_pos": 159, "type": "DATASET", "confidence": 0.921256709098816}]}, {"text": "Coarse labels consist of 11 main word classes, while the fine-grained tags (104 for English) are sequences of detailed morphological attributes.", "labels": [], "entities": []}, {"text": "Some of these attributes are not well-attested in English (e.g. gender) and some are mostly distinguishable via semantic analysis (e.g. 1st and 2nd person verbs).", "labels": [], "entities": []}, {"text": "Many tags are assigned only to one or a few words.", "labels": [], "entities": []}, {"text": "Scores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative.", "labels": [], "entities": []}, {"text": "Since take the intersection of these two sets for training and evaluation.", "labels": [], "entities": []}, {"text": "Evaluation With a few exceptions; Van Gael et al., 2009), POS induction systems normally require the user to specify the number of desired clusters, and the systems are evaluated with that number set to the number of tags in the gold standard.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.8587705194950104}]}, {"text": "For corpora such as MTE with both fine-grained and coarse-grained tages, previous evaluations have scored against the coarsegrained tags.", "labels": [], "entities": [{"text": "MTE", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8802297711372375}]}, {"text": "Though coarse-grained tags have their place, in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets, which typically distinguish between verb tenses, noun number and gender, and adjectival scale (comparative, superlative, etc.), so we feel that the evaluation against fine-grained tagset is more relevant here.", "labels": [], "entities": []}, {"text": "For better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings.", "labels": [], "entities": []}, {"text": "To provide some measure of the difficulty of the task, we report baseline scores using K-means clustering, which is relatively strong baseline in this task).", "labels": [], "entities": []}, {"text": "There are several measures commonly used for unsupervised POS induction.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9482762813568115}]}, {"text": "We report greedy one-to-one mapping accuracy (1-1)) and the information-theoretic score Vmeasure (V-m), which also varies from 0 to 100%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9654134511947632}, {"text": "information-theoretic score Vmeasure (V-m)", "start_pos": 60, "end_pos": 102, "type": "METRIC", "confidence": 0.750816653172175}]}, {"text": "In previous work it has been common to also report many-toone (m-1) mapping but this measure is particularly sensitive to the number of induced clusters (more clusters yield higher scores), which is variable for our models.", "labels": [], "entities": []}, {"text": "V-m can be somewhat sensitive to the number of clusters) but much less so than m-1.", "labels": [], "entities": []}, {"text": "With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa.", "labels": [], "entities": []}, {"text": "However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes.", "labels": [], "entities": []}, {"text": "In unsupervised POS induction it is standard to report accuracy on tokens even when the model itself works on types.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8962451219558716}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9754644632339478}]}, {"text": "Here we report also type-based measures because these can reveal differences in model behavior even when token-based measures are similar.", "labels": [], "entities": []}, {"text": "Experimental setup For baselines we use Kmeans and the IGMM, which both only learn from the word embeddings.", "labels": [], "entities": [{"text": "IGMM", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9010567665100098}]}, {"text": "The CRP prior in the IGMM has one hyperparameter (the concentration parameter \u03b1); we report results for \u03b1 = 5 and 20.", "labels": [], "entities": [{"text": "IGMM", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.9288580417633057}]}, {"text": "Both the IGMM and ddCRP have four hyperparameters controlling the prior over the Gaussian cluster parameters: \u039b 0 , \u00b5 0 , \u03bd 0 and \u03ba 0 . We set the prior scale matrix \u039b 0 by using the average covariance from a K-means run with K = 200.", "labels": [], "entities": [{"text": "IGMM", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9737598299980164}]}, {"text": "When setting the average covariance as the expected value of the IW distribution the suitable scale matrix can be computed as , where \u03bd 0 is the prior degrees of freedom (which we set to d + 10) and dis the data dimensionality (64 for the Polyglot embeddings).", "labels": [], "entities": [{"text": "IW distribution", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.8588636219501495}]}, {"text": "We set the prior mean \u00b5 0 equal to the sample mean of the data and \u03ba 0 to 0.01.", "labels": [], "entities": [{"text": "prior mean \u00b5 0", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.7701284438371658}]}, {"text": "We experiment with three different priors for the ddCRP model.", "labels": [], "entities": []}, {"text": "All our ddCRP models are nonsequential, allowing cycles to be formed.", "labels": [], "entities": []}, {"text": "The simplest model, ddCRP uniform, uses a uniform prior that sets the distance between any two words equal to one.", "labels": [], "entities": []}, {"text": "The second model, ddCRP learned, uses the log-linear prior with weights learned between each two Gibbs iterations as explained in section 4.", "labels": [], "entities": []}, {"text": "The final model, ddCRP exp, adds the prior exponentiation.", "labels": [], "entities": []}, {"text": "The \u03b1 parameter for the ddCRP is set to 1 in all experiments.", "labels": [], "entities": []}, {"text": "For ddCRP exp, we report results with the exponent a set to 5.: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokens using coarse-grained tags.", "labels": [], "entities": []}, {"text": "For each model we present the number of induced clusters K (or fixed K for K-means) and 1-1 / V-m scores.", "labels": [], "entities": []}, {"text": "The second column under each evaluation setting gives the scores for K-means with K equal to the number of clusters induced by the model in that row.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokens", "labels": [], "entities": []}]}