{"title": [{"text": "Aspect Extraction with Automated Prior Knowledge Learning", "labels": [], "entities": [{"text": "Aspect Extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9613981544971466}]}], "abstractContent": [{"text": "Aspect extraction is an important task in sentiment analysis.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9484648406505585}, {"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9659446775913239}]}, {"text": "Topic modeling is a popular method for the task.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8275860249996185}]}, {"text": "However, unsupervised topic models often generate incoherent aspects.", "labels": [], "entities": []}, {"text": "To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide mod-eling.", "labels": [], "entities": []}, {"text": "In this paper, we take a major step forward and show that in the big data era, without any user input, it is possible to learn prior knowledge automatically from a large amount of review data available on the Web.", "labels": [], "entities": []}, {"text": "Such knowledge can then be used by a topic model to discover more coherent aspects.", "labels": [], "entities": []}, {"text": "There are two key challenges: (1) learning quality knowledge from reviews of diverse domains, and (2) making the model fault-tolerant to handle possibly wrong knowledge.", "labels": [], "entities": []}, {"text": "A novel approach is proposed to solve these problems.", "labels": [], "entities": []}, {"text": "Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aspect extraction aims to extract target entities and their aspects (or attributes) that people have expressed opinions upon (.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9662326276302338}]}, {"text": "For example, in \"The voice is not clear,\" the aspect term is \"voice.\"", "labels": [], "entities": []}, {"text": "Aspect extraction has two subtasks: aspect term extraction and aspect term resolution.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9282249510288239}, {"text": "aspect term extraction", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.5767851173877716}, {"text": "aspect term resolution", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.6638837655385336}]}, {"text": "Aspect term resolution groups extracted synonymous aspect terms together.", "labels": [], "entities": []}, {"text": "For example, \"voice\" and \"sound\" should be grouped together as they refer to the same aspect of phones.", "labels": [], "entities": []}, {"text": "Recently, topic models have been extensively applied to aspect extraction because they can perform both subtasks at the same time while other existing methods all need two separate steps (see.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.834876149892807}]}, {"text": "Traditional topic models such as LDA () and pLSA) are unsupervised methods for extracting latent topics in text documents.", "labels": [], "entities": [{"text": "extracting latent topics in text documents", "start_pos": 79, "end_pos": 121, "type": "TASK", "confidence": 0.7640738983949026}]}, {"text": "Topics are aspects in our task.", "labels": [], "entities": []}, {"text": "Each aspect (or topic) is a distribution over (aspect) terms.", "labels": [], "entities": []}, {"text": "However, researchers have shown that fully unsupervised models often produce incoherent topics because the objective functions of topic models do not always correlate well with human judgments (.", "labels": [], "entities": []}, {"text": "To tackle the problem, several semi-supervised topic models, also called knowledge-based topic models, have been proposed.", "labels": [], "entities": []}, {"text": "DF-LDA) can incorporate two forms of prior knowledge from the user: must-links and cannot-links.", "labels": [], "entities": []}, {"text": "A must-link implies that two terms (or words) should belong to the same topic whereas a cannot-link indicates that two terms should not be in the same topic.", "labels": [], "entities": []}, {"text": "Ina similar but more generic vein, must-sets and cannot-sets are used in MC-LDA ().", "labels": [], "entities": []}, {"text": "They all allow prior knowledge to be specified by the user to guide the modeling process.", "labels": [], "entities": []}, {"text": "In this paper, we take a major step further.", "labels": [], "entities": []}, {"text": "We mine the prior knowledge directly from a large amount of relevant data without any user intervention, and thus make this approach fully automatic.", "labels": [], "entities": []}, {"text": "We hypothesize that it is possible to learn quality prior knowledge from the big data (of reviews) available on the Web.", "labels": [], "entities": []}, {"text": "The intuition is that although every domain is different, there is a decent amount of aspect overlapping across domains.", "labels": [], "entities": []}, {"text": "For example, every product domain has the aspect/topic of \"price,\" most electronic products share the aspect \"battery\" and some also share \"screen.\"", "labels": [], "entities": []}, {"text": "Thus, the shared aspect knowl-edge mined from a set of domains can potentially help improve aspect extraction in each of these domains, as well as in new domains.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.753648430109024}]}, {"text": "Our proposed method aims to achieve this objective.", "labels": [], "entities": []}, {"text": "There are two major challenges: (1) learning quality knowledge from a large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge.", "labels": [], "entities": []}, {"text": "We briefly introduce the proposed method below, which consists of two steps.", "labels": [], "entities": []}, {"text": "Learning quality knowledge: Clearly, learned knowledge from only a single domain can be erroneous.", "labels": [], "entities": []}, {"text": "However, if the learned knowledge is shared by multiple domains, the knowledge is more likely to be of high quality.", "labels": [], "entities": []}, {"text": "We thus propose to first use LDA to learn topics/aspects from each individual domain and then discover the shared aspects (or topics) and aspect terms among a subset of domains.", "labels": [], "entities": []}, {"text": "These shared aspects and aspect terms are more likely to be of good quality.", "labels": [], "entities": []}, {"text": "They can serve as the prior knowledge to guide a model to extract aspects.", "labels": [], "entities": []}, {"text": "A piece of knowledge is a set of semantically coherent (aspect) terms which are likely to belong to the same topic or aspect, i.e., similar to a must-link, but mined automatically.", "labels": [], "entities": []}, {"text": "Extraction guided by learned knowledge: For reliable aspect extraction using the learned prior knowledge, we must account for possible errors in the knowledge.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7843826711177826}]}, {"text": "In particular, apiece of automatically learned knowledge maybe wrong or domain specific (i.e., the words in the knowledge are semantically coherent in some domains but not in others).", "labels": [], "entities": []}, {"text": "To leverage such knowledge, the system must detect those inappropriate pieces of knowledge.", "labels": [], "entities": []}, {"text": "We propose a method to solve this problem, which also results in anew topic model, called AKL (Automated Knowledge LDA), whose inference can exploit the automatically learned prior knowledge and handle the issues of incorrect knowledge to produce superior aspects.", "labels": [], "entities": [{"text": "AKL", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.851235032081604}]}, {"text": "In summary, this paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "It proposes to exploit the big data to learn prior knowledge and leverage the knowledge in topic models to extract more coherent aspects.", "labels": [], "entities": []}, {"text": "The process is fully automatic.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, none of the existing models for aspect extraction is able to achieve this.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.844681590795517}]}, {"text": "2. It proposes an effective method to learn quality knowledge from raw topics produced using review corpora from many different domains.", "labels": [], "entities": []}, {"text": "3. It proposes anew inference mechanism for topic modeling, which can handle incorrect knowledge in aspect extraction.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.8781122267246246}, {"text": "aspect extraction", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.795973539352417}]}], "datasetContent": [{"text": "This section evaluates and compares the proposed AKL model with three baseline models LDA, MC-LDA, and GK-LDA.", "labels": [], "entities": [{"text": "AKL", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7251669764518738}]}, {"text": "LDA () is the most popular unsupervised topic model.", "labels": [], "entities": []}, {"text": "MC-LDA () is a recent knowledge-based model for aspect extraction.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8693323731422424}]}, {"text": "GK-LDA () handles wrong knowledge by setting prior weights using the ratio of word probabilities.", "labels": [], "entities": [{"text": "GK-LDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8909749984741211}]}, {"text": "Our automatically extracted knowledge is provided to these models.", "labels": [], "entities": []}, {"text": "Note that cannot-set of MC-LDA is not used in AKL.", "labels": [], "entities": []}, {"text": "We created a large dataset containing reviews from 36 product domains or types from Amazon.com.", "labels": [], "entities": []}, {"text": "The product domain names are listed in.", "labels": [], "entities": []}, {"text": "Each domain contains 1, 000 reviews.", "labels": [], "entities": []}, {"text": "This gives us 36 domain corpora.", "labels": [], "entities": []}, {"text": "We have made the dataset publically available at the website of the first author.", "labels": [], "entities": []}, {"text": "We followed () to employ standard pre-processing like lemmatization and stopword removal.", "labels": [], "entities": [{"text": "stopword removal", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7664624154567719}]}, {"text": "To have a fair comparison, we also treat each sentence as a document as in ().", "labels": [], "entities": []}, {"text": "For all models, posterior estimates of latent variables were taken with a sampling lag of 20 iterations in the post burn-in phase (first 200 iterations for burn-in) with 2, 000 iterations in total.", "labels": [], "entities": []}, {"text": "The model parameters were tuned on the development set in our pilot experiments and set to \u03b1 = 1, \u03b2 = 0.1, T = 15, and \u03c3 = 0.2.", "labels": [], "entities": [{"text": "T", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.9926162362098694}]}, {"text": "Furthermore, for each cluster, \u03b3 is set proportional to the number of terms in it.", "labels": [], "entities": []}, {"text": "The other parameters for MC-LDA and GK-LDA were set as in their original papers.", "labels": [], "entities": []}, {"text": "For parameters of AKL, we used the top 15 terms for each topic in the clustering phrase.", "labels": [], "entities": [{"text": "AKL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.5295684337615967}]}, {"text": "The number of clusters is set to the number of domains.", "labels": [], "entities": []}, {"text": "We will test the sensitivity of these clustering parameters in Section 6.4.", "labels": [], "entities": [{"text": "Section 6.4", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9188600480556488}]}, {"text": "The minimum support count for frequent pattern mining was set empirically to min(5, 0.4 where #T is the number of transactions (i.e., the number of topics from all domains) in a cluster.", "labels": [], "entities": [{"text": "frequent pattern mining", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.6367134253184}]}, {"text": "Test Settings: We use two test settings as below: 1.", "labels": [], "entities": []}, {"text": "(Sections 6.2, 6.3 and 6.4) Test on the same corpora as those used in learning the prior knowledge.", "labels": [], "entities": []}, {"text": "This is meaningful as the learning phrase is automatic and unsupervised).", "labels": [], "entities": []}, {"text": "2. (Section 6.5) Test on new/unseen domain corpora after knowledge learning.", "labels": [], "entities": []}, {"text": "As our objective is to discover more coherent aspects, we recruited two human judges.", "labels": [], "entities": []}, {"text": "Here we also use the test setting 1.", "labels": [], "entities": []}, {"text": "Each topic is annotated as coherent if the judge feels that most of its top terms coherently represent a real-world product aspect; otherwise incoherent.", "labels": [], "entities": []}, {"text": "For a coherent topic, each top term is annotated as correct if it reflects the aspect represented by the topic; otherwise incorrect.", "labels": [], "entities": []}, {"text": "We labeled the topics of each model at learning iteration 1 where the same pieces of knowledge (extracted from LDA topics at learning iteration 0) are provided to each model.", "labels": [], "entities": []}, {"text": "After learning iteration 1, the gap between AKL and the baseline models tends to widen.", "labels": [], "entities": [{"text": "AKL", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.5497002005577087}]}, {"text": "To be consistent, the results later in Sections 6.4 and 6.5 also show each model at learning iteration 1.", "labels": [], "entities": []}, {"text": "We also notice that after a few learning iterations, the topics from AKL model tend to have some resemblance across domains.", "labels": [], "entities": [{"text": "AKL model", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.782359778881073}]}, {"text": "We found that AKL with 2 learning iterations achieved the best topics.", "labels": [], "entities": [{"text": "AKL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.6178172826766968}]}, {"text": "Note that LDA cannot use any prior knowledge.", "labels": [], "entities": []}, {"text": "We manually labeled results from four domains, i.e., Camera, Computer, Headphone, and GPS.", "labels": [], "entities": []}, {"text": "We chose Headphone as it has a lot of overlapping of topics with other domains because many electronic products use headphone.", "labels": [], "entities": [{"text": "Headphone", "start_pos": 9, "end_pos": 18, "type": "DATASET", "confidence": 0.9255566596984863}]}, {"text": "GPS was chosen because it does not have much topic overlapping with other domains as its aspects are mostly about Navigation and Maps.", "labels": [], "entities": [{"text": "Navigation", "start_pos": 114, "end_pos": 124, "type": "TASK", "confidence": 0.9546306729316711}]}, {"text": "Domains Camera and Computer lay in between.", "labels": [], "entities": []}, {"text": "We want to see how domain overlapping influences the performance of AKL.", "labels": [], "entities": [{"text": "AKL", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8849442601203918}]}, {"text": "Cohen's Kappa scores for annotator agreement are 0.918 (for topics) and 0.872 (for terms).", "labels": [], "entities": []}, {"text": "To measure the results, we compute P recision@n (or p@n) based on the annotations, which was also used in).", "labels": [], "entities": [{"text": "P recision@n", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.8375362902879715}]}, {"text": "shows the precision@n results for n = 5 and 10.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9987015724182129}]}, {"text": "We can see that AKL makes improvements in all 4 domains.", "labels": [], "entities": [{"text": "AKL", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.5738233923912048}]}, {"text": "The improvement varies in domains with the most increase in Headphone and the least in GPS as Headphone overlaps more with other domains than GPS.", "labels": [], "entities": [{"text": "Headphone", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.9216442704200745}]}, {"text": "Note that if a domain shares aspects with many other domains, its model should benefit more; otherwise, it is reasonable to expect lesser improvements.", "labels": [], "entities": []}, {"text": "For the baselines, GK-LDA and MC-LDA perform similarly to LDA with minor variations, all of which are inferior to AKL.", "labels": [], "entities": [{"text": "AKL", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8133059144020081}]}, {"text": "AKL's improvements over other models are statistically significant based on paired t-test (p < 0.002).", "labels": [], "entities": [{"text": "AKL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7822995781898499}]}, {"text": "In terms of the number of coherent topics, AKL discovers one more coherent topic than LDA in Computer and one more coherent topic than GK-LDA and MC-LDA in Headphone.", "labels": [], "entities": [{"text": "AKL", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.6309434771537781}]}, {"text": "For the other domains, the numbers of coherent topics are the same for all models.", "labels": [], "entities": []}, {"text": "shows an example aspect (battery) and its top 10 terms produced by AKL and LDA for each domain to give a flavor of the kind of improvements made by AKL.", "labels": [], "entities": [{"text": "AKL", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9584567546844482}, {"text": "AKL", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.9380202293395996}]}, {"text": "The results for GK-LDA and MC-LDA are about the same as LDA (see also).", "labels": [], "entities": []}, {"text": "focuses on the aspects generated by AKL and LDA.", "labels": [], "entities": []}, {"text": "From, we can see that AKL discovers more correct and meaningful aspect terms at the top.", "labels": [], "entities": [{"text": "AKL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.537580668926239}]}, {"text": "Note that those terms marked in red and italicized are errors.", "labels": [], "entities": []}, {"text": "Apart from Table 2, many aspects are dramatically improved by AKL, including some commonly shared aspects such as Price, Screen, and Customer Service.", "labels": [], "entities": [{"text": "AKL", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.7845688462257385}, {"text": "Price", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.8940734267234802}, {"text": "Screen", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.910480260848999}]}], "tableCaptions": [{"text": " Tablet  Webcam  Cell Phone  Home Theater System  Monitor  Radar Detector Telephone Wireless Router  Computer  Keyboard  Mouse  Remote Control  TV  Xbox", "labels": [], "entities": [{"text": "Tablet  Webcam  Cell Phone  Home Theater System  Monitor  Radar Detector Telephone Wireless Router  Computer  Keyboard  Mouse  Remote Control  TV", "start_pos": 1, "end_pos": 146, "type": "TASK", "confidence": 0.5922276754128305}]}, {"text": " Table 2: Example aspect Battery from AKL and LDA in each domain. Errors are italicized in red.", "labels": [], "entities": [{"text": "AKL", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.7631133794784546}]}]}