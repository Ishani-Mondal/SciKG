{"title": [{"text": "Lexical Inference over Multi-Word Predicates: A Distributional Approach", "labels": [], "entities": [{"text": "Lexical Inference over Multi-Word Predicates", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8430727124214172}]}], "abstractContent": [{"text": "Representing predicates in terms of their argument distribution is common practice in NLP.", "labels": [], "entities": []}, {"text": "Multi-word predicates (MWPs) in this context are often either disregarded or considered as fixed expressions.", "labels": [], "entities": [{"text": "Multi-word predicates (MWPs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8190680921077729}]}, {"text": "The latter treatment is unsatisfactory in two ways: (1) identifying MWPs is notoriously difficult , (2) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts.", "labels": [], "entities": [{"text": "identifying MWPs", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7233460247516632}]}, {"text": "We propose a novel approach that integrates the distributional representation of multiple subsets of the MWP's words.", "labels": [], "entities": []}, {"text": "We assume a latent distribution over subsets of the MWP, and estimate it relative to a downstream prediction task.", "labels": [], "entities": []}, {"text": "Focusing on the supervised identification of lexical inference relations, we compare against state-of-the-art baselines that consider a single subset of an MWP, obtaining substantial improvements.", "labels": [], "entities": [{"text": "identification of lexical inference relations", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.7026069402694702}]}, {"text": "To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-word expressions (MWEs) constitute a large part of the lexicon and account for much of its growth).", "labels": [], "entities": []}, {"text": "However, despite their importance, MWEs remain difficult to define and model, and consequently pose serious difficulties for NLP applications ().", "labels": [], "entities": []}, {"text": "Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper.", "labels": [], "entities": []}, {"text": "MWPs are informally defined as multiple words that constitute a single predicate ().", "labels": [], "entities": []}, {"text": "MWPs encompass a wide range of phenomena, including causatives, light verbs, phrasal verbs, serial verb constructions and many others, and pose considerable challenges to both linguistic theory and NLP applications (see Section 2).", "labels": [], "entities": []}, {"text": "Part of the difficulty in treating them stems from their position on the borderline between syntax and the lexicon.", "labels": [], "entities": []}, {"text": "It is therefore often unclear whether they should be treated as fixed expressions, as compositional phrases that reflect the properties of their component parts or as both.", "labels": [], "entities": []}, {"text": "This work addresses the modelling of MWPs within the context of distributional semantics, in which predicates are represented through the distribution of arguments they may take.", "labels": [], "entities": []}, {"text": "In order to collect meaningful statistics, the predicate's lexical unit should be sufficiently frequent and semantically unambiguous.", "labels": [], "entities": []}, {"text": "MWPs pose a challenge to such models, as na\u00a8\u0131velyna\u00a8\u0131vely collecting statistics overall instances of highly ambiguous verbs is likely to result in noisy representations.", "labels": [], "entities": []}, {"text": "For instance, the verb \"take\" may appear in MWPs as varied as \"take time\", \"take effect\" and \"take to the hills\".", "labels": [], "entities": []}, {"text": "This heterogeneity of \"take\" is likely to have a negative effect on downstream systems that use its distributional representation.", "labels": [], "entities": []}, {"text": "For instance, while \"take\" and \"accept\" are often considered lexically similar, the high frequency in which \"take\" participates in non-compositional MWPs is likely to push the two verbs' distributional representations apart.", "labels": [], "entities": []}, {"text": "A straightforward approach to this problem is to represent the predicate as a conjunction of multiple words, thereby trading ambiguity for sparsity.", "labels": [], "entities": []}, {"text": "For instance, the verb \"take\" could be conjoined with its object (e.g., \"take care\", \"take a bus\").", "labels": [], "entities": []}, {"text": "This approach, however, raises the challenge of identifying the sub-set of the predicate's words that should betaken to represent it (henceforth, its lexical components or LCs).", "labels": [], "entities": []}, {"text": "We propose a novel approach that addresses this challenge in the context of identifying lexical inference relations between predicates (;, inter alia).", "labels": [], "entities": []}, {"text": "A (lexical) inference relation p L \u2192 p R is said to hold if the relation denoted by p R generally holds between a set of arguments whenever the relation p L does.", "labels": [], "entities": []}, {"text": "For instance, an inference relation holds between \"annex\" and \"control\" since if a country annexes another, it generally controls it.", "labels": [], "entities": []}, {"text": "Most works to this task use distributional similarity, either as their main component (), or as part of a more comprehensive system.", "labels": [], "entities": []}, {"text": "For example, consider the verb \"take\".", "labels": [], "entities": []}, {"text": "While the inference relation \"have \u2192 take\" does not generally hold, it does hold in the case of some light verbs, such as \"have a look \u2192 take a look\", underscoring the importance of taking more inclusive LCs into account.", "labels": [], "entities": []}, {"text": "On the other hand, the predicate \"likely to give a green light\" is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., \"likely\" or \"give a green light\") into account.", "labels": [], "entities": []}, {"text": "We present a novel approach to the task that models the selection and relative weighting of the predicate's LCs using latent variables.", "labels": [], "entities": []}, {"text": "This approach allows the classifier that uses the distributional representations to take into account the most relevant LCs in order to make the prediction.", "labels": [], "entities": []}, {"text": "By doing so, we avoid the notoriously difficult problem of defining and identifying MWPs and account for predicates of various sizes and degrees of compositionality.", "labels": [], "entities": [{"text": "defining and identifying MWPs", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.574206992983818}]}, {"text": "To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics.", "labels": [], "entities": []}, {"text": "We conduct experiments on the dataset of Zeichner et al.", "labels": [], "entities": []}, {"text": "(2012) and compare our methods with analogous ones that select a fixed LC, using stateof-the-art feature sets.", "labels": [], "entities": []}, {"text": "Our method obtains substantial performance gains across all scenarios.", "labels": [], "entities": []}, {"text": "Finally, we note that our approach is cognitively appealing.", "labels": [], "entities": []}, {"text": "Significant cognitive findings support the claim that a speaker's lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance).", "labels": [], "entities": []}], "datasetContent": [{"text": "As a reference corpus R, we use), a web-based corpus consisting of 15M web extractions of binary relations.", "labels": [], "entities": []}, {"text": "Each relation is a triplet of a predicate and two arguments, one preceding it and one following it.", "labels": [], "entities": []}, {"text": "Relations were extracted using regular expressions over the output of a POS tagger and an NP chunker.", "labels": [], "entities": []}, {"text": "Each predicate may consist of a single verb, a verb and a preposition or a sequence of words starting in a verb and ending in a preposition, between which there may nouns, adjectives, adverbs, pronouns, determiners and verbs.", "labels": [], "entities": []}, {"text": "The verb may also be a copula.", "labels": [], "entities": []}, {"text": "Examples of predicates are \"make the most of\", \"could be exchanged for\" and \"is happy with\".", "labels": [], "entities": []}, {"text": "Reverb is an appealing reference corpus for this task for several reasons.", "labels": [], "entities": [{"text": "Reverb", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9343924522399902}]}, {"text": "First, it uses fairly shallow preprocessing technology which is available for many domains and languages.", "labels": [], "entities": []}, {"text": "Second, Reverb applies considerable noise filtering, which results in extractions of fair quality.", "labels": [], "entities": [{"text": "Reverb", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.8420363068580627}]}, {"text": "Third, our evaluation dataset is based on Reverb extractions.", "labels": [], "entities": [{"text": "Reverb extractions", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6838874220848083}]}, {"text": "We evaluate our algorithm on the dataset of.", "labels": [], "entities": []}, {"text": "This publicly available corpus 3 provides pairs of Reverb binary relations and an indication of whether an inference relation holds between them within the context of a specific pair of argument fillers.", "labels": [], "entities": []}, {"text": "The corpus was compiled using distributional methods to detect pairs of relations in Reverb that are likely to have an inference relation between.", "labels": [], "entities": [{"text": "Reverb", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9622970223426819}]}, {"text": "Annotators, employed through Amazon Mechanical Turk, were then asked to determine whether each pair is meaningful, and if so, to determine whether an inference relation holds.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9461725950241089}]}, {"text": "Further measures were taken to monitor the accuracy of the annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9992350339889526}]}, {"text": "For example, the pair of predicates \"make the most of\" and \"take advantage of\" appears in the corpus as a pair between which an inference relation holds.", "labels": [], "entities": []}, {"text": "The arguments in this case are \"students\" and \"their university experience\".", "labels": [], "entities": []}, {"text": "An ex-ample of a pair between which an inference relation does not hold is \"tend to neglect\" and \"underestimate the importance of\", where the arguments are \"Robert\" and \"his family\".", "labels": [], "entities": []}, {"text": "The dataset contains 6,565 instances in total.", "labels": [], "entities": []}, {"text": "We use 5,411 pairs of them, discarding instances that were deemed as meaningless by the annotators.", "labels": [], "entities": []}, {"text": "We also discard cases where the set of arguments is reversed between the LHS and RHS predicates.", "labels": [], "entities": [{"text": "RHS", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.623093843460083}]}, {"text": "In these examples, p R (x, y) is inferable from p L (y, x), rather than from p L (x, y).", "labels": [], "entities": []}, {"text": "As there are less than 150 reversed instances in the corpus, experimenting on this sub-set is unlikely to be informative.", "labels": [], "entities": []}, {"text": "The average length of a predicate in the corpus is 2.7 words (including function words).", "labels": [], "entities": []}, {"text": "In 87.3% of the predicate pairs, there was more than one LC (i.e., |H p | > 1), underscoring the importance of correctly leveraging the different LCs.", "labels": [], "entities": []}, {"text": "We randomly partition the corpus into a training set which contains 4,343 instances (\u223c80%), and a test set that contains 1,068 instances, maintaining the same positive to negative label ratio in both datasets . Development was carried out using cross-validation on the training data (see below).", "labels": [], "entities": []}, {"text": "We use a Maximum Entropy POS Tagger, trained on the Penn Treebank, and the WordNet lemmatizer, both implemented within the NLTK package ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9963074624538422}, {"text": "WordNet lemmatizer", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9170257449150085}, {"text": "NLTK package", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9155475199222565}]}, {"text": "To obtain a coarse-grained set of POS tags, we collapse the tag set to 7 categories: nouns, verbs, adjectives, adverbs, prepositions, the word \"to\" and a category that includes all other words.", "labels": [], "entities": []}, {"text": "A Reverb argument is represented as the conjunction of its content words that appear more than 10 times in the corpus.", "labels": [], "entities": []}, {"text": "Function words are defined according to their POS tags and include determiners, possessive pronouns, existential \"there\", numbers and coordinating conjunctions.", "labels": [], "entities": []}, {"text": "Auxiliary verbs and copulas are also considered function words.", "labels": [], "entities": []}, {"text": "To compute the LDA features, we use the online variational Bayes algorithm of as implemented in the Gensim software package.", "labels": [], "entities": [{"text": "Gensim software package", "start_pos": 100, "end_pos": 123, "type": "DATASET", "confidence": 0.8905261754989624}]}, {"text": "The simplest baseline is ALLNEG, which predicts the most frequent label in the dataset (in our case: \"no inference\").", "labels": [], "entities": [{"text": "ALLNEG", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9742530584335327}]}, {"text": "The other evaluated systems are formed by taking various subsets of our feature set.", "labels": [], "entities": []}, {"text": "We experiment with 4 feature sets.", "labels": [], "entities": []}, {"text": "The smallest set, SIM, includes only the similarity features.", "labels": [], "entities": []}, {"text": "This feature set is related to the compositional distributional model of Mitchell and Lapata (2010) (see Section 6).", "labels": [], "entities": []}, {"text": "We note that despite recent advances in identifying predicate inference relations, the DIRT system () remains a strong baseline, and is often used as a component in state-of-the-art systems), and specifically in the two aforementioned works that used the same evaluation corpus.", "labels": [], "entities": []}, {"text": "The next feature set BASIC includes the features found to be most useful during the development of the model: the most frequent POS tag, the frequency features and the feature Common.", "labels": [], "entities": [{"text": "BASIC", "start_pos": 21, "end_pos": 26, "type": "TASK", "confidence": 0.9357941150665283}]}, {"text": "More inclusive is the feature set NO-LDA, which includes all features except the LDA features.", "labels": [], "entities": []}, {"text": "Experiments with this set were performed in order to isolate the effect of the LDA features.", "labels": [], "entities": []}, {"text": "Finally, ALL includes our complete set of features.", "labels": [], "entities": []}, {"text": "The more direct comparison is against partial implementations of our system where the LC h is deterministically selected.", "labels": [], "entities": []}, {"text": "Determining h for each predicate yields a regular log-linear binary classification model.", "labels": [], "entities": []}, {"text": "We use two variants of this baseline.", "labels": [], "entities": []}, {"text": "The first, LEFTMOST, selects the left-most content word for each predicate.", "labels": [], "entities": [{"text": "LEFTMOST", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9690009355545044}]}, {"text": "Similar selection strategy was carried out by.", "labels": [], "entities": []}, {"text": "The second, VPREP, selects h to be the verb along with its following preposition.", "labels": [], "entities": [{"text": "VPREP", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.8409080505371094}]}, {"text": "In cases the predicate contains multiple verbs, the one preceding the preposition is selected, and where the predicate does not contain any non-copula verbs, it regresses to LEFTMOST.", "labels": [], "entities": [{"text": "LEFTMOST", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9829506278038025}]}, {"text": "This LC selection method approximates a baseline that includes subcategorized prepositions.", "labels": [], "entities": [{"text": "LC selection", "start_pos": 5, "end_pos": 17, "type": "TASK", "confidence": 0.8470756411552429}]}, {"text": "Such cases are highly frequent and account fora large portion of the MWPs in English.", "labels": [], "entities": []}, {"text": "Including a verb's preposition in its LC was commonly done in previous work (e.g.,).", "labels": [], "entities": []}, {"text": "We also attempted to identify verb-preposition constructions using a dependency parser.", "labels": [], "entities": []}, {"text": "Unfortunately, our evaluation dataset is only available in a lemmatized version, which posed a difficulty for the parser.", "labels": [], "entities": []}, {"text": "Due to the low quality of the resulting parses, we implemented VPREP using POS-based regular expressions as defined above.", "labels": [], "entities": []}, {"text": "The full model is denoted with LATENTLC.", "labels": [], "entities": [{"text": "LATENTLC", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.963582456111908}]}, {"text": "For each system and feature set, we report results using 10-fold cross-validation on the training set, as well as results on the test set.", "labels": [], "entities": []}, {"text": "Both cases use the same set of parameters determined by crossvalidation on the training set.", "labels": [], "entities": []}, {"text": "As the task at hand is a binary classification problem, we use accuracy scores to rate the performance of our systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9987409710884094}]}, {"text": "presents the results of our experiments.", "labels": [], "entities": []}, {"text": "Rows correspond to the evaluated algorithms, while columns correspond to the feature sets used and the evaluation scenarios (i.e., training set cross-validation or test set evaluation).", "labels": [], "entities": []}, {"text": "Our experiments make first use of this dataset in its fullest form for the problem of supervised learning of inference relations, and may serve as a starting point for further exploration of this dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the various evaluated systems. Accuracy results are presented in percents, followed in the cross vali-", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9995137453079224}]}]}