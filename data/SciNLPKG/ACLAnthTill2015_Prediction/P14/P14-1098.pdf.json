{"title": [{"text": "Structured Learning for Taxonomy Induction with Belief Propagation", "labels": [], "entities": [{"text": "Taxonomy Induction", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8380051851272583}]}], "abstractContent": [{"text": "We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation.", "labels": [], "entities": []}, {"text": "Our model incorporates heterogeneous re-lational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts.", "labels": [], "entities": []}, {"text": "For efficient inference over tax-onomy structures, we use loopy belief propagation along with a directed spanning tree algorithm for the core hyper-nymy factor.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7426210641860962}]}, {"text": "To train the system, we extract sub-structures of WordNet and dis-criminatively learn to reproduce them, using adaptive subgradient stochastic optimization.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9581313729286194}]}, {"text": "On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9291448593139648}, {"text": "error reduction", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.9689037799835205}, {"text": "error reduction", "start_pos": 139, "end_pos": 154, "type": "METRIC", "confidence": 0.9635011255741119}]}, {"text": "On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 41, "end_pos": 65, "type": "METRIC", "confidence": 0.7681827942530314}]}], "introductionContent": [{"text": "Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.6887408296267191}, {"text": "question answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.86257404088974}, {"text": "information extraction", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8149026334285736}, {"text": "textual entailment", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7077538520097733}]}, {"text": "A recent example is IBM's Jeopardy!", "labels": [], "entities": [{"text": "IBM's Jeopardy!", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.40972813218832016}]}, {"text": "system, which used type information to restrict the set of answer candidates.", "labels": [], "entities": []}, {"text": "Information of this sort is present in term taxonomies (e.g.,), ontologies, and thesauri.", "labels": [], "entities": []}, {"text": "However, currently available taxonomies such as WordNet are incomplete in coverage, unavailable in many domains and languages, and time-intensive to create or extend manually.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9741383194923401}]}, {"text": "There has thus been considerable interest in building lexical taxonomies automatically.", "labels": [], "entities": []}, {"text": "In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output.", "labels": [], "entities": []}, {"text": "Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edges with a shared parent), as well as incorporating a global (directed spanning tree) structural constraint.", "labels": [], "entities": []}, {"text": "Inference for both learning and decoding uses structured loopy belief propagation (BP), incorporating standard spanning tree algorithms (.", "labels": [], "entities": [{"text": "structured loopy belief propagation (BP)", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.7393280778612409}]}, {"text": "The belief propagation approach allows us to efficiently and effectively incorporate heterogeneous relational evidence via hypernymy and siblinghood (e.g., coordination) cues, which we capture by semantic features based on simple surface patterns and statistics from Web n-grams and Wikipedia abstracts.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7210998684167862}]}, {"text": "We train our model to maximize the likelihood of existing example ontologies using stochastic optimization, automatically learning the most useful relational patterns for full taxonomy induction.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.8091486394405365}]}, {"text": "As an example of the relational patterns that our system learns, suppose we are interested in building a taxonomy for types of mammals (see.", "labels": [], "entities": []}, {"text": "Frequent attestation of hypernymy patterns like rat is a rodent in large corpora is a strong signal of the link rodent \u2192 rat.", "labels": [], "entities": []}, {"text": "Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent \u2192 rat and rodent \u2192 squirrel.", "labels": [], "entities": []}, {"text": "Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively.", "labels": [], "entities": []}, {"text": "There have been several previous studies on taxonomy induction.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9467920064926147}]}, {"text": "e.g., the incremental taxonomy induction system of, the longest path approach of, and the maximum spanning tree (MST) approach of Navigli et al.", "labels": [], "entities": []}, {"text": "(2011) (see Section 4 fora more detailed overview).", "labels": [], "entities": []}, {"text": "The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases.", "labels": [], "entities": []}, {"text": "Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were hand-selected or learned via pairwise classifiers on manually annotated co-occurrence patterns.", "labels": [], "entities": []}, {"text": "Finally, it is the first end-to-end (i.e., nonincremental) system to include sibling (e.g., coordination) patterns at all.", "labels": [], "entities": []}, {"text": "We test our approach in two ways.", "labels": [], "entities": []}, {"text": "First, on the task of recreating fragments of WordNet, we achieve a 51% error reduction on ancestor-based F1 over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.954927921295166}, {"text": "error reduction", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.9753116369247437}, {"text": "error reduction", "start_pos": 149, "end_pos": 164, "type": "METRIC", "confidence": 0.9663158059120178}]}, {"text": "Second, we also compare to the results of by predicting the large animal subtree of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.984713077545166}]}, {"text": "Here, we getup to 29% relative error reduction on ancestorbased F1.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.7846492330233256}, {"text": "ancestorbased", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8796970248222351}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.5952244997024536}]}, {"text": "We note that our approach falls at a different point in the space of performance tradeoffs from past work -by producing complete, highly articulated trees, we naturally see a more even balance between precision and recall, while past work generally focused on precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9977974891662598}, {"text": "recall", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.9752419590950012}, {"text": "precision", "start_pos": 260, "end_pos": 269, "type": "METRIC", "confidence": 0.9926384687423706}]}, {"text": "1 To While different applications will value precision and recall differently, and past work was often intentionally precision-focused, it is certainly the case that an ideal solution would maximize both.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9976150989532471}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9871814846992493}, {"text": "precision-focused", "start_pos": 117, "end_pos": 134, "type": "METRIC", "confidence": 0.9913122057914734}]}, {"text": "avoid presumption of a single optimal tradeoff, we also present results for precision-based decoding, where we trade off recall for precision.", "labels": [], "entities": [{"text": "precision-based", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.9759490489959717}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9978147745132446}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9977900981903076}]}], "datasetContent": [{"text": "We considered two distinct experimental setups, one that illustrates the general performance of our model by reproducing various medium-sized WordNet domains, and another that facilitates comparison to previous work by reproducing the much larger animal subtree provided by.", "labels": [], "entities": []}, {"text": "General setup: In order to test the accuracy of structured prediction on medium-sized fulldomain taxonomies, we extracted from WordNet 3.0 all bottomed-out full subtrees which had a tree-height of 3 (i.e., 4 nodes from root to leaf), and contained terms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9958983063697815}, {"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9433016777038574}]}, {"text": "11 This gives us 761 non-overlapping trees, which we partition into both these systems include term discovery in the taxonomy building process.", "labels": [], "entities": []}, {"text": "11 Subtrees that had a smaller or larger tree height were discarded in order to avoid overlap between the training and test divisions.", "labels": [], "entities": []}, {"text": "This makes it a much stricter setting than other tasks such as parsing, which usually has repeated sentences, clauses and phrases between training and test sets.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9781683087348938}]}, {"text": "To project WordNet synsets to terms, we used the first (most frequent) term in each synset.", "labels": [], "entities": []}, {"text": "A few WordNet synsets have multiple parents so we only keep the first of each such pair of overlapping trees.", "labels": [], "entities": []}, {"text": "We also discard a few trees with duplicate terms because this is mostly due to the projection of different synsets to the same term, and theoretically makes the tree a graph.", "labels": [], "entities": []}, {"text": "70/15/15% (533/114/114 trees) train/dev/test sets.", "labels": [], "entities": []}, {"text": "Comparison setup: We also compare our method (as closely as possible) with related previous work by testing on the much larger animal subtree made available by, who created this dataset by selecting a set of 'harvested' terms and retrieving all the WordNet hypernyms between each input term and the root (i.e., animal), resulting in \u223c700 terms and \u223c4,300 is-a ancestor-child links.", "labels": [], "entities": []}, {"text": "Our training set for this animal test case was generated from WordNet using the following process: First, we strictly remove the full animal subtree from WordNet in order to avoid any possible overlap with the test data.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9827232360839844}, {"text": "WordNet", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9757297039031982}]}, {"text": "Next, we create random 25-sized trees by picking random nodes as singleton trees, and repeatedly adding child edges from WordNet to the tree.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9358123540878296}]}, {"text": "This process gives us a total of \u223c1600 training trees.", "labels": [], "entities": []}, {"text": "Feature sources: The n-gram semantic features are extracted from the Google n-grams corpus (), a large collection of English n-grams (for n = 1 to 5) and their frequencies computed from almost 1 trillion tokens (95 billion sentences) of Web text.", "labels": [], "entities": [{"text": "Google n-grams corpus", "start_pos": 69, "end_pos": 90, "type": "DATASET", "confidence": 0.7984248995780945}]}, {"text": "The Wikipedia abstracts are obtained via the publicly available dump, which contains almost \u223c4.1 million articles.", "labels": [], "entities": []}, {"text": "Preprocessing includes standard XML parsing and tokenization.", "labels": [], "entities": [{"text": "XML parsing", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.6320845782756805}]}, {"text": "Efficient collection of feature statistics is important because these must be extracted for millions of query pairs (for each potential edge and sibling pair in each term set).", "labels": [], "entities": []}, {"text": "For this, we use a hash-trie on term pairs (similar to that of), and scan once through the n-gram (or abstract) set, skipping many n-grams (or abstracts) based on fast checks of missing unigrams, exceeding length, suffix mismatches, etc.", "labels": [], "entities": []}, {"text": "Ancestor F1: Measures the precision, recall, and F 1 = 2P R/(P + R) of correctly predicted ances-12 This is somewhat different from our general setup where we work with any given set of terms; they start with a large set of leaves which have substantial Web-based relational information based on their selected, hand-picked patterns.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.999553382396698}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9993891716003418}, {"text": "F 1 = 2P R/(P + R)", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.7547615617513657}]}, {"text": "Their data is available at http://www.isi.edu/ \u02dc kozareva/ downloads.html.", "labels": [], "entities": []}, {"text": "We tried this training regimen as different from that of the general setup (which contains only bottomed-out subtrees), so as to match the animal test tree, which is of depth 12 and has intermediate nodes from higher up in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 223, "end_pos": 230, "type": "DATASET", "confidence": 0.9708663821220398}]}, {"text": "We used the 20130102 dump.", "labels": [], "entities": [{"text": "20130102 dump", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.9864721596240997}]}, {"text": "tors, i.e., pairwise is-a relations: shows our main results for ancestor-based evaluation on the general setup.", "labels": [], "entities": []}, {"text": "We present a development set ablation study where we start with the edges-only model) and its random tree baseline (which chooses any arbitrary spanning tree for the term set).", "labels": [], "entities": []}, {"text": "Next, we show results on the edges-only model with surface features (Section 3.1), semantic features (Section 3.2), and both.", "labels": [], "entities": []}, {"text": "We see that both surface and semantic features make substantial contributions, and they also stack.", "labels": [], "entities": []}, {"text": "Finally, we add the sibling factors and features, Section 3.3), which further improves the results significantly (8% absolute and 15% relative error reduction over the edges-only results on the ancestor F1 metric).", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 134, "end_pos": 158, "type": "METRIC", "confidence": 0.8547909061113993}, {"text": "F1", "start_pos": 203, "end_pos": 205, "type": "METRIC", "confidence": 0.8920071125030518}]}, {"text": "The last row shows the final test set results for the full model with all features.", "labels": [], "entities": []}, {"text": "shows our results for comparison to the larger animal dataset of.", "labels": [], "entities": []}, {"text": "In the table, 'Kozareva2010' refers to and 'Navigli2011' refers to.", "labels": [], "entities": []}, {"text": "For appropri- These results are for the 1st order model due to the scale of the animal taxonomy (\u223c700 terms).", "labels": [], "entities": []}, {"text": "For scaling the 2nd order sibling model, one can use approximations, e.g., pruning the set of sibling factors based on 1st order link marginals, or a hierarchical coarse-to-fine approach based on taxonomy induction on subtrees, or a greedy approach of adding a few sibling factors at a time.", "labels": [], "entities": []}, {"text": "The ancestor results are obtained by using the output files provided on their webpage.", "labels": [], "entities": []}, {"text": "ate comparison to each previous work, we show results for two different setups.", "labels": [], "entities": []}, {"text": "The first setup 'Fixed Prediction' assumes that the model knows the true root and leaves of the taxonomy to provide fora somewhat fairer comparison to.", "labels": [], "entities": [{"text": "Fixed Prediction", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7327663898468018}]}, {"text": "We get substantial improvements on ancestor-based recall and F1 (a 29% relative error reduction).", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9717755913734436}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9994730353355408}, {"text": "relative error reduction", "start_pos": 71, "end_pos": 95, "type": "METRIC", "confidence": 0.8447845180829366}]}, {"text": "The second setup 'Free Prediction' assumes no prior knowledge and predicts the full tree (similar to the general setup case).", "labels": [], "entities": []}, {"text": "On this setup, we do compare as closely as possible to and see a small gain in F1, but regardless, we should note that their results are incomparable (denoted by in because they have a different ground-truth data condition: their definition and hypernym extraction phase involves using the Google define keyword, which often returns WordNet glosses itself.", "labels": [], "entities": [{"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9982070922851562}, {"text": "hypernym extraction", "start_pos": 245, "end_pos": 264, "type": "TASK", "confidence": 0.668999120593071}]}], "tableCaptions": [{"text": " Table 1: Main results on our general setup. On the devel- opment set, we present incremental results on the edges-only  model where we start with the chance baseline, then use sur- face features only, semantic features only, and both. Finally,  we add sibling factors and features to get results for the full,  edges+siblings model with all features, and also report the  final test result for this setting.", "labels": [], "entities": []}, {"text": " Table 2: Comparison results on the animal dataset of  Kozareva and Hovy (2010). Here, 'Kozareva2010' refers to  Kozareva and Hovy (2010) and 'Navigli2011' refers to Nav- igli et al. (2011). For appropriate comparison to each previ- ous work, we show our results both for the 'Fixed Prediction'  setup, which assumes the true root and leaves, and for the  'Free Prediction' setup, which doesn't assume any prior in- formation. The results of Navigli et al. (2011) represent a  different ground-truth data condition, making them incompa- rable to our results; see Section 5.3 for details.", "labels": [], "entities": []}]}