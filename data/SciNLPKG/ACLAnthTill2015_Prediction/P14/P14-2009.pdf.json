{"title": [{"text": "Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification", "labels": [], "entities": [{"text": "Target-dependent Twitter Sentiment Classification", "start_pos": 38, "end_pos": 87, "type": "TASK", "confidence": 0.652189701795578}]}], "abstractContent": [{"text": "We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification.", "labels": [], "entities": [{"text": "target-dependent Twitter sentiment classification", "start_pos": 58, "end_pos": 107, "type": "TASK", "confidence": 0.688530683517456}]}, {"text": "AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them.", "labels": [], "entities": []}, {"text": "It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions.", "labels": [], "entities": []}, {"text": "The experimental studies illustrate that AdaRNN improves the baseline methods.", "labels": [], "entities": [{"text": "AdaRNN", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.5460690259933472}]}, {"text": "Furthermore , we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.", "labels": [], "entities": [{"text": "target-dependent Twitter sentiment analysis", "start_pos": 60, "end_pos": 103, "type": "TASK", "confidence": 0.6432465314865112}]}], "introductionContent": [{"text": "Twitter becomes one of the most popular social networking sites, which allows the users to read and post messages (i.e. tweets) up to 140 characters.", "labels": [], "entities": []}, {"text": "Among the great varieties of topics, people in Twitter tend to express their opinions for the brands, celebrities, products and public events.", "labels": [], "entities": []}, {"text": "As a result, it attracts much attention to estimate the crowd's sentiments in Twitter.", "labels": [], "entities": []}, {"text": "For the tweets, our task is to classify their sentiments fora given target as positive, negative, and neutral.", "labels": [], "entities": []}, {"text": "People may mention several entities (or targets) in one tweet, which affects the availabilities for most of existing methods.", "labels": [], "entities": []}, {"text": "For example, the tweet \"@ballmer: windows phone is better than ios!\" has three targets (@ballmer, windows phone, and ios).", "labels": [], "entities": []}, {"text": "The user expresses neutral, positive, and negative sentiments for them, respectively.", "labels": [], "entities": []}, {"text": "If target information is ignored, it is difficult to obtain the correct sentiment fora specified target.", "labels": [], "entities": []}, {"text": "For target-dependent sentiment classification, the manual evaluation of  show that about 40% of errors are caused by not considering the targets in classification.", "labels": [], "entities": [{"text": "target-dependent sentiment classification", "start_pos": 4, "end_pos": 45, "type": "TASK", "confidence": 0.6302970548470815}]}, {"text": "The features used in traditional learning-based methods ( are independent to the targets, hence the results are computed despite what the targets are.", "labels": [], "entities": []}, {"text": "regard the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words.", "labels": [], "entities": []}, {"text": "combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.7044768333435059}]}, {"text": "In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the ability of deep learning models.", "labels": [], "entities": []}, {"text": "The neural models use distributed representation) to automatically learn features for target-dependent sentiment classification.", "labels": [], "entities": [{"text": "target-dependent sentiment classification", "start_pos": 86, "end_pos": 127, "type": "TASK", "confidence": 0.6995055377483368}]}, {"text": "RNN utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset).", "labels": [], "entities": [{"text": "RNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6979517340660095}, {"text": "sentiment analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7546433508396149}, {"text": "movie review dataset", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.611038088798523}]}, {"text": "The recursive neural models employ the semantic composition functions, which enables them to handle the complex compositionalities in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.9156039953231812}]}, {"text": "Specifically, we propose a framework which learns to propagate the sentiments of words towards the target depending on context and syntactic structure.", "labels": [], "entities": []}, {"text": "We employ a novel adaptive multi-compositionality layer in recursive neural network, which is named as).", "labels": [], "entities": []}, {"text": "It consists of more than one composition functions, and we model the adaptive sentiment propagations as learning distributions over these composition functions.", "labels": [], "entities": []}, {"text": "We automatically learn the composition functions and how to select them from supervisions, instead of choosing them heuristically or by hand-crafted rules.", "labels": [], "entities": []}, {"text": "AdaRNN determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena) in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8970652520656586}]}, {"text": "In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it.", "labels": [], "entities": []}, {"text": "The experimental results suggest that our approach yields better performances than the baseline methods.", "labels": [], "entities": []}, {"text": "As illustrated in, we obtain the representation of \"very good\" by the composition of \"very\" and \"good\", and the representation of trigram \"not very good\" is recursively obtained by the vectors of \"not\" and \"very good\".", "labels": [], "entities": []}, {"text": "The dimensions of parent node are calculated by linear combination of the child vectors' dimensions.", "labels": [], "entities": []}, {"text": "The vector representation v is obtained via: where v l , v rare the vectors of its left and right child, g is the composition function, f is the nonlinearity function (such as tanh, sigmoid, softsign, etc.), W \u2208 R D\u00d72D is the composition matrix, and b is the bias vector.", "labels": [], "entities": []}, {"text": "The dimension of v is the same as its child vectors, and it is recursively used in the next step.", "labels": [], "entities": []}, {"text": "Notably, the word vectors in the leaf nodes are regarded as the parameters, and will be updated according to the supervisions.", "labels": [], "entities": []}, {"text": "The vector representation of root node is then fed into a softmax classifier to predict the label.", "labels": [], "entities": []}, {"text": "The k-th element of softmax(x) is exp{x k } j exp{x j } . For a vector, the softmax obtains the distribution over K classes.", "labels": [], "entities": []}, {"text": "Specifically, the predicted distribution is y = softmax (Uv), where y is the predicted distribution, U \u2208 R K\u00d7D is the classification matrix, and v is the vector representation of node.", "labels": [], "entities": []}], "datasetContent": [{"text": "As people tend to post comments for the celebrities, products, and companies, we use these keywords (such as \"bill gates\", \"taylor swift\", \"xbox\", \"windows 7\", \"google\") to query the Twitter API.", "labels": [], "entities": []}, {"text": "After obtaining the tweets, we manually annotate the sentiment labels (negative, neutral, positive) for these targets.", "labels": [], "entities": []}, {"text": "In order to eliminate the effects of data imbalance problem, we randomly sample the tweets and make the data balanced.", "labels": [], "entities": []}, {"text": "Dependency tree: windows is target: ios is target: Figure 2: For the sentence \"windows is better than ios\", we convert its dependency tree for the different targets (windows and ios).", "labels": [], "entities": []}, {"text": "AdaRNN performs semantic compositions in bottom-up manner and forward propagates sentiment information to the target node.", "labels": [], "entities": []}, {"text": "The g 1 , . .", "labels": [], "entities": []}, {"text": ", g C are different composition functions, and the combined vectors and dependency types are used to select them adaptively.", "labels": [], "entities": []}, {"text": "These composition functions decide how to propagate the sentiments to the target. tweets.", "labels": [], "entities": []}, {"text": "We randomly sample some tweets, and they are assigned with sentiment labels by two annotators.", "labels": [], "entities": []}, {"text": "About 82.5% of them have the same labels.", "labels": [], "entities": []}, {"text": "The agreement percentage of polarity classification is higher than subjectivity classification.", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9926652312278748}, {"text": "polarity classification", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8019634187221527}, {"text": "subjectivity classification", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7195179611444473}]}, {"text": "To the best of our knowledge, this is the largest target-dependent Twitter sentiment classification dataset which is annotated manually.", "labels": [], "entities": [{"text": "target-dependent Twitter sentiment classification", "start_pos": 50, "end_pos": 99, "type": "TASK", "confidence": 0.6092564836144447}]}, {"text": "We make the dataset publicly available 1 for research purposes.", "labels": [], "entities": []}, {"text": "We preprocess the tweets by replacing the targets with $T$ and setting their POS tags to NN.) is used for baselines.", "labels": [], "entities": []}, {"text": "A tweet-specific tokenizer) is employed, and the dependency parsing results are computed by Stanford Parser ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.6791993826627731}, {"text": "Stanford Parser", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.7253664135932922}]}, {"text": "The hyper-parameters are chosen by cross-validation on the training split, and the test accuracy and macro-average F1-score score are reported.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9887900352478027}, {"text": "F1-score score", "start_pos": 115, "end_pos": 129, "type": "METRIC", "confidence": 0.9625224173069}]}, {"text": "For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function.", "labels": [], "entities": []}, {"text": "We employ 10 composition matrices in AdaRNN.", "labels": [], "entities": [{"text": "AdaRNN", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.913947343826294}]}, {"text": "The parameters are randomly initialized.", "labels": [], "entities": []}, {"text": "Notably, the word vectors will also be updated.", "labels": [], "entities": []}, {"text": "the target-independent (SVM-indep) and targetdependent features and uses SVM as the classifier.", "labels": [], "entities": []}, {"text": "There are seven rules to extract target-sensitive features.", "labels": [], "entities": []}, {"text": "We do not implement the social graph optimization and target expansion tricks in it.", "labels": [], "entities": [{"text": "social graph optimization and target expansion", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.6880414585272471}]}, {"text": "SVM-conn: The words, punctuations, emoticons, and #hashtags included in the converted dependency tree are used as the features for SVM.", "labels": [], "entities": [{"text": "SVM-conn", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9177518486976624}]}, {"text": "RNN: It is performed on the converted dependency tree without adaptive composition selection.", "labels": [], "entities": []}, {"text": "AdaRNN-w/oE: Our approach without using the dependency types as features in adaptive selection for the composition functions.", "labels": [], "entities": []}, {"text": "AdaRNN-w/E: Our approach with employing the dependency types as features in adaptive selection for the composition functions.", "labels": [], "entities": []}, {"text": "AdaRNN-comb: We combine the root vectors obtained by AdaRNN-w/E with the uni/bi-gram features, and they are fed into a SVM classifier.", "labels": [], "entities": [{"text": "AdaRNN-comb", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9237478971481323}]}], "tableCaptions": [{"text": " Table 1: Evaluation results on target-dependent  Twitter sentiment classification dataset. Our ap- proach outperforms the baseline methods.", "labels": [], "entities": [{"text": "target-dependent  Twitter sentiment classification", "start_pos": 32, "end_pos": 82, "type": "TASK", "confidence": 0.6207960769534111}]}]}