{"title": [{"text": "A Provably Correct Learning Algorithm for Latent-Variable PCFGs", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.4544602930545807}]}], "abstractContent": [{"text": "We introduce a provably correct learning algorithm for latent-variable PCFGs.", "labels": [], "entities": []}, {"text": "The algorithm relies on two steps: first, the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition.", "labels": [], "entities": [{"text": "EM", "start_pos": 192, "end_pos": 194, "type": "METRIC", "confidence": 0.8487669229507446}]}, {"text": "Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9710668921470642}, {"text": "language modeling", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7094613313674927}]}], "introductionContent": [{"text": "Latent-variable PCFGs (L-PCFGs) () give state-of-the-art performance on parsing problems.", "labels": [], "entities": []}, {"text": "The standard approach to parameter estimation in L-PCFGs is the EM algorithm, which has the usual problems with local optima.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.6761434674263}, {"text": "EM", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.8234339952468872}]}, {"text": "Recent work has introduced an alternative algorithm, based on spectral methods, which has provable guarantees.", "labels": [], "entities": []}, {"text": "Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform.", "labels": [], "entities": []}, {"text": "In practice, this is a limitation.", "labels": [], "entities": []}, {"text": "We describe an algorithm that, like EM, returns estimates of the original parameters of an L-PCFG, but, unlike EM, does not suffer from problems of local optima.", "labels": [], "entities": []}, {"text": "The algorithm relies on two key ideas: 1) A matrix decomposition algorithm (section 5) which is applicable to matrices Q of the form Q f,g = h p(h)p(f | h)p(g | h) where p(h), p(f | h) and p(g | h) are multinomial distributions.", "labels": [], "entities": [{"text": "matrix decomposition", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7172155529260635}]}, {"text": "This matrix form has clear relevance to latent variable models.", "labels": [], "entities": []}, {"text": "We apply the matrix decomposition algorithm to a co-occurrence matrix that can be estimated directly from a training set consisting of parse trees without latent annotations.", "labels": [], "entities": []}, {"text": "The resulting parameter estimates give us significant leverage over the learning problem.", "labels": [], "entities": []}, {"text": "2) Optimization of a convex objective function using EM.", "labels": [], "entities": []}, {"text": "We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM.", "labels": [], "entities": []}, {"text": "The algorithm provably learns the parameters of an L-PCFG (theorem 1), under an assumption that each latent state has at least one \"pivot\" feature.", "labels": [], "entities": []}, {"text": "This assumption is similar to the \"pivot word\" assumption used by and in the context of learning topic models.", "labels": [], "entities": []}, {"text": "We describe experiments on learning of LPCFGs, and also on learning of the latent-variable language model of.", "labels": [], "entities": []}, {"text": "A hybrid method, which uses our algorithm as an initializer for EM, performs at the same accuracy as EM, but requires significantly fewer iterations for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9989114999771118}]}, {"text": "While this paper's focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes parsing experiments using the learning algorithm for L-PCFGs.", "labels": [], "entities": []}, {"text": "We use the Penn WSJ treebank () for our experiments.", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9614425897598267}]}, {"text": "Sections 2-21 were used as training data, and sections 0 and 22 were used as development data.", "labels": [], "entities": []}, {"text": "Section 23 was used as the test set.", "labels": [], "entities": []}, {"text": "The experimental setup is the same as described by.", "labels": [], "entities": []}, {"text": "The trees are binarized () and for the EM algorithm we use the initialization method described in.", "labels": [], "entities": []}, {"text": "For the pivot algorithm we use multiple features \u03c4 1 (t) . .", "labels": [], "entities": []}, {"text": "\u03c4 K (t) and \u03c1 1 (o) . .", "labels": [], "entities": [{"text": "\u03c1 1 (o", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.931463211774826}]}, {"text": "\u03c1 L (o) over inside and outside trees, using the features described by. gives the F1 accuracy on the development and test sets for the following methods: For the EM and Pivot+EM algorithms, we give the number of iterations of EM required to reach optimal performance on the development data.", "labels": [], "entities": [{"text": "F1", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9993607401847839}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.8239482641220093}]}, {"text": "The results show that the EM, Spectral, and Pivot+EM algorithms all perform at a very similar level of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.99831223487854}]}, {"text": "The Pivot+EM results show that very few EM iterations-just 2 iterations inmost conditions-are required to reach optimal performance when the Pivot model is used as an initializer for EM.", "labels": [], "entities": []}, {"text": "The Pivot results lag behind the Pivot+EM results by around 2-3%, but they are close enough to optimality to require very few EM iterations when used as an initializer.", "labels": [], "entities": []}, {"text": "We now describe a second set of experiments, on the Saul and Pereira (1997) model for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.800381064414978}]}, {"text": "Define V to be the set of words in the vocabulary.", "labels": [], "entities": []}, {"text": "For any w 1 , w 2 \u2208 V , the Saul and Pereira (1997) model then defines p(w 2 | w 1 ) = m h=1 r(h | w 1 )s(w 2 | h) where r(h | w 1 ) and: Language model perplexity with the Brown corpus and the Gigaword corpus (New York Times portion) for the second half of the development set, and the test set.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 173, "end_pos": 185, "type": "DATASET", "confidence": 0.8687582612037659}, {"text": "Gigaword corpus (New York Times portion", "start_pos": 194, "end_pos": 233, "type": "DATASET", "confidence": 0.87300591809409}]}, {"text": "With EM and Pivot+EM, the number of iterations for EM to reach convergence is given below the perplexity.", "labels": [], "entities": []}, {"text": "The best result for each column (for each m value) is in bold.", "labels": [], "entities": []}, {"text": "The \"test\" column gives perplexity results on the test set.", "labels": [], "entities": []}, {"text": "Each perplexity calculation on the test set is done using the best model on the development set.", "labels": [], "entities": []}, {"text": "bi-KN+int and tri-KN+int are bigram and trigram Kneser-Ney interpolated models, using the SRILM toolkit.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.8600716292858124}]}, {"text": "s(w 2 | h) are parameters of the approach.", "labels": [], "entities": []}, {"text": "The conventional approach to estimation of the parameters r(h | w 1 ) and s(w 2 | h) from a corpus is to use the EM algorithm.", "labels": [], "entities": []}, {"text": "In this section we compare the EM algorithm to a pivot-based method.", "labels": [], "entities": []}, {"text": "It is straightforward to represent this model as an L-PCFG, and hence to use our implementation for estimation.", "labels": [], "entities": []}, {"text": "In this special case, the L-PCFG learning algorithm is equivalent to a simple algorithm, with the following steps: 1) define the matrix Q with entries Q w 1 ,w 2 = count(w 1 , w 2 )/N where count(w 1 , w 2 ) is the number of times that bigram (w 1 , w 2 ) is seen in the data, and N = w 1 ,w 2 count(w 1 , w 2 ).", "labels": [], "entities": [{"text": "count", "start_pos": 164, "end_pos": 169, "type": "METRIC", "confidence": 0.9821518659591675}]}, {"text": "Run the algorithm of section 5.2 on Q to recover estimates\u02c6sestimates\u02c6 estimates\u02c6s(w 2 | h); 2) estimat\u00ea r(h | w 1 ) using the EM algorithm to optimize the function w 1 ,w 2 Q w 1 ,w 2 log h \u02c6 r(h | w 1 )\u02c6 s(w 2 | h) with respect to th\u00ea r parameters; this function is concave in these parameters.", "labels": [], "entities": []}, {"text": "We performed the language modeling experiments fora number of reasons.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7867984175682068}]}, {"text": "First, because in this case the L-PCFG algorithm reduces to a simple algorithm, it allows us to evaluate the core ideas in the method very directly.", "labels": [], "entities": []}, {"text": "Second, it allows us to test the pivot method on the very large datasets that are available for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7423922121524811}]}, {"text": "We use two corpora for our experiments.", "labels": [], "entities": []}, {"text": "The first is the Brown corpus, as used by in language modeling experiments.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8897193968296051}, {"text": "language modeling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7126645594835281}]}, {"text": "Following, we use the first 800K words for training (and replace all words that appear once with an UNK token), the next 200K words for development, and the remaining data (165,171 tokens) as a test set.", "labels": [], "entities": []}, {"text": "The size of the vocabulary is 24,488 words.", "labels": [], "entities": []}, {"text": "The second corpus we use is the New York Times portion of the Gigaword corpus.", "labels": [], "entities": [{"text": "New York Times portion", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.8023777902126312}, {"text": "Gigaword corpus", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.8288107514381409}]}, {"text": "Here, the training set consists of 1.31 billion tokens.", "labels": [], "entities": []}, {"text": "We use 159 million tokens for development set and 156 million tokens for test.", "labels": [], "entities": []}, {"text": "All words that appeared less than 20 times in the training set were replaced with the UNK token.", "labels": [], "entities": [{"text": "UNK token", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.9193991720676422}]}, {"text": "The size of the vocabulary is 235,223 words.", "labels": [], "entities": []}, {"text": "Unknown words in test data are ignored when calculating perplexity (this is the standard set-up in the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.7430855333805084}]}, {"text": "In our experiments we use the first half of each development set to optimize the number of iterations of the EM or Pivot+EM algorithms.", "labels": [], "entities": []}, {"text": "As before, Pivot+EM uses 1 or more EM steps with parameter initialization from the Pivot method.", "labels": [], "entities": []}, {"text": "gives perplexity results for the different algorithms.", "labels": [], "entities": []}, {"text": "As in the parsing experiments, the Pivot method alone performs worse than EM, but the Pivot+EM method gives results that are competitive with EM.", "labels": [], "entities": [{"text": "parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9766460061073303}]}, {"text": "The Pivot+EM method requires fewer iterations of EM than the EM algorithm.", "labels": [], "entities": []}, {"text": "On the Brown corpus the difference is quite dramatic, with only 1 or 2 iterations required, as opposed to 10 or more for EM.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9609899520874023}]}, {"text": "For the NYT corpus the Pivot+EM method requires more iterations (around 10 or 20), but still requires significantly fewer iterations than the EM algorithm.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.949545294046402}]}, {"text": "On the Gigaword corpus, with m = 256, EM takes 12h57m (32 iterations at 24m18s per iteration) compared to 1h50m for the Pivot method.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9318329095840454}, {"text": "EM", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.675289511680603}]}, {"text": "On Brown, EM takes 1m47s (8 iterations) compared to 5m44s for the Pivot method.", "labels": [], "entities": [{"text": "Brown", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9501893520355225}]}, {"text": "Both the EM and pivot algorithm implementations were highly optimized, and written in Matlab.", "labels": [], "entities": []}, {"text": "Results at other values of mare similar.", "labels": [], "entities": []}, {"text": "From these results the Pivot method appears to become more competitive speed-wise as the data size increases (the Gigaword corpus is more than 1,300 times larger than the Brown corpus).", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9355441927909851}, {"text": "Brown corpus", "start_pos": 171, "end_pos": 183, "type": "DATASET", "confidence": 0.791449785232544}]}], "tableCaptions": [{"text": " Table 1: Results on the development data (section 22) and  test data (section 23) for various learning algorithms for L- PCFGs. For EM and pivot+EM experiments, the second line  denotes the number of iterations required to reach the given  optimal performance on development data. Results for sec- tion 23 are used with the best model for section 22 in the cor- responding row. The results for EM and spectral are reported  from Cohen et al. (2013).", "labels": [], "entities": []}]}