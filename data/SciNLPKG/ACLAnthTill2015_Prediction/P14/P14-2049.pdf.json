{"title": [{"text": "Improving sparse word similarity models with asymmetric measures", "labels": [], "entities": [{"text": "Improving sparse word similarity", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8909707516431808}]}], "abstractContent": [{"text": "We show that asymmetric models based on Tversky (1977) improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle-rank words.", "labels": [], "entities": [{"text": "nearest neighbor discovery", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.664568692445755}]}, {"text": "In accord with Tver-sky's discovery that asymmetric similarity judgments arise when comparing sparse and rich representations, improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing high-and mid-frequency words.", "labels": [], "entities": []}], "introductionContent": [{"text": "A key assumption of most models of similarity is that a similarity relation is symmetric.", "labels": [], "entities": []}, {"text": "This assumption is foundational for some conceptions, such as the idea of a similarity space, in which similarity is the inverse of distance; and it is deeply embedded into many of the algorithms that build on a similarity relation among objects, such as clustering algorithms.", "labels": [], "entities": []}, {"text": "The symmetry assumption is not, however, universal, and it is not essential to all applications of similarity, especially when it comes to modeling human similarity judgments.", "labels": [], "entities": [{"text": "symmetry", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9552508592605591}, {"text": "modeling human similarity judgments", "start_pos": 139, "end_pos": 174, "type": "TASK", "confidence": 0.7498941570520401}]}, {"text": "Citing a number of empirical studies, calls symmetry directly into question, and proposes two general models that abandon symmetry.", "labels": [], "entities": []}, {"text": "The one most directly related to a large body of word similarity work that followed is what he calls the ratio model, which defines sim(a, b) as: f (A \u2229 B) f (A \u2229 B) + \u03b1f (A\\B) + \u03b2f (B\\A) Here A and B represent feature sets for the objects a and b respectively; the term in the numerator is a function of the set of shared features, a measure of similarity, and the last two terms in the denominator measure dissimilarity: \u03b1 and \u03b2 are real-number weights; when \u03b1 \ud97b\udf59 = \u03b2, symmetry is abandoned.", "labels": [], "entities": []}, {"text": "To motivate such a measure, Tversky presents experimental data with asymmetric similarity results, including similarity comparisons of countries, line drawings of faces, and letters.", "labels": [], "entities": []}, {"text": "Tversky shows that many similarity judgment tasks have an inherent asymmetry; but he also argues, following, that certain kinds of stimuli are more naturally used as foci or standards than others.", "labels": [], "entities": [{"text": "similarity judgment tasks", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.7640384137630463}]}, {"text": "Goldstone (in press) summarizes the results succinctly: \"Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea.\"", "labels": [], "entities": []}, {"text": "Thus, one source of asymmetry is the comparison of sparse and dense representations.", "labels": [], "entities": []}, {"text": "The relevance of such considerations to word similarity becomes clear when we consider that for many applications, word similarity measures need to be well-defined when comparing very frequent words with infrequent words.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.755433201789856}]}, {"text": "To make this concrete, let us consider a word representation in the word-as-vector paradigm, using a dependency-based model.", "labels": [], "entities": []}, {"text": "Suppose we want to measure the semantic similarity of boat, rank 682 among the nouns in the BNC corpus studied below, which has 1057 nonzero dependency features based on 50 million words of data, with dinghy, rank 6200, which has only 113 nonzero features.", "labels": [], "entities": [{"text": "BNC corpus", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.928063303232193}]}, {"text": "At the level of the vector representations we are using, these are events of very different dimensionality; that is, there are ten times as many features in the representation of boat as there are in the representation of dinghy.", "labels": [], "entities": []}, {"text": "If in Tversky/Rosch terms, the more frequent word is also a more likely focus, then this is exactly the kind of situation in which asymmetric similarity judgments will arise.", "labels": [], "entities": []}, {"text": "Below we show that an asymmetric measure, using \u03b1 and \u03b2 biased in favor of the less frequent word, greatly improves the performance of a dependency-based vector model in capturing human similarity judgments.", "labels": [], "entities": [{"text": "capturing human similarity judgments", "start_pos": 170, "end_pos": 206, "type": "TASK", "confidence": 0.7794651240110397}]}, {"text": "Before presenting these results, it will be helpful to slightly reformulate and slightly generalize Tversky's ratio model.", "labels": [], "entities": []}, {"text": "The reformulation will allow us to directly draw the connection between the ratio model and a set of similarity measures that have played key roles in the similarity literature.", "labels": [], "entities": []}, {"text": "First, since Tversky has primarily additive fin mind, we can reformulate f(A \u2229 B) as follows Next, since we are interested in generalizing from sets of features, to real-valued vectors of features, w 1 , w 2 , we define so with SI set to MIN, Equation (3) includes Equation (2) as a special case.", "labels": [], "entities": [{"text": "MIN", "start_pos": 238, "end_pos": 241, "type": "DATASET", "confidence": 0.7366381287574768}]}, {"text": "Similarly, \u03c3(w 1 , w 1 ) represents the summed feature weights of w 1 , and therefore, In this generalized form, then, (1) becomes (4) Thus, if \u03b1 + \u03b2 = 1, Tversky's ratio model becomes simply: The computational advantage of this reformulation is that the core similarity operation \u03c3(w 1 , w 2 ) is done on what is generally only a small number of shared features, and the \u03c3(w i , w i ) calculations (which we will call self-similarities), can be computed in advance.", "labels": [], "entities": []}, {"text": "Note that sim(w 1 , w 2 ) is symmetric if and only if \u03b1 = 0.5.", "labels": [], "entities": []}, {"text": "When \u03b1 > 0.5, sim(w 1 , w2) is biased in favor of w 1 as the referent; When \u03b1 < 0.5, sim(w 1 , w2) is biased in favor of w 2 . Consider four similarity functions that have played important roles in the literature on similarity: COS(w 1 , w 2 ) = DICE PROD applied to unit vectors (6) The function DICE PROD is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, because it generalized the set comparison function of.", "labels": [], "entities": [{"text": "COS", "start_pos": 228, "end_pos": 231, "type": "METRIC", "confidence": 0.9805635213851929}]}, {"text": "Observe that cosine is a special case of DICE PROD.", "labels": [], "entities": [{"text": "DICE PROD", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.46731892228126526}]}, {"text": "DICE \u2020 was introduced in and was the most successful function in his evaluation.", "labels": [], "entities": [{"text": "DICE \u2020", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8986574709415436}]}, {"text": "Since LIN was introduced in; several different functions have born that name.", "labels": [], "entities": []}, {"text": "The version used here is the one used in.", "labels": [], "entities": []}, {"text": "The three distinct functions in Equation 6 have a similar form.", "labels": [], "entities": []}, {"text": "In fact, all can be defined in terms of \u03c3 functions differing only in their SI operation.", "labels": [], "entities": []}, {"text": "Let \u03c3 SI be a shared feature sum for operation SI, as defined in Equation (3).", "labels": [], "entities": []}, {"text": "We define the Tverskynormalized version of \u03c3 SI , written T SI , as: 1 Note that T SI is just the special case of Tversky's ratio model (5) in which \u03b1 = 0.5 and the similarity measure is symmetric.", "labels": [], "entities": []}, {"text": "We define three SI operations \u03c3 PROD 2 , \u03c3 MIN , and \u03c3 AVG as follows: 1 Paralleling is Jaccard-family normalization: \u03c3JACC(w1, w2) = \u03c3(w1, w2) \u03c3(w1, w1) + \u03c3(w2, w2) \u2212 \u03c3( It is easy to generalize the result from van for the original set-specific versions of show that all of the Tversky family functions discussed above are monotonic in Jaccard.", "labels": [], "entities": [{"text": "AVG", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9197526574134827}]}, {"text": "\u03c3PROD, of course, is dot product.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: System/Human correlations. Above the line: MALT Parser-based systems", "labels": [], "entities": [{"text": "MALT", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.5923392176628113}]}, {"text": " Table 3: Pairs contributing the biggest improve- ment, MC/RG word set", "labels": [], "entities": [{"text": "improve- ment", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9147494633992513}, {"text": "MC/RG word set", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.45778918266296387}]}, {"text": " Table 2: System/Human correlations for Stanford parser systems", "labels": [], "entities": []}]}