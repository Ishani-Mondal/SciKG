{"title": [{"text": "Learning Word Sense Distributions, Detecting Unattested Senses and Identifying Novel Senses Using Topic Models", "labels": [], "entities": [{"text": "Identifying Novel Senses", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.8321899573008219}]}], "abstractContent": [{"text": "Unsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data.", "labels": [], "entities": [{"text": "Unsupervised word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7442894365106311}]}, {"text": "Unsuper-vised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions.", "labels": [], "entities": [{"text": "WSD", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9863220453262329}]}, {"text": "This paper presents a fully unsu-pervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text.", "labels": [], "entities": [{"text": "sense frequency estimation", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.5984008411566416}]}, {"text": "We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren't attested in the corpus, and identifying novel senses in the corpus which aren't captured in the sense inventory.", "labels": [], "entities": [{"text": "predominant sense learning", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.6188413898150126}, {"text": "sense distribution acquisition", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.7452558875083923}]}], "introductionContent": [{"text": "The automatic determination of word sense information has been a long-term pursuit of the NLP community (.", "labels": [], "entities": [{"text": "automatic determination of word sense information", "start_pos": 4, "end_pos": 53, "type": "TASK", "confidence": 0.7532411267360052}]}, {"text": "Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.7744720925887426}]}, {"text": "Such an approach requires knowledge of predominant senses; however, word sense distributions -and predominant senses toovary from corpus to corpus.", "labels": [], "entities": []}, {"text": "Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method which uses topic models to estimate word sense distributions.", "labels": [], "entities": []}, {"text": "This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text.", "labels": [], "entities": []}, {"text": "Topic models have been used for WSD in a number of studies), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions.", "labels": [], "entities": [{"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9862577319145203}, {"text": "acquisition of prior word sense", "start_pos": 136, "end_pos": 167, "type": "TASK", "confidence": 0.7188660740852356}]}, {"text": "Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus.", "labels": [], "entities": []}, {"text": "A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses.", "labels": [], "entities": []}, {"text": "We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses -i.e., senses that are in the sense inventory but not attested in a given corpus.", "labels": [], "entities": []}, {"text": "In contrast to the previous work of on this topic which uses the sense ranking score from to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.9542496204376221}]}, {"text": "Corpus instances of a word can also correspond to senses that are not present in a given sense inventory.", "labels": [], "entities": []}, {"text": "This can be due to, for example, words taking on new meanings overtime (e.g. the rela-tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory.", "labels": [], "entities": []}, {"text": "A system for automatically identifying such novel senses -i.e. senses that are attested in the corpus but not in the sense inventory -would be a very valuable lexicographical tool for keeping sense inventories up-to-date ( . We further propose an application of our proposed method to the identification of such novel senses.", "labels": [], "entities": []}, {"text": "In contrast to, the use of topic models makes this possible, using topics as a proxy for sense.", "labels": [], "entities": []}, {"text": "Earlier work on identifying novel senses focused on individual tokens), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first test the proposed method over the tasks of predominant sense learning and sense distribution induction, using the WordNet-tagged dataset of, which is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE).", "labels": [], "entities": [{"text": "predominant sense learning", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6447504758834839}, {"text": "sense distribution induction", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.7262774308522543}, {"text": "WordNet-tagged dataset", "start_pos": 123, "end_pos": 145, "type": "DATASET", "confidence": 0.9801125526428223}, {"text": "FINANCE", "start_pos": 273, "end_pos": 280, "type": "METRIC", "confidence": 0.9607735276222229}]}, {"text": "For each domain, annotators were asked to sense-annotate a random selection of sentences for each of 40 target nouns, based on WordNet v1.7.", "labels": [], "entities": [{"text": "WordNet v1.7", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.9100692570209503}]}, {"text": "The predominant sense and distribution across senses for each target lemma was obtained by aggregating over the sense annotations.", "labels": [], "entities": []}, {"text": "The authors evaluated their method in terms of WSD accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9119111895561218}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.7896068096160889}]}, {"text": "For the remainder of the paper, we denote their system as MKWC.", "labels": [], "entities": [{"text": "MKWC", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9212167263031006}]}, {"text": "To compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of.", "labels": [], "entities": [{"text": "MKWC", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.962231457233429}]}, {"text": "For each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the WordNet senses (Equation), and rank the senses based on the prevalence scores (Equation).", "labels": [], "entities": [{"text": "WordNet senses", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9025863707065582}, {"text": "prevalence", "start_pos": 179, "end_pos": 189, "type": "METRIC", "confidence": 0.9492019414901733}, {"text": "Equation", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.8747380971908569}]}, {"text": "In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) Acc UB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation); 7 and (2) ERR, the error rate reduction between the accuracy fora given system (Acc) and the upper bound (Acc UB ), calculated as follows: Looking at the results in   both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies.", "labels": [], "entities": [{"text": "WSD", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9153152704238892}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.7568897604942322}, {"text": "Acc UB", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9710728824138641}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.4774765074253082}, {"text": "ERR", "start_pos": 265, "end_pos": 268, "type": "METRIC", "confidence": 0.997333288192749}, {"text": "error rate reduction", "start_pos": 274, "end_pos": 294, "type": "METRIC", "confidence": 0.9679288268089294}, {"text": "accuracy", "start_pos": 307, "end_pos": 315, "type": "METRIC", "confidence": 0.9870010614395142}]}, {"text": "Given that both systems compute a continuousvalued prevalence score for each sense of a target lemma, a distribution of senses can be obtained by normalising the prevalence scores across all senses.", "labels": [], "entities": [{"text": "continuousvalued prevalence score", "start_pos": 34, "end_pos": 67, "type": "METRIC", "confidence": 0.731168270111084}]}, {"text": "The predominant sense learning task of evaluates the ability of a method to identify only the head of this distribution, but it is also important to evaluate the full sense distribution (.", "labels": [], "entities": []}, {"text": "To this end, we introduce a second evaluation metric: the Jensen-Shannon (JS) divergence between the inferred sense distribution and the gold-standard sense distribution, noting that smaller values are better in this case, and that it is now theoretically possible to obtain a JS divergence of 0 in the case of a perfect estimate of the sense distribution.", "labels": [], "entities": []}, {"text": "Results are presented in HDP-WSI consistently achieves lower JS divergence, indicating that the distribution of senses that it finds is closer to the gold standard distribution.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.6400763094425201}]}, {"text": "Testing for statistical significance over the paired JS divergence values for each lemma using the Wilcoxon signed-rank test, the result for FI-NANCE is significant (p < 0.05) but the results for the other two datasets are not (p > 0.1 in each case).: Sense distribution evaluation of HDP-WSI on the Macmillan-annotated datasets as compared to corpus-and dictionary-based first sense methods, evaluated using JS divergence (lower values indicate better performance; the best system in each row is indicated in boldface).", "labels": [], "entities": [{"text": "FI-NANCE", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.5490803122520447}, {"text": "Macmillan-annotated datasets", "start_pos": 300, "end_pos": 328, "type": "DATASET", "confidence": 0.9806058704853058}]}, {"text": "To summarise, the results for MKWC and HDP-WSI are fairly even for predominant sense learning (each outperforms the other at a level of statistical significance over one dataset), but HDP-WSI is better at inducing the overall sense distribution.", "labels": [], "entities": [{"text": "MKWC", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8817855715751648}]}, {"text": "It is important to bear in mind that MKWC in these experiments makes use of full-text parsing in calculating the distributional similarity thesaurus, and the WordNet graph structure in calculating the similarity between associated words and different senses.", "labels": [], "entities": [{"text": "WordNet graph structure", "start_pos": 158, "end_pos": 181, "type": "DATASET", "confidence": 0.9109422961870829}]}, {"text": "Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9719181656837463}]}, {"text": "8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter:), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional \"flat\" sense inventories.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 187, "end_pos": 194, "type": "DATASET", "confidence": 0.9450500011444092}]}, {"text": "While comparable results on a different dataset have been achieved with a proximity thesaurus ( compared to a dependency one, 9 it is not stated how wide a window is needed for the proximity thesaurus.", "labels": [], "entities": []}, {"text": "This could be a significant issue with Twitter data, where context tends to be limited.", "labels": [], "entities": []}, {"text": "In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Macmillan English Dictionary.", "labels": [], "entities": [{"text": "Macmillan English Dictionary", "start_pos": 148, "end_pos": 176, "type": "DATASET", "confidence": 0.9644454518953959}]}, {"text": "In our second set of experiments, we move to anew dataset (Gella et al., to appear) based on text from ukWaC ( and Twitter, and annotated using the Macmillan English Dictionary 10 (henceforth \"Macmillan\").", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.9831129908561707}, {"text": "Macmillan English Dictionary 10", "start_pos": 148, "end_pos": 179, "type": "DATASET", "confidence": 0.9716702103614807}, {"text": "Macmillan", "start_pos": 193, "end_pos": 202, "type": "DATASET", "confidence": 0.7145304083824158}]}, {"text": "For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.", "labels": [], "entities": [{"text": "Macmillan", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.9642349481582642}]}, {"text": "11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (; (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes:).", "labels": [], "entities": [{"text": "Macmillan", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.9747779965400696}, {"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9693111777305603}, {"text": "sense tagging", "start_pos": 179, "end_pos": 192, "type": "TASK", "confidence": 0.7428764402866364}, {"text": "Macmillan", "start_pos": 261, "end_pos": 270, "type": "DATASET", "confidence": 0.9420347213745117}, {"text": "WordNet", "start_pos": 423, "end_pos": 430, "type": "DATASET", "confidence": 0.9714879393577576}]}, {"text": "The dataset is made up of 20 target nouns which were selected to span the high-to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.9923722147941589}]}, {"text": "The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet).", "labels": [], "entities": [{"text": "Macmillan", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.983107328414917}, {"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9644642472267151}]}, {"text": "100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3-Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 () for Twitter, and the POS tags provided with the corpus for ukWaC).", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.9846013784408569}, {"text": "ukWaC", "start_pos": 333, "end_pos": 338, "type": "DATASET", "confidence": 0.9704482555389404}]}, {"text": "Amazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to Macmillan, including allowing the annotators the option to label a usage as \"Other\" in instances where the usage was not captured by any of the Macmillan senses.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8843063712120056}, {"text": "Macmillan", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.9769890904426575}]}, {"text": "After quality control over the annotators/annotations (see (to appear) for details), and aggregation of the annotations into a single sense per usage (possibly \"Other\"), there were 2000 sense-tagged ukWaC sentences and Twitter messages over the 20 target nouns.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 199, "end_pos": 204, "type": "DATASET", "confidence": 0.951195478439331}]}, {"text": "We refer to these two datasets as UKWAC and TWITTER henceforth.", "labels": [], "entities": [{"text": "UKWAC", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.982319176197052}]}, {"text": "To apply our method to the two datasets, we use HDP-WSI to train a model for each target noun, based on the combined set of usages of that lemma in each of the two background corpora, namely the original Twitter crawl that gave rise to the TWIT-TER dataset, and all of ukWaC.", "labels": [], "entities": [{"text": "TWIT-TER dataset", "start_pos": 240, "end_pos": 256, "type": "DATASET", "confidence": 0.9008888900279999}, {"text": "ukWaC", "start_pos": 269, "end_pos": 274, "type": "DATASET", "confidence": 0.9881235957145691}]}], "tableCaptions": [{"text": " Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8685045838356018}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9698041677474976}, {"text": "MKWC", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8388140797615051}, {"text": "WordNet-annotated datasets", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.9810068607330322}]}, {"text": " Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).", "labels": [], "entities": [{"text": "WordNet-annotated datasets", "start_pos": 68, "end_pos": 94, "type": "DATASET", "confidence": 0.9847442507743835}, {"text": "JS divergence", "start_pos": 113, "end_pos": 126, "type": "METRIC", "confidence": 0.8370113968849182}]}, {"text": " Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8724856376647949}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9805070161819458}, {"text": "Macmillan-annotated datasets", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.9930092096328735}, {"text": "FS CORPUS", "start_pos": 238, "end_pos": 247, "type": "DATASET", "confidence": 0.5214158743619919}]}, {"text": " Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).", "labels": [], "entities": [{"text": "Macmillan-annotated datasets", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.9918391108512878}]}, {"text": " Table 6: Evaluation of our method for identify- ing unattested senses, averaged over 10 runs of 10- fold cross validation", "labels": [], "entities": [{"text": "identify- ing unattested senses", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.8677154183387756}]}, {"text": " Table 7: Classification of usages with novel sense for all target lemmas.", "labels": [], "entities": []}, {"text": " Table 8: Classification of usages with novel sense for target lemmas with a removed sense.", "labels": [], "entities": []}, {"text": " Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. target  lemmas without removed sense using novelty.", "labels": [], "entities": []}]}