{"title": [{"text": "Modelling function words improves unsupervised word segmentation", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7457419037818909}]}], "abstractContent": [{"text": "Inspired by experimental psychological findings suggesting that function words play a special role in word learning, we make a simple modification to an Adaptor Grammar based Bayesian word segmenta-tion model to allow it to learn sequences of monosyllabic \"function words\" at the beginnings and endings of collocations of (possibly multi-syllabic) words.", "labels": [], "entities": [{"text": "word learning", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.7654500603675842}]}, {"text": "This modification improves unsupervised word segmentation on the standard Bernstein-Ratner (1987) corpus of child-directed En-glish by more than 4% token f-score compared to a model identical except that it does not special-case \"function words\", setting anew state-of-the-art of 92.4% token f-score.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.773436963558197}]}, {"text": "Our function word model assumes that function words appear at the left periphery, and while this is true of languages such as English, it is not true universally.", "labels": [], "entities": []}, {"text": "We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones.", "labels": [], "entities": []}, {"text": "Thus our computational models support the hypothesis that function words play a special role in word learning.", "labels": [], "entities": [{"text": "word learning", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.8216501474380493}]}], "introductionContent": [{"text": "Over the past two decades psychologists have investigated the role that function words might play inhuman language acquisition.", "labels": [], "entities": []}, {"text": "Their experiments suggest that function words play a special role in the acquisition process: children learn function words before they learn the vast bulk of the associated content words, and they use function words to help identify context words.", "labels": [], "entities": []}, {"text": "The goal of this paper is to determine whether computational models of human language acquisition can provide support for the hypothesis that function words are treated specially inhuman language acquisition.", "labels": [], "entities": []}, {"text": "We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7175830751657486}]}, {"text": "Following and our word segmentation models identify word boundaries from unsegmented sequences of phonemes corresponding to utterances, effectively performing unsupervised learning of a lexicon.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7035335749387741}]}, {"text": "For example, given input consisting of unsegmented utterances such as the following: j u w \u0251 n t t u s i \u00f0 \u0259 b \u028a k a word segmentation model should segment this as ju w\u0251nt tu si \u00f0\u0259 b\u028ak, which is the IPA representation of \"you want to seethe book\".", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7264096736907959}]}, {"text": "We show that a model equipped with the ability to learn some rudimentary properties of the target language's function words is able to learn the vocabulary of that language more accurately than a model that is identical except that it is incapable of learning these generalisations about function words.", "labels": [], "entities": []}, {"text": "This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially inhuman lexical acquisition.", "labels": [], "entities": []}, {"text": "As a reviewer points out, we present no evidence that children use function words in the way that our model does, and we want to emphasise we make no such claim.", "labels": [], "entities": []}, {"text": "While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9497621655464172}, {"text": "word segmentation", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.7351081073284149}]}, {"text": "As a reviewer points out, the changes we make to our models to incorporate function words can be viewed as \"building in\" substantive information about possible human languages.", "labels": [], "entities": []}, {"text": "The model that achieves the best token f-score expects function words to appear at the left edge of phrases.", "labels": [], "entities": []}, {"text": "While this is true for languages such as English, it is not true universally.", "labels": [], "entities": []}, {"text": "By comparing the posterior probability of two models -one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases -we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the specific word segmentation models studied in this paper, and the way we extended them to capture certain properties of function words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7626216113567352}]}, {"text": "The word segmentation experiments are presented in section 3, and section 4 discusses how a learner could determine whether function words occur on the left-periphery or the rightperiphery in the language they are learning.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7310623973608017}]}, {"text": "Section 5 concludes and describes possible future work.", "labels": [], "entities": []}, {"text": "The rest of this introduction provides background on function words, the Adaptor Grammar models we use to describe lexical acquisition and the Bayesian inference procedures we use to infer these models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Mean token f-scores and boundary preci- sion and recall results averaged over 8 trials, each  consisting of 8 MCMC runs of models trained  and tested on the full Bernstein-Ratner (1987) cor- pus (the standard deviations of all values are less  than 0.006; Wilcox sign tests show the means of  all token f-scores differ p < 2e-4).", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9956892132759094}]}]}