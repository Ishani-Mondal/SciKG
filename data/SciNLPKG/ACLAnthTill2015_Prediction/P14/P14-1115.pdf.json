{"title": [{"text": "Abstractive Summarization of Spoken and Written Conversations Based on Phrasal Queries", "labels": [], "entities": [{"text": "Abstractive Summarization of Spoken and Written Conversations", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8028769450528281}]}], "abstractContent": [{"text": "We propose a novel abstractive query-based summarization system for conversations , where queries are defined as phrases reflecting a user information needs.", "labels": [], "entities": []}, {"text": "We rank and extract the utterances in a conversation based on the overall content and the phrasal query information.", "labels": [], "entities": []}, {"text": "We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model.", "labels": [], "entities": []}, {"text": "We propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster.", "labels": [], "entities": []}, {"text": "A resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation.", "labels": [], "entities": []}, {"text": "Automatic and manual evaluation results over meeting, chat and email conversations show that our approach significantly outperforms baselines and previous extractive models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our lives are increasingly reliant on multimodal conversations with others.", "labels": [], "entities": []}, {"text": "We email for business and personal purposes, attend meetings in person, chat online, and participate in blog or forum discussions.", "labels": [], "entities": []}, {"text": "While this growing amount of personal and public conversations represent a valuable source of information, going through such overwhelming amount of data, to satisfy a particular information need, often leads to an information overload problem ().", "labels": [], "entities": []}, {"text": "Automatic summarization has been proposed in the past as away to address this problem (e.g.,).", "labels": [], "entities": [{"text": "summarization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6987027525901794}]}, {"text": "However, often a good summary cannot be generic and should be a brief and well-organized paragraph that answer a user's information need.", "labels": [], "entities": []}, {"text": "The Document Understanding Conference (DUC) 1 has launched query-focused multidocument summarization as its main task since 2004, by focusing on complex queries with very specific answers.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.8125313917795817}, {"text": "query-focused multidocument summarization", "start_pos": 59, "end_pos": 100, "type": "TASK", "confidence": 0.5812071164449056}]}, {"text": "For example, \"How were the bombings of the US embassies in Kenya and Tanzania conducted?", "labels": [], "entities": []}, {"text": "How and where were the attacks planned?\".", "labels": [], "entities": []}, {"text": "Such complex queries are appropriate fora user who has specific information needs and can formulate the questions precisely.", "labels": [], "entities": []}, {"text": "However, especially when dealing with conversational data that tend to be less structured and less topically focused, a user is often initially only exploring the source documents, with less specific information needs.", "labels": [], "entities": []}, {"text": "Moreover, following the common practice in search engines, users are trained to form simpler and shorter queries.", "labels": [], "entities": []}, {"text": "For example, when a user is interested in certain characteristics of an entity in online reviews (e.g., \"location\" or \"screen\") or a specific entity in a blog discussion (e.g., \"new model of iphone\"), she would not initially compose a complex query.", "labels": [], "entities": []}, {"text": "To address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries.", "labels": [], "entities": [{"text": "conversation summarization", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7287455499172211}]}, {"text": "We define a phrasal query as a concatenation of two or more keywords, which is a more realistic representation of a user's information needs.", "labels": [], "entities": []}, {"text": "For conversational data, this definition is more similar to the concept of search queries in information retrieval systems as well as to the concept of topic labels in the task of topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 180, "end_pos": 194, "type": "TASK", "confidence": 0.7264317274093628}]}, {"text": "Example 1 shows two queries and their associated human written summaries based on a single chat log.", "labels": [], "entities": []}, {"text": "We can observe that the two summaries, although generated from the same chat log, are totally distinct.", "labels": [], "entities": []}, {"text": "This further demonstrates the importance of phrasal query-based summarization systems for long conversations.", "labels": [], "entities": [{"text": "phrasal query-based summarization", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.46141337354977924}]}, {"text": "To date, most systems in the area of summa-Query-1: Test/Sample database for GNUe Abstract-1: James Thompson asked Reinhard: I was going to work on the sample tonight.", "labels": [], "entities": []}, {"text": "You mentioned wanting a fishhook and all data types.", "labels": [], "entities": []}, {"text": "Any other things you want to see in there?", "labels": [], "entities": []}, {"text": "Reinhard said that master/detail would be good, as there have been bugs only appearing in 3-level case.", "labels": [], "entities": [{"text": "master", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9963899254798889}, {"text": "detail", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.697051465511322}]}, {"text": "James said he already included that and I know I need to add a boolean.", "labels": [], "entities": [{"text": "boolean", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9838804602622986}]}, {"text": "Did you want date as well as date-time?", "labels": [], "entities": []}, {"text": "Reinhard said yes -we also have time values (time without date).", "labels": [], "entities": [{"text": "time", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9514870047569275}]}, {"text": "James had not ever had use for something like that so I'm not sure where I would graft that in.", "labels": [], "entities": []}, {"text": "Query-2: Passing parameters to Forms Abstract-2: James Thompson (jamest) asked how did parameter support in forms change recently?", "labels": [], "entities": [{"text": "Abstract-2", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.885578989982605}, {"text": "jamest", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.4117843508720398}]}, {"text": "He reported the trigger namespace function referencesGFForm.parameters -which no longer exists.", "labels": [], "entities": []}, {"text": "Reinhard said every GFForm should have a parameters.", "labels": [], "entities": []}, {"text": "James said he was using parameters in on-startup.", "labels": [], "entities": []}, {"text": "Reinhard said that's probably the only place where they don't work.", "labels": [], "entities": []}, {"text": "James said that I'm thinking about moving that to on-activation instead of on-startup anyway as it should still work fora main form -but i still wonder if the on-startup parameter issue should be considered a bug -as it shouldn't choke.", "labels": [], "entities": []}, {"text": "Reinhard was sure it should be considered a bug but I have no idea how to fix it.", "labels": [], "entities": []}, {"text": "We haven't found away to deal with parameters that works for every case.", "labels": [], "entities": []}, {"text": "I don't know if there is any chance to pass the parameters to the form before it is activated.", "labels": [], "entities": []}, {"text": "James asked how are parameters handled now?", "labels": [], "entities": []}, {"text": "Reinhard replied that they are passed to activateForm so they are available from activation for the -main-form, the command line parameters are passed and for dialogs, the parameters are passed that were given in runDialog.", "labels": [], "entities": []}, {"text": "Example 1: Sample queries and associated human-written query-based summaries fora chat log.", "labels": [], "entities": []}, {"text": "rization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited.", "labels": [], "entities": [{"text": "rization", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9162454009056091}, {"text": "summarizing multiparty written conversations (e.g., chats, emails)", "start_pos": 74, "end_pos": 140, "type": "TASK", "confidence": 0.8489430384202437}]}, {"text": "This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling.", "labels": [], "entities": []}, {"text": "Even though some works try to address the problem of summarizing multiparty written conversions (e.g.,), they do so in a generic way (not querybased) and focus on only one conversational domain (e.g., meetings).", "labels": [], "entities": [{"text": "summarizing multiparty written conversions", "start_pos": 53, "end_pos": 95, "type": "TASK", "confidence": 0.8974584937095642}]}, {"text": "Moreover, most of the proposed systems for conversation summarization are extractive.", "labels": [], "entities": [{"text": "conversation summarization", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.8089015781879425}]}, {"text": "To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization.", "labels": [], "entities": [{"text": "multimodal conversation summarization", "start_pos": 130, "end_pos": 167, "type": "TASK", "confidence": 0.6587015589078268}]}, {"text": "Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on users phrasal queries, instead of well-formed questions.", "labels": [], "entities": []}, {"text": "As a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms).", "labels": [], "entities": []}, {"text": "2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e., fluency) of the sentence into consideration.", "labels": [], "entities": []}, {"text": "3) Although most of the current summarization approaches use supervised algorithms as apart of their system (e.g., ( ), our method can be totally unsupervised and does not depend on human annotation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9816158413887024}]}, {"text": "4) Although different conversational modalities (e.g., email vs. chat vs. meeting) underline domain-specific characteristics, in this work, we take advantage of their underlying similarities to generalize away from specific modalities and determine effective method for query-based summarization of multimodal conversations.", "labels": [], "entities": [{"text": "summarization of multimodal conversations", "start_pos": 282, "end_pos": 323, "type": "TASK", "confidence": 0.776388093829155}]}, {"text": "We evaluate our system over GNUe Traffic archive 2 Internet Relay Chat (IRC) logs, AMI meetings corpus () and BC3 emails dataset (.", "labels": [], "entities": [{"text": "AMI meetings corpus", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.795467476050059}, {"text": "BC3 emails dataset", "start_pos": 110, "end_pos": 128, "type": "DATASET", "confidence": 0.8794472018877665}]}, {"text": "Automatic evaluation on the chat dataset and manual evaluation over the meetings and emails show that our system uniformly and statistically significantly outperforms baseline systems, as well as a stateof-the-art query-based extractive summarization system.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we show the evaluation results of our proposed framework and its comparison to the baselines and a state-of-the-art query-focused extractive summarization system.", "labels": [], "entities": []}, {"text": "One of the challenges of this work is to find suitable conversational datasets that can be used for evaluating our query-based summarization system.", "labels": [], "entities": []}, {"text": "Most available conversational corpora do not contain any human written summaries, or the gold standard human written summaries are generic.", "labels": [], "entities": []}, {"text": "In this work, we use available corpora for emails and chats for written conversations, while for spoken conversation, we employ an available corpus in multiparty meeting conversations.", "labels": [], "entities": []}, {"text": "Chat: to the best of our knowledge, the only publicly available chat logs with human written summaries can be downloaded from the GNUe Traffic archive (.", "labels": [], "entities": [{"text": "GNUe Traffic archive", "start_pos": 130, "end_pos": 150, "type": "DATASET", "confidence": 0.9557528694470724}]}, {"text": "Each chat log has a human created summary in the form of a digest.", "labels": [], "entities": []}, {"text": "Each digest summarizes IRC logs fora period and consists of few summaries over each chat log with a unique title for the associated human written summary.", "labels": [], "entities": []}, {"text": "In this way, the title of each summary can be counted as a phrasal query and the corresponding summary is considered as the querybased abstract of the associated chat log including only the information most relevant to the title.", "labels": [], "entities": []}, {"text": "Therefore, we can use the human-written querybased abstract as gold standards and evaluate our system automatically.", "labels": [], "entities": []}, {"text": "Our chat dataset consists of 66 query-based (title-based) human written summaries with their associated queries (titles) and chat logs, created from 40 original chat logs.", "labels": [], "entities": []}, {"text": "The average number of tokens are 1840, 325 and 6 for chat logs, query-based summaries and queries, respectively.", "labels": [], "entities": []}, {"text": "Meeting: we use the AMI meeting corpus) that consists of 140 multiparty meetings with a wide range of annotations, including generic abstractive summaries for each meeting.", "labels": [], "entities": [{"text": "AMI meeting corpus", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.9346205592155457}]}, {"text": "In order to create queries, we extract three key-phrases from generic abstractive summaries using TextRank algorithm ().", "labels": [], "entities": []}, {"text": "We use the extracted key-phrases as queries to generate query-based abstracts.", "labels": [], "entities": []}, {"text": "Since there is no human-written query-based summary for AMI corpus, we randomly select 10 meetings and evaluate our system manually.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.8087987005710602}]}, {"text": "Email: we use BC3 (, which contains 40 threads from the W3C corpus.", "labels": [], "entities": [{"text": "BC3", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8508785963058472}, {"text": "W3C corpus", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.9412593245506287}]}, {"text": "BC3 corpus is annotated with generic human-written abstractive summaries, and it has been used in several previous works (e.g.,).", "labels": [], "entities": [{"text": "BC3 corpus", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.936067134141922}]}, {"text": "In order to adapt this corpus to our framework, we followed the same query generation process as for the meeting dataset.", "labels": [], "entities": []}, {"text": "Finally, we randomly select 10 emails threads and evaluate the results manually.", "labels": [], "entities": []}, {"text": "For preprocessing our dataset we use OpenNLP 3 for tokenization, stemming and part-of-speech tagging.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9675503969192505}, {"text": "part-of-speech tagging", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7208894789218903}]}, {"text": "We use six randomly selected querylogs from our chat dataset (about 10% of the dataset) for tuning the coefficient parameters.", "labels": [], "entities": []}, {"text": "We set the k parameter in our clustering phase to 10 based on the average number of sentences in the human written summaries.", "labels": [], "entities": []}, {"text": "For our language model, we use a tri-gram smoothed language model trained using the newswire text provided in the English Gigaword corpus (.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 114, "end_pos": 137, "type": "DATASET", "confidence": 0.7979939579963684}]}, {"text": "For the automatic evaluation we use the official ROUGE software with standard options and report ROUGE-1 and ROUGE-2 precision, recall and F-1 scores.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.8764274716377258}, {"text": "ROUGE-2", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.918278694152832}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.8795669078826904}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9989219903945923}, {"text": "F-1", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9931718111038208}]}, {"text": "Abstractive vs. Extractive: our full querybased abstractive summariztion system show statistically significant improvements over baselines 3 http://opennlp.apache.org/ and other pure extractive summarization systems for ROUGE-1 4 . This means our systems can effectively aggregate the extracted sentences and generate abstract sentences based on the query content.", "labels": [], "entities": []}, {"text": "We can also observe that our full system produces the highest ROUGE-1 precision score among all models, which further confirms the success of this model in meeting the user information needs imposed by queries.", "labels": [], "entities": [{"text": "ROUGE-1 precision score", "start_pos": 62, "end_pos": 85, "type": "METRIC", "confidence": 0.8880222042401632}]}, {"text": "The absolute improvement of 10% in precision for ROUGE-1 in our abstractive model over our extractive model (our pipeline) further confirms the effectiveness of our ranking method in generating the abstract sentences considering the query related information.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9992315769195557}, {"text": "ROUGE-1", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9378100037574768}]}, {"text": "Our extractive query-based method beats all other extractive systems with a higher ROUGE-1 and ROUGE-2 which shows the effectiveness of our utterance extraction model in comparison with other extractive models.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9901362061500549}, {"text": "ROUGE-2", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9941754937171936}, {"text": "utterance extraction", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.810916006565094}]}, {"text": "In other words, using our extractive model described in section 2.1, as a standalone system, is an effective query-based extractive summarization model.", "labels": [], "entities": []}, {"text": "We also observe that our extractive model outperforms our abstractive model for ROUGE-2 score.", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 80, "end_pos": 93, "type": "METRIC", "confidence": 0.9502472579479218}]}, {"text": "This can be due to word merging and word replacement choices in the word graph construction, which sometimes change or remove a word in a bigram and consequently may decrease the bigram overlap score.", "labels": [], "entities": [{"text": "word merging", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7292072921991348}, {"text": "word replacement", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.6962552666664124}, {"text": "bigram overlap score", "start_pos": 179, "end_pos": 199, "type": "METRIC", "confidence": 0.6934118270874023}]}, {"text": "Query Relevance: another interesting observation is that relying only on the cosine similarity (i.e., cosine-all) to measure the query relevance presents a quite strong baseline.", "labels": [], "entities": [{"text": "Query Relevance", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8065632581710815}]}, {"text": "This proves the importance of query content in our dataset and further supports the main claim of our work that a: Average rating and distribution over grammaticality scores for phrasal query abstraction system in comparison with original sentences.", "labels": [], "entities": []}, {"text": "good summary should express a brief and wellorganized abstract that answers the user's query.", "labels": [], "entities": []}, {"text": "Moreover, a precision of 71% for ROUGE-1 from the simple cosine-1st baseline confirms that some utterances contain more query relevant information in conversational discussions.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9984340071678162}, {"text": "ROUGE-1", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.985550045967102}]}, {"text": "Query-based vs. Generic: the high recall and low precision in TextRank baseline, both for the ROUGE-1 and ROUGE-2 scores, shows the strength of the model in extracting the generic information from chat conversations while missing the query-relevant content.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9991592168807983}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9991430044174194}, {"text": "TextRank baseline", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.8827064633369446}, {"text": "ROUGE-1", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.8867946863174438}]}, {"text": "The LexRank baseline improves the results of the TextRank system by increasing the precision and balancing the precision and recall scores for ROUGE-1 score.", "labels": [], "entities": [{"text": "LexRank baseline", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8745115995407104}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9994608759880066}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9993426203727722}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9979555606842041}, {"text": "ROUGE-1 score", "start_pos": 143, "end_pos": 156, "type": "METRIC", "confidence": 0.9441656768321991}]}, {"text": "We believe that this is due to the robustness of the LexRank method in dealing with noisy texts (chat conversations) ().", "labels": [], "entities": []}, {"text": "In addition, the Biased LexRank model slightly improves the generic LexRank system.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.5921581387519836}, {"text": "LexRank", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9219491481781006}]}, {"text": "Considering this marginal improvement and relatively high results of pure extractive systems, we can infer that the Biased LexRank extracted summaries do not carry much query relevant content.", "labels": [], "entities": [{"text": "Biased LexRank extracted summaries", "start_pos": 116, "end_pos": 150, "type": "DATASET", "confidence": 0.6888468340039253}]}, {"text": "In contrast, the significant improvement of our model over the extractive methods demonstrates the success of our approach in presenting the query related content in generated abstracts.", "labels": [], "entities": []}, {"text": "An example of a short chat log, its related query and corresponding manual and automatic summaries are shown in Example 3.", "labels": [], "entities": [{"text": "Example", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.8725492358207703}]}, {"text": "Content and User Preference: demonstrates overall quality, responsiveness (query relatedness) and user preference scores for the abstracts generated by our system and two baselines.", "labels": [], "entities": [{"text": "Content and User Preference", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5000381991267204}]}, {"text": "Results indicate that our system significantly outperforms baselines in overall quality and responsiveness, for both meeting and email datasets.", "labels": [], "entities": []}, {"text": "This confirms the validity of the results we obtained by conducting automatic evaluation over the chat dataset.", "labels": [], "entities": []}, {"text": "We also can observe that the absolute improvements in overall quality and responsiveness for emails (0.9 and 0.7) is greater than for meetings (0.4 and 0.6).", "labels": [], "entities": []}, {"text": "This is expected since dealing with spoken conversations is more challenging than written ones.", "labels": [], "entities": []}, {"text": "Note that the responsiveness scores are greater than overall scores.", "labels": [], "entities": [{"text": "responsiveness", "start_pos": 14, "end_pos": 28, "type": "METRIC", "confidence": 0.9603493213653564}]}, {"text": "This further proves the effectiveness of our approach in dealing with phrasal queries.", "labels": [], "entities": []}, {"text": "We also evaluate the users' summary preferences.", "labels": [], "entities": []}, {"text": "For both datasets (meeting and email), in majority of cases (70% and 60% respectively), the users prefer the query-based abstractive summary generated by our system.", "labels": [], "entities": []}, {"text": "Grammaticality: shows grammaticality scores and distributions over the three possible scores for all datasets.", "labels": [], "entities": []}, {"text": "The chat dataset results demonstrate the highest scores: 73% of the sentences generated by our phrasal query abstraction model are grammatically correct and 24% of the generated sentences are almost correct with only one grammatical error, while only 3% of the abstract sentences are grammatically incorrect.", "labels": [], "entities": []}, {"text": "However, the results varies moving to other datasets.", "labels": [], "entities": []}, {"text": "For meeting dataset, the percentage of completely grammatical sentences drops dramatically.", "labels": [], "entities": []}, {"text": "This is due to the nature of spoken conversations which is more error prone and ungrammatical.", "labels": [], "entities": []}, {"text": "The grammaticality score of the original sentences also proves that the sentences from meet-Query: Trigger namespace and the self property Chat log: A: good morning B: good morning C: good morning everyone D: good morning D: goodnight all F: New GNUe Traffic online F: loadsa deep metaphyisical stuff this week F: D & E discuss the meaning of 'self' ;-) E: yes, and he took the more metaphysical route, where I took the more scientific route E: I say self's meaning is derived from one's ancestry E: self's meaning is derived from how others use you E: okay, analogy extended too far, I guess :) F: is this a friends vs family debate?", "labels": [], "entities": []}, {"text": "E: also noted that the cool part about that is if you have code that needs to happen both on a pre-insert and a pre-update -but only a few lines of the code is different E: you could have one block of trigger code that used self.action to find out why it had been called and branch accordingly.", "labels": [], "entities": []}, {"text": "E: there was a big jump from the previous paragraph to that E: that took that out of context E: iirc, I was saying an alternative was that \"self\" could refer to neither the trigger's owner nor to the trigger's caller E: but to the event itself E: so self.action could be what action is being performed E: self.parent could be the trigger's owner E: and self.context could be the caller E: and self.name could be the name of the trigger F: umm, I'm afraid apparant non-sequiturs are always a hazard of doing summaries ;-) E: iow, I was presenting a 3rd alternative where self referred to something specific to the trigger F: I normally try to write around them, but not 100% successful E: I'm just convulsing my thoughts to the irc log E: for an errata next week :) E: I think convulsing is a good description F: heh TexRank: There was a big jump from the previous paragraph to that. that took that out of context iirc, I was saying an alternative was that \"self\" could refer to neither the trigger's owner nor to the trigger's caller.", "labels": [], "entities": []}, {"text": "but to the event itself.", "labels": [], "entities": []}, {"text": "so self.action could be what action is being performed, self.parent could be the trigger's owner, and self.context could be the caller and self.name could be the name of the trigger.", "labels": [], "entities": []}, {"text": "umm, I'm afraid apparant non-sequiturs are always a hazard of doing summaries ;-) LexRank: good morning everyone. heh.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9522644281387329}]}, {"text": "I'm just convulsing my thoughts to the irc log good morning.", "labels": [], "entities": []}, {"text": "Jason also noted that the cool part about that is if you have code that needs to happen both on a preinsert and a pre-update -but only a few lines of the code is differentyou could have one block of trigger code that used self.action to find out why it had been called and branch accordingly.", "labels": [], "entities": []}, {"text": "for an errata next week :) self's meaning is derived from how others use you.", "labels": [], "entities": []}, {"text": "I think convulsing is a good description reinhard & jcater discuss the meaning of 'self' ;-) Biased-LexRank: good morning everyone. heh.", "labels": [], "entities": [{"text": "Biased-LexRank", "start_pos": 93, "end_pos": 107, "type": "METRIC", "confidence": 0.9864315390586853}]}, {"text": "I'm just convulsing my thoughts to the irc log.", "labels": [], "entities": []}, {"text": "Jason also noted that the cool part about that is if you have code that needs to happen both on a pre-insert and a pre-update -but only a few lines of the code is different -you could have one block of trigger code that used self.action to find out why it had been called and branch accordingly.", "labels": [], "entities": []}, {"text": "yes, and he took the more metaphysical route, where I took the more scientific route there was a big jump from the previous paragraph to that but to the event itself.", "labels": [], "entities": []}, {"text": "iow, I was presenting a 3rd alternative where self referred to something specific to the trigger.", "labels": [], "entities": []}, {"text": "Our system: self could refer to neither the triggers owner nor caller.", "labels": [], "entities": []}, {"text": "I was saying an alternative where self referred to something specific to the trigger. and self.name could be the name.", "labels": [], "entities": []}, {"text": "so self.action could be what action is being performed, self.parent the triggers owner and self.context caller.", "labels": [], "entities": []}, {"text": "Gold: Further to, E clarified that he had suggested that \"self\" could refer to neither the trigger's owner nor to the trigger's caller -but to the event itself.", "labels": [], "entities": []}, {"text": "So self.action could be what action is being performed, self.parent could be the trigger's owner, and self.context could be the caller.", "labels": [], "entities": []}, {"text": "In other words, I was presenting a 3rd alternative where self referred to something specific to the trigger.", "labels": [], "entities": []}, {"text": "Summaries generated by our system and other baselines in comparison with the humanwritten summary fora short chat log.", "labels": [], "entities": []}, {"text": "Speaker information have been anonymized.", "labels": [], "entities": []}, {"text": "ing transcripts, although generated by humans, are not fully grammatical.", "labels": [], "entities": []}, {"text": "In comparison with the original sentences, for all datasets, our model reports slightly lower results for the grammaticality score.", "labels": [], "entities": []}, {"text": "Considering the fact that the abstract sentences are automatically generated and the original sentences are human-written, the grammaticality score and the percentage of fully grammatical sentences generated by our system, with higher ROUGE or quality scores in comparison with other methods, demonstrates that our system is an effective phrasal query abstraction framework for both spoken and written conversations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 235, "end_pos": 240, "type": "METRIC", "confidence": 0.9977128505706787}]}], "tableCaptions": [{"text": " Table 1: Performance of different summarization algorithms on chat logs for query-based chat sum- marization. Statistically significant improvements (p < 0.01) over the biased LexRank system are  marked with *.  \u2020 indicates statistical significance (p < 0.01) over extractive approaches", "labels": [], "entities": [{"text": "query-based chat sum- marization", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.5719115376472473}, {"text": "LexRank", "start_pos": 177, "end_pos": 184, "type": "DATASET", "confidence": 0.9351145625114441}]}, {"text": " Table 2: Manual evaluation scores for our phrasal query abstraction system in comparison with Biased  LexRank and LexRank (LR).", "labels": [], "entities": [{"text": "LexRank", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.6693145632743835}, {"text": "LexRank (LR)", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.8437401056289673}]}, {"text": " Table 3: Average rating and distribution over grammaticality scores for phrasal query abstraction system  in comparison with original sentences.", "labels": [], "entities": []}]}