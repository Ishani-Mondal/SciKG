{"title": [{"text": "Language-Aware Truth Assessment of Fact Candidates", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces FactChecker, language-aware approach to truth-finding.", "labels": [], "entities": [{"text": "FactChecker", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.7348325252532959}]}, {"text": "FactChecker differs from prior approaches in that it does not rely on iterative peer voting, instead it leverages language to infer believability of fact candidates.", "labels": [], "entities": [{"text": "FactChecker", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9441986083984375}]}, {"text": "In particular, FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated.", "labels": [], "entities": [{"text": "FactChecker", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.9392924904823303}]}, {"text": "To ensure that fact candidates mentioned in similar sources have similar believability, FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate.", "labels": [], "entities": []}, {"text": "Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches.", "labels": [], "entities": [{"text": "FactChecker", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.6480774283409119}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9987159967422485}]}], "introductionContent": [{"text": "Truth-finding algorithms aim to separate true statements (facts) from false information.", "labels": [], "entities": []}, {"text": "More specifically, given a set of statements whose truthfulness is unknown (fact candidates), the key goal of truth-finding algorithms is to generate a ranking such that true statements are ranked ahead of false ones.", "labels": [], "entities": []}, {"text": "Truth-finders have the potential to address a major obstacle on the Web: the problem of sources spreading inaccurate and conflicting information.", "labels": [], "entities": []}, {"text": "This problem continues to grow with the development of tools for easy Web authorship.", "labels": [], "entities": []}, {"text": "Blogs, forums and social networking websites are not subject to traditional journalistic standards.", "labels": [], "entities": []}, {"text": "Consequently, the accuracy of information reported by these sources is often unclear.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9991139769554138}]}, {"text": "Even more established newspapers and websites may sometimes report false information as they race to break stories.", "labels": [], "entities": []}, {"text": "Therefore, truth-finding is becoming an increasingly important problem.", "labels": [], "entities": [{"text": "truth-finding", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.9525319933891296}]}, {"text": "Information extraction projects aim to distill relational facts from natural language text.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8151318728923798}]}, {"text": "These projects have produced knowledge bases containing many millions of relational facts between entities.", "labels": [], "entities": []}, {"text": "However, despite these impressive advances, there are still major limitations regarding precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9971774816513062}]}, {"text": "Within the context of information extraction, fact extractors assign confidence scores to extracted facts.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8305298686027527}, {"text": "fact extractors", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7046542912721634}]}, {"text": "However, such scores are often tied to the extractor's ability to read and understand natural language text.", "labels": [], "entities": []}, {"text": "This is different from a score that indicates the degree to which a given fact candidate is believable.", "labels": [], "entities": []}, {"text": "Such a believability score is sometimes also referred to as a credibility score or truthfulness score.", "labels": [], "entities": []}, {"text": "The believability score reflects the likelihood that a given statement is true.", "labels": [], "entities": []}, {"text": "Truth-finding algorithms aim to compute this score for each fact candidate.", "labels": [], "entities": []}, {"text": "Prior truth-finding methods are mostly based on iterative voting, where votes are propagated from sources to fact candidates and then back to sources.", "labels": [], "entities": []}, {"text": "At the core of iterative voting is the assumption that candidates mentioned by many sources are more likely to be true.", "labels": [], "entities": []}, {"text": "However, additional aspects of a source influence its trustworthiness, besides external votes.", "labels": [], "entities": []}, {"text": "Our goal is to accurately assess truthfulness of fact candidates by taking into account the language of sources that mention them.", "labels": [], "entities": []}, {"text": "A Mechanical Turk study we carried out revealed that there is a significant correlation between objectivity of language and trustworthiness of sources.", "labels": [], "entities": []}, {"text": "Objectivity of language refers to the use of neutral, impartial language, which is not personal, judgmental, or emotional.", "labels": [], "entities": []}, {"text": "Trustworthiness refers to a source of information being reliable and truthful.", "labels": [], "entities": []}, {"text": "We use linguistics features to detect if a given source objectively states facts or is speculative and opinionated.", "labels": [], "entities": []}, {"text": "Additionally, in order to ensure that fact candidates mentioned in similar sources have similar believability scores, our believability computation model incorporates influence of comentions.", "labels": [], "entities": []}, {"text": "However, we must avoid falsely boosting co-mentioned fact candidates.", "labels": [], "entities": []}, {"text": "Our model addresses potential false boosts in two ways: first, by ensuring that co-mention influence is only propagated to related fact candidates; second, by ensuring that the degree of co-mention influence is determined by the trustworthiness of the sources in which co-mentions occur.", "labels": [], "entities": []}, {"text": "The contribution of this paper is a languageaware truth-finding approach.", "labels": [], "entities": []}, {"text": "More precisely, we make the following contributions: (1) Alternative Fact Candidates: Truth-finders rank a given fact candidate with respect to its alternatives.", "labels": [], "entities": []}, {"text": "For example, alternative places where Barack Obama could have been born.", "labels": [], "entities": []}, {"text": "Virtually all existing truth-finders assume that the alternatives are provided.", "labels": [], "entities": []}, {"text": "In contrast, we developed a method for generating alternative fact candidates.", "labels": [], "entities": []}, {"text": "(2) Objectivity-Trustworthiness Correlation: We hypothesize that objectivity of language and trustworthiness of sources are positively correlated.", "labels": [], "entities": [{"text": "Objectivity-Trustworthiness Correlation", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.7768291234970093}]}, {"text": "To test this hypothesis, we designed a Mechanical Turk study.", "labels": [], "entities": [{"text": "Mechanical Turk study", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.6082948247591654}]}, {"text": "The study showed that this correlation does in fact hold.", "labels": [], "entities": []}, {"text": "(3) Objectivity Classifier: Using labeled data from the Mechanical Turk study, we developed and trained an objectivity classifier which performed better than prior proposed lexicons from literature.", "labels": [], "entities": [{"text": "Objectivity Classifier", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8103747963905334}, {"text": "Mechanical Turk study", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.6605902115503947}]}, {"text": "(4) Believability Computation: We developed FactChecker, a truth-finding method that linearly combines objectivity and comention influence.", "labels": [], "entities": [{"text": "FactChecker", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.7515761256217957}]}, {"text": "Our experiments showed that FactChecker outperforms prior methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated FactChecker for accuracy.", "labels": [], "entities": [{"text": "FactChecker", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.954311728477478}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9989854693412781}]}, {"text": "We define accuracy as the probability of a true fact candidate having a higher believability score than a false candidate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993182420730591}]}, {"text": "Let \u03c4 (f i ) \u2208 {T, F } be the truthfulness of a fact candidate f i , accuracy is defined as:", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9995130300521851}]}], "tableCaptions": [{"text": " Table 3: Fact candidate datasets.", "labels": [], "entities": [{"text": "Fact candidate datasets", "start_pos": 10, "end_pos": 33, "type": "DATASET", "confidence": 0.7186165650685629}]}, {"text": " Table 4: Accuracy on politicians and quiz data sets", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981513619422913}]}]}