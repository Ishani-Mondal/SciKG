{"title": [{"text": "Translation Assistance by Translation of L1 Fragments in an L2 Context", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9640766978263855}]}], "abstractContent": [{"text": "In this paper we present new research in translation assistance.", "labels": [], "entities": [{"text": "translation assistance", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.9886910617351532}]}, {"text": "We describe a system capable of translating native language (L1) fragments to foreign language (L2) fragments in an L2 context.", "labels": [], "entities": [{"text": "translating native language (L1) fragments to foreign language (L2) fragments", "start_pos": 32, "end_pos": 109, "type": "TASK", "confidence": 0.703768653529031}]}, {"text": "Practical applications of this research can be framed in the context of second language learning.", "labels": [], "entities": [{"text": "second language learning", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.7108728686968485}]}, {"text": "The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fallback to their native language in case the correct word or expression is not known.", "labels": [], "entities": [{"text": "translation assistance", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9079165458679199}]}, {"text": "These code switches are subsequently translated to L2 given the L2 context.", "labels": [], "entities": []}, {"text": "We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense dis-ambiguation baselines.", "labels": [], "entities": []}, {"text": "A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contex-tual window spanning a small number of neighbouring words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Whereas machine translation generally concerns the translation of whole sentences or texts from one language to the other, this study focusses on the translation of native language (henceforth L1) words and phrases, i.e. smaller fragments, in a foreign language (L2) context.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7987011969089508}, {"text": "translation of native language (henceforth L1) words and phrases, i.e. smaller fragments", "start_pos": 150, "end_pos": 238, "type": "TASK", "confidence": 0.7484550913174947}]}, {"text": "Despite the major efforts and improvements, automatic translation does not yet rival human-level quality.", "labels": [], "entities": [{"text": "automatic translation", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7521466016769409}]}, {"text": "Vexing issues are morphology, word-order change and long-distance dependencies.", "labels": [], "entities": []}, {"text": "Although there is a morpho-syntactic component in this research, our scope is more constrained; its focus is on the faithful preservation of meaning from L1 to L2, akin to the role of the translation model in Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 209, "end_pos": 246, "type": "TASK", "confidence": 0.807028998931249}]}, {"text": "The cross-lingual context in our research question may at first seem artificial, but its design explicitly aims at applications related to computeraided language learning () and computer-aided translation ().", "labels": [], "entities": [{"text": "computer-aided translation", "start_pos": 178, "end_pos": 204, "type": "TASK", "confidence": 0.7196968495845795}]}, {"text": "Currently, language learners need to refer to a bilingual dictionary when in doubt about a translation of a word or phrase.", "labels": [], "entities": []}, {"text": "Yet, this problem arises in a context, not in isolation; the learner may have already translated successfully apart of the text into L2 leading up to the problematic word or phrase.", "labels": [], "entities": []}, {"text": "Dictionaries are not the best source to lookup context; they may contain example usages, but remain biased towards single words or short expressions.", "labels": [], "entities": []}, {"text": "The proposed application allows code switching and produces context-sensitive suggestions as writing progresses.", "labels": [], "entities": [{"text": "code switching", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.791677862405777}]}, {"text": "In this research we test the feasibility of the foundation of this idea.The following examples serve to illustrate the idea and demonstrate what output the proposed translation assistance system would ideally produce.", "labels": [], "entities": [{"text": "translation assistance", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.9279705286026001}]}, {"text": "The parts in bold correspond to respectively the inserted fragment and the system translation.", "labels": [], "entities": []}, {"text": "\u2022 Input (L1=English,L2=Spanish): \"Hoy vamos a the swimming pool.\"", "labels": [], "entities": []}, {"text": "Desired output: \"Hoy vamos a la piscina.\"", "labels": [], "entities": []}, {"text": "\u2022 Input (L1-English, L2=German): \"Das wetter ist wirklich abominable.\"", "labels": [], "entities": []}, {"text": "Desired output: \"Das wetter ist wirklich ekelhaft.\"", "labels": [], "entities": []}, {"text": "\u2022 Input (L1=French,L2=English): \"I rentr\u00e8 a la maison because I am tired.\"", "labels": [], "entities": [{"text": "Input", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9762697815895081}]}, {"text": "Desired output: \"I return home because I am tired.\"", "labels": [], "entities": []}, {"text": "\u2022 Input (L1=Dutch, L2=English): \"Workers are facing a massive aanval op their employ-ment and social rights.\"", "labels": [], "entities": [{"text": "Input", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9492650628089905}]}, {"text": "Desired output: \"Workers are facing a massive attack on their employment and social rights.\"", "labels": [], "entities": []}, {"text": "The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Several automated metrics exist for the evaluation of L2 system output against the L2 reference output in the test set.", "labels": [], "entities": []}, {"text": "We first measure absolute accuracy by simply counting all output fragments that exactly match the reference fragments, as a fraction of the total amount of fragments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9845746159553528}]}, {"text": "This measure maybe too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9268656969070435}]}, {"text": "If output o is a subset of reference r then a score of |o| |r| is assigned for that sentence pair.", "labels": [], "entities": []}, {"text": "If instead, r is a subset of o, then a score of |r| |o| will be assigned.", "labels": [], "entities": []}, {"text": "A perfect match will result in a score of 1 whereas a complete lack of overlap will be scored 0.", "labels": [], "entities": [{"text": "overlap", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9719179272651672}]}, {"text": "The word accuracy for the entire set is then computed by taking the sum of the word accuracies per sentence pair, divided by the total number of sentence pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9313862919807434}]}, {"text": "We also compute a recall metric that measures the number of fragments that the system provided a translation for as a fraction of the total number of fragments in the input, regardless of whether the fragment is translated correctly or not.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.99832683801651}]}, {"text": "The system may skip fragments for which it can find no solution at all.", "labels": [], "entities": []}, {"text": "In addition to these, the system's output can be compared against the L2 reference translation(s) using established Machine Translation evaluation metrics.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.7990878621737162}]}, {"text": "We report on BLEU, NIST, METEOR, and word error rate metrics WER and PER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9992554783821106}, {"text": "NIST", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.663826584815979}, {"text": "METEOR", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9789916276931763}, {"text": "word error rate metrics", "start_pos": 37, "end_pos": 60, "type": "METRIC", "confidence": 0.7827395051717758}, {"text": "WER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.6058265566825867}, {"text": "PER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9743091464042664}]}, {"text": "These scores should generally be much better than the typical MT system performances as only local changes are made to otherwise \"perfect\" L2 sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9653612375259399}]}, {"text": "The data for our experiments were drawn from the Europarl parallel corpus () from which we extracted two sets of 200, 000 sentence pairs each for several language pairs.", "labels": [], "entities": [{"text": "Europarl parallel corpus", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.941774050394694}]}, {"text": "These were used to form the training and test sets.", "labels": [], "entities": []}, {"text": "The final test sets area randomly sampled 5, 000 sentence pairs from the 200, 000-sentence test split for each language pair.", "labels": [], "entities": []}, {"text": "All input data for the experiments in this section are publicly available 2 . Let us first zoom in to convey a sense of scale on a specific language pair.", "labels": [], "entities": []}, {"text": "The actual Europarl training set we generate for English (L1) to Spanish (L2), i.e. English fallback in a Spanish context, consists of 5, 608, 015 sentence pairs.", "labels": [], "entities": [{"text": "Europarl training set", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.9507509072621664}]}, {"text": "This number is much larger than the 200, 000 we mentioned before because single sentence pairs maybe reused multiple times with different marked fragments.", "labels": [], "entities": []}, {"text": "From this training set of sentence pairs over 100, 000 classifier experts are derived.", "labels": [], "entities": []}, {"text": "The eleven largest classifiers are shown in  For the classifier-based system, we tested various different feature vector configurations.", "labels": [], "entities": []}, {"text": "The first experiment, of which the results are shown in, sets a fixed and symmetric local context size across all classifiers, and tests three context widths.", "labels": [], "entities": []}, {"text": "Here we observe that a context width of one yields the best results.", "labels": [], "entities": []}, {"text": "The BLEU scores, not included in the figure but shown in, show a similar trend.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.999232292175293}]}, {"text": "This trend holds for all the MT metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9342511296272278}]}, {"text": "shows the results for English to Spanish in more detail and adds a comparison with the two baseline systems.", "labels": [], "entities": []}, {"text": "The various lXrY configurations use the same feature vector setup for all classifier experts.", "labels": [], "entities": []}, {"text": "Here X indicates the left context size and Y the right context size.", "labels": [], "entities": []}, {"text": "The auto configuration does not uniformly apply the same feature vector setup to all classifier experts but instead seeks to find the optimal setup per classifier expert.", "labels": [], "entities": []}, {"text": "This shall be further discussed in Section 6.1.", "labels": [], "entities": []}, {"text": "As expected, the LM baseline substantially outperforms the context-insensitive MLF baseline.", "labels": [], "entities": []}, {"text": "Second, our classifier approach attains a substantially higher accuracy than the LM baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9991031885147095}]}, {"text": "Third, we observe that adding the language model to our classifier leads to another significant gain  Statistical significance on the BLEU scores was tested using pairwise bootstrap sampling).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9978625178337097}]}, {"text": "All significance tests were performed with 5, 000 iterations.", "labels": [], "entities": [{"text": "significance", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.8672596216201782}]}, {"text": "We compared the outcomes of several key configurations.", "labels": [], "entities": []}, {"text": "We first tested l1r1 against both baselines; both differences are significant at p < 0.01 for both.", "labels": [], "entities": []}, {"text": "The same significance level was found when comparing l1r1+LM against l1r1, auto+LM against auto, as well as the LM baseline against the MLF baseline.", "labels": [], "entities": [{"text": "MLF baseline", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.8422380983829498}]}, {"text": "Automatic feature selection auto was found to perform statistically better than l1r1, but only at p < 0.05.", "labels": [], "entities": []}, {"text": "Conclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration was found to not be significantly better than that of the l2r2 configuration.", "labels": [], "entities": []}, {"text": "However, l1r1 performs significantly better than l3r3 at p < 0.01, and l2r2 performs significantly better than l3r3 at p < 0.01.", "labels": [], "entities": []}, {"text": "In we present some illustrative examples from the English\u2192Spanish Europarl data.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.8505907952785492}]}, {"text": "We show the difference between the most-likelyfragment baseline and our system.", "labels": [], "entities": []}, {"text": "Likewise, exemplifies small fragments from the l1r1 configuration compared to the same configuration enriched with a language model.", "labels": [], "entities": []}, {"text": "We observe in this data that the language model often has the added power to choose a correct translation that is not the first prediction of the classifier, but one of the weaker alternatives that nevertheless fits better.", "labels": [], "entities": []}, {"text": "Though the classifier generally works best in the l1r1 configuration, i.e. with context size one, the trigram-based language model allows further left-context information to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives.", "labels": [], "entities": []}, {"text": "This combination of a classifier with context size one and trigrambased language model proves to be most effective and reaches the best results so far.", "labels": [], "entities": []}, {"text": "We have not conducted experiments with language models of other orders.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Europarl results for English to Spanish (i.e English fallback in Spanish context). Recall =  0.9422", "labels": [], "entities": [{"text": "Europarl", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.8291009664535522}, {"text": "Recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9974615573883057}]}, {"text": " Table 6: Results on different datasets and language pairs. The iwslt12ted set is the dataset used in the", "labels": [], "entities": []}]}