{"title": [{"text": "Improving Citation Polarity Classification with Product Reviews", "labels": [], "entities": [{"text": "Improving Citation Polarity Classification", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8854488879442215}]}], "abstractContent": [{"text": "Recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering.", "labels": [], "entities": []}, {"text": "While this result confirms that citation classification is feasible, there are two drawbacks to this approach: (i) it requires a large annotated corpus for supervised classification, which in the case of scientific literature is quite expensive; and (ii) feature engineering that is too specific to one area of scientific literature may not be portable to other domains, even within scientific literature.", "labels": [], "entities": [{"text": "citation classification", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.9235604107379913}, {"text": "supervised classification", "start_pos": 156, "end_pos": 181, "type": "TASK", "confidence": 0.7380259037017822}]}, {"text": "In this paper we address these two drawbacks.", "labels": [], "entities": []}, {"text": "First, we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains.", "labels": [], "entities": [{"text": "citation classification", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.9580595791339874}]}, {"text": "Then, to avoid over-engineering specific citation features fora particular scientific domain, we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features.", "labels": [], "entities": []}, {"text": "We achieve better citation classification results with this cross-domain approach than using in-domain classification .", "labels": [], "entities": [{"text": "citation classification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8706521987915039}]}], "introductionContent": [{"text": "Citations have been categorized and studied fora half-century to better understand when and how citations are used, and to record and measure how information is exchanged (e.g., networks of co-cited papers or authors ().", "labels": [], "entities": []}, {"text": "Recently, the value of this information has been shown in practical applications such as information retrieval (IR) (, summarization, and even identifying scientific breakthroughs ().", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.8416024506092071}, {"text": "summarization", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.9941312670707703}]}, {"text": "We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications.", "labels": [], "entities": []}, {"text": "There has been no consensus on what aspects or functions of a citation should be annotated and how.", "labels": [], "entities": []}, {"text": "Early citation classification focused more on citation motivation, while later classification considered more the citation function.", "labels": [], "entities": [{"text": "citation classification", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.702311247587204}]}, {"text": "Recent studies using automatic classification have continued this tradition of introducing anew classification scheme with each new investigation into the use of citations.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7464735209941864}]}, {"text": "One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations.", "labels": [], "entities": [{"text": "citation classification", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.7125556766986847}]}, {"text": "The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9384689629077911}]}, {"text": "We follow much of the recent work on citation classification and concentrate on citation polarity.", "labels": [], "entities": [{"text": "citation classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.9251466691493988}]}], "datasetContent": [{"text": "Our initial experiments simply extend those of (2012) train on all \"labeled\" data and test on the \"unlabeled\" data).", "labels": [], "entities": []}, {"text": "These experiments should help answer two questions: does a larger amount of training data, even if out of domain, improve citation classification; and how well do the different product domains generalize to citations (i.e., which domains are most similar to citations)?", "labels": [], "entities": [{"text": "citation classification", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.8750306367874146}]}, {"text": "In contrast to previous work using MDSD, a lot of the work in domain adaptation also leverages a small amount of labeled target data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.758985161781311}]}, {"text": "In our second set of experiments, we follow the domain adaptation approaches described in and train on product review and citation data before testing on citations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Macro-F 1 results on CITD using different domain adaptation approaches.", "labels": [], "entities": []}]}