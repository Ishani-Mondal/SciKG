{"title": [{"text": "Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project", "labels": [], "entities": [{"text": "Wikipedia Bitaxonomy Project", "start_pos": 41, "end_pos": 69, "type": "DATASET", "confidence": 0.944868803024292}]}], "abstractContent": [{"text": "We present WiBi, an approach to the automatic creation of a bitaxonomy for Wikipedia, that is, an integrated taxon-omy of Wikipage pages and categories.", "labels": [], "entities": []}, {"text": "We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy.", "labels": [], "entities": []}, {"text": "Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9410723447799683}, {"text": "YAGO", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.7196160554885864}, {"text": "MENTA", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.774070680141449}]}, {"text": "WiBi is available at http://wibitaxonomy.org.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge has unquestionably become a key component of current intelligent systems in many fields of Artificial Intelligence.", "labels": [], "entities": []}, {"text": "The creation and use of machine-readable knowledge has not only entailed researchers) developing huge, broadcoverage knowledge bases (, but it has also hit big industry players such as Google and IBM, which are moving fast towards large-scale knowledge-oriented systems.", "labels": [], "entities": []}, {"text": "The creation of very large knowledge bases has been made possible by the availability of collaboratively-curated online resources such as Wikipedia and Wiktionary.", "labels": [], "entities": []}, {"text": "These resources are increasingly becoming enriched with new content in many languages and, although they are only partially structured, they provide a great deal of valuable knowledge which can be harvested and transformed into structured form.", "labels": [], "entities": []}, {"text": "Prominent examples include,), YAGO and WikiNet.", "labels": [], "entities": [{"text": "YAGO", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.7865757942199707}]}, {"text": "The types of semantic relation in these resources range from domain-specific, as in Freebase (, to unspecified relations, as in BabelNet.", "labels": [], "entities": []}, {"text": "However, unlike the case with smaller manually-curated resources such as WordNet, in many large automatically-created resources the taxonomical information is either missing, mixed across resources, e.g., linking Wikipedia categories to WordNet synsets as in YAGO, or coarsegrained, as in DBpedia whose hypernyms link to a small upper taxonomy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9703318476676941}]}, {"text": "Current approaches in the literature have mostly focused on the extraction of taxonomies from the network of Wikipedia categories.", "labels": [], "entities": []}, {"text": "WikiTaxonomy (, the first approach of this kind, is based on the use of heuristics to determine whether is-a relations hold between a category and its subcategories.", "labels": [], "entities": [{"text": "WikiTaxonomy", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9052828550338745}]}, {"text": "Subsequent approaches have also exploited heuristics, but have extended them to any kind of semantic relation expressed in the category names.", "labels": [], "entities": []}, {"text": "But while the aforementioned attempts provide structure for categories that supply meta-information for Wikipedia pages, surprisingly little attention has been paid to the acquisition of a full-fledged taxonomy for Wikipedia pages themselves.", "labels": [], "entities": []}, {"text": "For instance, provide a general vector-based method which, however, is incapable of linking pages which do not have a WordNet counterpart.", "labels": [], "entities": []}, {"text": "Higher coverage is provided by thanks to the use of a set of effective heuristics, however, the approach also draws on WordNet and sense frequency information.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9658231735229492}]}, {"text": "In this paper we address the task of taxonomizing Wikipedia in away that is fully independent of other existing resources such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9688303470611572}]}, {"text": "We present WiBi, a novel approach to the creation of a Wikipedia bitaxonomy, that is, a taxonomy of Wikipedia pages aligned to a taxonomy of categories.", "labels": [], "entities": []}, {"text": "At the core of our approach lies the idea that the information at the page and category level are mutually beneficial for inducing a widecoverage and fine-grained integrated taxonomy.", "labels": [], "entities": []}], "datasetContent": [{"text": "Statistics We applied the above linkers to the October 2012 English Wikipedia dump.", "labels": [], "entities": [{"text": "October 2012 English Wikipedia dump", "start_pos": 47, "end_pos": 82, "type": "DATASET", "confidence": 0.7124746561050415}]}, {"text": "Out of the 3,829,058 total pages, 4,270,232 hypernym lemmas were extracted in the syntactic step for 3,697,113 pages (covering more than 96% of the total).", "labels": [], "entities": []}, {"text": "Due to illformed definitions, though, it was not always possible to extract the hypernym lemma: for example, 6 APRIL 2010 BAGHDAD BOMBINGS is defined as \"A series of bomb explosions destroyed several buildings in Baghdad\", which only implicitly provides the hypernym.", "labels": [], "entities": [{"text": "APRIL 2010 BAGHDAD BOMBINGS", "start_pos": 111, "end_pos": 138, "type": "METRIC", "confidence": 0.5626593828201294}]}, {"text": "The semantic step disambiguated 3,718,612 hypernyms for 3,294,562 Wikipedia pages, i.e., covering more than 86% of the English pages with at least one disambiguated hypernym.", "labels": [], "entities": []}, {"text": "plots the number and distribution of hypernyms disambiguated by our hypernym linkers.", "labels": [], "entities": []}, {"text": "Taxonomy quality To evaluate the quality of our page taxonomy we randomly sampled 1,000 Wikipedia pages.", "labels": [], "entities": []}, {"text": "For each page we provided: i) a list of suitable hypernym lemmas for the page, mainly selected from its definition; ii) for each lemma the correct hypernym page(s).", "labels": [], "entities": []}, {"text": "We calculated precision as the average ratio of correct hypernym lemmas (senses) to the total number of lemmas (senses) returned for all the pages in the dataset, recall as the number of correct lemmas (senses) over the total number of lemmas (senses) in the dataset, and coverage as the fraction of pages for which at least one lemma (sense) was returned, independently of its correctness.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9994669556617737}, {"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9993289709091187}, {"text": "coverage", "start_pos": 272, "end_pos": 280, "type": "METRIC", "confidence": 0.9987413287162781}]}, {"text": "Results, both at lemma-and sense-level, are reported in Table 1.", "labels": [], "entities": []}, {"text": "Not only does our taxonomy show high precision and recall in extracting ambiguous hypernyms, it also disambiguates more than 3/4 of the hypernyms with high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9987892508506775}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9994848966598511}, {"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9732975363731384}]}, {"text": "Category taxonomy statistics We applied phases 2 and 3 to the output of phase 1, which was evaluated in Section 3.3.", "labels": [], "entities": []}, {"text": "In we show the increase in category coverage at each iteration throughout the execution of the two phases (1SUP, SUB and SUPER correspond to the three above heuristics of phase 3).", "labels": [], "entities": []}, {"text": "The final outcome is a category taxonomy which includes 594,917 hypernymy links between categories,  covering more than 96% of the 618,641 categories in the October 2012 English Wikipedia dump.", "labels": [], "entities": [{"text": "English Wikipedia dump", "start_pos": 170, "end_pos": 192, "type": "DATASET", "confidence": 0.8899056911468506}]}, {"text": "The graph shows the steepest slope in the first iterations of phase 2, which converges around 400k categories at iteration 30, and a significant boost due to phase 3 producing another 175k hypernymy edges, with the super-category heuristic contributing most.", "labels": [], "entities": []}, {"text": "78.90% of the nodes in T C belong to the same connected component.", "labels": [], "entities": []}, {"text": "The average height of the biggest component of T C is 23.26 edges and the maximum height is 49.", "labels": [], "entities": []}, {"text": "We note that the average height of T C is much greater than that of T P , which reflects the category taxonomy distinguishing between very subtle classes, such as ALBUMS BY ARTISTS, ALBUMS BY RECORDING LOCATION, etc.", "labels": [], "entities": [{"text": "ALBUMS BY ARTISTS", "start_pos": 163, "end_pos": 180, "type": "METRIC", "confidence": 0.7712159156799316}, {"text": "ALBUMS BY RECORDING LOCATION", "start_pos": 182, "end_pos": 210, "type": "METRIC", "confidence": 0.6558967977762222}]}, {"text": "Category taxonomy quality To estimate the quality of the category taxonomy, we randomly sampled 1,000 categories and, for each of them, we manually associated the super-categories which were deemed to be appropriate hypernyms.", "labels": [], "entities": []}, {"text": "shows the performance trend as the algorithm iteratively covers more and more categories.", "labels": [], "entities": []}, {"text": "Phase 2 is particularly robust across iterations, as it leads to increased recall while retaining very high precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9994028806686401}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9966942071914673}]}, {"text": "As regards phase 3, the super-categories heuristic leads to only a slight precision decrease, while improving recall considerably.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994593262672424}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9992520213127136}]}, {"text": "Overall, the final taxonomy T C achieves 85.80% precision, 83.40% recall and 97.20% coverage on our dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9988896250724792}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.998705267906189}, {"text": "coverage", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9982693195343018}]}, {"text": "Page taxonomy improvement As a result of phase 2, 141,105 additional hypernymy links were also added to the page taxonomy, resulting in an overall 82.99% precision, 77.90% recall and 92.10% coverage, with a non-negligible 3% boost from phase 1 to phase 2 in terms of recall and coverage on our Wikipedia page dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9919624924659729}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9994334578514099}, {"text": "coverage", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9905812740325928}, {"text": "recall", "start_pos": 267, "end_pos": 273, "type": "METRIC", "confidence": 0.9989963173866272}, {"text": "coverage", "start_pos": 278, "end_pos": 286, "type": "METRIC", "confidence": 0.9839974641799927}, {"text": "Wikipedia page dataset", "start_pos": 294, "end_pos": 316, "type": "DATASET", "confidence": 0.8154396613438925}]}, {"text": "We also calculated some statistics for the resulting taxonomy obtained by aggregating the 3.8M   We compared our resource (WiBi) against the Wikipedia taxonomies of the major knowledge resources in the literature providing hypernym links, namely DBpedia, WikiNet, MENTA, WikiTaxonomy and YAGO (see Section 6).", "labels": [], "entities": [{"text": "YAGO", "start_pos": 288, "end_pos": 292, "type": "METRIC", "confidence": 0.6105390191078186}]}, {"text": "As datasets, we used our gold standards of 1,000 randomlysampled pages (see Section 3.3) and categories (see Section 5.1).", "labels": [], "entities": []}, {"text": "In order to ensure a level playing field, we detected those pages (categories) which do not exist in any of the above resources and removed them to ensure full coverage of the dataset across all resources.", "labels": [], "entities": []}, {"text": "For each resource we calculated precision, by manually marking each hypernym returned for each page (category) as corrector not.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996153116226196}]}, {"text": "As regards recall, we note that in two cases (i.e., DBpedia returning page supertypes from its upper taxonomy, YAGO linking categories to WordNet synsets) the generalizations are neither pages nor categories and that MENTA returns heterogeneous hypernyms as mixed sets of WordNet synsets, Wikipedia pages and categories.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9868733286857605}, {"text": "YAGO", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.8795268535614014}]}, {"text": "Given this heterogeneity, standard recall across resources could not be calculated.", "labels": [], "entities": [{"text": "standard", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9562788605690002}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.8692485094070435}]}, {"text": "For this reason we calculated recall as described in Section 3.3.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9994187355041504}]}], "tableCaptions": [{"text": " Table 1: Page taxonomy performance.", "labels": [], "entities": []}, {"text": " Table 2: Page and category taxonomy evaluation.   \u2020 (  \u2020 \u2020 ) denotes statistically significant difference,  using \u03c7 2 test, p < 0.02 (p < 0.01) between WiBi  and the daggered resource.", "labels": [], "entities": []}]}