{"title": [{"text": "Graph Ranking for Collective Named Entity Disambiguation", "labels": [], "entities": [{"text": "Collective Named Entity Disambiguation", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.6177916526794434}]}], "abstractContent": [{"text": "Named Entity Disambiguation (NED) refers to the task of mapping different named entity mentions in running text to their correct interpretations in a specific knowledge base (KB).", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED) refers to the task of mapping different named entity mentions in running text to their correct interpretations in a specific knowledge base (KB)", "start_pos": 0, "end_pos": 178, "type": "Description", "confidence": 0.7159760027162491}]}, {"text": "This paper presents a collective disambiguation approach using a graph model.", "labels": [], "entities": []}, {"text": "All possible NE candidates are represented as nodes in the graph and associations between different candidates are represented by edges between the nodes.", "labels": [], "entities": []}, {"text": "Each node has an initial confidence score, e.g. entity popularity.", "labels": [], "entities": []}, {"text": "Page-Rank is used to rank nodes and the final rank is combined with the initial confidence for candidate selection.", "labels": [], "entities": []}, {"text": "Experiments on 27,819 NE textual mentions show the effectiveness of using Page-Rank in conjunction with initial confidence: 87% accuracy is achieved, outperforming both baseline and state-of-the-art approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9993141889572144}]}], "introductionContent": [{"text": "Named entities (NEs) have received much attention over the last two decades (, mostly focused on recognizing the boundaries of textual NE mentions and classifying them as, e.g., Person, Organization or Location.", "labels": [], "entities": [{"text": "Named entities (NEs)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6156486868858337}, {"text": "recognizing the boundaries of textual NE mentions", "start_pos": 97, "end_pos": 146, "type": "TASK", "confidence": 0.651019641331264}]}, {"text": "However, references to entities in the real world are often ambiguous: there is a many-to-many relation between NE mentions and the entities they denote in the real world.", "labels": [], "entities": []}, {"text": "For example, Norfolk may refer to a person, \"Peter Norfolk, a wheelchair tennis player\", a place in the UK, \"Norfolk County\", or in the US, \"Norfolk, Massachusetts\"; conversely, one entity maybe known by many names, such as \"Cat Stevens\", \"Yusuf Islam\" and \"Steven Georgiou\".", "labels": [], "entities": []}, {"text": "The NED task is to establish a correct mapping between each NE mention in a document and the real world entity it denotes.", "labels": [], "entities": []}, {"text": "Following most researchers in this area, we treat entries in a large knowledge base (KB) as surrogates for real world entities when carrying out NED and, in particular, use Wikipedia as the reference KB for disambiguating NE mentions.", "labels": [], "entities": []}, {"text": "NED is important for tasks like KB population, where we want to extract new information from text about an entity and add this to a pre-existing entry in a KB; or for information retrieval, where we may want to cluster or filter results for different entities with the same textual mentions.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 167, "end_pos": 188, "type": "TASK", "confidence": 0.7666653394699097}]}, {"text": "The main hypothesis in this work is that different NEs in a document help to disambiguate each other.", "labels": [], "entities": []}, {"text": "The problem is that other textual mentions in the document are also ambiguous.", "labels": [], "entities": []}, {"text": "So, what is needed is a collective disambiguation approach that jointly disambiguates all NE textual mentions.", "labels": [], "entities": []}, {"text": "In our approach we model each possible candidate for every NE mention in a document as a distinct node in a graph and model candidate coherence by links between the nodes.", "labels": [], "entities": []}, {"text": "We call such graphs solution graphs.", "labels": [], "entities": []}, {"text": "shows an example of the solution graph for three mentions \"A\", \"B\", and \"C\" found in a document, where the candidate entities for each mention are referred to using the lowercase form of the mention's letter together with a distinguishing subscript.", "labels": [], "entities": []}, {"text": "The goal of disambiguation is to find a set of nodes where only one candidate is selected from the set of entities associated with each mention, e.g. a 3 , b 2 , c 2 . Our approach first ranks all nodes in the solution graph using the Page-Rank algorithm, then re-ranks all nodes by combining the initial confidence and graph ranking scores.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.9628638029098511}]}, {"text": "We consider several different measures for computing the initial confidence assigned to each node and several measures for determining and weighting the graph edges.", "labels": [], "entities": []}, {"text": "Node linking relies on the fact that the textual portion of KB entries typically contains mentions of other NEs.", "labels": [], "entities": [{"text": "Node linking", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9198121428489685}]}, {"text": "When these mentions are hyper-linked to KB entries, we can infer that there is some relation between the real world entities corresponding to the KB entries, i.e. that they should be linked in our solution graph.", "labels": [], "entities": []}, {"text": "These links also allow us to buildup statistical co-occurrence counts between entities that occur in the same context which maybe used to weight links in our graph.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the AIDA dataset.", "labels": [], "entities": [{"text": "AIDA dataset", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9633646607398987}]}, {"text": "Comparison with the baseline approach and some state-of-the-art approaches shows our approach offers substantial improvements in disambiguation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8938043713569641}]}], "datasetContent": [{"text": "We used AIDA dataset 3 , which is based on the CoNLL 2003 data for NER tagging.", "labels": [], "entities": [{"text": "AIDA dataset 3", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8798387249310812}, {"text": "CoNLL 2003 data", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9776344299316406}, {"text": "NER tagging", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9397865831851959}]}, {"text": "All mentions are manually disambiguated against Wikipedia).", "labels": [], "entities": []}, {"text": "This dataset contains 1393 documents and 34,965 annotated mentions.", "labels": [], "entities": []}, {"text": "We only consider NE mentions with an entry in the Wikipedia KB, ignoring the 20% of query mentions (7136) without a link to the KB, as Hoffart did.", "labels": [], "entities": [{"text": "Wikipedia KB", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9078545272350311}]}, {"text": "Micro-averaged and macro-averaged accuracy are used for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9555119872093201}]}, {"text": "In this context microaveraged accuracy corresponds to the proportion of textual mentions correctly disambiguated while macro-averaged accuracy corresponds to the proportion of textual mentions correctly disambiguated per entity, averaged overall entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9276670217514038}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.536467969417572}]}], "tableCaptions": [{"text": " Table 1: Results comparison between Proposed Approach and State-of-the-art", "labels": [], "entities": [{"text": "Approach", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.6536946892738342}]}, {"text": " Table 2: Results using initial confidence (P RI )", "labels": [], "entities": [{"text": "initial confidence (P RI )", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.8678396940231323}]}, {"text": " Table 3: Results using weighted edges (P RC )", "labels": [], "entities": [{"text": "P RC", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.7193639278411865}]}, {"text": " Table 4: Results using IConf and weighted edges P RIC", "labels": [], "entities": [{"text": "RIC", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9297462105751038}]}]}