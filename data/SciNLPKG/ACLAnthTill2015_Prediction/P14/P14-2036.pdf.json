{"title": [{"text": "A Novel Content Enriching Model for Microblog Using News Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a novel model for enriching the content of microblogs by exploiting external knowledge, thus improving the data sparseness problem in short text classification.", "labels": [], "entities": [{"text": "short text classification", "start_pos": 160, "end_pos": 185, "type": "TASK", "confidence": 0.6790506442387899}]}, {"text": "We assume that mi-croblogs share the same topics with external knowledge.", "labels": [], "entities": []}, {"text": "We first build an optimization model to infer the topics of mi-croblogs by employing the topic-word distribution of the external knowledge.", "labels": [], "entities": []}, {"text": "Then the content of microblogs is further enriched by relevant words from external knowledge.", "labels": [], "entities": []}, {"text": "Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods.", "labels": [], "entities": [{"text": "microblog classification", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6786050051450729}, {"text": "text classification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7111477553844452}]}], "introductionContent": [{"text": "During the past decade, the short text representation has been intensively studied.", "labels": [], "entities": [{"text": "short text representation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6485046744346619}]}, {"text": "Previous researches ( show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area.", "labels": [], "entities": []}, {"text": "Meanwhile, external knowledge has been found helpful () in tackling the data scarcity problem by enriching short texts with informative context.", "labels": [], "entities": []}, {"text": "Well-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8762711882591248}]}, {"text": "Nowadays, most of the work on short text focuses on microblog.", "labels": [], "entities": []}, {"text": "As anew form of short text, microblog has some unique features like informal spelling and emerging words, and many microblogs are strongly related to up-to-date topics as well.", "labels": [], "entities": []}, {"text": "Every day, a great quantity of microblogs * Corresponding author more than we can read is pushed to us, and finding what we are interested in becomes rather difficult, so the ability of choosing what kind of microblogs to read is urgently demanded by common user.", "labels": [], "entities": []}, {"text": "Such ability can be implemented by effective short text classification.", "labels": [], "entities": [{"text": "short text classification", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6613679428895315}]}, {"text": "Treating microblogs as standard texts and directly classifying them cannot achieve the goal of effective classification because of sparseness problem.", "labels": [], "entities": []}, {"text": "On the other hand, news on the Internet is of information abundance and many microblogs are news-related.", "labels": [], "entities": []}, {"text": "They share up-to-date topics and sometimes quote each other.", "labels": [], "entities": []}, {"text": "Thus, external knowledge, such as news, provides rich supplementary information for analysing and mining microblogs.", "labels": [], "entities": []}, {"text": "Motivated by the idea of using topic model and external knowledge mentioned above, we present an LDA-based enriching method using the news corpus, and apply it to the task of microblog classification.", "labels": [], "entities": [{"text": "microblog classification", "start_pos": 175, "end_pos": 199, "type": "TASK", "confidence": 0.7778637707233429}]}, {"text": "The basic assumption in our model is that news articles and microblogs tend to share the same topics.", "labels": [], "entities": []}, {"text": "We first infer the topic distribution of each microblog based on the topic-word distribution of news corpus obtained by the LDA estimation.", "labels": [], "entities": [{"text": "LDA estimation", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.8185749351978302}]}, {"text": "With the above two distributions, we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog, since words not appearing in the microblog may still be highly relevant.", "labels": [], "entities": []}, {"text": "To sum up, our contributions are: (1) We formulate the topic inference problem for short texts as a convex optimization problem.", "labels": [], "entities": []}, {"text": "(2) We enrich the content of microblogs by inferring the association between microblogs and external words in a probabilistic perspective.", "labels": [], "entities": []}, {"text": "(3) We evaluate our method on the real datasets and experiment results outperform the baseline methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our method, we build our own datasets.", "labels": [], "entities": []}, {"text": "We crawl 95028 Chinese news reports from Sina News website, segment them, and remove stop words and rare words.", "labels": [], "entities": [{"text": "Chinese news reports from Sina News website", "start_pos": 15, "end_pos": 58, "type": "DATASET", "confidence": 0.7745007744857243}]}, {"text": "After preprocessing, these news documents are used as external knowledge.", "labels": [], "entities": []}, {"text": "As for microblog, we crawl a number of microblogs from Sina Weibo, and ask unbiased assessors to manually classify them into 9 categories following the column setting of Sina News.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8413822650909424}, {"text": "Sina News", "start_pos": 170, "end_pos": 179, "type": "DATASET", "confidence": 0.8996374607086182}]}, {"text": "After the manual classification, we remove short microblogs (less than 10 words), usernames, links and some special characters, then we segment them and remove rare words as well.", "labels": [], "entities": []}, {"text": "Finally, we get 1671 classified microblogs as our microblog dataset.", "labels": [], "entities": [{"text": "microblog dataset", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.8207152187824249}]}, {"text": "The size of each category is shown in Table 1.", "labels": [], "entities": []}, {"text": "There are some important details of our implementation.", "labels": [], "entities": []}, {"text": "In step (a) of Section 3.1 we estimate LDA model using GibbsLDA++, a C/C++ implementation of LDA using Gibbs Sampling.", "labels": [], "entities": [{"text": "GibbsLDA++", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9173105657100677}]}, {"text": "In step (b) of Section3.2, OPTI toolbox on Matlab is used to help solve the convex problems.", "labels": [], "entities": [{"text": "Section3.2", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.7480646967887878}]}, {"text": "In the classification tasks shown below, we use LibSVM as classifier and perform ten-fold cross validation to evaluate the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9482828974723816}]}, {"text": "In this section, we report the average precision of each method as shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9842580556869507}]}, {"text": "The enhanced vector is the representation generated by our method.", "labels": [], "entities": []}, {"text": "Two baselines are TFIDF vector and boolean vector (word occurrence) of the original microblog.", "labels": [], "entities": [{"text": "TFIDF vector", "start_pos": 18, "end_pos": 30, "type": "METRIC", "confidence": 0.9428450465202332}]}, {"text": "In the table, our method increases the classification accuracy GibbsLDA++: http://gibbslda.sourceforge.net OPTI Toolbox: http://www.i2c2.aut.ac.nz/Wiki/OPTI/ SVM.NET: http://www.matthewajohnson.org/software /svm.html from 75.52% to 84.53% when considering additional information, which means our method indeed improves the representation of microblogs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9536320567131042}, {"text": "GibbsLDA", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.7083320021629333}]}, {"text": "The experiment corresponding to is to discover how the classification accuracy changes when we fix the number of topics (K = 100) and change the number of added words (L) in our method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9388941526412964}]}, {"text": "Result shows that more added words do not mean higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9987486600875854}]}, {"text": "By studying some cases, we find out that if we add too many words, the proportion of \"noisy words\" will increase.", "labels": [], "entities": []}, {"text": "We reach the best result when number of added words is 300.", "labels": [], "entities": []}, {"text": "Top Relevant Words (Translated) Kim Jong Un held an emergency meeting this morning, and commanded the missile units to prepare for attacking U.S. military bases at anytime.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Microblog number of every category", "labels": [], "entities": []}, {"text": " Table 3: Case study (Translated from Chinese)", "labels": [], "entities": []}]}