{"title": [{"text": "Discovering Latent Structure in Task-Oriented Dialogues", "labels": [], "entities": [{"text": "Discovering Latent Structure in Task-Oriented Dialogues", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.7997910579045614}]}], "abstractContent": [{"text": "A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue, since it provides a basis for analysing, evaluating, and building conversational systems.", "labels": [], "entities": []}, {"text": "We propose three new unsupervised models to discover latent structures in task-oriented dialogues.", "labels": [], "entities": []}, {"text": "Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states).", "labels": [], "entities": []}, {"text": "We apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and human-human text-based chats from a live technical support service.", "labels": [], "entities": []}, {"text": "We show that our models extract meaningful state representations and dialogue structures consistent with human annotations.", "labels": [], "entities": []}, {"text": "Quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modeling human conversation is a fundamental scientific pursuit.", "labels": [], "entities": [{"text": "Modeling human conversation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8950867454210917}]}, {"text": "In addition to yielding basic insights into human communication, computational models of conversation underpin a host of real-world applications, including interactive dialogue systems), dialogue summarization (;, and even medical applications such as diagnosis of psychological conditions).", "labels": [], "entities": [{"text": "dialogue summarization", "start_pos": 187, "end_pos": 209, "type": "TASK", "confidence": 0.8264948427677155}]}, {"text": "Computational models of conversation can be broadly divided into two genres: modeling and control.", "labels": [], "entities": []}, {"text": "Control is concerned with choosing actions in interactive settings-for example to maximize task completion-using reinforcement learn- * Work done at Microsoft Research.", "labels": [], "entities": [{"text": "Microsoft Research", "start_pos": 149, "end_pos": 167, "type": "DATASET", "confidence": 0.8496312201023102}]}, {"text": "ing (), supervised learning, hand-crafted rules), or mixtures of these.", "labels": [], "entities": []}, {"text": "By contrast, modeling-the genre of this paper-is concerned with inferring a phenomena in an existing corpus, such as dialogue acts in two-party conversations () or topic shifts in multi-party dialogues (;).", "labels": [], "entities": []}, {"text": "Many past works rely on supervised learning or human annotations, which usually requires manual labels and annotation guidelines (.", "labels": [], "entities": []}, {"text": "It constrains scaling the size of training examples, and application domains.", "labels": [], "entities": []}, {"text": "By contrast, unsupervised methods operate only on the observable signal (e.g. words) and are estimated without labels or their attendant limitations).", "labels": [], "entities": []}, {"text": "They are particularly relevant because conversation is a temporal process where models are trained to infer a latent state which evolves as the dialogue progresses (.", "labels": [], "entities": []}, {"text": "Our basic approach is to assume that each utterance in the conversation is in a latent state, which has a causal effect on the words the conversants produce.", "labels": [], "entities": []}, {"text": "Inferring this model yields basic insights into the structure of conversation and also has broad practical benefits, for example, speech recognition (, natural language generation (, and new features for dialogue policy optimization ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.8123064637184143}, {"text": "natural language generation", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.6840294400850931}, {"text": "dialogue policy optimization", "start_pos": 204, "end_pos": 232, "type": "TASK", "confidence": 0.7194831967353821}]}, {"text": "There has been limited past work on unsupervised methods for conversation modeling.", "labels": [], "entities": [{"text": "conversation modeling", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.9014698565006256}]}, {"text": "Chotimongkol (2008) studies task-oriented conversation and proposed a model based on a hidden Markov model (HMM).", "labels": [], "entities": []}, {"text": "extends it by introducing additional word sources, and applies to non-task-oriented conversationssocial interactions on Twitter, where the subjects discussed are very diffuse.", "labels": [], "entities": []}, {"text": "The additional word sources capture the subjects, leaving the statespecific models to express common dialogue flows such as question/answer pairs.", "labels": [], "entities": []}, {"text": "In this paper, we retain the underlying HMM, but assume words are emitted using topic models (TM), exemplified by latent Dirichlet allocation (.", "labels": [], "entities": []}, {"text": "LDA assumes each word in an utterance is drawn from one of a set of latent topics, where each topic is a multinomial distribution over the vocabulary.", "labels": [], "entities": []}, {"text": "The key idea is that the set of topics is shared across all states, and each state corresponds to a mixture of topics.", "labels": [], "entities": []}, {"text": "We propose three model variants that link topics and states in different ways.", "labels": [], "entities": []}, {"text": "Sharing topics across states is an attractive property in task-oriented dialogue, where a single concept can be discussed at many points in a dialogue, yet different topics often appear in predictable sequences.", "labels": [], "entities": [{"text": "Sharing topics across states", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8463929146528244}]}, {"text": "Compared to past works, the decoupling of states and topics gives our models more expressive power and the potential to be more data efficient.", "labels": [], "entities": []}, {"text": "Empirically, we find that our models outperform past approaches on two realworld corpora of task-oriented dialogues.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 introduces two task-oriented domains and corpora; Section 3 details three new unsupervised generative models which combine HMMs and LDA and efficient inference schemes; Section 4 evaluates our models qualitatively and quantitatively, and finally conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we examine the effectiveness of our models.", "labels": [], "entities": []}, {"text": "We first evaluate our models qualitatively by exploring the inferred state diagram.", "labels": [], "entities": []}, {"text": "We then perform quantitative analysis with log likelihood measurements and an ordering task on a held-out test set.", "labels": [], "entities": [{"text": "log likelihood measurements", "start_pos": 43, "end_pos": 70, "type": "METRIC", "confidence": 0.8032546043395996}]}, {"text": "We train all models with 80% of the entire dataset and use the rest for testing.", "labels": [], "entities": []}, {"text": "We run the Gibbs samplers for 1000 iterations and update all hyper-parameters using slice sampling) every 10 iterations.", "labels": [], "entities": []}, {"text": "The training likelihood suggest all models converge within 500\u2212800 iterations.", "labels": [], "entities": []}, {"text": "For all Chib-style estimators, we collect 100 samples along the Markov chain to approximate the marginal likelihood.", "labels": [], "entities": []}, {"text": "shows the state diagram for BusTime corpus inferred by TM-HMM without any supervision.", "labels": [], "entities": []}, {"text": "Every dialogue is opened by asking the user to say a bus route, or to say \"I'm not sure.\"", "labels": [], "entities": []}, {"text": "It then transits to a state about location, e.g., origin and destination.", "labels": [], "entities": []}, {"text": "Both these two states may continue to a confirmation step immediately after.", "labels": [], "entities": []}, {"text": "After verifying all the necessary information, the system asks if the user wants \"the next few buses\".", "labels": [], "entities": []}, {"text": "Otherwise, the system follows up with the user on the particular date and time information.", "labels": [], "entities": []}, {"text": "After system reads out bus times, the user has options to \"repeat\" or ask for subsequent schedules.", "labels": [], "entities": []}, {"text": "In addition, we also include the humanannotated dialogue flow in for reference.", "labels": [], "entities": []}, {"text": "It only illustrates the most common design of system actions, without showing edge cases.", "labels": [], "entities": []}, {"text": "Comparing these two figures, the dialogue flow inferred by our model along the most probable path (highlighted in bold red in is consistent with underlying design.", "labels": [], "entities": []}, {"text": "Furthermore, our models are able to capture edge cases-omitted for space-through a more general and probabilistic fashion.", "labels": [], "entities": []}, {"text": "In summary, our Recall in BusTime, state transitions occur after each pair of system/user utterances, so we display them synchronously.", "labels": [], "entities": []}, {"text": "The system was designed this way because most users say \"yes\" to this question, obviating the date and time.", "labels": [], "entities": []}, {"text": "models yield a very similar flowchart to the underlying design in a completely unsupervised way.", "labels": [], "entities": []}, {"text": "7 shows part of the flowchart for the TechSupport corpus, generated by the TM-HMMSS model.", "labels": [], "entities": [{"text": "TechSupport corpus", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.9441239535808563}]}, {"text": "8 A conversation usually starts with a welcome message from a customer support agent.", "labels": [], "entities": []}, {"text": "Next, clients sometimes report a problem; otherwise, the agent gathers the client's identity.", "labels": [], "entities": []}, {"text": "After these preliminaries, the agent usually checks the system version or platform settings.", "labels": [], "entities": []}, {"text": "Then, information about the problem is exchanged, and a cycle ensues where agents propose solutions, and clients attempt them, reporting results.", "labels": [], "entities": []}, {"text": "Usually, a conversation loops among these states until either the problem is resolved (as the case shown in the or the client is left with a reference number for future follow-up (not shown due to space limit).", "labels": [], "entities": []}, {"text": "Although technical support is taskoriented, the scope of possible issues is vast and not prescribed.", "labels": [], "entities": []}, {"text": "The table in lists the top ranked words of selected topics-the categories clients often report problems in.", "labels": [], "entities": []}, {"text": "It illustrates that, qualitatively, TM-HMMSS discovers both problem categories and conversation structures on our data.", "labels": [], "entities": []}, {"text": "As one of the baseline model, we also include apart of flowchart generated by LM-HMM model with similar settings of T = 20 states.", "labels": [], "entities": []}, {"text": "Illustrated by the highlighted states in 6, LM-HMM model conflates interactions that commonly occur at the beginning and end of a dialogue-i.e., \"acknowledge agent\" and \"resolve problem\", since their underlying language models are likely to produce similar probability distributions over words.", "labels": [], "entities": []}, {"text": "By incorporating topic information, our proposed models (e.g., TM-HMMSS in) are able to enforce the state transitions towards more frequent flow patterns, which further helps to overcome the weakness of language model.", "labels": [], "entities": []}, {"text": "In this section, we evaluate our models using log likelihood and an ordering task on a held-out test set.", "labels": [], "entities": []}, {"text": "Both evaluation metrics measure the predictive power of a conversation model.", "labels": [], "entities": []}, {"text": "Downtown, is that correct?", "labels": [], "entities": []}, {"text": "Did you just say Norwood?", "labels": [], "entities": []}, {"text": "Say just the day you want.", "labels": [], "entities": []}, {"text": "Say just the <me you want.", "labels": [], "entities": []}, {"text": "I'm sorry, I can't find any bus at all that run from Milton to Norwell.", "labels": [], "entities": [{"text": "Milton", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9773442149162292}, {"text": "Norwell", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9067738056182861}]}, {"text": "I checked route 61C and I also checked all the other bus routes I know too.", "labels": [], "entities": []}, {"text": "Repeat, next, previous  Log Likelihood The likelihood metric measures the probability of generating the test set under a specified model.", "labels": [], "entities": []}, {"text": "As shown in, our models yield as good or better likelihood than LM-HMM and LM-HMMS models on both datasets under all settings.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.967706024646759}]}, {"text": "For our proposed models, TM-HMMS and TM-HMMSS perform better than TM-HMM on TechSupport, but not necessarily on BusTime.", "labels": [], "entities": []}, {"text": "In addition, we notice that the marginal benefit of TM-HMMSS over TM-HMM is greater on TechSupport dataset, where each dialogue focuses on one of many possible tasks.", "labels": [], "entities": [{"text": "TechSupport dataset", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.9550525546073914}]}, {"text": "This coincides with our belief that topics are more conversation dependent and shared across the entire corpus in customer support data-i.e., different clients in different sessions might ask about similar issues.", "labels": [], "entities": []}, {"text": "Ordering Test proposes an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data.", "labels": [], "entities": [{"text": "rank correlation coefficient", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.8443097472190857}]}, {"text": "They use Kendall's \u03c4 as evaluation metric, which is based on the agreement between pairwise orderings of two sequences.", "labels": [], "entities": []}, {"text": "It ranges from \u22121 to +1, where +1 indicates an identical ordering and \u22121 indicates a reverse ordering.", "labels": [], "entities": []}, {"text": "The idea is to generate all permutations of the utterances in a dialogue (including true ordering), and compute the log likelihood for each under the model.", "labels": [], "entities": []}, {"text": "Then, Kendall's \u03c4 is computed between the most probable permutation and true ordering.", "labels": [], "entities": []}, {"text": "The result is the average of \u03c4 values for all dialogues in test corpus.", "labels": [], "entities": []}, {"text": "limits their dataset by choosing Twitter dialogues containing 3 to 6 posts (utterances), making it tractable to enumerate all permutations.", "labels": [], "entities": []}, {"text": "However, our datasets are much larger, and enumerating all possible permutations of dialogues with more than 20 or 30 utterances is infeasible.", "labels": [], "entities": []}, {"text": "Instead, we incrementally buildup the permutation set by adding one random permutation at a time, and taking the most probable permutation after each addition.", "labels": [], "entities": []}, {"text": "If this process were continued (intractably!) until all permutations are enumerated, the true value of Kendall's \u03c4 test would be reached.", "labels": [], "entities": []}, {"text": "In practice, the value appears to plateau after a few dozen measurements.", "labels": [], "entities": []}, {"text": "We present our results in.", "labels": [], "entities": []}, {"text": "Our models consistently perform as good or better than     states.", "labels": [], "entities": []}, {"text": "Cyan blocks are system actions and yellow blocks are user responses.", "labels": [], "entities": []}, {"text": "In every block, the upper cell shows the top ranked words, and the lower cell shows example word sequences or string patterns of that state.", "labels": [], "entities": []}, {"text": "Transition probability cut-off is 0.05.", "labels": [], "entities": []}, {"text": "A poorly-inferred state is highlighted, which seems to conflate the \"acknowledge agent\" and \"resolve problem\" states, and TM-HMMSS model has properly disentangled (.", "labels": [], "entities": []}, {"text": "For BusTime data, all models perform relatively well except LM-HMM which only indicates weak correlations.", "labels": [], "entities": []}, {"text": "TM-HMM out-performs all other models under all settings.", "labels": [], "entities": [{"text": "TM-HMM out-performs", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.5576823353767395}]}, {"text": "This is also true for TechSupport dataset.", "labels": [], "entities": [{"text": "TechSupport dataset", "start_pos": 22, "end_pos": 41, "type": "DATASET", "confidence": 0.9808482229709625}]}, {"text": "LM-HMMS, TM-HMMS and TM-HMMSS models perform considerably well on BusTime, but not on TechSupport data.", "labels": [], "entities": [{"text": "TechSupport data", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.9134970009326935}]}, {"text": "These three models allow words to be generated from additional sources other than states.", "labels": [], "entities": []}, {"text": "Although this improves log likelihood, it is possible these models encode less information about the state sequences, at least in the more diffuse TechSupport data.", "labels": [], "entities": [{"text": "TechSupport data", "start_pos": 147, "end_pos": 163, "type": "DATASET", "confidence": 0.9587587118148804}]}, {"text": "In summary, under both quantitative evaluation measures, our models advance state-of-the-art, however which of our models is best depends on the application.", "labels": [], "entities": []}], "tableCaptions": []}