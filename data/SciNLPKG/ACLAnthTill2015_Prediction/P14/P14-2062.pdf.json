{"title": [{"text": "Experiments with crowdsourced re-annotation of a POS tagging data set", "labels": [], "entities": [{"text": "POS tagging data set", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.7801283001899719}]}], "abstractContent": [{"text": "Crowdsourcing lets us collect multiple annotations for an item from several annota-tors.", "labels": [], "entities": []}, {"text": "Typically, these are annotations for non-sequential classification tasks.", "labels": [], "entities": []}, {"text": "While there has been some work on crowdsourc-ing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 137, "end_pos": 165, "type": "TASK", "confidence": 0.6498243689537049}]}, {"text": "This paper shows that workers can actually annotate sequential data almost as well as experts.", "labels": [], "entities": []}, {"text": "Further , we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive.", "labels": [], "entities": []}, {"text": "showed, however, that crowdsourced annotations can produce similar results to annotations made by experts.", "labels": [], "entities": []}, {"text": "Crowdsourcing services such as Amazon's Mechanical Turk has since been successfully used for various annotation tasks in NLP (.", "labels": [], "entities": []}, {"text": "However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica.", "labels": [], "entities": [{"text": "document classification", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7858008146286011}]}, {"text": "A large part of NLP problems, however, are structured prediction tasks.", "labels": [], "entities": []}, {"text": "Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.779407262802124}]}, {"text": "Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder.", "labels": [], "entities": []}, {"text": "Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.6382715106010437}]}, {"text": "Results for this are good.", "labels": [], "entities": []}, {"text": "However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable.", "labels": [], "entities": [{"text": "LOC", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.8846489787101746}, {"text": "ORG", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.8955976366996765}, {"text": "PER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9844986796379089}]}, {"text": "The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.6449774980545044}]}, {"text": "In this paper, we investigate how well lay annotators can produce POS labels for Twitter data.", "labels": [], "entities": []}, {"text": "In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side).", "labels": [], "entities": []}, {"text": "Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power, and retraining models on new samples of more representative data becomes necessary.", "labels": [], "entities": []}, {"text": "Expensive professional annotation maybe prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter.", "labels": [], "entities": []}, {"text": "We use a minimum of instructions and require few qualifications.", "labels": [], "entities": []}, {"text": "Obviously, lay annotation is generally less reliable than professional annotation.", "labels": [], "entities": [{"text": "lay annotation", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.6679594218730927}]}, {"text": "It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations.", "labels": [], "entities": []}, {"text": "In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE ().", "labels": [], "entities": []}, {"text": "We also show how we can use Wiktionary, a crowdsourced lexicon, to filter crowdsourced annotations.", "labels": [], "entities": []}, {"text": "We evaluate the annotations in several ways: (a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on the annotations across several existing data sets, as well as (c) by applying our models in downstream tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9984533786773682}]}, {"text": "We show that with minimal context and annotation effort, we can produce structured annotations of near-expert quality.", "labels": [], "entities": []}, {"text": "We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexicons ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.8505104184150696}]}, {"text": "Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each of the two aggregation schemes above produces a final label sequenc\u00ea y for our training corpus.", "labels": [], "entities": []}, {"text": "We evaluate the resulting annotated data in three ways.", "labels": [], "entities": []}, {"text": "1. We compar\u00ea y to the available expert annotation on the training data.", "labels": [], "entities": []}, {"text": "This tells us how similar lay annotation is to professional annotation.", "labels": [], "entities": []}, {"text": "2. Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data.", "labels": [], "entities": []}, {"text": "To test this, we train a CRF model () with simple orthographic features and word clusters ( on the annotated Twitter data described in.", "labels": [], "entities": []}, {"text": "Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in), the test set from, and the data set described in.", "labels": [], "entities": [{"text": "RITTER", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9971680045127869}]}, {"text": "We will make the preprocessed data sets available to the public to facilitate comparison.", "labels": [], "entities": []}, {"text": "In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system ( re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9588086009025574}]}, {"text": "We use parameter settings from, as well as their Wikipedia dump, available from their project website.", "labels": [], "entities": []}, {"text": "3. POS tagging is often the first step for further analysis, such as chunking, parsing, etc.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.7302252948284149}, {"text": "chunking", "start_pos": 69, "end_pos": 77, "type": "TASK", "confidence": 0.9648303985595703}, {"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.7987127304077148}]}, {"text": "We test the downstream performance of the POS models from the previous step on chunking and NER.", "labels": [], "entities": []}, {"text": "We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model.", "labels": [], "entities": []}, {"text": "For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets.", "labels": [], "entities": []}, {"text": "For chunking, we use the test sets from Foster et al.  aggregated using MV agree with the expert annotations in 79.54% of the cases.", "labels": [], "entities": [{"text": "chunking", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9845466017723083}]}, {"text": "If we pre-filter the data using Wiktionary, the agreement becomes 80.58%.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.9166066646575928}]}, {"text": "MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75).", "labels": [], "entities": [{"text": "MACE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.35023096203804016}, {"text": "agreement", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9829807281494141}]}, {"text": "The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required.", "labels": [], "entities": [{"text": "Bronze-level qualification", "start_pos": 104, "end_pos": 130, "type": "METRIC", "confidence": 0.7357291877269745}]}, {"text": "Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e. y / \u2208 Z i . The best possible result either of them could achieve (the oracle) would be matching all but the missing labels, an agreement of 89.63%.", "labels": [], "entities": []}, {"text": "Most of the cases where the correct label was not among the annotations belong to a small set of confusions.", "labels": [], "entities": []}, {"text": "The most frequent was mislabeling \":\" and \".", "labels": [], "entities": []}, {"text": ".\", both mapped to X.", "labels": [], "entities": []}, {"text": "Annotators mostly decided to label these tokens as punctuation (.).", "labels": [], "entities": []}, {"text": "They also predominantly labeled your, my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET.: POS tagging accuracies (%).", "labels": [], "entities": [{"text": "PRON", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9352133274078369}, {"text": "DET.", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9210589528083801}, {"text": "POS tagging accuracies", "start_pos": 149, "end_pos": 171, "type": "METRIC", "confidence": 0.8246168295542399}]}], "tableCaptions": [{"text": " Table 1: Accuracy (%) of different annotations wrt  gold data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986392855644226}]}, {"text": " Table 2: POS tagging accuracies (%).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.681673601269722}]}, {"text": " Table 3: Downstream accuracy for chunking (l)  and NER (r) of models using POS.", "labels": [], "entities": [{"text": "Downstream", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9785066246986389}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.943544864654541}, {"text": "NER (r)", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9610831141471863}]}]}