{"title": [], "abstractContent": [{"text": "We propose an online learning algorithm based on tensor-space models.", "labels": [], "entities": []}, {"text": "A tensor-space model represents data in a compact way, and via rank-1 approximation the weight tensor can be made highly struc-tured, resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models.", "labels": [], "entities": []}, {"text": "This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training.", "labels": [], "entities": []}, {"text": "We apply with the proposed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a ten-sor model performs well, and gives significantly better results than standard learning algorithms based on traditional vector-space models.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9231727719306946}]}], "introductionContent": [{"text": "Many NLP applications use models that try to incorporate a large number of linguistic features so that as much human knowledge of language can be brought to bear on the (prediction) task as possible.", "labels": [], "entities": []}, {"text": "This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably.", "labels": [], "entities": []}, {"text": "Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space.", "labels": [], "entities": []}, {"text": "Many learning algorithms applied to NLP problems, such as the Perceptron), MIRA (, PRO), RAMPION ( etc., are based on vector-space models.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9128572940826416}, {"text": "RAMPION", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.937888503074646}]}, {"text": "Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set.", "labels": [], "entities": []}, {"text": "When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight.", "labels": [], "entities": []}, {"text": "In this paper, we shift the model from vectorspace to tensor-space.", "labels": [], "entities": []}, {"text": "Data can be represented in a compact and structured way using tensors as containers.", "labels": [], "entities": []}, {"text": "Tensor representations have been applied to computer vision problems () and information retrieval) along time ago.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7861177027225494}]}, {"text": "More recently, it has also been applied to parsing and semantic analysis.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.974852442741394}, {"text": "semantic analysis", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8304835259914398}]}, {"text": "A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors.", "labels": [], "entities": []}, {"text": "This low-rank approximation imposes structural constraints on the feature weights and can be regarded as a form of regularization.", "labels": [], "entities": []}, {"text": "With this representation, we no longer need to estimate individual feature weights directly but only a small number of \"bases\" instead.", "labels": [], "entities": []}, {"text": "This property makes the the tensor model very effective when training a large number of feature weights in a low-resource environment.", "labels": [], "entities": []}, {"text": "On the other hand, tensor models have many more degrees of \"design freedom\" than vector space models.", "labels": [], "entities": []}, {"text": "While this makes them very flexible, it also creates much difficulty in designing an optimal tensor structure fora given training set.", "labels": [], "entities": []}, {"text": "We give detailed description of the tensor space 666 model in Section 2.", "labels": [], "entities": []}, {"text": "Several issues that come with the tensor model construction are addressed in Section 3.", "labels": [], "entities": [{"text": "tensor model construction", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6404977440834045}]}, {"text": "A tensor weight learning algorithm is then proposed in 4.", "labels": [], "entities": [{"text": "tensor weight learning", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.7137763698895773}]}, {"text": "Finally we give our experimental results on a parsing task and analysis in Section 5.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.9055706560611725}]}], "datasetContent": [{"text": "In this section we shows empirical results of the training algorithm on a parsing task.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.9181746542453766}]}, {"text": "We used the Charniak parser) for our experiment, and we used the proposed algorithm to train the reranking feature weights.", "labels": [], "entities": []}, {"text": "For comparison, we also investigated training the reranker with Perceptron and MIRA.", "labels": [], "entities": [{"text": "Perceptron", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.6993240714073181}, {"text": "MIRA", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9513609409332275}]}, {"text": "To simulate a low-resource training environment, our training sets were selected from sections 2-9 of the Penn WSJ treebank, section 24 was used as the held-out set and section 23 as the evaluation set.", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 106, "end_pos": 123, "type": "DATASET", "confidence": 0.941627562046051}]}, {"text": "We applied the default settings of the parser.", "labels": [], "entities": []}, {"text": "There are around V = 1.33 million features in all defined for reranking, and the n-best size for reranking is set to 50.", "labels": [], "entities": []}, {"text": "We selected the parse with the highest f -score from the 50-best list as the oracle.", "labels": [], "entities": []}, {"text": "We would like to observe from the experiments how the amount of training data as well as different settings of the tensor degrees of freedom affects the algorithm performance.", "labels": [], "entities": []}, {"text": "Therefore we tried all combinations of the following experimental parameters:", "labels": [], "entities": []}], "tableCaptions": []}