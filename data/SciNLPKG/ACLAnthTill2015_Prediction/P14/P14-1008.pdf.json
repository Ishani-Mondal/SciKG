{"title": [{"text": "Logical Inference on Dependency-based Compositional Semantics", "labels": [], "entities": [{"text": "Logical Inference", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8181479871273041}]}], "abstractContent": [{"text": "Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics.", "labels": [], "entities": [{"text": "Dependency-based Compositional Semantics (DCS)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7805348833401998}]}, {"text": "In this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS.", "labels": [], "entities": []}, {"text": "An inference engine is built to achieve inference on abstract denota-tions.", "labels": [], "entities": []}, {"text": "Furthermore, we propose away to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation.", "labels": [], "entities": [{"text": "tree transformation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7730591893196106}]}, {"text": "Experiments on FraCaS and PASCAL RTE datasets show promising results.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9747392535209656}, {"text": "PASCAL RTE datasets", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.8796207904815674}]}], "introductionContent": [{"text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees.", "labels": [], "entities": [{"text": "Dependency-based Compositional Semantics (DCS)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7777562439441681}]}, {"text": "It is expressive enough to represent complex natural language queries on a relational database, yet simple enough to be latently learned from question-answer pairs.", "labels": [], "entities": []}, {"text": "In this paper, we equip DCS with logical inference, which, in one point of view, is \"the best way of testing an NLP system's semantic capacity\".", "labels": [], "entities": []}, {"text": "It should be noted that, however, a framework primarily designed for question answering is not readily suited for logical inference.", "labels": [], "entities": [{"text": "question answering", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8550536036491394}]}, {"text": "Because, answers returned by a query depend on the specific database, but implication is independent of any databases.", "labels": [], "entities": []}, {"text": "For example, answers to the question \"What books are read by students?\", should always be a subset of answers to \"What books are ever read by anyone?\", no matter how we store the data of students and how many records of books are therein our database.", "labels": [], "entities": []}, {"text": "Thus, our first step is to fix a notation which abstracts the calculation process of DCS trees, so as to clarify its meaning without the aid of any existing database.", "labels": [], "entities": []}, {"text": "The idea is to borrow a minimal set of operators from relational algebra, which is already able to formulate the calculation in DCS and define abstract denotation, which is an abstraction of the computation of denotations guided by DCS trees.", "labels": [], "entities": []}, {"text": "Meanings of sentences then can be represented by primary relations among abstract denotations.", "labels": [], "entities": []}, {"text": "This formulation keeps the simpleness and computability of DCS trees mostly unaffected; for example, our semantic calculation for DCS trees is parallel to the denotation computation in original DCS.", "labels": [], "entities": []}, {"text": "An inference engine is built to handle inference on abstract denotations.", "labels": [], "entities": []}, {"text": "Moreover, to compensate the lack of background knowledge in practical inference, we combine our framework with the idea of tree transformation (, to propose away of generating knowledge in logical representation from entailment rules, which are by now typically considered as syntactic rewriting rules.", "labels": [], "entities": [{"text": "tree transformation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7888524532318115}]}, {"text": "We test our system on FraCaS ( and PASCAL RTE datasets ().", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.9437535405158997}, {"text": "PASCAL RTE datasets", "start_pos": 35, "end_pos": 54, "type": "DATASET", "confidence": 0.8116197387377421}]}, {"text": "The experiments show: (i) a competitive performance on FraCaS dataset; (ii) a big impact of our automatically generated on-the-fly knowledge in achieving high recall fora logicbased RTE system; and (iii) a result that outperforms state-of-the-art RTE system on RTE5 data.", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9851669371128082}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9905017614364624}, {"text": "RTE5 data", "start_pos": 261, "end_pos": 270, "type": "DATASET", "confidence": 0.7648693025112152}]}, {"text": "Our whole system is publicly released and can be downloaded from http://kmcs.nii.ac. jp/tianran/tifmo/.: Databases of student, book, and read 2.1 DCS trees DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees ().", "labels": [], "entities": []}, {"text": "For the sentence \"students read books\", imagine a database consists of three tables, namely, a set of students, a set of books, and a set of \"reading\" events (.", "labels": [], "entities": []}, {"text": "The DCS tree in is interpreted as a command for querying these tables, obtaining \"reading\" entries whose \"SUBJ\" field is student and whose \"OBJ\" field is book.", "labels": [], "entities": []}, {"text": "The result is a set {John reads Ulysses, . .", "labels": [], "entities": [{"text": "John reads Ulysses", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.6681293050448099}]}, {"text": ".}, which is called a denotation.", "labels": [], "entities": []}, {"text": "DCS trees can be extended to represent linguistic phenomena such as quantification and coreference, with additional markers introducing additional operations on tables.", "labels": [], "entities": [{"text": "coreference", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.936553418636322}]}, {"text": "shows an example with a quantifier \"every\", which is marked as \"\u2282\" on the edge (love)OBJ-ARG(dog) and interpreted as a division operator q OBJ \u2282 ( \u00a72.2).", "labels": [], "entities": [{"text": "OBJ-ARG", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.7001746296882629}]}, {"text": "Optimistically, we believe DCS can provide a framework of semantic representation with sufficiently wide coverage for real-world texts.", "labels": [], "entities": []}, {"text": "The strict semantics of DCS trees brings us the idea of applying DCS to logical inference.", "labels": [], "entities": []}, {"text": "This is not trivial, however, because DCS works under the assumption that databases are explicitly available.", "labels": [], "entities": []}, {"text": "Obviously this is unrealistic for logical inference on unrestricted texts, because we cannot prepare a database for everything in the world.", "labels": [], "entities": []}, {"text": "This fact fairly restricts the applicable tasks of DCS.", "labels": [], "entities": [{"text": "DCS", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8901623487472534}]}, {"text": "Our solution is to redefine DCS trees without the aid of any databases, by considering each node of a DCS tree as a content word in a sentence (but may no longer be a table in a specific database), while each edge represents semantic relations between two words.", "labels": [], "entities": []}, {"text": "The labels on both ends of an edge, such as SUBJ (subject) and OBJ (object), are considered as semantic roles of the cor- responding words 1 . To formulate the database querying process defined by a DCS tree, we provide formal semantics to DCS trees by employing relational algebra) for representing the query.", "labels": [], "entities": [{"text": "OBJ", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9623687863349915}]}, {"text": "As described below, we represent meanings of sentences with abstract denotations, and logical relations among sentences are computed as relations among their abstract denotations.", "labels": [], "entities": []}, {"text": "In this way, we can perform inference over formulas of relational algebra, without computing database entries explicitly.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our system on FraCaS ( \u00a74.2) and PASCAL RTE datasets ( \u00a74.3).", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.9669268727302551}, {"text": "PASCAL RTE datasets", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.7617354790369669}]}, {"text": "The FraCaS test suite contains 346 inference problems divided into 9 sections, each focused on a category of semantic phenomena.", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.944459080696106}]}, {"text": "We use the data by, and experiment on the first section, Quantifiers, following: Accuracy (%) on FraCaS the problems do not require lexical knowledge, so we use our primary textual inference system without on-the-fly knowledge nor WordNet, to test the performance of the DCS framework as formal semantics.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9891007542610168}, {"text": "FraCaS", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.9741887450218201}, {"text": "WordNet", "start_pos": 231, "end_pos": 238, "type": "DATASET", "confidence": 0.9360204935073853}]}, {"text": "To obtain the three-valued output (i.e. yes, no, and unknown), we output \"yes\" if H is proven, or try to prove the negation of H if H is not proven.", "labels": [], "entities": []}, {"text": "To negate H, we use the root negation as described in \u00a72.5.", "labels": [], "entities": [{"text": "negate H", "start_pos": 3, "end_pos": 11, "type": "TASK", "confidence": 0.8059495389461517}]}, {"text": "If the negation of H is proven, we output \"no\", otherwise we output \"unknown\".", "labels": [], "entities": []}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "Since our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences.", "labels": [], "entities": []}, {"text": "Still, our system outperforms's probabilistic CCG-parser.", "labels": [], "entities": []}, {"text": "Compared to MacCartney and and, our system does not need a pretrained alignment model, and it improves by making multi-sentence inferences.", "labels": [], "entities": []}, {"text": "To sum up, the result shows that DCS is good at handling universal quantifiers and negations.", "labels": [], "entities": []}, {"text": "Most errors are due to wrongly generated DCS trees (e.g. wrongly assigned semantic roles) or unimplemented quantifier triggers (e.g. \"neither\") or generalized quantifiers (e.g. \"at least a few\").", "labels": [], "entities": []}, {"text": "These could be addressed by future work.", "labels": [], "entities": []}, {"text": "On PASCAL RTE datasets, strict logical inference is known to have very low recall (), so on-the-fly knowledge is crucial in this setting.", "labels": [], "entities": [{"text": "PASCAL RTE datasets", "start_pos": 3, "end_pos": 22, "type": "DATASET", "confidence": 0.6619017620881399}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9990808963775635}]}, {"text": "We test the effect of on-the-fly knowledge on RTE2, RTE3, RTE4 and RTE5 datasets, and compare our system with other approaches.", "labels": [], "entities": [{"text": "RTE2", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8455138802528381}, {"text": "RTE3", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.7547460198402405}, {"text": "RTE4", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.7471911311149597}, {"text": "RTE5 datasets", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.8723726570606232}]}], "tableCaptions": [{"text": " Table 5: Impact of on-the-fly knowledge", "labels": [], "entities": []}]}