{"title": [{"text": "A Recursive Recurrent Neural Network for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8686451315879822}]}], "abstractContent": [{"text": "In this paper, we propose a novel recursive recurrent neural network (R 2 NN) to model the end-to-end decoding process for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 123, "end_pos": 154, "type": "TASK", "confidence": 0.7684064507484436}]}, {"text": "R 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neu-ral networks, so that language model and translation model can be integrated naturally ; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner.", "labels": [], "entities": []}, {"text": "A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly.", "labels": [], "entities": []}, {"text": "Experiments on a Chinese to En-glish translation task show that our proposed R 2 NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU.", "labels": [], "entities": [{"text": "Chinese to En-glish translation task", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.5760233402252197}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9828813076019287}]}], "introductionContent": [{"text": "Deep Neural Network (DNN), which essentially is a multi-layer neural network, has re-gained more and more attentions these years.", "labels": [], "entities": []}, {"text": "With the efficient training methods, such as (), DNN is widely applied to speech and image processing, and has achieved breakthrough results (.", "labels": [], "entities": []}, {"text": "Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first.", "labels": [], "entities": []}, {"text": "Word embedding is a dense, low dimensional, real-valued vector.", "labels": [], "entities": []}, {"text": "Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic properties ().", "labels": [], "entities": []}, {"text": "Word embedding is usually learnt from large amount of monolingual corpus at first, and then fine tuned for special distinct tasks.", "labels": [], "entities": [{"text": "Word embedding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6737364232540131}]}, {"text": "propose a multi-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labelling.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.6929477453231812}, {"text": "named entity recognition", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.6124411920706431}, {"text": "semantic role labelling", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.6454974313577017}]}, {"text": "Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily longtime ().", "labels": [], "entities": []}, {"text": "Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics ().", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.7161103884379069}]}, {"text": "DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.8529532849788666}, {"text": "word alignment", "start_pos": 141, "end_pos": 155, "type": "TASK", "confidence": 0.7914522886276245}, {"text": "language modelling", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7585553228855133}, {"text": "translation modelling", "start_pos": 177, "end_pos": 198, "type": "TASK", "confidence": 0.9880721271038055}, {"text": "distortion modelling", "start_pos": 203, "end_pos": 223, "type": "TASK", "confidence": 0.6985464841127396}]}, {"text": "adapt and extend the CD-DNN-HMM () method to HMM-based word alignment model.", "labels": [], "entities": [{"text": "HMM-based word alignment", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6563613216082255}]}, {"text": "In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information.", "labels": [], "entities": []}, {"text": "propose a joint language and translation model, based on a recurrent neural network.", "labels": [], "entities": []}, {"text": "Their model predicts a target word, with an unbounded history of both source and target words.", "labels": [], "entities": []}, {"text": "propose an additive neural network for SMT decoding.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9574927091598511}]}, {"text": "Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model.", "labels": [], "entities": [{"text": "translation confidence score", "start_pos": 45, "end_pos": 73, "type": "METRIC", "confidence": 0.7527690728505453}]}, {"text": "For distortion modeling,  use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier).", "labels": [], "entities": [{"text": "distortion modeling", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8434044122695923}]}, {"text": "Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R 2 NN to model the end-to-end decoding process.", "labels": [], "entities": [{"text": "SMT framework", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.9179600775241852}]}, {"text": "R 2 NN is a combination of recursive neural network and recurrent neural network.", "labels": [], "entities": []}, {"text": "In R 2 NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure can be built, as recursive neural networks.", "labels": [], "entities": []}, {"text": "To generate the translation candidates in a commonly used bottom-up manner, recursive neural networks are naturally adopted to build the tree structure.", "labels": [], "entities": []}, {"text": "In recursive neural networks, all the representations of nodes are generated based on their child nodes, and it is difficult to integrate additional global information, such as language model and distortion model.", "labels": [], "entities": []}, {"text": "In order to integrate these crucial information for better translation prediction, we combine recurrent neural networks into the recursive neural networks, so that we can use global information to generate the next hidden state, and select the better translation candidate.", "labels": [], "entities": [{"text": "translation prediction", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9819447100162506}]}, {"text": "We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy.", "labels": [], "entities": []}, {"text": "So as to model the translation confidence fora translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network.", "labels": [], "entities": []}, {"text": "The sparse features are phrase pairs in translation table, and recurrent neural network is utilized to learn a smoothed translation score with the source and target side information.", "labels": [], "entities": []}, {"text": "We conduct experiments on a Chinese-to-English translation task to test our proposed methods, and we get about 1.5 BLEU points improvement, compared with a state-of-the-art baseline system.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.7155534525712332}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9996126294136047}]}, {"text": "The rest of this paper is organized as follows: Section 2 introduces related work on applying DNN to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9676016569137573}]}, {"text": "Our R 2 NN framework is introduced in detail in Section 3, followed by our three-step semi-supervised training approach in Section 4.", "labels": [], "entities": []}, {"text": "Phrase pair embedding method using translation confidence is elaborated in Section 5.", "labels": [], "entities": []}, {"text": "We introduce our conducted experiments in Section 6, and conclude our work in Section 7.", "labels": [], "entities": []}, {"text": "adapt and extend CD-DNN-HMM () to word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7905233800411224}]}, {"text": "In their work, initial word embedding is firstly trained with a huge mono-lingual corpus, then the word embedding is adapted and fine tuned bilingually in a context-depended DNN HMM framework.", "labels": [], "entities": []}, {"text": "Word embeddings capturing lexical translation information and surrounding words modeling context information are leveraged to improve the word alignment performance.", "labels": [], "entities": [{"text": "Word embeddings capturing lexical translation information", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.6123905032873154}, {"text": "word alignment", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.7492839992046356}]}, {"text": "Unfortunately, the better word alignment result generated by this model, cannot bring significant performance improvement on a end-to-end SMT evaluation task.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7465583682060242}, {"text": "SMT evaluation task", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.9446902672449747}]}], "datasetContent": [{"text": "In this section, we conduct experiments to test our method on a Chinese-to-English translation task.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 64, "end_pos": 99, "type": "TASK", "confidence": 0.7555350462595621}]}, {"text": "The evaluation method is the case insensitive IB-M BLEU-4 ().", "labels": [], "entities": [{"text": "IB-M", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.587500810623169}, {"text": "BLEU-4", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.7769562602043152}]}, {"text": "Significant testing is carried out using bootstrap re-sampling method proposed by) with a 95% confidence level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. We can see that, our R 2 NN models  with WEPPE and TCBPPE are both better than the  baseline system. WEPPE cannot get significan- t improvement, while TCBPPE does, compared  with the baseline result. TCBPPE is much better  than WEPPE.", "labels": [], "entities": [{"text": "TCBPPE", "start_pos": 210, "end_pos": 216, "type": "DATASET", "confidence": 0.7826142311096191}]}, {"text": " Table 2: Translation results of our proposed R 2 NN  Model with two phrase embedding methods, com- pared with the baseline. Setting \"WEPPE+R 2 NN\"  is the result with word embedding based phrase  pair embedding and our R 2 NN Model, and  \"TCBPPE+R 2 NN\" is the result of translation con- fidence based phrase pair embedding and our  R 2 NN Model. The results with \u2191 are significantly  better than the baseline.", "labels": [], "entities": []}, {"text": " Table 3. Without the recurrent input  vectors, R 2 NN degenerates into recursive neural  network (RNN).", "labels": [], "entities": []}, {"text": " Table 3: Experimental results to test the effects of  recurrent input vector. WEPPE /TCBPPE+RNN  are the results removing recurrent input vectors  with WEPPE /TCBPPE.", "labels": [], "entities": [{"text": "WEPPE", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.6518117785453796}, {"text": "RNN", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.885402262210846}]}, {"text": " Table 4: Experimental results to test the effects of  sparse features and recurrent network features.", "labels": [], "entities": []}]}