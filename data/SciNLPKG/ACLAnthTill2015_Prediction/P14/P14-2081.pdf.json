{"title": [{"text": "Two-Stage Hashing for Fast Document Retrieval", "labels": [], "entities": [{"text": "Fast Document Retrieval", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6376882394154867}]}], "abstractContent": [{"text": "This work fulfills sublinear time Nearest Neighbor Search (NNS) in massive-scale document collections.", "labels": [], "entities": [{"text": "sublinear time Nearest Neighbor Search (NNS)", "start_pos": 19, "end_pos": 63, "type": "TASK", "confidence": 0.699631966650486}]}, {"text": "The primary contribution is to propose a two-stage unsupervised hashing framework which harmoniously integrates two state-of-the-art hashing algorithms Locality Sensitive Hashing (LSH) and Iterative Quantization (ITQ).", "labels": [], "entities": []}, {"text": "LSH accounts for neighbor candidate pruning, while ITQ provides an efficient and effective reranking over the neighbor pool captured by LSH.", "labels": [], "entities": [{"text": "LSH", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.890771210193634}]}, {"text": "Furthermore , the proposed hashing framework capitalizes on both term and topic similarity among documents, leading to precise document retrieval.", "labels": [], "entities": []}, {"text": "The experimental results convincingly show that our hashing based document retrieval approach well approximates the conventional Information Retrieval (IR) method in terms of retrieving semantically similar documents, and meanwhile achieves a speedup of over one order of magnitude in query time.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.781225836277008}]}], "introductionContent": [{"text": "A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset.", "labels": [], "entities": [{"text": "Nearest Neighbor Search (NNS) task", "start_pos": 2, "end_pos": 36, "type": "TASK", "confidence": 0.7083303758076259}]}, {"text": "NNS is essential in dealing with many search related tasks, and also fundamental to abroad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction, document translation pair acquisition (), large-scale similar noun list generation (), lexical variants mining, and large-scale first story detection (.", "labels": [], "entities": [{"text": "person name spelling correction", "start_pos": 165, "end_pos": 196, "type": "TASK", "confidence": 0.8060235530138016}, {"text": "document translation pair acquisition", "start_pos": 198, "end_pos": 235, "type": "TASK", "confidence": 0.7400076389312744}, {"text": "noun list generation", "start_pos": 260, "end_pos": 280, "type": "TASK", "confidence": 0.6277715265750885}, {"text": "lexical variants mining", "start_pos": 285, "end_pos": 308, "type": "TASK", "confidence": 0.6581106781959534}, {"text": "first story detection", "start_pos": 326, "end_pos": 347, "type": "TASK", "confidence": 0.624537855386734}]}, {"text": "Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection) and recognition (), large-scale image retrieval, and large-scale machine learning (.", "labels": [], "entities": [{"text": "tackling fast NNS", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8501944343249003}, {"text": "visual object detection) and recognition", "start_pos": 147, "end_pos": 187, "type": "TASK", "confidence": 0.7200995484987894}, {"text": "image retrieval", "start_pos": 204, "end_pos": 219, "type": "TASK", "confidence": 0.7003188580274582}]}, {"text": "However, hashing has received limited attention in the NLP field to the date.", "labels": [], "entities": [{"text": "hashing", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.9833200573921204}]}, {"text": "The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of \"0\" or \"1\").", "labels": [], "entities": []}, {"text": "When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory.", "labels": [], "entities": []}, {"text": "for example, one can store 250 million documents with 1.9G memory using only 64 bits for each document while a large news corpus such as the English Gigaword fifth edition 1 stores 10 million documents in a 26G hard drive; 2) the time efficiency of manipulating binary codes, for example, computing the hamming distance between a pair of binary codes is several orders of magnitude faster than computing the real-valued cosine similarity over a pair of document vectors.", "labels": [], "entities": [{"text": "English Gigaword fifth edition 1", "start_pos": 141, "end_pos": 173, "type": "DATASET", "confidence": 0.845256507396698}]}, {"text": "The early explorations of hashing focused on using random permutations or projections to construct randomized hash functions, e.g., the wellknown Min-wise Hashing (MinHash)) and Locality Sensitive Hashing (LSH).", "labels": [], "entities": []}, {"text": "In contrast to such data-independent hashing schemes, recent research has been geared to studying data-dependent hashing through learning compact hash codes from a training dataset.", "labels": [], "entities": []}, {"text": "The state-of-the-art unsupervised learning-based hashing methods include Spectral Hashing (SH) (), Anchor Graph Hashing (AGH) (, and Iterative Quantization (ITQ) (), all of which endeavor to make the learned hash codes preserve or reveal some intrinsic structure, such as local neighborhood structure, lowdimensional manifolds, or the closest hypercube, underlying the training data.", "labels": [], "entities": []}, {"text": "Despite achieving data-dependent hash codes, most of these \"learning to hash\" methods cannot guarantee a high success rate of looking a query code up in a hash table (referred to as hash table lookup in literature), which is critical to the high efficacy of exploiting hashing in practical uses.", "labels": [], "entities": []}, {"text": "It is worth noting that we choose to use ITQ in the proposed twostage hashing framework for its simplicity and efficiency.", "labels": [], "entities": []}, {"text": "ITQ has been found to work better than SH by, and be more efficient than AGH in terms of training time by.", "labels": [], "entities": [{"text": "ITQ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.786390483379364}, {"text": "AGH", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.49947965145111084}]}, {"text": "To this end, in this paper we propose a novel two-stage unsupervised hashing framework to simultaneously enhance the hash lookup success rate and increase the search accuracy by integrating the advantages of both LSH and ITQ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9919044375419617}]}, {"text": "Furthermore, we make the hashing framework applicable to combine different similarity measures in NNS.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments, we use the English portion of the standard TDT-5 dataset, which consists of 278, 109 documents from a time spanning April 2003 to September 2003.", "labels": [], "entities": [{"text": "TDT-5 dataset", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9790037274360657}]}, {"text": "126 topics are annotated with an average of 51 documents per topic, and other unlabeled documents are irrelevant to them.", "labels": [], "entities": []}, {"text": "We select six largest topics for the top-K NNS evaluation, with each including more than 250 documents.", "labels": [], "entities": []}, {"text": "We randomly select 60 documents from each of the six topics for testing.", "labels": [], "entities": []}, {"text": "The six topics are (1).", "labels": [], "entities": []}, {"text": "Bombing in Riyadh, Saudi Arabia (2).", "labels": [], "entities": []}, {"text": "Mad cow disease in North America (3).", "labels": [], "entities": [{"text": "Mad cow disease", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5177056590716044}]}, {"text": "Casablanca bombs (4).", "labels": [], "entities": [{"text": "Casablanca bombs", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9570633769035339}]}, {"text": "Swedish Foreign Minister killed (5).", "labels": [], "entities": [{"text": "Swedish Foreign Minister", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8103010654449463}]}, {"text": "Liberian former president arrives in exile and (6).", "labels": [], "entities": []}, {"text": "UN official killed in attack.", "labels": [], "entities": [{"text": "UN", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8255555629730225}]}, {"text": "For each document, we apply the Stanford Tokenizer 2 for tokenization; remove stopwords based on the stop list from InQuery (, and apply Porter Stemmer for stemming.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9682013392448425}]}, {"text": "If one retrieved document shares the same topic label with the query document, they are true neighbors.", "labels": [], "entities": []}, {"text": "We evaluate the precision of the top-K candidate documents returned by each method and calculate the average precision across all queries.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.999000608921051}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.987443745136261}]}], "tableCaptions": []}