{"title": [], "abstractContent": [{"text": "The AIDA-YAGO dataset is a popular target for whole-document entity recognition and disambiguation, despite lacking a shared evaluation tool.", "labels": [], "entities": [{"text": "AIDA-YAGO dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9140936434268951}, {"text": "whole-document entity recognition", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.624419371287028}]}, {"text": "We review evaluation regimens in the literature while comparing the output of three approaches, and identify research opportunities.", "labels": [], "entities": []}, {"text": "This utilises our open, accessible evaluation tool.", "labels": [], "entities": []}, {"text": "We exemplify anew paradigm of distributed, shared evaluation, in which evaluation software and standardised, ver-sioned system outputs are provided online.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern entity annotation systems detect mentions in text and disambiguate them to a knowledge base (KB).", "labels": [], "entities": []}, {"text": "Disambiguation typically returns the corresponding Wikipedia page or NIL if none exists.", "labels": [], "entities": [{"text": "Disambiguation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9352995157241821}]}, {"text": "Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (.", "labels": [], "entities": [{"text": "Named entity linking (NEL)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7591795921325684}]}, {"text": "Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation.", "labels": [], "entities": [{"text": "whole-document named entity recognition", "start_pos": 116, "end_pos": 155, "type": "TASK", "confidence": 0.5990756973624229}]}, {"text": "Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation.", "labels": [], "entities": [{"text": "KB-driven mention spotting", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.6564640601476034}]}, {"text": "Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory.", "labels": [], "entities": [{"text": "NEL", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9054795503616333}]}, {"text": "Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems.", "labels": [], "entities": []}, {"text": "Public data sets have also driven research in whole-document entity disambiguation).", "labels": [], "entities": [{"text": "whole-document entity disambiguation", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.6155090828736623}]}, {"text": "However, with many task variants and evaluation methodologies proposed, it is very difficult to synthesise a clear picture of the state of the art.", "labels": [], "entities": []}, {"text": "We present an evaluation suite for named entity linking, leveraging and advocating for the AIDA disambiguation annotations over the large and widely used CoNLL NER data.", "labels": [], "entities": [{"text": "CoNLL NER data", "start_pos": 154, "end_pos": 168, "type": "DATASET", "confidence": 0.9397836526234945}]}, {"text": "This builds on recent rationalisation and benchmarking work (, adding an isolated evaluation of disambiguation.", "labels": [], "entities": []}, {"text": "Contributions include: \u2022 a simple, open-source evaluation suite for end-to-end, whole-document NEL; \u2022 disambiguation evaluation facilitated by gold-standard mentions; \u2022 reference outputs from state-of-the-art and wikification systems published with the suite for easy comparison; \u2022 implementation of statistical significance and error sub-type analysis, which are often lacking in entity linking evaluation; \u2022 avenue for publishing benchmark results continuously, complementing the annual cycle of shared tasks; \u2022 a repository for versioned corrections to ground truth annotation.", "labels": [], "entities": [{"text": "statistical significance and error sub-type analysis", "start_pos": 300, "end_pos": 352, "type": "TASK", "confidence": 0.5367788175741831}]}, {"text": "We see this repository, at https://github.", "labels": [], "entities": []}, {"text": "com/wikilinks/conll03_nel_eval, as a model for the future of informal shared evaluation.", "labels": [], "entities": []}, {"text": "We survey entity annotation tasks and evaluation, proposing a core suite of metrics for end-toend linking and tagging, and settings that isolate mention detection and disambiguation.", "labels": [], "entities": [{"text": "end-toend linking and tagging", "start_pos": 88, "end_pos": 117, "type": "TASK", "confidence": 0.6106718853116035}, {"text": "mention detection", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.748026579618454}]}, {"text": "A comparison of state-of-the-art NEL and wikification systems illustrates how key differences in mention handling affect performance.", "labels": [], "entities": [{"text": "mention handling", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7036356627941132}]}, {"text": "Analysis suggests that focusing evaluation too tightly on subtasks like candidate ranking can lead to results that do not reflect end-to-end performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Finally, contains end-to-end entity annotation results.", "labels": [], "entities": []}, {"text": "Again, these results highlight key differences in mention handling between NEL and wikification.", "labels": [], "entities": []}, {"text": "Coreference modelling helps NEL detect and link ambiguous names (e.g., 'President Bush') that refer to the same entity as unambiguous names in the same text (e.g., 'George W. Bush').", "labels": [], "entities": [{"text": "NEL detect", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9003814160823822}]}, {"text": "And restricting the the universe to named entities is appropriate for the CoNLL-YAGO data.", "labels": [], "entities": [{"text": "CoNLL-YAGO data", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9485391974449158}]}, {"text": "The advantage is marked in the mention-level link evaluation (f L ).", "labels": [], "entities": [{"text": "mention-level link evaluation (f L )", "start_pos": 31, "end_pos": 67, "type": "METRIC", "confidence": 0.9265143956456866}]}, {"text": "However, the systems are statistically indistinguishable in the document-level tag evaluation (f T ).", "labels": [], "entities": [{"text": "document-level tag evaluation", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.5689089099566141}]}, {"text": "Thus the extra NER and coreference machinery may not be justified if the application is document indexing or social media mining (, wherein a KB-driven mention detector maybe favourable for other reasons.", "labels": [], "entities": [{"text": "NER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8568702340126038}, {"text": "document indexing", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.6609688401222229}]}, {"text": "evaluation without the linkable constraint is important, especially if the application requires detecting and disambiguating all mentions.", "labels": [], "entities": []}, {"text": "The comparison here highlights a notable evaluation intricacy.", "labels": [], "entities": []}, {"text": "The Schwa system disambiguates all gold mentions rather than those with KB links, and the document compatibility approach means that evidence from a NIL mention may offer confounding evidence when linking linkable mentions.", "labels": [], "entities": []}, {"text": "Further, although using the same mentions, systems use search resources with different recall characteristics, so the Schwa system may not retrieve the correct candidate to disambiguate.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9835582375526428}]}, {"text": "Several evaluations in the literature are beyond the scope of this paper but planned for future versions of the code.", "labels": [], "entities": []}, {"text": "This includes further diagnostic subtask evaluation, particularly candidate set recall (), NIL accuracy (Ji and Grishman, 2011) and weak mention matching ().", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9186130166053772}, {"text": "NIL", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.7048383355140686}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.571704089641571}]}, {"text": "With a score for each prediction, further metrics are possible: rank evaluation of tag annotation with r-precision, mean reciprocal rank and mean average precision (; and rank evaluation of mentions for comparison to and.", "labels": [], "entities": [{"text": "mean average precision", "start_pos": 141, "end_pos": 163, "type": "METRIC", "confidence": 0.7466558218002319}]}], "tableCaptions": [{"text": " Table 1: Mention detection results. Cucerzan re- sults as reported (Cucerzan, 2007).", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7284601330757141}]}, {"text": " Table 2: Disambiguation results for mention-level  linking and document-level tagging.", "labels": [], "entities": [{"text": "mention-level  linking", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6854978203773499}, {"text": "document-level tagging", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6649562418460846}]}, {"text": " Table 3: End-to-end results for mention-level link- ing and document-level tagging.", "labels": [], "entities": [{"text": "mention-level link- ing", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.5924485996365547}, {"text": "document-level tagging", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6800012439489365}]}, {"text": " Table 4: f L HOF and f L error profiles.", "labels": [], "entities": []}]}