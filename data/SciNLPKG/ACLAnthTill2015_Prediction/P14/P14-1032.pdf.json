{"title": [{"text": "Product Feature Mining: Semantic Clues versus Syntactic Constituents", "labels": [], "entities": [{"text": "Product Feature Mining", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6207451423009237}]}], "abstractContent": [{"text": "Product feature mining is a key subtask in fine-grained opinion mining.", "labels": [], "entities": [{"text": "Product feature mining", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5766027172406515}, {"text": "fine-grained opinion mining", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6549551586310068}]}, {"text": "Previous works often use syntax constituents in this task.", "labels": [], "entities": []}, {"text": "However, syntax-based methods can only use discrete contextual information , which may suffer from data sparsity.", "labels": [], "entities": []}, {"text": "This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues.", "labels": [], "entities": [{"text": "product feature mining", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.700664738814036}]}, {"text": "Lexical semantic clue verifies whether a candidate term is related to the target product, and contextual semantic clue serves as a soft pattern miner to find candidates, which exploits semantics of each word in context so as to alleviate the data sparsity problem.", "labels": [], "entities": []}, {"text": "We build a semantic similarity graph to encode lexical semantic clue, and employ a convolutional neural model to capture contextual semantic clue.", "labels": [], "entities": []}, {"text": "Then Label Propagation is applied to combine both semantic clues.", "labels": [], "entities": []}, {"text": "Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches, which not only mines product features more accurately, but also extracts more infrequent product features.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, opinion mining has helped customers a lotto make informed purchase decisions.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.8162451982498169}]}, {"text": "However, with the rapid growth of e-commerce, customers are no longer satisfied with the overall opinion ratings provided by traditional sentiment analysis systems.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.8954097032546997}]}, {"text": "The detailed functions or attributes of products, which are called product features, receive more attention.", "labels": [], "entities": []}, {"text": "Nevertheless, a product may have thousands of features, which makes it impractical fora customer to investigate them all.", "labels": [], "entities": []}, {"text": "Therefore, mining product features automatically from online reviews is shown to be a key step for opinion summarization () and fine-grained sentiment analysis.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7092535346746445}, {"text": "fine-grained sentiment analysis", "start_pos": 128, "end_pos": 159, "type": "TASK", "confidence": 0.6456882754961649}]}, {"text": "Previous works often mine product features via syntactic constituent matching (.", "labels": [], "entities": [{"text": "syntactic constituent matching", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.71980086962382}]}, {"text": "The basic idea is that reviewers tend to comment on product features in similar syntactic structures.", "labels": [], "entities": []}, {"text": "Therefore, it is natural to mine product features by using syntactic patterns.", "labels": [], "entities": []}, {"text": "For example, in, the upper box shows a dependency tree produced by Stanford Parser (de), and the lower box shows a common syntactic pattern from (, where <feature/NN> is a wildcard to befit in reviews and NN denotes the required POS tag of the wildcard.", "labels": [], "entities": []}, {"text": "Usually, the product name mp3 is specified, and when screen matches the wildcard, it is likely to be a product feature of mp3.", "labels": [], "entities": [{"text": "mp3", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9476879835128784}]}, {"text": "Figure 1: An example of syntax-based product feature mining procedure.", "labels": [], "entities": [{"text": "syntax-based product feature mining", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.6471173018217087}]}, {"text": "The word screen matches the wildcard <feature/NN>.", "labels": [], "entities": []}, {"text": "Therefore, screen is likely to be a product feature of mp3.", "labels": [], "entities": [{"text": "screen", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9157724976539612}, {"text": "mp3", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9559862017631531}]}, {"text": "Generally, such syntactic patterns extract product features well but they still have some limitations.", "labels": [], "entities": []}, {"text": "For example, the product-have-feature pattern may fail to find the fm tuner in a very similar casein Example 1(a), where the product is mentioned by using player instead of mp3.", "labels": [], "entities": []}, {"text": "Similarly, it may also fail on Example 1(b), just with have replaced by support.", "labels": [], "entities": [{"text": "Example 1", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.8937308490276337}]}, {"text": "In essence, syntactic pattern is a kind of one-hot representation for encoding the contexts, which can only use partial and discrete features, such as some key words (e.g., have) or shallow information (e.g., POS tags).", "labels": [], "entities": []}, {"text": "Therefore, such a representation often suffers from the data sparsity problem (.", "labels": [], "entities": []}, {"text": "One possible solution for this problem is using a more general pattern such as NP-VB-feature, where NP represents a noun or noun phrase and VB stands for any verb.", "labels": [], "entities": []}, {"text": "However, this pattern becomes too general that it may find many irrelevant cases such as the one in Example 1(c), which is not talking about the product.", "labels": [], "entities": [{"text": "Example 1", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.8357132971286774}]}, {"text": "Consequently, it is very difficult fora pattern designer to balance between precision and generalization.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9993808269500732}]}, {"text": "To solve the problems stated above, it is argued that deeper semantics of contexts shall be exploited.", "labels": [], "entities": []}, {"text": "For example, we can try to automatically discover that the verb have indicates a part-whole relation ( and support indicates a product-function relation, so that both sth.", "labels": [], "entities": []}, {"text": "support suggest that terms following them are product features, where sth.", "labels": [], "entities": []}, {"text": "can be replaced by any terms that refer to the target product (e.g., mp3, player, etc.).", "labels": [], "entities": []}, {"text": "This is called contextual semantic clue.", "labels": [], "entities": [{"text": "contextual semantic clue", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.669147272904714}]}, {"text": "Nevertheless, only using contexts is not sufficient enough.", "labels": [], "entities": []}, {"text": "As in Example 1(d), we can see that the word flaws follows mp3 have, but it is not a product feature.", "labels": [], "entities": []}, {"text": "Thus, a noise term maybe extracted even with high contextual support.", "labels": [], "entities": []}, {"text": "Therefore, we shall also verify whether a candidate is really related to the target product.", "labels": [], "entities": []}, {"text": "We call it lexical semantic clue.", "labels": [], "entities": [{"text": "lexical semantic clue", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6627851724624634}]}, {"text": "This paper proposes a novel bootstrapping approach for product feature mining, which leverages both semantic clues discussed above.", "labels": [], "entities": [{"text": "product feature mining", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.6557722687721252}]}, {"text": "Firstly, some reliable product feature seeds are automatically extracted.", "labels": [], "entities": []}, {"text": "Then, based on the assumption that terms that are more semantically similar to the seeds are more likely to be product features, a graph which measures semantic similarities between terms is built to capture lexical semantic clue.", "labels": [], "entities": []}, {"text": "At the same time, a semi-supervised convolutional neural model) is employed to encode contextual semantic clue.", "labels": [], "entities": []}, {"text": "Finally, the two kinds of semantic clues are combined by a Label Propagation algorithm.", "labels": [], "entities": []}, {"text": "In the proposed method, words are represented by continuous vectors, which capture latent semantic factors of the words ().", "labels": [], "entities": []}, {"text": "The vectors can be unsupervisedly trained on large scale corpora, and words with similar semantics will have similar vectors.", "labels": [], "entities": []}, {"text": "This enables our method to be less sensitive to lexicon change, so that the data sparsity problem can be alleviated . The contributions of this paper include: \u2022 It uses semantics of words to encode contextual clues, which exploits deeper level information than syntactic constituents.", "labels": [], "entities": []}, {"text": "As a result, it mines product features more accurately than syntaxbased methods.", "labels": [], "entities": []}, {"text": "\u2022 It exploits semantic similarity between words to capture lexical clues, which is shown to be more effective than co-occurrence relation between words and syntactic patterns.", "labels": [], "entities": []}, {"text": "In addition, experiments show that the semantic similarity has the advantage of mining infrequent product features, which is crucial for this task.", "labels": [], "entities": []}, {"text": "For example, one may say \"This hotel has low water pressure\", where low water pressure is seldom mentioned, but fatal to someone's taste.", "labels": [], "entities": []}, {"text": "\u2022 We compare the proposed semantics-based approach with three state-of-the-art syntax-based methods.", "labels": [], "entities": []}, {"text": "Experiments show that our method achieves significantly better results.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the proposed method in details.", "labels": [], "entities": []}, {"text": "Section 4 gives the experimental results.", "labels": [], "entities": []}, {"text": "Lastly, we conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "For English corpora, the pre-processing are the same as that in (, and for Chinese corpora, the Stanford Word Segmenter () is used to perform word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.7331101149320602}]}, {"text": "We select three state-of-the-art syntax-based methods to be compared with our method: DP uses a bootstrapping algorithm named as Double Propagation (, which is a conventional syntax-based method.", "labels": [], "entities": []}, {"text": "DP-HITS is an enhanced version of DP proposed by, which ranks product feature candidates by where importance(t) is estimated by the HITS algorithm.", "labels": [], "entities": [{"text": "DP-HITS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8404330015182495}]}, {"text": "SGW is the Sentiment Graph Walking algorithm proposed in (, which first extracts syntactic patterns and then uses random walking to rank candidates.", "labels": [], "entities": [{"text": "Sentiment Graph Walking", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7549508810043335}]}, {"text": "Afterwards, wordsyntactic pattern co-occurrence statistic is used as feature fora semi-supervised classifier TSVM) to further refine the results.", "labels": [], "entities": []}, {"text": "This two-stage method is denoted as SGW-TSVM.", "labels": [], "entities": []}, {"text": "LEX only uses lexical semantic clue.", "labels": [], "entities": [{"text": "LEX", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.782722532749176}]}, {"text": "Label Propagation is applied alone in a self-training manner.", "labels": [], "entities": [{"text": "Label Propagation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6724845468997955}]}, {"text": "The dimension of word embedding n = 100, the convergence threshold \u03b5 = 10 \u22127 , and the number of expanded seeds T = 40.", "labels": [], "entities": [{"text": "convergence threshold \u03b5", "start_pos": 45, "end_pos": 68, "type": "METRIC", "confidence": 0.9506998062133789}]}, {"text": "The size of the seed set N is 40.", "labels": [], "entities": []}, {"text": "To output product features, it ranks candidates in descent order by using the positive score L + f (t).", "labels": [], "entities": []}, {"text": "CONT only uses contextual semantic clue, which only contains the CNN.", "labels": [], "entities": [{"text": "CONT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8442762494087219}, {"text": "CNN", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9178575873374939}]}, {"text": "The window size l is 5.", "labels": [], "entities": []}, {"text": "The CNN is trained with a mini-batch size of 50.", "labels": [], "entities": []}, {"text": "The hidden layer size h = 250.", "labels": [], "entities": []}, {"text": "Finally, importance(t) in Equ.", "labels": [], "entities": [{"text": "importance", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9972085356712341}]}, {"text": "13 is replaced with r + tin Equ.", "labels": [], "entities": []}, {"text": "LEX&CONT leverages both semantic clues.: Experimental results of product feature mining.", "labels": [], "entities": [{"text": "LEX", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8066155910491943}, {"text": "product feature mining", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.6377456784248352}]}, {"text": "The precision or recall of CONT is the average performance over five runs with different random initialization of parameters of the CNN.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995254278182983}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9853293299674988}, {"text": "CONT", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.40375277400016785}]}, {"text": "stands for the average score.", "labels": [], "entities": [{"text": "average score", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.9777291119098663}]}], "tableCaptions": [{"text": " Table 1: Experimental results of product feature mining. The precision or recall of CONT is the average  performance over five runs with different random initialization of parameters of the CNN. Avg. stands  for the average score.", "labels": [], "entities": [{"text": "product feature mining", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.6722724636395773}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.999613344669342}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9823330044746399}, {"text": "CONT", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.5130087733268738}, {"text": "Avg.", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9758162796497345}]}, {"text": " Table 2: The recall of frequent product features.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9985460042953491}]}, {"text": " Table 3: The results of convolutional method vs. the results of non-convolutional methods.", "labels": [], "entities": []}]}