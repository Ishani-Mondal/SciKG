{"title": [{"text": "Learning Polylingual Topic Models from Code-Switched Social Media Documents", "labels": [], "entities": []}], "abstractContent": [{"text": "Code-switched documents are common in social media, providing evidence for polylingual topic models to infer aligned topics across languages.", "labels": [], "entities": []}, {"text": "We present Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multilingual corpus analysis.", "labels": [], "entities": [{"text": "multilingual corpus analysis", "start_pos": 136, "end_pos": 164, "type": "TASK", "confidence": 0.6538121004899343}]}, {"text": "We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human anno-tators.", "labels": [], "entities": [{"text": "English-Chinese Weibo data", "start_pos": 78, "end_pos": 104, "type": "DATASET", "confidence": 0.5947317282358805}]}], "introductionContent": [{"text": "Topic models () have become standard tools for analyzing document collections, and topic analyses are quite common for social media (.", "labels": [], "entities": []}, {"text": "Their popularity owes in part to their data driven nature, allowing them to adapt to new corpora and languages.", "labels": [], "entities": []}, {"text": "In social media especially, there is a large diversity in terms of both the topic and language, necessitating the modeling of multiple languages simultaneously.", "labels": [], "entities": []}, {"text": "A good candidate for multi-lingual topic analyses are polylingual topic models ( ), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.", "labels": [], "entities": []}, {"text": "Polylingual topic models enable cross language analysis by grouping documents by topic regardless of language.", "labels": [], "entities": [{"text": "cross language analysis", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7243524591128031}]}, {"text": "Training of polylingual topic models requires parallel or comparable corpora: document tuples from multiple languages that discuss the same topic.", "labels": [], "entities": []}, {"text": "While additional non-aligned documents User 1: \u00a1Don Samuel es un crack!", "labels": [], "entities": []}, {"text": "#VamosM\u00e9xico #DaleTri RT @User4: Arriba!", "labels": [], "entities": [{"text": "DaleTri RT", "start_pos": 14, "end_pos": 24, "type": "DATASET", "confidence": 0.6919156014919281}, {"text": "Arriba", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.6699503660202026}]}, {"text": "medal match in \"Football\"!", "labels": [], "entities": [{"text": "Football\"", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.8045061826705933}]}, {"text": "User 2: @user1 rodo que tal el nuevo Mountain ? User 3: @User1 @User4 wow this is something !!", "labels": [], "entities": []}, {"text": "Ja ja ja Football well said can be folded in during training, the \"glue\" documents are required to aid in the alignment across languages.", "labels": [], "entities": []}, {"text": "However, the ever changing vocabulary and topics of social media (Eisenstein, 2013) make finding suitable comparable corpora difficult.", "labels": [], "entities": []}, {"text": "Standard techniques -such as relying on machine translation parallel corpora or comparable documents extracted from Wikipedia in different languages -fail to capture the specific terminology of social media.", "labels": [], "entities": []}, {"text": "Alternate methods that rely on bilingual lexicons) similarly fail to adapt to shifting vocabularies.", "labels": [], "entities": []}, {"text": "The result: an inability to train polylingual models on social media.", "labels": [], "entities": []}, {"text": "In this paper, we offer a solution: utilize codeswitched social media to discover correlations across languages.", "labels": [], "entities": []}, {"text": "Social media is filled with examples of code-switching, where users switch between two or more languages, both in a conversation and even a single message (.", "labels": [], "entities": []}, {"text": "This mixture of languages in the same context suggests alignments between words across languages through the common topics discussed in the context.", "labels": [], "entities": []}, {"text": "We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics.", "labels": [], "entities": []}, {"text": "Our model improves both in terms of perplexity and a human evaluation, and we provide some example analyses of social media that rely on our learned topics.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated csLDA on the two datasets and evaluated each model using perplexity on held out data and human judgements.", "labels": [], "entities": []}, {"text": "While our goal is to learn polylingual topics, we cannot compare to previous polylingual models since they require comparable data, which we lack.", "labels": [], "entities": []}, {"text": "Instead, we constructed a baseline from LDA run on the entire dataset (no language information.)", "labels": [], "entities": []}, {"text": "For each model, we measured the document completion perplexity) on the held out data.", "labels": [], "entities": []}, {"text": "We experimented with different numbers of topics (T ).", "labels": [], "entities": []}, {"text": "Since csLDA duplicates topic distributions (T \u00d7L) we used twice as many topics for LDA.", "labels": [], "entities": []}, {"text": "shows test perplexity for varying T and perplexity for the best setting of csLDA (T =60) and LDA (T =120).", "labels": [], "entities": [{"text": "perplexity", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9665620923042297}, {"text": "LDA", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9329172372817993}]}, {"text": "The table lists both monolingual and code-switched test data; csLDA improves over LDA in almost every case, and across all values of T . The background distribution (-bg) has mixed results for LDA, whereas for csLDA it shows consistent improvement.", "labels": [], "entities": []}, {"text": "While there are some mistakes, overall the topics are coherent and aligned.", "labels": [], "entities": []}, {"text": "We use the available per-token LID system ( for Spanish/English to justify csLDA's ability to infer the hidden language variables.", "labels": [], "entities": []}, {"text": "We ran csLDA-bg with l i set to the value provided by the LID system for codeswitched documents (csLDA-bg with LID), which gives csLDA high quality LID labels.", "labels": [], "entities": []}, {"text": "While we see gains for the code-switched data, overall the results for csLDA-bg and csLDA-bg with LID are similar, suggesting that the model can operate effectively even without a supervised per-token LID system.", "labels": [], "entities": []}, {"text": "We evaluate topic alignment quality through a human judgements).", "labels": [], "entities": [{"text": "topic alignment", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.775090366601944}]}, {"text": "For each aligned topic, we show an annotator the 20 most frequent words from the foreign language topic (Chinese or Spanish) with the 20 most frequent words from the aligned English topic and two random English topics.", "labels": [], "entities": []}, {"text": "The annotators are asked to select the most related English topic among the three; the one with the most votes is considered the aligned topic.", "labels": [], "entities": []}, {"text": "We count how often the model's alignments agree.", "labels": [], "entities": []}, {"text": "LDA may learn comparable topics in different languages but gives no explicit alignments.", "labels": [], "entities": []}, {"text": "We create alignments by classifying each LDA topic by language using the KL-divergence between the topic's words distribution and a word distribution for the English/foreign language inferred from the monolingual documents.", "labels": [], "entities": []}, {"text": "Language is assigned to a topic by taking the minimum KL.", "labels": [], "entities": [{"text": "KL", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.839783787727356}]}, {"text": "For Weibo data, this was not effective since the vocabularies of each language are highly unbalanced.", "labels": [], "entities": [{"text": "Weibo data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9380476176738739}]}, {"text": "Instead,: Examples of aligned topics from Olympics (left) and Weibo (right).", "labels": [], "entities": [{"text": "Weibo", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9501277804374695}]}, {"text": "we manually labeled the topics by language.", "labels": [], "entities": []}, {"text": "We then pair topics across languages using the cosine similarity of their co-occurrence statistics in codeswitched documents.", "labels": [], "entities": []}, {"text": "Topic pairs with similarity above tare considered aligned topics.", "labels": [], "entities": []}, {"text": "We also used a threshold p ( \u00a73.2) to select aligned topics in csLDA.", "labels": [], "entities": []}, {"text": "To ensure a fair comparison, we select the same number of aligned topics for LDA and csLDA.", "labels": [], "entities": []}, {"text": "We used the best performing setting: csLDA T =60, LDA T =120, which produced 12 alignments from Olympics and 28 from Weibo.", "labels": [], "entities": [{"text": "LDA T", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9123445749282837}, {"text": "Olympics", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9470698833465576}, {"text": "Weibo", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.9825584888458252}]}, {"text": "Using Mechanical Turk we collected multiple judgements per alignment.", "labels": [], "entities": []}, {"text": "For Spanish, we removed workers who disagreed with the majority more than 50% of the time (83 deletions), leaving 6.5 annotations for each alignment (85.47% inter-annotator agreement.)", "labels": [], "entities": []}, {"text": "For Chinese, since quality of general Chinese turkers is low) we invited specific workers and obtained 9.3 annotations per alignment (78.72% inter-annotator agreement.)", "labels": [], "entities": []}, {"text": "For Olympics, LDA alignments matched the judgements 25% of the time, while csLDA matched 50% of the time.", "labels": [], "entities": []}, {"text": "While csLDA found 12 alignments and LDA 29, the 12 topics evaluated from both models show that csLDA's alignments are higher quality.", "labels": [], "entities": [{"text": "LDA", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.5884144306182861}]}, {"text": "For the Weibo data, LDA matched judgements 71.4%, while csLDA matched 75%.", "labels": [], "entities": [{"text": "Weibo data", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9899913668632507}, {"text": "LDA", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.688441812992096}, {"text": "judgements", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.8654220104217529}]}], "tableCaptions": []}