{"title": [{"text": "Negation Focus Identification with Contextual Discourse Information", "labels": [], "entities": [{"text": "Negation Focus Identification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7967476844787598}]}], "abstractContent": [{"text": "Negative expressions are common in natural language text and play a critical role in information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.8426060676574707}]}, {"text": "However, the performances of current systems are far from satisfaction , largely due to its focus on intra-sentence information and its failure to consider inter-sentence information.", "labels": [], "entities": []}, {"text": "In this paper, we propose a graph model to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives.", "labels": [], "entities": []}, {"text": "Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information.", "labels": [], "entities": [{"text": "*SEM 2012 shared task corpus", "start_pos": 18, "end_pos": 46, "type": "DATASET", "confidence": 0.6364650775988897}, {"text": "negation focus identification", "start_pos": 111, "end_pos": 140, "type": "TASK", "confidence": 0.9390199780464172}]}], "introductionContent": [{"text": "Negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition.", "labels": [], "entities": []}, {"text": "For example, sentence (1) could be interpreted as it is not the case that he stopped.", "labels": [], "entities": []}, {"text": "(1) He didn't stop.", "labels": [], "entities": []}, {"text": "Negation expressions are common in natural language text.", "labels": [], "entities": []}, {"text": "According to the statistics on biomedical literature genre (, 19.44% of sentences contain negative expressions.", "labels": [], "entities": []}, {"text": "The percentage rises to 22.5% on Conan Doyle stories.", "labels": [], "entities": [{"text": "percentage", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9528611302375793}, {"text": "Conan Doyle stories", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.5871307849884033}]}, {"text": "It is interesting that a negative sentence may have both negative and positive meanings.", "labels": [], "entities": []}, {"text": "For example, sentence (2) could be interpreted as He stopped, but not until he got to Jackson Hole with positive part he stopped and negative part until he got to Jackson Hole.", "labels": [], "entities": [{"text": "Jackson Hole", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.9317094683647156}, {"text": "Jackson Hole", "start_pos": 163, "end_pos": 175, "type": "DATASET", "confidence": 0.9548488259315491}]}, {"text": "Moreover, a nega-* Corresponding author tive expression normally interacts with some special part in the sentence, referred as negation focus in linguistics.", "labels": [], "entities": []}, {"text": "Formally, negation focus is defined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression.", "labels": [], "entities": [{"text": "negation focus", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9830304384231567}]}, {"text": "Hereafter, we denote negative expression in boldface and negation focus underlined.", "labels": [], "entities": []}, {"text": "(2) He didn't stop until he got to Jackson Hole.", "labels": [], "entities": [{"text": "Jackson Hole", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9709257483482361}]}, {"text": "While people tend to employ stress or intonation in speech to emphasize negation focus and thus it is easy to identify negation focus in speech corpora, such stress or intonation information often misses in the dominating text corpora.", "labels": [], "entities": []}, {"text": "This poses serious challenges on negation focus identification.", "labels": [], "entities": [{"text": "negation focus identification", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.9903850754102071}]}, {"text": "Current studies (e.g.,) sort to various kinds of intra-sentence information, such as lexical features, syntactic features, semantic role features and soon, ignoring less-obvious inter-sentence information.", "labels": [], "entities": []}, {"text": "This largely defers the performance of negation focus identification and its wide applications, since such contextual discourse information plays a critical role on negation focus identification.", "labels": [], "entities": [{"text": "negation focus identification", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.939464271068573}, {"text": "negation focus identification", "start_pos": 165, "end_pos": 194, "type": "TASK", "confidence": 0.9123620589574178}]}, {"text": "Take following sentence as an example.", "labels": [], "entities": []}, {"text": "(3) Helen didn't allow her youngest son to play the violin.", "labels": [], "entities": []}, {"text": "Scenario B: Given sentence She thought that he didn't have the artistic talent like her eldest son as next sentence, the negation focus should be the youngest son, yielding interpretation Helen thought that her eldest son had the talent to play the violin, but the youngest son didn't.", "labels": [], "entities": []}, {"text": "Scenario C: Given sentence Because of her neighbors' protests as previous sentence, the negation focus should be play the violin, yielding interpretation Helen didn't allow her youngest son to play the violin, but it didn't show whether he was allowed to do other things.", "labels": [], "entities": []}, {"text": "In this paper, to well accommodate such contextual discourse information in negation focus identification, we propose a graph model to enrich normal intra-sentence features with various kinds of inter-sentence features from both lexical and topic perspectives.", "labels": [], "entities": [{"text": "negation focus identification", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.9591159423192342}]}, {"text": "Besides, the standard PageRank algorithm is employed to optimize the graph model.", "labels": [], "entities": []}, {"text": "Evaluation on the *SEM 2012 shared task corpus justifies our approach over several strong baselines.", "labels": [], "entities": [{"text": "*SEM 2012 shared task corpus", "start_pos": 18, "end_pos": 46, "type": "DATASET", "confidence": 0.607933113972346}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 overviews the related work.", "labels": [], "entities": []}, {"text": "Section 3 presents several strong baselines on negation focus identification with only intra-sentence features.", "labels": [], "entities": [{"text": "negation focus identification", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.9811364809672037}]}, {"text": "Section 4 introduces our topic-driven word-based graph model with contextual discourse information.", "labels": [], "entities": []}, {"text": "Section 5 reports the experimental results and analysis.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe experimental settings and systematically evaluate our negation focus identification approach with focus on exploring the effectiveness of contextual discourse information.", "labels": [], "entities": [{"text": "negation focus identification", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.8672693570454916}]}, {"text": "In all our experiments, we employ the *SEM'2012 shared task corpus (Morante and Blanco, 2012) 2 . As a freely downloadable resource, the *SEM shared task corpus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 184, "end_pos": 192, "type": "DATASET", "confidence": 0.9693242907524109}, {"text": "WSJ section of the Penn TreeBank", "start_pos": 209, "end_pos": 241, "type": "DATASET", "confidence": 0.8498456279436747}]}, {"text": "In particular, negation focus annotation on this corpus is restricted to verbal negations (with corresponding mark MNEG in PropBank).", "labels": [], "entities": [{"text": "negation focus annotation", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.9315843383471171}, {"text": "MNEG", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.6271159648895264}, {"text": "PropBank", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.9071971774101257}]}, {"text": "On 50% of the corpus annotated by two annotators, the inter-annotator agreement was 0.72 (.", "labels": [], "entities": []}, {"text": "Along with negation focus annotation, this corpus also contains other annotations, such as POS tag, named entity, chunk, constituent tree, dependency tree, and semantic role.", "labels": [], "entities": []}, {"text": "In total, this corpus provides 3,544 instances of negation focus annotations.", "labels": [], "entities": [{"text": "negation focus annotations", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.9259018898010254}]}, {"text": "For fair comparison, we adopt the same partition as *SEM'2012 shared task in all our experiments, i.e., with 2,302 for training, 530 for development, and 712 for testing.", "labels": [], "entities": []}, {"text": "Although for each instance, the corpus only provides the current sentence, the previous and next sentences as its context, we sort to the Penn TreeBank 3 to obtain the corresponding document as its discourse context.", "labels": [], "entities": [{"text": "Penn TreeBank 3", "start_pos": 138, "end_pos": 153, "type": "DATASET", "confidence": 0.9887000918388367}]}, {"text": "Same as the *SEM'2012 shared task, the evaluation is made using precision, recall, and F1-score.", "labels": [], "entities": [{"text": "SEM'2012 shared task", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7091240286827087}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9996625185012817}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9992501139640808}, {"text": "F1-score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9989473223686218}]}, {"text": "Especially, a true positive (TP) requires an exact match for the negation focus, a false positive (FP) occurs when a system predicts a non-existing negation focus, and a false negative (FN) occurs when the gold annotations specify a negation focus but the system makes no prediction.", "labels": [], "entities": [{"text": "FP)", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8960384726524353}, {"text": "false negative (FN)", "start_pos": 170, "end_pos": 189, "type": "METRIC", "confidence": 0.7429787874221802}]}, {"text": "For each instance, the predicted focus is considered correct if it is a complete match with a gold annotation.", "labels": [], "entities": []}, {"text": "Beside, to show whether an improvement is significant, we conducted significance testing using z-test, as described in.", "labels": [], "entities": []}, {"text": "With Only Intra-sentence Information shows the performance of the two baselines, the decision tree-based classifier as in Blanco and Moldovan (2011) and our ranking SVM-based classifier.", "labels": [], "entities": []}, {"text": "It shows that our ranking SVM-based baseline slightly improves the F1-measure by 2.52% over the decision tree-based baseline, largely due to the incorporation of more refined features..", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9988130331039429}]}, {"text": "Performance of baselines with only intra-sentence information.", "labels": [], "entities": []}, {"text": "Error analysis of the ranking SVM-based baseline on development data shows that 72% of them are caused by the ignorance of intersentence information.", "labels": [], "entities": []}, {"text": "For example, among the 42 instances listed in the category of \"#Inter-Sentence Only\" in, only 7 instances can be identified correctly by the ranking SVMbased classifier.", "labels": [], "entities": []}, {"text": "With about 4 focus candidates in one sentence on average, this percentage is even lower than random.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Performance of baselines with only  intra-sentence information.", "labels": [], "entities": []}, {"text": " Table 4. Performance with only inter-sentence  information.", "labels": [], "entities": []}, {"text": " Table 5. Performance comparison of systems on  negation focus identification.", "labels": [], "entities": [{"text": "negation focus identification", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.9614983399709066}]}, {"text": " Table 6. Performance comparison of systems on  negation focus identification with automatically  extracted features.", "labels": [], "entities": [{"text": "negation focus identification", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.9524123072624207}]}]}