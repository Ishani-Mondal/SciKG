{"title": [], "abstractContent": [{"text": "This paper presents the first results on parsing the Penn Parsed Corpus of Modern British English (PPCMBE), a million-word historical treebank with an annotation style similar to that of the Penn Tree-bank (PTB).", "labels": [], "entities": [{"text": "Penn Parsed Corpus of Modern British English (PPCMBE), a million-word historical treebank", "start_pos": 53, "end_pos": 142, "type": "DATASET", "confidence": 0.8127305289109548}, {"text": "Penn Tree-bank (PTB)", "start_pos": 191, "end_pos": 211, "type": "DATASET", "confidence": 0.9732216954231262}]}, {"text": "We describe key features of the PPCMBE annotation style that differ from the PTB, and present some experiments with tree transformations to better compare the results to the PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8781353831291199}, {"text": "PTB", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.9353708624839783}]}, {"text": "First steps in parser analysis focus on problematic structures created by the parser.", "labels": [], "entities": [{"text": "parser analysis", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.9288717806339264}]}], "introductionContent": [{"text": "We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (, showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB)).", "labels": [], "entities": [{"text": "Penn Parsed Corpus of Modern British English (PPCMBE)", "start_pos": 45, "end_pos": 98, "type": "DATASET", "confidence": 0.9490485310554504}, {"text": "F-score", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.9969905614852905}, {"text": "Penn Treebank (PTB))", "start_pos": 174, "end_pos": 194, "type": "DATASET", "confidence": 0.9726648926734924}]}, {"text": "We discuss some of the differences in annotation style and source material that make a direct comparison problematic.", "labels": [], "entities": []}, {"text": "Some first steps at analysis of the parsing results indicate aspects of the annotation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.", "labels": [], "entities": []}, {"text": "The PPCMBE is a million-word treebank created for researching changes in English syntax.", "labels": [], "entities": []}, {"text": "It covers the years  and is the most modern in the series of treebanks created for historical research.", "labels": [], "entities": []}, {"text": "Due to the historical nature of the PPCMBE, it shares some of the characteristics of treebanks based on modern unedited text (), such as spelling variation.", "labels": [], "entities": [{"text": "PPCMBE", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.8348601460456848}]}, {"text": "The size of the PPCMBE is roughly the same as the WSJ section of the PTB, and its annotation style is similar to that of the PTB, but with differences, particularly with regard to coordination and NP structure.", "labels": [], "entities": [{"text": "PTB", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.5022798776626587}, {"text": "PTB", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.9318301677703857}]}, {"text": "However, except for, we have found no discussion of this corpus in the literature.", "labels": [], "entities": []}, {"text": "There is also much additional material annotated in this style, increasing the importance of analyzing parser performance on this annotation style.", "labels": [], "entities": []}], "datasetContent": [{"text": "The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser ( and score using the standard evalb program.", "labels": [], "entities": []}, {"text": "We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters).", "labels": [], "entities": []}, {"text": "Since the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.5995938628911972}]}, {"text": "shows the results for both modes.", "labels": [], "entities": []}, {"text": "8 Consider first the results for the Dev section with the parser using the gold tags.", "labels": [], "entities": []}, {"text": "The score for all sentences increases from 83.7 for the Release corpus (row 1) to 84.7 for the Reduced corpus (row 2), reflecting the POS tag simplifications in the Reduced corpus.", "labels": [], "entities": []}, {"text": "The score goes up by a further 2.0 to 86.7 (row 2 to 4) for the Reduced+NPs corpus and up again by 0.4 to 87.1 (row 5) for the Reduced+NPs+VPs corpus, showing the ef-fects of the extra NP and VP brackets.", "labels": [], "entities": [{"text": "Reduced+NPs corpus", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.546050950884819}]}, {"text": "We evaluated the Test section on the Reduced corpus (row 3), with a result 0.8 higher than the Dev (85.5 in row 3 compared to 84.7 in row 2).", "labels": [], "entities": [{"text": "Reduced corpus", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8349347114562988}]}, {"text": "The score for sentences of length <= 40 (a larger percentage of the PPCMBE than the PTB) is 2.4 higher than the score for all sentences, with both the gold and parser tags (row 5).", "labels": [], "entities": [{"text": "PTB", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.5334556102752686}]}, {"text": "The results with the parser choosing its own POS tags naturally go down, with the Test section suffering more.", "labels": [], "entities": []}, {"text": "In general, the PPCMBE is affected by the lack of gold tags more than the PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9288421273231506}]}, {"text": "In sum, the parser results show that the PPCMBE can be parsed at a level approaching that of the PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9052510261535645}]}, {"text": "We are not proposing that the current version be replaced by the Reduced+NPs+VPs version, on the grounds that the latter gets the highest score.", "labels": [], "entities": []}, {"text": "Our goal was to determine whether the parsing results fell in the same general range as for the PTB by roughly compensating for the difference in annotation style.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9695689678192139}, {"text": "PTB", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9417544603347778}]}, {"text": "The results in show that this is the case.", "labels": [], "entities": []}, {"text": "As a final note, the PPCMBE consists of unedited data spanning more than 200 years, while the PTB is edited newswire, and so to some extent there would almost certainly be some difference in score.", "labels": [], "entities": [{"text": "PTB", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9760317206382751}]}], "tableCaptions": [{"text": " Table 2: Token count and data split for PPCMBE", "labels": [], "entities": [{"text": "PPCMBE", "start_pos": 41, "end_pos": 47, "type": "TASK", "confidence": 0.48575079441070557}]}, {"text": " Table 4: Parsing results with Berkeley Parser. The corpus versions used are Release (Rl), Reduced (Rd),  Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs). Results are shown for the parser forced  to use the gold POS tags from the corpus, and with the parser supplying its own tags. For the latter case,  the tagging accuracy is shown in the last column.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 320, "end_pos": 328, "type": "METRIC", "confidence": 0.9686411023139954}]}, {"text": " Table 3: Average sentence length and percentage  of sentences of length <=40 in the PPCMBE and  PTB.", "labels": [], "entities": [{"text": "Average sentence length", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.7641795178254446}, {"text": "PPCMBE", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9271983504295349}, {"text": "PTB", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.6460614800453186}]}]}