{"title": [{"text": "Vector space semantics with frequency-driven motifs", "labels": [], "entities": [{"text": "Vector space semantics", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7683620055516561}]}], "abstractContent": [{"text": "Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lex-emes and complexities of modeling semantic composition when dealing with structures larger than single lexical items.", "labels": [], "entities": []}, {"text": "In this work, we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents, or motifs.", "labels": [], "entities": []}, {"text": "The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal con-situents, and circumvents some problems of data sparsity by design.", "labels": [], "entities": []}, {"text": "We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically dis-ambiguated.", "labels": [], "entities": []}, {"text": "Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Meaning in language is a confluence of experientially acquired semantics of words or multi-word phrases, and their semantic composition to create new meanings.", "labels": [], "entities": []}, {"text": "For instance, successfully interpreting a sentence such as The old senator kicked the bucket.", "labels": [], "entities": [{"text": "interpreting a sentence", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8597152431805929}]}, {"text": "requires the knowledge that the semantic connotations of 'kicking the bucket' as a unit are the same as those for 'dying'.", "labels": [], "entities": []}, {"text": "Short of explicit supervision, such semantic mappings must be inferred by anew language speaker through inductive mechanisms operating on observed linguistic usage.", "labels": [], "entities": []}, {"text": "This perspective of acquired meaning aligns with the 'meaning is usage' adage, consonant with Wittgenstein's view of semantics.", "labels": [], "entities": []}, {"text": "At the same time, the ability to adaptively communicate elaborate meanings can only be conciled through Frege's principle of compositionality, i.e., meanings of larger linguistic constructs can be derived from the meanings of individual components, modulated by their syntactic interrelations.", "labels": [], "entities": []}, {"text": "Indeed, most linguistic usage appears compositional.", "labels": [], "entities": []}, {"text": "This is supported by the fact even with very limited vocabulary, children and non-native speakers can often communicate surprisingly effectively.", "labels": [], "entities": []}, {"text": "It can be argued that to be sustainable, inductive aspects of meaning must be recurrent enough to be learnable by new users.", "labels": [], "entities": []}, {"text": "That is, a noncompositional phrase such as 'kick the bucket' is likely to persist in common parlance only if it is frequently used with its associated semantic mapping.", "labels": [], "entities": []}, {"text": "If a usage-driven meaning of a motif is not recurrent enough, learning this mapping is inefficient in two ways.", "labels": [], "entities": []}, {"text": "First, the sparseness of observations would severely limit accurate inductive acquisition by new observers.", "labels": [], "entities": [{"text": "accurate inductive acquisition", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.680069218079249}]}, {"text": "Second, the value of learning a very infrequent semantic mapping is likely marginal.", "labels": [], "entities": []}, {"text": "This motivates the need fora frequency-driven view of lexical semantics.", "labels": [], "entities": []}, {"text": "In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below.", "labels": [], "entities": []}, {"text": "Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics (.", "labels": [], "entities": []}, {"text": "Such models have engendered improvements in diverse applications such as selectional preference modeling, word-sense discrimination, automatic dictionary building, and information retrieval.", "labels": [], "entities": [{"text": "selectional preference modeling", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.8006092508633932}, {"text": "word-sense discrimination", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.8192760646343231}, {"text": "dictionary building", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.6940826773643494}, {"text": "information retrieval", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.8332595825195312}]}, {"text": "However, while conventional DSMs consider colloca-With the bad press in wake of the financial crisis, businesses are leaving our shores . crisis: <bad, businesses, financial, leaving, press, shores, wake> financial crisis: <bad press, businesses, in wake of, leaving our shores>: Meaning representation by conventional DSMs vs notional ideal tion strengths (through counts and PMI scores) of word neighbourhoods, they disregard much of the regularity inhuman language.", "labels": [], "entities": []}, {"text": "Most significantly, word tokens that act as latent dimensions are often derived from arbitrary tokenization.", "labels": [], "entities": []}, {"text": "The example given in succinctly describes this.", "labels": [], "entities": []}, {"text": "The first row in the table shows a representation of the meaning of the token 'crisis' that a conventional DSM might extract from the given sentence after stopword removal.", "labels": [], "entities": []}, {"text": "While helpful, the representation seems unsatisfying since words such as 'press', 'wake' and 'shores' seem to have little to do with a crisis.", "labels": [], "entities": []}, {"text": "From a semantic perspective, a representation similar to the second is more valuable: not only does it represent a semantic mapping fora more specific meaning, but the latent dimensions of the representation have are less noisy (e.g., while 'wake' is semantically ambiguous, its surrounding context in 'in wake of' disambiguates it) and more intuitive in regards of semantic interepretability.", "labels": [], "entities": []}, {"text": "This is the overarching theme of this work: we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive, recurrent lexical units or motifs.", "labels": [], "entities": []}, {"text": "We propose to identify such semantically cohesive motifs in terms of features inspired from frequency-characteristics, linguistic idiosyncrasies, and shallow syntactic analysis; and explore both supervised and semi-supervised models to optimally segment a sentence into such motifs.", "labels": [], "entities": []}, {"text": "Through exploiting regularities in language usage, the framework can efficiently account for both compositional and non-compositional word usage, while avoiding the issue of data-sparsity by design.", "labels": [], "entities": []}, {"text": "Our principal contributions in this paper are: \u2022 We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens \u2022 We present a simple model to segment a sentence into such motifs using a feature-set drawing from frequency statistics, information theory, linguistic theories and shallow syntactic analysis \u2022 Word and phrasal representations learnt through the approach outperform conventional DSM representations on empirical tasks This paper is organized as follows: In Section 2, we briefly review related work in the domain of compositional distributional semantics, and motivate our formulation.", "labels": [], "entities": []}, {"text": "Section 3 describes our methodology, which consists of a frequencydriven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications.", "labels": [], "entities": []}, {"text": "We present experiments and empirical evaluations for our method in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 5 with a summary of our principal findings, and a discussion of possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe some experimental evaluations and findings for our approach.", "labels": [], "entities": []}, {"text": "We first quantitatively and qualitatively analyze the performance of the segmentation model, and then evaluate the distributional motif representations learnt by the model through two downstream applications.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for motif segmentations", "labels": [], "entities": [{"text": "motif segmentations", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7580311596393585}]}, {"text": " Table 4: Results for Sentence Polarity detection", "labels": [], "entities": [{"text": "Sentence Polarity detection", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.9813661376635233}]}, {"text": " Table 5: Results for Metaphor identification", "labels": [], "entities": []}]}