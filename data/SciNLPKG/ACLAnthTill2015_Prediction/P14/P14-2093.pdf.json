{"title": [{"text": "Effective Selection of Translation Model Training Data", "labels": [], "entities": [{"text": "Translation Model Training", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.8566928505897522}]}], "abstractContent": [{"text": "Data selection has been demonstrated to bean effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6294918209314346}, {"text": "statistical machine translation", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.7241397698720297}]}, {"text": "Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus.", "labels": [], "entities": []}, {"text": "By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model.", "labels": [], "entities": []}, {"text": "In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection.", "labels": [], "entities": []}, {"text": "The results show that our methods outperform previous methods.", "labels": [], "entities": []}, {"text": "When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.", "labels": [], "entities": [{"text": "MT task", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.913846343755722}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.999438464641571}]}], "introductionContent": [{"text": "Statistical machine translation depends heavily on large scale parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.6890670508146286}]}, {"text": "The corpora are necessary priori knowledge for training effective translation model.", "labels": [], "entities": []}, {"text": "However, domain-specific machine translation has few parallel corpora for translation model training in the domain of interest.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6786914467811584}, {"text": "translation model training", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.9245512882868449}]}, {"text": "For this, an effective approach is to automatically select and expand domain-specific sentence pairs from large scale general-domain parallel corpus.", "labels": [], "entities": []}, {"text": "The approach is named Data Selection.", "labels": [], "entities": [{"text": "Data Selection", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8183971345424652}]}, {"text": "Current data selection methods mostly use language models trained on small scale indomain data to measure domain relevance and select domain-relevant parallel sentence pairs to expand training corpora.", "labels": [], "entities": []}, {"text": "Related work in literature has proven that the expanded corpora can substantially improve the performance of ma-* Corresponding author chine translation (.", "labels": [], "entities": [{"text": "Corresponding author chine translation", "start_pos": 114, "end_pos": 152, "type": "TASK", "confidence": 0.5622963011264801}]}, {"text": "However, the methods are still far from satisfactory for real application for the following reasons: \uf09f There isn't ready-made domain-specific parallel bitext.", "labels": [], "entities": []}, {"text": "So it's necessary for data selection to have significant capability in mining parallel bitext in those assorted free texts.", "labels": [], "entities": [{"text": "data selection", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7153370380401611}]}, {"text": "But the existing methods seldom ensure parallelism in the target domain while selecting domain-relevant bitext.", "labels": [], "entities": []}, {"text": "\uf09f Available domain-relevant bitext needs keep high domain-relevance at both the sides of source and target language.", "labels": [], "entities": []}, {"text": "But it's difficult for current method to maintain two-sided domain-relevance when we aim at enhancing parallelism of bitext.", "labels": [], "entities": []}, {"text": "Ina word, current data selection methods can't well maintain both parallelism and domainrelevance of bitext.", "labels": [], "entities": []}, {"text": "To overcome the problem, we first propose the method combining translation model with language model in data selection.", "labels": [], "entities": []}, {"text": "The language model measures the domainspecific generation probability of sentences, being used to select domain-relevant sentences at both sides of source and target language.", "labels": [], "entities": []}, {"text": "Meanwhile, the translation model measures the translation probability of sentence pair, being used to verify the parallelism of the selected domainrelevant bitext.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9604602456092834}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Translation performances of In-domain and  General-domain baseline systems", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9805048108100891}]}]}