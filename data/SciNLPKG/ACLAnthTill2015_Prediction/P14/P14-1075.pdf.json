{"title": [], "abstractContent": [{"text": "This paper studies the idea of removing low-frequency words from a corpus, which is a common practice to reduce computational costs, from a theoretical standpoint.", "labels": [], "entities": []}, {"text": "Based on the assumption that a corpus follows Zipf's law, we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary.", "labels": [], "entities": []}, {"text": "In addition , we show an approximate behavior of each formula under certain conditions.", "labels": [], "entities": []}, {"text": "We verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Removing low-frequency words from a corpus (often called cutoff) is a common practice to save on the computational costs involved in learning language models and topic models.", "labels": [], "entities": []}, {"text": "In the case of language models, we often have to remove low-frequency words because of alack of computational resources, since the feature space of kgrams tends to be so large that we sometimes need cutoffs even in a distributed environment).", "labels": [], "entities": []}, {"text": "In the case of topic models, the intuition is that low-frequency words do not make a large contribution to the statistics of the models.", "labels": [], "entities": []}, {"text": "Actually, when we try to roughly analyze a corpus with topic models, a reduced corpus is enough for the purpose.", "labels": [], "entities": []}, {"text": "A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance?", "labels": [], "entities": []}, {"text": "Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance?", "labels": [], "entities": []}, {"text": "There have been many stud-ies addressing the question as it pertains to different strategies;.", "labels": [], "entities": []}, {"text": "Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 160, "end_pos": 175, "type": "METRIC", "confidence": 0.6443220476309458}]}, {"text": "To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models.", "labels": [], "entities": []}, {"text": "In this paper, we first address the question from a theoretical standpoint.", "labels": [], "entities": []}, {"text": "We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful method that is worth studying; as reported in;), a cutoff is competitive with sophisticated strategies such as entropy pruning.", "labels": [], "entities": []}, {"text": "As the basis of our theory, we assume Zipf's law, which is an empirical rule representing a long-tail property of words in a corpus.", "labels": [], "entities": []}, {"text": "Our approach is essentially the same as those in physics, in the sense of constructing a theory while believing experimentally observed results.", "labels": [], "entities": []}, {"text": "For example, we can derive the distance to the landing point of a ball thrown up in the air with initial speed v 0 and angle \u03b8 as v 0 2 sin(2\u03b8)/g by believing in the experimentally observed gravity acceleration g.", "labels": [], "entities": []}, {"text": "Ina similar fashion, we will try to clarify the trade-off relationship by believing Zipf's law.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we define the notation and briefly explain Zipf's law and perplexity.", "labels": [], "entities": []}, {"text": "In Section 3, we theoretically derive the trade-off formulae of the cutoff for unigram models, k-gram models, and topic models, each of which represents its perplexity with respect to a reduced vocabulary, under the assumption that the corpus follows Zipf's law.", "labels": [], "entities": []}, {"text": "In addition, we show an approximate behavior of each formula under certain conditions.", "labels": [], "entities": []}, {"text": "In Section 4, we verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on several real corpora.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on three real corpora (Reuters, 20news, and Enwiki) and two synthetic corpora (Zipf1 and ZipfMix) to verify the correctness of our theory and to examine the gap between theory and practice.", "labels": [], "entities": []}, {"text": "Reuters and 20news here denote corpora extracted from the Reuters-21578 and 20 Newsgroups data sets, respectively.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.98052978515625}, {"text": "Reuters-21578 and 20 Newsgroups data sets", "start_pos": 58, "end_pos": 99, "type": "DATASET", "confidence": 0.8165839612483978}]}, {"text": "Enwiki is a 1/100 corpus of the English Wikipedia.", "labels": [], "entities": [{"text": "Enwiki is a 1/100 corpus of the English Wikipedia", "start_pos": 0, "end_pos": 49, "type": "DATASET", "confidence": 0.6357236802577972}]}, {"text": "Zipf1 is a synthetic corpus generated by Zipf's law, whose corpus is the same size as Reuters, and ZipfMix is a mixture of 20 synthetic corpora, sizes are 1/20th of Reuters.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9436619877815247}, {"text": "Reuters", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9694091081619263}]}, {"text": "We used ZipfMix only for the experiments on topic models.", "labels": [], "entities": []}, {"text": "lists the details of all five corpora.", "labels": [], "entities": []}, {"text": "shows the word frequency of Reuters, 20news, Enwiki, and Zipf1 versus frequency ranking on a log-log graph.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9578210115432739}, {"text": "20news", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9046598076820374}]}, {"text": "In all corpora, we can regard each curve as linear with a gradient close to 1.", "labels": [], "entities": []}, {"text": "This means that all corpora roughly follow Zipf's law.", "labels": [], "entities": []}, {"text": "Furthermore, since the curve of Zipf1 is similar to that of Reuters, Zipf1 can be regarded as acceptable.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9566231966018677}]}, {"text": "plots the perplexity of unigram models learned from Reuters, 20news, Enwiki, and Zipf1 versus the size of reduced vocabulary on a log-log graph.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9564923048019409}]}, {"text": "Each value is the average over different test sets of five-fold cross validation.", "labels": [], "entities": []}, {"text": "Theory1 is calculated using the formula in Theorem 3.", "labels": [], "entities": []}, {"text": "The graph shows that the curve of Theory1 is nearly identical to that of Zipf1.", "labels": [], "entities": [{"text": "Theory1", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9400737285614014}]}, {"text": "Since the vocabulary size W \u03c4 of each test set is small in this experiment, some errors appear when W \u2032 is large, i.e., W \u03c4 < W \u2032 . This clearly means that our theory is theoretically correct for an ideal corpus Zipf1.", "labels": [], "entities": []}, {"text": "Comparing Zipf1 with Reuters, however, we find that their perplexities are quite different.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.961403489112854}]}, {"text": "The reason is that the gap between the frequencies of low-ranking (highfrequency) words is considerably large.", "labels": [], "entities": []}, {"text": "For example, the frequency of the 1st-rank word of Reuters is f (w) = 136, 371, while that of Zipf1 is f (w) = 234, 705.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.8353390097618103}]}, {"text": "Our theory seems to be suited for inferring the growth rate of perplexity rather than the perplexity value itself.", "labels": [], "entities": []}, {"text": "As for the approximate formul\u00e3 PP 1 of Theorem 3, we can surely regard the curve of Zipf1 as being roughly quadratic.", "labels": [], "entities": []}, {"text": "The curves of real corpora also have a similar tendency, although their gradients are slightly steeper.", "labels": [], "entities": []}, {"text": "This difference might have been caused by the above-mentioned errors.", "labels": [], "entities": []}, {"text": "However, at least, we can ascertain the important fact that the results for the corpora reduced by 1/100 are not so different from those of the original corpora from the perspective of their perplexity measures.", "labels": [], "entities": []}, {"text": "plots the frequency of k-grams (k \u2208 {1, 2, 3}) in Reuters versus frequency ranking on a log-log graph.", "labels": [], "entities": []}, {"text": "TheoryFreq (1-3) are calculated using C kin Lemma 4 and \u03c0 kin Lemma 5.", "labels": [], "entities": [{"text": "\u03c0 kin Lemma 5", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.8428884446620941}]}, {"text": "A comparison of TheoryFreq and Zipf verifies the correctness of our theory.", "labels": [], "entities": []}, {"text": "However, comparing Zipf and Reuters, we see that C k is poorly estimated when the gram size is large, whereas \u03c0 k is roughly correct.", "labels": [], "entities": []}, {"text": "This may have happened because we did not put any assumptions on the word se-  quences in our simple model.", "labels": [], "entities": []}, {"text": "The frequencies of high-order k-grams tend to be lower than in reality.", "labels": [], "entities": []}, {"text": "We might need to place a hierarchical assumption on the a power law, as in done in hierarchical Pitman-Yor processes).", "labels": [], "entities": []}, {"text": "TheoryGrad is calculated using \u03c0 kin Lemma 5.", "labels": [], "entities": [{"text": "TheoryGrad", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.921824038028717}, {"text": "\u03c0 kin Lemma", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9159814914067587}]}, {"text": "Surprisingly, the real exponents of Reuters are almost the same as the theoretical estimate \u03c0 k based on our \"stupid\" model that does not care about the order of words.", "labels": [], "entities": []}, {"text": "Note that we do not use any information other than the vocabulary size W and the gram size k for estimating \u03c0 k . plots the perplexity of k-gram models (k \u2208 {1, 2, 3}) learned from Reuters versus the size of reduced vocabulary on a log-log graph.", "labels": [], "entities": []}, {"text": "Theory2 and Theory3 are calculated using the formula in Corollary 6.", "labels": [], "entities": [{"text": "Theory2", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9304421544075012}, {"text": "Theory3", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9178372025489807}]}, {"text": "In the case of bigrams, the perplexities of Theory2 are almost the same as that of Zipf2 when the size of reduced vocabulary is large.", "labels": [], "entities": []}, {"text": "However, in the case of trigrams, the perplexities of Theory3 are far from those of Zipf3.", "labels": [], "entities": []}, {"text": "This difference maybe due to the sparseness of trigrams in Zipf3.", "labels": [], "entities": []}, {"text": "To verify the correctness of our theory for higher order k-gram models, we need to make assumptions that include backoff and smoothing.", "labels": [], "entities": []}, {"text": "plots the perplexity of LDA models with 20 topics learned from Reuters, 20news, Enwiki, Zipf1, and ZipfMix versus the size of reduced vocabulary on a log-log graph.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9648962020874023}]}, {"text": "We used a collapsed Gibbs sampler with 100 iterations to infer the parameters and set the hyper parameters, \u03b1 = 0.1 and \u03b2 = 0.1.", "labels": [], "entities": []}, {"text": "In evaluating the perplexity, we estimated a posterior document-topic distribu- Therefore, we can use TheoryAve as a heuristic function for estimating the perplexity of topic models.", "labels": [], "entities": []}, {"text": "Since we can calculate an inverse of TheoryAve from the bisection or Newton-Raphson method, we can maximize the reduction rate and ensure an acceptable perplexity based on a user-specified deterioration rate.", "labels": [], "entities": [{"text": "TheoryAve", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.8953027725219727}]}, {"text": "According to the fact that the three real corpora with different sizes have a similar tendency, it is expected that we can use our theory fora larger corpus.", "labels": [], "entities": []}, {"text": "Finally, let us examine the computational costs for LDA learning.", "labels": [], "entities": [{"text": "LDA learning", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.9745213091373444}]}, {"text": "shows computational time and memory size for LDA learning on the original corpus, (1/10)-reduced corpus, and (1/20)-reduced corpus of Reuters.", "labels": [], "entities": [{"text": "LDA learning", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.9019134640693665}, {"text": "Reuters", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9624201655387878}]}, {"text": "Comparing the memory used in the learning with the original corpus and with the (1/10)-reduced corpus of Reuters, we find that the learning on the (1/10)-reduced corpus used 60% of the memory used by the learning on the original corpus.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9564118385314941}]}, {"text": "While the computational time decreased a little, we believe that reducing the memory size helps to reduce computational time fora larger corpus in the sense that it can relax the constraint for in-memory computing.", "labels": [], "entities": []}, {"text": "Although we did not examine the accuracy of real tasks in this paper, there is an interesting report that the word error rate of language models follows a power law with respect to perplexity).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9957001209259033}, {"text": "word error rate", "start_pos": 110, "end_pos": 125, "type": "METRIC", "confidence": 0.6769345303376516}]}, {"text": "Thus, we conjecture that the word error rate also has a similar tendency as perplexity with respect to the reduced vocabulary size.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.6029053330421448}]}], "tableCaptions": [{"text": " Table 1: Details of Reuters, 20news, Enwiki,  Zipf1, and ZipfMix.  vocab. size corpus size doc. size  Reuters  70,258  2,754,800  18,118  20news  192,667  4,471,151  19,997  Enwiki  409,902 16,711,226  51,231  Zipf1  69,786  2,754,800  18,118  ZipfMix  70,093  2,754,800  18,118", "labels": [], "entities": [{"text": "ZipfMix.  vocab. size corpus size doc. size  Reuters  70,258", "start_pos": 58, "end_pos": 118, "type": "DATASET", "confidence": 0.7609487013383345}]}, {"text": " Table 2: Computational time and memory size  for LDA learning on the original corpus, (1/10)- reduced corpus, and (1/20)-reduced corpus of  Reuters.  corpus  time  memory perplexity  original 4m3.80s 71,548KB  500  (1/10) 3m55.70s 46,648KB  550  (1/20) 3m42.63s 34,024KB  611", "labels": [], "entities": [{"text": "LDA learning", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.8610362708568573}, {"text": "Reuters", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9571637511253357}]}]}