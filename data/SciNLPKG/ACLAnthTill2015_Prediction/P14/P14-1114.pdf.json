{"title": [{"text": "Probabilistic Soft Logic for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.5603790581226349}]}], "abstractContent": [{"text": "Probabilistic Soft Logic (PSL) is a recently developed framework for proba-bilistic logic.", "labels": [], "entities": [{"text": "Probabilistic Soft Logic (PSL)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6195430209239324}]}, {"text": "We use PSL to combine logical and distributional representations of natural-language meaning, where distri-butional information is represented in the form of weighted inference rules.", "labels": [], "entities": []}, {"text": "We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of natural-language sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distribu-tional approach.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.776841789484024}, {"text": "judging the semantic similarity of natural-language sentences)", "start_pos": 79, "end_pos": 141, "type": "TASK", "confidence": 0.6490587070584297}]}], "introductionContent": [{"text": "When will people say that two sentences are similar?", "labels": [], "entities": []}, {"text": "This question is at the heart of the Semantic Textual Similarity task (STS)(.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task (STS)(", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.804931470326015}]}, {"text": "Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity.", "labels": [], "entities": []}, {"text": "But that can be misleading.", "labels": [], "entities": []}, {"text": "A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.", "labels": [], "entities": []}, {"text": "Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts.", "labels": [], "entities": []}, {"text": "proposed a hybrid approach to sentence similarity: They use a very deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similarity ratings at the word and phrase level.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7182185798883438}]}, {"text": "Sentence similarity is then modelled as mutual entailment in a probabilistic logic.", "labels": [], "entities": [{"text": "Sentence similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9330214560031891}]}, {"text": "This approach is interesting in that it uses a very deep and precise representation of meaning, which can then be relaxed in a controlled fashion using distributional similarity.", "labels": [], "entities": []}, {"text": "But the approach faces large hurdles in practice, stemming from efficiency issues with the Markov Logic Networks (MLN) () that they use for performing probabilistic logical inference.", "labels": [], "entities": []}, {"text": "In this paper, we use the same combined logicbased and distributional framework as but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (.", "labels": [], "entities": []}, {"text": "PSL is a probabilistic logic framework designed to have efficient inference.", "labels": [], "entities": []}, {"text": "Inference in MLNs is theoretically intractable in the general case, and existing approximate inference algorithms are computationally expensive and sometimes inaccurate.", "labels": [], "entities": []}, {"text": "Consequently, the MLN approach of was unable to scale to long sentences and was only tested on the relatively short sentences in the Microsoft video description corpus used for STS ().", "labels": [], "entities": []}, {"text": "On the other hand, inference in PSL reduces to a linear programming problem, which is theoretically and practically much more efficient.", "labels": [], "entities": [{"text": "PSL", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8978173136711121}]}, {"text": "Empirical results on a range of problems have confirmed that inference in PSL is much more efficient than in MLNs, and frequently more accurate (.", "labels": [], "entities": [{"text": "PSL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9327265620231628}]}, {"text": "We show how to use PSL for STS, and describe changes to the PSL framework that make it more effective for STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9076526165008545}]}, {"text": "For evaluation, we test on three STS datasets, and compare our PSL system with the MLN approach of and with distributional-only baselines.", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.7327695190906525}]}, {"text": "Experimental results demonstrate that, overall, PSL models human similarity judgements more accurately than these alternative approaches, and is significantly faster than MLNs.", "labels": [], "entities": [{"text": "PSL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9812136888504028}, {"text": "human similarity judgements", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.5875419775644938}]}, {"text": "The rest of the paper is organized as follows: section 2 presents relevant background material, section 3 explains how we adapted PSL for the STS task, section 4 presents the evaluation, and sections 5 and 6 discuss future work and conclusions, respectively.", "labels": [], "entities": [{"text": "STS task", "start_pos": 142, "end_pos": 150, "type": "TASK", "confidence": 0.8681912422180176}]}], "datasetContent": [{"text": "This section evaluates the performance of PSL on the STS task.", "labels": [], "entities": [{"text": "PSL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9828774929046631}, {"text": "STS task", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.7897906005382538}]}, {"text": "We evaluate our system on three STS datasets.", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9066310226917267}]}, {"text": "\u2022 msr-vid: Microsoft Video Paraphrase Corpus from STS 2012.", "labels": [], "entities": [{"text": "Microsoft Video Paraphrase Corpus from STS 2012", "start_pos": 11, "end_pos": 58, "type": "DATASET", "confidence": 0.8459782430103847}]}, {"text": "The dataset consists of 1,500 pairs of short video descriptions collected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task ().", "labels": [], "entities": []}, {"text": "Half of the dataset is for training, and the second half is for testing.", "labels": [], "entities": []}, {"text": "\u2022 msr-par: Microsoft Paraphrase Corpus from STS 2012 task.", "labels": [], "entities": [{"text": "Microsoft Paraphrase Corpus from STS 2012 task", "start_pos": 11, "end_pos": 57, "type": "DATASET", "confidence": 0.7655752301216125}]}, {"text": "The dataset is 5,801 pairs of sentences collected from news sources ().", "labels": [], "entities": []}, {"text": "Then, for STS 2012, 1,500 pairs were selected and annotated with similarity scores.", "labels": [], "entities": [{"text": "STS 2012", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.6758628487586975}, {"text": "similarity", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9440872669219971}]}, {"text": "Half of the dataset is for training, and the second half is for testing.", "labels": [], "entities": []}, {"text": "\u2022 SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014.", "labels": [], "entities": [{"text": "Sentences Involving Compositional Knowledge", "start_pos": 8, "end_pos": 51, "type": "TASK", "confidence": 0.7059438973665237}, {"text": "SemEval 2014", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.7948325276374817}]}, {"text": "Only the training set is available at this point, which consists of 5,000 pairs of sentences.", "labels": [], "entities": []}, {"text": "Pairs are annotated for RTE and STS, but we only use the STS data.", "labels": [], "entities": [{"text": "RTE", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.4867629110813141}, {"text": "STS data", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.7474546432495117}]}, {"text": "Training and testing was done using 10-fold cross validation.", "labels": [], "entities": []}, {"text": "Systems are evaluated on two metrics, Pearson correlation and average CPU time per pair of sentences.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.9040644466876984}]}, {"text": "\u2022 Pearson correlation: The Pearson correlation between the system's similarity scores and the human gold-standards.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 2, "end_pos": 21, "type": "METRIC", "confidence": 0.9031018912792206}, {"text": "Pearson correlation", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.9743589460849762}]}, {"text": "\u2022 CPU time: This metric only applies to MLN and PSL.", "labels": [], "entities": [{"text": "CPU time", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.8574469983577728}]}, {"text": "The CPU time taken by the inference step is recorded and averaged overall pairs in each of the test datasets.", "labels": [], "entities": []}, {"text": "In many cases, MLN inference is very slow, so we timeout after 10 minutes and report the number of timed-out pairs on each dataset.", "labels": [], "entities": [{"text": "MLN inference", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9553543925285339}]}, {"text": "We also evaluated the effect of changing the grounding limit on both Pearson correlation and CPU time for the msr-par dataset.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 69, "end_pos": 88, "type": "METRIC", "confidence": 0.8849635720252991}, {"text": "msr-par dataset", "start_pos": 110, "end_pos": 125, "type": "DATASET", "confidence": 0.6582848578691483}]}, {"text": "Most of the sentences in msr-par are long, which results is large number of groundings, and limiting the number of groundings has a visible effect on the overall performance.", "labels": [], "entities": []}, {"text": "In the other two datasets, the sentences are fairly short, and the full number of groundings is not large; therefore, changing the grounding limit does not significantly affect the results.", "labels": [], "entities": []}, {"text": "shows the results for Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 22, "end_pos": 41, "type": "METRIC", "confidence": 0.7597563564777374}]}, {"text": "PSL out-performs the purely distributional baselines (vec-add and vec-mul) because PSL is able to combine the information available to vec-add and vec-mul in a better way that takes sentence structure into account.", "labels": [], "entities": []}, {"text": "PSL also outperforms the unaided probabilistic-logic baseline that does not use distributional information (PSL-no-DIR).", "labels": [], "entities": []}, {"text": "PSL-no-DIR works fairly well because there is significant overlap in the exact words and structure of the paired sentences in the test data, and PSL combines the evidence from these similarities effectively.", "labels": [], "entities": []}, {"text": "In addition, PSL always does significantly better than MLN, because of the large: Effect of PSL's grounding limit on the correlation score for the msr-par dataset number of timeouts, and because the conjunctionaveraging in PSL is combining evidence better than MLN's average-combiner, whose performance is sensitive to various parameters.", "labels": [], "entities": [{"text": "correlation score", "start_pos": 121, "end_pos": 138, "type": "METRIC", "confidence": 0.9640578925609589}]}, {"text": "These results further support the claim that using probabilistic logic to integrate logical and distributional information is a promising approach to natural-language semantics.", "labels": [], "entities": []}, {"text": "More specifically, they strongly indicate that PSL is a more effective probabilistic logic for judging semantic similarity than MLNs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: STS Pearson Correlations", "labels": [], "entities": [{"text": "STS Pearson Correlations", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.44523494442303974}]}, {"text": " Table 2: Average CPU time per STS pair, and  number of timed-out pairs in MLN with a 10  minute time limit. PSL's grounding limit is set to  10,000 groundings.", "labels": [], "entities": [{"text": "Average CPU time", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7649545967578888}]}]}