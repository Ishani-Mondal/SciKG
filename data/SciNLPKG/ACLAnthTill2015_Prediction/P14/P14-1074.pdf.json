{"title": [{"text": "Linguistic Structured Sparsity in Text Categorization", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization.", "labels": [], "entities": []}, {"text": "These regularizers impose linguistic bias in feature weights, enabling us to incorporate prior knowledge into conventional bag-of-words models.", "labels": [], "entities": []}, {"text": "We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis , and forecasting.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 246, "end_pos": 261, "type": "TASK", "confidence": 0.7124661505222321}, {"text": "topic classification", "start_pos": 272, "end_pos": 292, "type": "TASK", "confidence": 0.8108009994029999}, {"text": "sentiment analysis", "start_pos": 294, "end_pos": 312, "type": "TASK", "confidence": 0.9465955793857574}, {"text": "forecasting", "start_pos": 319, "end_pos": 330, "type": "TASK", "confidence": 0.964966356754303}]}], "introductionContent": [{"text": "What is the best way to exploit linguistic information in statistical text processing models?", "labels": [], "entities": []}, {"text": "For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open question, as cheap \"bag-of-words\" models often perform well.", "labels": [], "entities": [{"text": "text classification", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7678030431270599}, {"text": "sentiment analysis", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9715681970119476}, {"text": "text-driven forecasting", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.6159695982933044}]}, {"text": "Much recent work in NLP has focused on linguistic feature engineering () or representation learning.", "labels": [], "entities": [{"text": "linguistic feature engineering", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6630095144112905}, {"text": "representation learning", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.9431113004684448}]}, {"text": "In this paper, we propose a radical alternative.", "labels": [], "entities": []}, {"text": "We embrace the conventional bag-of-words representation of text, instead bringing linguistic bias to bear on regularization.", "labels": [], "entities": []}, {"text": "Since the seminal work of, the importance of regularization in discriminative models of textincluding language modeling, structured prediction, and classification-has been widely recognized.", "labels": [], "entities": [{"text": "textincluding language modeling", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.621496170759201}, {"text": "structured prediction", "start_pos": 121, "end_pos": 142, "type": "TASK", "confidence": 0.7621148526668549}]}, {"text": "The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model).", "labels": [], "entities": []}, {"text": "Recently, structured (or composite) regularization has been introduced; simply put, it reasons about different weights jointly.", "labels": [], "entities": []}, {"text": "The most widely explored variant, group lasso () seeks to avoid large 2 norms for groups of weights.", "labels": [], "entities": []}, {"text": "Group lasso has been shown useful in a range of applications, including computational biology, signal processing (), and NLP).", "labels": [], "entities": [{"text": "signal processing", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7300178408622742}]}, {"text": "For text categorization problems, proposed groups based on sentences, an idea generalized hereto take advantage of richer linguistic information.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7631604671478271}]}, {"text": "In this paper, we show how linguistic information of various kinds-parse trees, thematic topics, and hierarchical word clusterings-can be used to construct group lasso variants that impose linguistic bias without introducing any new features.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that structured regularizers can squeeze higher performance out of conventional bag-of-words models on seven out of eight of text categorization tasks tested, in six cases with more compact models than the best-performing unstructured-regularized model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use publicly available datasets to evaluate our model described in more detail below.", "labels": [], "entities": []}, {"text": "We consider four binary categorization tasks from the 20 Newsgroups dataset.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 54, "end_pos": 75, "type": "DATASET", "confidence": 0.6777935226758321}]}, {"text": "Each task involves categorizing a document according to two related categories: comp.sys: ibm.pc.hardware vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space; and alt.atheism vs. soc.religion.christian.", "labels": [], "entities": []}, {"text": "One task in sentiment analysis is predicting the polarity of apiece of text, i.e., whether the author is favorably inclined toward a (usually known) subject of discussion or proposition (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9574571847915649}]}, {"text": "Sentiment analysis, even at the coarse level of polarity we consider here, can be confused by negation, stylistic use of irony, and other linguistic phenomena.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9338833689689636}]}, {"text": "Our sentiment analysis datasets consist of movie reviews from the Stanford sentiment treebank, and floor speeches by U.S. Congressmen alongside \"yea\"/\"nay\" votes on the bill under discussion ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9062742292881012}, {"text": "Stanford sentiment treebank", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.9335196812947592}]}, {"text": "For the Stanford sentiment treebank, we only predict binary classifications (positive or negative) and exclude neutral reviews.", "labels": [], "entities": [{"text": "Stanford sentiment treebank", "start_pos": 8, "end_pos": 35, "type": "DATASET", "confidence": 0.9178080360094706}]}, {"text": "Forecasting from text requires identifying textual correlates of a response variable revealed in the future, most of which will be weak and many of which will be spurious ().", "labels": [], "entities": []}, {"text": "We consider two such problems.", "labels": [], "entities": []}, {"text": "The first one is predicting whether a scientific paper will be cited or not within three years of its publication  the dataset comes from the ACL Anthology and consists of research papers from the Association for Computational Linguistics and citation data (.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.9622306525707245}]}, {"text": "The second task is predicting whether a legislative bill will be recommended by a Congressional committee ().", "labels": [], "entities": []}, {"text": "13 summarizes statistics about the datasets used in our experiments.", "labels": [], "entities": []}, {"text": "In total, we evaluate our method on eight binary classification tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Descriptive statistics about the datasets.", "labels": [], "entities": []}, {"text": " Table 5: Classification accuracies on the 20N datasets for  lasso, ridge, and elastic net models with additional LDA fea- tures (top) and Brown cluster features (bottom). The last col- umn shows structured regularized models from Table 3.", "labels": [], "entities": [{"text": "20N datasets", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9662624299526215}]}]}