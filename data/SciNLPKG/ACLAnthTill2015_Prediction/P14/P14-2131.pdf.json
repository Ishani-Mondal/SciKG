{"title": [{"text": "Tailoring Continuous Word Representations for Dependency Parsing", "labels": [], "entities": [{"text": "Tailoring Continuous Word Representations", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5517771914601326}, {"text": "Parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.7326388359069824}]}], "abstractContent": [{"text": "Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8114201426506042}]}, {"text": "In this paper, we investigate the use of continuous word representations as features for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8646694421768188}]}, {"text": "We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains.", "labels": [], "entities": []}, {"text": "We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9635403156280518}]}, {"text": "Explicitly tailoring the representations for the task leads to further improvements.", "labels": [], "entities": []}, {"text": "Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging), named entity recognition (), chunking (, and syntactic parsing ().", "labels": [], "entities": [{"text": "partof-speech (POS) tagging", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.6443357706069947}, {"text": "named entity recognition", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.611184557278951}, {"text": "syntactic parsing", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.7583061754703522}]}, {"text": "Most word representations fall into one of two categories.", "labels": [], "entities": []}, {"text": "Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the algorithm.", "labels": [], "entities": []}, {"text": "Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models ( or spectral methods.", "labels": [], "entities": []}, {"text": "found improvement on indomain dependency parsing using features based on discrete Brown clusters.", "labels": [], "entities": [{"text": "indomain dependency parsing", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6522207061449686}]}, {"text": "In this paper, we experiment with parsing features derived from continuous representations.", "labels": [], "entities": []}, {"text": "We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing.", "labels": [], "entities": []}, {"text": "We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy.", "labels": [], "entities": []}, {"text": "We compare several types of continuous representations, including those made available by other researchers, and embeddings we have trained using the approach of, which is orders of magnitude faster than the others.", "labels": [], "entities": []}, {"text": "The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation.", "labels": [], "entities": []}, {"text": "We report significant improvements over our baseline on both the Penn Treebank (PTB;) and the English Web treebank.", "labels": [], "entities": [{"text": "Penn Treebank (PTB", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.971528023481369}, {"text": "English Web treebank", "start_pos": 94, "end_pos": 114, "type": "DATASET", "confidence": 0.9668441017468771}]}, {"text": "While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses.", "labels": [], "entities": []}, {"text": "To this end, we use two simple modifications to the models of: a smaller context window, and conditioning on syntactic context (dependency links and labels).", "labels": [], "entities": []}, {"text": "Interestingly, the Brown clusters of prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on all test sets) in a fraction of the training time.", "labels": [], "entities": []}, {"text": "Finally, a simple parser ensemble on all the representations achieves the best results, suggesting their complementarity for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7900724112987518}]}], "datasetContent": [{"text": "Short of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks?", "labels": [], "entities": [{"text": "parsing tasks", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.9075093567371368}]}, {"text": "Several methods have been proposed for intrinsic evaluation of word representa-  tions; we discuss two here: Word similarity (SIM): One widely-used evaluation compares distances in the continuous space to human judgments of word similarity using the 353-pair dataset of.", "labels": [], "entities": [{"text": "Word similarity (SIM)", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.5412431716918945}]}, {"text": "We compute cosine similarity between the two vectors in each word pair, then order the word pairs by similarity and compute Spearman's rank correlation coefficient (\u03c1) with the gold similarities.", "labels": [], "entities": [{"text": "rank correlation coefficient (\u03c1)", "start_pos": 135, "end_pos": 167, "type": "METRIC", "confidence": 0.85967917740345}]}, {"text": "Embeddings with high \u03c1 capture similarity in terms of paraphrase and topical relationships.", "labels": [], "entities": []}, {"text": "Clustering-based tagging accuracy (M-1): Intuitively, we expect embeddings to help parsing the most if they can tell us when two words are similar syntactically.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.7943545579910278}]}, {"text": "To this end, we use a metric based on unsupervised evaluation of POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.6024594902992249}]}, {"text": "We perform clustering and map each cluster to one POS tag so as to maximize tagging accuracy, where multiple clusters can map to the same tag.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.981921911239624}]}, {"text": "We cluster vectors corresponding to the tokens in PTB WSJ sections 00-21. 3 shows these metrics for representations used in this paper.", "labels": [], "entities": [{"text": "PTB WSJ sections 00-21.", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.932518869638443}]}, {"text": "The BROWN clusters have the highest M-1, indicating high cluster purity in terms of POS tags.", "labels": [], "entities": [{"text": "BROWN", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.7976410388946533}, {"text": "M-1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.991187572479248}]}, {"text": "The HUANG embeddings have the highest SIM score but low M-1, presumably because they were trained with global context, making them more tuned to capture topical similarity.", "labels": [], "entities": [{"text": "SIM score", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.8293553590774536}, {"text": "M-1", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9876052141189575}]}, {"text": "We compare several values for the window size (w) used when training the SKIP embeddings, finding that small w leads to higher M-1 and lower SIM.", "labels": [], "entities": [{"text": "M-1", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.8783942461013794}, {"text": "SIM", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9003637433052063}]}, {"text": "shows examples of clusters obtained by clustering SKIP embeddings of w = 1 versus w = 10, and we see that the former correspond closely to POS tags, while the latter are: Example clusters for SKIP embeddings with window size w = 1 (syntactic) and w = 10 (topical).", "labels": [], "entities": []}, {"text": "much more topically-coherent and contain mixed POS tags.", "labels": [], "entities": []}, {"text": "For parsing experiments, we choose w = 2 for CBOW and w = 1 for SKIP.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9784438610076904}, {"text": "CBOW", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.8399012684822083}, {"text": "SKIP", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.8727830648422241}]}, {"text": "Finally, our SKIP DEP embeddings, trained with syntactic context and w = 1 ( \u00a72.1.2), achieve the highest M-1 of all continuous representations.", "labels": [], "entities": [{"text": "SKIP DEP", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.46911488473415375}]}, {"text": "In \u00a74, we will relate these intrinsic metrics to extrinsic parsing performance.", "labels": [], "entities": [{"text": "extrinsic parsing", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6305243372917175}]}, {"text": "Setup: We use the publicly-available MSTParser for all experiments, specifically its secondorder projective model.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9403994083404541}]}, {"text": "We remove all features that occur only once in the training data.", "labels": [], "entities": []}, {"text": "For WSJ parsing, we use the standard train(02-21)/dev(22)/test(23) split and apply the NP bracketing patch by.", "labels": [], "entities": [{"text": "WSJ parsing", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7883034348487854}, {"text": "NP bracketing", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.5977733880281448}]}, {"text": "For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the 'English Web Treebank' (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.", "labels": [], "entities": [{"text": "Web parsing", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7976032793521881}, {"text": "WSJ 02-21", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.9744406938552856}, {"text": "English Web Treebank' (LDC2012T13)", "start_pos": 136, "end_pos": 170, "type": "DATASET", "confidence": 0.942814826965332}]}, {"text": "For both treebanks, we convert from constituent to dependency format using pennconverter, and generate POS tags using the MXPOST tagger.", "labels": [], "entities": [{"text": "MXPOST tagger", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.8644482791423798}]}, {"text": "To evaluate, we use Our bucketing function bucket k (x) converts the real value x to its closest multiple of k.", "labels": [], "entities": []}, {"text": "We choose a k value of around 1/5th of the embedding's absolute range.", "labels": [], "entities": []}, {"text": "We initially experimented directly with real-valued features (instead of bucketed indicator features) and similar conjunction variants, but these did not perform well.", "labels": [], "entities": []}, {"text": "We use prefixes of length 4, 6, 8, 12, 16, 20, and fulllength, again tuned on the development set.", "labels": [], "entities": []}, {"text": "We use the recommended MSTParser settings: trainingk:5 iters:10 loss-type:nopunc decode-type:proj Our setup is different from SANCL 2012  unlabeled attachment score (UAS).", "labels": [], "entities": [{"text": "trainingk", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.8743144273757935}, {"text": "SANCL 2012  unlabeled attachment score (UAS)", "start_pos": 126, "end_pos": 170, "type": "METRIC", "confidence": 0.6495545655488968}]}, {"text": "We report statistical significance (p < 0.01, 100K samples) using the bootstrap test.", "labels": [], "entities": [{"text": "significance", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.6260342597961426}]}, {"text": "Comparing bucket and bit string features: In, we find that bucket features based on individual embedding dimensions do not lead to improvements in test accuracy, while bit string features generally do.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9837287068367004}]}, {"text": "This is likely because individual embedding dimensions rarely correspond to interpretable or useful distinctions among words, whereas the hierarchical bit strings take into account all dimensions of the representations simultaneously.", "labels": [], "entities": []}, {"text": "Their prefixes also naturally define features at multiple levels of granularity.", "labels": [], "entities": []}, {"text": "WSJ results: Web results: shows our main Web results.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9183433055877686}]}, {"text": "Here, we see that the SENNA, BROWN, and SKIP DEP embeddings perform the best on average (and are statistically indistinguishable, except SENNA vs. SKIP DEP on the reviews domain).", "labels": [], "entities": [{"text": "BROWN", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.8080625534057617}]}, {"text": "They yield statistically significant UAS improvements over the baseline across all domains, except weblog for SENNA (narrowly misses significance, p=0.014) and email for SKIP DEP . Ensemble results: When analyzing errors, we see differences among the representations, e.g., BROWN does better at attaching proper nouns, prepositions, and conjunctions, while CBOW does better on plural common nouns and adverbs.", "labels": [], "entities": [{"text": "UAS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.6514902114868164}, {"text": "SKIP DEP", "start_pos": 170, "end_pos": 178, "type": "TASK", "confidence": 0.5423935353755951}, {"text": "BROWN", "start_pos": 274, "end_pos": 279, "type": "METRIC", "confidence": 0.8939212560653687}]}, {"text": "This suggests that the representations might be complementary and could benefit from combination.", "labels": [], "entities": []}, {"text": "To test this, we use a simple ensemble parser that chooses the highest voted parent for each argument.", "labels": [], "entities": []}, {"text": "As shown in the last two rows of Tables 5 and 6, this leads to substantial gains.", "labels": [], "entities": []}, {"text": "The 'ALL -BROWN' ensemble combines votes from all non-BROWN continuous representations, and the 'ALL' ensemble also includes BROWN.", "labels": [], "entities": [{"text": "BROWN", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.8312292695045471}]}, {"text": "Characteristics of representations: We now relate the intrinsic metrics from \u00a72.2 to parsing performance.", "labels": [], "entities": []}, {"text": "The clearest correlation appears when comparing variations of a single model, e.g., for SKIP, the WSJ dev accuracies are 93.33 (SKIP DEP ), 92.94 (w = 1), 92.86 (w = 5), and 92.70 (w = 10), which matches the M-1 score order and is the reverse of the SIM score order.", "labels": [], "entities": [{"text": "WSJ dev accuracies", "start_pos": 98, "end_pos": 116, "type": "METRIC", "confidence": 0.6242895921071371}, {"text": "SKIP DEP )", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.6349888245264689}, {"text": "SIM score order", "start_pos": 250, "end_pos": 265, "type": "METRIC", "confidence": 0.7910820643107096}]}], "tableCaptions": [{"text": " Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous  representations require an additional 4 hours to run hierarchical clustering to generate features ( \u00a73.2). RCV1 = Reuters Corpus,  Volume 1.  *  = time reported by authors.  \u2020 = run by us on a 3.50 GHz desktop, using a single thread.", "labels": [], "entities": [{"text": "Reuters Corpus", "start_pos": 236, "end_pos": 250, "type": "DATASET", "confidence": 0.9072731733322144}]}, {"text": " Table 2: Intrinsic evaluation of representations. SIM column  has Spearman's \u03c1 \u00d7 100 for 353-pair word similarity dataset.  M-1 is our unsupervised POS tagging metric. For BROWN,  M-1 is simply many-to-one accuracy of the clusters. Best  score in each column is bold.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.684167355298996}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9836030006408691}]}, {"text": " Table 4: Bucket vs. bit string features (UAS on WSJ).", "labels": [], "entities": [{"text": "UAS", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9554065465927124}, {"text": "WSJ", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8867927193641663}]}, {"text": " Table 5: Full results with bit string features (UAS on WSJ).", "labels": [], "entities": [{"text": "UAS", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9566188454627991}, {"text": "WSJ", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8298725485801697}]}, {"text": " Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.", "labels": [], "entities": [{"text": "BR", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9944281578063965}, {"text": "BROWN", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.579833984375}, {"text": "Avg", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.993304431438446}]}]}