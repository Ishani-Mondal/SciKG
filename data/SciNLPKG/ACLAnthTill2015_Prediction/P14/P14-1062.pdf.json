{"title": [{"text": "A Convolutional Neural Network for Modelling Sentences", "labels": [], "entities": []}], "abstractContent": [{"text": "The ability to accurately represent sentences is central to language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7089520841836929}]}, {"text": "We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences.", "labels": [], "entities": []}, {"text": "The network uses Dynamic k-Max Pooling , a global pooling operation over linear sequences.", "labels": [], "entities": [{"text": "Dynamic k-Max Pooling", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.544275571902593}]}, {"text": "The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations.", "labels": [], "entities": []}, {"text": "The network does not rely on a parse tree and is easily applicable to any language.", "labels": [], "entities": []}, {"text": "We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction , six-way question classification and Twitter sentiment prediction by distant supervision.", "labels": [], "entities": [{"text": "multi-class sentiment prediction", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.695087711016337}, {"text": "six-way question classification", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.5727305809656779}, {"text": "Twitter sentiment prediction", "start_pos": 132, "end_pos": 160, "type": "TASK", "confidence": 0.6777765949567159}]}, {"text": "The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 91, "end_pos": 106, "type": "METRIC", "confidence": 0.980544239282608}]}], "introductionContent": [{"text": "The aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation.", "labels": [], "entities": [{"text": "classification or generation", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.696755717198054}]}, {"text": "The sentence modelling problem is at the core of many tasks involving a degree of natural language comprehension.", "labels": [], "entities": [{"text": "sentence modelling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8310295939445496}]}, {"text": "These tasks include sentiment analysis, paraphrase detection, entailment recognition, summarisation, discourse analysis, machine translation, grounded language learning and image retrieval.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.971286952495575}, {"text": "paraphrase detection", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.9315624833106995}, {"text": "entailment recognition", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8030710816383362}, {"text": "summarisation", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.9861374497413635}, {"text": "discourse analysis", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7828643321990967}, {"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7974457740783691}, {"text": "image retrieval", "start_pos": 173, "end_pos": 188, "type": "TASK", "confidence": 0.7545033395290375}]}, {"text": "Since individual sentences are rarely observed or not observed at all, one must represent a sentence in terms of features that depend on the words and short n-grams in the sentence that are frequently observed.", "labels": [], "entities": []}, {"text": "The core of a sentence model involves a feature function that defines the process The cat sat on the red mat The cat sat on the red mat: Subgraph of a feature graph induced over an input sentence in a Dynamic Convolutional Neural Network.", "labels": [], "entities": []}, {"text": "The full induced graph has multiple subgraphs of this kind with a distinct set of edges; subgraphs may merge at different layers.", "labels": [], "entities": []}, {"text": "The left diagram emphasises the pooled nodes.", "labels": [], "entities": []}, {"text": "The width of the convolutional filters is 3 and 2 respectively.", "labels": [], "entities": []}, {"text": "With dynamic pooling, a filter with small width at the higher layers can relate phrases far apart in the input sentence. by which the features of the sentence are extracted from the features of the words or n-grams.", "labels": [], "entities": []}, {"text": "Various types of models of meaning have been proposed.", "labels": [], "entities": []}, {"text": "Composition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases.", "labels": [], "entities": []}, {"text": "In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors.", "labels": [], "entities": []}, {"text": "In other cases, a composition function is learned and either tied to particular syntactic relations) or to particular word types (.", "labels": [], "entities": []}, {"text": "Another approach represents the meaning of sentences byway of automatically extracted logical forms.", "labels": [], "entities": []}, {"text": "A central class of models are those based on neural networks.", "labels": [], "entities": []}, {"text": "These range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations.", "labels": [], "entities": []}, {"text": "Neural sentence models have a number of advantages.", "labels": [], "entities": []}, {"text": "They can be trained to obtain generic vectors for words and phrases by predicting, for instance, the contexts in which the words and phrases occur.", "labels": [], "entities": []}, {"text": "Through supervised training, neural sentence models can fine-tune these vectors to information that is specific to a certain task.", "labels": [], "entities": []}, {"text": "Besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word).", "labels": [], "entities": []}, {"text": "We define a convolutional neural network architecture and apply it to the semantic modelling of sentences.", "labels": [], "entities": []}, {"text": "The network handles input sequences of varying length.", "labels": [], "entities": []}, {"text": "The layers in the network interleave one-dimensional convolutional layers and dynamic k-max pooling layers.", "labels": [], "entities": []}, {"text": "Dynamic k-max pooling is a generalisation of the max pooling operator.", "labels": [], "entities": []}, {"text": "The max pooling operator is a non-linear subsampling function that returns the maximum of a set of values ().", "labels": [], "entities": []}, {"text": "The operator is generalised in two respects.", "labels": [], "entities": []}, {"text": "First, kmax pooling over a linear sequence of values returns the subsequence of k maximum values in the sequence, instead of the single maximum value.", "labels": [], "entities": [{"text": "kmax pooling", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.9547616541385651}]}, {"text": "Secondly, the pooling parameter k can be dynamically chosen by making k a function of other aspects of the network or the input.", "labels": [], "entities": []}, {"text": "The convolutional layers apply onedimensional filters across each row of features in the sentence matrix.", "labels": [], "entities": []}, {"text": "Convolving the same filter with the n-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence.", "labels": [], "entities": []}, {"text": "A convolutional layer followed by a dynamic pooling layer and a non-linearity form a feature map.", "labels": [], "entities": []}, {"text": "Like in the convolutional networks for object recognition (), we enrich the representation in the first layer by computing multiple feature maps with different filters applied to the input sentence.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7677791714668274}]}, {"text": "Subsequent layers also have multiple feature maps computed by convolving filters with all the maps from the layer below.", "labels": [], "entities": []}, {"text": "The weights at these layers form an order-4 tensor.", "labels": [], "entities": []}, {"text": "The resulting architecture is dubbed a Dynamic Convolutional Neural Network.", "labels": [], "entities": []}, {"text": "Multiple layers of convolutional and dynamic pooling operations induce a structured feature graph over the input sentence.", "labels": [], "entities": []}, {"text": "Small filters at higher layers can capture syntactic or semantic relations between noncontinuous phrases that are far apart in the input sentence.", "labels": [], "entities": []}, {"text": "The feature graph induces a hierarchical structure somewhat akin to that in a syntactic parse tree.", "labels": [], "entities": []}, {"text": "The structure is not tied to purely syntactic relations and is internal to the neural network.", "labels": [], "entities": []}, {"text": "We experiment with the network in four settings.", "labels": [], "entities": []}, {"text": "The first two experiments involve predicting the sentiment of movie reviews).", "labels": [], "entities": [{"text": "predicting the sentiment of movie reviews", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.884213387966156}]}, {"text": "The network outperforms other approaches in both the binary and the multi-class experiments.", "labels": [], "entities": []}, {"text": "The third experiment involves the categorisation of questions in six question types in the TREC dataset ().", "labels": [], "entities": [{"text": "TREC dataset", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9104181826114655}]}, {"text": "The network matches the accuracy of other state-of-theart methods that are based on large sets of engineered features and hand-coded knowledge resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9991432428359985}]}, {"text": "The fourth experiment involves predicting the sentiment of Twitter posts using distant supervision ().", "labels": [], "entities": [{"text": "predicting the sentiment of Twitter posts", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.8682696024576823}]}, {"text": "The network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs in them.", "labels": [], "entities": []}, {"text": "On the hand-labelled test set, the network achieves a greater than 25% reduction in the prediction error with respect to the strongest unigram and bigram baseline reported in.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the background to the DCNN including central concepts and related neural sentence models.", "labels": [], "entities": []}, {"text": "Section 3 defines the relevant operators and the layers of the network.", "labels": [], "entities": []}, {"text": "Section 4 treats of the induced feature graph and other properties of the network.", "labels": [], "entities": []}, {"text": "Section 5 discusses the experiments and inspects the learnt feature detectors.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the network on four different experiments.", "labels": [], "entities": []}, {"text": "We begin by specifying aspects of the implementation and the training of the network.", "labels": [], "entities": []}, {"text": "We then relate the results of the experiments and we inspect the learnt feature detectors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of sentiment prediction in the  movie reviews dataset. The first four results are  reported from Socher et al. (2013b). The baselines  NB and BINB are Naive Bayes classifiers with,  respectively, unigram features and unigram and bi- gram features. SVM is a support vector machine  with unigram and bigram features. RECNTN is a  recursive neural network with a tensor-based fea- ture function, which relies on external structural  features given by a parse tree and performs best  among the RecNNs.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9433362185955048}, {"text": "movie reviews dataset", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.6288572549819946}, {"text": "BINB", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9857262372970581}]}]}