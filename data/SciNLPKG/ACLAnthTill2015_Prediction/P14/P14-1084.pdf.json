{"title": [{"text": "Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.7084857821464539}]}], "abstractContent": [{"text": "ive text summarization of news requires away of representing events, such as a collection of pattern clusters in which every cluster represents an event (e.g., marriage) and every pattern in the cluster is away of expressing the event (e.g., X married Y, X and Y tied the knot).", "labels": [], "entities": [{"text": "summarization of news", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.8291430473327637}]}, {"text": "We compare three ways of extracting event patterns: heuristics-based, compression-based and memory-based.", "labels": [], "entities": []}, {"text": "While the former has been used previously in multi-document abstraction, the latter two have never been used for this task.", "labels": [], "entities": []}, {"text": "Compared with the first two techniques, the memory-based method allows for generating significantly more grammatical and informative sentences, at the cost of searching avast space of hundreds of millions of parse trees of known grammatical utterances.", "labels": [], "entities": []}, {"text": "To this end, we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text summarization beyond extraction requires a semantic representation that abstracts away from words and phrases and from which a summary can be generated.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7582019865512848}]}, {"text": "Following and extending recent work in semantic parsing, information extraction (IE), paraphrase generation and summarization, the representation we consider in this paper is a large collec- * Work done during an internship at Google Zurich.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7646749019622803}, {"text": "information extraction (IE)", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.8514472067356109}, {"text": "paraphrase generation", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.9394891262054443}, {"text": "summarization", "start_pos": 112, "end_pos": 125, "type": "TASK", "confidence": 0.9772243499755859}]}, {"text": "[John Smith] and wed in...", "labels": [], "entities": []}, {"text": "[Smith] tied the knot with this Monday...", "labels": [], "entities": []}, {"text": "tion of clusters of event patterns.", "labels": [], "entities": []}, {"text": "An abstractive summarization system relying on such a representation proceeds by (1) detecting the most relevant event cluster fora given sentence or sentence collection, and (2) using the most representative pattern from the cluster to generate a concise summary sentence.", "labels": [], "entities": []}, {"text": "illustrates the summarization architecture we are assuming in this paper.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9379172325134277}]}, {"text": "Given input text(s) with resolved and typed entity mentions, event mentions and the most relevant event cluster are detected (first arrow).", "labels": [], "entities": []}, {"text": "Then, a summary sentence is generated from the event and entity representations (second arrow).", "labels": [], "entities": []}, {"text": "However, the utility of such a representation for summarization depends on the quality of pattern clusters.", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9804946184158325}]}, {"text": "In particular, event patterns must correspond to grammatically correct sentences.", "labels": [], "entities": []}, {"text": "Introducing an incomplete or incomprehensible pattern (e.g., PER said PER) may negatively affect both event detection and sentence generation.", "labels": [], "entities": [{"text": "event detection", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.7455017566680908}, {"text": "sentence generation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7481942474842072}]}, {"text": "Related work on paraphrase detection and relation extraction is mostly heuristics-based and has relied on hand-crafted rules to collect such patterns (see.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9547042548656464}, {"text": "relation extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8989452421665192}]}, {"text": "A standard approach is to focus on binary relations between entities and extract the dependency path between the two entities as an event representation.", "labels": [], "entities": []}, {"text": "An obvious limitation of this approach is there is no guarantee that the extracted pattern corresponds to a grammatically correct sentence, e.g., that an essential prepositional phrase is retained like in file fora divorce.", "labels": [], "entities": []}, {"text": "In this paper we explore two novel, data-driven methods for event pattern extraction.", "labels": [], "entities": [{"text": "event pattern extraction", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7155793706576029}]}, {"text": "The first, compression-based method uses a robust sentence compressor with an aggressive compression rate to get to the core of the sentence (Sec. 3).", "labels": [], "entities": []}, {"text": "The second, memory-based method relies on avast collection of human-written headlines and sentences to find a substructure which is known to be grammatically correct.", "labels": [], "entities": []}, {"text": "While the latter method comes closer to ensuring perfect grammaticality, it introduces a problem of efficiently searching the vast space of known well-formed patterns.", "labels": [], "entities": []}, {"text": "Since standard iterative approaches comparing every pattern with every sentence are prohibitive here, we present a search strategy which scales well to huge collections (hundreds of millions) of sentences.", "labels": [], "entities": []}, {"text": "In order to evaluate the three methods, we consider an abstractive summarization task where the goal is to get the gist of single sentences by recognizing the underlying event and generating a short summary sentence.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first time that this task has been proposed; it can be considered as abstractive sentence compression, in contrast to most existing sentence compression systems which are based on selecting words from the original sentence or rewriting with simpler paraphrase tables.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 111, "end_pos": 143, "type": "TASK", "confidence": 0.6174468000729879}, {"text": "sentence compression", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.7497459053993225}]}, {"text": "An extensive evaluation with human raters demonstrates the utility of the new pattern extraction techniques.", "labels": [], "entities": [{"text": "pattern extraction", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.763712078332901}]}, {"text": "Our analysis highlights advantages and disadvantages of the three methods.", "labels": [], "entities": []}, {"text": "To better isolate the qualities of the three extraction methodologies, all three methods use the same training data and share components of the Algorithm 1 HEURISTICEXTRACTOR(T, E): heuristically extract relational patterns for the dependency parse T and the set of entities E.", "labels": [], "entities": [{"text": "HEURISTICEXTRACTOR", "start_pos": 156, "end_pos": 174, "type": "METRIC", "confidence": 0.9804838299751282}]}, {"text": "1: /* Global constants /* 2: global Vp, Vc, Np, Nc 3: Vc \u2190 {subj, nsubj, nsubjpass, dobj, iobj, xcomp, 4: acomp, expl, neg, aux, attr, prt} 5: Vp \u2190 {xcomp} 6: Nc \u2190 {det, predet, num, ps, poss, nc, conj} 7: Np \u2190 {ps, poss, subj, nsubj, nsubjpass, dobj, iobj} 8: /* Entry point /* 9: P \u2190 \u2205 10: for all C \u2208 COMBINATIONS(E) do 11: N \u2190 MENTIONNODES(T, C) 12: N \u2190 APPLYHEURISTICS(T, BUILDMST(T, N )) for all n \u2208 N do 21: if n.ISVERB() then 22: return {n} 37: else return \u2205 very same summarization architecture, as shown in: an event model is constructed by clustering the patterns extracted according to the selected extraction method.", "labels": [], "entities": [{"text": "BUILDMST", "start_pos": 377, "end_pos": 385, "type": "METRIC", "confidence": 0.95123291015625}]}, {"text": "Then, the same extraction method is used to collect patterns from sentences in never-seen-before news articles.", "labels": [], "entities": []}, {"text": "Finally, the patterns are used to query the event model and generate an abstractive summary.", "labels": [], "entities": []}, {"text": "The three different pattern extractors are detailed in the next three sections.", "labels": [], "entities": [{"text": "pattern extractors", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.720130980014801}]}], "datasetContent": [{"text": "All the models for the experiments that we present have been trained using the same corpus of news crawled from the web between 2008 and 2013.", "labels": [], "entities": []}, {"text": "The news have been processed with a tokenizer, a sentence splitter (), a part-of-speech tagger and dependency parser), a co-reference resolution module and an entity linker based on.", "labels": [], "entities": [{"text": "sentence splitter", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7183397114276886}]}, {"text": "We use Freebase types as finegrained named entity types, so we are also able to label e.g. instances of sports teams as such instead of the coarser label ORG.", "labels": [], "entities": []}, {"text": "Next, the news have been grouped based on temporal closeness and cosine similarity (using tf\u00b7idf weights).", "labels": [], "entities": []}, {"text": "For each of the three pattern extraction methods we used the same summarization pipeline (as shown above in): 1.", "labels": [], "entities": [{"text": "pattern extraction", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7354230731725693}]}, {"text": "Run pattern extraction on the news.", "labels": [], "entities": [{"text": "pattern extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7671169936656952}]}, {"text": "2. For every news collection Coll and entity set E, generate a set containing all the extracted patterns from news in Coll mentioning all the entities in E.", "labels": [], "entities": []}, {"text": "These are patterns that are likely to be paraphrasing each other.", "labels": [], "entities": []}, {"text": "3. Run a clustering algorithm to group together patterns that typically co-occur in the sets generated in the previous step.", "labels": [], "entities": []}, {"text": "There are many choices for clustering algorithms Zodiac Aerospace posted a 7.9 percent rise in first-quarter revenue, below market expectations, but reaffirmed its full-year financial targets.", "labels": [], "entities": []}, {"text": "Zodiac Aerospace has reported arise in profits.", "labels": [], "entities": [{"text": "Zodiac Aerospace", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8227666616439819}]}, {"text": "(C) Australian free-agent closer Grant Balfour has agreed to terms with the Baltimore Orioles on a two-year deal, the Baltimore Sun reported on Tuesday citing multiple industry sources.", "labels": [], "entities": []}, {"text": "Balfour will join the Baltimore Orioles.   and pattern cluster c i , the network provides p(p j |c i ) -the probability that c i will generate p j -and p(c i |p j ) -the probability that, given a pattern p j , c i was the hidden event that generated it.", "labels": [], "entities": []}, {"text": "At generation time we proceed in the following way: 1.", "labels": [], "entities": []}, {"text": "Given the title or first sentence of a news article, run the same pattern extraction method that was used in training and, if possible, obtain a pattern p involving some entities.", "labels": [], "entities": [{"text": "pattern extraction", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7599109709262848}]}, {"text": "2. Find the model clusters that contain this pattern, C p = {c i such that p(c i |p) > 0}.", "labels": [], "entities": []}, {"text": "3. Return a ranked list of model patterns output = {(p j , score(p j ))}, scored as follows: where p was the input pattern.", "labels": [], "entities": []}, {"text": "4. Replace the entity placeholders in the topscored patterns p j with the entities that were actually mentioned in the input news article.", "labels": [], "entities": []}, {"text": "In all cases the parameters of the network were predefined as 20,000 nodes in the hidden layer (model clusters) and 40 Expectation Maximization (EM) training iterations.", "labels": [], "entities": []}, {"text": "Training was distributed across 20 machines with 10 GB of memory each.", "labels": [], "entities": []}, {"text": "For testing we used 37,584 news crawled during December 2013, which had not been used for training the models.", "labels": [], "entities": []}, {"text": "shows one pattern cluster example from each of the three trained models.", "labels": [], "entities": []}, {"text": "The table shows only the surface form of the pattern for simplicity.", "labels": [], "entities": []}, {"text": "Pattern cluster (MEMORY-BASED) organization1 gets organization0 nod for drug organization1 gets organization0 nod for tablets organization0 approves organization1 drug organizations0 approves organization1 's drug organization1 gets organization0 nod for capsules Pattern cluster (HEURISTIC) organization0 to buy organization1 organization0 to acquire organization1 organization0 buys organization1 organization0 acquires organization1 organization0 to acquire organizations1 organization0 buys organizations1 organization0 acquires organizations1 organization0 agrees to buy organization1 organization0 snaps up organization1 organization0 to purchase organizations1 organization0 is to acquire organization1 organization0 has agreed to buy organization1 organization0 announces acquisition of organizations1 organization0 may bid for organization1 organization1 sold to organization0 organization1 acquired by organization0 Pattern cluster (COMPRESSION) the sports team1 have acquired person0 from the sports team2 the sports team1 acquired person0 from the sports team2 the sports team2 have traded person0 to the sports team1 sports team1 acquired the rights to person0 from sports team2 sports team2 acquired from sports team1 in exchange for person0 sports team2 have acquired from the sports team1 in exchange for person0: Examples of pattern clusters.", "labels": [], "entities": []}, {"text": "In each cluster c i , patterns are sorted by p(p j |c i ).", "labels": [], "entities": []}, {"text": "shows the number of extracted patterns from the test set, and the number of abstractive event descriptions produced.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Abstraction examples from compression (C), heuristic (H) and memory-based (M) patterns.", "labels": [], "entities": [{"text": "Abstraction", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9636626243591309}]}, {"text": " Table 2: Patterns extracted in each method, before  Noisy-OR inference.", "labels": [], "entities": []}, {"text": " Table 4: Results for the three methods when rating  the top-ranked abstraction.", "labels": [], "entities": []}, {"text": " Table 5: Sources of errors for the top 25 items with high readability and low informativeness.", "labels": [], "entities": []}]}