{"title": [{"text": "Encoding Relation Requirements for Relation Extraction via Joint Inference", "labels": [], "entities": [{"text": "Encoding Relation Requirements", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7617299954096476}, {"text": "Relation Extraction via Joint Inference", "start_pos": 35, "end_pos": 74, "type": "TASK", "confidence": 0.6957659065723419}]}], "abstractContent": [{"text": "Most existing relation extraction models make predictions for each entity pair locally and individually, while ignoring implicit global clues available in the knowledge base, sometimes leading to conflicts among local predictions from different entity pairs.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7353226691484451}]}, {"text": "In this paper, we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions.", "labels": [], "entities": []}, {"text": "We exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation.", "labels": [], "entities": []}, {"text": "Experimental results on three datasets, in both English and Chinese, show that our framework outperforms the state-of-the-art relation extraction models when such clues are applicable to the datasets.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7415979206562042}]}, {"text": "And, we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying predefined kinds of relationship between pairs of entities is crucial for many knowledge base related applications).", "labels": [], "entities": []}, {"text": "In the literature, relation extraction (RE) is usually investigated in a classification style, where relations are simply treated as isolated class labels, while their definitions or background information are sometimes ignored.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.934205973148346}]}, {"text": "Take the relation Capital as an example, we can imagine that this relation will expect a country as its subject and a city as object, and inmost cases, a city can be the capital of only one country.", "labels": [], "entities": []}, {"text": "All these clues are no doubt helpful, for instance,  explicitly modeled the expected types of a relation's arguments with the help of Freebase's type taxonomy and obtained promising results for RE.", "labels": [], "entities": [{"text": "RE", "start_pos": 194, "end_pos": 196, "type": "TASK", "confidence": 0.71468585729599}]}, {"text": "* Yansong Feng is the corresponding author.", "labels": [], "entities": []}, {"text": "However, properly capturing and utilizing such typing clues are not trivial.", "labels": [], "entities": []}, {"text": "One of the hurdles here is the lack of off-the-shelf resources and such clues often have to be coded by human experts.", "labels": [], "entities": []}, {"text": "Many knowledge bases do not have a well-defined typing system, let alone fine-grained typing taxonomies with corresponding type recognizers, which are crucial to explicitly model the typing requirements for arguments of a relation, but rather expensive and time-consuming to collect.", "labels": [], "entities": []}, {"text": "Similarly, the cardinality requirements of arguments, e.g., a person can have only one birthdate and a city can only be labeled as capital of one country, should be considered as a strong indicator to eliminate wrong predictions, but has to be coded manually as well.", "labels": [], "entities": []}, {"text": "On the other hand, most previous relation extractors process each entity pair (we will use entity pair and entity tuple exchangeably in the rest of the paper) locally and individually, i.e., the extractor makes decisions solely based on the sentences containing the current entity pair and ignores other related pairs, therefore has difficulties to capture possible disagreements among different entity pairs.", "labels": [], "entities": []}, {"text": "However, when looking at the output of a multi-class relation predictor globally, we can easily find possible incorrect predictions such as a university locates in two different cities, two different cities have been labeled as capital for one country, a country locates in a city and soon.", "labels": [], "entities": []}, {"text": "In this paper, we will address how to derive and exploit two categories of these clues: the expected types and the cardinality requirements of a relation's arguments, in the scenario of relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.8268467485904694}]}, {"text": "We propose to perform joint inference upon multiple local predictions by leveraging implicit clues that are encoded with relation specific requirements and can be learnt from existing knowledge bases.", "labels": [], "entities": []}, {"text": "Specifically, the joint inference framework operates on the output of a sentence level relation extractor as input, derives 5 types of constraints from an existing KB to implicitly capture the expected type and cardinality requirements fora relation's arguments, and jointly resolve the disagreements among candidate predictions.", "labels": [], "entities": []}, {"text": "We formalize this procedure as a constrained optimization problem, which can be solved by many optimization frameworks.", "labels": [], "entities": []}, {"text": "We use integer linear programming (ILP) as the solver and evaluate our framework on English and Chinese datasets.", "labels": [], "entities": [{"text": "solver", "start_pos": 47, "end_pos": 53, "type": "TASK", "confidence": 0.9618942737579346}]}, {"text": "The experimental results show that our framework performs better than the state-of-the-art approaches when such clues are applicable to the datasets.", "labels": [], "entities": []}, {"text": "We also show that the automatically learnt clues perform comparably to those refined manually.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail.", "labels": [], "entities": []}, {"text": "Experimental setup and results are discussed in Section 4.", "labels": [], "entities": []}, {"text": "We conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach on three datasets, including two English datasets and one Chinese dataset.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7250252962112427}, {"text": "Chinese dataset", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.7393933087587357}]}, {"text": "The first English dataset, Riedel's dataset, is the one used in (, with the same split.", "labels": [], "entities": [{"text": "English dataset, Riedel's dataset", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.6897036035855612}]}, {"text": "It uses Freebase as the knowledge base and New York Time corpus as the text corpus, including about 60,000 entity tuples in the training set, and about 90,000 entity tuples in the testing set.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9723552465438843}, {"text": "New York Time corpus", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.8817019760608673}]}, {"text": "We generate the second English dataset, DBpedia dataset, by mapping the triples in DBpedia () to the sentences in New York Time corpus.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.7230812311172485}, {"text": "DBpedia dataset", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9208731949329376}, {"text": "New York Time corpus", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.8854738175868988}]}, {"text": "We map 51 different relations to the corpus and result in about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 sentences for testing.", "labels": [], "entities": []}, {"text": "For the Chinese dataset, we derive knowledge facts and construct a Chinese KB from the Infoboxes of HudongBaike, one of the largest Chinese online encyclopedias.", "labels": [], "entities": [{"text": "Chinese dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.6770995110273361}, {"text": "HudongBaike", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.7958230376243591}]}, {"text": "We collect four national economic newspapers in 2009 as our corpus.", "labels": [], "entities": []}, {"text": "28 different relations are mapped to the corpus and this results in 60,000 entity tuples, 120,000 sentences for training and 40,000 tuples, 83,000 sentences for testing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of the improvements made by ILP  in the DBpedia and Chinese datasets.", "labels": [], "entities": [{"text": "DBpedia and Chinese datasets", "start_pos": 58, "end_pos": 86, "type": "DATASET", "confidence": 0.7640225738286972}]}, {"text": " Table 2: Results of the highest F1 score on all three datasets.  DBpedia  Riedel  Chinese", "labels": [], "entities": [{"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9822494983673096}, {"text": "DBpedia  Riedel  Chinese", "start_pos": 66, "end_pos": 90, "type": "DATASET", "confidence": 0.9443063537279764}]}]}