{"title": [{"text": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning", "labels": [], "entities": [{"text": "Resolving Lexical Ambiguity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9129964510599772}]}], "abstractContent": [{"text": "This paper provides a method for improving tensor-based compositional distribu-tional models of meaning by the addition of an explicit disambiguation step prior to composition.", "labels": [], "entities": []}, {"text": "In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression.", "labels": [], "entities": []}, {"text": "The results we get in two experiments show the superiority of the prior dis-ambiguation method and suggest that the effectiveness of this approach is model-independent.", "labels": [], "entities": []}], "introductionContent": [{"text": "The provision of compositionality in distributional models of meaning, where a word is represented as a vector of co-occurrence counts with every other word in the vocabulary, offers a solution to the fact that no text corpus, regardless of its size, is capable of providing reliable co-occurrence statistics for anything but very short text constituents.", "labels": [], "entities": []}, {"text": "By composing the vectors for the words within a sentence, we are still able to create a vectorial representation for that sentence that is very useful in a variety of natural language processing tasks, such as paraphrase detection, sentiment analysis or machine translation.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 210, "end_pos": 230, "type": "TASK", "confidence": 0.9277072846889496}, {"text": "sentiment analysis", "start_pos": 232, "end_pos": 250, "type": "TASK", "confidence": 0.9487492144107819}, {"text": "machine translation", "start_pos": 254, "end_pos": 273, "type": "TASK", "confidence": 0.797948569059372}]}, {"text": "Hence, given a sentence w 1 w 2 . .", "labels": [], "entities": []}, {"text": "w n , a compositional distributional model provides a function f such that: where \u2212 \u2192 w i is the distributional vector of the ith word in the sentence and \u2212 \u2192 s the resulting composite sentential vector.", "labels": [], "entities": []}, {"text": "An interesting question that has attracted the attention of researchers lately refers to the way in which these models affect ambiguous words; in other words, given a sentence such as \"a man was waiting by the bank\", we are interested to know to what extent a composite vector can appropriately reflect the intended use of word 'bank' in that context, and how such a vector would differ, for example, from the vector of the sentence \"a fisherman was waiting by the bank\".", "labels": [], "entities": []}, {"text": "Recent experimental evidence suggests that fora number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations.", "labels": [], "entities": []}, {"text": "In other words, the suggestion is that Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.6067827939987183}]}, {"text": "1 should be replaced by: where the purpose of function \u03c6 is to return a disambiguated version of each word vector given the rest of the context (e.g. all the other words in the sentence).", "labels": [], "entities": []}, {"text": "The composition operation, whatever that could be, is then applied on these unambiguous representations of the words, instead of the original distributional vectors.", "labels": [], "entities": []}, {"text": "Until now this idea has been verified on relatively simple compositional functions, usually involving some form of element-wise operation between the word vectors, such as addition or multiplication.", "labels": [], "entities": []}, {"text": "An exception to this is the work of , who apply Eq.", "labels": [], "entities": []}, {"text": "2 on partial tensor-based compositional models.", "labels": [], "entities": []}, {"text": "Ina tensor-based model, relational words such as verbs and adjectives are represented by multilinear maps; composition takes place as the application of those maps on vectors representing the arguments (usually nouns).", "labels": [], "entities": []}, {"text": "What makes the models of the above work 'partial' is that the authors used simplified versions of the linear maps, projected onto spaces of order lower than that required by the theoretical framework.", "labels": [], "entities": []}, {"text": "As a result, a certain amount of transformational power was traded off for efficiency.", "labels": [], "entities": []}, {"text": "A potential explanation then for the effectiveness of the proposed prior disambiguation method can besought on the limitations imposed by the compositional models under test.", "labels": [], "entities": []}, {"text": "After all, the idea of having disambiguation emerge as a direct consequence of the compositional process, without the introduction of any explicit step, seems more natural and closer to the way the human mind resolves lexical ambiguities.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to investigate the hypothesis whether prior disambiguation is important in a pure tensor-based compositional model, where no simplifying assumptions have been made.", "labels": [], "entities": []}, {"text": "We create such a model by using linear regression, and we explain how an explicit disambiguation step can be introduced to this model prior to composition.", "labels": [], "entities": []}, {"text": "We then proceed by comparing the composite vectors produced by this approach with those produced by the model alone in a number of experiments.", "labels": [], "entities": []}, {"text": "The results show a clear superiority of the priorly disambiguated models following Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 83, "end_pos": 85, "type": "DATASET", "confidence": 0.9292683005332947}]}, {"text": "2, confirming previous research and suggesting that the reasons behind the success of this approach are more fundamental than the form of the compositional function.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our basic vector space is trained from the ukWaC corpus, originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content).", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9935222566127777}]}, {"text": "We created vectors for all content words with at least 100 occurrences in the corpus.", "labels": [], "entities": []}, {"text": "As context we considered a 5-word window from either side of the target word, while as our weighting scheme we used local mutual information (i.e. point-wise mutual information multiplied by raw counts).", "labels": [], "entities": []}, {"text": "This initial semantic space achieved a score of 0.77 Spearman's \u03c1 (and 0.71 Pearson's r) on the well-known benchmark dataset of.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.841718594233195}, {"text": "Pearson's r)", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.8849002718925476}]}, {"text": "In order to reduce the time of regression training, our vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD).", "labels": [], "entities": [{"text": "regression training", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9130202829837799}]}, {"text": "The performance of the reduced space on the R&G dataset was again very satisfying, specifically 0.73 Spearman's \u03c1 and 0.72 Pearson's r.", "labels": [], "entities": [{"text": "R&G dataset", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.5960705950856209}, {"text": "Pearson's r", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.6666906873385111}]}, {"text": "In order to create the vector space of the holistic verb phrase vectors, we first collected all instances where a verb participating in the experiments appeared at least 100 times in a verb-object relationship with some noun in the corpus.", "labels": [], "entities": []}, {"text": "As context of a verb phrase we considered any content word that falls into a 5-word window from either side of the verb or the object.", "labels": [], "entities": []}, {"text": "For the 68 verbs participating in our experiments, this procedure resulted in 22k verb phrases, a vector space that again was projected into 300 dimensions using SVD.", "labels": [], "entities": []}, {"text": "Linear regression For each verb we use simple linear regression with gradient descent directly applied on matrices X and Y, where the rows of X correspond to vectors of the nouns that appear as objects for the given verb and the rows of Y to the holistic vectors of the corresponding verb phrases.", "labels": [], "entities": []}, {"text": "Our objective function then becomes: where m is the number of training examples and \u03bb a regularization parameter.", "labels": [], "entities": []}, {"text": "The matrix Wis used as the tensor for the specific verb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the supervised task. 'Amb.'  refers to models without the explicit disambigua- tion step, and 'Dis.' to models with that step.", "labels": [], "entities": []}, {"text": " Table 3: Results for the phrase similarity task. The  difference between the ambiguous and the disam- biguated version is s.s. with p < 0.001.", "labels": [], "entities": [{"text": "phrase similarity task", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.8536368012428284}]}]}