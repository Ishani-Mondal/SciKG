{"title": [{"text": "A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case.", "labels": [], "entities": []}, {"text": "In this paper we motivate, formalize and present our approach.", "labels": [], "entities": []}, {"text": "In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements.", "labels": [], "entities": []}, {"text": "Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data.", "labels": [], "entities": []}, {"text": "Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9623342752456665}]}], "introductionContent": [], "datasetContent": [{"text": "To evaluate the quality of our generalized language models we empirically compare their ability to explain sequences of words.", "labels": [], "entities": []}, {"text": "To this end we use text corpora, split them into test and training data, build language models as well as generalized language models over the training data and apply them on the test data.", "labels": [], "entities": []}, {"text": "We employ established metrics, such as cross entropy and perplexity.", "labels": [], "entities": []}, {"text": "In the following we explain the details of our experimental setup.", "labels": [], "entities": []}, {"text": "All data sets have been randomly split into a training and a test set on a sentence level.", "labels": [], "entities": []}, {"text": "The training sets consist of 80% of the sentences, which have been used to derive n-grams, skip n-grams and corresponding continuation counts for values of n between 1 and 5.", "labels": [], "entities": []}, {"text": "Note that we have trained a prediction model for each data set individually.", "labels": [], "entities": []}, {"text": "From the remaining 20% of the sequences we have randomly sampled a separate set of 100, 000 sequences of 5 words each.", "labels": [], "entities": []}, {"text": "These test sequences have also been shortened to sequences of length 3, and 4 and provide a basis to conduct our final experiments to evaluate the performance of the different algorithms.", "labels": [], "entities": []}, {"text": "We learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences fora direct comparison.", "labels": [], "entities": []}, {"text": "To ensure rigour and openness of research the data set for training as well as the test sequences and the entire source code is open source.", "labels": [], "entities": []}, {"text": "We compared the probabilities of our language model implementation (which is a subset of the generalized language model) using KN as well as MKN smoothing with the Kyoto Language Model Toolkit . Since we got the same results for small n and small data sets we believe that our implementation is correct.", "labels": [], "entities": [{"text": "Kyoto Language Model Toolkit", "start_pos": 164, "end_pos": 192, "type": "DATASET", "confidence": 0.8919597119092941}]}, {"text": "Ina second experiment we have investigated the impact of the size of the training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.793484091758728}]}, {"text": "The wikipedia corpus consists of 1.7 bn. words.", "labels": [], "entities": [{"text": "wikipedia corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7405468225479126}]}, {"text": "Thus, the 80% split for training consists of 1.3 bn. words.", "labels": [], "entities": []}, {"text": "We have iteratively created smaller training sets by decreasing the split factor by an order of magnitude.", "labels": [], "entities": []}, {"text": "So we created 8% / 92% and 0.8% / 99.2% split, and soon.", "labels": [], "entities": []}, {"text": "We have stopped at the 0.008%/99.992% split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split.", "labels": [], "entities": []}, {"text": "Then we trained a generalized language model as well as a standard language model with modified Kneser-Ney smoothing on each of these samples of the training data.", "labels": [], "entities": []}, {"text": "Again we have evaluated these language models on the same random sample of 100, 000 sequences as mentioned above.", "labels": [], "entities": []}, {"text": "As evaluation metric we use perplexity: a standard measure in the field of language models).", "labels": [], "entities": []}, {"text": "First we calculate the cross entropy of a trained language model given a test set using Where P alg will be replaced by the probability estimates provided by our generalized language models and the estimates of a language model using modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "P MLE , instead, is a maximum likelihood estimator of the test sequence to occur in the test corpus.", "labels": [], "entities": [{"text": "P MLE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.796561598777771}]}, {"text": "Finally, T is the set of test sequences.", "labels": [], "entities": []}, {"text": "The perplexity is defined as: Lower perplexity values indicate better results.", "labels": [], "entities": []}, {"text": "The perplexity values for all data sets and various model orders can be seen in: Absolute perplexity values and relative reduction of perplexity from MKN to GLM on all data sets for models of order 3 to 5 As we can see, the GLM clearly outperforms the baseline for all model lengths and data sets.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9862678050994873}]}, {"text": "In general we see a larger improvement in performance for models of higher orders (n = 5).", "labels": [], "entities": []}, {"text": "The gain for 3-gram models, instead, is negligible.", "labels": [], "entities": []}, {"text": "For German texts the increase in performance is the highest (12.7%) fora model of order 5.", "labels": [], "entities": []}, {"text": "We also note that GLMs seem to work better on broad domain text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora.", "labels": [], "entities": [{"text": "JRC corpora", "start_pos": 195, "end_pos": 206, "type": "DATASET", "confidence": 0.8993112444877625}]}, {"text": "We made consistent observations in our second experiment where we iteratively shrank the size of the training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.7900435328483582}]}, {"text": "We calculated the relative reduction in perplexity from MKN to GLM for various model lengths and the different sizes of the training data.", "labels": [], "entities": [{"text": "MKN", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.728380560874939}]}, {"text": "The results for the English Wikipedia data set are illustrated in.", "labels": [], "entities": [{"text": "English Wikipedia data set", "start_pos": 20, "end_pos": 46, "type": "DATASET", "confidence": 0.9619795680046082}]}, {"text": "We see that the GLM performs particularly well on small training data.", "labels": [], "entities": [{"text": "GLM", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.8543769717216492}]}, {"text": "As the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7% compared to language models with modified Kneser-Ney smoothing on the same data set.", "labels": [], "entities": []}, {"text": "The absolute perplexity values for this experiment are presented in model length Experiments n = 3 n = 4 n = 5: Absolute perplexity values and relative reduction of perplexity from MKN to GLM on shrunk training data sets for the English Wikipedia for models of order 3 to 5 Our theory as well as the results so far suggest that the GLM performs particularly well on sparse training data.", "labels": [], "entities": []}, {"text": "This conjecture has been investigated in a last experiment.", "labels": [], "entities": []}, {"text": "For each model length we have split the test data of the largest English Wikipedia corpus into two disjoint evaluation data sets.", "labels": [], "entities": [{"text": "English Wikipedia corpus", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.8114346265792847}]}, {"text": "The data set unseen consists of all test sequences which have never been observed in the training data.", "labels": [], "entities": []}, {"text": "The set observed consists only of test sequences which have been observed at least once in the training data.", "labels": [], "entities": []}, {"text": "Again we have calculated the perplexity of each set.", "labels": [], "entities": []}, {"text": "For reference, also the values of the complete test data set are shown in: Absolute perplexity values and relative reduction of perplexity from MKN to GLM for the complete and split test file into observed and unseen sequences for models of order 3 to 5.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9793360829353333}, {"text": "GLM", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.902060329914093}]}, {"text": "The data set is the largest English Wikipedia corpus.", "labels": [], "entities": [{"text": "English Wikipedia corpus", "start_pos": 28, "end_pos": 52, "type": "DATASET", "confidence": 0.8692698876063029}]}, {"text": "As expected we seethe overall perplexity values rise for the unseen test case and decline for the observed test case.", "labels": [], "entities": []}, {"text": "More interestingly we see that the relative reduction of perplexity of the GLM over MKN increases from 10.5% to 15.6% on the unseen test case.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9434207677841187}, {"text": "GLM", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7992374300956726}, {"text": "MKN", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.7241663932800293}]}, {"text": "This indicates that the superior performance of the GLM on small training corpora and for higher order models indeed comes from its good performance properties with regard to sparse training data.", "labels": [], "entities": []}, {"text": "It also confirms that our motivation to produce lower order n-grams by omitting not only the first word of the local context but systematically all words has been fruitful.", "labels": [], "entities": []}, {"text": "However, we also see that for the observed sequences the GLM performs slightly worse than MKN.", "labels": [], "entities": [{"text": "GLM", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9476040601730347}, {"text": "MKN", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.7506113052368164}]}, {"text": "For the observed cases we find the relative change to be negligible.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Word statistics and size of of evaluation  corpora", "labels": [], "entities": []}, {"text": " Table 3: Absolute perplexity values and relative  reduction of perplexity from MKN to GLM on all  data sets for models of order 3 to 5", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9945909976959229}]}, {"text": " Table 4: Absolute perplexity values and relative  reduction of perplexity from MKN to GLM on  shrunk training data sets for the English Wikipedia  for models of order 3 to 5", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9914864301681519}]}, {"text": " Table 5: Absolute perplexity values and relative  reduction of perplexity from MKN to GLM for the  complete and split test file into observed and un- seen sequences for models of order 3 to 5. The  data set is the largest English Wikipedia corpus.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9704599380493164}, {"text": "English Wikipedia corpus", "start_pos": 223, "end_pos": 247, "type": "DATASET", "confidence": 0.8930856188138326}]}, {"text": " Table 6. The fraction of total n- grams which appear only once in our Wikipedia  corpus increases for higher values of n. However,  for the same value of n the skip n-grams are less  rare. Our generalized language models leverage  this additional information to obtain more reliable  estimates for the probability of word sequences.", "labels": [], "entities": []}, {"text": " Table 6: Percentage of generalized n-grams which  occur only once in the English Wikipedia cor- pus. Total means a percentage relative to the total  amount of sequences. Unique means a percentage  relative to the amount of unique sequences of this  pattern in the data set.", "labels": [], "entities": []}]}