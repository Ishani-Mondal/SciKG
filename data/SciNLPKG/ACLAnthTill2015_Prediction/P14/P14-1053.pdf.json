{"title": [{"text": "Generating Code-switched Text for Lexical Learning", "labels": [], "entities": [{"text": "Lexical Learning", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7403115630149841}]}], "abstractContent": [{"text": "A vast majority of L1 vocabulary acquisition occurs through incidental learning during reading (Nation, 2001; Schmitt et al., 2001).", "labels": [], "entities": [{"text": "L1 vocabulary acquisition", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.5933454235394796}]}, {"text": "We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading.", "labels": [], "entities": [{"text": "retention", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9467149972915649}]}, {"text": "Our model that takes as input a bilingual dictionary and an English text, and generates a code-switched text that optimizes a defined \"learnability\" metric by constructing a factor graph over lexical mentions.", "labels": [], "entities": []}, {"text": "Using an artificial language vocabulary, we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task.", "labels": [], "entities": [{"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9977911710739136}]}], "introductionContent": [{"text": "Today, an adult trying to learn anew language is likely to embrace an age-old and widely accepted practice of learning vocabulary through curated word lists and rote memorization.", "labels": [], "entities": []}, {"text": "Yet, it is not uncommon to find yourself surrounded by speakers of a foreign language and instinctively pickup words and phrases without ever seeing the definition in your native tongue.", "labels": [], "entities": []}, {"text": "Hearing \"pass le sale please\" at the dinner table from your in-laws visiting from abroad, is unlikely to make you think twice about passing the salt.", "labels": [], "entities": []}, {"text": "Humans are extraordinarily good at inferring meaning from context, whether this context is your physical surrounding, or the surrounding text in the paragraph of the word that you don't yet understand.", "labels": [], "entities": []}, {"text": "Recently, a novel method of L2 language teaching had been shown effective in improving adult lexical acquisition rate and retention . This tech-1 authors' unpublished work nique relies on a phenomenon that elicits a natural simulation of L1-like vocabulary learning in adults -significantly closer to L1 learning for L2 learners than any model studied previously.", "labels": [], "entities": []}, {"text": "By infusing foreign words into text in the learner's native tongue into low-surprisal contexts, the lexical acquisition process is facilitated naturally and non-obtrusively.", "labels": [], "entities": []}, {"text": "Incidentally, this phenomenon occurs \"in the wild\" and is termed code-switching or code-mixing, and refers to the linguistic pattern of bilingual speakers swapping words and phrases between two languages during speech.", "labels": [], "entities": []}, {"text": "While this phenomenon had received significant attention from both a socio-linguistic ( and theoretical linguistic perspectives) (including some computational studies), only recently has it been hypothesizes that \"code-switching\" is a marking of bilingual proficiency, rather than deficiency.", "labels": [], "entities": []}, {"text": "Until recently it was widely believed that incidental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the \"noticing\" effect needed for lexical acquisition to occur.", "labels": [], "entities": [{"text": "incidental lexical acquisition", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6529720425605774}, {"text": "lexical acquisition", "start_pos": 215, "end_pos": 234, "type": "TASK", "confidence": 0.7118429392576218}]}, {"text": "Recent neurophysiological findings, however, indicate that even a single incidental exposure to a novel word in a sufficiently constrained context is sufficient to trigger an early integration of the word in the brain's semantic network ().", "labels": [], "entities": []}, {"text": "An approach explored in this paper, and motivated by the above findings, exploits \"constraining\" contexts in text to introduce novel words.", "labels": [], "entities": []}, {"text": "A state-of-the-art approach for generating such text is based on an expert annotator whose job is to decide which words to \"switch out\" with novel foreign words (from hereon we will refer to the \"switched out\" word as the source word and to the \"switched in\" word as the target word).", "labels": [], "entities": []}, {"text": "Consequently the process is labor-intensive and leads to a \"one size fits all solution\" that is insensitive to the learner's skill level or vocabulary proficiency.", "labels": [], "entities": []}, {"text": "This limitation is also cited in literature as a significant roadblock to the widespread adaptation of graded reading series.", "labels": [], "entities": []}, {"text": "A readingbased tool that follows the same principle, i.e. by systematic exposure of a learner to an incrementally more challenging text, will result in more effective learning.", "labels": [], "entities": []}, {"text": "To address the above limitation, we develop an approach for automatically generating such \"codeswitched\" text with an explicit goal of maximizing the lexical acquisition rate in adults.", "labels": [], "entities": []}, {"text": "Our method is based on a global optimization approach that incorporates a \"knowledge model\" of a user with the content of the text, to generate a sequence of lexical \"switches\".", "labels": [], "entities": []}, {"text": "To facilitate the selection of \"switch points\", we learn a discriminative model for predicting switch point locations on a corpus that we collect for this purpose (and release to the community).", "labels": [], "entities": []}, {"text": "Below is a high-level outline of this paper.", "labels": [], "entities": []}, {"text": "\u2022 We formalize our approach within a probabilistic graphical model framework, inference in which yields \"code-switched\" text that maximizes a surrogate to the acquisition rate objective.", "labels": [], "entities": []}, {"text": "\u2022 We compare this global method to several baseline techniques, including the strong \"high-frequency\" baseline.", "labels": [], "entities": []}, {"text": "\u2022 We analyze the operating range in which our model is effective and motivate the nearfuture extension of this approach with the proposed improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out experiments on the effectiveness of our approach using the Amazon Mechanical Turk platform.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk platform", "start_pos": 74, "end_pos": 105, "type": "DATASET", "confidence": 0.962336540222168}]}, {"text": "Our experimental procedure was as follows: 162 turkers were partitioned into four groups, each corresponding to a treatment condition: OP T (N=34), HF (N=41), RAN DOM (N=43), M AN (N=44).", "labels": [], "entities": [{"text": "OP T", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.92615607380867}, {"text": "RAN DOM", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.6740954220294952}, {"text": "M AN", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.8016300201416016}]}, {"text": "Each condition corre-: Visualization of the most \"predictable\" words in an excerpt from the \"The Man who Repented\" by Ana Maria Matute (English translation).", "labels": [], "entities": []}, {"text": "Font-size correlates with the score given by judge turkers in evaluating guesses of other turkers that were presented with the same text, but the word replaced with a blank.", "labels": [], "entities": [{"text": "Font-size", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9943580031394958}]}, {"text": "Snippet of the dataset that we release publicly.", "labels": [], "entities": []}, {"text": "sponded to a model used to generate the presented code-switched text.", "labels": [], "entities": []}, {"text": "For all experiments, the text used was a short story \"Lottery\" by Shirley Jackson, and a total number of replaced words was controlled (34).", "labels": [], "entities": []}, {"text": "Target vocabulary consisted of words from an artificial language, generated statically by a mix of words from several languages.", "labels": [], "entities": []}, {"text": "Below we describe the individual treatment conditions: RANDOM (Baseline): words for switching are selected at random from content only words.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9709569811820984}]}, {"text": "HF (High Frequency) Baseline: words for switching are selected at random from a ranked list of words that occur most frequently in the presented text.", "labels": [], "entities": []}, {"text": "MAN (Manual) Baseline: words for switching are selected manually by the author, based on the intuition of which words are most likely to be guessed in context.", "labels": [], "entities": []}, {"text": "OPT (Optimization-based): factor graph-based model proposed in this paper is used for generating code-switched content.", "labels": [], "entities": []}, {"text": "The total number of switched words generated by this method is used as a constant for all baselines.", "labels": [], "entities": []}, {"text": "Turkers were solicited to participate in a study that involved \"reading a short story with a twist\" (title of HIT).", "labels": [], "entities": []}, {"text": "Not the title, nor the description gave away the purpose of the study, nor that it would be followed by a quiz.", "labels": [], "entities": []}, {"text": "Time was not controlled for this study, but on average turkers took 27 minutes to complete the reading.", "labels": [], "entities": [{"text": "Time", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9651840925216675}]}, {"text": "Upon completing the reading portion of the task, turkers were presented with novel sentences that featured the words observed during reading, where only one of the sentences used the word in a semantically correct way.", "labels": [], "entities": []}, {"text": "Turkers were asked to select the sentence that \"made the most sense\".", "labels": [], "entities": []}, {"text": "An example of the sentences presented during the test:", "labels": [], "entities": []}], "tableCaptions": []}