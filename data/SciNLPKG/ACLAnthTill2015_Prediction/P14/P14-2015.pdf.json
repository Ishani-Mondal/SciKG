{"title": [], "abstractContent": [{"text": "Sentiment relevance detection problems occur when there is a sentiment expression in a text, and there is the question of whether or not the expression is related to a given entity or, more generally, to a given situation.", "labels": [], "entities": [{"text": "Sentiment relevance detection", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.946516752243042}]}, {"text": "The paper discusses variants of the problem, and shows that it is distinct from other somewhat similar problems occurring in the field of sentiment analysis and opinion mining.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.9554482102394104}, {"text": "opinion mining", "start_pos": 161, "end_pos": 175, "type": "TASK", "confidence": 0.8256993293762207}]}, {"text": "We experimentally demonstrate that using the information about relevancy significantly affects the final sentiment evaluation of the entities.", "labels": [], "entities": []}, {"text": "We then compare a set of different algorithms for solving the relevance detection problem.", "labels": [], "entities": [{"text": "relevance detection problem", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.9255672494570414}]}, {"text": "The most accurate results are achieved by algorithms that use certain document level information about the target entities.", "labels": [], "entities": []}, {"text": "We show that this information can be accurately extracted using supervised classification methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment extraction by modern sentiment analysis (SA) systems is usually based on searching the input text for sentiment-bearing words and expressions, either general (language-wide) or domain-specific.", "labels": [], "entities": [{"text": "Sentiment extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9605634808540344}, {"text": "sentiment analysis (SA)", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.8917993783950806}]}, {"text": "In most common SA approaches, each such expression carries a polarity value (\"positive\" or \"negative\") which is possibly weighted.", "labels": [], "entities": []}, {"text": "The sum of all polarity values from all expressions found in a text becomes the sentiment score for the whole text.", "labels": [], "entities": []}, {"text": "People are, however, usually interested in sentiments regarding some entity or situation, and not in sentiments of a particular document.", "labels": [], "entities": []}, {"text": "A natural way to make the SA more focused is to explicitly bind each sentiment expression to a specific entity, or to a small set of entities from among all entities mentioned in the document.", "labels": [], "entities": []}, {"text": "The choice of which entity to bind a sentiment expression to, can be made according to the proximity (physical, syntactical, and/or semantic) and/or salience of the entities.", "labels": [], "entities": []}, {"text": "In this paper, we argue that all of these methods can be useful in different contexts, and so the best single algorithm should use all available proximity information, of all kinds, together with additional context information -position in the document, section, or paragraph; proximity of other entities; lexical contents; etc.", "labels": [], "entities": []}, {"text": "One of the most important context information is the type of relation between the target entity and the document -whether the entity is the main topic of the document, or one of several main topics, or mentioned in passing, etc.", "labels": [], "entities": []}, {"text": "Another layer that we'd like to add concerns the interaction of different entity types during SA.", "labels": [], "entities": [{"text": "SA", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9736050963401794}]}, {"text": "Ina typical situation, there is only one entity type which is the target for SA.", "labels": [], "entities": [{"text": "SA", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9681361317634583}]}, {"text": "In such cases, clearly distinguishing between the relevancy of target and non-target entities types is not essential.", "labels": [], "entities": []}, {"text": "For example, when the general topic is a COMPANY, and there is a sentiment expression referring to a PERSON or a PRODUCT, this sentiment expression is still relevant to the company and can be regarded as such.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9278155565261841}]}, {"text": "In other situations, SA users maybe specifically interested in an interaction between entities of different types.", "labels": [], "entities": []}, {"text": "For example, in a medical forum setting, it maybe interesting to know the users' sentiments regarding a given DRUG in the context of a given DISEASE.", "labels": [], "entities": []}, {"text": "We will show that such situations are modeled well enough using intersections of regions of relevance of the participating entity types, with the relevance region for each type calculated separately.", "labels": [], "entities": []}, {"text": "We purposefully exclude possible interactions between entities of the same type, because they behave in a different way.", "labels": [], "entities": []}, {"text": "The precise analysis of such interactions is a different topic from rele-vance detection, and so it is mostly ignored in this paper.", "labels": [], "entities": [{"text": "rele-vance detection", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7130381613969803}]}], "datasetContent": [{"text": "For the experiments, we use two manuallyannotated corpora 2 , a financial corpus 3 and a medical 4 corpus.", "labels": [], "entities": []}, {"text": "In the Financial corpus, COMPANIEs are used as target entities and in the medical corpus, DISEASEs, DRUGs and PERSONs are the entity types that are used as target entities.", "labels": [], "entities": [{"text": "Financial corpus", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.8176327347755432}]}, {"text": "For the purpose of the experiments, we are interested only in single-entity sentiments about DRUGs, and multiple-entity sentiments about DRUGs + DISEASEs, and DRUGs + DISEASEs + PERSONs.", "labels": [], "entities": []}, {"text": "The evaluation metrics in all of the experiments are precision, recall, and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9997732043266296}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9993805885314941}, {"text": "F1", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9996035695075989}]}, {"text": "For the classification-based algorithms, unless stated otherwise, we use 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "In the first experiment, we demonstrate the importance of using relevance when calculating the consolidated sentiment score of an entity within a set of documents.", "labels": [], "entities": []}, {"text": "For each entity, we set the 'correct' consolidated sentiment score to the average of polarities of all sentiments in a corpus which are labeled as relevant to the entity.", "labels": [], "entities": [{"text": "correct' consolidated sentiment score", "start_pos": 29, "end_pos": 66, "type": "METRIC", "confidence": 0.64657963514328}]}, {"text": "Then, we compare the correct value to the two scores calculated without considering relevance: \uf0b7 'Baseline' -the average of polarities of all sentiments in all documents where the entity is mentioned, and \uf0b7 'TargetedOnly' -the average of polarities of all sentiments in the documents where the entity is labeled as target (main topic of the document).", "labels": [], "entities": []}, {"text": "This case models the typical state of a relevance-agnostic SA system.", "labels": [], "entities": []}, {"text": "For this evaluation, we only compare the sign of the final sentiment scores, without considering their magnitudes (unless it is close to zero, in Fully annotating texts for semantic relevance is an arduous task, thus the used annotated corpora are relatively small.", "labels": [], "entities": []}, {"text": "Sample can be found at http://goo.gl/6HONHP.", "labels": [], "entities": []}, {"text": "A corpus of 160 financial news documents on at least one entity of interest, of average size ~5Kb, downloaded from various financial news websites.", "labels": [], "entities": []}, {"text": "The dataset mentions 424 different companies.", "labels": [], "entities": []}, {"text": "A corpus of 160 documents, of average size ~7Kb, downloaded following Google queries on a set of a few common drugs and diseases.", "labels": [], "entities": []}, {"text": "The dataset mentions 722 different people, 46 diseases, and 175 drugs.", "labels": [], "entities": []}, {"text": "which it is considered 'neutral').", "labels": [], "entities": []}, {"text": "The errors at this level indicate definite SA errors -miscalculating entity's sentiment into its opposite.", "labels": [], "entities": [{"text": "SA errors", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.901836484670639}]}, {"text": "The results of the evaluation are as follows: The 'Baseline' scores show a large difference from the correct scores, with 33% and 38% of entities having wrong final polarity in the financial (COMPANY) and medical (DRUG) domains, respectively.", "labels": [], "entities": []}, {"text": "The 'TargetedOnly' scores are somewhat closer to correct, with 12% and 28% of entities with incorrect final polarities.", "labels": [], "entities": [{"text": "correct", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9867385625839233}]}, {"text": "However, the 'TargetedOnly' method naturally suffers from a very low recall, with only 19% and 38% of entities covered in the financial and medical domains, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.999103307723999}]}, {"text": "In this experiment, we compare the performance of various algorithms while either providing or withholding the information about the documenttype-with-respect-to-the-target-entity.", "labels": [], "entities": []}, {"text": "The performance of the physical proximity algorithms on the financial corpus is shown at the top left hand side of.", "labels": [], "entities": []}, {"text": "The set of all instances of relevance detection problems in the corpus (an instance consists of a sentiment expression within a text, together with a target entity) is divided into three subsets, according to the status of the target entity within the document.", "labels": [], "entities": [{"text": "relevance detection problems", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.8188454508781433}]}, {"text": "As expected, the three flavors of the physical proximity algorithm perform much better on the corpus subsets they are adapted to.", "labels": [], "entities": []}, {"text": "At the bottom left hand side of, we similarly show the performance of the two flavors of the syntaxproximity-based algorithm on the medical domain (DRUG entities).", "labels": [], "entities": []}, {"text": "Same as above, there is a large difference in the performance of the two flavors of the algorithm on different subsets of the problem set.", "labels": [], "entities": []}, {"text": "Finally, at the top of, we compare the performance of the two classification-based algorithms on the two (whole) problem sets, while either keeping or withholding the entity status information from the classifier.", "labels": [], "entities": []}, {"text": "The difference in results is less pronounced here, but is still noticeable.", "labels": [], "entities": []}, {"text": "The reason for the smaller difference, we hypothesize, is the ability of the classifiers to partially infer the entity status from the various context clues that are used as classification features (see the experiment 5.3).", "labels": [], "entities": []}, {"text": "In this experiment, we confirm that it is possible to identify the entity status within documents using supervised classification.", "labels": [], "entities": []}, {"text": "The results of direct evaluation show that the accuracies of the Medical and Financial corpora (using 10-fold X-validation) are 87.8% and 82.2% respectively, and the accuracy when using the Medical corpus for training the Financial corpus for testing and vice versa, are 78.2% and 86.1% , respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9944530129432678}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9993913173675537}, {"text": "Medical corpus", "start_pos": 190, "end_pos": 204, "type": "DATASET", "confidence": 0.9057733118534088}]}, {"text": "The results of relevance detection using the automatically extracted entity status values are shown at the right hand side of, which utilize the same datasets and algorithms as at the left hand side of and at the top of.", "labels": [], "entities": [{"text": "relevance detection", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.9305330216884613}]}, {"text": "As can be seen from the tables, the drop in performance is small, demonstrating the success of classification-based extraction of entity status information.", "labels": [], "entities": [{"text": "classification-based extraction of entity status information", "start_pos": 95, "end_pos": 155, "type": "TASK", "confidence": 0.8798625071843466}]}, {"text": "In this experiment, we test how well the classifiers trained on data from one domain work on input from a different domain.", "labels": [], "entities": []}, {"text": "The classification results using different types of training data are shown in.", "labels": [], "entities": []}, {"text": "Performance of classification-based algorithms using different training data (F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.997294008731842}]}, {"text": "The table confirms general independence of the classification performance on the domain.", "labels": [], "entities": []}, {"text": "Comparing the 2-fold and 10-fold crossvalidation results (the difference is equivalent to doubling the amount of training data), shows that the amount of training data is sufficient.", "labels": [], "entities": []}, {"text": "In this experiment, we simply compare the overall accuracy of various algorithms for relevance discernment, operating at their best parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9991305470466614}, {"text": "relevance discernment", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8739092946052551}]}, {"text": "The results are shown at the bottom of.", "labels": [], "entities": []}, {"text": "Overall, classification-based algorithms perform better than the deterministic ones, with sequenceclassification performing significantly better than direct classification.", "labels": [], "entities": []}, {"text": "Syntactic proximity-based is precise, but has relatively low recall, reducing its overall performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9994421601295471}]}, {"text": "Physical proximity-based is simplest, and produce reasonably high overall results, although worse than the best-performing classification-based methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Performance of different algorithms on three subsets of the corpus with a different status of  the target entity within the document.", "labels": [], "entities": []}, {"text": " Table 2. Performance of different algorithms  on the different domains.", "labels": [], "entities": []}, {"text": " Table 3. Performance of classification-based  algorithms using different training data (F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9949819445610046}]}]}