{"title": [{"text": "Inferring User Political Preferences from Streaming Communications", "labels": [], "entities": [{"text": "Inferring User Political Preferences from Streaming Communications", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.7895819119044712}]}], "abstractContent": [{"text": "Existing models for social media personal analytics assume access to thousands of messages per user, even though most users author content only sporadically overtime.", "labels": [], "entities": []}, {"text": "Given this sparsity, we: (i) leverage content from the local neighborhood of a user; (ii) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and (iii) estimate the amount of time and tweets required fora dynamic model to predict user preferences.", "labels": [], "entities": []}, {"text": "We show that even when limited or no self-authored data is available, language from friend, retweet and user mention communications provide sufficient evidence for prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 164, "end_pos": 174, "type": "TASK", "confidence": 0.9699788689613342}]}, {"text": "When updating models overtime based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author's tweeting frequency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inferring latent user attributes such as gender, age, and political preferences automatically from personal communications and social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available.", "labels": [], "entities": [{"text": "Inferring latent user attributes such as gender, age, and political preferences automatically from personal communications and social media including emails, blog posts or public discussions", "start_pos": 0, "end_pos": 190, "type": "Description", "confidence": 0.7198597980397088}]}, {"text": "Resources like Twitter 1 or Facebook 2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population The existing batch models for predicting latent user attributes rely on thousands of tweets per author (.", "labels": [], "entities": []}, {"text": "However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9969549179077148}]}, {"text": "Moreover, recent changes to Twitter API querying rates further restrict the speed of access to this resource, effectively reducing the amount of data that can be collected in a given time period.", "labels": [], "entities": []}, {"text": "In this paper we analyze and go beyond static models formulating personal analytics in social media as a streaming task.", "labels": [], "entities": [{"text": "formulating personal analytics", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.8306334217389425}]}, {"text": "We first evaluate batch models that are cognizant of low-resource prediction setting described above, maximizing the efficiency of content in calculating personal analytics.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work that makes explicit the tradeoff between accuracy and cost (manifest as calls to the Twitter API), and optimizes to a different tradeoff than state-ofthe-art approaches, seeking maximal performance when limited data is available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9985015392303467}]}, {"text": "In addition, we propose streaming models for personal analytics that dynamically update user labels based on their stream of communications which has been addressed previously by Van Durme (2012b).", "labels": [], "entities": []}, {"text": "Such models better capture the real-time nature of evidence being used in latent author attribute predictions tasks.", "labels": [], "entities": []}, {"text": "Our main contributions include: -develop low-resource and real-time dynamic approaches for personal analytics using as an example the prediction of political preference of Twitter users; -examine the relative utility of six different notions of \"similarity\" between users in an implicit Twitter social network for personal analytics; -experiments are performed across multiple datasets supporting the prediction of political preference in Twitter, to highlight the significant differences in performance that arise from the underlying collection and annotation strategies.", "labels": [], "entities": []}], "datasetContent": [{"text": "We design a set of experiments to analyze static and dynamic models for political affiliation classification defined in Sections 3 and 4.", "labels": [], "entities": [{"text": "political affiliation classification", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.6934605538845062}]}, {"text": "We first answer whether communications from user-local neighborhoods can help predict political preference for the user.", "labels": [], "entities": []}, {"text": "To explore the contribution of different neighborhood types we learn static user and neighbor models on G cand , G geo and G ZLR graphs.", "labels": [], "entities": []}, {"text": "We also examine the ability of our static models to predict user political preferences in low-resource setting e.g., 5 tweets.", "labels": [], "entities": []}, {"text": "The existing models follow a standard setup when either user or neighbor tweets are available during train and test.", "labels": [], "entities": []}, {"text": "For a static neighbor model we go beyond that, and train our the model on all data available per user, but only apply part of the data at the test time, pushing the boundaries of how little is truly required for classification.", "labels": [], "entities": []}, {"text": "For example, we only use follower tweets for G test , but we use tweets from all types of neighbors for G train . Such setup will simulate different realworld prediction scenarios which have not been previously explored, to our knowledge e.g., when a user has a private profile or has not tweeted yet, and only user neighbor tweets are available.", "labels": [], "entities": []}, {"text": "We experiment with our static neighbor model defined in Eq.2 with the aim to: 1.", "labels": [], "entities": [{"text": "Eq.2", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9432269930839539}]}, {"text": "evaluate neighborhood size influence, we change the number of neighbors and try n = [1, 2, 5, 10] neighbor(s) per user; 2.", "labels": [], "entities": []}, {"text": "estimate neighbor content influence, we alternate the amount of content per neighbor and try t = tweets.", "labels": [], "entities": []}, {"text": "We perform 10-fold cross validation and run 100 random restarts for every n and t parameter combination.", "labels": [], "entities": []}, {"text": "We compare our static neighbor and user models using the cost functions from Eq.3 and Eq.4.", "labels": [], "entities": [{"text": "Eq.3", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9689369797706604}, {"text": "Eq.4", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.910359799861908}]}, {"text": "For all experiments we use, integrated in the Jerboa toolkit (Van Durme, 2012a).", "labels": [], "entities": [{"text": "Jerboa toolkit (Van Durme, 2012a)", "start_pos": 46, "end_pos": 79, "type": "DATASET", "confidence": 0.9483397006988525}]}, {"text": "Both models defined in Eq.1 and Eq.2 are learned using normalized count-based word ngram features extracted from either user or neighbor tweets.", "labels": [], "entities": [{"text": "Eq.1", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.9605709910392761}, {"text": "Eq.2", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8113190531730652}]}, {"text": "We evaluate our models with dynamic Bayesian updates on a continuous stream of communications overtime as shown in.", "labels": [], "entities": []}, {"text": "Unlike static model experiments, we are not modeling the influence of the number of neighbors or the amount of content per neighbor.", "labels": [], "entities": []}, {"text": "Here, we order user and neighbor communication streams by real world time of posting and measure changes in posterior probabilities overtime.", "labels": [], "entities": []}, {"text": "The main purpose of these experiments is to quantitatively evaluate (1) the number of tweets and (2) the amount of real world time it takes to observe enough evidence on Twitter to make reliable predictions.", "labels": [], "entities": []}, {"text": "We experiment with log-linear models defined in Eq.", "labels": [], "entities": []}, {"text": "1 and 2 and continuously estimate the posterior probabilities P (R | T ) as defined in Eq.6.", "labels": [], "entities": []}, {"text": "We average the posterior probability results over the users in G cand , G geo and G ZLR graphs.", "labels": [], "entities": []}, {"text": "We train streaming models on an attribute balanced subset of tweets for each user vi excluding vi 's tweets (or vi 's neighbor tweets fora joint model).", "labels": [], "entities": []}, {"text": "This setup is similar to leave-one-out classification.", "labels": [], "entities": [{"text": "leave-one-out classification", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.7141508162021637}]}, {"text": "The classifier is learned using binary word ngram features extracted from user or user-neighbor communications.", "labels": [], "entities": []}, {"text": "We prefer binary to normalized countbased features to overcome sparsity issues caused by making predictions on each tweet individually.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the existing approaches for political  preference classification in Twitter.", "labels": [], "entities": [{"text": "political  preference classification", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.6440979739030203}]}]}