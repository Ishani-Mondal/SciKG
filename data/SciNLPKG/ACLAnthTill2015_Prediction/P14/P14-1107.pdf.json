{"title": [{"text": "Are Two Heads Better than One? Crowdsourced Translation via a Two-Step Collaboration of Non-Professional Translators and Editors", "labels": [], "entities": [{"text": "Crowdsourced Translation", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6657953262329102}]}], "abstractContent": [{"text": "Crowdsourcing is a viable mechanism for creating training data for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8169685900211334}]}, {"text": "It provides a low cost, fast turnaround way of processing large volumes of data.", "labels": [], "entities": []}, {"text": "However, when compared to professional translation, naive collection of translations from non-professionals yields low-quality results.", "labels": [], "entities": []}, {"text": "Careful quality control is necessary for crowdsourcing to work well.", "labels": [], "entities": []}, {"text": "In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals.", "labels": [], "entities": []}, {"text": "We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals .", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems are trained using bilingual sentence-aligned parallel corpora.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8092465698719025}]}, {"text": "Theoretically, SMT can be applied to any language pair, but in practice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc.", "labels": [], "entities": [{"text": "SMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9926562309265137}]}, {"text": "SMT gets stuck in a severe bottleneck for many minority or 'low resource' languages with insufficient data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9730935096740723}]}, {"text": "This drastically limits which languages SMT can be successfully applied to.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9696047306060791}]}, {"text": "Because of this, collecting parallel corpora for minor languages has become an interesting research challenge.", "labels": [], "entities": [{"text": "collecting parallel corpora for minor languages", "start_pos": 17, "end_pos": 64, "type": "TASK", "confidence": 0.8027955194314321}]}, {"text": "There are various options for creating training data for new language pairs.", "labels": [], "entities": []}, {"text": "Past approaches have examined harvesting translated documents from the web, or discovering parallel fragments from comparable corpora (Munteanu and.", "labels": [], "entities": []}, {"text": "Until relatively recently, little consideration has been given to creating parallel data from scratch.", "labels": [], "entities": []}, {"text": "This is because the cost of hiring professional translators is prohibitively high.", "labels": [], "entities": []}, {"text": "For instance, hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators fora short-term commitment.", "labels": [], "entities": []}, {"text": "Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators.", "labels": [], "entities": []}, {"text": "Facebook localized its website into different languages using volunteers.", "labels": [], "entities": []}, {"text": "DuoLingo turns translation into an educational game, and translates web content using its language learners.", "labels": [], "entities": []}, {"text": "Rather than relying on volunteers or gamification, NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk (MTurk) platform).", "labels": [], "entities": [{"text": "crowdsourcing translation", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7029251456260681}, {"text": "Amazon Mechanical Turk (MTurk) platform", "start_pos": 132, "end_pos": 171, "type": "DATASET", "confidence": 0.7922447664397103}]}, {"text": "This setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward.", "labels": [], "entities": []}, {"text": "A natural approach for trying to shore up the skills of weak bilinguals is to pair them with a native speaker of the target language to edit their translations.", "labels": [], "entities": []}, {"text": "We review relevant research from NLP and human-computer interaction (HCI) on collaborative translation processes in Section 2.", "labels": [], "entities": []}, {"text": "To sort good translations from bad, researchers often solicit multiple, redundant translations and then build models to try to predict which translations are the best, or which translators tend to produce the highest quality translations.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: \u2022 An analysis of the difficulties posed by a twostep collaboration between editors and translators in Mechanical Turk-style crowdsourcing environments.", "labels": [], "entities": []}, {"text": "Editors vary in quality, and poor editing can be difficult to detect.", "labels": [], "entities": []}, {"text": "\u2022 A new graph-based algorithm for selecting the best translation among multiple translations of the same input.", "labels": [], "entities": []}, {"text": "This method takes into account the collaborative relationship between the translators and the editors.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are interested in testing our random walk method, which incorporates information from both the candidate translations and from the Turkers.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9609301686286926}]}, {"text": "We want to test two versions of our proposed collaborative co-ranking method: 1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations.", "labels": [], "entities": []}, {"text": "Metric Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score () for one professional translator (P1) using the other three (P2,3,4) as a reference set.", "labels": [], "entities": [{"text": "Bilingual Evaluation Understudy (BLEU) score", "start_pos": 78, "end_pos": 122, "type": "METRIC", "confidence": 0.9145510537283761}]}, {"text": "We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation.", "labels": [], "entities": []}, {"text": "In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9987228512763977}]}, {"text": "Therefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets.", "labels": [], "entities": []}, {"text": "This allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9767770171165466}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9934506416320801}]}, {"text": "Baselines As a naive baseline, we choose one candidate translation at random for each input Urdu sentence.", "labels": [], "entities": []}, {"text": "To establish an upper bound for our methods, and to determine if there exist highquality Turker translations at all, we compute four: Overall BLEU performance for all methods (with and without post-editing).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9992941617965698}]}, {"text": "The highlighted result indicates the best performance, which is based on both candidate sentences and Turker information.", "labels": [], "entities": [{"text": "Turker", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9175565838813782}]}, {"text": "The first oracle operates at the segment level on the sentences produced by translators only: for each source segment, we choose from the translations the one that scores highest (in terms of BLEU) against the reference sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9990586638450623}]}, {"text": "The second oracle is applied similarly, but chooses from the candidates produced by the collaboration of translator/post-editor pairs.", "labels": [], "entities": []}, {"text": "The third oracle operates at the worker level: for each source segment, we choose from the translations the one provided by the worker whose translations (over all sentences) score the highest on average.", "labels": [], "entities": []}, {"text": "The fourth oracle also operates at the worker level, but selects from sentences produced by translator/post-editor collaborations.", "labels": [], "entities": []}, {"text": "These oracle methods represent ideal solutions under our scenario.", "labels": [], "entities": []}, {"text": "We also examine two voting-inspired methods.", "labels": [], "entities": []}, {"text": "The first method selects the translation with the minimum average TER () against the other translations; intuitively, this would represent the \"consensus\" translation.", "labels": [], "entities": [{"text": "TER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9974178075790405}]}, {"text": "The second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER.", "labels": [], "entities": [{"text": "TER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9977116584777832}]}, {"text": "Results A summary of our results in given in Table 2.", "labels": [], "entities": []}, {"text": "As expected, random selection yields bad performance, with a BLEU score of 30.52.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9997411370277405}]}, {"text": "The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9440515637397766}]}, {"text": "Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations.", "labels": [], "entities": []}, {"text": "On average, the reference translations give a score of 42.38.", "labels": [], "entities": []}, {"text": "To put this in perspective, the output of a state-of-the-: Effect of candidate-Turker coupling (\u03bb) on BLEU score.", "labels": [], "entities": [{"text": "candidate-Turker coupling (\u03bb)", "start_pos": 69, "end_pos": 98, "type": "METRIC", "confidence": 0.7622143745422363}, {"text": "BLEU score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.972042441368103}]}, {"text": "art machine translation system (the syntax-based variant of Joshua) achieves a score of 26.91, which is reported in).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.776950478553772}]}, {"text": "The approach which selects the translations with the minimum average TER () against the other three translations (the \"consensus\" translation) achieves BLEU scores of 35.78.", "labels": [], "entities": [{"text": "TER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9977692365646362}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9991762042045593}]}, {"text": "Using the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and CallisonBurch (2011)' s reported score of 28.13, which they achieved using a linear feature-based classification.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.999362051486969}]}, {"text": "Their linear classifier achieved a reported score of 39.06 2 when combining information from both translators and editors.", "labels": [], "entities": []}, {"text": "In contrast, our proposed graph-based ranking framework achieves a score of 41.43 when using the same information.", "labels": [], "entities": []}, {"text": "This boost in BLEU score confirms our intuition that the hidden collaboration networks between candidate translations and transltor/editor pairs are indeed useful.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9698992073535919}]}, {"text": "Parameter Tuning There are two parameters in our experimental setups: \u00b5 controls the probability of starting anew random walk and \u03bb controls the coupling between the candidate and Turker subgraphs.", "labels": [], "entities": [{"text": "Parameter Tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.620212197303772}]}, {"text": "We set the damping factor \u00b5 to 0.85, following the standard PageRank paradigm.", "labels": [], "entities": [{"text": "damping factor \u00b5", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.746783177057902}, {"text": "PageRank paradigm", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.916474312543869}]}, {"text": "In order to determine a value for \u03bb, we used the average BLEU, computed against the professional refer-  ence translations, as a tuning metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9989111423492432}]}, {"text": "We experimented with values of \u03bb ranging from 0 to 1, with a step size of 0.05 ().", "labels": [], "entities": []}, {"text": "Small \u03bb values place little emphasis on the candidate/Turker coupling, whereas larger values rely more heavily on the coranking.", "labels": [], "entities": []}, {"text": "Overall, we observed better performance with values within the range of 0.05-0.15.", "labels": [], "entities": []}, {"text": "This suggests that both sources of information-the candidate itself and its authors-are important for the crowdsourcing translation task.", "labels": [], "entities": [{"text": "crowdsourcing translation", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.9077674448490143}]}, {"text": "In all of our reported results, we used the \u03bb = 0.1.", "labels": [], "entities": []}, {"text": "Analysis We examine the relative contribution of each component of our approach on the overall performance.", "labels": [], "entities": []}, {"text": "We first examine the centroid-based ranking on the candidate sub-graph (G C ) alone to seethe effect of voting among translated sentences; we denote this strategy as plain ranking.", "labels": [], "entities": []}, {"text": "Then we incorporate the standard random walk on the Turker graph (G T ) to include the structural information but without yet including any collaboration information; that is, we incorporate information from G T and G C without including edges linking the two together.", "labels": [], "entities": [{"text": "Turker graph (G T )", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.923546572526296}]}, {"text": "The co-ranking paradigm is exactly the same as the framework described in Section 3.2, but with simplified structures.", "labels": [], "entities": []}, {"text": "Finally, we examine the two-step collaboration based candidate-Turker graph using several variations on edge establishment.", "labels": [], "entities": []}, {"text": "As before, the nodes are the translator/post-editor working pairs.", "labels": [], "entities": []}, {"text": "We investigate three settings in which 1) edges connect two nodes when they share only a translator, 2) edges connect two nodes when they share only a post-editor, and 3) edges connect two nodes when they share either a translator or a post-editor.", "labels": [], "entities": []}, {"text": "These results are summarized in.", "labels": [], "entities": []}, {"text": "Interestingly, we observe that when modeling the linkage between the collaboration pairs, connecting Turker pairs which share either a translator or the post-editor achieves better performance than connecting pairs that share only translators or connecting pairs which share only editors.", "labels": [], "entities": []}, {"text": "This result supports the intuition that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence provides better predictions for candidate quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overall BLEU performance for all  methods (with and without post-editing). The  highlighted result indicates the best performance,  which is based on both candidate sentences and  Turker information.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9976922273635864}]}, {"text": " Table 3: Variations of all component settings.", "labels": [], "entities": []}]}