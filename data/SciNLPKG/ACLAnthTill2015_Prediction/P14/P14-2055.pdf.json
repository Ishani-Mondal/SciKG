{"title": [{"text": "A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization", "labels": [], "entities": [{"text": "Automatic Text Summarization", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6499834458033243}]}], "abstractContent": [{"text": "In order to summarize a document, it is often useful to have a background set of documents from the domain to serve as a reference for determining new and important information in the input document.", "labels": [], "entities": [{"text": "summarize a document", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8731511831283569}]}, {"text": "We present a model based on Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus.", "labels": [], "entities": []}, {"text": "Specifically, the method quantifies the degree to which pieces of information in the input change one's beliefs' about the world represented in the background.", "labels": [], "entities": []}, {"text": "We develop systems for generic and update summariza-tion based on this idea.", "labels": [], "entities": []}, {"text": "Our method provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Important facts in anew text are those which deviate from previous knowledge on the topic.", "labels": [], "entities": []}, {"text": "When people create summaries, they use their knowledge about the world to decide what content in an input document is informative to include in a summary.", "labels": [], "entities": []}, {"text": "Understandably in automatic summarization as well, it is useful to keep a background set of documents to represent general facts and their frequency in the domain.", "labels": [], "entities": [{"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.826216459274292}]}, {"text": "For example, in the simplest setting of multidocument summarization of news, systems are asked to summarize an input set of topicallyrelated news documents to reflect its central content.", "labels": [], "entities": [{"text": "multidocument summarization of news", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.7437417209148407}, {"text": "summarize an input set of topicallyrelated news documents", "start_pos": 98, "end_pos": 155, "type": "TASK", "confidence": 0.7661626935005188}]}, {"text": "In this GENERIC task, some of the best reported results were obtained by a system) which computed importance scores for words in the input by examining if the word occurs with significantly higher probability in the input compared to a large background collection of news articles.", "labels": [], "entities": []}, {"text": "Other specialized summarization tasks explicitly require the use of background information.", "labels": [], "entities": [{"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.982751190662384}]}, {"text": "In the UPDATE summarization task, a system is given two sets of news documents on the same topic; the second contains articles published later in time.", "labels": [], "entities": [{"text": "UPDATE summarization task", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.8427108724912008}]}, {"text": "The system should summarize the important updates from the second set assuming a user has already read the first set of articles.", "labels": [], "entities": []}, {"text": "In this work, we present a Bayesian model for assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents.", "labels": [], "entities": []}, {"text": "Our model is based on the idea of Bayesian Surprise ().", "labels": [], "entities": [{"text": "Bayesian Surprise", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.5799793303012848}]}, {"text": "For illustration, assume that a user's background knowledge comprises of multiple hypotheses about the current state of the world and a probability distribution over these hypotheses indicates his degree of belief in each hypothesis.", "labels": [], "entities": []}, {"text": "For example, one hypothesis maybe that the political situation in Ukraine is peaceful, another where it is not.", "labels": [], "entities": []}, {"text": "Apriori assume the user favors the hypothesis about a peaceful Ukraine, i.e. the hypothesis has higher probability in the prior distribution.", "labels": [], "entities": []}, {"text": "Given new data, the evidence can be incorporated using Bayes Rule to compute the posterior distribution over the hypotheses.", "labels": [], "entities": []}, {"text": "For example, upon viewing news reports about riots in the country, a user would update his beliefs and the posterior distribution of the user's knowledge would have a higher probability fora riotous Ukraine.", "labels": [], "entities": []}, {"text": "Bayesian surprise is the difference between the prior and posterior distributions over the hypotheses which quantifies the extent to which the new data (the news report) has changed a user's prior beliefs about the world.", "labels": [], "entities": []}, {"text": "In this work, we exemplify how Bayesian surprise can be used to do content selection for text summarization.", "labels": [], "entities": [{"text": "content selection", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.6830038130283356}, {"text": "text summarization", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7601641118526459}]}, {"text": "Here a user's prior knowledge is approximated by a background corpus and we show how to identify sentences from the input set which are most surprising with respect to this background.", "labels": [], "entities": []}, {"text": "We use the method to do two types of summarization tasks: a) GENERIC news summarization which uses a large random collection of news articles as the background, and b) UP-DATE summarization where the background is a smaller but specific set of news documents on the same topic as the input set.", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.977792501449585}, {"text": "GENERIC news summarization", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6796746055285136}, {"text": "UP-DATE summarization", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.5691633522510529}]}, {"text": "We find that our method performs competitively with a previous log-likelihood ratio approach which identifies words with significantly higher probability in the input compared to the background.", "labels": [], "entities": []}, {"text": "The Bayesian approach is more advantageous in the update task, where the background corpus is smaller in size.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation results for generic summaries.  Systems in parentheses are significantly better.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results for update summaries.  Systems in parentheses are significantly better.", "labels": [], "entities": []}]}