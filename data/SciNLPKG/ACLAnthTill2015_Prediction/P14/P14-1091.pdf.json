{"title": [{"text": "Knowledge-Based Question Answering as Machine Translation", "labels": [], "entities": [{"text": "Knowledge-Based Question Answering", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5664387842019399}, {"text": "Machine Translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7563599646091461}]}], "abstractContent": [{"text": "A typical knowledge-based question answering (KB-QA) system faces two challenges: one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs.", "labels": [], "entities": [{"text": "knowledge-based question answering (KB-QA)", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.8061241606871287}]}, {"text": "Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework.", "labels": [], "entities": []}, {"text": "We translate questions to answers based on CYK parsing.", "labels": [], "entities": [{"text": "CYK parsing", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.7685602307319641}]}, {"text": "Answers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated.", "labels": [], "entities": [{"text": "question translation", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7251993268728256}]}, {"text": "A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs.", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 48, "end_pos": 75, "type": "METRIC", "confidence": 0.7734449803829193}]}, {"text": "Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs).", "labels": [], "entities": [{"text": "Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions", "start_pos": 0, "end_pos": 94, "type": "TASK", "confidence": 0.8479108293851216}]}, {"text": "Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser This work was finished while the author was visiting Microsoft Research Asia.; Then, the answers are retrieved from existing KBs using generated MRs as queries.", "labels": [], "entities": []}, {"text": "Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7389253973960876}, {"text": "answer retrieval", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7653535306453705}, {"text": "semantic parsing", "start_pos": 162, "end_pos": 178, "type": "TASK", "confidence": 0.7643576860427856}, {"text": "question answering procedure", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.7759263416131338}]}, {"text": "Borrowing ideas from machine translation (MT), we treat the QA task as a translation procedure.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8452707648277282}]}, {"text": "Like MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which uses offline-generated translation tables to translate source phrases into target translations, a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly, based on question patterns and relation expressions.", "labels": [], "entities": [{"text": "MT", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.8606218099594116}, {"text": "CYK parsing", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.6431168466806412}, {"text": "semantic parsing-based question translation", "start_pos": 269, "end_pos": 312, "type": "TASK", "confidence": 0.8512601554393768}]}, {"text": "The final answers can be obtained from the root cell.", "labels": [], "entities": []}, {"text": "Derivations generated during such a translation procedure are modeled by a linear model, and minimum error rate training (MERT)) is used to tune feature weights based on a set of question-answer pairs.", "labels": [], "entities": [{"text": "minimum error rate training (MERT))", "start_pos": 93, "end_pos": 128, "type": "METRIC", "confidence": 0.8600275942257473}]}, {"text": "shows an example: the question director of movie starred by Tom Hanks is translated to one of its answers Robert Zemeckis by three main steps: (i) translate director of to director of ; (ii) translate movie starred by Tom Hanks to one of its answers Forrest Gump; (iii) translate director of Forrest Gump to a final answer Robert Zemeckis.", "labels": [], "entities": []}, {"text": "Note that the updated question covered by Cell is obtained by combining the answers to question spans covered by Cell and Cell.", "labels": [], "entities": []}, {"text": "The contributions of this work are two-fold: (1) We propose a translation-based KB-QA method that integrates semantic parsing and QA in one unified framework.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.725541889667511}]}, {"text": "The benefit of our method is that we don't need to explicitly generate complete semantic structures for input questions.", "labels": [], "entities": []}, {"text": "Be-Cell Cell Cell  2 Translation-Based KB-QA", "labels": [], "entities": []}], "datasetContent": [{"text": "We first show the overall evaluation results of our KB-QA system and compare them with baseline's results on Dev and Test.", "labels": [], "entities": []}, {"text": "Note that we do not reimplement the baseline system, but just list their evaluation numbers reported in the paper.", "labels": [], "entities": []}, {"text": "Comparison results are listed in.", "labels": [], "entities": [{"text": "Comparison", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9379018545150757}]}, {"text": "Dev (Accuracy) Test (Accuracy) Baseline 32.9% 31.4% Our Method 42.5% (+9.6%) 37.5% (+6.1%): Accuracy on evaluation sets.", "labels": [], "entities": [{"text": "Dev (Accuracy) Test (Accuracy) Baseline", "start_pos": 0, "end_pos": 39, "type": "METRIC", "confidence": 0.7446278002527025}, {"text": "Accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9959134459495544}]}, {"text": "Accuracy is defined as the number of correctly answered questions divided by the total number of questions.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9920119643211365}]}, {"text": "shows our KB-QA method outperforms baseline on both Dev and Test.", "labels": [], "entities": []}, {"text": "We think the potential reasons of this improvement include: \u2022 Different methods are used to map NL phrases to KB predicates.", "labels": [], "entities": []}, {"text": "have used a lexicon extracted from a subset of ReVerb triples (, which is similar to the relation expression set used in question translation.", "labels": [], "entities": [{"text": "question translation", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.7609454989433289}]}, {"text": "But as our relation expressions are extracted by an in-house extractor, we can record their extraction-related statistics as extra information, and use them as features to measure the mapping quality.", "labels": [], "entities": []}, {"text": "Besides, as a portion of entities in our KB are extracted from Wiki, we know the oneto-one correspondence between such entities and Wiki pages, and use this information in relation expression extraction for entity disambiguation.", "labels": [], "entities": [{"text": "relation expression extraction", "start_pos": 172, "end_pos": 202, "type": "TASK", "confidence": 0.6906781395276388}, {"text": "entity disambiguation", "start_pos": 207, "end_pos": 228, "type": "TASK", "confidence": 0.7180411964654922}]}, {"text": "A lower disambiguation error rate results in better relation expressions.", "labels": [], "entities": []}, {"text": "\u2022 Question patterns are used to map NL context to KB predicates.", "labels": [], "entities": []}, {"text": "Context can be either continuous or discontinues phrases.", "labels": [], "entities": []}, {"text": "Although the size of this set is limited, they can actually cover head questions/queries 6 very well.", "labels": [], "entities": []}, {"text": "The underlying intuition of using patterns is that those high-frequent questions/queries should and can be treated and solved in the QA task, by involving human effort at a relative small price but with very impressive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9913672804832458}]}, {"text": "In order to figure out the impacts of question patterns and relation expressions, another experiment: Impacts of question patterns and relation expressions.", "labels": [], "entities": []}, {"text": "P recision is defined as the number of correctly answered questions divided by the number of questions with non-empty answers generated by our KB-QA system.", "labels": [], "entities": [{"text": "P recision", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8773346543312073}]}, {"text": "From we can see that the accuracy of RE only on Test (32.5%) is slightly better than baseline's result (31.4%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9998263716697693}, {"text": "RE", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9693612456321716}]}, {"text": "We think this improvement comes from two aspects: (1) The quality of the relation expressions is better than the quality of the lexicon entries used in the baseline; and (2) We use the extraction-related statistics of relation expressions as features, which brings more information to measure the confidence of mapping between NL phrases and KB predicates, and makes the model to be more flexible.", "labels": [], "entities": []}, {"text": "Meanwhile, QP only perform worse (11.8%) than RE only , due to coverage issue.", "labels": [], "entities": [{"text": "QP", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.8572533130645752}, {"text": "RE", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9760345220565796}, {"text": "coverage", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9976930022239685}]}, {"text": "But by comparing the precisions of these two settings, we find QP only (97.5%) outperforms RE only (73.2%) significantly, due to its high quality.", "labels": [], "entities": [{"text": "precisions", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.998713493347168}, {"text": "QP", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9536311626434326}, {"text": "RE", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9947739243507385}]}, {"text": "This means how to extract highquality question patterns is worth to be studied for the question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.8487999240557352}]}, {"text": "As the performance of our KB-QA system relies heavily on the k-best beam approximation, we evaluate the impact of the beam size and list the comparison results in.", "labels": [], "entities": []}, {"text": "We can see that as we increase k incrementally, the accuracy increase at the same time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9997561573982239}]}, {"text": "However, a larger k (e.g. 200) cannot bring significant improvements comparing to a smaller one (e.g., 20), but using a large k has a tremendous impact on system efficiency.", "labels": [], "entities": []}, {"text": "So we choose k = 20 as the optimal value in above experiments, which trades off between accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9994660019874573}]}, {"text": "Actually, the size of our system's search space is much smaller than the one of the semantic parser used in the baseline.This is due to the fact that, if triple queries generated by the question translation component cannot derive any answer from KB, we will discard such triple queries directly during the QA procedure.", "labels": [], "entities": []}, {"text": "We can see that using a small k can achieve better results than baseline, where the beam size is set to be 200.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of question patterns and relation  expressions.", "labels": [], "entities": []}, {"text": " Table 4: Impacts of question patterns and relation  expressions. P recision is defined as the num- ber of correctly answered questions divided by the  number of questions with non-empty answers gen- erated by our KB-QA system.", "labels": [], "entities": [{"text": "P recision", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9321243166923523}]}]}