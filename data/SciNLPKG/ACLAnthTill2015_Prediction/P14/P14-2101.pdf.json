{"title": [{"text": "Automatic Labelling of Topic Models Learned from Twitter by Summarisation", "labels": [], "entities": [{"text": "Automatic Labelling of Topic Models", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7542954087257385}, {"text": "Summarisation", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.8916448354721069}]}], "abstractContent": [{"text": "Latent topics derived by topic models such as Latent Dirichlet Allocation (LDA) are the result of hidden thematic structures which provide further insights into the data.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.5991837829351425}]}, {"text": "The automatic labelling of such topics derived from social media poses however new challenges since topics may characterise novel events happening in the real world.", "labels": [], "entities": []}, {"text": "Existing automatic topic labelling approaches which depend on external knowledge sources become less applicable here since relevant articles/concepts of the extracted topics may not exist in external sources.", "labels": [], "entities": []}, {"text": "In this paper we propose to address the problem of automatic labelling of latent topics learned from Twit-ter as a summarisation problem.", "labels": [], "entities": [{"text": "automatic labelling of latent topics learned from Twit-ter", "start_pos": 51, "end_pos": 109, "type": "TASK", "confidence": 0.6920675337314606}, {"text": "summarisation", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.9618750214576721}]}, {"text": "We introduce a framework which apply sum-marisation algorithms to generate topic labels.", "labels": [], "entities": []}, {"text": "These algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic.", "labels": [], "entities": []}, {"text": "We compare the efficiency of existing state of the art summarisation algorithms.", "labels": [], "entities": []}, {"text": "Our results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9808031320571899}]}], "introductionContent": [{"text": "Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis and event detection ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.9542838931083679}, {"text": "event detection", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.7068418860435486}]}, {"text": "However, one of the main challenges is the task of understanding the semantics of a topic.", "labels": [], "entities": [{"text": "understanding the semantics of a topic", "start_pos": 51, "end_pos": 89, "type": "TASK", "confidence": 0.8380281428496043}]}, {"text": "This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence ( and for characterising the semantic content of a topic through automatic labelling techniques (.", "labels": [], "entities": [{"text": "characterising the semantic content of a topic", "start_pos": 132, "end_pos": 178, "type": "TASK", "confidence": 0.8346258231571743}]}, {"text": "In this paper we focus on the latter.", "labels": [], "entities": []}, {"text": "Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic.", "labels": [], "entities": [{"text": "automatic labelling a topic", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.7320149764418602}]}, {"text": "The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (.", "labels": [], "entities": []}, {"text": "Such top words are usually ranked using the marginal probabilities P (w i |t j ) associated with each word w i fora given topic t j . This task can be illustrated by considering the following topic derived from social media related to Education: school protest student fee choic motherlod tuition teacher anger polic where the top 10 words ranked by P (w i |t j ) for this topic are listed.", "labels": [], "entities": [{"text": "school protest student fee choic motherlod tuition teacher anger polic", "start_pos": 246, "end_pos": 316, "type": "TASK", "confidence": 0.652991133928299}]}, {"text": "Therefore the task is to find the top-n terms which are more representative of the given topic.", "labels": [], "entities": []}, {"text": "In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which can be used as a good label for this topic.", "labels": [], "entities": []}, {"text": "However previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic (.", "labels": [], "entities": [{"text": "interpreting the coherent meaning of a topic", "start_pos": 66, "end_pos": 110, "type": "TASK", "confidence": 0.8323512077331543}]}, {"text": "More recent approaches have explored the use of external sources (e.g. Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical ( or graphbased () algorithms applied on these sources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.8844224214553833}]}, {"text": "proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model.", "labels": [], "entities": []}, {"text": "Their proposed approach was defined as an optimisation problem involving the minimisation of the KL divergence between a given topic and the candidate labels while maximising the mutual information between these two word distributions.", "labels": [], "entities": []}, {"text": "proposed to label topics by selecting top-n terms to label the overall topic based on different ranking mechanisms including pointwise mutual information and conditional probabilities.", "labels": [], "entities": []}, {"text": "Methods relying on external sources for automatic labelling of topics include the work by which derived candidate topic labels for topics induced by LDA using the hierarchy obtained from the Google Directory service and expanded through the use of the OpenOffice English Thesaurus.", "labels": [], "entities": []}, {"text": "generated label candidates fora topic based on topranking topic terms and titles of Wikipedia articles.", "labels": [], "entities": []}, {"text": "They then built a Support Vector Regression (SVR) model for ranking the label candidates.", "labels": [], "entities": [{"text": "Support Vector Regression (SVR)", "start_pos": 18, "end_pos": 49, "type": "METRIC", "confidence": 0.8466035525004069}]}, {"text": "More recently, proposed to make use of a structured data source (DBpedia) and employed graph centrality measures to generate semantic concept labels which can characterise the content of a topic.", "labels": [], "entities": []}, {"text": "Most previous topic labelling approaches focus on topics derived from well formatted and static documents.", "labels": [], "entities": [{"text": "topic labelling", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7382640242576599}]}, {"text": "However in contrast to this type of content, the labelling of topics derived from tweets presents different challenges.", "labels": [], "entities": []}, {"text": "In nature micropost content is sparse and present ill-formed words.", "labels": [], "entities": []}, {"text": "Moreover, the use of Twitter as the \"what'shappening-right now\" tool, introduces new eventdependent relations between words which might not have a counterpart in existing knowledge sources (e.g. Wikipedia).", "labels": [], "entities": []}, {"text": "Our original interest in labelling topics stems from work in topic model based event extraction from social media, in particular from tweets (.", "labels": [], "entities": [{"text": "topic model based event extraction from social media", "start_pos": 61, "end_pos": 113, "type": "TASK", "confidence": 0.7185263559222221}]}, {"text": "As opposed to previous approaches, the research presented in this paper addresses the labelling of topics exposing event-related content that might not have a counterpart on existing external sources.", "labels": [], "entities": []}, {"text": "Based on the observation that a short summary of a collection of documents can serve as a label characterising the collection, we propose to generate topic label candidates based on the summarisation of a topic's relevant documents.", "labels": [], "entities": []}, {"text": "Our contributions are two-fold: -We propose a novel approach for topics labelling that relies on term relevance of documents relating to a topic; and -We show that summarisation algorithms, which are independent of extenal sources, can be used with success to label topics, presenting a higher perfomance than the top-n terms baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our Twitter Corpus (TW) was collected between November 2010 and January 2011.", "labels": [], "entities": [{"text": "Twitter Corpus (TW) was collected", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.9036866341318402}]}, {"text": "TW comprises over 1 million tweets.", "labels": [], "entities": [{"text": "TW", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8213990330696106}]}, {"text": "We used the OpenCalais' document categorisation service 1 to generate categorical sets.", "labels": [], "entities": [{"text": "OpenCalais' document categorisation service 1", "start_pos": 12, "end_pos": 57, "type": "DATASET", "confidence": 0.8677004814147949}]}, {"text": "In particular, we considered four different categories which contain many real-world events, namely: War and Conflict (War), Disaster and Accident (DisAc), Education (Edu) and Law and Crime (LawCri).", "labels": [], "entities": []}, {"text": "The final TW dataset after removing retweets and short microposts (less than 5 words after removing stopwords) contains 7000 tweets in each category.", "labels": [], "entities": [{"text": "TW dataset", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.9427089393138885}]}, {"text": "We preprocessed TW by first removing: punctuation, numbers, non-alphabet characters, stop words, user mentions, and URL links.", "labels": [], "entities": []}, {"text": "We then performed Porter stemming) in order to reduce the vocabulary size.", "labels": [], "entities": [{"text": "Porter stemming", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.665240153670311}]}, {"text": "Finally to address the issue of data sparseness in the TW dataset, we removed words with a frequency lower than 5.", "labels": [], "entities": [{"text": "TW dataset", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9779632389545441}]}, {"text": "We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.966277003288269}]}, {"text": "These TT set corresponds to the top x terms ranked based on the probability of the word given the topic (p(w|k)) from the topic model.", "labels": [], "entities": [{"text": "TT", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.8326427340507507}]}, {"text": "We evaluated these summarisation approaches with the ROUGE-1 method), a widely used summarisation evaluation metric that correlates well with human evaluation (.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9688528776168823}, {"text": "ROUGE-1", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9585710763931274}, {"text": "summarisation evaluation", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.8986424803733826}]}, {"text": "This method measures the overlap of words between the generated summary and a reference, in our case the GS generated from the NW dataset.", "labels": [], "entities": [{"text": "NW dataset", "start_pos": 127, "end_pos": 137, "type": "DATASET", "confidence": 0.8304364681243896}]}, {"text": "The evaluation was performed at x = {1, .., 10}.", "labels": [], "entities": []}, {"text": "presents the ROUGE-1 performance of the summarisation approaches as the lengthx of the generated topic label increases.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9890748858451843}, {"text": "lengthx", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9646787643432617}]}, {"text": "We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases.", "labels": [], "entities": []}, {"text": "In particular, in both the Education and Law & Crime categories, both SB and TFIDF outperforms TT and TR by a large margin.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.786747932434082}]}, {"text": "The obtained ROUGE-1 performance is within the same range of performance previously reported on Social Media summarisation.: Average ROUGE-1 for topic labels at x = {1..10}, generated from the TW dataset.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9454551935195923}, {"text": "Social Media summarisation.", "start_pos": 96, "end_pos": 123, "type": "DATASET", "confidence": 0.8289884527524313}, {"text": "ROUGE-1", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9430643916130066}, {"text": "TW dataset", "start_pos": 193, "end_pos": 203, "type": "DATASET", "confidence": 0.9925958812236786}]}, {"text": "The generated labels with summarisation at x = 5 are presented in, where GS represents the label generated from the Newswire headlines.", "labels": [], "entities": [{"text": "GS", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9740372896194458}, {"text": "Newswire headlines", "start_pos": 116, "end_pos": 134, "type": "DATASET", "confidence": 0.984450101852417}]}, {"text": "Different summarisation techniques reveal words which do not appear in the top terms but which are relevant to the information clustered by the topic.", "labels": [], "entities": []}, {"text": "In this way, the labels generated for topics belonging to different categories generally extend the information provided by the top terms.", "labels": [], "entities": []}, {"text": "For example in, the DisAc headline is characteristic of the New Zealand's Pike River's coalmine blast accident, which is an event occurred in November 2010.", "labels": [], "entities": [{"text": "DisAc headline", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.903461366891861}, {"text": "New Zealand's Pike River's coalmine blast accident", "start_pos": 60, "end_pos": 110, "type": "DATASET", "confidence": 0.8135912153455946}]}, {"text": "Although the top 5 terms set from the LDA topic extracted from TW (listed under TT) does capture relevant information related to the event, it does not provide information regarding the blast.", "labels": [], "entities": [{"text": "LDA topic extracted from TW", "start_pos": 38, "end_pos": 65, "type": "DATASET", "confidence": 0.7950062394142151}]}, {"text": "In this sense the topic label generated by SB more accurately describes this event.", "labels": [], "entities": []}, {"text": "We can also notice that the GS labels generated from Newswire media presented in appear on their own, to be good labels for the TW topics.", "labels": [], "entities": [{"text": "GS labels generated from Newswire media", "start_pos": 28, "end_pos": 67, "type": "DATASET", "confidence": 0.7795113523801168}, {"text": "TW topics", "start_pos": 128, "end_pos": 137, "type": "TASK", "confidence": 0.5612537860870361}]}, {"text": "However as we described in the introduction we want to avoid relaying on external sources for the derivation of topic labels.", "labels": [], "entities": []}, {"text": "This experiment shows that frequency based summarisation techniques outperform graphbased and relevance based summarisation techniques for generating topic labels that improve upon the top-terms baseline, without relying on external sources.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9456033706665039}]}, {"text": "This is an attractive property for automatically generating topic labels for tweets where their event-related content might not have a counterpart on existing external sources.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average ROUGE-1 for topic labels at x =  {1..10}, generated from the TW dataset.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9459840059280396}, {"text": "ROUGE-1", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.6188521981239319}, {"text": "TW dataset", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9901755750179291}]}]}