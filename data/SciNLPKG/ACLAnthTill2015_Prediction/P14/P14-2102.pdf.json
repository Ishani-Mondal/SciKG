{"title": [{"text": "Stochastic Contextual Edit Distance and Probabilistic FSTs", "labels": [], "entities": [{"text": "FSTs", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.6910413503646851}]}], "abstractContent": [{"text": "String similarity is most often measured by weighted or unweighted edit distance d(x, y).", "labels": [], "entities": []}, {"text": "Ristad and Yianilos (1998) defined stochastic edit distance-a probability distribution p(y | x) whose parameters can be trained from data.", "labels": [], "entities": []}, {"text": "We generalize this so that the probability of choosing each edit operation can depend on contex-tual features.", "labels": [], "entities": []}, {"text": "We show how to construct and train a probabilistic finite-state transducer that computes our stochastic con-textual edit distance.", "labels": [], "entities": []}, {"text": "To illustrate the improvement from conditioning on context, we model typos found in social media text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many problems in natural language processing can be viewed as stochastically mapping one string to another: e.g., transliteration, pronunciation modeling, phonology, morphology, spelling correction, and text normalization.", "labels": [], "entities": [{"text": "pronunciation modeling", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.8232986927032471}, {"text": "spelling correction", "start_pos": 178, "end_pos": 197, "type": "TASK", "confidence": 0.8639176487922668}, {"text": "text normalization", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.7688132226467133}]}, {"text": "describe how to train the parameters of a stochastic editing process that moves through the input string x from left to right, transforming it into the output stringy.", "labels": [], "entities": []}, {"text": "In this paper we generalize this process so that the edit probabilities are conditioned on input and output context.", "labels": [], "entities": []}, {"text": "We further show how to model the conditional distribution p(y | x) as a probabilistic finite-state transducer (PFST), which can be easily combined with other transducers or grammars for particular applications.", "labels": [], "entities": []}, {"text": "We contrast our probabilistic transducers with the more general framework of weighted finite-state transducers (WFST), explaining why our restriction provides computational advantages when reasoning about unknown strings.", "labels": [], "entities": []}, {"text": "Constructing the finite-state transducer is tricky, so we give the explicit construction for use by others.", "labels": [], "entities": []}, {"text": "We describe how to train its parameters when the contextual edit probabilities are given by a loglinear model.", "labels": [], "entities": []}, {"text": "We provide a library for training both PFSTs and WFSTs that works with OpenFST (, and we illustrate its use with simple experiments on typos, which demonstrate the benefit of context.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8310201168060303}]}], "datasetContent": [{"text": "To demonstrate the utility of contextual edit transducers, we examine spelling errors in social media data.", "labels": [], "entities": []}, {"text": "Models of spelling errors are useful in a variety of settings including spelling correction itself and phylogenetic models of string variation ().", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.9048759341239929}]}, {"text": "To eliminate experimental confounds, we use no dictionary or language model as one would in practice, but directly evaluate our ability to model p(correct | misspelled).", "labels": [], "entities": []}, {"text": "Consider (x k , y k ) = (feeel, feel).", "labels": [], "entities": []}, {"text": "Our model defines p(y | x k ) for ally.", "labels": [], "entities": []}, {"text": "Our training objective (section 6) tries to make this large for y = y k . A contextual edit model learns here that e \u2192 is more likely in the context of ee.", "labels": [], "entities": []}, {"text": "We report on test data how much probability mass lands on the true y k . We also report how much mass lands \"near\" y k , by measuring the expected edit distance of the predicted y to the truth.", "labels": [], "entities": []}, {"text": "Expected edit distance is defined as y p \u03b8 (y | x k )d(y, y k ) where d(y, y k ) is the Levenshtein distance between two strings.", "labels": [], "entities": [{"text": "Expected edit distance", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.5891332725683848}]}, {"text": "It can be computed using standard finite-state algorithms.", "labels": [], "entities": []}], "tableCaptions": []}