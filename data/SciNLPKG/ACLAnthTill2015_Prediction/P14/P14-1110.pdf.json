{"title": [{"text": "Polylingual Tree-Based Topic Models for Translation Domain Adaptation", "labels": [], "entities": [{"text": "Translation Domain Adaptation", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.9195993741353353}]}], "abstractContent": [{"text": "Topic models, an unsupervised technique for inferring translation domains improve machine translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7750945091247559}]}, {"text": "However, previous work uses only the source language and completely ignores the target language, which can disambiguate domains.", "labels": [], "entities": []}, {"text": "We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes.", "labels": [], "entities": []}, {"text": "We evaluate our model on a Chinese to En-glish translation task and obtain up to 1.2 BLEU improvement over strong baselines.", "labels": [], "entities": [{"text": "Chinese to En-glish translation task", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.5714376330375671}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9992607235908508}]}], "introductionContent": [{"text": "Probabilistic topic models, exemplified by latent Dirichlet allocation (, are one of the most popular statistical frameworks for navigating large unannotated document collections.", "labels": [], "entities": [{"text": "navigating large unannotated document collections", "start_pos": 129, "end_pos": 178, "type": "TASK", "confidence": 0.7449840426445007}]}, {"text": "Topic models discover-without any supervision-the primary themes presented in a dataset: the namesake topics.", "labels": [], "entities": []}, {"text": "Topic models have two primary applications: to aid human exploration of corpora () or serve as a low-dimensional representation for downstream applications.", "labels": [], "entities": []}, {"text": "We focus on the second application, which has been fruitful for computer vision), computational biology (, and information retrieval).", "labels": [], "entities": [{"text": "computational biology", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.793546736240387}, {"text": "information retrieval", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.8067680299282074}]}, {"text": "In particular, we use topic models to aid statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7211820284525553}]}, {"text": "Modern machine translation systems use millions of examples of translations to learn translation rules.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7300490736961365}]}, {"text": "These systems work best when the training corpus has consistent genre, register, and topic.", "labels": [], "entities": []}, {"text": "Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.713647797703743}]}, {"text": "As we review in Section 2, topic models area promising solution for automatically discovering domains in machine translation corpora.", "labels": [], "entities": [{"text": "machine translation corpora", "start_pos": 105, "end_pos": 132, "type": "TASK", "confidence": 0.7483797371387482}]}, {"text": "However, past work either relies solely on monolingual source-side models, or limited modeling of the target side ().", "labels": [], "entities": []}, {"text": "In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models () are wilfully ignoring available information on the target side that could aid domain discovery.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7797553241252899}, {"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9903362989425659}, {"text": "domain discovery", "start_pos": 295, "end_pos": 311, "type": "TASK", "confidence": 0.7363118827342987}]}, {"text": "This is not fora lack of multilingual topic models.", "labels": [], "entities": []}, {"text": "Topic models bridge the chasm between languages using document connections ( ), dictionaries, and word alignments ().", "labels": [], "entities": [{"text": "word alignments", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.6934696137905121}]}, {"text": "In Section 2, we review these models for discovering topics in multilingual datasets and discuss how they can improve SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.996130108833313}]}, {"text": "However, no models combine multiple bridges between languages.", "labels": [], "entities": []}, {"text": "In Section 3, we create a model-the polylingual tree-based topic models (ptLDA)-that uses information from both external dictionaries and document alignments simultaneously.", "labels": [], "entities": []}, {"text": "In Section 4, we derive both MCMC and variational inference for this new topic model.", "labels": [], "entities": [{"text": "variational", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9431118369102478}]}, {"text": "In Section 5, we evaluate our model on the task of SMT using aligned datasets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9957526922225952}]}, {"text": "We show that ptLDA offers better domain adaptation than other topic models for machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.740431159734726}, {"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7032326012849808}]}, {"text": "Finally, in Section 6, we show how these topic models improve SMT with detailed examples.", "labels": [], "entities": [{"text": "SMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9957385063171387}]}], "datasetContent": [{"text": "We evaluate our new topic model, ptLDA, and existing topic models-LDA, pLDA, and tLDA-on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7807170152664185}]}, {"text": "Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).", "labels": [], "entities": []}, {"text": "We also examine different inference algorithmsGibbs sampling (gibbs), variational inference (variational) and hybrid approach (variationalhybrid)-on the effects of SMT performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.9961708188056946}]}, {"text": "In all experiments, we set the per-document Dirichlet parameter \u03b1 = 0.01 and the number of topics to 10, as used in.", "labels": [], "entities": [{"text": "Dirichlet parameter \u03b1", "start_pos": 44, "end_pos": 65, "type": "METRIC", "confidence": 0.871069073677063}]}, {"text": "Resources for Prior Tree To build the tree for tLDA and ptLDA, we extract the word correlations from a Chinese-English bilingual dictionary.", "labels": [], "entities": []}, {"text": "We filter the dictionary using the NIST vocabulary, and keep entries mapping single Chinese and single English words.", "labels": [], "entities": [{"text": "NIST vocabulary", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9745881855487823}]}, {"text": "The prior tree has about 1000 word pairs (dict).", "labels": [], "entities": []}, {"text": "We also extract the bidirectional word alignments between Chinese and English using GIZA++.", "labels": [], "entities": []}, {"text": "We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align).", "labels": [], "entities": []}, {"text": "We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDAalign.", "labels": [], "entities": []}, {"text": "However, tLDA-align and ptLDA-align do worse than tLDA-dict and ptLDA-dict, so we omit tLDA-align in the results.", "labels": [], "entities": []}, {"text": "Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics-BLEU () and TER).", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7657823264598846}, {"text": "domain adaptation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7429990172386169}, {"text": "SMT evaluation", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.9045416116714478}, {"text": "TER", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9887563586235046}]}, {"text": "We report the results on three different test sets, and all SMT results are averaged over five runs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9862169027328491}]}, {"text": "We refer to the SMT model without domain adaptation as baseline.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9890230894088745}]}, {"text": "LDA marginally improves machine translation (less than half a BLEU point).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7911213636398315}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9993883371353149}]}, {"text": "Polylingual topic models pLDA and tree-based topic models tLDA-dict are consistently better than LDA, suggesting that incorporating additional bilingual knowledge improves topic models.", "labels": [], "entities": []}, {"text": "These improvements are not redundant: our new ptLDA-dict model, which has aspects of both models yields the best performance among these approaches-up to a 1.2 BLEU point gain (higher is better), and -2.6 TER improvement (lower is better).", "labels": [], "entities": [{"text": "BLEU point gain", "start_pos": 160, "end_pos": 175, "type": "METRIC", "confidence": 0.9754577080408732}, {"text": "TER", "start_pos": 205, "end_pos": 208, "type": "METRIC", "confidence": 0.9970492720603943}]}, {"text": "The BLEU improvement is significant) at p = 0.01, 6 except on MT03 with variational and variationalhybrid inference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9994301199913025}, {"text": "MT03", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9207555651664734}, {"text": "variational", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9507958292961121}]}, {"text": "While ptLDA-align performs better than baseline SMT and LDA, it is worse than ptLDA-dict, possibly because of errors in the word alignments, making the tree priors less effective.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9145814776420593}]}, {"text": "Scalability While gibbs has better translation scores than variational and variational-hybrid, it is less scalable to larger datasets.", "labels": [], "entities": []}, {"text": "With 1.6M NIST Because we have multiple runs of each topic model (and thus different translation models), we select the run closest to the average BLEU for the translation significance test.", "labels": [], "entities": [{"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8534665107727051}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9985319375991821}, {"text": "translation significance", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.7947709262371063}]}, {"text": "training sentences, gibbs takes nearly a week to run 1000 iterations.", "labels": [], "entities": []}, {"text": "In contrast, the parallelized variational and variational-hybrid approaches, which we implement in MapReduce (, take less than a day to converge.", "labels": [], "entities": []}], "tableCaptions": []}