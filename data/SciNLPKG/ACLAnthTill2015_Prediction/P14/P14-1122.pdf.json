{"title": [{"text": "Validating and Extending Semantic Knowledge Bases using Video Games with a Purpose", "labels": [], "entities": [{"text": "Extending Semantic Knowledge Bases", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.8795149177312851}]}], "abstractContent": [{"text": "Large-scale knowledge bases are important assets in NLP.", "labels": [], "entities": [{"text": "NLP", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9031599164009094}]}, {"text": "Frequently, such resources are constructed through automatic mergers of complementary resources, such as WordNet and Wikipedia.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9449657201766968}]}, {"text": "However, manually validating these resources is prohibitively expensive, even when using methods such as crowdsourcing.", "labels": [], "entities": []}, {"text": "We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose.", "labels": [], "entities": []}, {"text": "Two video games were created to validate concept-concept and concept-image relations.", "labels": [], "entities": []}, {"text": "In experiments comparing with crowdsourc-ing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.7752016186714172}]}, {"text": "Semantic knowledge bases such as WordNet,, and BabelNet () provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness) and similarity (, paraphrasing), and word sense disambiguation ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9499503970146179}, {"text": "word sense disambiguation", "start_pos": 202, "end_pos": 227, "type": "TASK", "confidence": 0.6929311354955038}]}, {"text": "Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable.", "labels": [], "entities": []}, {"text": "However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required.", "labels": [], "entities": []}, {"text": "Recent approaches have attempted to build or extend these knowledge bases automatically.", "labels": [], "entities": []}, {"text": "For example, and extend WordNet using distributional or structural features to identify novel semantic connections between concepts.", "labels": [], "entities": []}, {"text": "The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases) through automatically merging WordNet and Wikipedia.", "labels": [], "entities": []}, {"text": "While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications.", "labels": [], "entities": []}, {"text": "To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing ().", "labels": [], "entities": []}, {"text": "However, these methods, too, are limited by the resources required for acquiring large numbers of responses.", "labels": [], "entities": []}, {"text": "In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose.", "labels": [], "entities": []}, {"text": "Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task.", "labels": [], "entities": []}, {"text": "While prior efforts in NLP have incorporated games for performing annotation and validation, these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task.", "labels": [], "entities": []}, {"text": "In contrast, we introduce two video games with graphical 2D gameplay that is similar to what game players are familiar with.", "labels": [], "entities": []}, {"text": "The fun nature of the games provides an intrinsic motivation for players to keep playing, which can increase the quality of their work and lower the cost per annotation.", "labels": [], "entities": []}, {"text": "Our work provides the following three contributions.", "labels": [], "entities": []}, {"text": "First, we demonstrate effective video gamebased methods for both validating and extending semantic networks, using two games that operate on complementary sources of information: semantic relations and sense-image mappings.", "labels": [], "entities": []}, {"text": "In contrast to previous work, the annotation quality is determined in a fully automatic way.", "labels": [], "entities": []}, {"text": "Second, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing.", "labels": [], "entities": []}, {"text": "Third, for both games, we show that games produce better quality annotations than crowdsourcing.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two experiments were performed with Infection and TKT: (1) an evaluation of players' ability to play accurately and to validate semantic relations and image associations and (2) a comprehensive cost comparison.", "labels": [], "entities": []}, {"text": "Each experiment compared (a) free and financially-incentivized versions of each game, (b) crowdsourcing, and (c) a non-video game with a purpose.", "labels": [], "entities": []}, {"text": "Gold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced fora 10% sample of each dataset (cf. Section 3.2).", "labels": [], "entities": [{"text": "Gold Standard Data", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9431346456209818}]}, {"text": "Two annotators independently rated the items and, in cases of disagreement, a third expert annotator adjudicated.", "labels": [], "entities": []}, {"text": "Unlike in the game setting, annotators were free to consult additional resources such as Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.938464343547821}]}, {"text": "To measure inter-annotator agreement (IAA) on the gold standard annotations, we calculated Krip-pendorff's \u03b1; \u03b1 ranges between [-1,1] where 1 indicates complete agreement, -1 indicates systematic disagreement, and values near 0 indicate agreement at chance levels.", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 11, "end_pos": 42, "type": "METRIC", "confidence": 0.8253507256507874}]}, {"text": "Gold standard annotators had high agreement, 0.774, for conceptconcept relations.", "labels": [], "entities": []}, {"text": "However, image-concept agreement was only moderate, 0.549.", "labels": [], "entities": []}, {"text": "A further analysis revealed differences in the annotators' thresholds for determining association, with one annotator permitting more abstract relations.", "labels": [], "entities": []}, {"text": "However, the adjudication process resolved these disputes, resulting in substantial agreement by all annotators on the final gold annotations.", "labels": [], "entities": []}, {"text": "Incentives At the start of each game, players were shown brief descriptions of the game and a description of a contest where the top-ranked players would win either (1) monetary prizes in the form of gift cards, or (2) a mention and thanks in this paper.", "labels": [], "entities": [{"text": "Incentives", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9295548796653748}]}, {"text": "We refer to these as the paid and free versions of the game, respectively.", "labels": [], "entities": []}, {"text": "In the paid setting, the five top-ranking players were offered gift cards valued at 25, 15, 15, 10, and 10 USD, starting from first place (a total of 75 USD per game).", "labels": [], "entities": []}, {"text": "To increase competition among players and to perform a fairer time comparison with crowdsourcing, the contest period was limited to two weeks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotation statistics from all sources. N -Accuracy denotes accuracy at rejecting items from N ;  G.S. Agreement denotes percentage agreement of the aggregated annotations with the gold standard.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.5057017803192139}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9970623850822449}, {"text": "G.S. Agreement", "start_pos": 108, "end_pos": 122, "type": "METRIC", "confidence": 0.8284409443537394}]}]}