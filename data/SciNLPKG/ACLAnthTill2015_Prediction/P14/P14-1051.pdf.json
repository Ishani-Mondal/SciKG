{"title": [{"text": "ReNew: A Semi-Supervised Framework for Generating Domain-Specific Lexicons and Sentiment Analysis", "labels": [], "entities": [{"text": "ReNew", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8554980754852295}, {"text": "Sentiment Analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8084030151367188}]}], "abstractContent": [{"text": "The sentiment captured in opinionated text provides interesting and valuable information for social media services.", "labels": [], "entities": []}, {"text": "However, due to the complexity and diversity of linguistic representations, it is challenging to build a framework that accurately extracts such sentiment.", "labels": [], "entities": []}, {"text": "We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level.", "labels": [], "entities": []}, {"text": "Our framework can greatly reduce the human effort for building a domain-specific sentiment lexicon with high quality.", "labels": [], "entities": []}, {"text": "Specifically, in our evaluation, working with just 20 manually labeled reviews, it generates a domain-specific sentiment lexicon that yields weighted average F-Measure gains of 3%.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9447118043899536}]}, {"text": "Our sentiment classification model achieves approximately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9438493847846985}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.998946487903595}]}], "introductionContent": [{"text": "Automatically extracting sentiments from usergenerated opinionated text is important in building social media services.", "labels": [], "entities": []}, {"text": "However, the complexity and diversity of the linguistic representations of sentiments make this problem challenging.", "labels": [], "entities": []}, {"text": "High-quality sentiment lexicons can improve the performance of sentiment analysis models over general-purpose lexicons.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.874224990606308}]}, {"text": "More advanced methods such as () adopt domain knowledge by extracting sentiment words from the domain-specific corpus.", "labels": [], "entities": []}, {"text": "However, depending on the context, the same word can have different polarities even in the same domain.", "labels": [], "entities": []}, {"text": "In respect to sentiment classification, infer the sentiments using basic features, such as bag-of-words.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.9200548827648163}]}, {"text": "To capture more complex linguistic phenomena, leading approaches) apply more advanced models but assume one document or sentence holds one sentiment.", "labels": [], "entities": []}, {"text": "However, this is often not the case.", "labels": [], "entities": []}, {"text": "Sentiments can change within one document, one sentence, or even one clause.", "labels": [], "entities": [{"text": "Sentiments", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.940567135810852}]}, {"text": "Also, existing approaches infer sentiments without considering the changes of sentiments within or between clauses.", "labels": [], "entities": []}, {"text": "However, these changes can be successfully exploited for inferring fine-grained sentiments.", "labels": [], "entities": []}, {"text": "To address the above shortcomings of lexicon and granularity, we propose a semi-supervised framework named ReNew.", "labels": [], "entities": []}, {"text": "(1) Instead of using sentences, ReNew uses segments as the basic units for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.9601792395114899}]}, {"text": "Segments can be shorter than sentences and therefore help capture fine-grained sentiments.", "labels": [], "entities": []}, {"text": "(2) ReNew leverages the relationships between consecutive segments to infer their sentiments and automatically generates a domain-specific sentiment lexicon in a semi-supervised fashion.", "labels": [], "entities": []}, {"text": "(3) To capture the contextual sentiment of words, ReNew uses dependency relation pairs as the basic elements in the generated sentiment lexicon.", "labels": [], "entities": []}, {"text": "Consider apart of a review from Tripadvisor.", "labels": [], "entities": [{"text": "Tripadvisor", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9359006881713867}]}, {"text": "We split it into six segments with sentiment labels.", "labels": [], "entities": []}, {"text": "(1: POS) The hotel was clean and comfortable.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9776949286460876}]}, {"text": "(2: POS) Service was friendly (3: POS) even providing us a late-morning check-in.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7787561416625977}, {"text": "POS", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.878703236579895}]}, {"text": "(4: POS) The room was quiet and comfortable, (5: NEG) but it was beginning to show a few small signs of wear and tear.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9582566022872925}, {"text": "NEG", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.6531174778938293}]}, {"text": "\" visualizes the sentiment changes within the text.", "labels": [], "entities": []}, {"text": "The sentiment remains the same across Segments 1 to 4.", "labels": [], "entities": []}, {"text": "The sentiment transition between Segments 4 and 5 is indicated by the transition cue \"but\"-which signals conflict and contradiction.", "labels": [], "entities": []}, {"text": "Assuming we know Segment 4 is positive, given the fact that Segment 5 starts with \"but,\" we can infer with high confidence that the sentiment in Segment 5 changes to neutral or negative even without looking at its content.", "labels": [], "entities": []}, {"text": "After classifying the sentiment of Segment 5 as NEG, we associate the dependency relation pairs {\"sign\", \"wear\"} and {\"sign\", \"tear\"} with that sentiment.", "labels": [], "entities": [{"text": "NEG", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.5142726898193359}]}, {"text": "ReNew can greatly reduce the human effort for building a domain-specific sentiment lexicon with high quality.", "labels": [], "entities": []}, {"text": "Specifically, in our evaluation on two real datasets, working with just 20 manually labeled reviews, ReNew generates a domainspecific sentiment lexicon that yields weighted average F-Measure gains of 3%.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9162253141403198}]}, {"text": "Additionally, our sentiment classification model achieves approximately 1% greater accuracy than a state-of-theart approach based on elementary discourse units (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.9404096901416779}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9989981055259705}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces some essential background.", "labels": [], "entities": []}, {"text": "Section 4 presents our experiments and results.", "labels": [], "entities": []}, {"text": "Section 5 reviews some related work.", "labels": [], "entities": []}, {"text": "Section 6 concludes this paper and outlines some directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess ReNew's effectiveness, we prepare two hotel review datasets crawled from Tripadvisor.", "labels": [], "entities": [{"text": "Tripadvisor", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.7204195857048035}]}, {"text": "One dataset contains a total of 4,017 unlabeled reviews regarding 802 hotels from seven US cities.", "labels": [], "entities": []}, {"text": "The reviews are posted by 340 users, each of whom contributes at least ten reviews.", "labels": [], "entities": []}, {"text": "The other dataset contains 200 reviews randomly selected from Tripadvisor.", "labels": [], "entities": [{"text": "Tripadvisor", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.9683879017829895}]}, {"text": "We collected ground-truth labels for this dataset by inviting six annotators in two groups of three.", "labels": [], "entities": []}, {"text": "Each group labeled the same 100 reviews.", "labels": [], "entities": []}, {"text": "We obtained the labels for each segment consist as positive, neutral, or negative.", "labels": [], "entities": []}, {"text": "Fleiss' kappa scores for the two groups were 0.70 and 0.68, respectively, indicating substantial agreement between our annotators.", "labels": [], "entities": []}, {"text": "The results we present in the remainder of this section rely upon the following parameter values.", "labels": [], "entities": []}, {"text": "The confidence thresholds used in the Label Integrator and filter are both set to 0.9 for positive labels and 0.7 for negative and neutral labels.", "labels": [], "entities": []}, {"text": "The minimum frequency used in the Lexicon Integrator for selecting triples is set to 4.", "labels": [], "entities": []}, {"text": "Our first experiment evaluates the effects of different combinations of features.", "labels": [], "entities": []}, {"text": "To do this, we first divide all features into four basic feature sets: T (transition cues), P (punctuations, special nameentities, and segment positions), G (grammar), and OD (opinion words and dependency relations).", "labels": [], "entities": [{"text": "OD", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9527528882026672}]}, {"text": "We train 15 sentiment classification models using all basic features and their combinations.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.9108371734619141}]}, {"text": "shows the results of a 10-fold cross validation on the 200-review dataset (light grey bars show the accuracy of the model trained without using transition cue features).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9989533424377441}]}, {"text": "The feature OD yields the best accuracy, followed by G, P, and T.", "labels": [], "entities": [{"text": "OD", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9392576813697815}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995996356010437}]}, {"text": "Although T yields the worst accuracy, incorporating it improves the resulting accuracy of the other features, as shown by the dark grey bars.", "labels": [], "entities": [{"text": "T", "start_pos": 9, "end_pos": 10, "type": "METRIC", "confidence": 0.9849073886871338}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994223117828369}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992052912712097}]}, {"text": "In particular, the accuracy of OD is markedly improved by adding T.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999614953994751}, {"text": "OD", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.4990161955356598}, {"text": "T", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.9769052267074585}]}, {"text": "The model trained using all the feature sets yields the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9987950325012207}]}, {"text": "Our second experiment evaluates the impact of the relationship learners and the label integrator.", "labels": [], "entities": []}, {"text": "To this end, we train and compare sentiment classification models using three configurations.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.9339912235736847}]}, {"text": "The first configuration (FW-L) uses only the FR learner; the second (BW-L) only the BR learner.", "labels": [], "entities": [{"text": "FW-L", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.7379800081253052}, {"text": "FR learner", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.8438454866409302}, {"text": "BW-L", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9217925667762756}, {"text": "BR", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.7980905771255493}]}, {"text": "ALL-L uses both the FR and BR learners, together with a label integrator.", "labels": [], "entities": [{"text": "FR", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.8849666118621826}, {"text": "BR", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.7736911773681641}]}, {"text": "We evaluate them with 10-fold cross 547 validation on the 200-review dataset.", "labels": [], "entities": [{"text": "200-review dataset", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.7076651602983475}]}, {"text": "reports the accuracy, macro F-score, and micro F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997950196266174}, {"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.8395785689353943}, {"text": "micro F-score", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.753694474697113}]}, {"text": "It shows that the BR learner produces better accuracy and a micro F-score than the FR learner but a slightly worse macro F-score.", "labels": [], "entities": [{"text": "BR", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9499421119689941}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993898868560791}, {"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9574114084243774}, {"text": "F-score", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.8759180903434753}]}, {"text": "Jointly considering both learners with the label integrator achieves better results than either alone.", "labels": [], "entities": []}, {"text": "The results demonstrate the effectiveness of our sentiment labeling component.", "labels": [], "entities": [{"text": "sentiment labeling", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8767004907131195}]}], "tableCaptions": [{"text": " Table 4: Comparison results of different lexicons.", "labels": [], "entities": []}]}