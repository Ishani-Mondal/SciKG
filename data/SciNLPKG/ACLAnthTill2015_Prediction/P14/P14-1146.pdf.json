{"title": [{"text": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification *", "labels": [], "entities": [{"text": "Twitter Sentiment Classification", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.5602788329124451}]}], "abstractContent": [{"text": "We present a method that learns word embedding for Twitter sentiment classification in this paper.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.7177905340989431}]}, {"text": "Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.", "labels": [], "entities": []}, {"text": "This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9744166135787964}]}, {"text": "We address this issue by learning sentiment-specific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.", "labels": [], "entities": []}, {"text": "Specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions.", "labels": [], "entities": []}, {"text": "To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.", "labels": [], "entities": []}, {"text": "Experiments on applying SS-WE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenat-ing SSWE with existing feature set.", "labels": [], "entities": [{"text": "Twitter sentiment classification dataset in SemEval 2013", "start_pos": 45, "end_pos": 101, "type": "DATASET", "confidence": 0.6838416712624686}]}], "introductionContent": [{"text": "Twitter sentiment classification has attracted increasing research interest in recent years).", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6544363796710968}]}, {"text": "The objective is to classify the sentiment polarity of a tweet as positive, negative or neutral.", "labels": [], "entities": []}, {"text": "The majority of existing approaches follow and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.", "labels": [], "entities": []}, {"text": "Under this direction, most studies focus on designing effective features to obtain better classification performance.", "labels": [], "entities": []}, {"text": "For example, build the top-performed system in the Twitter sentiment classification track of, using diverse sentiment lexicons and a variety of hand-crafted features.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.5462314685185751}]}, {"text": "Feature engineering is important but laborintensive.", "labels": [], "entities": [{"text": "Feature engineering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8376210629940033}]}, {"text": "It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering.", "labels": [], "entities": []}, {"text": "For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.9375732243061066}]}, {"text": "Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector fora word.", "labels": [], "entities": []}, {"text": "Although existing word embedding learning algorithms) are intuitive choices, they are not effective enough if directly used for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.9713308215141296}]}, {"text": "The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.", "labels": [], "entities": []}, {"text": "As a result, words with opposite polarity, such as good and bad, are mapped into close vectors.", "labels": [], "entities": []}, {"text": "It is meaningful for some tasks such as pos-tagging ( as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.9438005983829498}]}, {"text": "In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.9494941532611847}]}, {"text": "We encode the sentiment information in-to the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum.", "labels": [], "entities": []}, {"text": "To this end, we extend the existing word embedding learning algorithm) and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions.", "labels": [], "entities": []}, {"text": "We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations.", "labels": [], "entities": []}, {"text": "These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentimentspecific word embedding.", "labels": [], "entities": []}, {"text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.621546596288681}, {"text": "benchmark dataset in SemEval 2013", "start_pos": 122, "end_pos": 155, "type": "DATASET", "confidence": 0.6882819950580596}]}, {"text": "In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%).", "labels": [], "entities": [{"text": "predicting positive/negative polarity of tweets", "start_pos": 15, "end_pos": 62, "type": "TASK", "confidence": 0.8810571943010602}]}, {"text": "After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1.", "labels": [], "entities": []}, {"text": "The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons.", "labels": [], "entities": [{"text": "SSWE", "start_pos": 15, "end_pos": 19, "type": "TASK", "confidence": 0.9497394561767578}]}, {"text": "In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9977566599845886}]}, {"text": "The major contributions of the work presented in this paper are as follows.", "labels": [], "entities": []}, {"text": "\u2022 We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations; \u2022 To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.", "labels": [], "entities": [{"text": "sentiment-specific word embedding (SSWE) from massive distant-supervised tweets", "start_pos": 44, "end_pos": 123, "type": "TASK", "confidence": 0.7289325356483459}, {"text": "Twitter sentiment classification", "start_pos": 232, "end_pos": 264, "type": "TASK", "confidence": 0.6976713240146637}]}, {"text": "We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013; \u2022 We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks.", "labels": [], "entities": [{"text": "SemEval 2013", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.6512032747268677}, {"text": "sentiment analysis", "start_pos": 263, "end_pos": 281, "type": "TASK", "confidence": 0.8045043349266052}]}], "datasetContent": [{"text": "We conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification.", "labels": [], "entities": [{"text": "SSWE", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.9584221243858337}, {"text": "Twitter sentiment classification", "start_pos": 101, "end_pos": 133, "type": "TASK", "confidence": 0.6808362603187561}]}, {"text": "We also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons.", "labels": [], "entities": [{"text": "SSWE", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.8995275497436523}]}], "tableCaptions": [{"text": " Table 1. We train sentiment  classifier with", "labels": [], "entities": [{"text": "sentiment  classifier", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7269806861877441}]}, {"text": " Table 1: Statistics of the SemEval 2013 Twitter  sentiment classification dataset.", "labels": [], "entities": [{"text": "SemEval 2013 Twitter  sentiment classification", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.8701386213302612}]}, {"text": " Table 2: Macro-F1 on positive/negative classifica- tion of tweets.", "labels": [], "entities": []}, {"text": " Table 3: Macro-F1 on positive/negative classifica- tion of tweets with different word embeddings.", "labels": [], "entities": []}, {"text": " Table 4: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and  MPQA with the same sentiment polarity.", "labels": [], "entities": [{"text": "Join- t", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.8837965925534567}]}]}