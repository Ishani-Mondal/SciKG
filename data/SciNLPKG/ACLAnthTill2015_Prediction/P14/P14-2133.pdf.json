{"title": [{"text": "How much do word embeddings encode about syntax?", "labels": [], "entities": []}], "abstractContent": [{"text": "Do continuous word embeddings encode any useful information for constituency parsing?", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.9113354980945587}]}, {"text": "We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon.", "labels": [], "entities": []}, {"text": "We test each of these hypotheses with a targeted change to a state-of-the-art base-line.", "labels": [], "entities": []}, {"text": "Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data.", "labels": [], "entities": []}, {"text": "Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from tree-banks in other ways.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space.", "labels": [], "entities": []}, {"text": "Word embeddings-representations of lexical items as points in areal vector space-have along history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval.", "labels": [], "entities": [{"text": "Word embeddings-representations of lexical items as points in areal vector space-have", "start_pos": 0, "end_pos": 85, "type": "TASK", "confidence": 0.7069654085419395}, {"text": "natural language processing", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6860992113749186}, {"text": "latent semantic analysis (LSA)", "start_pos": 170, "end_pos": 200, "type": "TASK", "confidence": 0.7908221383889517}, {"text": "information retrieval", "start_pos": 205, "end_pos": 226, "type": "TASK", "confidence": 0.7623150646686554}]}, {"text": "While word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity).", "labels": [], "entities": []}, {"text": "Semi-supervised and unsupervised models fora variety of core NLP tasks, including named-entity recognition, partof-speech tagging, and chunking) have been shown to benefit from the inclusion of word embeddings as features.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.7312474250793457}, {"text": "partof-speech tagging", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.8100778460502625}]}, {"text": "In the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally (.", "labels": [], "entities": []}, {"text": "Dependency parsers have seen gains from distributional statistics in the form of discrete word clusters (, and recent work ( suggests that similar gains can be derived from embeddings like the ones used in this paper.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7816843092441559}]}, {"text": "It has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.874208927154541}]}, {"text": "There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of, and even parsers which use pre-trained word embeddings to represent the lexicon, such as.", "labels": [], "entities": []}, {"text": "In these parsers, however, use of word vectors is a structural choice, rather than an added feature, and it is difficult to disentangle whether vector-space lexicons are actually more powerful than their discrete analogs-perhaps the performance of neural network parsers comes entirely from the model's extra-lexical syntactic structure.", "labels": [], "entities": []}, {"text": "In order to isolate the contribution from word embeddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-art performance without vector representations.", "labels": [], "entities": []}, {"text": "The fundamental question we want to explore is whether embeddings provide any information beyond what a conventional parser is able to induce from labeled parse trees.", "labels": [], "entities": []}, {"text": "It could be that the distinctions between lexical items that embeddings capture are already modeled by parsers in other ways and therefore provide no further benefit.", "labels": [], "entities": []}, {"text": "In this paper, we investigate this question empirically, by isolating three potential mechanisms for improvement from pre-trained word embed-  dings.", "labels": [], "entities": []}, {"text": "Our result is mostly negative.", "labels": [], "entities": []}, {"text": "With extremely limited training data, parser extensions using word embeddings give modest improvements inaccuracy (relative error reduction on the order of 1.5%).", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 115, "end_pos": 139, "type": "METRIC", "confidence": 0.7435591419537863}]}, {"text": "However, with reasonably-sized training corpora, performance does not improve even when a wide variety of embedding methods, parser modifications, and parameter settings are considered.", "labels": [], "entities": []}, {"text": "The fact that word embedding features result in nontrivial gains for discriminative dependency parsing (), but do not appear to be effective for constituency parsing, points to an interesting structural difference between the two tasks.", "labels": [], "entities": [{"text": "discriminative dependency parsing", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.6874098579088846}, {"text": "constituency parsing", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.8473456799983978}]}, {"text": "We hypothesize that dependency parsers benefit from the introduction of features (like clusters and embeddings) that provide syntactic abstractions; but that constituency parsers already have access to such abstractions in the form of supervised preterminal tags.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Maryland implementation of the Berkeley parser as our baseline for the kernelsmoothed lexicon, and the Maryland featured parser as our baseline for the embedding-featured lexicon.", "labels": [], "entities": []}, {"text": "For all experiments, we use 50-dimensional word embeddings.", "labels": [], "entities": []}, {"text": "Embeddings labeled C&W are from; embeddings labeled CBOW are from, trained with a context window of size 2.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.849129319190979}]}, {"text": "Experiments are conducted on the Wall Street Journal portion of the English Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the English Penn Treebank", "start_pos": 33, "end_pos": 89, "type": "DATASET", "confidence": 0.9499539666705661}]}, {"text": "We prepare three training sets: the complete training set of 39,832 sentences from the treebank (sections 2 through 21), a smaller training set, consisting of the first 3000 sentences, and an even smaller set of the first 300.", "labels": [], "entities": []}, {"text": "Per-corpus-size settings of the parameter \u03b2 are set by searching over several possible settings on the development set.", "labels": [], "entities": []}, {"text": "For each training corpus size we also choose a different setting of the number of splitting iterations over which the Berkeley parser is run; for 300 sentences this is two splits, and for: Test set experiments with the best combination of models (based on development experiments).", "labels": [], "entities": []}, {"text": "Again, we observe small gains with restricted training sets but no gains on the full training set.", "labels": [], "entities": []}, {"text": "Entries marked * are statistically significant (p < 0.05) under a paired bootstrap resampling test.", "labels": [], "entities": []}, {"text": "This is necessary to avoid overfitting on smaller training sets.", "labels": [], "entities": []}, {"text": "Consistent with the existing literature, we stop at six splits when using the full training corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Contributions from OOV, lexical pooling  and featured models, for two kinds of embeddings  (C&W and CBOW). For both choices of embed- ding, the pooling and OOV models provide small  gains with very little training data, but no gains  on the full training set. The featured model never  achieves scores higher than the generative base- line.", "labels": [], "entities": []}, {"text": " Table 2: Test set experiments with the best com- bination of models (based on development exper- iments). Again, we observe small gains with re- stricted training sets but no gains on the full train- ing set. Entries marked  *  are statistically signifi- cant (p < 0.05) under a paired bootstrap resam- pling test.", "labels": [], "entities": []}, {"text": " Table 3: Experiments for other corpora, using the  same combined model (lexicon pooling and OOV)  as in", "labels": [], "entities": [{"text": "OOV", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9562583565711975}]}, {"text": " Table 2. Again, we observe no significant  gains over the baseline.", "labels": [], "entities": []}]}