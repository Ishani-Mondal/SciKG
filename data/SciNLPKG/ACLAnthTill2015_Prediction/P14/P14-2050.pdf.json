{"title": [], "abstractContent": [{"text": "While continuous word embeddings are gaining popularity, current models are based solely on linear contexts.", "labels": [], "entities": []}, {"text": "In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts.", "labels": [], "entities": []}, {"text": "In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings.", "labels": [], "entities": []}, {"text": "The dependency-based embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word representation is central to natural language processing.", "labels": [], "entities": [{"text": "Word representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7093890309333801}, {"text": "natural language processing", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6639027893543243}]}, {"text": "The default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization.", "labels": [], "entities": []}, {"text": "For example, the symbolic representation of the words \"pizza\" and \"hamburger\" are completely unrelated: even if we know that the word \"pizza\" is a good argument for the verb \"eat\", we cannot infer that \"hamburger\" is also a good argument.", "labels": [], "entities": []}, {"text": "We thus seek a representation that captures semantic and syntactic similarities between words.", "labels": [], "entities": []}, {"text": "Avery common paradigm for acquiring such representations is based on the distributional hypothesis of, stating that words in similar contexts have similar meanings.", "labels": [], "entities": []}, {"text": "Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community.", "labels": [], "entities": []}, {"text": "On one end of the spectrum, words are grouped into clusters based on their contexts (.", "labels": [], "entities": []}, {"text": "On the other end, words * Supported by the European Community's Seventh Framework Programme under grant agreement no.", "labels": [], "entities": [{"text": "European Community's Seventh Framework Programme", "start_pos": 43, "end_pos": 91, "type": "DATASET", "confidence": 0.7249340415000916}]}, {"text": "287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see) fora comprehensive survey).", "labels": [], "entities": [{"text": "287923", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8599660992622375}]}, {"text": "In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD () or LDA ().", "labels": [], "entities": []}, {"text": "Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling ().", "labels": [], "entities": []}, {"text": "These representations, referred to as \"neural embeddings\" or \"word embeddings\", have been shown to perform well across a variety of tasks (.", "labels": [], "entities": []}, {"text": "Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations.", "labels": [], "entities": []}, {"text": "Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by and implemented in the word2vec software.", "labels": [], "entities": []}, {"text": "1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies.", "labels": [], "entities": []}, {"text": "Previous work on neural word embeddings take the contexts of a word to be its linear contextwords that precede and follow the target word, typically in a window of k tokens to each side.", "labels": [], "entities": []}, {"text": "However, other types of contexts can be explored too.", "labels": [], "entities": []}, {"text": "In this work, we generalize the SKIP-GRAM model, and move from linear bag-of-words contexts to arbitrary word contexts.", "labels": [], "entities": []}, {"text": "Specifically, following work in sparse vector-space models, we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees.", "labels": [], "entities": []}, {"text": "The different kinds of contexts produce noticeably different embeddings, and induce different word similarities.", "labels": [], "entities": []}, {"text": "In particular, the bag-ofwords nature of the contexts in the \"original\" SKIPGRAM model yield broad topical similarities, while the dependency-based contexts yield more functional similarities of a cohyponym nature.", "labels": [], "entities": []}, {"text": "This effect is demonstrated using both qualitative and quantitative analysis (Section 4).", "labels": [], "entities": []}, {"text": "The neural word-embeddings are considered opaque, in the sense that it is hard to assign meanings to the dimensions of the induced representation.", "labels": [], "entities": []}, {"text": "In Section 5 we show that the SKIP-GRAM model does allow for some introspection by querying it for contexts that are \"activated by\" a target word.", "labels": [], "entities": []}, {"text": "This allows us to peek into the learned representation and explore the contexts that are found by the learning process to be most discriminative of particular words (or groups of words).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to suggest such an analysis of discriminativelytrained word-embedding models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with 3 training conditions: BOW5 (bag-of-words contexts with k = 5), BOW2 (same, with k = 2) and DEPS (dependency-based syntactic contexts).", "labels": [], "entities": [{"text": "BOW5", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8574360609054565}, {"text": "BOW2", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9345260858535767}, {"text": "DEPS", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.8974729776382446}]}, {"text": "We modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings.", "labels": [], "entities": []}, {"text": "For bag-of-words contexts we used the original word2vec implementation, and for syntactic contexts, we used our modified version.", "labels": [], "entities": []}, {"text": "The negative-sampling parameter (how many negative contexts to sample for every correct one) was 15.", "labels": [], "entities": []}, {"text": "All embeddings were trained on English Wikipedia.", "labels": [], "entities": []}, {"text": "For DEPS, the corpus was tagged with parts-of-speech using the Stanford tagger ( and parsed into labeled Stanford dependencies () using an implementation of the parser described in).", "labels": [], "entities": []}, {"text": "All tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered.", "labels": [], "entities": []}, {"text": "This resulted in a vocabulary of about 175,000 words, with over 900,000 distinct syntactic contexts.", "labels": [], "entities": []}, {"text": "We report results for 300 dimension embeddings, though similar trends were also observed with 600 dimensions.", "labels": [], "entities": []}, {"text": "Our first evaluation is qualitative: we manually inspect the 5 most similar words (by cosine similarity) to a given set of target words (.", "labels": [], "entities": []}, {"text": "The first target word, Batman, results in similar sets across the different setups.", "labels": [], "entities": []}, {"text": "This is the case for many target words.", "labels": [], "entities": []}, {"text": "However, other target words show clear differences between embeddings.", "labels": [], "entities": []}, {"text": "In Hogwarts -the school of magic from the fictional Harry Potter series -it is evident that BOW contexts reflect the domain aspect, whereas DEPS yield a list of famous schools, capturing the semantic type of the target word.", "labels": [], "entities": []}, {"text": "This observation holds for Turing 3 and many other nouns as well; BOW find words that associate with w, while DEPS find words that behave like w.", "labels": [], "entities": [{"text": "BOW", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9850375056266785}, {"text": "DEPS", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.5703930258750916}]}, {"text": "Turney (2012) described this distinction as domain similarity versus functional similarity.", "labels": [], "entities": []}, {"text": "The Florida example presents an ontological difference; bag-of-words contexts generate meronyms (counties or cities within Florida), while dependency-based contexts provide cohyponyms (other US states).", "labels": [], "entities": []}, {"text": "We observed the same behavior with other geographical locations, particularly with countries (though not all of them).", "labels": [], "entities": []}, {"text": "The next two examples demonstrate that similarities induced from DEPS share a syntactic function (adjectives and gerunds), while similarities based on BOW are more diverse.", "labels": [], "entities": [{"text": "BOW", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.925945520401001}]}, {"text": "Finally, we observe that while both BOW5 and BOW2 yield topical similarities, the larger window size result in more topicality, as expected.", "labels": [], "entities": [{"text": "BOW5", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7830396890640259}]}, {"text": "We also tried using the subsampling option () with BOW contexts (not shown).", "labels": [], "entities": []}, {"text": "Since word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality.", "labels": [], "entities": []}, {"text": "We supplement the examples in with quantitative evaluation to show that the qualitative differences pointed out in the previous section are indeed widespread.", "labels": [], "entities": []}, {"text": "To that end, we use the WordSim353 dataset (.", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.987772136926651}]}, {"text": "This dataset contains pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations.", "labels": [], "entities": []}, {"text": "We use the embeddings in a retrieval/ranking setup, where the task is to rank the similar pairs in the dataset above the related ones.", "labels": [], "entities": []}, {"text": "The pairs are ranked according to cosine similarities between the embedded words.", "labels": [], "entities": []}, {"text": "We then draw a recall-precision curve that describes the embedding's affinity towards one subset (\"similarity\") over another (\"relatedness\").", "labels": [], "entities": [{"text": "recall-precision", "start_pos": 15, "end_pos": 31, "type": "METRIC", "confidence": 0.9889089465141296}]}, {"text": "We expect DEPS's curve to be higher than BOW2's curve, which in turn is expected to be higher than BOW5's.", "labels": [], "entities": [{"text": "DEPS", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.542478084564209}]}, {"text": "The graph in shows this is indeed the case.", "labels": [], "entities": []}, {"text": "We repeated the experiment with a different dataset () that was used by to distinguish between domain and functional similarities.", "labels": [], "entities": []}, {"text": "The results show a similar trend).", "labels": [], "entities": []}, {"text": "When reversing the task such that the goal is to rank the related terms above the similar ones, the results are reversed, as expected (not shown).", "labels": [], "entities": []}], "tableCaptions": []}