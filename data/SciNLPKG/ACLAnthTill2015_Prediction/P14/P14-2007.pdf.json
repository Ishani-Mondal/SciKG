{"title": [{"text": "Measuring Sentiment Annotation Complexity of Text", "labels": [], "entities": [{"text": "Sentiment Annotation Complexity", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.867517868677775}]}], "abstractContent": [{"text": "The effort required fora human annota-tor to detect sentiment is not uniform for all texts, irrespective of his/her expertise.", "labels": [], "entities": [{"text": "detect sentiment", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.89118891954422}]}, {"text": "We aim to predict a score that quantifies this effort, using linguistic properties of the text.", "labels": [], "entities": []}, {"text": "Our proposed metric is called Sentiment Annotation Complexity (SAC).", "labels": [], "entities": [{"text": "Sentiment Annotation Complexity (SAC)", "start_pos": 30, "end_pos": 67, "type": "METRIC", "confidence": 0.6269783476988474}]}, {"text": "As for training data, since any direct judgment of complexity by a human annota-tor is fraught with subjectivity, we rely on cognitive evidence from eye-tracking.", "labels": [], "entities": []}, {"text": "The sentences in our dataset are labeled with SAC scores derived from eye-fixation duration.", "labels": [], "entities": [{"text": "SAC", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9951731562614441}]}, {"text": "Using linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation.", "labels": [], "entities": [{"text": "mean error rate", "start_pos": 101, "end_pos": 116, "type": "METRIC", "confidence": 0.6972409884134928}]}, {"text": "We also study the correlation between a human annotator's perception of complexity and a machine's confidence in polarity determination.", "labels": [], "entities": []}, {"text": "The merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.8900301158428192}]}], "introductionContent": [{"text": "The effort required by a human annotator to detect sentiment is not uniform for all texts.", "labels": [], "entities": [{"text": "detect sentiment", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7474883198738098}]}, {"text": "Compare the hypothetical tweet \"Just what I wanted: a good pizza.\" with \"Just what I wanted: a cold pizza.\".", "labels": [], "entities": []}, {"text": "The two are lexically and structurally similar.", "labels": [], "entities": []}, {"text": "However, because of the sarcasm in the second tweet (in \"cold\" pizza, an undesirable situation followed by a positive sentiment phrase \"just what I wanted\", as discussed in), it is more complex than the first for sentiment annotation.", "labels": [], "entities": [{"text": "sentiment annotation", "start_pos": 213, "end_pos": 233, "type": "TASK", "confidence": 0.9641133248806}]}, {"text": "Thus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others.", "labels": [], "entities": []}, {"text": "With regard to this, we introduce a metric called sentiment annotation complexity (SAC).", "labels": [], "entities": [{"text": "sentiment annotation complexity (SAC)", "start_pos": 50, "end_pos": 87, "type": "METRIC", "confidence": 0.765370175242424}]}, {"text": "The SAC of a given piece of text (sentences, in our case) can be predicted using the linguistic properties of the text as features.", "labels": [], "entities": [{"text": "SAC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5503024458885193}]}, {"text": "The primary question is whether such complexity measurement is necessary at all.", "labels": [], "entities": []}, {"text": "Fort et al (2012) describe the necessity of annotation complexity measurement in manual annotation tasks.", "labels": [], "entities": []}, {"text": "Measuring annotation complexity is beneficial in annotation crowdsourcing.", "labels": [], "entities": []}, {"text": "If the complexity of the text can be estimated even before the annotation begins, the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example).", "labels": [], "entities": []}, {"text": "Also, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier maybe chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences).", "labels": [], "entities": []}, {"text": "Our metric adds value to sentiment annotation and sentiment analysis, in these two ways.", "labels": [], "entities": [{"text": "sentiment annotation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.9178557693958282}, {"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.93746018409729}]}, {"text": "The fact that sentiment expression maybe complex is evident from a study of comparative sentences by, sarcasm by, thwarting by or implicit sentiment by.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no general approach to \"measure\" how complex apiece of text is, in terms of sentiment annotation.", "labels": [], "entities": [{"text": "sentiment annotation", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.7426985800266266}]}, {"text": "The central challenge here is to annotate a data set with SAC.", "labels": [], "entities": []}, {"text": "To measure the \"actual\" time spent by an annotator on apiece of text, we use an eyetracker to record eye-fixation duration: the time for which the annotator has actually focused on the sentence during annotation.", "labels": [], "entities": [{"text": "eye-fixation duration", "start_pos": 101, "end_pos": 122, "type": "METRIC", "confidence": 0.8500227630138397}]}, {"text": "Eye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by and sense disambiguation by. present a technique to determine translation difficulty index.", "labels": [], "entities": [{"text": "translation", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.950692355632782}, {"text": "sense disambiguation", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.6713413745164871}]}, {"text": "The work closest to ours is by who use eye-tracking to study the role of emotion words in reading.", "labels": [], "entities": []}, {"text": "The novelty of our work is three-fold: (a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features.", "labels": [], "entities": [{"text": "sentiment annotation", "start_pos": 199, "end_pos": 219, "type": "TASK", "confidence": 0.7480801641941071}, {"text": "SAC", "start_pos": 279, "end_pos": 282, "type": "TASK", "confidence": 0.8891016244888306}]}], "datasetContent": [{"text": "We wish to predict sentiment annotation complexity of the text using a supervised technique.", "labels": [], "entities": [{"text": "sentiment annotation complexity", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.8438199162483215}]}, {"text": "As stated above, the time-to-annotate is one good candidate.", "labels": [], "entities": [{"text": "time-to-annotate", "start_pos": 21, "end_pos": 37, "type": "METRIC", "confidence": 0.956261157989502}]}, {"text": "However, \"simple time measurement\" is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction.", "labels": [], "entities": []}, {"text": "To accurately record the time, we use an eye-tracking device that measures the \"duration of eye-fixations 1 \".", "labels": [], "entities": [{"text": "duration of eye-fixations 1", "start_pos": 80, "end_pos": 107, "type": "METRIC", "confidence": 0.8945680409669876}]}, {"text": "Another attribute recorded by the eye-tracker that may have been used is \"saccade duration 2 \".", "labels": [], "entities": [{"text": "saccade duration 2", "start_pos": 74, "end_pos": 92, "type": "METRIC", "confidence": 0.7531399428844452}]}, {"text": "However, saccade duration is not significant for annotation of short text, as in our case.", "labels": [], "entities": [{"text": "saccade duration", "start_pos": 9, "end_pos": 25, "type": "METRIC", "confidence": 0.5626244395971298}]}, {"text": "Hence, the SAC labels of our dataset are fixation durations with appropriate normalization.", "labels": [], "entities": []}, {"text": "It maybe noted that the eye-tracking device is used only to annotate training data.", "labels": [], "entities": []}, {"text": "The actual prediction of SAC is done using linguistic features alone.", "labels": [], "entities": [{"text": "prediction of SAC", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7479360699653625}]}, {"text": "We use a sentiment-annotated data set consisting of movie reviews by) and tweets from http://help.sentiment140.", "labels": [], "entities": [{"text": "sentiment-annotated data set", "start_pos": 9, "end_pos": 37, "type": "DATASET", "confidence": 0.7463972469170889}]}, {"text": "A total of 1059 sentences (566 from a movie corpus, 493 from atwitter corpus) are selected.", "labels": [], "entities": [{"text": "atwitter corpus", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.853334367275238}]}, {"text": "We then obtain two kinds of annotation from five paid annotators: (a) sentiment (positive, negative and objective), (b) eye-movement as recorded 3.", "labels": [], "entities": []}, {"text": "The experiment then continues in modules of 50 sentences at a time.", "labels": [], "entities": []}, {"text": "This is to prevent fatigue over a period of time.", "labels": [], "entities": []}, {"text": "Thus, each annotator participates in this experiment over a number of sittings.", "labels": [], "entities": []}, {"text": "We ensure the quality of our dataset in different ways: (a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment.", "labels": [], "entities": []}, {"text": "(b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above.", "labels": [], "entities": [{"text": "TOEFL iBT score", "start_pos": 276, "end_pos": 291, "type": "METRIC", "confidence": 0.8663909832636515}]}, {"text": "We understand that sentiment is nuanced-towards a target, through constructs like sarcasm and presence of multiple entities.", "labels": [], "entities": []}, {"text": "However, we want to capture the most natural form of sentiment annotation.", "labels": [], "entities": [{"text": "sentiment annotation", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8764517307281494}]}, {"text": "So, the guidelines are kept to a bare minimum of \"annotating a sentence as positive, negative and objective as per the speaker\".", "labels": [], "entities": []}, {"text": "This experiment results in a data set of 1059 sentences with a fixation duration recorded for each sentenceannotator pair The multi-rater kappa IAA for sentiment annotation is 0.686.", "labels": [], "entities": [{"text": "IAA", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.6597620844841003}, {"text": "sentiment annotation", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.8821187615394592}]}, {"text": "The linguistic features described in Average distance of all pairs of dependent words in the sentence (Lin, 1996) -Non-terminal to Terminal ratio Ratio of the number of non-terminals to the number of terminals in the constituency parse of a sentence Semantic -Discourse connectors Number of discourse connectors -Co-reference distance Sum of token distance between co-referring entities of anaphora in a sentence -Perplexity Trigram perplexity using language models trained on a mixture of sentences from the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus (mentioned in Sections 3 and 5) Sentiment-related (Computed using SentiWordNet ( Sum of SentiWordNet scores of all words -Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa counts as one sentiment flip", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 509, "end_pos": 521, "type": "DATASET", "confidence": 0.9036545753479004}, {"text": "Amazon Movie corpus", "start_pos": 527, "end_pos": 546, "type": "DATASET", "confidence": 0.8203779061635336}, {"text": "Stanford twitter corpus", "start_pos": 551, "end_pos": 574, "type": "DATASET", "confidence": 0.7634556492169698}]}], "tableCaptions": [{"text": " Table 2: Performance of Predictive Framework for 5-fold in-domain and cross-domain validation using  Mean Squared Error (MSE), Mean Absolute Error (MAE) and Mean Percentage Error (MPE) estimates  and correlation with the gold labels.", "labels": [], "entities": [{"text": "Mean Squared Error (MSE)", "start_pos": 102, "end_pos": 126, "type": "METRIC", "confidence": 0.8617289761702219}, {"text": "Mean Absolute Error (MAE)", "start_pos": 128, "end_pos": 153, "type": "METRIC", "confidence": 0.9456711709499359}, {"text": "Mean Percentage Error (MPE)", "start_pos": 158, "end_pos": 185, "type": "METRIC", "confidence": 0.9257035056749979}]}]}