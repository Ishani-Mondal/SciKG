{"title": [{"text": "Grammatical Relations in Chinese: GB-Ground Extraction and Data-Driven Parsing", "labels": [], "entities": [{"text": "Grammatical Relations", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7849671244621277}, {"text": "GB-Ground Extraction", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.5819384008646011}]}], "abstractContent": [{"text": "This paper is concerned with building linguistic resources and statistical parsers for deep grammatical relation (GR) analysis of Chinese texts.", "labels": [], "entities": [{"text": "deep grammatical relation (GR) analysis of Chinese texts", "start_pos": 87, "end_pos": 143, "type": "TASK", "confidence": 0.7897151470184326}]}, {"text": "A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs.", "labels": [], "entities": []}, {"text": "The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9654658138751984}]}, {"text": "Based on the converted corpus, we study transition-based, data-driven models for GR parsing.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 81, "end_pos": 91, "type": "TASK", "confidence": 0.9359602332115173}]}, {"text": "We present a novel transition system which suits GR graphs better than existing systems.", "labels": [], "entities": []}, {"text": "The key idea is to introduce anew type of transition that reorders top k elements in the memory module.", "labels": [], "entities": []}, {"text": "Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.9508005976676941}]}], "introductionContent": [{"text": "Grammatical relations (GRs) represent functional relationships between language units in a sentence.", "labels": [], "entities": []}, {"text": "They are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc.", "labels": [], "entities": []}, {"text": "GRs have assumed an important role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories.", "labels": [], "entities": [{"text": "generative grammar", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.9561721086502075}]}, {"text": "For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG;) and Head-driven Phrase Structure Grammar (HPSG; encode grammatical functions directly.", "labels": [], "entities": [{"text": "Head-driven Phrase Structure Grammar", "start_pos": 99, "end_pos": 135, "type": "TASK", "confidence": 0.6220109984278679}]}, {"text": "In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (.", "labels": [], "entities": []}, {"text": "In this paper, we address the question of analyzing Chinese sentences with deep GRs.", "labels": [], "entities": []}, {"text": "To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding (GB;) grounded phrase structure treebank, i.e. Chinese Treebank (CTB;) to a deep dependency bank where GRs are explicitly represented.", "labels": [], "entities": [{"text": "Chinese Treebank (CTB", "start_pos": 168, "end_pos": 189, "type": "DATASET", "confidence": 0.913547620177269}]}, {"text": "Different from popular shallow dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not only local but also various long-distance dependencies, such as coordinations, control/raising constructions, topicalization, relative clauses and many other complicated linguistic phenomena that goes beyond shallow syntax (see for example.).", "labels": [], "entities": [{"text": "shallow dependency parsing", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7278882066408793}]}, {"text": "Manual evaluation highlights the reliability of our linguistically-motivated GR extraction algorithm: The overall dependency-based precision and recall are 99.17 and 98.87.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9355948269367218}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9837443232536316}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9994444251060486}]}, {"text": "The automatically-converted corpus would be of use fora wide variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Recent years have seen the introduction of a number of treebank-guided statistical parsers capable of generating considerably accurate parses for Chinese.", "labels": [], "entities": []}, {"text": "With the high-quality GR resource at hand, we study data-driven GR parsing.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.9098433554172516}]}, {"text": "Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed trees.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8412160873413086}]}, {"text": "We notice two exceptions.  and individually studied two transition systems that can generate more general graphs rather than trees.", "labels": [], "entities": []}, {"text": "Inspired by their work, we study transition-based models for building deep dependency structures.", "labels": [], "entities": []}, {"text": "The existence of a large number of crossing arcs in GR graphs makes left-to-right, incremental graph spanning computationally hard.", "labels": [], "entities": []}, {"text": "Applied to our data, the two existing systems cover only 51.0% and 76.5% GR graphs respectively.", "labels": [], "entities": [{"text": "GR", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.8239954113960266}]}, {"text": "To better suit  The symbol \"*ldd\" indicates long-distance dependencies; \"subj*ldd\" between the word \"/involve\" and the word \"/documents\" represents a long-range subject-predicate relation.", "labels": [], "entities": []}, {"text": "The arguments and adjuncts of the coordinated verbs, namely \"/issue\" and \"/practice,\" are separately yet distributively linked the two heads.", "labels": [], "entities": []}, {"text": "our problem, we extend Titov et al.'s work and study what we call K-permutation transition system.", "labels": [], "entities": []}, {"text": "The key idea is to introduce anew type of transition that reorders top k (2 \u2264 k \u2264 K) elements in the memory module of a stack-based transition system.", "labels": [], "entities": []}, {"text": "With the increase of K, the expressiveness of the corresponding system strictly increases.", "labels": [], "entities": []}, {"text": "We propose an oracle deriving method which is guaranteed to find a sound transition sequence if one exits.", "labels": [], "entities": []}, {"text": "Moreover, we introduce an effective approximation of that oracle, which decreases decoding ambiguity but practically covers almost exactly the same graphs for our data.", "labels": [], "entities": []}, {"text": "Based on the stronger transition system, we build a GR parser with a discriminative model for disambiguation and abeam decoder for inference.", "labels": [], "entities": []}, {"text": "We conduct experiments on CTB 6.0 to profile this parser.", "labels": [], "entities": [{"text": "CTB 6.0", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9441501498222351}]}, {"text": "With the increase of the K, the parser is able to utilize more GR graphs for training and the numeric performance is improved.", "labels": [], "entities": []}, {"text": "Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.9508005976676941}]}, {"text": "Detailed analysis reveal some important factors that may possibly boost the performance.", "labels": [], "entities": []}, {"text": "To our knowledge, this work provides the first result of extensive experiments of parsing Chinese with GRs.", "labels": [], "entities": [{"text": "parsing Chinese", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.8734772205352783}]}, {"text": "We release our GR processing kit and goldstandard annotations for research purposes.", "labels": [], "entities": [{"text": "GR processing kit", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.7047308484713236}]}, {"text": "These resources can be downloaded at http://www.", "labels": [], "entities": []}, {"text": "icst.pku.edu.cn/lcwm/omg.", "labels": [], "entities": [{"text": "icst.pku.edu.cn/lcwm/omg", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8456467270851136}]}], "datasetContent": [{"text": "To have a precise understanding of whether our extraction algorithm works well, we have selected 20 files that contains 209 sentences in total for manual evaluation.", "labels": [], "entities": []}, {"text": "Linguistic experts carefully examine the corresponding GR graphs derived by our extraction algorithm and correct all errors.", "labels": [], "entities": []}, {"text": "In other words, a gold standard GR annotation set is created.", "labels": [], "entities": [{"text": "GR annotation set", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.5993240674336752}]}, {"text": "The measure for comparing two dependency graphs is precision/recall of GR tokens which are defined as w h , w d , l tuples, where w h is the head, w dis the dependent and l is the relation.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9993601441383362}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.985366940498352}]}, {"text": "Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the automatic generator, while unlabeled precision/recall (UP/UR) is the ratio regardless of l.", "labels": [], "entities": [{"text": "precision/recall (LP/LR)", "start_pos": 8, "end_pos": 32, "type": "METRIC", "confidence": 0.8556649684906006}, {"text": "precision/recall (UP/UR)", "start_pos": 121, "end_pos": 145, "type": "METRIC", "confidence": 0.8490864038467407}]}, {"text": "F-score is a harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9714664816856384}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9995357990264893}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9972677230834961}]}, {"text": "These measures correspond to attachment scores (LAS/UAS) in dependency tree parsing.", "labels": [], "entities": [{"text": "attachment scores (LAS/UAS)", "start_pos": 29, "end_pos": 56, "type": "METRIC", "confidence": 0.8927062579563686}, {"text": "dependency tree parsing", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.6583877007166544}]}, {"text": "To evaluate our GR parsing models that will be introduced later, we also report these metrics.", "labels": [], "entities": [{"text": "GR parsing", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.8689206838607788}]}, {"text": "The overall performance is summarized in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9593702256679535}]}, {"text": "1. We can see that the automatical GR extraction achieves relatively high performance.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8950054049491882}]}, {"text": "There are two sources of errors in treebank conversion: (1) inadequate conversion rules and (2) wrong or inconsistent original annotations.", "labels": [], "entities": []}, {"text": "During the creation of the gold standard corpus, we find that the former is mainly caused by complicated unbounded dependencies and the lack of internal structure for some kinds of phrases.", "labels": [], "entities": [{"text": "gold standard corpus", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.7614950239658356}]}, {"text": "Such problems are very hard to solve through rules only, if not possible, since original annotations do not provide sufficient information.", "labels": [], "entities": []}, {"text": "The latter problem is more scattered and unpredictable.", "labels": [], "entities": []}, {"text": "CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (, and syntactic parsing (.", "labels": [], "entities": [{"text": "CTB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.849888801574707}, {"text": "word segmentation", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.7723642587661743}, {"text": "POS tagging", "start_pos": 207, "end_pos": 218, "type": "TASK", "confidence": 0.884968489408493}, {"text": "syntactic parsing", "start_pos": 226, "end_pos": 243, "type": "TASK", "confidence": 0.7573153674602509}]}, {"text": "We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task.", "labels": [], "entities": [{"text": "CTB 6.0", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9471004903316498}, {"text": "CoNLL 2009 shared task", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.9531667977571487}]}, {"text": "We use gold-standard word segmentation and POS taging results as inputs.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.6876188367605209}, {"text": "POS taging", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.7391921281814575}]}, {"text": "All transition-based parsing models are trained with beam 16 and iteration 30.", "labels": [], "entities": []}, {"text": "Overall precision/recall/f-score with respect to dependency tokens is reported.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9995664954185486}, {"text": "recall/f-score", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.7532715400060018}]}, {"text": "To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9991403818130493}]}], "tableCaptions": [{"text": " Table 1: Manual evaluation of 209 sentences.", "labels": [], "entities": []}, {"text": " Table 3: Coverage and accuracy of the GR parser on the development data.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9446231722831726}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994834661483765}, {"text": "GR parser", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.636166900396347}]}, {"text": " Table 4: Performance on the test data.", "labels": [], "entities": []}]}