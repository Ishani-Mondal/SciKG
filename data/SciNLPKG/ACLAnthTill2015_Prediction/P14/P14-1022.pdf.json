{"title": [], "abstractContent": [{"text": "We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure.", "labels": [], "entities": []}, {"text": "For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP.", "labels": [], "entities": []}, {"text": "Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as X-bar grammars.", "labels": [], "entities": []}, {"text": "Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks.", "labels": [], "entities": []}, {"text": "On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bj\u00f6rkelund et al.", "labels": [], "entities": [{"text": "SPMRL 2013 multilingual constituency parsing shared task", "start_pos": 7, "end_pos": 63, "type": "TASK", "confidence": 0.7152089263711657}]}, {"text": "(2013) on a range of languages.", "labels": [], "entities": []}, {"text": "In addition , despite being designed for syntactic analysis, our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al.", "labels": [], "entities": []}, {"text": "Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.770174503326416}, {"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9419068694114685}]}], "introductionContent": [{"text": "Na\u00a8\u0131veNa\u00a8\u0131ve context-free grammars, such as those embodied by standard treebank annotations, do not parse well because their symbols have too little context to constrain their syntactic behavior.", "labels": [], "entities": []}, {"text": "For example, to PPs usually attach to verbs and of PPs usually attach to nouns, but a context-free PP symbol can equally well attach to either.", "labels": [], "entities": []}, {"text": "Much of the last few decades of parsing research has therefore focused on propagating contextual information from the leaves of the tree to internal nodes.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9849270582199097}]}, {"text": "For example, head lexicalization, structural annotation, and state-splitting () are all designed to take coarse symbols like PP and decorate them with additional context.", "labels": [], "entities": [{"text": "head lexicalization", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8584980666637421}]}, {"text": "The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function.", "labels": [], "entities": []}, {"text": "There have been non-local approaches as well, such as tree-substitution parsers), neural net parsers, and rerankers.", "labels": [], "entities": []}, {"text": "These non-local approaches can actually go even further in enriching the grammar's structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference.", "labels": [], "entities": []}, {"text": "In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features.", "labels": [], "entities": []}, {"text": "We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly.", "labels": [], "entities": []}, {"text": "We therefore begin with a minimal grammar and iteratively augment it with rich input features that do not enrich the context-free backbone.", "labels": [], "entities": []}, {"text": "Previous work has also used surface features in their parsers, but the focus has been on machine learning methods), latent annotations (, or implementation (.", "labels": [], "entities": []}, {"text": "By contrast, we investigate the extent to which we need a grammar at all.", "labels": [], "entities": []}, {"text": "As a thought experiment, consider a parser with no grammar, which functions by independently classifying each span (i, j) of a sentence as an NP, VP, and soon, or null if that span is a non-constituent.", "labels": [], "entities": []}, {"text": "For example, spans that begin with the might tend to be NPs, while spans that end with of might tend to be non-constituents.", "labels": [], "entities": []}, {"text": "An independent classification approach is actually very viable for part-of-speech tagging (, but is problematic for parsing -if nothing else, parsing comes with a structural requirement that the output be a well-formed, nested tree.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7104687094688416}, {"text": "parsing", "start_pos": 116, "end_pos": 123, "type": "TASK", "confidence": 0.9793731570243835}]}, {"text": "Our parser uses a minimal PCFG backbone grammar to ensure a basic level of structural well-formedness, but relies mostly on features of surface spans to drive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9971848130226135}]}, {"text": "Formally, our model is a CRF where the features factor over anchored rules of a small backbone grammar, as shown in.", "labels": [], "entities": []}, {"text": "Some aspects of the parsing problem, such as the tree constraint, are clearly best captured by a PCFG.", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.910027801990509}, {"text": "PCFG", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8905477523803711}]}, {"text": "Others, such as heaviness effects, are naturally captured using surface information.", "labels": [], "entities": []}, {"text": "The open question is whether surface features are adequate for key effects like subcategorization, which have deep definitions but regular surface reflexes (e.g. the preposition selected by a verb will often linearly follow it).", "labels": [], "entities": []}, {"text": "Empirically, the answer seems to be yes, and our system produces strong results, e.g. up to 90.5 F1 on English parsing.", "labels": [], "entities": [{"text": "F1", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9871292114257812}]}, {"text": "Our parser is also able to generalize well across languages with little tuning: it achieves state-of-the-art results on multilingual parsing, scoring higher than the best single-parser system from the SPMRL 2013 Shared Task on a range of languages, as well as on the competition's average F1 metric.", "labels": [], "entities": [{"text": "SPMRL 2013 Shared Task", "start_pos": 201, "end_pos": 223, "type": "TASK", "confidence": 0.6045143753290176}, {"text": "F1", "start_pos": 289, "end_pos": 291, "type": "METRIC", "confidence": 0.998999297618866}]}, {"text": "One advantage of a system that relies on surface features and a simple grammar is that it is portable not only across languages but also across tasks to an extent.", "labels": [], "entities": []}, {"text": "For example, demonstrates that sentiment analysis, which is usually approached as a flat classification task, can be viewed as tree-structured.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9792421162128448}]}, {"text": "In their work, they propagate real-valued vectors up a tree using neural tensor nets and see gains from their recursive approach.", "labels": [], "entities": []}, {"text": "Our parser can be easily adapted to this task by replacing the X-bar grammar over treebank symbols with a grammar over the sentiment values to encode the output variables and then adding n-gram indicators to our feature set to capture the bulk of the lexical effects.", "labels": [], "entities": []}, {"text": "When applied to this task, our system generally matches their accuracy overall and is able to outperform it on the overall sentence-level subtask.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9993067979812622}]}], "datasetContent": [{"text": "Finally, shows our final evaluation on Section 23 of the Penn Treebank.", "labels": [], "entities": [{"text": "Section 23 of the Penn Treebank", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.868233193953832}]}, {"text": "We use the v = 1, h = 0 grammar.", "labels": [], "entities": []}, {"text": "While we do not do as well as the Berkeley parser, we will see in Section 6 that our parser does a substantially better job of generalizing to other languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the Penn Treebank development set, reported in F1 on sentences of length \u2264 40  on Section 22, for a number of incrementally growing feature sets. We show that each feature type  presented in Section 4 adds benefit over the previous, and in combination they produce a reasonably  good yet simple parser.", "labels": [], "entities": [{"text": "Penn Treebank development set", "start_pos": 26, "end_pos": 55, "type": "DATASET", "confidence": 0.9890475869178772}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9950898885726929}, {"text": "Section 22", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.9710766077041626}]}, {"text": " Table 2: Results for the Penn Treebank develop- ment set, sentences of length \u2264 40, for different  annotation schemes implemented on top of the X- bar grammar.", "labels": [], "entities": [{"text": "Penn Treebank develop- ment set", "start_pos": 26, "end_pos": 57, "type": "DATASET", "confidence": 0.9518139064311981}]}, {"text": " Table 3: Final Parseval results for the v = 1, h = 0  parser on Section 23 of the Penn Treebank.", "labels": [], "entities": [{"text": "Section 23 of the Penn Treebank", "start_pos": 65, "end_pos": 96, "type": "DATASET", "confidence": 0.8875866929690043}]}, {"text": " Table 4: Results for the nine treebanks in the SPMRL 2013 Shared Task; all values are F-scores for  sentences of all lengths using the version of evalb distributed with the shared task. Berkeley-Rep is  the best single parser from", "labels": [], "entities": [{"text": "SPMRL 2013 Shared Task", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.5950480550527573}, {"text": "F-scores", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9829441905021667}]}, {"text": " Table 5: Fine-grained sentiment analysis results  on the Stanford Sentiment Treebank of Socher et  al. (2013). We compare against the printed num- bers in Socher et al. (2013) as well as the per- formance of the corresponding release, namely  the sentiment component in the latest version of  the Stanford CoreNLP at the time of this writ- ing. Our model handily outperforms the results  from Socher et al. (2013) at root classification and  edges out the performance of the latest version of  the Stanford system. On all spans of the tree, our  model has comparable accuracy to the others.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.9246005614598592}, {"text": "root classification", "start_pos": 418, "end_pos": 437, "type": "TASK", "confidence": 0.7737584412097931}, {"text": "accuracy", "start_pos": 568, "end_pos": 576, "type": "METRIC", "confidence": 0.9978571534156799}]}]}