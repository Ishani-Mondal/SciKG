{"title": [], "abstractContent": [{"text": "This paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing robust shallow syntactic structures for improving model adaptability.", "labels": [], "entities": [{"text": "Opinion Mining (OM)", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8569237470626831}]}, {"text": "We rely on the tree kernel technology to automatically extract and learn features with better generalization power than bag-of-words.", "labels": [], "entities": []}, {"text": "An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural models in a cross-domain setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9541523456573486}]}], "introductionContent": [{"text": "Social media such as Twitter, Facebook or YouTube contain rapidly changing information generated by millions of users that can dramatically affect the reputation of a person or an organization.", "labels": [], "entities": []}, {"text": "This raises the importance of automatic extraction of sentiments and opinions expressed in social media.", "labels": [], "entities": [{"text": "automatic extraction of sentiments and opinions expressed in social media", "start_pos": 30, "end_pos": 103, "type": "TASK", "confidence": 0.8615835011005402}]}, {"text": "YouTube is a unique environment, just like Twitter, but probably even richer: multi-modal, with asocial graph, and discussions between people sharing an interest.", "labels": [], "entities": []}, {"text": "Hence, doing sentiment research in such an environment is highly relevant for the community.", "labels": [], "entities": [{"text": "sentiment research", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9444389939308167}]}, {"text": "While the linguistic conventions used on Twitter and YouTube indeed show similarities (, focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research.", "labels": [], "entities": []}, {"text": "Nevertheless, there is almost no work showing effective OM on YouTube comments.", "labels": [], "entities": [{"text": "OM", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9748417735099792}]}, {"text": "To the best of our knowledge, the only exception is given by the classification system of YouTube comments proposed by.", "labels": [], "entities": []}, {"text": "While previous state-of-the-art models for opinion classification have been successfully applied to traditional corpora (, YouTube comments pose additional challenges: (i) polarity words can refer to either video or product while expressing contrasting sentiments; (ii) many comments are unrelated or contain spam; and (iii) learning supervised models requires training data for each different YouTube domain, e.g., tablets, automobiles, etc.", "labels": [], "entities": [{"text": "opinion classification", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7240531593561172}]}, {"text": "For example, consider atypical comment on a YouTube review video about a Motorola Xoom tablet: this guy really puts a negative spin on this , and I 'm not sure why , this seems crazy fast , and I 'm not entirely sure why his pinch to zoom his laggy all the other xoom reviews The comment contains a product name xoom and some negative expressions, thus, a bag-of-words model would derive a negative polarity for this product.", "labels": [], "entities": []}, {"text": "In contrast, the opinion towards the product is neutral as the negative sentiment is expressed towards the video.", "labels": [], "entities": []}, {"text": "Similarly, the following comment: iPad 2 is better.", "labels": [], "entities": []}, {"text": "the superior apps just destroy the xoom.", "labels": [], "entities": []}, {"text": "contains two positive and one negative word, yet the sentiment towards the product is negative (the negative word destroy refers to Xoom).", "labels": [], "entities": []}, {"text": "Clearly, the bag-of-words lacks the structural information linking the sentiment with the target product.", "labels": [], "entities": []}, {"text": "In this paper, we carryout a systematic study on OM targeting YouTube comments; its contribution is three-fold: firstly, to solve the problems outlined above, we define a classification schema, which separates spam and not related comments from the informative ones, which are, in turn, further categorized into video-or product-related comments (type classification).", "labels": [], "entities": [{"text": "OM targeting YouTube comments", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.8383529782295227}]}, {"text": "At the final stage, different classifiers assign polarity (positive, negative or neutral) to each type of a meaningful comment.", "labels": [], "entities": []}, {"text": "This allows us to filter out irrelevant comments, providing accurate OM distinguishing comments about the video and the target product.", "labels": [], "entities": [{"text": "OM distinguishing comments", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.7803725202878317}]}, {"text": "The second contribution of the paper is the creation and annotation (by an expert coder) of a comment corpus containing 35k manually labeled comments for two product YouTube domains: tablets and automobiles.", "labels": [], "entities": []}, {"text": "1 It is the first manually annotated corpus that enables researchers to use supervised methods on YouTube for comment classification and opinion analysis.", "labels": [], "entities": [{"text": "comment classification", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7784921228885651}, {"text": "opinion analysis", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.7130299359560013}]}, {"text": "The comments from different product domains exhibit different properties (cf. Sec.", "labels": [], "entities": []}, {"text": "5.2), which give the possibility to study the domain adaptability of the supervised models by training on one category and testing on the other (and vice versa).", "labels": [], "entities": []}, {"text": "The third contribution of the paper is a novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e., tags generalizing the specific topic of the video, e.g., iPad, Kindle, Toyota Camry.", "labels": [], "entities": []}, {"text": "Given the complexity and the novelty of the task, we exploit structural kernels to automatically engineer novel features.", "labels": [], "entities": []}, {"text": "In particular, we define an efficient tree kernel derived from the Partial Tree Kernel,), suitable for encoding structural representation of comments into Support Vector Machines (SVMs).", "labels": [], "entities": [{"text": "encoding structural representation of comments into Support Vector Machines (SVMs)", "start_pos": 103, "end_pos": 185, "type": "TASK", "confidence": 0.7859937647978464}]}, {"text": "Finally, our results show that our models are adaptable, especially when the structural information is used.", "labels": [], "entities": []}, {"text": "Structural models generally improve on both tasks -polarity and type classification -yielding up to 30% of relative improvement, when little data is available.", "labels": [], "entities": [{"text": "type classification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7618593573570251}]}, {"text": "Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports: (i) experiments on individual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learning on one domain and testing on the other; (iv) learning curves that provide an indication on the required amount and type of data and the scalability to other domains.", "labels": [], "entities": [{"text": "type classification", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7142624855041504}, {"text": "predicting type and sentiment", "start_pos": 119, "end_pos": 148, "type": "TASK", "confidence": 0.854759082198143}]}, {"text": "We compare FVEC and STRUCT models on three tasks described in Sec.", "labels": [], "entities": [{"text": "FVEC", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.7877461314201355}, {"text": "STRUCT", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.7976087927818298}]}, {"text": "5.1: sentiment, type and full.", "labels": [], "entities": []}, {"text": "reports the per-class performance and the overall accuracy of the multi-class classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9993983507156372}]}, {"text": "Firstly, we note that the performance on TABLETS is much higher than on AUTO across all tasks.", "labels": [], "entities": [{"text": "TABLETS", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.8187742829322815}, {"text": "AUTO", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.706356942653656}]}, {"text": "This can be explained by the following: (i) TABLETS contains more training data and (ii) videos from AUTO and TABLETS categories draw different types of audiences -well-informed users and geeks expressing better-motivated opinions about a product for the former vs. more general audience for the latter.", "labels": [], "entities": [{"text": "TABLETS", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.7412373423576355}, {"text": "AUTO", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.9451667070388794}]}, {"text": "This results in the different quality of comments with the AUTO being more challenging to analyze.", "labels": [], "entities": [{"text": "AUTO", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8450462222099304}]}, {"text": "Secondly, we observe that the STRUCT model provides 1-3% of absolute improvement inaccuracy over FVEC for every task.", "labels": [], "entities": [{"text": "STRUCT", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.7568562626838684}, {"text": "absolute improvement inaccuracy", "start_pos": 60, "end_pos": 91, "type": "METRIC", "confidence": 0.8715824683507284}, {"text": "FVEC", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9336192011833191}]}, {"text": "For individual categories the F1 scores are also improved by the STRUCT model (except for the negative classes for AUTO, where we see a small drop).", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9996516704559326}, {"text": "STRUCT", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9441844820976257}, {"text": "AUTO", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.4358065724372864}]}, {"text": "We conjecture that sentiment prediction for AUTO category is largely driven by one-shot phrases and statements where it is hard to improve upon the bag-of-words and sentiment lexicon features.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.9568554162979126}, {"text": "AUTO category", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.8756860792636871}]}, {"text": "In contrast, comments from TABLETS category tend to be more elaborated and well-argumented, thus, benefiting from the expressiveness of the structural representations.", "labels": [], "entities": []}, {"text": "Considering per-class performance, correctly predicting negative sentiment is most difficult for both AUTO and TABLETS, which is probably caused by the smaller proportion of the negative comments in the training set.", "labels": [], "entities": [{"text": "predicting negative sentiment", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.8844016989072164}, {"text": "AUTO", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.5803170204162598}, {"text": "TABLETS", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9703962802886963}]}, {"text": "For the type task, video-related class is substantially more difficult than product-related for both categories.", "labels": [], "entities": [{"text": "type task", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.9091291427612305}]}, {"text": "For the full task, the class video-negative accounts for the largest error.", "labels": [], "entities": []}, {"text": "This is confirmed by the results from the previous sentiment and type tasks, where we saw that handling negative sentiment and detecting video-related comments are most difficult.", "labels": [], "entities": []}, {"text": "To understand the performance of our classifiers on other YouTube domains, we perform a set of cross-domain experiments by training on the data from one product category and testing on the other.: In-domain learning curves.", "labels": [], "entities": []}, {"text": "ALL refers to the entire TRAIN set fora given product category, i.e., AUTO and TABLETS (see and in the opposite direction (TABLETS\u2192AUTO: Cross-domain experiment.", "labels": [], "entities": [{"text": "AUTO", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.8282387852668762}, {"text": "TABLETS", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9977395534515381}]}, {"text": "Accuracy using FVEC and STRUCT models when trained/tested in both directions, i.e. AUTO\u2192TABLETS and TABLETS\u2192AUTO.", "labels": [], "entities": [{"text": "FVEC", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.7673225402832031}, {"text": "STRUCT", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.8268746733665466}, {"text": "AUTO", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.7127026915550232}, {"text": "TABLETS", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.641492486000061}, {"text": "TABLETS\u2192AUTO", "start_pos": 100, "end_pos": 112, "type": "METRIC", "confidence": 0.7672313849131266}]}, {"text": "\u2020 denotes results statistically significant at 95% level (via pairwise t-test).", "labels": [], "entities": []}, {"text": "provement, except for the sentiment task.", "labels": [], "entities": [{"text": "sentiment task", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8982579410076141}]}, {"text": "Similar to the in-domain experiments, we studied the effect of the source domain size on the target test performance.", "labels": [], "entities": []}, {"text": "This is useful to assess the adaptability of features exploited by the FVEC and STRUCT models with the change in the number of labeled examples available for training.", "labels": [], "entities": [{"text": "FVEC", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8488585948944092}]}, {"text": "Additionally, we considered a setting including a small amount of training data from the target data (i.e., supervised domain adaptation).", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.7399658759435018}]}, {"text": "For this purpose, we drew the learning curves of the FVEC and STRUCT models applied to the sentiment and type tasks): AUTO is used as the source domain to train models, which are tested on TABLETS.", "labels": [], "entities": [{"text": "FVEC", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9214848875999451}, {"text": "AUTO", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.7444473505020142}, {"text": "TABLETS", "start_pos": 189, "end_pos": 196, "type": "DATASET", "confidence": 0.551232635974884}]}, {"text": "8 The plot shows that when  little training data is available, the features generated by the STRUCT model exhibit better adaptability (up to 10% of improvement over FVEC).", "labels": [], "entities": [{"text": "FVEC", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.8320248126983643}]}, {"text": "The bag-of-words model seems to be affected by the data sparsity problem which becomes a crucial issue when only a small training set is available.", "labels": [], "entities": []}, {"text": "This difference becomes smaller as we add data from the same domain.", "labels": [], "entities": []}, {"text": "This is an important advantage of our structural approach, since we cannot realistically expect to obtain manual annotations for 10k+ comments for each (of many thousands) product domains present on YouTube.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of YouTube comments data  used in the sentiment, type and full classification  tasks. The comments come from two product cate- gories: AUTO and TABLETS. Numbers in paren- thesis show proportion w.r.t. to the total number of  comments used in a task.", "labels": [], "entities": [{"text": "full classification  tasks", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.7119757334391276}, {"text": "AUTO", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9538864493370056}, {"text": "TABLETS", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9865650534629822}]}, {"text": " Table 2: In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT. The  results are reported for sentiment, type and full classification tasks. The metrics used are precision (P),  recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc).", "labels": [], "entities": [{"text": "AUTO", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.5805829763412476}, {"text": "FVEC", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9560129046440125}, {"text": "STRUCT", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.7892028093338013}, {"text": "precision (P)", "start_pos": 185, "end_pos": 198, "type": "METRIC", "confidence": 0.9489552229642868}, {"text": "recall (R)", "start_pos": 201, "end_pos": 211, "type": "METRIC", "confidence": 0.9539947807788849}, {"text": "F1", "start_pos": 216, "end_pos": 218, "type": "METRIC", "confidence": 0.9942314624786377}, {"text": "accuracy", "start_pos": 261, "end_pos": 269, "type": "METRIC", "confidence": 0.9988594055175781}, {"text": "multi-class classifier (Acc)", "start_pos": 277, "end_pos": 305, "type": "METRIC", "confidence": 0.6220332562923432}]}]}