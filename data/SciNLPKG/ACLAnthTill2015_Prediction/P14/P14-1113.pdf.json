{"title": [{"text": "Learning Semantic Hierarchies via Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic hierarchy construction aims to build structures of concepts linked by hypernym-hyponym (\"is-a\") relations.", "labels": [], "entities": [{"text": "Semantic hierarchy construction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7461122870445251}]}, {"text": "A major challenge for this task is the automatic discovery of such relations.", "labels": [], "entities": []}, {"text": "This paper proposes a novel and effective method for the construction of semantic hierarchies based on word em-beddings, which can be used to measure the semantic relationship between words.", "labels": [], "entities": []}, {"text": "We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms.", "labels": [], "entities": []}, {"text": "Our result, an F-score of 73.74%, outperforms the state-of-the-art methods on a manually labeled test dataset.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9994874000549316}]}, {"text": "Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F-score to 80.29%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9954783320426941}]}], "introductionContent": [{"text": "Semantic hierarchies are natural ways to organize knowledge.", "labels": [], "entities": []}, {"text": "They are the main components of ontologies or semantic thesauri).", "labels": [], "entities": []}, {"text": "In the WordNet hierarchy, senses are organized according to the \"is-a\" relations.", "labels": [], "entities": []}, {"text": "For example, \"dog\" and \"canine\" are connected by a directed edge.", "labels": [], "entities": []}, {"text": "Here, \"canine\" is called a hypernym of \"dog.\"", "labels": [], "entities": []}, {"text": "Conversely, \"dog\" is a hyponym of \"canine.\"", "labels": [], "entities": []}, {"text": "As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications.", "labels": [], "entities": []}, {"text": "However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming.", "labels": [], "entities": []}, {"text": "Therefore, many researchers * Email correspondence.", "labels": [], "entities": [{"text": "Email", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.8602169752120972}]}, {"text": "have attempted to automatically extract semantic relations or to construct taxonomies.", "labels": [], "entities": []}, {"text": "A major challenge for this task is the automatic discovery of hypernym-hyponym relations.", "labels": [], "entities": []}, {"text": "propose a distant supervision method to extract hypernyms for entities from multiple sources.", "labels": [], "entities": []}, {"text": "The output of their model is a list of hypernyms fora given enity (left panel,).", "labels": [], "entities": []}, {"text": "However, there usually also exists hypernym-hyponym relations among these hypernyms.", "labels": [], "entities": []}, {"text": "For instance, \"\u690d \u7269 (plant)\" and \"\u6bdb \u831b \u79d1 (Ranunculaceae)\" are both hypernyms of the entity \"\u4e4c \u5934 (aconit),\" and \"\u690d \u7269 (plant)\" is also a hypernym of \"\u6bdb \u831b \u79d1 (Ranunculaceae).\"", "labels": [], "entities": []}, {"text": "Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel,).", "labels": [], "entities": []}, {"text": "Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (.", "labels": [], "entities": []}, {"text": "However, the coverage is limited by the scope of the resources.", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9448257684707642}]}, {"text": "Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances).", "labels": [], "entities": []}, {"text": "Besides, distributional similarity methods) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used.", "labels": [], "entities": []}, {"text": "However, it is not always rational.", "labels": [], "entities": []}, {"text": "Our previous method based on web mining () works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics.", "labels": [], "entities": [{"text": "hypernym extraction of entity names", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.8730784773826599}, {"text": "semantic hierarchy construction", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6657418509324392}]}, {"text": "Moreover, all of these methods do not use the word semantics effectively.", "labels": [], "entities": []}, {"text": "This paper proposes a novel approach for semantic hierarchy construction based on word embeddings.", "labels": [], "entities": [{"text": "semantic hierarchy construction", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8015685280164083}]}, {"text": "Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors.", "labels": [], "entities": []}, {"text": "Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words ().", "labels": [], "entities": []}, {"text": "For example, where v(w) is the embedding of the word w.", "labels": [], "entities": []}, {"text": "We observe that a similar property also applies to the hypernym-hyponym relationship (Section 3.3), which is the main inspiration of the present study.", "labels": [], "entities": []}, {"text": "However, we further observe that hypernymhyponym relations are more complicated than a single offset can represent.", "labels": [], "entities": []}, {"text": "To address this challenge, we propose a more sophisticated and general method -learning a linear projection which maps words to their hypernyms (Section 3.3.1).", "labels": [], "entities": []}, {"text": "Furthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym-hyponym relations (Section 3.3.2).", "labels": [], "entities": [{"text": "relation clustering", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7006561011075974}]}, {"text": "Subsequently, we identify whether an unknown word pair is a hypernym-hyponym relation using the projections (Section 3.4).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to apply word embeddings to this task.", "labels": [], "entities": []}, {"text": "For evaluation, we manually annotate a dataset containing 418 Chinese entities and their hypernym hierarchies, which is the first dataset for this task as far as we know.", "labels": [], "entities": []}, {"text": "The experimental results show that our method achieves an F-score of 73.74% which significantly outperforms the previous state-of-the-art methods.", "labels": [], "entities": [{"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9997655749320984}]}, {"text": "Moreover, combining our method with the manually-built hierarchy extension method proposed by can further improve F-score to 80.29%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9947178959846497}]}], "datasetContent": [{"text": "In this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 , which contains about 30 million sentences (about 780 million words).", "labels": [], "entities": []}, {"text": "The Chinese segmentation is provided by the open-source Chinese language processing platform LTP.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.5814037173986435}]}, {"text": "Then, we employ the Skip-gram method (Section 3.2) to train word embeddings.", "labels": [], "entities": []}, {"text": "Finally we obtain the embedding vectors of 0.56 million words.", "labels": [], "entities": []}, {"text": "The training data for projection learning is collected from CilinE (Section 3.3.3).", "labels": [], "entities": [{"text": "projection learning", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.959007740020752}, {"text": "CilinE (Section 3.3.3)", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.8827361464500427}]}, {"text": "We obtain 15,247 word pairs of hypernym-hyponym relations (9,288 for direct relations and 5,959 for indirect relations).", "labels": [], "entities": []}, {"text": "For evaluation, we collect the hypernyms for 418 entities, which are selected randomly from Baidubaike, following.", "labels": [], "entities": [{"text": "Baidubaike", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.9315826892852783}]}, {"text": "We then ask two annotators to manually label the semantic hierarchies of the correct hypernyms.", "labels": [], "entities": []}, {"text": "The final data set contains 655 unique hypernyms and 1,391 hypernym-hyponym relations among them.", "labels": [], "entities": []}, {"text": "We randomly split the labeled data into 1/5 for development and 4/5 for testing ().", "labels": [], "entities": []}, {"text": "The hierarchies are represented as relations of pairwise words.", "labels": [], "entities": []}, {"text": "We measure the inter-annotator agreement using the kappa coefficient.", "labels": [], "entities": []}, {"text": "The kappa value is 0.96, which indicates a good strength of agreement.", "labels": [], "entities": [{"text": "kappa", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9421504735946655}]}, {"text": "We use precision, recall, and F-score as our metrics to evaluate the performances of the methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9995927214622498}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9991384744644165}, {"text": "F-score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9988608360290527}]}, {"text": "Since hypernym-hyponym relations and its reverse (hyponym-hypernym) have one-to-one correspondence, their performances are equal.", "labels": [], "entities": []}, {"text": "For 4 Baidubaike (baike.baidu.com) is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of 5 www.ltp-cloud.com/demo/   simplicity, we only report the performance of the former in the experiments.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9590513110160828}]}], "tableCaptions": [{"text": " Table 2: The evaluation data.  *  Since hypernym- hyponym relations and hyponym-hypernym  relations have one-to-one correspondence, their  numbers are the same.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the proposed method with  existing methods in the test set.", "labels": [], "entities": []}, {"text": " Table 5: Performance on the out-of-CilinE data in  the test set.", "labels": [], "entities": [{"text": "out-of-CilinE data in  the test set", "start_pos": 29, "end_pos": 64, "type": "DATASET", "confidence": 0.7395702749490738}]}]}