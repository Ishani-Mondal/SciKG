{"title": [{"text": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block.", "labels": [], "entities": []}, {"text": "Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches.", "labels": [], "entities": []}, {"text": "In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings.", "labels": [], "entities": []}, {"text": "The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .", "labels": [], "entities": []}], "introductionContent": [{"text": "A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions.", "labels": [], "entities": []}, {"text": "In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g., co-occurring words) in which target terms appear in a large corpus as proxies for meaning representations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words.", "labels": [], "entities": []}, {"text": "It has been clear for decades now that raw cooccurrence counts don't work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques.", "labels": [], "entities": []}, {"text": "This vector optimization process is generally unsupervised, and based on independent considerations (for example, context reweighting is often justified by information-theoretic considerations, dimensionality reduction optimizes the amount of preserved variance, etc.).", "labels": [], "entities": [{"text": "vector optimization", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7433429956436157}]}, {"text": "Occasionally, some kind of indirect supervision is used: Several parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning.", "labels": [], "entities": []}, {"text": "The last few years have seen the development of anew generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus ().", "labels": [], "entities": [{"text": "vector estimation problem", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.7755636970202128}]}, {"text": "The traditional construction of context vectors is turned on its head: Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear.", "labels": [], "entities": []}, {"text": "Since similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words.", "labels": [], "entities": []}, {"text": "This new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single, well-defined supervised learning step.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.8980032801628113}]}, {"text": "At the same time, supervision comes at no manual annotation cost, given that the context windows used for training can be automatically extracted from an unannotated corpus (indeed, they are the very same data used to build traditional DSMs).", "labels": [], "entities": []}, {"text": "Moreover, at least some of the relevant methods can efficiently scale up to process very large amounts of input data.", "labels": [], "entities": []}, {"text": "We will refer to DSMs builtin the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their trainingbased alternative as predict(ive) models.", "labels": [], "entities": []}, {"text": "Now, the most natural question to ask, of course, is which of the two approaches is best in empirical terms.", "labels": [], "entities": []}, {"text": "Surprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks (, the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs.", "labels": [], "entities": []}, {"text": "This is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as away to initialize feature vectors in neuralnetwork-based \"deep learning\" NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesting side effect.", "labels": [], "entities": []}, {"text": "Sociological reasons might also be partly responsible for the lack of systematic comparisons: Context-predictive models were developed within the neural-network community, with little or no awareness of recent DSM work in computational linguistics.", "labels": [], "entities": []}, {"text": "Whatever the reasons, we know of just three works reporting direct comparisons, all limited in their scope.", "labels": [], "entities": []}, {"text": "compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark of their paper).", "labels": [], "entities": [{"text": "passing", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9767891764640808}, {"text": "WordSim353 benchmark", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.9639452993869781}]}, {"text": "In this experiment, the count model actually outperforms the best predictive approach.", "labels": [], "entities": []}, {"text": "Instead, in a word-similarity-in-context task, the best predict model outperforms the count model, albeit not by a large margin.", "labels": [], "entities": []}, {"text": "Blacoe and Lapata (2012) compare count and predict representations as input to composition functions.", "labels": [], "entities": []}, {"text": "Count vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment.", "labels": [], "entities": [{"text": "phrase similarity task", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.8249092698097229}, {"text": "paraphrase classification experiment", "start_pos": 118, "end_pos": 154, "type": "TASK", "confidence": 0.8464670181274414}]}, {"text": "3 Allocation (LDA) models (, where parameters are set to optimize the joint probability distribution of words and documents.", "labels": [], "entities": []}, {"text": "However, the fully probabilistic LDA models have problems scaling up to large data sets.", "labels": [], "entities": []}, {"text": "We owe the first term to Hinrich Sch\u00fctze (p.c.).", "labels": [], "entities": []}, {"text": "Predictive DSMs are also called neural language models, because their supervised context prediction training is performed with neural networks, or, more cryptically, \"embeddings\".", "labels": [], "entities": [{"text": "context prediction", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7209116816520691}]}, {"text": "We refer hereto the updated results reported in the erratum at http://homepages.inf.ed.ac.uk/ s1066731/pdf/emnlp2012erratum.pdf Finally, compare their predict models to \"Latent Semantic Analysis\" (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior.", "labels": [], "entities": []}, {"text": "However, they provide very little details about the LSA count vectors they use.", "labels": [], "entities": []}, {"text": "In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks.", "labels": [], "entities": []}, {"text": "Our title already gave away what we discovered.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our models on a variety of benchmarks, most of them already widely used to test and compare DSMs.", "labels": [], "entities": []}, {"text": "The following benchmark descriptions also explain the figures of merit and stateof-the-art results reported in.", "labels": [], "entities": []}, {"text": "Semantic relatedness A first set of semantic benchmarks was constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale.", "labels": [], "entities": [{"text": "Semantic relatedness", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.787104457616806}]}, {"text": "The performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the previous art, we use Pearson correlation for rg, Spearman in all other cases).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 245, "end_pos": 264, "type": "METRIC", "confidence": 0.9072073996067047}]}, {"text": "The classic data set of (rg) consists of 65 noun pairs.", "labels": [], "entities": []}, {"text": "State of the art performance on this set has been reported by Hassan and Mihalcea (2011) using a technique that exploits the Wikipedia linking structure and word sense disambiguation techniques.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.6745766997337341}]}, {"text": "introduced the widely used WordSim353 set (ws) that, as the name suggests, consists of 353 pairs.", "labels": [], "entities": [{"text": "WordSim353 set (ws)", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.9397495031356812}]}, {"text": "The current state of the art is reached by with a method that is in the spirit of the predict models, but lets synonymy information from WordNet constrain the learning process (by favoring solutions in which WordNet synonyms are near in semantic space).", "labels": [], "entities": []}, {"text": "split the ws set into similarity (wss) and relatedness (wsr) subsets.", "labels": [], "entities": []}, {"text": "The first contains tighter taxonomic relations, such as synonymy and cohyponymy (king/queen) whereas the second encompasses broader, possibly topical or syntagmatic relations (family/planning).", "labels": [], "entities": []}, {"text": "We report stateof-the-art performance on the two subsets from the work of Agirre and colleagues, who used different kinds of count vectors extracted from a very large corpus (orders of magnitude larger than ours).", "labels": [], "entities": []}, {"text": "Finally, we use (the test section of) MEN (men), that comprises 1,000 word pairs., the developers of this benchmark, achieve state-ofthe-art performance by extensive tuning on ad-hoc training data, and by using both textual and imageextracted features to represent word meaning.", "labels": [], "entities": []}, {"text": "Synonym detection The classic TOEFL (toefl) set was introduced by.", "labels": [], "entities": [{"text": "Synonym detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8471545577049255}, {"text": "TOEFL", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9347920417785645}]}, {"text": "It contains 80 multiple-choice questions that pair a target term with 4 synonym candidates.", "labels": [], "entities": []}, {"text": "For example, for the target levied one must choose between imposed (correct), believed, requested and correlated.", "labels": [], "entities": []}, {"text": "The DSMs compute cosines of each candidate vector with the target, and pick the candidate with largest cosine as their answer.", "labels": [], "entities": []}, {"text": "Performance is evaluated in terms of correct-answer accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9518263936042786}]}, {"text": "Bullinaria and Levy (2012) achieved 100% accuracy by a very thorough exploration of the count model parameter space.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9994471669197083}]}, {"text": "Concept categorization Given a set of nominal concepts, the task is to group them into natural categories (e.g., helicopters and motorcycles should go to the vehicle class, dogs and elephants into the mammal class).", "labels": [], "entities": [{"text": "Concept categorization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7802141904830933}]}, {"text": "Following previous art, we tackle categorization as an unsupervised clustering task.", "labels": [], "entities": []}, {"text": "The vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit, with the repeated bisections with global optimization method and CLUTO's default settings otherwise (these are standard choices in the literature).", "labels": [], "entities": []}, {"text": "Performance is evaluated in terms of purity, a measure of the extent to which each cluster contains concepts from a single gold category.", "labels": [], "entities": [{"text": "purity", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9637278318405151}]}, {"text": "If the gold partition is reproduced perfectly, purity reaches 100%; it approaches 0 as cluster quality deteriorates.", "labels": [], "entities": [{"text": "purity", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9954142570495605}]}, {"text": "The Almuhareb-Poesio (ap) benchmark contains 402 concepts organized into 21 categories).", "labels": [], "entities": []}, {"text": "State-of-the-art purity was reached by with a count model based on carefully crafted syntactic links.", "labels": [], "entities": []}, {"text": "The ESSLLI 2008 Distributional Semantic Workshop shared-task set (esslli) contains 44 concepts to be clustered into 6 categories (Baroni et al., 2008) (we ignore here the 3-and 2-way higher-level partitions coming with this set).", "labels": [], "entities": [{"text": "ESSLLI 2008 Distributional Semantic Workshop shared-task set (esslli)", "start_pos": 4, "end_pos": 73, "type": "DATASET", "confidence": 0.8676151275634766}]}, {"text": "reached top performance on this set using the full Web as a corpus and manually crafted, linguistically motivated patterns.", "labels": [], "entities": []}, {"text": "Finally, the Battig (battig) test set introduced by  includes 83 concepts from 10 categories.", "labels": [], "entities": [{"text": "Battig (battig) test set", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.7754875620206197}]}, {"text": "Current state of the art was reached by the window-based count model of Baroni and.", "labels": [], "entities": []}, {"text": "Selectional preferences We experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g., people received a high average score as subject of to eat, and a low score as object of the same verb).", "labels": [], "entities": []}, {"text": "We follow the procedure proposed by to tackle this challenge: For each verb, we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb as subjects or objects, and we average the vectors of these nouns to obtain a \"prototype\" vector for the relevant argument slot.", "labels": [], "entities": []}, {"text": "We then measure the cosine of the vector fora target noun with the relevant prototype vector (e.g., the cosine of people with the eating subject prototype vector).", "labels": [], "entities": []}, {"text": "Systems are evaluated by Spearman correlation of these cosines with the averaged human typicality ratings.", "labels": [], "entities": []}, {"text": "Our first data set was introduced by Ulrike Pad\u00f3 (2007) and includes 211 pairs (up).", "labels": [], "entities": []}, {"text": "Top-performance was reached by the supervised count vector system of Herda\u02d8 gdelen and Baroni (2009) (supervised in the sense that they directly trained a classifier on gold data, as opposed to the 0-cost supervision of the context-learning methods).", "labels": [], "entities": []}, {"text": "The mcrae set () consists of 100 noun-verb pairs, with top performance reached by the DepDM system of, a count DSM relying on syntactic information.", "labels": [], "entities": [{"text": "DepDM system", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.9192488789558411}]}, {"text": "Analogy While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in specifically to test predict models.", "labels": [], "entities": []}, {"text": "The data-set contains about 9K semantic and 10.5K syntactic analogy questions.", "labels": [], "entities": [{"text": "syntactic analogy", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7671906352043152}]}, {"text": "A semantic question gives an example pair (brothersister), a test word (grandson) and asks to find another word that instantiates the relation illustrated by the example with respect to the test word (granddaughter).", "labels": [], "entities": []}, {"text": "A syntactic question is similar, but in this case the relationship is of a grammatical nature (work-works, speak.", "labels": [], "entities": []}, {"text": "Mikolov and colleagues tackle the challenge by subtracting the second example term vector from the first, adding the test term, and looking for the nearest neighbour of the resulting vector (what is the nearest neighbour of brother \u2212 sister + grandson?).", "labels": [], "entities": []}, {"text": "Systems are evaluated in terms of proportion of questions where the nearest neighbour from the whole semantic space is the correct answer (the given example and test vector triples are excluded from the nearest neighbour search).", "labels": [], "entities": []}, {"text": "reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987183809280396}]}, {"text": "Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993140697479248}]}, {"text": "(2013c) using a skip-gram predict model.", "labels": [], "entities": []}, {"text": "Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: summarizes the evaluation results.", "labels": [], "entities": []}, {"text": "The first block of the table reports the maximum pertask performance (across all considered parameter settings) for count and predict vectors.", "labels": [], "entities": []}, {"text": "The latter emerge as clear winners, with a large margin over count vectors inmost tasks.", "labels": [], "entities": []}, {"text": "Indeed, the predictive models achieve an impressive overall performance, beating the current state of the art in several cases, and approaching it in many more.", "labels": [], "entities": []}, {"text": "It is worth stressing that, as reviewed in Section 3, the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning.", "labels": [], "entities": []}, {"text": "Our predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Top count models in terms of mean  performance-based model ranking across all tasks.  The first row states that the window-2, PMI, 300K  count model was the best count model, and, across  all tasks, its average rank, when ALL models are  decreasingly ordered by performance, was 35. See  Section 2.1 for explanation of the parameters.", "labels": [], "entities": []}, {"text": " Table 4: Top predict models in terms of mean  performance-based model ranking across all tasks.  See Section 2.2 for explanation of the parameters.", "labels": [], "entities": []}]}