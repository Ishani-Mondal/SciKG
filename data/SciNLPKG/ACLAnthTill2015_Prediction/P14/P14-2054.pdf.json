{"title": [{"text": "Polynomial Time Joint Structural Inference for Sentence Compression", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.926567405462265}]}], "abstractContent": [{"text": "We propose two polynomial time inference algorithms to compress sentences under bigram and dependency-factored objectives.", "labels": [], "entities": []}, {"text": "The first algorithm is exact and requires O(n 6) running time.", "labels": [], "entities": [{"text": "O", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9351046681404114}]}, {"text": "It extends Eisner's cubic time parsing algorithm by using virtual dependency arcs to link deleted words.", "labels": [], "entities": [{"text": "cubic time parsing", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6657761136690775}]}, {"text": "Two signatures are added to each span, indicating the number of deleted words and the rightmost kept word within the span.", "labels": [], "entities": []}, {"text": "The second algorithm is a fast approximation of the first one.", "labels": [], "entities": []}, {"text": "It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n 4) running time.", "labels": [], "entities": [{"text": "O", "start_pos": 94, "end_pos": 95, "type": "METRIC", "confidence": 0.9691521525382996}]}, {"text": "Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7258908748626709}]}], "introductionContent": [{"text": "Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9048441350460052}]}, {"text": "It has been widely used in compressive summarization (.", "labels": [], "entities": [{"text": "compressive summarization", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.8579453527927399}]}, {"text": "To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence).", "labels": [], "entities": []}, {"text": "Recent studies used a subtree deletion model for compression, which deletes a word only if its modifier in the parse tree is deleted.", "labels": [], "entities": []}, {"text": "Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see).", "labels": [], "entities": []}, {"text": "In fact, we parsed the Edinburgh sentence compression corpus using the MSTparser 1 , and found that 2561 of 5379 sentences (47.6%) do not satisfy the subtree deletion model.", "labels": [], "entities": [{"text": "Edinburgh sentence compression corpus", "start_pos": 23, "end_pos": 60, "type": "DATASET", "confidence": 0.8705861866474152}, {"text": "MSTparser 1", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.9229420721530914}]}, {"text": "Methods beyond the subtree model are also explored.", "labels": [], "entities": []}, {"text": "proposed synchronous tree substitution grammar, which allows local distortion of the tree topology and can thus naturally capture structural mismatches.", "labels": [], "entities": [{"text": "tree substitution grammar", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.7711168924967448}]}, {"text": "proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence.", "labels": [], "entities": []}, {"text": "However, the time complexity greatly increases since the parse tree dynamically depends on the compression.", "labels": [], "entities": []}, {"text": "They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew exact decoding algorithm for the joint model using dynamic programming.", "labels": [], "entities": []}, {"text": "Our method extends Eisner's cubic time parsing algorithm by adding signatures to each span, which indicate the number of deleted words and the rightmost kept word within the span, resulting in O(n 6 ) time complexity and O(n 4 ) space complexity.", "labels": [], "entities": [{"text": "cubic time parsing", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6929142475128174}]}, {"text": "We further propose a faster approximate algorithm based on Lagrangian relaxation, which has T O(n 4 ) running time and O(n 3 ) space complexity (T is the iteration number in the subgradient decent algorithm the proposed approach is 10 times faster than a high-performance commercial ILP solver.", "labels": [], "entities": [{"text": "O(n 3 ) space complexity", "start_pos": 119, "end_pos": 143, "type": "METRIC", "confidence": 0.8904571448053632}, {"text": "ILP solver", "start_pos": 283, "end_pos": 293, "type": "TASK", "confidence": 0.7971697747707367}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. As  expected, the joint models (ours and TM13) con- sistently outperform the subtree deletion model, s- ince the joint models do not suffer from the sub- tree restriction. They also outperform McDon- ald's, demonstrating the effectiveness of consid- ering the grammar structure for compression. It  is not surprising that CRFs achieve high unigram  F scores but low syntactic F scores as they do not", "labels": [], "entities": [{"text": "unigram  F scores", "start_pos": 350, "end_pos": 367, "type": "METRIC", "confidence": 0.6663796901702881}, {"text": "syntactic F scores", "start_pos": 376, "end_pos": 394, "type": "METRIC", "confidence": 0.7335867484410604}]}, {"text": " Table 2: Comparison results under various quality  metrics, including unigram F1 score (F uni )", "labels": [], "entities": [{"text": "unigram F1 score", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.7978232105573019}]}]}