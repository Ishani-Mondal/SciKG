{"title": [{"text": "Learning Soft Linear Constraints with Application to Citation Field Extraction", "labels": [], "entities": [{"text": "Citation Field Extraction", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.4992906451225281}]}], "abstractContent": [{"text": "Accurately segmenting a citation string into fields for authors, titles, etc. is a challenging task because the output typically obeys various global constraints.", "labels": [], "entities": [{"text": "segmenting a citation string", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.8406609743833542}]}, {"text": "Previous work has shown that modeling soft constraints , where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance.", "labels": [], "entities": []}, {"text": "On the other hand, for imposing hard constraints, dual decomposition is a popular technique for efficient prediction given existing algorithms for uncon-strained inference.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8931218385696411}]}, {"text": "We extend dual decomposition to perform prediction subject to soft constraints.", "labels": [], "entities": []}, {"text": "Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training.", "labels": [], "entities": []}, {"text": "This allows us to obtain substantial gains inaccuracy on anew, challenging citation extraction dataset.", "labels": [], "entities": [{"text": "citation extraction", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8410259783267975}]}], "introductionContent": [{"text": "Citation field extraction, an instance of information extraction, is the task of segmenting and labeling research paper citation strings into their constituent parts, including authors, editors, year, journal, volume, conference venue, etc.", "labels": [], "entities": [{"text": "Citation field extraction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6466232935587565}, {"text": "information extraction", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7305992543697357}, {"text": "segmenting and labeling research paper citation strings into their constituent parts, including authors, editors, year, journal, volume, conference venue, etc", "start_pos": 81, "end_pos": 239, "type": "Description", "confidence": 0.7834926931946365}]}, {"text": "This task is important because citation data is often provided only in plain text; however, having an accurate structured database of bibliographic information is necessary for many scientometric tasks, such as mapping scientific sub-communities, discovering research trends, and analyzing networks of researchers.", "labels": [], "entities": []}, {"text": "Automated citation field extraction needs further research because it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems.", "labels": [], "entities": [{"text": "Automated citation field extraction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7339530140161514}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9987595081329346}]}, {"text": "Hidden Markov models and linear-chain conditional random fields (CRFs) have previously been applied to citation extraction) . These models support efficient dynamic-programming inference, but only model local dependencies in the output label sequence.", "labels": [], "entities": [{"text": "citation extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.947939395904541}]}, {"text": "However citations have strong global regularities not captured by these models.", "labels": [], "entities": []}, {"text": "For example many book citations contain both an author section and an editor section, but none have two disjoint author sections.", "labels": [], "entities": []}, {"text": "Since linear-chain models are unable to capture more than Markov dependencies, the models sometimes mislabel the editor as a second author.", "labels": [], "entities": []}, {"text": "If we could enforce the global constraint that there should be only one author section, accuracy could be improved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9993878602981567}]}, {"text": "One framework for adding such global constraints into tractable models is constrained inference, in which at inference time the original model is augmented with restrictions on the outputs such that they obey certain global regularities.", "labels": [], "entities": []}, {"text": "When hard constraints can be encoded as linear equations on the output variables, and the underlying model's inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) ().", "labels": [], "entities": []}, {"text": "Alternatively, one can employ dual decomposition ( . Dual decompositions's advantage over ILP is is that it can leverage existing inference algorithms for the original model as a black box.", "labels": [], "entities": []}, {"text": "Such a modular algorithm is easy to implement, and works quite well in practice, providing certificates of optimality for most examples.", "labels": [], "entities": []}, {"text": "The above two approaches have previously been applied to impose hard constraints on a model's output.", "labels": [], "entities": []}, {"text": "On the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints (.", "labels": [], "entities": [{"text": "citation field extraction", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.8136825561523438}]}, {"text": "Here, the model is not required obey the global constraints, but merely pays a penalty for their vi-593 .  This paper introduces a novel method for imposing soft constraints via dual decomposition.", "labels": [], "entities": []}, {"text": "We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints.", "labels": [], "entities": []}, {"text": "Because our learning method drives many penalties to zero, it allows practitioners to perform 'constraint selection,' in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful constraints, which can be run quickly attest time.", "labels": [], "entities": []}, {"text": "Using our new method, we are able to incorporate not only all the soft global constraints of, but also far more complex data-driven constraints, while also providing stronger optimality certificates than their beam search technique.", "labels": [], "entities": []}, {"text": "On anew, more broadly representative, and challenging citation field extraction data set, we show that our methods achieve a 17.9% reduction in error versus a linear-chain conditional random field.", "labels": [], "entities": [{"text": "citation field extraction data set", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.7185065686702728}, {"text": "error", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.9849272966384888}]}, {"text": "Furthermore, we demonstrate that our inference technique can use and benefit from the constraints of, but that including our data-driven constraints on top of these is beneficial.", "labels": [], "entities": []}, {"text": "While this paper focusses on an application to citation field extraction, the novel methods introduced here would easily generalize to many problems with global output regularities.", "labels": [], "entities": [{"text": "citation field extraction", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.8187715411186218}]}], "datasetContent": [{"text": "Our baseline is the one used in, with some labeling errors removed.", "labels": [], "entities": []}, {"text": "This is a chain-structured CRF trained to maximize the conditional likelihood using L-BFGS with L2 regularization.", "labels": [], "entities": []}, {"text": "We use the same features as Anzaroot and, which include word type, capitalization, binned location in citation, regular expression matches, and matches into lexicons.", "labels": [], "entities": []}, {"text": "In addition, we use a rule-based segmenter that segments the citation string based on punctuation as well as probable start or end segment words (e.g. 'in' and 'volume').", "labels": [], "entities": []}, {"text": "We add a binary feature to tokens that correspond to the start of a segment in the output of this simple segmenter.", "labels": [], "entities": []}, {"text": "This final feature improves the F1 score on the cleaned test set from 94.0 F1 to 94.44 F1, which we use as a baseline score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9878693521022797}, {"text": "cleaned test set", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.8059079448382059}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.989404559135437}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9944391250610352}]}, {"text": "We then use the development set to learn the penalties for the soft constraints, using the perceptron algorithm described in section 3.1.", "labels": [], "entities": []}, {"text": "MAP inference in the model with soft constraints is performed using Soft-DD, shown in Algorithm 2.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9602788686752319}]}, {"text": "We instantiate constraints from each template in section 5.1, iterating overall possible labels that contain a B prefix at any level in the hierarchy and pruning all constraints with imp(c) < 2.75 calculated on the development set.", "labels": [], "entities": []}, {"text": "We asses performance in terms of field-level F1 score, which is the harmonic mean of precision and recall for predicted segments.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9331226944923401}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.999393105506897}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9977601766586304}]}, {"text": "shows how each type of constraint family improved the F1 score on the dataset.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9877901673316956}]}, {"text": "Learning all the constraints jointly provides the largest improvement in F1 at 95.39.", "labels": [], "entities": [{"text": "F1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9992302656173706}]}, {"text": "This improvement in F1 over the baseline CRF as well as the improvement in F1 over using only-one constraints was shown to be statistically significant using the Wilcoxon signed rank test with p-values < 0.05.", "labels": [], "entities": [{"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9993921518325806}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.999313235282898}]}, {"text": "In the all-constraints settings, 32.96% of the constraints have a learned parameter of 0, and therefore only  Since performing inference in the CRF is by far the most computationally intensive step in the iterative algorithm, this means our procedure requires approximately twice as much work as running the baseline CRF on the dataset.", "labels": [], "entities": []}, {"text": "On examples where unconstrained inference does not satisfy the constraints, Soft-DD converges after 4.52 iterations on average.", "labels": [], "entities": []}, {"text": "For 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints.", "labels": [], "entities": []}, {"text": "We could have enforced these constraints as hard constraints rather than soft ones.", "labels": [], "entities": []}, {"text": "This experiment is shown in the last row of, where F1 only improves to 94.6.", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9997535347938538}]}, {"text": "In addition, running the DD algorithm with these constraints takes 5.21 iterations on average per example, which is 2.8 times slower than Soft-DD with learned penalties.", "labels": [], "entities": []}, {"text": "In, we analyze the performance of Soft-DD when we don't necessarily run it to convergence, but stop after a fixed number of iterations on each test set example.", "labels": [], "entities": [{"text": "convergence", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.9531347155570984}]}, {"text": "We find that a large portion of our gain inaccuracy can be obtained when we allow ourselves as few as 2 dual decomposition iterations.", "labels": [], "entities": []}, {"text": "However, this only amounts to 1.24 times as much work as running the baseline CRF on the dataset, since the constraints are satisfied immediately for many examples.", "labels": [], "entities": []}, {"text": "In we consider two applications of our Soft-DD algorithm, and provide analysis in the caption.", "labels": [], "entities": []}, {"text": "We train and evaluate on the UMass dataset instead of CORA, because it is significantly larger, has a useful finer-grained labeling schema, and its annotation is more consistent.", "labels": [], "entities": [{"text": "UMass dataset", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9859582185745239}, {"text": "CORA", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.7891387343406677}]}, {"text": "We were able to obtain better performance on CORA using our baseline CRF than the HM M CCM results presented in, which include soft constraints.", "labels": [], "entities": [{"text": "CORA", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7884430289268494}]}, {"text": "Given this high performance of our base model on CORA, we did not apply our Soft-DD algorithm to the dataset.", "labels": [], "entities": [{"text": "CORA", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9142278432846069}]}, {"text": "Furthermore, since the dataset is so small, learning the penalties for our large collection of constraints is difficult, and test set results are unreliable.", "labels": [], "entities": []}, {"text": "Rather than compare our work to via results on CORA, we apply their constraints on the UMass data using Soft-DD and demonstrate accuracy gains, as discussed above.", "labels": [], "entities": [{"text": "CORA", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.896714985370636}, {"text": "UMass data", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.9617760181427002}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9991744160652161}]}], "tableCaptions": [{"text": " Table 1: Set of constraints learned and F1 scores.  The last row depicts the result of inference using  all constraints as hard constraints.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9770520329475403}]}, {"text": " Table 2: Performance from terminating Soft-DD  early. Column 1 is the number of iterations we  allow each example. Column 3 is the % of test  examples that converged. Column 4 is the aver- age number of necessary iterations, a surrogate for  the slowdown over performing unconstrained in- ference.", "labels": [], "entities": []}, {"text": " Table 3: Labels with highest improvement in F1.  U is in unconstrained inference. C is the results of  constrained inference. + is the improvement in F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9981241822242737}, {"text": "F1", "start_pos": 151, "end_pos": 153, "type": "METRIC", "confidence": 0.9930070638656616}]}]}