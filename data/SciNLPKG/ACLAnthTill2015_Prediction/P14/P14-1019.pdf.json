{"title": [{"text": "Steps to Excellence: Simple Inference with Refined Scoring of Dependency Trees", "labels": [], "entities": []}], "abstractContent": [{"text": "Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8425619304180145}]}, {"text": "In contrast, we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures.", "labels": [], "entities": []}, {"text": "Specifically, we introduce a sampling-based parser that can easily handle arbitrary global features.", "labels": [], "entities": []}, {"text": "Inspired by SampleRank, we learn to take guided stochastic steps towards a high scoring parse.", "labels": [], "entities": []}, {"text": "We introduce two samplers for traversing the space of trees, Gibbs and Metropolis-Hastings with Random Walk.", "labels": [], "entities": []}, {"text": "The model outperforms state-of-the-art results when evaluated on 14 languages of non-projective CoNLL datasets.", "labels": [], "entities": [{"text": "CoNLL datasets", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9333457052707672}]}, {"text": "Our sampling-based approach naturally extends to joint prediction scenarios, such as joint parsing and POS correction.", "labels": [], "entities": [{"text": "joint parsing", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.67268306016922}, {"text": "POS correction", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.8247648775577545}]}, {"text": "The resulting method outperforms the best reported results on the CATiB dataset, approaching performance of parsing with gold tags.", "labels": [], "entities": [{"text": "CATiB dataset", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9350025951862335}]}], "introductionContent": [{"text": "Dependency parsing is commonly cast as a maximization problem over a parameterized scoring function.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8917773962020874}]}, {"text": "In this view, the use of more expressive scoring functions leads to more challenging combinatorial problems of finding the maximizing parse.", "labels": [], "entities": []}, {"text": "Much of the recent work on parsing has been focused on improving methods for solving the combinatorial maximization inference problems.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9747714400291443}]}, {"text": "Indeed, state-of-the-art results have been ob-tained by adapting powerful tools from optimization ().", "labels": [], "entities": []}, {"text": "We depart from this view and instead focus on using highly expressive scoring functions with substantially simpler inference procedures.", "labels": [], "entities": []}, {"text": "The key ingredient in our approach is how learning is coupled with inference.", "labels": [], "entities": []}, {"text": "Our combination outperforms the state-of-the-art parsers and remains comparable even if we adopt their scoring functions.", "labels": [], "entities": []}, {"text": "Rich scoring functions have been used for sometime.", "labels": [], "entities": []}, {"text": "They first appeared in the context of reranking, where a simple parser is used to generate a candidate list which is then reranked according to the scoring function.", "labels": [], "entities": [{"text": "reranking", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.968819260597229}]}, {"text": "Because the number of alternatives is small, the scoring function could in principle involve arbitrary (global) features of parse trees.", "labels": [], "entities": []}, {"text": "The power of this methodology is nevertheless limited by the initial set of alternatives from the simpler parser.", "labels": [], "entities": []}, {"text": "Indeed, the set may already omit the gold parse.", "labels": [], "entities": []}, {"text": "We dispense with the notion of a candidate set and seek to exploit the scoring function more directly.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a sampling-based parser that places few or no constraints on the scoring function.", "labels": [], "entities": []}, {"text": "Starting with an initial candidate tree, our inference procedure climbs the scoring function in small (cheap) stochastic steps towards a high scoring parse.", "labels": [], "entities": []}, {"text": "The proposal distribution over the moves is derived from the scoring function itself.", "labels": [], "entities": []}, {"text": "Because the steps are small, the complexity of the scoring function has limited impact on the computational cost of the procedure.", "labels": [], "entities": []}, {"text": "We explore two alternative proposal distributions.", "labels": [], "entities": []}, {"text": "Our first strategy is akin to Gibbs sampling and samples anew head for each word in the sentence, modifying one arc at a time.", "labels": [], "entities": []}, {"text": "The second strategy relies on a provably correct sampler for firstorder scores, and uses it within a Metropolis-Hastings algorithm for general scoring functions.", "labels": [], "entities": []}, {"text": "It turns out that the latter optimizes the 197 score more efficiently than the former.", "labels": [], "entities": []}, {"text": "Because the inference procedure is so simple, it is important that the parameters of the scoring function are chosen in a manner that facilitates how we climb the scoring function in small steps.", "labels": [], "entities": []}, {"text": "One way to achieve this is to make sure that improvements in the scoring functions are correlated with improvements in the quality of the parse.", "labels": [], "entities": []}, {"text": "This approach was suggested in the SampleRank framework) for training structured prediction models.", "labels": [], "entities": []}, {"text": "This method was originally developed fora sequence labeling task with local features, and was shown to be more effective than state-of-the-art alternatives.", "labels": [], "entities": [{"text": "sequence labeling task", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7236963510513306}]}, {"text": "Here we apply SampleRank to parsing, applying several modifications such as the proposal distributions mentioned earlier.", "labels": [], "entities": [{"text": "parsing", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9835839867591858}]}, {"text": "The benefits of sampling-based learning go beyond stand-alone parsing.", "labels": [], "entities": []}, {"text": "For instance, we can use the framework to correct preprocessing mistakes in features such as part-of-speech (POS) tags.", "labels": [], "entities": []}, {"text": "In this case, we combine the scoring function for trees with a stand-alone tagging model.", "labels": [], "entities": []}, {"text": "When proposing a small move, i.e., sampling ahead of the word, we can also jointly sample its POS tag from a set of alternatives provided by the tagger.", "labels": [], "entities": []}, {"text": "As a result, the selected tag is influenced by abroad syntactic context above and beyond the initial tagging model and is directly optimized to improve parsing performance.", "labels": [], "entities": []}, {"text": "Our joint parsing-tagging model provides an alternative to the widely-adopted pipeline setup.", "labels": [], "entities": []}, {"text": "We evaluate our method on benchmark multilingual dependency corpora.", "labels": [], "entities": []}, {"text": "Our method outperforms the Turbo parser across 14 languages on average by 0.5%.", "labels": [], "entities": []}, {"text": "On four languages, we top the best published results.", "labels": [], "entities": []}, {"text": "Our method provides a more effective mechanism for handling global features than reranking, outperforming it by 1.3%.", "labels": [], "entities": []}, {"text": "In terms of joint parsing and tagging on the CATiB dataset, we nearly bridge (88.38%) the gap between independently predicted (86.95%) and gold tags (88.45%).", "labels": [], "entities": [{"text": "CATiB dataset", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9759915173053741}]}, {"text": "This is better than the best published results in the 2013 SPMRL shared task, including parser ensembles.", "labels": [], "entities": [{"text": "SPMRL shared task", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7366603215535482}]}], "datasetContent": [{"text": "Datasets We evaluate our model on standard.", "labels": [], "entities": []}, {"text": "CATiB mostly includes projective trees.", "labels": [], "entities": [{"text": "CATiB", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8829858303070068}]}, {"text": "The trees are annotated with both gold and predicted versions of POS tags and morphology information.", "labels": [], "entities": []}, {"text": "Following, for this dataset we use 12 core POS tags, word lemmas, determiner features, rationality features and functional genders and numbers.", "labels": [], "entities": []}, {"text": "Some CATiB sentences exceed 200 tokens.", "labels": [], "entities": []}, {"text": "For efficiency, we limit the sentence length to 70 tokens in training and development sets.", "labels": [], "entities": []}, {"text": "However, we do not impose this constraint during testing.", "labels": [], "entities": []}, {"text": "We handle long sentences during testing by applying a simple split-merge strategy.", "labels": [], "entities": []}, {"text": "We split the sentence based on the ending punctuation, predict the parse tree for each segment and group the roots of resulting trees into a single node.", "labels": [], "entities": []}, {"text": "Evaluation Measures Following standard practice, we use Unlabeled Attachment Score (UAS) as the evaluation metric in all our experiments.", "labels": [], "entities": [{"text": "Evaluation Measures", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6787731051445007}, {"text": "Unlabeled Attachment Score (UAS)", "start_pos": 56, "end_pos": 88, "type": "METRIC", "confidence": 0.8942009508609772}]}, {"text": "We report UAS excluding punctuation on CoNLL datasets, following.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6118817925453186}, {"text": "CoNLL datasets", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9654909670352936}]}, {"text": "For the CATiB dataset, we report UAS including punctuation in order to be consistent with the published results in the 2013 SPMRL shared task).", "labels": [], "entities": [{"text": "CATiB dataset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.955292284488678}, {"text": "UAS", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9931035041809082}, {"text": "SPMRL shared task", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.46829907099405926}]}, {"text": "Baselines We compare our model with the Turbo parser and the MST parser.", "labels": [], "entities": [{"text": "MST", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8903958201408386}]}, {"text": "For the Turbo parser, we directly compare with the recent published results in).", "labels": [], "entities": [{"text": "Turbo parser", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.623816579580307}]}, {"text": "For the MST parser, we train a second-order non-projective model using the most recent version of the code 3 . We also compare our model against a discriminative reranker.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.5181148648262024}]}, {"text": "The reranker operates over the 3 http://sourceforge.net/projects/mstparser/ top-50 list obtained from the MST parser . We use a 10-fold cross-validation to generate candidate lists for training.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.8107574880123138}]}, {"text": "We then train the reranker by running 10 epochs of cost-augmented MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9214552640914917}]}, {"text": "The reranker uses the same features as our model, along with the tree scores obtained from the MST parser (which is a standard practice in reranking).", "labels": [], "entities": []}, {"text": "Experimental Details Following, we always first train a first-order pruner.", "labels": [], "entities": []}, {"text": "For each word xi , we prune away the incoming dependencies hi , xi with probability less than 0.005 times the probability of the most likely head, and limit the number of candidate heads up to 30.", "labels": [], "entities": []}, {"text": "This gives a 99% pruning recall on the CATiB development set.", "labels": [], "entities": [{"text": "pruning", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9427968859672546}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.6284910440444946}, {"text": "CATiB development set", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.9303247332572937}]}, {"text": "The first-order model is also trained using the algorithm in.", "labels": [], "entities": []}, {"text": "After pruning, we tune the regularization parameter C = {0.1, 0.01, 0.001} on development sets for different languages.", "labels": [], "entities": []}, {"text": "Because the CoNLL datasets do not have a standard development set, we randomly select a held out of 200 sentences from the training set.", "labels": [], "entities": [{"text": "CoNLL datasets", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9428080022335052}]}, {"text": "We also pick the training epochs from {50, 100, 150} which gives the best performance on the development set for each language.", "labels": [], "entities": []}, {"text": "After tuning, the model is trained on the full training set with the selected parameters.", "labels": [], "entities": []}, {"text": "We apply the Random Walk-based sampling method (see Section 3.2.2) for the standard dependency parsing task.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7718908190727234}]}, {"text": "However, for the joint parsing and POS correction on the CATiB dataset we do not use the Random Walk method because the first-order features in normal parsing are no longer first-order when POS tags are also variables.", "labels": [], "entities": [{"text": "POS correction", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.6369718909263611}, {"text": "CATiB dataset", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9678203165531158}]}, {"text": "Therefore, the first-order distribution is not well-defined and we only employ Gibbs sampling for simplicity.", "labels": [], "entities": []}, {"text": "On the CATiB dataset, we restrict the sample trees to always be projective as described in Section 3.2.1.", "labels": [], "entities": [{"text": "CATiB dataset", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9636151790618896}]}, {"text": "However, we do not impose this constraint for the CoNLL datasets.", "labels": [], "entities": [{"text": "CoNLL datasets", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9682559370994568}]}], "tableCaptions": [{"text": " Table 1: POS tag feature templates. t i and w i de- notes the POS tag and the word at the current posi- tion. t i\u2212x and t i+x denote the left and right context  tags, and similarly for words.", "labels": [], "entities": []}, {"text": " Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Bj\u00f6rkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Bj\u00f6rkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9809378981590271}, {"text": "corrective tagging", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.794682502746582}, {"text": "CATiB dataset", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9320325553417206}, {"text": "UAS", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9883977770805359}, {"text": "POS correction", "start_pos": 151, "end_pos": 165, "type": "METRIC", "confidence": 0.9428052008152008}, {"text": "UAS", "start_pos": 185, "end_pos": 188, "type": "METRIC", "confidence": 0.9964929223060608}, {"text": "SPMRL shared task", "start_pos": 217, "end_pos": 234, "type": "TASK", "confidence": 0.6559262871742249}]}]}