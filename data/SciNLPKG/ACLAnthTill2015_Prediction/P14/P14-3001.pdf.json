{"title": [{"text": "Bayesian Kernel Methods for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Kernel methods are heavily used in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.7598513861497244}]}, {"text": "Frequen-tist approaches like Support Vector Machines are the state-of-the-art in many tasks.", "labels": [], "entities": []}, {"text": "However, these approaches lack efficient procedures for model selection, which hinders the usage of more advanced kernels.", "labels": [], "entities": [{"text": "model selection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.6966761946678162}]}, {"text": "In this work, we propose the use of a Bayesian approach for kernel methods, Gaussian Processes, which allow easy model fitting even for complex kernel combinations.", "labels": [], "entities": []}, {"text": "Our goal is to employ this approach to improve results in a number of regression and classification tasks in NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last years, kernel methods have been successfully employed in many Natural Language Processing tasks.", "labels": [], "entities": [{"text": "Natural Language Processing tasks", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.6941962540149689}]}, {"text": "These methods allow the building of non-parametric models which make less assumptions about the underlying pattern in the data.", "labels": [], "entities": []}, {"text": "Another advantage of kernels is that they can be defined in arbitrary structures like strings or trees, which greatly reduce the need for careful feature engineering in these structures.", "labels": [], "entities": []}, {"text": "The properties cited above make kernel methods ideal for problems where we do not have much prior knowledge about how the data behaves.", "labels": [], "entities": []}, {"text": "This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs).", "labels": [], "entities": []}, {"text": "Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (), Sentiment Analysis ( and Question Classification).", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.9230123162269592}, {"text": "Question Classification", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.7944862842559814}]}, {"text": "Recently, they were also employed in regression settings like Machine Translation Quality Estimation () and structured prediction.", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.8168116360902786}, {"text": "structured prediction", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.8666784167289734}]}, {"text": "SVMs area frequentist method: they aim to find an approximation to the exact latent function that explains the data.", "labels": [], "entities": [{"text": "SVMs area frequentist", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6747443477312723}]}, {"text": "This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing overall its possible values.", "labels": [], "entities": []}, {"text": "Although there is some discussion about which approach is better (Murphy, 2012, Sec.", "labels": [], "entities": []}, {"text": "6.6.4), Bayesian methods offer many useful theoretical properties.", "labels": [], "entities": []}, {"text": "In fact, they have been used before in NLP, especially in grammar induction) and word segmentation ().", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7411782443523407}, {"text": "word segmentation", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7732832431793213}]}, {"text": "However, only very recently kernel methods have been applied in NLP using the Bayesian approach.", "labels": [], "entities": []}, {"text": "Gaussian Processes (GPs) are the Bayesian counterpart of kernel methods and are widely considered the state-of-the-art for inference on functions ().", "labels": [], "entities": []}, {"text": "They have a number of advantages which are very useful in NLP: \u2022 Kernels in general can be combined and parameterized in many ways.", "labels": [], "entities": []}, {"text": "This parameterization lead to the problem of model selection, which is difficult in frequentist approaches (mainly based on cross validation).", "labels": [], "entities": [{"text": "model selection", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7404374182224274}]}, {"text": "The Bayesian formulation of GPs let them deal with model selection in a much more more efficient and elegant way: by maximizing the likelihood on the training data.", "labels": [], "entities": [{"text": "model selection", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.6857978403568268}]}, {"text": "This opens the door for the use of heavily parameterized kernel combinations, like multi-task kernels for example.", "labels": [], "entities": []}, {"text": "\u2022 Being a probabilistic framework, they are able to naturally encode uncertainty in the predictions, which can be propagated if the task is part of a larger system pipeline.", "labels": [], "entities": []}, {"text": "Besides these properties, GPs have also been applied sucessfully in many Machine Learning tasks.", "labels": [], "entities": [{"text": "Machine Learning tasks", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.7997275193532308}]}, {"text": "Examples include Robotics (), Bioinformatics (), Geolocation () and Computer Vision (.", "labels": [], "entities": [{"text": "Computer Vision", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.6926576048135757}]}, {"text": "In NLP, GPs have been used only very recently and focused on regression tasks.", "labels": [], "entities": []}, {"text": "In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}