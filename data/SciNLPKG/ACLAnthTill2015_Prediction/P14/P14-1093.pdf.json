{"title": [{"text": "Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features", "labels": [], "entities": [{"text": "Extracting Event Causality Exploiting Semantic Relation", "start_pos": 35, "end_pos": 90, "type": "TASK", "confidence": 0.8673480848471323}]}], "abstractContent": [{"text": "We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture\u2192exacerbate desertification from the web using semantic relation (between nouns), context, and association features.", "labels": [], "entities": []}, {"text": "Experiments show that our method outperforms base-lines that are based on state-of-the-art methods.", "labels": [], "entities": []}, {"text": "We also propose methods of generating future scenarios like conduct slash-and-burn agriculture\u2192exacerbate desertification\u2192increase Asian dust (from China)\u2192asthma gets worse.", "labels": [], "entities": []}, {"text": "Experiments show that we can generate 50,000 scenarios with 68% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9983745813369751}]}, {"text": "We also generated a scenario deforestation con-tinues\u2192global warming worsens\u2192sea temperatures rise\u2192vibrio parahaemolyti-cus fouls (water), which is written in no document in our input web corpus crawled in 2007.", "labels": [], "entities": [{"text": "input web corpus crawled in 2007", "start_pos": 178, "end_pos": 210, "type": "DATASET", "confidence": 0.888598620891571}]}, {"text": "But the vibrio risk due to global warming was observed in Baker-Austin et al.", "labels": [], "entities": []}, {"text": "Thus, we \"predicted\" the future event sequence in a sense.", "labels": [], "entities": []}], "introductionContent": [{"text": "The world can be seen as a network of causality where people, organizations, and other kinds of entities causally depend on each other.", "labels": [], "entities": []}, {"text": "This network is so huge and complex that it is almost impossible for humans to exhaustively predict the consequences of a given event.", "labels": [], "entities": []}, {"text": "Indeed, after the Great East Japan Earthquake in 2011, few expected that it would lead to an enormous trade deficit in Japan due to a sharp increase in energy imports.", "labels": [], "entities": []}, {"text": "For effective decision making that carefully considers any form of future risks and chances, we need a system that helps humans do scenario planning, which is a decision-making scheme that examines possible future events and assesses their potential chances and risks.", "labels": [], "entities": [{"text": "decision making", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7526968121528625}, {"text": "scenario planning", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.767968475818634}]}, {"text": "Our ultimate goal is to develop a system that supports scenario planning through generating possible future events using big data, which would contain what Donald Rumsfeld called \"unknown unknowns\" 1 (.", "labels": [], "entities": [{"text": "scenario planning", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.881434291601181}]}, {"text": "To this end, we propose a supervised method of extracting such event causality as conduct slash-and-burn agriculture\u2192exacerbate desertification and use its output to generate future scenarios (scenarios), which are chains of causality that have been or might be observed in this world like conduct slash-and-burn agriculture\u2192exacerbate desertification\u2192increase Asian dust (from China)\u2192asthma gets worse.", "labels": [], "entities": []}, {"text": "Note that, in this paper, A\u2192B denotes that A causes B, which means that \"if A happens, the probability of B increases.\"", "labels": [], "entities": []}, {"text": "Our notion of causality should be interpreted probabilistically rather than logically.", "labels": [], "entities": []}, {"text": "Our method extracts event causality based on three assumptions that are embodied as features of our classifier.", "labels": [], "entities": []}, {"text": "First, we assume that two nouns (e.g. slash-and-burn agriculture and desertification) that take some specific binary semantic relations (e.g. A CAUSES B) tend to constitute event causality if combined with two predicates (e.g. conduct and exacerbate).", "labels": [], "entities": [{"text": "A CAUSES B", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.8181180556615194}]}, {"text": "Note that semantic relations are not restricted to those directly relevant to causality like A CAUSES B but can be those that might seem irrelevant to causality like A IS AN INGREDIENT FOR B (e.g. plutonium and atomic bomb as in plutonium is stolen\u2192atomic bomb is made).", "labels": [], "entities": [{"text": "A CAUSES B", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.8244938453038534}, {"text": "A IS AN INGREDIENT FOR B", "start_pos": 166, "end_pos": 190, "type": "METRIC", "confidence": 0.8123366634051005}]}, {"text": "Our underlying intuition is the observation that event causality tends to hold between two entities linked by semantic relations which roughly entail that one entity strongly affects the other.", "labels": [], "entities": []}, {"text": "Such semantic relations can be expressed by (otherwise unintuitive) patterns like A IS AN INGRE-DIENT FOR B.", "labels": [], "entities": [{"text": "A IS AN INGRE-DIENT FOR B", "start_pos": 82, "end_pos": 107, "type": "METRIC", "confidence": 0.7568760017553965}]}, {"text": "As such, semantic relations like the MATERIAL relation can also be useful.", "labels": [], "entities": [{"text": "MATERIAL", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.6538054347038269}]}, {"text": "(See Sec-tion 3.2.1 fora more intuitive explanation.)", "labels": [], "entities": []}, {"text": "Our second assumption is that there are grammatical contexts in which event causality is more likely to appear.", "labels": [], "entities": []}, {"text": "We implement what we consider likely contexts for event causality as context features.", "labels": [], "entities": []}, {"text": "For example, a likely context of event causality (underlined) would be: CO2 levels rose, so climatic anomalies were observed, while an unlikely context would be: It remains uncertain whether if the recession is bottomed the declining birthrate is halted.", "labels": [], "entities": []}, {"text": "Useful context information includes the mood of the sentences (e.g., the uncertainty mood expressed by uncertain above), which is represented by lexical features (Section 3.2.2).", "labels": [], "entities": []}, {"text": "The last assumption embodied in our association features is that each word of the cause phrase must have a strong association (i.e., PMI, for example) with that of the effect phrase as slash-andburn agriculture and desertification in the above example, as in.", "labels": [], "entities": [{"text": "PMI", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.6481180787086487}]}, {"text": "Our method exploits these features on top of our base features such as nouns and predicates.", "labels": [], "entities": []}, {"text": "Experiments using 600 million web pages show that our method outperforms baselines based on state-of-the-art methods) by more than 19% of average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9969493746757507}]}, {"text": "We require that event causality be selfcontained, i.e., intelligible as causality without the sentences from which it was extracted.", "labels": [], "entities": []}, {"text": "For example, omit toothbrushing\u2192get a cavity is selfcontained, but omit toothbrushing\u2192get a girlfriend is not since this is not intelligible without a context: He omitted toothbrushing everyday and got a girlfriend who was a dental assistant of dental clinic he went to for his cavity.", "labels": [], "entities": []}, {"text": "This is important since future scenarios, which are generated by chaining event causality as described below, must be self-contained, unlike.", "labels": [], "entities": []}, {"text": "To make event causality self-contained, we wrote guidelines for manually annotating training/development/test data.", "labels": [], "entities": []}, {"text": "Annotators regarded as event causality only phrase pairs that were interpretable as event causality without contexts (i.e., self-contained).", "labels": [], "entities": []}, {"text": "From the training data, our method seemed to successfully learn what selfcontained event causality is.", "labels": [], "entities": []}, {"text": "Our scenario generation method generates scenarios by chaining extracted event causality; generating A\u2192B\u2192C from A\u2192B and B\u2192C.", "labels": [], "entities": [{"text": "scenario generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8166585862636566}]}, {"text": "The challenge is that many acceptable scenarios are overlooked if we require the joint part of the chain (B above) to bean exact match.", "labels": [], "entities": []}, {"text": "To increase the number of acceptable scenarios, our method identifies compatibility w.r.t causality between two phrases by a recently proposed semantic polarity, excitation (, which properly relaxes the chaining condition (Section 3.1 describes it).", "labels": [], "entities": []}, {"text": "For example, our method can identify the compatibility between sea temperatures are high and sea temperatures rise to chain global warming worsens\u2192sea temperatures are high and sea temperatures rise\u2192vibrio parahaemolyticus 2 fouls (water).", "labels": [], "entities": []}, {"text": "Accordingly, we generated a scenario deforestation continues\u2192global warming worsens\u2192sea temperatures rise\u2192vibrio parahaemolyticus fouls (water), which is written in no document in our input web corpus that was crawled in 2007, but the vibrio risk due to global warming has actually been observed in the Baltic sea and reported in.", "labels": [], "entities": []}, {"text": "Ina sense, we \"predicted\" the event sequence reported in 2013 by documents written in 2007.", "labels": [], "entities": []}, {"text": "Our experiments also show that we generated 50,000 scenarios with 68% precision, which include conduct terrorist operations\u2192terrorist bombing occurs\u2192cause fatalities and injuries\u2192cause economic losses and the above \"slash-and-burn agriculture\" scenario (Section 5.2).", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9992380142211914}]}, {"text": "Neither is written in any document in our input corpus.", "labels": [], "entities": []}, {"text": "In this paper, our target language is Japanese.", "labels": [], "entities": []}, {"text": "However, we believe that our ideas and methods are applicable to many languages.", "labels": [], "entities": []}, {"text": "Examples are translated into English for ease of explanation.", "labels": [], "entities": []}, {"text": "Supplementary notes of this paper are available at http://khn.nict.go.jp/analysis/ member/ch/acl2014-sup.pdf.", "labels": [], "entities": []}], "datasetContent": [{"text": "The numbers of two-and three-step scenarios generated by Proposed were 217,836 and 5,288,352, while those of Exact were 22,910 and 72,746.", "labels": [], "entities": [{"text": "Exact", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.8099511861801147}]}, {"text": "We sampled 2,000 from Proposed's two-and three-step scenarios and 1,000 from those of Exact.", "labels": [], "entities": []}, {"text": "We applied the filters to the sampled scenarios of Proposed, and the results were regarded as the sample scenarios of Proposed+Orig and Proposed+Orig+Comm.", "labels": [], "entities": [{"text": "Proposed", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8723296523094177}]}, {"text": "shows the number and precision of the samples.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9994841814041138}]}, {"text": "Note that, for the diversity of the sampled scenarios, our sampling proceeded as follows: (i) Randomly sample a beginning event phrase from the generated scenarios.", "labels": [], "entities": []}, {"text": "(ii) Randomly sample an effect phrase for the beginning event phrase from the scenarios.", "labels": [], "entities": []}, {"text": "(iii) Regarding the effect phrase as a cause phrase, randomly sample an effect phrase for it, and repeat (iii) up to the specified number of steps (2 or 3).", "labels": [], "entities": []}, {"text": "The samples were annotated by three annotators (not the authors), who were instructed to regard a sample as acceptable if each event causality that constitutes it is plausible and the sample as a whole constitutes a single coherent story.", "labels": [], "entities": []}, {"text": "Final judgment was made by majority vote.", "labels": [], "entities": []}, {"text": "Fleiss' kappa of their judgments was 0.53 (moderate agreement), which is lower than the kappa for the causality judgment.", "labels": [], "entities": []}, {"text": "This is probably because   Result 1 shows the estimated number of acceptable scenarios generated with 70% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9973862767219543}]}, {"text": "The estimated number is calculated as the product of the recall at 70% precision and the number of acceptable scenarios in all the generated scenarios, which is estimated by the annotated samples.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9966101050376892}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9989427924156189}]}, {"text": "show the precisionscenario curves for the two-and three-step scenarios, which illustrate how many acceptable scenarios can be generated with what precision.", "labels": [], "entities": [{"text": "precisionscenario", "start_pos": 9, "end_pos": 26, "type": "METRIC", "confidence": 0.9992621541023254}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9498682022094727}]}, {"text": "The curve is drawn in the same way as the precisionrecall curve except that the X-axis indicates the estimated number of acceptable scenarios.", "labels": [], "entities": [{"text": "precisionrecall", "start_pos": 42, "end_pos": 57, "type": "METRIC", "confidence": 0.9989983439445496}]}, {"text": "At 70% precision, all of the proposed methods outperformed Exact in the two-step setting, and Proposed+Orig+Comm outperformed Exact in the three-step setting.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9993237257003784}]}, {"text": "Result 2 To evaluate the top-ranked scenarios of Proposed+Orig+Comm in the three-step setting with more samples, the annotators labeled 500 samples from the top 50,000 of its output.", "labels": [], "entities": []}, {"text": "341 (68.20%) were acceptable, and the estimated number of acceptable scenarios at a precision rate of 70% and 80% are 26,700 and 5,200 (See Section H in the supplementary notes).", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.999248206615448}]}, {"text": "The \"terrorist operations\" scenario and the \"slash-and-burn agriculture\" scenario in Section 1 were ranked 16,386th Next we examined how many of the top 50,000 scenarios were acceptable and nontrivial, i.e., found in no page in our input web corpus, using the 341 acceptable samples.", "labels": [], "entities": []}, {"text": "A scenario was regarded as non-trivial if its nouns co-occur in no page of the corpus.", "labels": [], "entities": []}, {"text": "22 among the 341 samples were non-trivial.", "labels": [], "entities": []}, {"text": "Accordingly, we estimate that we can generate 2,200 ( 50,000\u00d722", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Ablation tests on semantic relations.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9397059082984924}]}, {"text": " Table 5: Ablation tests on association features.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9912960529327393}]}, {"text": " Table 6: Average precision of our proposed meth- ods and baselines using CEA.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9325839877128601}, {"text": "CEA", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.723061203956604}]}, {"text": " Table 7: Average precision of our proposed  method and baselines using Cs.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9730947613716125}]}, {"text": " Table 9: Estimated number of acceptable scenar- ios with a 70% precision rate.", "labels": [], "entities": [{"text": "Estimated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.997007429599762}, {"text": "precision rate", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.9892712533473969}]}]}