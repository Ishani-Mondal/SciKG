{"title": [{"text": "Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms", "labels": [], "entities": []}], "abstractContent": [{"text": "Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization.", "labels": [], "entities": []}, {"text": "However, these new methods lack the rich priors associated with probabilistic models.", "labels": [], "entities": []}, {"text": "We examine Arora et al.'s anchor words algorithm for topic mod-eling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models.", "labels": [], "entities": []}, {"text": "Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models are of practical and theoretical interest.", "labels": [], "entities": []}, {"text": "Practically, they have been used to understand political perspective, improve machine translation), reveal literary trends, and understand scientific discourse (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7471893727779388}]}, {"text": "Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena.", "labels": [], "entities": []}, {"text": "Modern topic models are formulated as a latent variable model.", "labels": [], "entities": []}, {"text": "Like hidden Markov models, each token comes from one of K unknown distributions.", "labels": [], "entities": []}, {"text": "Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics.", "labels": [], "entities": []}, {"text": "Posterior inference discovers the hidden variables that best explain a dataset.", "labels": [], "entities": []}, {"text": "Typical solutions use MCMC () or variational EM (, which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood.", "labels": [], "entities": []}, {"text": "An exciting vein of new research provides provable polynomial-time alternatives.", "labels": [], "entities": []}, {"text": "These approaches provide solutions to hidden Markov models (, mixture models (), and latent variable grammars (.", "labels": [], "entities": []}, {"text": "The key insight is not to directly optimize observation likelihood but to instead discover latent variables that can reconstruct statistics of the assumed generative model.", "labels": [], "entities": []}, {"text": "Unlike search-based methods, which can be caught in local minima, these techniques are often guaranteed to find global optima.", "labels": [], "entities": []}, {"text": "These general techniques can be improved by making reasonable assumptions about the models.", "labels": [], "entities": []}, {"text": "For example,'s approach for inference in topic models assume that each topic has a unique \"anchor\" word (thus, we call this approach anchor).", "labels": [], "entities": []}, {"text": "This approach is fast and effective; because it only uses word co-occurrence information, it can scale to much larger datasets than MCMC or EM alternatives.", "labels": [], "entities": []}, {"text": "We review the anchor method in Section 2.", "labels": [], "entities": [{"text": "anchor", "start_pos": 14, "end_pos": 20, "type": "TASK", "confidence": 0.9466954469680786}]}, {"text": "Despite their advantages, these techniques are not a panacea.", "labels": [], "entities": []}, {"text": "They do not accommodate the rich priors that modelers have come to expect.", "labels": [], "entities": []}, {"text": "Priors can improve performance (, provide domain adaptation, and guide models to reflect users' needs (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7315273582935333}]}, {"text": "In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors.", "labels": [], "entities": []}, {"text": "Another shortcoming is that these models have not been scrutinized using standard NLP evaluations.", "labels": [], "entities": []}, {"text": "Because these approaches emerged from the theory community, anchor's evaluations, when present, typically use training reconstruction.", "labels": [], "entities": []}, {"text": "In Section 4, we show that our regularized models can generalize to previously unseen data-as measured by held-out likelihood (-and are more interpretable).", "labels": [], "entities": []}, {"text": "We also show that our extension to the anchor method enables new applications: for set of anchor word indexes {s1, . .", "labels": [], "entities": []}, {"text": "sK } \u03bb regularization weight: Notation used.", "labels": [], "entities": []}, {"text": "Matrices are in bold (Q, C), sets are in script S example, using an informed priors to discover concepts of interest.", "labels": [], "entities": []}, {"text": "Having shown that regularization does improve performance, in Section 5 we explore why.", "labels": [], "entities": [{"text": "regularization", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.9492914080619812}]}, {"text": "We discuss the trade-off of training data reconstruction with sparsity and why regularized topics are more interpretable.", "labels": [], "entities": [{"text": "training data reconstruction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6842952370643616}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The number of documents in the train,  development, and test folds in our three datasets.", "labels": [], "entities": []}, {"text": " Table 4: Topics from anchor and anchor-beta with M = 100 on 20NEWS with 20 topics. Each topic is  identified with its associated anchor word. When M = 100, the topics of anchor suffer: the four colored  words appear in almost every topic. anchor-beta, in contrast, is less sensitive to suboptimal M .", "labels": [], "entities": [{"text": "M", "start_pos": 50, "end_pos": 51, "type": "METRIC", "confidence": 0.9463000297546387}, {"text": "20NEWS", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9607186317443848}]}]}