{"title": [{"text": "Enriching Cold Start Personalized Language Model Using Social Network Information", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a generalized framework to enrich the personalized language models for cold start users.", "labels": [], "entities": []}, {"text": "The cold start problem is solved with content written by friends on social network services.", "labels": [], "entities": [{"text": "cold start", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8787707388401031}]}, {"text": "Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph.", "labels": [], "entities": []}, {"text": "The factor graph is used to incorporate prior knowledge and heuris-tics to identify the most appropriate weights.", "labels": [], "entities": []}, {"text": "The intrinsic and extrinsic experiments show significant improvement on cold start users.", "labels": [], "entities": []}], "introductionContent": [{"text": "Personalized language models (PLM) on social network services are useful in many aspects (, For instance, if the authorship of a document is in doubt, a PLM maybe used as a generative model to identify it.", "labels": [], "entities": []}, {"text": "In this sense, a PLM serves as a proxy of one's writing style.", "labels": [], "entities": []}, {"text": "Furthermore, PLMs can improve the quality of information retrieval and content-based recommendation systems, where documents or topics can be recommended based on the generative probabilities.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7150126844644547}]}, {"text": "However, it is challenging to build a PLM for users who just entered the system, and whose content is thus insufficient to characterize them.", "labels": [], "entities": []}, {"text": "These are called \"cold start\" users.", "labels": [], "entities": []}, {"text": "Producing better recommendations is even more critical for cold start users to make them continue to use the system.", "labels": [], "entities": []}, {"text": "Therefore, this paper focuses on how to overcome the cold start problem and obtain a better PLM for cold start users.", "labels": [], "entities": []}, {"text": "The content written by friends on asocial network service, such as Facebook or Twitter, is exploited.", "labels": [], "entities": []}, {"text": "It can be either a reply to an original post or posts by friends.", "labels": [], "entities": []}, {"text": "Here the hypothesis is that friends, who usually share common interests, tend to discuss similar topics and use similar words than non-friends.", "labels": [], "entities": []}, {"text": "In other words, we believe that a cold start user's language model can be enriched and better personalized by incorporating content written by friends.", "labels": [], "entities": []}, {"text": "Intuitively, a linear combination of documentlevel language models can be used to incorporate content written by friends.", "labels": [], "entities": []}, {"text": "However, it should be noticed that some documents are more relevant than others, and should be weighted higher.", "labels": [], "entities": []}, {"text": "To obtain better weights, some simple heuristics could be exploited.", "labels": [], "entities": []}, {"text": "For example, we can measure the similarity or distance between a user language model and a document language model.", "labels": [], "entities": []}, {"text": "In addition, documents that are shared frequently in asocial network are usually considered to be more influential, and could contribute more to the language model.", "labels": [], "entities": []}, {"text": "More complex heuristics can also be derived.", "labels": [], "entities": []}, {"text": "For instance, if two documents are posted by the same person, their weights should be more similar.", "labels": [], "entities": []}, {"text": "The main challenge lies in how such heuristics can be utilized in a systematic manner to infer the weights of each document-level language model.", "labels": [], "entities": []}, {"text": "In this paper, we exploit the information on social network services in two ways.", "labels": [], "entities": []}, {"text": "First, we impose the social dependency assumption via a finite mixture model.", "labels": [], "entities": []}, {"text": "We model the true, albeit unknown, personalized language model as a combination of a biased user language model and a set of relevant document language models.", "labels": [], "entities": []}, {"text": "Due to the noise inevitably contained in social media content, instead of using all available documents, we argue that by properly specifying the set of relevant documents, a better personalized language model can be learnt.", "labels": [], "entities": []}, {"text": "In other words, each user language model is enriched by a personalized collection of background documents.", "labels": [], "entities": []}, {"text": "Second, we propose a factor graph model (FGM) to incorporate prior knowledge (e.g. the heuristics described above) into our model.", "labels": [], "entities": []}, {"text": "Each mixture weight is represented by a random variable in the factor graph, and an efficient algorithm is proposed to optimize the model and infer the marginal distribution of these variables.", "labels": [], "entities": []}, {"text": "Useful information about these variables is encoded by a set of potential functions.", "labels": [], "entities": []}, {"text": "The main contributions of this work are summarized below: \uf09f To solve the cold start problem encountered when estimating PLMs, a generalized framework based on FGM is proposed.", "labels": [], "entities": [{"text": "FGM", "start_pos": 159, "end_pos": 162, "type": "DATASET", "confidence": 0.8328409194946289}]}, {"text": "We incorporate social network information into user language models through the use of FGM.", "labels": [], "entities": [{"text": "FGM", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.8607720732688904}]}, {"text": "An iterative optimization procedure utilizing perplexity is presented to learn the parameters.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first proposal to use FGM to enrich language models.", "labels": [], "entities": []}, {"text": "\uf09f Perplexity is selected as an intrinsic evaluation, and experiment on authorship attribution is used as an extrinsic evaluation.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7593218386173248}]}, {"text": "The results show that our model yields significant improvements for cold start users.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on the Twitter dataset collected by.", "labels": [], "entities": [{"text": "Twitter dataset collected", "start_pos": 30, "end_pos": 55, "type": "DATASET", "confidence": 0.8718849420547485}]}, {"text": "Twitter data have been used to verify models with different purposes).", "labels": [], "entities": []}, {"text": "To emphasize on the cold start scenario, we randomly selected 15 users with about 35 tweets and 70 friends as candidates for an authorship attribution task.", "labels": [], "entities": []}, {"text": "Our experiment corpus consists of 4322 tweets.", "labels": [], "entities": []}, {"text": "All words with less than 5 occurrences are removed.", "labels": [], "entities": []}, {"text": "Stop words and URLs are also removed and all tweets are stemmed.", "labels": [], "entities": []}, {"text": "We identify the 100 most frequent terms as categories.", "labels": [], "entities": []}, {"text": "The size of the vocabulary set is 1377.", "labels": [], "entities": []}, {"text": "We randomly partitioned the tweets of each user into training, validation and testing sets.", "labels": [], "entities": []}, {"text": "The reported result is the average of 10 random splits.", "labels": [], "entities": []}, {"text": "In all experiments, we vary the size of training data from 1% to 15%, and holdout the same number of tweets from each user as validation and testing data.", "labels": [], "entities": []}, {"text": "The statistics of our dataset, given 15% training data, are shown in.", "labels": [], "entities": []}, {"text": "Loopy belief propagation (LBP) is used to obtain the marginal probabilities of the variables ().", "labels": [], "entities": [{"text": "Loopy belief propagation (LBP)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7019527107477188}]}, {"text": "Parameters are searched with the pattern search algorithm).", "labels": [], "entities": []}, {"text": "To not lose generality, we use the default configuration in all experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Testing set perplexity. ** indicates that the best score among all methods is significantly bet- ter than the next highest score, by t-test at a significance level of 0.05.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy (%) of authorship attribution. ** indicates that the best score among all methods is  significantly better than the next highest score, by t-test at a significance level of 0.05.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998831570148468}]}]}