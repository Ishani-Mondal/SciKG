{"title": [{"text": "Max-Margin Tensor Neural Network for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6005350053310394}]}], "abstractContent": [{"text": "Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6097005009651184}]}, {"text": "By exploiting tag embeddings and tensor-based transformation, MMTNN has the ability to model complicated interactions between tags and context characters.", "labels": [], "entities": []}, {"text": "Furthermore , anew tensor factorization approach is proposed to speedup the model and avoid overfitting.", "labels": [], "entities": []}, {"text": "Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering.", "labels": [], "entities": []}, {"text": "Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.5978641013304392}, {"text": "sequence labeling tasks", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7090359528859457}]}], "introductionContent": [{"text": "Unlike English and other western languages, Chinese do not delimit words by white-space.", "labels": [], "entities": []}, {"text": "Therefore, word segmentation is a preliminary and important pre-process for Chinese language processing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7596756815910339}, {"text": "Chinese language processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.694512665271759}]}, {"text": "Most previous systems address this problem by treating this task as a sequence labeling problem where each character is assigned a tag indicating its position in the word.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.6867405772209167}]}, {"text": "These systems are effective because researchers can incorporate a large body of handcrafted features into the models.", "labels": [], "entities": []}, {"text": "However, the ability of these models is restricted * Corresponding author by the design of features and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus.", "labels": [], "entities": []}, {"text": "Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.791367381811142}]}, {"text": "developed the SENNA system that approaches or surpasses the state-of-the-art systems on a variety of sequence labeling tasks for English.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.7046091159184774}]}, {"text": "applied the architecture of to Chinese word segmentation and POS tagging and proposed a perceptronstyle algorithm to speedup the training process with negligible loss in performance.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7058595567941666}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7281171232461929}]}, {"text": "Workable as previous neural network models seem, a limitation of them to be pointed out is that the tag-tag interaction, tag-character interaction and character-character interaction are not well modeled.", "labels": [], "entities": []}, {"text": "In conventional feature-based linear (log-linear) models, these interactions are explicitly modeled as features.", "labels": [], "entities": []}, {"text": "Take phrase \"\u6253\u7bee \u7403(play basketball)\" as an example, assuming we are labeling character C 0 =\"\u7bee\", possible features could be: 1 C \u22121 =\"\u6253\" and C 1 =\"\u7403\" and y 0 =\"B\" 0 else f 2 = 1 C 0 =\"\u7bee\" and y 0 =\"B\" and y \u22121 =\"S\" 0 else To capture more interactions, researchers have designed a large number of features based on linguistic intuition and statistical information.", "labels": [], "entities": []}, {"text": "In previous neural network models, however, hardly can such interactional effects be fully captured relying only on the simple transition score and the single non-linear transformation (See section 2).", "labels": [], "entities": []}, {"text": "In order to address this problem, we propose anew model called Max-Margin Tensor Neural Network (MMTNN) that explicitly models the interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation.", "labels": [], "entities": []}, {"text": "Moreover, we propose a tensor factorization approach that effectively improves the model efficiency and prevents from overfitting.", "labels": [], "entities": []}, {"text": "We evaluate the performance of Chinese word segmentation on the PKU and MSRA benchmark datasets in the second International Chinese Word Segmentation Bakeoff () which are commonly used for evaluation of Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6053264836470286}, {"text": "PKU", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9539749622344971}, {"text": "MSRA benchmark datasets", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.9527505238850912}, {"text": "International Chinese Word Segmentation Bakeoff", "start_pos": 110, "end_pos": 157, "type": "TASK", "confidence": 0.7418786287307739}, {"text": "Chinese word segmentation", "start_pos": 203, "end_pos": 228, "type": "TASK", "confidence": 0.6188401381174723}]}, {"text": "Experiment results show that our model outperforms other neural network models.", "labels": [], "entities": []}, {"text": "Although we focus on the question that how far we can go without using feature engineering in this paper, the study of deep learning for NLP tasks is still anew area in which it is currently challenging to surpass the state-of-the-art without additional features.", "labels": [], "entities": []}, {"text": "Following , we wonder how well our model can perform with minimal feature engineering.", "labels": [], "entities": []}, {"text": "Therefore, we integrate additional simple character bigram features into our model and the result shows that our model can achieve a competitive performance that other systems hardly achieve unless they use more complex task-specific features.", "labels": [], "entities": []}, {"text": "The main contributions of our work are as follows: \u2022 We propose a Max-Margin Tensor Neural Network for Chinese word segmentation without feature engineering.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.6328186293443044}]}, {"text": "The test results on the benchmark dataset show that our model outperforms previous neural network models.", "labels": [], "entities": []}, {"text": "\u2022 We propose anew tensor factorization approach that models each tensor slice as the product of two low-rank matrices.", "labels": [], "entities": []}, {"text": "Not only does this approach improve the efficiency of our model but also it avoids the risk of overfitting.", "labels": [], "entities": []}, {"text": "\u2022 Compared with previous works that use a large number of handcrafted features, our model can achieve a competitive performance with minimal feature engineering.", "labels": [], "entities": []}, {"text": "\u2022 Despite Chinese word segmentation being a specific case, our approach can be easily generalized to other sequence labeling tasks.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5943056543668112}, {"text": "sequence labeling tasks", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.7204699814319611}]}, {"text": "The remaining part of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the details of conventional neural network architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first perform a close test on the PKU dataset to show the effect of different model configurations.", "labels": [], "entities": [{"text": "PKU dataset", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.965249627828598}]}, {"text": "We also compare our model with the CRF model (), which is a widely used log-linear model for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.5786089102427164}]}, {"text": "The input feature to the CRF model is simply the context characters (unigram feature) without any additional feature engineering.", "labels": [], "entities": []}, {"text": "We use an open source toolkit CRF++ 4 to train the CRF model.", "labels": [], "entities": []}, {"text": "All the neural networks are trained using the Max-Margin approach described in Section 3.4.", "labels": [], "entities": []}, {"text": "As we can see, by using Tag embedding, the Fscore is improved by +0.6% and OOV recall is improved by +1.0%, which shows that tag embeddings succeed in modeling the tag-tag interaction and tag-character interaction.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9974029660224915}, {"text": "OOV", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9971879124641418}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.7641597390174866}]}, {"text": "Model performance is further boosted after using tensor-based transformation.", "labels": [], "entities": []}, {"text": "The F-score is improved by +0.6% while OOV recall is improved by +3.2%, which denotes that tensor-based transformation captures more interactional information than simple nonlinear transformation.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9982883334159851}, {"text": "OOV", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9956355690956116}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.8422743678092957}]}, {"text": "Another important result in is that our neural network models perform much better than CRF-based model when only unigram features are used.", "labels": [], "entities": []}, {"text": "Compared with CRF, there are two differences in neural network models.", "labels": [], "entities": []}, {"text": "First, the discrete feature vector is replaced with dense character embeddings.", "labels": [], "entities": []}, {"text": "Second, the non-linear transformation: Examples of character embeddings is used to discover higher level representation.", "labels": [], "entities": []}, {"text": "In fact, CRF can be regarded as a special neural network without non-linear function ().", "labels": [], "entities": []}, {"text": "conduct an empirical study on the effect of non-linearity and the results suggest that non-linear models are highly effective only when distributed representation is used.", "labels": [], "entities": []}, {"text": "To explain why distributed representation captures more information than discrete features, we show in the effect of character embeddings which are obtained from the lookup table of MMTNN after training.", "labels": [], "entities": [{"text": "MMTNN", "start_pos": 182, "end_pos": 187, "type": "DATASET", "confidence": 0.8462384939193726}]}, {"text": "The first row lists three characters we are interested in.", "labels": [], "entities": []}, {"text": "In each column, we list the top 5 characters that are nearest (measured by Euclidean distance) to the corresponding character in the first row according to their embeddings.", "labels": [], "entities": []}, {"text": "As we can see, characters in the first column are all Chinese number characters and characters in the second column and the third column are all Chinese family names and Chinese punctuations respectively.", "labels": [], "entities": []}, {"text": "Therefore, compared with discrete feature representations, distributed representation can capture the syntactic and semantic similarity between characters.", "labels": [], "entities": []}, {"text": "As a result, the model can still perform well even if some words do not appear in the training cases.", "labels": [], "entities": []}, {"text": "We further compare our model with previous neural network models on both PKU and MSRA datasets.", "labels": [], "entities": [{"text": "PKU", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8770843148231506}, {"text": "MSRA datasets", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.8702066242694855}]}, {"text": "Since did not report the results on the these datasets, we reimplemented their model and tested it on the test data.", "labels": [], "entities": []}, {"text": "The results are listed in the first three rows of, which shows that our model achieved higher F-score than the previous neural network models.", "labels": [], "entities": [{"text": "F-score", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.999319314956665}]}], "tableCaptions": [{"text": " Table 1: Details of the PKU and MSRA datasets", "labels": [], "entities": [{"text": "PKU", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9062627553939819}, {"text": "MSRA datasets", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8689374029636383}]}, {"text": " Table 2: Hyperparameters of our model", "labels": [], "entities": []}, {"text": " Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.", "labels": [], "entities": []}, {"text": " Table 5: Comparison with previous neural network models", "labels": [], "entities": []}]}