{"title": [{"text": "Learning Topic Representation for SMT with Neural Networks *", "labels": [], "entities": [{"text": "Learning Topic Representation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5416767597198486}, {"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9715580344200134}]}], "abstractContent": [{"text": "Statistical Machine Translation (SMT) usually utilizes contextual information to disambiguate translation candidates.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8264893293380737}]}, {"text": "However, it is often limited to contexts within sentence boundaries, hence broader topical information cannot be leveraged.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data.", "labels": [], "entities": [{"text": "learning topic representation", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.6234041055043539}]}, {"text": "By associating each translation rule with the topic representation, topic relevant rules are selected according to the dis-tributional similarity with the source text during SMT decoding.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 174, "end_pos": 186, "type": "TASK", "confidence": 0.9343225955963135}]}, {"text": "Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9542978405952454}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9172539114952087}, {"text": "NIST Chinese-to-English translation task", "start_pos": 93, "end_pos": 133, "type": "TASK", "confidence": 0.788702055811882}]}], "introductionContent": [{"text": "Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems.", "labels": [], "entities": [{"text": "Making translation decisions", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7593035300572714}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.8197491466999054}]}, {"text": "Current translation modeling approaches usually use context dependent information to disambiguate translation candidates.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.9623617231845856}]}, {"text": "For example, translation sense disambiguation approaches are proposed for phrase-based SMT systems.", "labels": [], "entities": [{"text": "translation sense disambiguation", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.9097915093104044}, {"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8150254487991333}]}, {"text": "Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9125294089317322}]}, {"text": "Although these methods are effective and proven successful in many SMT systems, they only leverage within- * This work was done while the first and fourth authors were visiting Microsoft Research.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.994059681892395}]}, {"text": "sentence contexts which are insufficient in exploring broader information.", "labels": [], "entities": []}, {"text": "For example, the word driver often means \"the operator of a motor vehicle\" in common texts.", "labels": [], "entities": []}, {"text": "But in the sentence \"Finally, we write the user response to the buffer, i.e., pass it to our driver\", we understand that driver means \"computer program\".", "labels": [], "entities": []}, {"text": "In this case, people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge.", "labels": [], "entities": []}, {"text": "Therefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance.", "labels": [], "entities": []}, {"text": "Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7812098562717438}, {"text": "characterizing various semantic concepts embedded in a collection of documents", "start_pos": 57, "end_pos": 135, "type": "TASK", "confidence": 0.6417465478181839}]}, {"text": "Attempts on topic-based translation modeling include topic-specific lexicon translation models (, topic similarity models for synchronous rules, and document-level translation with topic coherence.", "labels": [], "entities": [{"text": "topic-based translation modeling", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.6740542054176331}, {"text": "topic-specific lexicon translation", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.6682151357332865}]}, {"text": "In addition, topic-based approaches have been used in domain adaptation for SMT (, where they view different topics as different domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7385265380144119}, {"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9900888800621033}]}, {"text": "One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given.", "labels": [], "entities": []}, {"text": "In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (  or Hidden Topic Markov Model (HTMM) ( ).", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 137, "end_pos": 170, "type": "METRIC", "confidence": 0.825048178434372}]}, {"text": "Most of them also assume that the input must be in document level.", "labels": [], "entities": []}, {"text": "However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries.", "labels": [], "entities": []}, {"text": "In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9908887147903442}]}, {"text": "Although we can easily apply LDA at the sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence.", "labels": [], "entities": []}, {"text": "This makes previous approaches inefficient when applied them in real-world commercial SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9903678894042969}]}, {"text": "Therefore, we need to devise a systematical approach to enriching the sentence and inferring its topic more accurately.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to learning topic representations for sentences.", "labels": [], "entities": []}, {"text": "Since the information within the sentence is insufficient for topic modeling, we first enrich sentence contexts via Information Retrieval (IR) methods using content words in the sentence as queries, so that topic-related monolingual documents can be collected.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.8698662221431732}]}, {"text": "These topic-related documents are utilized to learn a specific topic representation for each sentence using a neural network based approach.", "labels": [], "entities": []}, {"text": "Neural network is an effective technique for learning different levels of data representations.", "labels": [], "entities": []}, {"text": "The levels inferred from neural network correspond to distinct levels of concepts, where high-level representations are obtained from low-level bag-ofwords input.", "labels": [], "entities": []}, {"text": "It is able to detect correlations among any subset of input features through non-linear transformations, which demonstrates the superiority of eliminating the effect of noisy words which are irrelevant to the topic.", "labels": [], "entities": []}, {"text": "Our problem fits well into the neural network framework and we expect that it can further improve inferring the topic representations for sentences.", "labels": [], "entities": []}, {"text": "To incorporate topic representations as translation knowledge into SMT, our neural network based approach directly optimizes similarities between the source language and target language in a compact topic space.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9911888837814331}]}, {"text": "This underlying topic space is learned from sentence-level parallel data in order to share topic information across the source and target languages as much as possible.", "labels": [], "entities": []}, {"text": "Additionally, our model can be discriminatively trained with a large number of training instances, without expensive sampling methods such as in LDA or HTMM, thus it is more practicable and scalable.", "labels": [], "entities": []}, {"text": "Finally, we associate the learned representation to each bilingual translation rule.", "labels": [], "entities": []}, {"text": "Topic-related rules are selected according to distributional similarity with the source text, which helps hypotheses generation in SMT decoding.", "labels": [], "entities": [{"text": "hypotheses generation", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7043589949607849}, {"text": "SMT decoding", "start_pos": 131, "end_pos": 143, "type": "TASK", "confidence": 0.9313781261444092}]}, {"text": "We integrate topic similarity features in the log-linear model and evaluate the performance on the NIST Chinese-to-English translation task.", "labels": [], "entities": [{"text": "NIST Chinese-to-English translation task", "start_pos": 99, "end_pos": 139, "type": "TASK", "confidence": 0.7899735569953918}]}, {"text": "Experimental results demonstrate that our model significantly improves translation accuracy over a state-of-the-art baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9565286636352539}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8852681517601013}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Effectiveness of different features in BLEU% (p < 0.05), with N =10 and L=100. \"Sim\"  denotes the rule similarity feature and \"Sen\" denotes rule sensitivity feature. \"Src\" and \"Trg\" means  utilizing source-side/target-side rule topic vectors to calculate similarity or sensitivity, respectively. The  \"Average\" setting is the averaged result of four datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.998461127281189}]}, {"text": " Table 4: 0  20  40  60  80  100  0", "labels": [], "entities": []}, {"text": " Table 3: Development and testing data used in the  experiments.", "labels": [], "entities": []}]}