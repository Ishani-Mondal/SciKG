{"title": [{"text": "How to make words with vectors: Phrase generation in distributional semantics", "labels": [], "entities": [{"text": "Phrase generation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.9771798849105835}]}], "abstractContent": [{"text": "We introduce the problem of generation in distributional semantics: Given a distri-butional vector representing some meaning , how can we generate the phrase that best expresses that meaning?", "labels": [], "entities": []}, {"text": "We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions.", "labels": [], "entities": []}, {"text": "We test this in a monolingual scenario (paraphrase generation) as well as in a cross-lingual setting (translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian).", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7843517363071442}]}], "introductionContent": [{"text": "Distributional methods for semantics approximate the meaning of linguistic expressions with vectors that summarize the contexts in which they occur in large samples of text.", "labels": [], "entities": []}, {"text": "This has been a very successful approach to lexical semantics, where semantic relatedness is assessed by comparing vectors.", "labels": [], "entities": []}, {"text": "Recently these methods have been extended to phrases and sentences by means of composition operations (see for an overview).", "labels": [], "entities": []}, {"text": "For example, given the vectors representing red and car, composition derives a vector that approximates the meaning of red car.", "labels": [], "entities": []}, {"text": "However, the link between language and meaning is, obviously, bidirectional: As message recipients we are exposed to a linguistic expression and we must compute its meaning (the synthesis problem).", "labels": [], "entities": []}, {"text": "As message producers we start from the meaning we want to communicate (a \"thought\") and we must encode it into a word sequence (the generation problem).", "labels": [], "entities": []}, {"text": "If distributional semantics is to be considered a proper semantic theory, then it must deal not only with synthesis (going from words to vectors), but also with generation (from vectors to words).", "labels": [], "entities": []}, {"text": "Besides these theoretical considerations, phrase generation from vectors has many useful applications.", "labels": [], "entities": [{"text": "phrase generation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.804673045873642}]}, {"text": "We can, for example, synthesize the vector representing the meaning of a phrase or sentence, and then generate alternative phrases or sentences from this vector to accomplish true paraphrase generation (as opposed to paraphrase detection or ranking of candidate paraphrases).", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.7297073006629944}, {"text": "paraphrase detection", "start_pos": 217, "end_pos": 237, "type": "TASK", "confidence": 0.8789166212081909}]}, {"text": "Generation can be even more useful when the source vector comes from another modality or language.", "labels": [], "entities": []}, {"text": "Recent work on grounding language in vision shows that it is possible to represent images and linguistic expressions in a common vectorbased semantic space (.", "labels": [], "entities": []}, {"text": "Given a vector representing an image, generation can be used to productively construct phrases or sentences that describe the image (as opposed to simply retrieving an existing description from a set of candidates).", "labels": [], "entities": []}, {"text": "Translation is another potential application of the generation framework: Given a semantic space shared between two or more languages, one can compose a word sequence in one language and generate translations in another, with the shared semantic vector space functioning as interlingua.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9474262595176697}]}, {"text": "Distributional semantics assumes a lexicon of atomic expressions (that, for simplicity, we take to be words), each associated to a vector.", "labels": [], "entities": [{"text": "Distributional semantics", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7996486127376556}]}, {"text": "Thus, at the single-word level, the problem of generation is solved by a trivial generation-by-synthesis approach: Given an arbitrary target vector, \"generate\" the corresponding word by searching through the lexicon for the word with the closest vector to the target.", "labels": [], "entities": []}, {"text": "This is however unfeasible for larger expressions: Given n vocabulary elements, this approach requires checking n k phrases of length k.", "labels": [], "entities": []}, {"text": "This becomes prohibitive already for relatively short phrases, as reasonably-sized vocabularies do not go below tens of thousands of words.", "labels": [], "entities": []}, {"text": "The search space for 3-word phrases in a 10K-word vocabulary is already in the order of trillions.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a more direct approach to phrase generation, inspired by the work in compositional distributional semantics.", "labels": [], "entities": [{"text": "phrase generation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.8408702313899994}]}, {"text": "In short, we revert the composition process and we propose a framework of data-induced, syntax-dependent functions that decompose a single vector into a vector sequence.", "labels": [], "entities": []}, {"text": "The generated vectors can then be efficiently matched against those in the lexicon or fed to the decomposition system again to produce longer phrases recursively.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our empirical part, we focus on noun phrase generation.", "labels": [], "entities": [{"text": "noun phrase generation", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7993898590405782}]}, {"text": "A noun phrase can be a single noun or a noun with one or more modifiers, where a modifier can bean adjective or a prepositional phrase.", "labels": [], "entities": []}, {"text": "A prepositional phrase is in turn composed of a preposition and a noun phrase.", "labels": [], "entities": []}, {"text": "We learn two composition (and corresponding decomposition) functions: one for modifier-noun phrases, trained on adjective-noun (AN) pairs, and a second one for prepositional phrases, trained on preposition-noun (PN) combinations.", "labels": [], "entities": []}, {"text": "For the rest of this section we describe the construction of the vector spaces and the (de)composition function learning procedure.", "labels": [], "entities": []}, {"text": "Construction of vector spaces We test two types of vector representations.", "labels": [], "entities": []}, {"text": "The cbow model introduced in learns vector representations using a neural network architecture by trying to predict a target word given the words surrounding it.", "labels": [], "entities": []}, {"text": "We use the word2vec software 3 to build vectors of size 300 and using a context window of 5 words to either side of the target.", "labels": [], "entities": []}, {"text": "We set the sub-sampling option to 1e-05 and estimate the probability of a target word with the negative sampling method, drawing 10 samples from the noise distribution (see for details).", "labels": [], "entities": []}, {"text": "We also implement a standard countbased bag-of-words distributional space which counts occurrences of a target word with other words within asymmetric window of size 5.", "labels": [], "entities": []}, {"text": "We build a 300Kx300K symmetric co-occurrence matrix using the topmost frequent words in our source corpus, apply positive PMI weighting and Singular Value Decomposition to reduce the space to 300 dimensions.", "labels": [], "entities": []}, {"text": "For both spaces, the vectors are finally normalized to unit length.", "labels": [], "entities": []}, {"text": "For both types of vectors we use 2.8 billion tokens as input (ukWaC + Wikipedia + BNC).", "labels": [], "entities": [{"text": "ukWaC + Wikipedia + BNC", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.8561482191085815}]}, {"text": "The Italian language vectors for the cross-lingual experiments of Section 6 were trained on 1.6 billion tokens from itWaC.", "labels": [], "entities": [{"text": "itWaC", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.9826409220695496}]}, {"text": "A word token is a wordform + POS-tag string.", "labels": [], "entities": []}, {"text": "We extract both word vectors and the observed phrase vectors which are required for the training procedures.", "labels": [], "entities": []}, {"text": "We sanitycheck the two spaces on MEN (), a 3,000 items word similarity data set.", "labels": [], "entities": []}, {"text": "cbow significantly outperforms count (0.80 vs. 0.72 Spearman correlations with human judgments).", "labels": [], "entities": [{"text": "count", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.988568902015686}]}, {"text": "count performance is consistent with previously reported results.", "labels": [], "entities": [{"text": "count", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9416627883911133}]}, {"text": "(De)composition function training The training data sets consist of the 50K most frequent u, v, p tuples for each phrase type, for example, red, car, red.car or in, car, in.car.", "labels": [], "entities": []}, {"text": "We concatenate u and v vectors to obtain the [U ; V ] matrix and we use the observed p vectors (e.g., the corpus vector of the red.car bigram) to obtain the phrase matrix P . We use these data sets to solve the least squares regression problems in eqs. and, obtaining estimates of the composition and decomposition matrices, respectively.", "labels": [], "entities": []}, {"text": "For the decomposition function in eq., we replace the observed phrase vectors with those composed with f comp R ( u, v), where f comp R is the previously estimated composition function for relation R.", "labels": [], "entities": []}, {"text": "Composition function performance Since the experiments below also use composed vectors as input to the generation process, it is important to provide independent evidence that the composition model is of high quality.", "labels": [], "entities": []}, {"text": "This is indeed the case: We tested our composition approach on the task of retrieving observed AN and PN vectors, based on their composed vectors (similarly to, we want to retrieve the observed red.car vector using f comp AN (red, car)).", "labels": [], "entities": []}, {"text": "We obtain excellent results, with minimum accuracy of 0.23 (chance level <0.0001).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9896835088729858}, {"text": "chance level", "start_pos": 60, "end_pos": 72, "type": "METRIC", "confidence": 0.9701782166957855}]}, {"text": "We also test on the AN-N paraphrasing test set used in  tors we obtain a median rank that is considerably higher than that of the methods they test.", "labels": [], "entities": [{"text": "AN-N paraphrasing test set", "start_pos": 20, "end_pos": 46, "type": "DATASET", "confidence": 0.6936809867620468}]}], "tableCaptions": [{"text": " Table 6: Accuracy of En\u2192It and It\u2192En phrase  translation: phrases are composed in source lan- guage and decomposed in target language. Train- ing on composed phrase representations (eq.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9728559255599976}, {"text": "phrase  translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6950336247682571}]}, {"text": " Table 7: En\u2192It translation examples (back-translations of generated phrases in parenthesis).", "labels": [], "entities": []}, {"text": " Table 9: AN-AN translation accuracy (both A and  N correct) when imposing a confidence threshold  (random: 1/20K 2 ).", "labels": [], "entities": [{"text": "AN-AN", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9744324088096619}, {"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.5479359030723572}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.4992384612560272}]}]}