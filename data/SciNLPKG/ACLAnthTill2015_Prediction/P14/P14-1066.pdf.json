{"title": [{"text": "Learning Continuous Phrase Representations for Translation Modeling", "labels": [], "entities": [{"text": "Translation Modeling", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9596272706985474}]}], "abstractContent": [{"text": "This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations.", "labels": [], "entities": [{"text": "estimating phrase translation probabilities", "start_pos": 43, "end_pos": 86, "type": "TASK", "confidence": 0.8087947219610214}]}, {"text": "A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space, where their translation score is computed by the distance between the pair in this new space.", "labels": [], "entities": []}, {"text": "The projection is performed by a neural network whose weights are learned on parallel training data.", "labels": [], "entities": []}, {"text": "Experimental evaluation has been performed on two WMT translation tasks.", "labels": [], "entities": [{"text": "WMT translation tasks", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.9626156091690063}]}, {"text": "Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.5703254789113998}, {"text": "WMT 2012 French-English data", "start_pos": 126, "end_pos": 154, "type": "DATASET", "confidence": 0.9484629184007645}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9990033507347107}]}], "introductionContent": [{"text": "The phrase translation model, also known as the phrase table, is one of the core components of phrase-based statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.853859931230545}, {"text": "phrase-based statistical machine translation (SMT)", "start_pos": 95, "end_pos": 145, "type": "TASK", "confidence": 0.7224459861006055}]}, {"text": "The most common method of constructing the phrase table takes a two-phase approach ().", "labels": [], "entities": []}, {"text": "First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data.", "labels": [], "entities": []}, {"text": "The second phase, which is the focus of this paper, is parameter estimation where each phrase pair is assigned with some scores that are estimated based on counting these phrases or their words using the same word-aligned training data.", "labels": [], "entities": []}, {"text": "Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations.", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7223658263683319}]}, {"text": "However, longer phrases occur less often in training data, leading to a severe data sparseness problem in parameter estimation.", "labels": [], "entities": []}, {"text": "There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model (e.g.,.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8251119256019592}]}, {"text": "This paper revisits the problem of scoring a phrase translation pair by developing a Continuous-space Phrase Translation Model (CPTM).", "labels": [], "entities": [{"text": "scoring a phrase translation pair", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.6689481198787689}, {"text": "Continuous-space Phrase Translation", "start_pos": 85, "end_pos": 120, "type": "TASK", "confidence": 0.7012834548950195}]}, {"text": "The translation score of a phrase pair in this model is computed as follows.", "labels": [], "entities": []}, {"text": "First, we represent each phrase as a bag-of-words vector, called word vector henceforth.", "labels": [], "entities": []}, {"text": "We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional space that is language independent.", "labels": [], "entities": []}, {"text": "The projection is performed by a multi-layer neural network.", "labels": [], "entities": []}, {"text": "The projected feature vector forms the continuous representation of a phrase.", "labels": [], "entities": []}, {"text": "Finally, the translation score of a source-target phrase pair is computed by the distance between their feature vectors.", "labels": [], "entities": []}, {"text": "The main motivation behind the CPTM is to alleviate the data sparseness problem associated with the traditional counting-based methods by grouping phrases with a similar meaning across different languages.", "labels": [], "entities": []}, {"text": "This style of grouping is made possible because of the distributed nature of the continuous-space representations for phrases.", "labels": [], "entities": []}, {"text": "No such sharing was possible in the original symbolic space for representing words or phrases.", "labels": [], "entities": []}, {"text": "In this model, semantically or grammatically related phrases, in both the source and the target languages, would tend to have similar (close) feature vectors in the continuous space, guided by the training objective.", "labels": [], "entities": []}, {"text": "Since the translation score is a smooth function of these feature vectors, a small change in the features should only lead to a small change in the translation score.", "labels": [], "entities": []}, {"text": "The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9954723715782166}]}, {"text": "Motivated by recent studies on continuous-space language models (e.g.,), we use a neural network to project a word vector to a feature vector.", "labels": [], "entities": []}, {"text": "Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, fora given source phrase.", "labels": [], "entities": []}, {"text": "However, there is no training data with explicit annotation on the quality of phrase translations.", "labels": [], "entities": [{"text": "phrase translations", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.724904015660286}]}, {"text": "The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6543807983398438}]}, {"text": "The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured by BLEU, which contain the phrase pair.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7252467274665833}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9986957907676697}]}, {"text": "In order to overcome this challenge and let the BLEU metric guide the projection learning, we propose anew method to learn the parameters of a neural network.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9975797533988953}]}, {"text": "This new method, via the choice of an appropriate objective function in training, automatically forces the feature vector of a source phrase to be closer to the feature vectors of its candidate translations.", "labels": [], "entities": []}, {"text": "As a result, the BLEU score is improved when these translations are selected by an SMT decoder to produce final, sentence-level translations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9819321036338806}, {"text": "SMT decoder", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.8781020939350128}]}, {"text": "The new learning method makes use of the L-BFGS algorithm and the expected BLEU as the objective function defined on N-best lists.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9983012080192566}]}, {"text": "To the best of our knowledge, the CPTM proposed in this paper is the first continuous-space phrase translation model that makes use of joint representations of a phrase in the source language and its translation in the target language (to be detailed in Section 4) and that is shown to lead to significant improvement over a standard phrasebased SMT system (to be detailed in Section 6).", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7271706610918045}, {"text": "SMT", "start_pos": 346, "end_pos": 349, "type": "TASK", "confidence": 0.8028927445411682}]}, {"text": "Like the traditional phrase translation model, the translation score of each bilingual phrase pair is modeled explicitly in our model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8446798622608185}]}, {"text": "However, instead of estimating the phrase translation score on aligned parallel data, our model intends to capture the grammatical and semantic similarity between a source phrase and its paired target phrase by projecting them into a common, continuous space that is language independent.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7229662537574768}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews previous work.", "labels": [], "entities": []}, {"text": "Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.70737224817276}, {"text": "CPTM", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8880587220191956}]}, {"text": "Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6.", "labels": [], "entities": []}, {"text": "Finally, Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section evaluates the CPTM presented on two translation tasks using WMT data sets.", "labels": [], "entities": [{"text": "WMT data sets", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9486629962921143}]}, {"text": "We first describe the data sets and baseline setup.", "labels": [], "entities": []}, {"text": "Then we present experiments where we compare different versions of the CPTM and previous models.", "labels": [], "entities": [{"text": "CPTM", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8847350478172302}]}, {"text": "We experiment with an in-house phrase-based system similar to, where the translation candidates are scored by a set of common features including maximum likelihood estimates of source given target phrase mappings \u00ed \u00b5\u00ed\u00b1\u0083 \u00ed \u00b5\u00ed\u00b1\u0080\u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b0\u00b8(\u00b5\u00ed\u00b0\u00b8(\u00ed \u00b5\u00ed\u00b1\u0092|\u00ed \u00b5\u00ed\u00b1\u0093) and vice versa \u00ed \u00b5\u00ed\u00b1\u0083 \u00ed \u00b5\u00ed\u00b1\u0080\u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b0\u00b8(\u00b5\u00ed\u00b0\u00b8(\u00ed \u00b5\u00ed\u00b1\u0093|\u00ed \u00b5\u00ed\u00b1\u0092), as well as lexical weighting estimates \u00ed \u00b5\u00ed\u00b1\u0083 \u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b1\u008a (\u00ed \u00b5\u00ed\u00b1\u0092|\u00ed \u00b5\u00ed\u00b1\u0093) and \u00ed \u00b5\u00ed\u00b1\u0083 \u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b1\u008a (\u00ed \u00b5\u00ed\u00b1\u0093|\u00ed \u00b5\u00ed\u00b1\u0092), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature.", "labels": [], "entities": []}, {"text": "The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below.", "labels": [], "entities": []}, {"text": "Log-linear weights are estimated with the MERT algorithm).", "labels": [], "entities": [{"text": "MERT", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8158440589904785}]}, {"text": "We test our models on two different data sets.", "labels": [], "entities": []}, {"text": "First, we train an English to French system based on the data of WMT 2006 shared task).", "labels": [], "entities": [{"text": "WMT 2006 shared task", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.9392740726470947}]}, {"text": "The parallel corpus includes 688K sentence pairs of parliamentary proceedings for training.", "labels": [], "entities": []}, {"text": "The development set contains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task.", "labels": [], "entities": [{"text": "WMT 2006 shared task", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.6973132640123367}]}, {"text": "Second, we experiment with a French to English system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign.", "labels": [], "entities": [{"text": "WMT 2012 campaign", "start_pos": 146, "end_pos": 163, "type": "DATASET", "confidence": 0.8006743987401327}]}, {"text": "The majority of the training data set is parliamentary proceedings except for 5M words which are newswire.", "labels": [], "entities": []}, {"text": "We use the 2009 newswire data set, comprising 2525 sentences, as the development set.", "labels": [], "entities": [{"text": "2009 newswire data set", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.8260643929243088}]}, {"text": "We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set, containing 2034 to 3003 sentences.", "labels": [], "entities": [{"text": "newswire domain test sets", "start_pos": 20, "end_pos": 45, "type": "DATASET", "confidence": 0.8324575126171112}, {"text": "2010 system combination test set", "start_pos": 86, "end_pos": 118, "type": "DATASET", "confidence": 0.6730717420578003}]}, {"text": "In this study we perform a detailed empirical comparison using the WMT 2006 data set, and verify our best models and results using the larger WMT 2012 data set.", "labels": [], "entities": [{"text": "WMT 2006 data set", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9881370812654495}, {"text": "WMT 2012 data set", "start_pos": 142, "end_pos": 159, "type": "DATASET", "confidence": 0.983620822429657}]}, {"text": "The metric used for evaluation is case insensitive BLEU score (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9675584733486176}]}, {"text": "We also perform a significance test using the Wilcoxon signed rank test.", "labels": [], "entities": [{"text": "significance", "start_pos": 18, "end_pos": 30, "type": "METRIC", "confidence": 0.8986196517944336}, {"text": "Wilcoxon signed rank test", "start_pos": 46, "end_pos": 71, "type": "METRIC", "confidence": 0.5654846131801605}]}, {"text": "Differences are considered statistically significant when the p-value is less than 0.05.", "labels": [], "entities": []}, {"text": "shows the results measured in BLEU evaluated on the WMT 2006 data set, where Row 1 is the baseline system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9961280226707458}, {"text": "WMT 2006 data set", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9802026599645615}]}, {"text": "Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM.", "labels": [], "entities": [{"text": "CPTM", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9628952741622925}]}, {"text": "Rows 5 to 7 present the results of previous models.", "labels": [], "entities": []}, {"text": "Row 8 is our best system.", "labels": [], "entities": []}, {"text": "shows the main results on the WMT 2012 data set.", "labels": [], "entities": [{"text": "WMT 2012 data set", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9580545872449875}]}, {"text": "CPTM is the model described in Sections 4.", "labels": [], "entities": [{"text": "CPTM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8994557857513428}]}, {"text": "As illustrated in, the number of the nodes in the input layer is the vocabulary size \u00ed \u00b5\u00ed\u00b1\u0091.", "labels": [], "entities": [{"text": "vocabulary size \u00ed \u00b5\u00ed", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.7597760856151581}]}, {"text": "Both the hidden layer and the output layer have 100 nodes . That is, \u00ed \u00b5\u00ed\u00b0\u0096 1 is a \u00ed \u00b5\u00ed\u00b1\u0091 \u00d7 100 matrix and \u00ed \u00b5\u00ed\u00b0\u0096 2 a 100 \u00d7 100 matrix.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b0\u0096 1", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.8310143202543259}]}, {"text": "The result shows that CPTM leads to a substantial improvement over the baseline system with a statistically significant margin of 1.0 BLEU points as in.", "labels": [], "entities": [{"text": "CPTM", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.7350025773048401}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9981585144996643}]}], "tableCaptions": [{"text": " Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts \u03b1  and \u03b2 indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981562495231628}, {"text": "WMT 2006 data set", "start_pos": 106, "end_pos": 123, "type": "DATASET", "confidence": 0.9779263585805893}, {"text": "CPTM", "start_pos": 227, "end_pos": 231, "type": "DATASET", "confidence": 0.9081854820251465}]}, {"text": " Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in", "labels": [], "entities": [{"text": "CPTM", "start_pos": 352, "end_pos": 356, "type": "DATASET", "confidence": 0.9141860604286194}, {"text": "WMT 2006 data set", "start_pos": 456, "end_pos": 473, "type": "DATASET", "confidence": 0.9886969029903412}]}, {"text": " Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts \u03b1 and \u03b2 indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982603192329407}, {"text": "WMT 2012 data set", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.9756899327039719}, {"text": "MRFp", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.5404629707336426}]}]}