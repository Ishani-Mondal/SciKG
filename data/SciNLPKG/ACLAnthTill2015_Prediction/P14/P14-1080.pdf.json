{"title": [{"text": "Enhancing Grammatical Cohesion: Generating Transitional Expressions for SMT", "labels": [], "entities": [{"text": "Enhancing Grammatical Cohesion", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8059410055478414}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9874000549316406}]}], "abstractContent": [{"text": "Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization, which together help improve readability of a text.", "labels": [], "entities": []}, {"text": "However, inmost current statistical machine translation (SMT) systems, the outputs of compound-complex sentences still lack proper transitional expressions.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.7764550348122915}]}, {"text": "As a result, the translations are often hard to read and understand.", "labels": [], "entities": []}, {"text": "To address this issue, we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure (CSS).", "labels": [], "entities": []}, {"text": "Our models include a CSS-based translation model, which generates new CSS-based translation rules, and a generative transfer model, which encourages producing transitional expressions during decoding.", "labels": [], "entities": [{"text": "CSS-based translation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7012147009372711}, {"text": "generative transfer", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8664647340774536}]}, {"text": "The two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6583275943994522}]}, {"text": "The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth.", "labels": [], "entities": []}], "introductionContent": [{"text": "During the last decade, great progress has been made on statistical machine translation (SMT) models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.818859244386355}]}, {"text": "However, these translations still suffer from poor readability, especially translations of compound-complex sentences.", "labels": [], "entities": []}, {"text": "One of the main reasons maybe that most existing models concentrate more on producing well-translated local sentence fragments, but largely ignore global cohesion between the fragments.", "labels": [], "entities": []}, {"text": "Generally, cohesion, including lexical and grammatical cohesion, contributes much to the understandability and smoothness of a text.", "labels": [], "entities": []}, {"text": "Recently, researchers have begun addressing the lexical cohesion of SMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9539072513580322}]}, {"text": "These efforts focus mainly on the cooccurrence of lexical items in a similar environment.", "labels": [], "entities": []}, {"text": "Grammatical cohesion 1) in SMT has been little mentioned in previous work.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.968848705291748}]}, {"text": "Translations without grammatical cohesion is hard to read, mostly due to loss of cohesive and transitional expressions between two sentence fragments.", "labels": [], "entities": []}, {"text": "Thus, generating transitional expressions is necessary for achieving grammatical cohesion.", "labels": [], "entities": []}, {"text": "However, it is not easy to produce such transitional expressions in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9823019504547119}]}, {"text": "As an example, consider the Chinese-to-English translation in. already , more show environment protection of urgent .", "labels": [], "entities": []}], "datasetContent": [{"text": "To obtain the CSSs of Chinese sentences, we use the Chinese parser proposed in ().", "labels": [], "entities": []}, {"text": "Their parser first segments the compoundcomplex sentence into a series of elementary units, and then builds structure of the hierarchical relationships among these elementary units.", "labels": [], "entities": []}, {"text": "Their parser was reported to achieve an F-score for elementary unit segmentation of approximately 0.89.", "labels": [], "entities": [{"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9994656443595886}, {"text": "elementary unit segmentation", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.5487123827139536}]}, {"text": "The progressive, causal, and condition terms of functional relationships can be recognized with precisions of 0.86, 0.8, and 0.75, respectively, while others, such as purpose, parallel, and flowing, achieve only 0.5, 0.59 and 0.62, respectively.", "labels": [], "entities": [{"text": "precisions", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9904006123542786}]}, {"text": "The translation experiments have been conducted in the Chinese-to-English direction.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9715585112571716}]}, {"text": "The bilingual training data for translation model and CSS-based transfer model is FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9741453528404236}, {"text": "CSS-based transfer", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8257034122943878}, {"text": "FBIS corpus", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.8663261532783508}]}, {"text": "We obtain the word alignment with the grow-diag-final-and strategy with GIZA++.", "labels": [], "entities": []}, {"text": "Before training the CSS-based transfer model, the alignment for transfer model is modified by our dynamic cleaning method.", "labels": [], "entities": []}, {"text": "During the cleaning process, the maximum size of hypothesis is limited to 5.", "labels": [], "entities": []}, {"text": "A 5-gram language model is trained with SRILM 5 on the combination of the Xinhua portion of the English Gigaword corpus combined with the English part of FBIS.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.7175642053286234}]}, {"text": "For tuning and testing, we use NIST03 evaluation data as the development set.", "labels": [], "entities": [{"text": "NIST03 evaluation data", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.9089576999346415}]}, {"text": "NIST04/05/06, CWMT08-Development 6 and CWMT08-Evaluation data are used for testing under the measure metric of BLEU-4 ( with the shortest length penalty.", "labels": [], "entities": [{"text": "NIST04/05/06", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9787015199661255}, {"text": "CWMT08-Development 6", "start_pos": 14, "end_pos": 34, "type": "DATASET", "confidence": 0.8324866592884064}, {"text": "CWMT08-Evaluation data", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.897987425327301}, {"text": "BLEU-4", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9968345761299133}]}, {"text": "shows how the CSS is distributed in all testing sets.", "labels": [], "entities": []}, {"text": "According to the statistics in Table 1, we see that CSS is really widely distributed in the NIST and CWMT corpora, which implies that the translation quality may benefit substantially from the CSS information, if it is well considered in SMT.", "labels": [], "entities": [{"text": "NIST and CWMT corpora", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.7606806755065918}, {"text": "SMT", "start_pos": 238, "end_pos": 241, "type": "TASK", "confidence": 0.9786064624786377}]}, {"text": "To further evaluate the effectiveness of the proposed models, we also conducted an experiment on a larger set of bilingual training data from the LDC corpus 7 for translation model and transfer model.", "labels": [], "entities": [{"text": "LDC corpus 7", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.9115585883458456}, {"text": "translation", "start_pos": 163, "end_pos": 174, "type": "TASK", "confidence": 0.8788782358169556}]}, {"text": "The training corpus contains 2.1M sentence pairs with approximately 27.7M Chinese words and 31.9M English words.", "labels": [], "entities": []}, {"text": "All the other settings were the same as the SMT experiments of sub-section 4.3.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.984180212020874}]}, {"text": "The final BLEU scores on NIST05 and NIST06 are given in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982428550720215}, {"text": "NIST05", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9880436062812805}, {"text": "NIST06", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.9739375710487366}]}, {"text": "The results in further verify the effectiveness of our proposed models.", "labels": [], "entities": []}, {"text": "The best performance with bold marking scored as high as 0.83 and 0.64 BLEU points, respectively over the 7 LDC category number: LDC2000T50, DC2002E18, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.998712420463562}]}, {"text": "baseline system on NIST05 and NIST06 evaluation data.", "labels": [], "entities": [{"text": "NIST05", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.9718834757804871}, {"text": "NIST06 evaluation data", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.936296264330546}]}], "tableCaptions": [{"text": " Table 3. BLEU scores of the testing sets with different integrating strategies", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992269277572632}]}, {"text": " Table 5. We observe that compared to  the baseline, our approach has obvious ad- vantages on translating the implicit relations, due  to generating translational expressions on target  side. Moreover, with the transitional expressions,  cohesion of the entire translation improves. No- tably, the transitional expressions in this work  like \"including, there are, the core of which\" are  not linguistic conjunctions. We would like to call  them \"generalized\" conjunctions, because they  tie semantic fragments together, analogously to  linguistic conjunctions.", "labels": [], "entities": []}, {"text": " Table 4. BLEU scores on the large-scale training  data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988884329795837}]}]}