{"title": [], "abstractContent": [{"text": "1 Tutorial Overview Statistical natural language processing relies on probabilistic models of linguistic structure.", "labels": [], "entities": [{"text": "Statistical natural language processing", "start_pos": 20, "end_pos": 59, "type": "TASK", "confidence": 0.6189457848668098}]}, {"text": "More complex models can help capture our intuitions about language, by adding linguistically meaningful interactions and latent variables.", "labels": [], "entities": []}, {"text": "However, inference and learning in the models we want often poses a serious computational challenge.", "labels": [], "entities": []}, {"text": "Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods.", "labels": [], "entities": [{"text": "Belief propagation (BP)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.895856523513794}]}, {"text": "These approaches can handle joint models of interacting components , are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing , phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.8593586087226868}, {"text": "CCG parsing", "start_pos": 239, "end_pos": 250, "type": "TASK", "confidence": 0.7888565063476562}, {"text": "phrase extraction", "start_pos": 253, "end_pos": 270, "type": "TASK", "confidence": 0.8508889377117157}, {"text": "semantic role labeling", "start_pos": 272, "end_pos": 294, "type": "TASK", "confidence": 0.6475910743077596}, {"text": "information extraction", "start_pos": 300, "end_pos": 322, "type": "TASK", "confidence": 0.8203368782997131}]}, {"text": "This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks.", "labels": [], "entities": [{"text": "BP", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9492377042770386}]}, {"text": "Our goal is to elucidate how these approaches can easily be applied to new problems.", "labels": [], "entities": []}, {"text": "We also cover the theory underlying them.", "labels": [], "entities": []}, {"text": "Our target audience is researchers inhuman language technologies; we do not assume familarity with BP.", "labels": [], "entities": [{"text": "BP", "start_pos": 99, "end_pos": 101, "type": "DATASET", "confidence": 0.7340540885925293}]}, {"text": "In the first three sections, we discuss applications of BP to NLP problems, the basics of mod-eling with factor graphs and message passing, and the theoretical underpinnings of \"what BP is do-ing\" and how it relates to other variational inference techniques.", "labels": [], "entities": [{"text": "message passing", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.7265974879264832}]}, {"text": "In the second three sections, we cover key extensions to the standard BP algorithm to enable modeling of linguistic structure, efficient inference, and approximation-aware training.", "labels": [], "entities": []}, {"text": "We survey a variety of software tools and introduce anew software framework that incorporates many of the modern approaches covered in this tutorial.", "labels": [], "entities": []}, {"text": "Applications [15 min., Eisner] \u2022 Intro: Modeling with factor graphs \u2022 Morphological paradigms \u2022 Dependency and constituency parsing \u2022 Alignment; Phrase extraction \u2022 Relation extraction; Semantic role labeling \u2022 Targeted sentiment \u2022 Joint models for NLP 2.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.7074690759181976}, {"text": "Phrase extraction", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.8168239891529083}, {"text": "Relation extraction", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.8522034883499146}, {"text": "Semantic role labeling", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.7358430624008179}, {"text": "Targeted sentiment", "start_pos": 211, "end_pos": 229, "type": "TASK", "confidence": 0.75581493973732}]}, {"text": "Belief Propagation Basics [40 min., Eisner] \u2022 Messages and beliefs \u2022 Sum-product, max-product, and determin-istic annealing \u2022 Relation to forward-backward and inside-outside \u2022 Acyclic vs. loopy graphs \u2022 Synchronous vs. asynchronous propagation 3.", "labels": [], "entities": [{"text": "Synchronous vs. asynchronous propagation", "start_pos": 203, "end_pos": 243, "type": "TASK", "confidence": 0.646345779299736}]}, {"text": "Theory [25 min., Gormley] \u2022 From arc consistency to BP \u2022 From Gibbs sampling to particle BP to BP \u2022 Other message-passing algorithms \u2022 Bethe free energy \u2022 Connection to PFCGs and FSMs 4.", "labels": [], "entities": [{"text": "BP", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9849076271057129}]}, {"text": "Incorporating Structure into Factors and Variables [30 min., Gormley] \u2022 Embedding dynamic programs (e.g. inside-outside) within factors \u2022 String-valued and tree-valued variables 5.", "labels": [], "entities": [{"text": "Gormley", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.8681198954582214}]}, {"text": "Message approximation and scheduling [20 min., Eisner] \u2022 Pruning messages \u2022 Variational approximations \u2022 Residual BP and new variants 6.", "labels": [], "entities": [{"text": "Message approximation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8312912583351135}, {"text": "Residual BP", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.7115627527236938}]}, {"text": "Approximation-aware Training [30 min., Gorm-ley] \u2022 Empirical risk minimization under approximations (ERMA) \u2022 BP as a computational expression graph \u2022 Automatic differentiation (AD) 7.", "labels": [], "entities": [{"text": "Approximation-aware", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9525405168533325}, {"text": "Gorm-ley", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9543156027793884}, {"text": "Empirical risk minimization", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7030877073605856}, {"text": "Automatic differentiation (AD)", "start_pos": 150, "end_pos": 180, "type": "TASK", "confidence": 0.7326703906059265}]}, {"text": "Software [10 min., Gormley] 9", "labels": [], "entities": [{"text": "Gormley", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9365041851997375}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}