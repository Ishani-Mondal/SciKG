{"title": [{"text": "Unsupervised Feature Learning for Visual Sign Language Identification", "labels": [], "entities": [{"text": "Visual Sign Language Identification", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.6311540007591248}]}], "abstractContent": [{"text": "Prior research on language identification fo-cused primarily on text and speech.", "labels": [], "entities": [{"text": "language identification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7101384103298187}]}, {"text": "In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples.", "labels": [], "entities": []}, {"text": "The method is trained on unlabelled video data (un-supervised feature learning) and using these features, it is trained to discriminate between six sign languages (supervised learning).", "labels": [], "entities": []}, {"text": "We ran experiments on short video samples involving 30 signers (about 6 hours in total).", "labels": [], "entities": []}, {"text": "Using leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9993458390235901}]}, {"text": "Given that sign languages are under-resourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification .", "labels": [], "entities": [{"text": "sign language identification", "start_pos": 160, "end_pos": 188, "type": "TASK", "confidence": 0.6940341393152872}]}], "introductionContent": [{"text": "The task of automatic language identification is to quickly identify the identity of the language given utterances.", "labels": [], "entities": [{"text": "automatic language identification", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.6390011310577393}]}, {"text": "Performing this task is key in applications involving multiple languages such as machine translation and information retrieval (e.g. metadata creation for large audiovisual archives).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7779732048511505}, {"text": "information retrieval", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.7300349026918411}, {"text": "metadata creation", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.7432080209255219}]}, {"text": "Prior research on language identification is heavily biased towards written and spoken languages).", "labels": [], "entities": [{"text": "language identification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7330674231052399}]}, {"text": "While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages.", "labels": [], "entities": [{"text": "language identification", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.7145305424928665}]}, {"text": "Written languages can be identified to about 99% accuracy using Markov models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9975916147232056}]}, {"text": "This accuracy is so high that current research has shifted to related more challenging problems: language variety identification (, native language identification () and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9992251396179199}, {"text": "language variety identification", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.6730590363343557}, {"text": "native language identification", "start_pos": 132, "end_pos": 162, "type": "TASK", "confidence": 0.6327325403690338}]}, {"text": "Spoken languages can be identified to accuracies that range from 79-98% using different models).", "labels": [], "entities": []}, {"text": "The methods used in spoken language identification have also been extended to a related class of problems: native accent identification ( and foreign accent identification (.", "labels": [], "entities": [{"text": "spoken language identification", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6664603253205618}, {"text": "native accent identification", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.6835572818915049}, {"text": "foreign accent identification", "start_pos": 142, "end_pos": 171, "type": "TASK", "confidence": 0.711481640736262}]}, {"text": "While some work exists on sign language recognition 1, very little research exists on sign language identification except for the work by, where it is shown that sign language identification can be done using linguistically motivated features.", "labels": [], "entities": [{"text": "sign language recognition 1", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7168079167604446}, {"text": "sign language identification", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6574924190839132}, {"text": "sign language identification", "start_pos": 162, "end_pos": 190, "type": "TASK", "confidence": 0.6635860502719879}]}, {"text": "Accuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9815560579299927}]}, {"text": "This paper has two goals.", "labels": [], "entities": []}, {"text": "First, to present a method to identify sign languages using features learned by unsupervised techniques).", "labels": [], "entities": []}, {"text": "Second, to evaluate the method on six sign languages under different conditions.", "labels": [], "entities": []}, {"text": "Our contributions: a) show that unsupervised feature learning techniques, currently popular in many pattern recognition problems, also work for visual sign languages.", "labels": [], "entities": [{"text": "pattern recognition problems", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.8108835021654764}]}, {"text": "More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification.", "labels": [], "entities": [{"text": "sign language identification", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.6482256253560384}]}, {"text": "b) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental data consist of videos of 30 signers equally divided between six sign languages: British sign language (BSL), Danish (DSL), French Belgian (FBSL), Flemish (FSL), Greek (GSL), and Dutch (NGT).", "labels": [], "entities": []}, {"text": "The data for the unsupervised feature learning comes from half of the BSL and GSL videos in the Dicta-Sign corpus 2 . Part of the other half, involving 5 signers, is used along with the other sign language videos for learning and testing classifiers.", "labels": [], "entities": [{"text": "BSL and GSL videos in the Dicta-Sign corpus 2", "start_pos": 70, "end_pos": 115, "type": "DATASET", "confidence": 0.7582603626781039}]}, {"text": "For the unsupervised feature learning, two types of patches are created: 2D dimensions and 3D.", "labels": [], "entities": []}, {"text": "Each type consists of randomly selected 100,000 patches and involves 16 different signers.", "labels": [], "entities": []}, {"text": "For the supervised learning, 200 videos (consisting of 1 through 4 frames taken at a step of 2) are randomly sampled per sign language per signer (for a total of 6,000 samples).", "labels": [], "entities": []}, {"text": "We evaluate our system in terms of average accuracies.", "labels": [], "entities": []}, {"text": "We train and test our system in leave-onesigner-out cross-validation, where videos from four signers are used for training and videos of the remaining signer are used for testing.", "labels": [], "entities": []}, {"text": "Classification algorithms are used with their default settings and the classification strategy is one-vs.-rest.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: 2D filters (15  *  15): Leave-one-signer-out  cross-validation average accuracies.", "labels": [], "entities": []}, {"text": " Table 2: 3D filters (15  *  15  *  2): Leave-one-signer- out cross-validation average accuracies.", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix -confusions averaged  over all settings for K-means and sparse autoen- coder with 2D and 3D filters (i.e. for all # of  frames, all filter sizes and all classifiers).", "labels": [], "entities": []}]}