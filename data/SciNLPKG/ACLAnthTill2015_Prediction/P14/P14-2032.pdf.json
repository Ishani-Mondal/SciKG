{"title": [{"text": "Two Knives Cut Better Than One: Chinese Word Segmentation with Dual Decomposition", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.5766106645266215}]}], "abstractContent": [{"text": "There are two dominant approaches to Chinese word segmentation: word-based and character-based models, each with respective strengths.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6351556281248728}]}, {"text": "Prior work has shown that gains in segmentation performance can be achieved from combining these two types of models; however, past efforts have not provided a practical technique to allow mainstream adoption.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.977719783782959}]}, {"text": "We propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference.", "labels": [], "entities": []}, {"text": "Our method is simple and easy to implement.", "labels": [], "entities": []}, {"text": "Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets.", "labels": [], "entities": [{"text": "SIGHAN 2003 and 2005 evaluation datasets", "start_pos": 15, "end_pos": 55, "type": "DATASET", "confidence": 0.8336101075013479}]}], "introductionContent": [{"text": "Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.7629206875960032}, {"text": "Chinese natural language processing", "start_pos": 155, "end_pos": 190, "type": "TASK", "confidence": 0.6367743462324142}]}, {"text": "As demonstrated by), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.813802570104599}, {"text": "POS tagging", "start_pos": 144, "end_pos": 155, "type": "TASK", "confidence": 0.8805398046970367}]}, {"text": "State-of-the-art performance in CWS is high, with F-scores in the upper 90s.", "labels": [], "entities": [{"text": "CWS", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8369932174682617}, {"text": "F-scores", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9983237385749817}]}, {"text": "Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word-or dictionary-based approaches.", "labels": [], "entities": []}, {"text": "Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as \u624d\u80fd (\"talent\") and \u624d / \u80fd (\"just able\") (.", "labels": [], "entities": []}, {"text": "There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters;, and word-based, where the units are full words based on some dictionary or training lexicon.", "labels": [], "entities": []}, {"text": "Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans.", "labels": [], "entities": []}, {"text": "Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive.", "labels": [], "entities": []}, {"text": "In this work, we propose a simple and principled joint decoding method for combining character-based and word-based segmenters based on dual decomposition.", "labels": [], "entities": []}, {"text": "This method has strong optimality guarantees and works very well empirically.", "labels": [], "entities": []}, {"text": "It is easy to implement and does not require retraining of existing character-and wordbased segmenters.", "labels": [], "entities": []}, {"text": "Perhaps most importantly, this work presents a much more practical and usable form of classifier combination in the CWS context than existing methods offer.", "labels": [], "entities": [{"text": "classifier combination", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.8590607345104218}]}, {"text": "Experimental results on standard SIGHAN 2003 and 2005 bake-off evaluations show that our model outperforms the character and word baselines by a significant margin.", "labels": [], "entities": []}, {"text": "In particular, out approach improves OOV recall rates and segmentation consistency, and gives the best reported results to date on 6 out of 7 datasets.", "labels": [], "entities": [{"text": "OOV", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.6165593266487122}, {"text": "recall rates", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.8946492671966553}, {"text": "consistency", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.7144377827644348}]}], "datasetContent": [{"text": "We conduct experiments on the) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm.", "labels": [], "entities": []}, {"text": "We use the publicly available Stanford CRF segmenter () 2 as our character-based baseline model, and reproduce the perceptron-based segmenter from as our word-based baseline model.", "labels": [], "entities": []}, {"text": "We adopted the development setting from (, and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset.", "labels": [], "entities": []}, {"text": "The optimized hyper-parameters used are: 2 regularization parameter \u03bb in CRF is set to 3; the perceptron is trained for 10 iterations with beam size 200; dual decomposition is run to max iteration of 100 (T in Algo. 1) with step size 0.1 (\u03b1 tin Algo. 1).", "labels": [], "entities": []}, {"text": "Beyond standard precision (P), recall (R) and F 1 scores, we also evaluate segmentation consistency as proposed by , who have shown that increased segmentation consistency is correlated with better machine translation performance.", "labels": [], "entities": [{"text": "standard precision (P)", "start_pos": 7, "end_pos": 29, "type": "METRIC", "confidence": 0.8546053051948548}, {"text": "recall (R)", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9612811654806137}, {"text": "F 1", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.8746445775032043}, {"text": "segmentation consistency", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.7405436038970947}, {"text": "machine translation", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.7242015302181244}]}, {"text": "The consistency measure calculates the entropy of segmentation variationsthe lower the score the better.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.941363513469696}]}, {"text": "We also report out-of-vocabulary recall (R oov ) as an estimation of the model's generalizability to previously unseen words.", "labels": [], "entities": [{"text": "recall (R oov )", "start_pos": 33, "end_pos": 48, "type": "METRIC", "confidence": 0.9329194068908692}]}, {"text": "shows our empirical results on SIGHAN 2005 dataset.", "labels": [], "entities": [{"text": "SIGHAN 2005 dataset", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.887919286886851}]}, {"text": "Our dual decomposition method outperforms both the word-based and characterbased baselines consistently across all four subsets in both F 1 and OOV recall (R oov ).", "labels": [], "entities": [{"text": "F 1", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9047943651676178}, {"text": "OOV recall", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.7820622324943542}]}, {"text": "Our method demonstrates a robustness across domains and segmentation standards regardless of which baseline model was stronger.", "labels": [], "entities": []}, {"text": "Of particular note is DD's is much more robust in R oov , where the two baselines swing a lot.", "labels": [], "entities": []}, {"text": "This is an important property for downstream applications such as entity recognition.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.860716313123703}]}, {"text": "The DD algorithm is also more consistent, which would likely lead to improvements in applications such as machine translation .", "labels": [], "entities": [{"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.8278033137321472}]}], "tableCaptions": [{"text": " Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.", "labels": [], "entities": [{"text": "SIGHAN 2005 datasets", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.786812831958135}, {"text": "OOV", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9629665017127991}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.8380587100982666}, {"text": "segmentation", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.9655175805091858}, {"text": "consistency", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.5278319120407104}]}, {"text": " Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8926421999931335}, {"text": "SIGHAN  2003 and 2005 datasets", "start_pos": 88, "end_pos": 118, "type": "DATASET", "confidence": 0.8049368977546691}, {"text": "F 1 score", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9830935994784037}]}]}