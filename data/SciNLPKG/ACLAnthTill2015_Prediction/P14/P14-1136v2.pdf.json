{"title": [{"text": "Semantic Frame Identification with Distributed Word Representations", "labels": [], "entities": [{"text": "Semantic Frame Identification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6666130522886912}]}], "abstractContent": [{"text": "We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings.", "labels": [], "entities": [{"text": "semantic frame identification", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7257689237594604}]}, {"text": "Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation.", "labels": [], "entities": []}, {"text": "The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-of-the-art results on FrameNet-style frame-semantic analysis.", "labels": [], "entities": [{"text": "semantic frame identification", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7238035996754965}, {"text": "argument identification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7689474523067474}, {"text": "FrameNet-style frame-semantic analysis", "start_pos": 164, "end_pos": 202, "type": "TASK", "confidence": 0.6823540925979614}]}, {"text": "Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work.", "labels": [], "entities": [{"text": "PropBank-style semantic role labeling", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.6914878040552139}]}], "introductionContent": [{"text": "Distributed representations of words have proved useful fora number of tasks.", "labels": [], "entities": []}, {"text": "By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis), topic classification () or word-word similarity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.9578716158866882}, {"text": "topic classification", "start_pos": 191, "end_pos": 211, "type": "TASK", "confidence": 0.8337018191814423}, {"text": "word-word similarity", "start_pos": 218, "end_pos": 238, "type": "TASK", "confidence": 0.7182373255491257}]}, {"text": "We present anew technique for semantic frame identification that leverages distributed word representations.", "labels": [], "entities": [{"text": "semantic frame identification", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7241522868474325}]}, {"text": "According to the theory of frame semantics, a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the * The majority of this research was carried out during an internship at Google. event.", "labels": [], "entities": []}, {"text": "Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame's semantic roles (.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7252530604600906}, {"text": "frame identification", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7182969450950623}, {"text": "argument identification", "start_pos": 175, "end_pos": 198, "type": "TASK", "confidence": 0.7430080473423004}, {"text": "semantic role labeling)", "start_pos": 203, "end_pos": 226, "type": "TASK", "confidence": 0.7406553998589516}]}, {"text": "Here, we focus on the first subtask of frame identification forgiven predicates; we use our novel method ( \u00a73) in conjunction with a standard argument identification model ( \u00a74) to perform full frame-semantic parsing.", "labels": [], "entities": [{"text": "frame identification forgiven predicates", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.7952434495091438}, {"text": "frame-semantic parsing", "start_pos": 194, "end_pos": 216, "type": "TASK", "confidence": 0.7316983640193939}]}, {"text": "We present experiments on two tasks.", "labels": [], "entities": []}, {"text": "First, we show that for frame identification on the FrameNet corpus (, we outperform the prior state of the art ().", "labels": [], "entities": [{"text": "frame identification", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.779191255569458}, {"text": "FrameNet corpus", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9384868144989014}]}, {"text": "Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7191019356250763}, {"text": "frame identification", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.6962422728538513}, {"text": "argument identification", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7418068945407867}]}, {"text": "Second, we present results on PropBank-style semantic role labeling, that approach strong baselines, and are on par with prior state of the art).", "labels": [], "entities": [{"text": "PropBank-style semantic role labeling", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.6981987208127975}]}], "datasetContent": [{"text": "We process our PropBank and FrameNet training, development and test corpora with a shift-reduce dependency parser that uses the Stanford conventions ( and uses an arc-eager transition system with beam size of 8; the parser and its features are described by.", "labels": [], "entities": []}, {"text": "Before parsing the data, it is tagged with a POS tagger trained with a conditional random field () with the following emission features: word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation.", "labels": [], "entities": []}, {"text": "Beyond the bias transition feature, we have two cluster features for the left and right words in the transition.", "labels": [], "entities": []}, {"text": "We use Brown clusters learned using the algorithm of on a large English newswire corpus for cluster features.", "labels": [], "entities": [{"text": "English newswire corpus", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.7231928110122681}]}, {"text": "We use the same word clusters for the argument identification features in.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7241901457309723}]}, {"text": "We learn the initial embedding representations for our frame identification model ( \u00a73) using a deep neural language model similar to the one proposed by.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7292835116386414}]}, {"text": "We use 3 hidden layers each with 1024 neurons and learn a 128-dimensional embedding from a large corpus containing over 100 billion tokens.", "labels": [], "entities": []}, {"text": "In order to speedup learning, we use an unnormalized output layer and a hinge-loss objective.", "labels": [], "entities": []}, {"text": "The objective tries to ensure that the correct word scores higher than a random incorrect word, and we train with minibatch stochastic gradient descent.", "labels": [], "entities": []}, {"text": "Hyperparameters For our frame identification model with embeddings, we search for the WSA-BIE hyperparameters using the development data.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7611247897148132}, {"text": "WSA-BIE", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.7960384488105774}]}, {"text": "We search for the stochastic gradient learning rate in {0.0001, 0.001, 0.01}, the margin \u03b3 \u2208 {0.001, 0.01, 0.1, 1} and the dimensionality of the final vector space m \u2208 {256, 512}, to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.8917253017425537}]}, {"text": "The underlined values are the chosen hyperparameters used to analyze the test data.", "labels": [], "entities": []}, {"text": "Argument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in \u00a74) was adapted from the algorithm of applied to dependency trees.", "labels": [], "entities": [{"text": "candidate argument extraction", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6219276289145151}, {"text": "FrameNet data", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9351014792919159}]}, {"text": "Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates: we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate's head; this helped capture cases like \"a few months\" (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate's head to the position immediately before the predicate, for cases like \"your gift to Goodwill\" (where to is the predicate and your gift is the argument).", "labels": [], "entities": [{"text": "Goodwill", "start_pos": 594, "end_pos": 602, "type": "DATASET", "confidence": 0.8956158757209778}]}, {"text": "10 10 Note that describe the state of the art in FrameNet-based analysis, but their argument identification strategy considered all possible dependency subtrees in Frame Lexicon In our experimental setup, we scanned the XML files in the \"frames\" directory of the FrameNet 1.5 release, which lists all the frames, the corresponding roles and the associated lexical units, and created a frame lexicon to be used in our frame and argument identification models.", "labels": [], "entities": [{"text": "FrameNet-based analysis", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7503363192081451}, {"text": "argument identification", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7246905118227005}, {"text": "Frame Lexicon", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.9266888201236725}]}, {"text": "We noted that this renders every lexical unit as seen; in other words, at frame disambiguation time on our test set, for all instances, we only had to score the frames in F fora predicate with lexical unit (see \u00a73 and \u00a75.2).", "labels": [], "entities": []}, {"text": "We call this setup FULL LEXICON.", "labels": [], "entities": [{"text": "FULL LEXICON", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.7818340957164764}]}, {"text": "While comparing with prior state of the art on the same corpus, we noted that found several unseen predicates attest time.", "labels": [], "entities": []}, {"text": "For fair comparison, we took the lexical units for the predicates that Das et al. considered as seen, and constructed a lexicon with only those; training instances, if any, for the unseen predicates under Das et al.'s setup were thrown out as well.", "labels": [], "entities": []}, {"text": "We call this setup SEMAFOR LEXICON.", "labels": [], "entities": [{"text": "SEMAFOR", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.7888676524162292}, {"text": "LEXICON", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.5062553882598877}]}, {"text": "We also experimented on the set of unseen instances used by Das et al.", "labels": [], "entities": []}, {"text": "ILP constraints For FrameNet, we used three ILP constraints during argument identification ( \u00a74).", "labels": [], "entities": [{"text": "argument identification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.7190234810113907}]}, {"text": "1) each span could have only one role, 2) each core role could be present only once, and 3) all overt arguments had to be non-overlapping.", "labels": [], "entities": []}, {"text": "a parse, resulting in a much larger search space.: Full frame-structure prediction results for Propbank.", "labels": [], "entities": [{"text": "frame-structure prediction", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6472368091344833}, {"text": "Propbank", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9540870189666748}]}, {"text": "This is a metric that takes into account frames and arguments together.", "labels": [], "entities": []}, {"text": "See \u00a75.7 for more details.", "labels": [], "entities": []}, {"text": "Hyperparameters As in \u00a75.4, we made a hyperparameter sweep in the same space.", "labels": [], "entities": []}, {"text": "The chosen learning rate was 0.01, while the other values were \u03b3 = 0.01 and m = 512.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9539745151996613}]}, {"text": "Ambiguous lexical units were used for this selection process.", "labels": [], "entities": []}, {"text": "Argument Candidates For PropBank we use the algorithm of applied to dependency trees.", "labels": [], "entities": []}, {"text": "Frame Lexicon For the PropBank experiments we scanned the frame files for propositions in Ontonotes 4.0, and stored possible core roles for each verb frame.", "labels": [], "entities": []}, {"text": "The lexical units were simply the verb associating with the verb frames.", "labels": [], "entities": []}, {"text": "There were no unseen verbs attest time.", "labels": [], "entities": [{"text": "time", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9771918654441833}]}, {"text": "ILP constraints We used the constraints of. presents accuracy results on frame identification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9994053840637207}, {"text": "frame identification", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.821680873632431}]}, {"text": "We present results on all predicates, ambiguous predicates seen in the lexicon or the training data, and rare ambiguous predicates that appear \u2264 11 times in the training data.", "labels": [], "entities": []}, {"text": "The WS-ABIE EMBEDDING model from \u00a73 performs significantly better than the LOG-LINEAR WORDS baseline, while LOG-LINEAR EMBEDDING underperforms in every metric.", "labels": [], "entities": []}, {"text": "For the SEMAFOR LEXICON setup, we also compare with the state of the art from Das We do not report partial frame accuracy that has been reported by prior work.", "labels": [], "entities": [{"text": "SEMAFOR", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.5412914156913757}, {"text": "LEXICON", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.41760966181755066}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.714640200138092}]}, {"text": "et al., who used a semi-supervised learning method to improve upon a supervised latentvariable log-linear model.", "labels": [], "entities": []}, {"text": "For unseen predicates from the Das et al. system, we perform better as well.", "labels": [], "entities": []}, {"text": "Finally, for the FULL LEXICON setting, the absolute accuracy numbers are even better for our best model.", "labels": [], "entities": [{"text": "FULL", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9097838997840881}, {"text": "LEXICON", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.5202704668045044}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9449468851089478}]}, {"text": "presents results on the full frame-semantic parsing task (measured by a reimplementation of the SemEval 2007 shared task evaluation script) when our argument identification model ( \u00a74) is used after frame identification.", "labels": [], "entities": [{"text": "frame-semantic parsing task", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7684477667013804}, {"text": "SemEval 2007 shared task evaluation script", "start_pos": 96, "end_pos": 138, "type": "TASK", "confidence": 0.5840586523214976}, {"text": "frame identification", "start_pos": 199, "end_pos": 219, "type": "TASK", "confidence": 0.685565710067749}]}, {"text": "We notice similar trends as in, and our results outperform the previously published best results, setting anew state of the art.", "labels": [], "entities": []}, {"text": "shows frame identification results on the PropBank data.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.6745070517063141}, {"text": "PropBank data", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9891954660415649}]}, {"text": "On the development set, our best model performs with the highest accuracy on all and ambiguous predicates, but performs worse on rare ambiguous predicates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9988861680030823}]}, {"text": "On the test set, the LOG-LINEAR WORDS baseline performs best by a very narrow margin.", "labels": [], "entities": [{"text": "LOG-LINEAR WORDS baseline", "start_pos": 21, "end_pos": 46, "type": "METRIC", "confidence": 0.7971862355868021}]}, {"text": "See \u00a76 fora discussion.", "labels": [], "entities": []}, {"text": "presents results where we measure precision, recall and F 1 for frames and arguments together; this strict metric penalizes arguments for mismatched frames, like in.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.999366819858551}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9994643330574036}, {"text": "F 1", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9908780455589294}]}, {"text": "We seethe same trend as in.", "labels": [], "entities": []}, {"text": "Finally, presents SRL results that measures argument performance only, irrespective of the frame; we use the evaluation script from).", "labels": [], "entities": [{"text": "SRL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.651407778263092}]}, {"text": "We note that with a better frame identification model, our performance on SRL improves in general.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7994941174983978}, {"text": "SRL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.8810270428657532}]}, {"text": "Here, too, the embedding model barely misses the performance of the best baseline, but we are at par and sometimes better than the single parser setting of a state-of-the-art SRL system).", "labels": [], "entities": []}, {"text": "14 The last row of refers to a system which used the For FrameNet, the WSABIE EMBEDDING model we propose strongly outperforms the baselines on all metrics, and sets anew state of the art.", "labels": [], "entities": [{"text": "WSABIE EMBEDDING", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.4542570561170578}]}, {"text": "We believe that the WSABIE EMBEDDING model performs better than the LOG-LINEAR EMBEDDING baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space.", "labels": [], "entities": [{"text": "WSABIE EMBEDDING", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.4436049312353134}]}, {"text": "Consequently, the WSABIE EMBEDDING model can share more information between different examples in the training data than the LOG-LINEAR EMBEDDING model.", "labels": [], "entities": [{"text": "WSABIE EMBEDDING", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.3683456629514694}]}, {"text": "Since the LOG-LINEAR WORDS model always performs better than the LOG-LINEAR EMBEDDING model, we conclude that the primary benefit does not come from the input embedding representation.", "labels": [], "entities": []}, {"text": "On the PropBank data, we see that the LOG-LINEAR WORDS baseline has roughly the same performance as our model on most metrics: slightly better on the test data and slightly worse on the development data.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.965383768081665}, {"text": "LOG-LINEAR WORDS baseline", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8217489719390869}]}, {"text": "This can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9218845963478088}]}, {"text": "Another important distinction between PropBank and FrameNet is that the latter shares frames between multiple lexical units.", "labels": [], "entities": []}, {"text": "The effect of this is clearly observable from the \"Rare\" column in.", "labels": [], "entities": [{"text": "Rare\" column", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.8192060192426046}]}, {"text": "WSABIE EMBEDDING performs poorly in this setting while LOG-LINEAR EMBEDDING performs well.", "labels": [], "entities": [{"text": "WSABIE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5659399032592773}, {"text": "LOG-LINEAR", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.8559081554412842}]}, {"text": "Part of the explanation has to do with the specifics of WSABIE training.", "labels": [], "entities": [{"text": "WSABIE training", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7819710671901703}]}, {"text": "Recall that the WSABIE EMBEDDING model needs to estimate the label location in R m for each frame.", "labels": [], "entities": [{"text": "WSABIE EMBEDDING", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.3081391453742981}]}, {"text": "In other words, it must estimate 512 parameters based on at most 10 training examples.", "labels": [], "entities": []}, {"text": "However, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M . By contrast, in the log-linear models each label has its own set of parameters, and they interact only via the normalization constant.", "labels": [], "entities": []}, {"text": "The LOG-LINEAR WORDS model does not have this entanglement, but cannot share information between words.", "labels": [], "entities": []}, {"text": "For PropBank, combination of two syntactic parsers as input.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9080629944801331}]}, {"text": "One could imagine training a WSABIE model with word features, but we did not perform this experiment.", "labels": [], "entities": []}, {"text": "these drawbacks and benefits balance out and we see similar performance for LOG-LINEAR WORDS and LOG-LINEAR EMBEDDING.", "labels": [], "entities": [{"text": "LOG-LINEAR WORDS", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.5283655226230621}]}, {"text": "For FrameNet, estimating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent.", "labels": [], "entities": []}, {"text": "For example, we might have seen the SENDING frame many times, even though telex.V is a rare lexical unit.", "labels": [], "entities": [{"text": "SENDING", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.8684705495834351}]}], "tableCaptions": [{"text": " Table 2: Frame identification results for FrameNet. See  \u00a75.6.", "labels": [], "entities": [{"text": "Frame identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8775843977928162}, {"text": "FrameNet", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8674650192260742}]}, {"text": " Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7140341401100159}, {"text": "FrameNet", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.843998372554779}, {"text": "LOG-LINEAR EMBEDDING", "start_pos": 135, "end_pos": 155, "type": "METRIC", "confidence": 0.8262786865234375}]}, {"text": " Table 4: Frame identification accuracy results for PropBank.  The model and the column names have the same semantics  as Table 2.", "labels": [], "entities": [{"text": "Frame identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7353500425815582}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9207644462585449}, {"text": "PropBank", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9273748397827148}]}, {"text": " Table 5: Full frame-structure prediction results for Propbank.  This is a metric that takes into account frames and arguments  together. See  \u00a75.7 for more details.", "labels": [], "entities": [{"text": "frame-structure prediction", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6610869765281677}, {"text": "Propbank", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9187970757484436}]}, {"text": " Table 6: Argument only evaluation (semantic role labeling  metrics) using the CoNLL 2005 shared task evaluation script  (Carreras and M` arquez, 2005). Results from Punyakanok et  al. (2008) are taken from Table 11 of that paper.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.669965664545695}, {"text": "CoNLL 2005 shared task evaluation script", "start_pos": 79, "end_pos": 119, "type": "DATASET", "confidence": 0.8909944295883179}]}, {"text": " Table 7: List of files used as development set for the FrameNet 1.5 corpus.", "labels": [], "entities": [{"text": "FrameNet 1.5 corpus", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.8709488312403361}]}]}