{"title": [{"text": "Enforcing Structural Diversity in Cube-pruned Dependency Parsing", "labels": [], "entities": [{"text": "Enforcing Structural Diversity", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7517328361670176}]}], "abstractContent": [{"text": "In this paper we extend the cube-pruned dependency parsing framework of Zhang et al.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.6736249923706055}]}, {"text": "(2012; 2013) by forcing inference to maintain both label and structural ambiguity.", "labels": [], "entities": []}, {"text": "The resulting parser achieves state-of-the-art accuracies, in particular on datasets with a large set of dependency labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsers assign a syntactic dependency tree to an input sentence, as exemplified in.", "labels": [], "entities": []}, {"text": "Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (), sibling or grandparent arcs) or higher-order substructures (.", "labels": [], "entities": []}, {"text": "As the scope of each feature function increases so does parsing complexity, e.g., o(n 5 ) for fourth-order dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.6787657290697098}]}, {"text": "This has led to work on approximate inference, typically via pruning Recently, it has been shown that cube-pruning can efficiently introduce higherorder dependencies in graph-based parsing.", "labels": [], "entities": [{"text": "approximate inference", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7867772281169891}]}, {"text": "Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms.", "labels": [], "entities": [{"text": "Cube-pruned dependency parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.4992203414440155}, {"text": "bottom-up chart parsing", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.705748458703359}]}, {"text": "Similar to k-best inference, each chart cell maintains abeam of kbest partial dependency structures.", "labels": [], "entities": []}, {"text": "Higher-order features are scored when combining beams during inference.", "labels": [], "entities": []}, {"text": "Cube-pruning is an approximation, as the highest scoring tree may fallout of the beam before being fully scored with higher-order features.", "labels": [], "entities": []}, {"text": "However, observe stateof-the-art results when training accounts for errors that arise due to such approximations.", "labels": [], "entities": []}, {"text": "In this work we extend the cube-pruning framework of Zhang et al. by observing that dependency parsing has two fundamental sources of ambiguity.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8348297476768494}]}, {"text": "The first, structural ambiguity, pertains to confusions about the unlabeled structure of the tree, e.g., the classic prepositional phrase attachment problem.", "labels": [], "entities": [{"text": "prepositional phrase attachment problem", "start_pos": 117, "end_pos": 156, "type": "TASK", "confidence": 0.7353962883353233}]}, {"text": "The second, label ambiguity, pertains to simple label confusions, e.g., whether a verbal object is director indirect.", "labels": [], "entities": []}, {"text": "Distinctions between arc labels are frequently fine-grained and easily confused by parsing models.", "labels": [], "entities": []}, {"text": "For example, in the Stanford dependency label set), the labels TMOD (temporal modifier), NPADVMOD (nounphrase adverbial modifier), IOBJ (indirect object) and DOBJ (direct object) can all be noun phrases that modify verbs to their right.", "labels": [], "entities": [{"text": "TMOD", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.8526325225830078}]}, {"text": "In the context of cube-pruning, during inference, the system opts to maintain a large amount of label ambiguity at the expense of structural ambiguity.", "labels": [], "entities": []}, {"text": "Frequently, the beam stores only label ambiguities and the resulting set of trees have identical unlabeled structure.", "labels": [], "entities": []}, {"text": "For example, in, the aforementioned label ambiguity around noun objects to the right of the verb (DOBJ vs. IOBJ vs. TMP) could lead one or more of the structural ambiguities falling out of the beam, especially if the beam is small.", "labels": [], "entities": []}, {"text": "To combat this, we introduce a secondary beam for each unique unlabeled structure.", "labels": [], "entities": []}, {"text": "That is, we partition the primary (entire) beam into disjoint groups according to the identity of unlabeled structure.", "labels": [], "entities": []}, {"text": "By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam.", "labels": [], "entities": []}, {"text": "The resulting parser consistently improves on the state-of-the-art parser of.", "labels": [], "entities": []}, {"text": "In Figure 2: Structures and rules for parsing with the algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9707295298576355}]}, {"text": "Solid lines show only the construction of right-pointing first-order dependencies.", "labels": [], "entities": []}, {"text": "l is the predicted arc label.", "labels": [], "entities": [{"text": "arc label", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8936510682106018}]}, {"text": "Dashed lines are the additional sibling modifier signatures in a generalized algorithm, specifically the previous modifier incomplete chart items.", "labels": [], "entities": []}, {"text": "particular, data sets with large label sets (and thus a large number of label confusions) typically seethe largest jumps inaccuracy.", "labels": [], "entities": []}, {"text": "Finally, we show that the same result cannot be achieved by simply increasing the size of the beam, but requires explicit enforcing of beam diversity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the cube-pruned dependency parser of as our baseline system.", "labels": [], "entities": []}, {"text": "To make an apples-to-apples comparison, we use the same online learning algorithm and the same feature templates.", "labels": [], "entities": []}, {"text": "The feature templates include firstto-third-order labeled features and valency features.", "labels": [], "entities": []}, {"text": "More details of these features are described in.", "labels": [], "entities": []}, {"text": "For online learning, we apply the same violation-fixing strategy (so-called single-node max-violation) on MIRA and run 8 epochs of training for all experiments.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.7431145906448364}]}, {"text": "For English, we conduct experiments on the commonly-used constituency-to-dependencyconverted Penn Treebank data sets.", "labels": [], "entities": [{"text": "Penn Treebank data sets", "start_pos": 93, "end_pos": 116, "type": "DATASET", "confidence": 0.9292799234390259}]}, {"text": "The first one, Penn-YM, was created by the Penn2Malt 1 software.", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.9778470993041992}, {"text": "Penn2Malt 1 software", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.952971875667572}]}, {"text": "The second one, Penn-S-2.0.5, used the Stanford dependency framework) by applying version 2.0.5 of the Stanford parser.", "labels": [], "entities": [{"text": "Penn-S-2.0.5", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.9757850170135498}]}, {"text": "The third one, Penn-S-3.3.0 was converted by version 3.3.0 of the Stanford parser.", "labels": [], "entities": [{"text": "Penn-S-3.3.0", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.9835073947906494}]}, {"text": "The train/dev/test split was standard: sections 2-21 for training; 22 for validation; and 23 for evaluation.", "labels": [], "entities": []}, {"text": "Automatic POS tags for Penn-YM and Penn-S-2.0.5 are provided by TurboTagger () with an accuracy of 97.3% on section 23.", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9881917834281921}, {"text": "Penn-S-2.0.5", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.967934787273407}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9995189905166626}]}, {"text": "For Chinese, we use the CTB-5 dependency treebank which was converted from the original constituent treebank by: English and Chinese results for cube pruning dependency parsing with the enforcement of structural diversity.", "labels": [], "entities": [{"text": "CTB-5 dependency treebank", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.8751962582270304}, {"text": "cube pruning dependency parsing", "start_pos": 145, "end_pos": 176, "type": "TASK", "confidence": 0.6138284802436829}]}, {"text": "PENN-S and CTB-5 are significant at p < 0.05.", "labels": [], "entities": [{"text": "PENN-S", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8586654663085938}, {"text": "CTB-5", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8185964226722717}]}, {"text": "Penn-S-2.0.5 TurboParser result is from.", "labels": [], "entities": [{"text": "Penn-S-2.0.5 TurboParser result", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.8901558319727579}]}, {"text": "Following, we trained our models on Penn-S-3.3.0 with gold POS tags and evaluated with both non-gold (Stanford tagger) and gold tags.", "labels": [], "entities": [{"text": "Penn-S-3.3.0", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9893720746040344}]}, {"text": "shows the main results of the paper.", "labels": [], "entities": []}, {"text": "Both the baseline and the new system keep abeam of size 6 for each chart cell.", "labels": [], "entities": []}, {"text": "The difference is that the new system enforces structural diversity with the introduction of a secondary beam for label variants.", "labels": [], "entities": []}, {"text": "We choose the secondary beam that yields the highest LAS on the development data sets for Penn-YM, Penn-S-2.0.5 and CTB-5.", "labels": [], "entities": [{"text": "LAS", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9964054822921753}, {"text": "Penn-YM", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9643065929412842}, {"text": "Penn-S-2.0.5", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9661915302276611}, {"text": "CTB-5", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.865425705909729}]}, {"text": "Indeed we observe larger improvements for the data sets with larger label sets.", "labels": [], "entities": []}, {"text": "Penn-S-2.0.5 has 49 labels and observes a 0.2% absolute improvement in LAS.", "labels": [], "entities": [{"text": "Penn-S-2.0.5", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.975140392780304}, {"text": "absolute", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.975177526473999}, {"text": "LAS", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.7995755076408386}]}, {"text": "Although CTB-5 has a small label set (18), we do see similar improvements for both UAS and LAS.", "labels": [], "entities": [{"text": "CTB-5", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.914091944694519}, {"text": "UAS", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.41726675629615784}]}, {"text": "There is a slight improvement for Penn-YM despite the fact that Penn-YM has the most compact label set (12).", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9794307351112366}, {"text": "Penn-YM", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9807778000831604}]}, {"text": "These results are the highest known in the literature.", "labels": [], "entities": []}, {"text": "For the Penn-S-3.3.0 results we can see that our model outperforms TurboPaser and is competitive with the Berkeley constituency parser ().", "labels": [], "entities": [{"text": "Penn-S-3.3.0", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9652068614959717}]}, {"text": "In particular, if gold tags are assumed, cube-pruning significantly outperforms Berkeley.", "labels": [], "entities": []}, {"text": "This suggests that joint tagging and parsing should improve performance further in the non-gold tag setting, as that is a differentiating characteristic of constituency parsers.", "labels": [], "entities": [{"text": "joint tagging", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.6121292859315872}, {"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9720026254653931}]}, {"text": "shows the results on the CoNLL 2006/2007 data sets).", "labels": [], "entities": [{"text": "CoNLL 2006/2007 data sets", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.9842203954855601}]}, {"text": "For simplicity, we set the secondary beam to 3 for all.", "labels": [], "entities": []}, {"text": "We can see that overall there is an improvement inaccuracy and this is highly correlated with the size of the label set.", "labels": [], "entities": []}, {"text": "In order to examine the importance of balancing structural diversity and labeled diversity, we let the size of the secondary beam vary from one to the size of the primary beam.", "labels": [], "entities": []}, {"text": "In, we show the results of all combinations of beam settings of primary beam sizes 4 and 6 for three data sets: Penn-YM, Penn-S-2.0.5, and CTB-5 respectively.", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9851887226104736}, {"text": "Penn-S-2.0.5", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9406096339225769}, {"text": "CTB-5", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.9255996942520142}]}, {"text": "In the table, we highlight the best results for each beam size and data set on the development data.", "labels": [], "entities": []}, {"text": "For 5 of the total of 6 comparison groups -three lan-  guages times two primary beams -the best result is obtained by choosing a secondary beam size that is close to one half the size of the primary beam.", "labels": [], "entities": []}, {"text": "Contrasting, the accuracy improvements are consistent across the development set and the test set for all three data sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994163513183594}]}, {"text": "A reasonable question is whether such improvements could be obtained by simply enlarging the beam in the baseline parser.", "labels": [], "entities": []}, {"text": "The bottom row of shows the parsing results for the three data sets when the beam is enlarged to 16.", "labels": [], "entities": [{"text": "parsing", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9505850672721863}]}, {"text": "On Penn-S-2.0.5, the baseline with beam 16 is at roughly the same speed as the highlighted best system with primary beam 6 and secondary beam 3.", "labels": [], "entities": [{"text": "Penn-S-2.0.5", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.9889819025993347}]}, {"text": "On CTB-5, the beam 16 baseline is 30% slower.", "labels": [], "entities": [{"text": "CTB-5", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9597887992858887}, {"text": "beam 16 baseline", "start_pos": 14, "end_pos": 30, "type": "METRIC", "confidence": 0.8664642771085104}]}, {"text": "indicates that simply enlarging the beam -relative to parsing speed -does not recover the wins of structural diversity on Penn-S-2.0.5 and CTB-5, though it does reduce the gap on Penn-S-2.0.5.", "labels": [], "entities": [{"text": "parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.9426586627960205}, {"text": "Penn-S-2.0.5", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.9868448972702026}, {"text": "CTB-5", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.8731721639633179}, {"text": "Penn-S-2.0.5", "start_pos": 179, "end_pos": 191, "type": "DATASET", "confidence": 0.9943307638168335}]}, {"text": "On Penn-YM, the beam 16 baseline is slightly better than the new system, but 90% slower.", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9852521419525146}, {"text": "beam 16 baseline", "start_pos": 16, "end_pos": 32, "type": "METRIC", "confidence": 0.8487947781880697}]}, {"text": "To better understand the behaviour of structural diversity pruning relative to increasing the beam, we looked at the unlabeled attachment F-score per dependency label in the Penn-S-2.0.5 development set 2 . shows the 10 labels with the largest increase in attachment scores for structural diversity pruning relative to standard pruning.", "labels": [], "entities": [{"text": "attachment F-score per dependency label", "start_pos": 127, "end_pos": 166, "type": "METRIC", "confidence": 0.7417750239372254}, {"text": "Penn-S-2.0.5 development set", "start_pos": 174, "end_pos": 202, "type": "DATASET", "confidence": 0.9776725172996521}]}, {"text": "Importantly, the biggest wins are primarily for labels in which unlabeled attachment is lower than average (93.99, 8 out of 10).", "labels": [], "entities": []}, {"text": "Thus, diversity pruning gets most of its wins on difficult attachment decisions.", "labels": [], "entities": [{"text": "diversity pruning", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.9591171443462372}]}, {"text": "Indeed, many of the relations represent clausal dependencies that are frequently structurally ambiguous.", "labels": [], "entities": []}, {"text": "There are also cases of relatively short dependencies that can be difficult to attach.", "labels": [], "entities": []}, {"text": "For instance, quantmod dependencies are typically adverbs occurring after verbs that modify quantities to their right.", "labels": [], "entities": []}, {"text": "But these can be confused as adverbial modifiers of the verb to the left.", "labels": [], "entities": []}, {"text": "These results support our hypothesis that label ambiguity is causing hard attachment decisions to be pruned and that structural diversity can ameliorate this.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from", "labels": [], "entities": [{"text": "cube pruning dependency parsing", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6618856936693192}, {"text": "PENN-S", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.844333291053772}, {"text": "CTB-5", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.8960849642753601}, {"text": "Penn-S-2.0.5 TurboParser", "start_pos": 167, "end_pos": 191, "type": "DATASET", "confidence": 0.8503792881965637}]}, {"text": " Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.", "labels": [], "entities": [{"text": "CoNLL  2006/2007 shared tasks", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.9055594901243845}]}, {"text": " Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.", "labels": [], "entities": []}]}