{"title": [], "abstractContent": [{"text": "We explore the extent to which high-resource manual annotations such as tree-banks are necessary for the task of semantic role labeling (SRL).", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.7760809659957886}]}, {"text": "We examine how performance changes without syntactic supervision, comparing both joint and pipelined methods to induce latent syntax.", "labels": [], "entities": []}, {"text": "This work highlights anew application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.771076887845993}, {"text": "SRL", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9910457730293274}]}, {"text": "Our best models obtain competitive results in the high-resource setting and state-of-the-art results in the low resource setting, reaching 72.48% F1 averaged across languages.", "labels": [], "entities": [{"text": "F1", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9991214871406555}]}, {"text": "We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of semantic role labeling (SRL) is to identify predicates and arguments and label their semantic contribution in a sentence.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.7957330346107483}]}, {"text": "Such labeling defines who did what to whom, when, where and how.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"The kids ran the marathon\", ran assigns a role to kids to denote that they are the runners; and a role to marathon to denote that it is the racecourse.", "labels": [], "entities": []}, {"text": "Models for SRL have increasingly come to rely on an array of NLP tools (e.g., parsers, lemmatizers) in order to obtain state-of-the-art results.", "labels": [], "entities": [{"text": "SRL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9926031231880188}]}, {"text": "Each tool is typically trained on hand-annotated data, thus placing SRL at the end of a very highresource NLP pipeline.", "labels": [], "entities": []}, {"text": "However, richly annotated data such as that provided in parsing treebanks is expensive to produce, and maybe tied to specific domains (e.g., newswire).", "labels": [], "entities": []}, {"text": "Many languages do http://www.cs.jhu.edu/ \u02dc mrg/software/ not have such supervised resources (low-resource languages), which makes exploring SRL crosslinguistically difficult.", "labels": [], "entities": [{"text": "SRL crosslinguistically", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.8808136284351349}]}, {"text": "The problem of SRL for low-resource languages is an important one to solve, as solutions pave the way fora wide range of applications: Accurate identification of the semantic roles of entities is a critical step for any application sensitive to semantics, from information retrieval to machine translation to question answering.", "labels": [], "entities": [{"text": "SRL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.990346372127533}, {"text": "Accurate identification of the semantic roles of entities", "start_pos": 135, "end_pos": 192, "type": "TASK", "confidence": 0.846051812171936}, {"text": "information retrieval", "start_pos": 261, "end_pos": 282, "type": "TASK", "confidence": 0.7271168380975723}, {"text": "machine translation", "start_pos": 286, "end_pos": 305, "type": "TASK", "confidence": 0.7414461374282837}, {"text": "question answering", "start_pos": 309, "end_pos": 327, "type": "TASK", "confidence": 0.839871883392334}]}, {"text": "In this work, we explore models that minimize the need for high-resource supervision.", "labels": [], "entities": []}, {"text": "We examine approaches in a joint setting where we marginalize over latent syntax to find the optimal semantic role assignment; and a pipeline setting where we first induce an unsupervised grammar.", "labels": [], "entities": []}, {"text": "We find that the joint approach is a viable alternative for making reasonable semantic role predictions, outperforming the pipeline models.", "labels": [], "entities": []}, {"text": "These models can be effectively trained with access to only SRL annotations, and mark a state-of-the-art contribution for low-resource SRL.", "labels": [], "entities": [{"text": "SRL annotations", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.8781929016113281}, {"text": "SRL", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9174113273620605}]}, {"text": "To better understand the effect of the lowresource grammars and features used in these models, we further include comparisons with (1) models that use higher-resource versions of the same features; (2) state-of-the-art high resource models; and (3) previous work on low-resource grammar induction.", "labels": [], "entities": [{"text": "low-resource grammar induction", "start_pos": 266, "end_pos": 296, "type": "TASK", "confidence": 0.6938822964827219}]}, {"text": "In sum, this paper makes several experimental and modeling contributions, summarized below.", "labels": [], "entities": []}, {"text": "Experimental contributions: \u2022 Comparison of pipeline and joint models for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9606451392173767}]}, {"text": "\u2022 Subtractive experiments that consider the removal of supervised data.", "labels": [], "entities": []}, {"text": "\u2022 Analysis of the induced grammars in unsupervised, distantly-supervised, and joint training settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are interested in the effects of varied supervision using pipeline and joint training for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.980381965637207}]}, {"text": "To compare to prior work (i.e., submissions to the CoNLL-2009 Shared Task), we also consider the joint task of semantic role labeling and predicate sense disambiguation.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.6910100777943929}, {"text": "predicate sense disambiguation", "start_pos": 138, "end_pos": 168, "type": "TASK", "confidence": 0.7684944669405619}]}, {"text": "Our experiments are subtractive, beginning with all supervision available and then successively removing (a) dependency syntax, (b) morphological features, (c) POS tags, and (d) lemmas.", "labels": [], "entities": []}, {"text": "Dependency syntax is the most expensive and difficult to obtain of these various forms of supervision.", "labels": [], "entities": [{"text": "Dependency syntax", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7978235483169556}]}, {"text": "We explore the importance of both the labels and structure, and what quantity of supervision is useful.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word and edge properties in templates.", "labels": [], "entities": []}, {"text": " Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9974561333656311}, {"text": "SRL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.990882396697998}, {"text": "sense disambiguation", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.6262909173965454}, {"text": "CoNLL'09", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8985494375228882}, {"text": "F1", "start_pos": 209, "end_pos": 211, "type": "METRIC", "confidence": 0.9907220602035522}]}, {"text": " Table 5: F1 for SRL approaches (without sense  disambiguation) in matched and mismatched  train/test settings for CoNLL 2005 span and 2008  head supervision. We contrast low-resource ()  and high-resource settings ( ), where latter uses a  treebank. See  \u00a7 4.4 for caveats to this comparison.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9985935091972351}, {"text": "SRL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9706120491027832}, {"text": "CoNLL 2005 span", "start_pos": 115, "end_pos": 130, "type": "DATASET", "confidence": 0.9559426506360372}]}, {"text": " Table 6: Subtractive experiments. Each row con- tains the F1 for SRL only (without sense disam- biguation) where the supervision type of that row  and all above it have been removed. Removed su- pervision types (Rem) are: syntactic dependencies  (Dep), morphology (Mor), POS tags (POS), and  lemmas (Lem). #FT indicates the number of fea- ture templates used (unigrams+bigrams).", "labels": [], "entities": [{"text": "SRL", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8881852626800537}, {"text": "FT", "start_pos": 308, "end_pos": 310, "type": "METRIC", "confidence": 0.9972568154335022}]}, {"text": " Table 7: Unlabeled directed dependency accuracy  on CoNLL'09 test set in low-resource settings.  DMV models are trained on either POS tags (pos)  or Brown clusters (bc). *Indicates the supervised parser", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9590209722518921}, {"text": "CoNLL'09 test set", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.938666025797526}]}]}