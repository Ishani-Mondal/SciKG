{"title": [{"text": "Linguistically debatable or just plain wrong?", "labels": [], "entities": []}], "abstractContent": [{"text": "In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement.", "labels": [], "entities": []}, {"text": "However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them.", "labels": [], "entities": []}, {"text": "We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages.", "labels": [], "entities": []}, {"text": "This points to an underlying ambiguity rather than random errors.", "labels": [], "entities": []}, {"text": "Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors.", "labels": [], "entities": [{"text": "tag confusions", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7001571804285049}]}, {"text": "Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmo-tivated.", "labels": [], "entities": []}], "introductionContent": [{"text": "In NLP, we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory.", "labels": [], "entities": []}, {"text": "If this was true, the specific theory should be learnable from the annotated data.", "labels": [], "entities": []}, {"text": "However, it is well known that there are linguistically hard cases, where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions.", "labels": [], "entities": []}, {"text": "For example, in parsing auxiliary verbs may head main verbs, or vice versa, and in part-of-speech (POS) tagging, possessive pronouns may belong to the category of determiners or the category of pronouns.", "labels": [], "entities": [{"text": "parsing auxiliary verbs", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.874621053536733}, {"text": "part-of-speech (POS) tagging", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.6341574549674988}]}, {"text": "This position paper argues that annotation projects should embrace these hard cases rather than pretend they can be unambiguously resolved.", "labels": [], "entities": []}, {"text": "Instead of using overly specific annotation guidelines, designed to minimize inter-annotator disagreement, and adjudicating between annotators of different opinions, we should embrace systematic inter-annotator disagreements.", "labels": [], "entities": []}, {"text": "To motivate this, we present an empirical analysis showing 1. that certain inter-annotator disagreements are systematic, and 2. that actual errors are in fact so infrequent as to be negligible, even when linguists annotate without guidelines.", "labels": [], "entities": []}, {"text": "The empirical analysis presented below relies on text corpora annotated with syntactic categories or parts-of-speech (POS).", "labels": [], "entities": []}, {"text": "POS is part of most linguistic theories, but nevertheless, there are still many linguistic constructions -even very frequent ones -whose POS analysis is widely debated.", "labels": [], "entities": [{"text": "POS analysis", "start_pos": 137, "end_pos": 149, "type": "TASK", "confidence": 0.7915801405906677}]}, {"text": "The following sentences exemplify some of these hard cases that annotators frequently disagree on.", "labels": [], "entities": []}, {"text": "Note that we do not claim that both analyses in each of these cases (1-3) are equally good, but that there is some linguistic motivation for either analysis in each case.", "labels": [], "entities": []}, {"text": "(. This suggests that most annotation differences derive from hard cases, rather than random errors.", "labels": [], "entities": []}, {"text": "We then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation errors, and which ones reflect linguistically hard cases (Section 3).", "labels": [], "entities": []}, {"text": "The results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors.", "labels": [], "entities": []}, {"text": "This suggests that inter-annotator agreement scores often hide the fact that the vast majority of annotations are actually linguistically motivated.", "labels": [], "entities": []}, {"text": "In our case, less than 2% of the overall annotations are linguistically unmotivated.", "labels": [], "entities": []}, {"text": "Finally, in Section 4, we present an experiment trying to learn a model to distinguish between hard cases and annotation errors.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}