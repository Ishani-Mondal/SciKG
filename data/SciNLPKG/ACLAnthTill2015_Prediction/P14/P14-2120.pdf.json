{"title": [{"text": "Recognizing Implied Predicate-Argument Relationships in Textual Inference", "labels": [], "entities": [{"text": "Recognizing Implied Predicate-Argument Relationships in Textual Inference", "start_pos": 0, "end_pos": 73, "type": "TASK", "confidence": 0.8182109253747123}]}], "abstractContent": [{"text": "We investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure.", "labels": [], "entities": []}, {"text": "While prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.6410063604513804}]}, {"text": "Such scenarios provide prior information, which substantially eases the task.", "labels": [], "entities": []}, {"text": "We provide a large and freely available evaluation dataset for our task setting, and propose methods to cope with it, while obtaining promising results in empirical evaluations.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "This section describes a semi-automatic method for extracting candidate instances of implied predicate-argument relationship from an RTE dataset.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.7145278453826904}]}, {"text": "This extraction process directly follows our task formalization.", "labels": [], "entities": []}, {"text": "Given a Text Hypothesis pair, we locate a predicate-argument relationship in the Hypothesis, where both the predicate and the argument appear also in the Text, while the relationship between them is not expressed in its syntactic structure.", "labels": [], "entities": []}, {"text": "This process is performed automatically, based on syntactic parsing (see below).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6984826773405075}]}, {"text": "Then, a human reader annotates each instance as \"Yes\" -meaning that the implied relationship indeed holds in the Text, or \"No\" otherwise.", "labels": [], "entities": []}, {"text": "Example instances, constructed by this process, are shown in.", "labels": [], "entities": []}, {"text": "In this work we used lemma-level lexical matching, as well as nominalization matching, to align the Text predicates and arguments to the Hypothesis.", "labels": [], "entities": [{"text": "nominalization matching", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.9146202802658081}]}, {"text": "We note that more advanced matching, e.g., by utilizing knowledge resources (like WordNet), can be performed as well.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9371579885482788}]}, {"text": "To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7667384743690491}]}, {"text": "Nominalization matching (e.g., example 1 of) was performed with.", "labels": [], "entities": [{"text": "Nominalization matching", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8054864406585693}]}, {"text": "By applying this method on the RTE-6 dataset (, we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones.", "labels": [], "entities": [{"text": "RTE-6 dataset", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9676827192306519}]}, {"text": "This dataset is significantly larger than prior datasets for the implied SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 73, "end_pos": 81, "type": "TASK", "confidence": 0.8829960525035858}]}, {"text": "To calculate inter-annotator agreement, the first author also annotated 185 randomly-selected instances.", "labels": [], "entities": []}, {"text": "We have reached high agreement score of 0.80 Kappa.", "labels": [], "entities": [{"text": "agreement score", "start_pos": 21, "end_pos": 36, "type": "METRIC", "confidence": 0.9585280120372772}, {"text": "Kappa", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.5567266941070557}]}, {"text": "The dataset is freely available at www.cs.biu.ac.il/ \u02dc nlp/resources/ downloads/implied-relationships.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Algorithmic features. p and a denote the candidate predicate and argument respectively.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of our method, followed by  baselines and ablation tests.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9948896169662476}, {"text": "ablation", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9881216287612915}]}]}