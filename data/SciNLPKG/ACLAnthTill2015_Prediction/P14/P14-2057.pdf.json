{"title": [{"text": "Tri-Training for Authorship Attribution with Limited Training Data", "labels": [], "entities": [{"text": "Authorship Attribution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8896287977695465}]}], "abstractContent": [{"text": "Authorship attribution (AA) aims to identify the authors of a set of documents.", "labels": [], "entities": [{"text": "Authorship attribution (AA)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8355939030647278}]}, {"text": "Traditional studies in this area often assume that there area large set of labeled documents available for training.", "labels": [], "entities": []}, {"text": "However, in the real life, it is often difficult or expensive to collect a large set of labeled data.", "labels": [], "entities": []}, {"text": "For example, in the online review domain, most reviewers (authors) only write a few reviews, which are not enough to serve as the training data for accurate classification.", "labels": [], "entities": [{"text": "accurate classification", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.5375137776136398}]}, {"text": "In this paper, we present a novel three-view tri-training method to iteratively identify authors of unlabeled data to augment the training set.", "labels": [], "entities": []}, {"text": "The key idea is to first represent each document in three distinct views, and then perform tri-training to exploit the large amount of un-labeled documents.", "labels": [], "entities": []}, {"text": "Starting from 10 training documents per author, we systematically evaluate the effectiveness of the proposed tri-training method for AA.", "labels": [], "entities": [{"text": "AA", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.984050452709198}]}, {"text": "Experimental results show that the proposed approach outperforms the state-of-the-art semi-supervised method CNG+SVM and other baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Existing approaches to authorship attribution (AA) are mainly based on supervised classification.", "labels": [], "entities": [{"text": "authorship attribution (AA)", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.9032875895500183}]}, {"text": "Although this is an effective approach, it has a major weakness, i.e., for each author a large number of his/her articles are needed as the training data.", "labels": [], "entities": []}, {"text": "This is possible if the author has written a large number of articles, but will be difficult if he/she has not.", "labels": [], "entities": []}, {"text": "For example, in the online review domain, most authors (reviewers) only write a few reviews (documents).", "labels": [], "entities": []}, {"text": "It was shown that on average each reviewer only has 2.72 reviews in amazon.com, and only 8% of the reviewers have at least 5 reviews (Jindal and.", "labels": [], "entities": [{"text": "amazon.com", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.860035240650177}]}, {"text": "The small number of labeled documents makes it extremely challenging for supervised learning to train an accurate classifier.", "labels": [], "entities": []}, {"text": "In this paper, we consider AA with only a few labeled examples.", "labels": [], "entities": [{"text": "AA", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9686235189437866}]}, {"text": "By exploiting the redundancy inhuman languages, we tackle the problem using anew three-view tri-training algorithm (TTA).", "labels": [], "entities": []}, {"text": "Specifically, we first represent each document in three distinct views, and then tri-train three classifiers in these views.", "labels": [], "entities": []}, {"text": "The predictions of two classifiers on unlabeled examples are used to augment the training set for the third classifier.", "labels": [], "entities": []}, {"text": "This process repeats until a termination condition is met.", "labels": [], "entities": []}, {"text": "The enlarged labeled sets are finally used to train classifiers to classify the test data.", "labels": [], "entities": []}, {"text": "To our knowledge, no existing work has addressed AA in a tri-training framework.", "labels": [], "entities": [{"text": "AA", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.981404185295105}]}, {"text": "The AA problem with limited training data was attempted in.", "labels": [], "entities": [{"text": "AA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9582340717315674}]}, {"text": "However, neither of them used a semisupervised approach to augment the training set with additional documents.", "labels": [], "entities": []}, {"text": "Kourtis and Stamatatos (2011) introduced a variant of the selftraining method in).", "labels": [], "entities": []}, {"text": "Note that the original self-training uses one classifier on one view.", "labels": [], "entities": []}, {"text": "However, the self-training method in () uses two classifiers (CNG and SVM) on one view.", "labels": [], "entities": []}, {"text": "Both the self-training and tri-training are semisupervised learning methods.", "labels": [], "entities": []}, {"text": "However, the proposed approach is not a simple extension of the self-training method CNG+SVM of ().", "labels": [], "entities": []}, {"text": "First, in their experimental setting, about 115 and 129 documents per author on average are used for two experimental corpora.", "labels": [], "entities": []}, {"text": "This number of labeled documents is still very large.", "labels": [], "entities": []}, {"text": "We consider a much more realistic problem, where the size of the training set is very small.", "labels": [], "entities": []}, {"text": "Only 10 samples per author are used in training.", "labels": [], "entities": []}, {"text": "Second, CNG+SVM uses two learning methods on a single character n-gram view.", "labels": [], "entities": []}, {"text": "In contrast, besides the character n-gram view, we also make use of the lexical and syntactic views.", "labels": [], "entities": []}, {"text": "That is, three distinct views are used for building classifiers.", "labels": [], "entities": []}, {"text": "The redundant information inhuman language is combined in the tri-training procedure.", "labels": [], "entities": []}, {"text": "Third, in each round of self-training in CNG+SVM, each classifier is refined by the same newly labeled examples.", "labels": [], "entities": []}, {"text": "However, in the proposed tri-training method (TTA), the examples labeled by the classifiers of every two views are added to the third view.", "labels": [], "entities": []}, {"text": "By doing so, each classifier can borrow information from the other two views.", "labels": [], "entities": []}, {"text": "And the predictions made by two classifiers are more reliable than those by one classifier.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is thus the proposed three-view tri-training scheme which has a much better generalization ability by exploiting three different views of the same document.", "labels": [], "entities": []}, {"text": "Experimental results on the IMDb review dataset show that the proposed method dramatically improves the CNG+SVM method.", "labels": [], "entities": [{"text": "IMDb review dataset", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9551952481269836}]}, {"text": "It also outperforms the co-training method) based on our proposed views.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate the proposed method.", "labels": [], "entities": []}, {"text": "We use logistic regression (LR) with L2 regularization) and the SVM multiclass (SVM) system (Joachims, 2007) with its default settings as the classifiers.", "labels": [], "entities": []}, {"text": "We conduct experiments on the IMDb dataset (", "labels": [], "entities": [{"text": "IMDb dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9683025777339935}]}], "tableCaptions": [{"text": " Table 1. Effects of three aggregation strategies:", "labels": [], "entities": []}, {"text": " Table 2. It  is clear that CNG is almost unable to correctly", "labels": [], "entities": []}, {"text": " Table 2. Results for the CNG+SVM baseline", "labels": [], "entities": [{"text": "CNG+SVM baseline", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.5738812536001205}]}, {"text": " Table 4. Again, tri- training beats co-training consistently. The best  performance of co-training is 92.81% achieved  on the character and lexical views after 60 itera- tions. However, the accuracy is worse than that  of tri-training. The key reason is that tri-training  considers three views, while co-training uses on- ly two. Also, the predictions by two classifiers  are more reliable than those by one classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9995864033699036}]}, {"text": " Table 4. Co-training vs. tri-training", "labels": [], "entities": []}]}