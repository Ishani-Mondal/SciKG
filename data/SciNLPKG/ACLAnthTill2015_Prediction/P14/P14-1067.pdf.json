{"title": [{"text": "Adaptive Quality Estimation for Machine Translation", "labels": [], "entities": [{"text": "Adaptive Quality Estimation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.727051724990209}, {"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.812354564666748}]}], "abstractContent": [{"text": "The automatic estimation of machine translation (MT) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role.", "labels": [], "entities": [{"text": "automatic estimation of machine translation (MT) output", "start_pos": 4, "end_pos": 59, "type": "TASK", "confidence": 0.7475263012780083}]}, {"text": "When moving from controlled lab evaluations to real-life scenarios the task becomes even harder.", "labels": [], "entities": []}, {"text": "For current MT quality estimation (QE) systems , additional complexity comes from the difficulty to model user and domain changes.", "labels": [], "entities": [{"text": "MT quality estimation (QE)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.9013529717922211}]}, {"text": "Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions.", "labels": [], "entities": []}, {"text": "To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes.", "labels": [], "entities": []}, {"text": "Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tangible mutual benefit.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.8277605175971985}]}, {"text": "On one side, SMT research brings to the industry improved output quality and a number of appealing solutions useful to increase translators' productivity.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9944551587104797}]}, {"text": "On the other side, the market needs suggest concrete problems to solve, providing real-life scenarios to develop and evaluate new ideas with rapid turnaround.", "labels": [], "entities": []}, {"text": "The evolution of computer-assisted translation (CAT) environments is an evidence of this trend, shown by the increasing interest towards the integration of suggestions obtained from MT engines with those derived from translation memories (TMs).", "labels": [], "entities": [{"text": "computer-assisted translation (CAT)", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.8536029577255249}, {"text": "MT engines", "start_pos": 182, "end_pos": 192, "type": "TASK", "confidence": 0.890295684337616}]}, {"text": "The possibility to speedup the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges.", "labels": [], "entities": [{"text": "translation process", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9185625910758972}, {"text": "MT output", "start_pos": 100, "end_pos": 109, "type": "TASK", "confidence": 0.586599200963974}]}, {"text": "Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way.", "labels": [], "entities": []}, {"text": "In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (.", "labels": [], "entities": [{"text": "QE", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.7386943101882935}]}, {"text": "Despite the substantial progress done so far in the field and in successful evaluation campaigns, focusing on concrete market needs makes possible to further define the scope of research on QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 190, "end_pos": 192, "type": "TASK", "confidence": 0.8783645629882812}]}, {"text": "For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job.", "labels": [], "entities": []}, {"text": "Such variability is due to two main reasons: 1.", "labels": [], "entities": []}, {"text": "The notion of MT output quality is highly subjective.", "labels": [], "entities": [{"text": "MT output", "start_pos": 14, "end_pos": 23, "type": "TASK", "confidence": 0.8616014122962952}]}, {"text": "Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of anew user; 2.", "labels": [], "entities": []}, {"text": "Each translation job has its own specificities (domain, complexity of the source text, average target quality).", "labels": [], "entities": []}, {"text": "Since data from anew job may differ from those used to train the QE model, its estimates on the new instances might result to be biased or uninformative.", "labels": [], "entities": []}, {"text": "The ability of a system to self-adapt to the be-haviour of specific users and domain changes is a facet of the QE problem that so far has been disregarded.", "labels": [], "entities": []}, {"text": "To cope with these issues and deal with the erratic conditions of real-world translation workflows, we propose an adaptive approach to QE that is sensitive and robust to differences between training and test data.", "labels": [], "entities": [{"text": "QE", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.9543092250823975}]}, {"text": "Along this direction, our main contribution is a framework in which QE models can be trained and can continuously evolve overtime accounting for knowledge acquired from post editors' work.", "labels": [], "entities": []}, {"text": "Our approach is based on the online learning paradigm and exploits a key difference between such framework and the batch learning methods currently used.", "labels": [], "entities": []}, {"text": "On one side, the QE models obtained with batch methods are learned exclusively from a predefined set of training examples under the assumption that they have similar characteristics with respect to the test data.", "labels": [], "entities": []}, {"text": "This makes them suitable for controlled evaluation scenarios where such condition holds.", "labels": [], "entities": []}, {"text": "On the other side, online learning techniques are designed to learn in a stepwise manner (either from scratch, or by refining an existing model) from new, unseen test instances by taking advantage of external feedback.", "labels": [], "entities": []}, {"text": "This makes them suitable for real-life scenarios where the new instances to be labelled can considerably differ from the data used to train the QE model.", "labels": [], "entities": []}, {"text": "To develop our approach, different online algorithms have been embedded in the backbone of a QE system.", "labels": [], "entities": []}, {"text": "This required the adaptation of its standard batch learning workflow to: 1.", "labels": [], "entities": []}, {"text": "Perform online feature extraction from a source-target pair (i.e. one instance at a time instead of processing an entire training set); 2.", "labels": [], "entities": []}, {"text": "Emit a prediction for the input instance; 3.", "labels": [], "entities": []}, {"text": "Gather user feedback for the instance (i.e. calculating a \"true label\" based on the amount of user post-editions); 4.", "labels": [], "entities": []}, {"text": "Send the true label back to the model to update its predictions for future instances.", "labels": [], "entities": []}, {"text": "Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach.", "labels": [], "entities": []}, {"text": "The evaluation is carried out by measuring the global error of each algorithm on test sets featuring different degrees of similarity with the data used for training.", "labels": [], "entities": []}, {"text": "Our results show that the sensitivity of online QE models to different distributions of training and test instances makes them more suitable than batch methods for integration in a CAT framework.", "labels": [], "entities": []}, {"text": "Our adaptive QE infrastructure has been released as open source.", "labels": [], "entities": []}, {"text": "Its C++ implementation is available at http://hlt.fbk.eu/technologies/ aqet.", "labels": [], "entities": []}], "datasetContent": [{"text": "To measure the adaptation capability of different QE models, we experiment with a range of conditions defined by variable degrees of similarity between training and test data.", "labels": [], "entities": []}, {"text": "The degree of similarity depends on several factors: the MT engine used, the domain of the documents to be translated, and the post-editing style of individual translators.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9387725591659546}]}, {"text": "In our experiments, the degree of similarity is measured in terms of \u2206HTER, which is computed as the absolute value of the difference between the average HTER of the training and test sets.", "labels": [], "entities": [{"text": "similarity", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.6129499673843384}, {"text": "HTER", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9903894066810608}]}, {"text": "Large values indicate a low similarity between training and test data and a more challenging scenario for the learning algorithms.", "labels": [], "entities": [{"text": "similarity", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9547153115272522}]}, {"text": "In the range of possible evaluation scenarios, our experiments cover: \u2022 One artificial setting ( \u00a75) obtained from the WMT12 QE shared task data, in which training/test instances are arranged to reflect homogeneous distributions of the HTER labels.", "labels": [], "entities": [{"text": "WMT12 QE shared task data", "start_pos": 119, "end_pos": 144, "type": "DATASET", "confidence": 0.8512256741523743}]}, {"text": "\u2022 Two settings obtained from data collected with a CAT tool in real working conditions, in which different facets of the adaptive QE problem interact with each other.", "labels": [], "entities": []}, {"text": "In the first (user change, \u00a76.1), training and test data from the same domain are obtained from different users.", "labels": [], "entities": []}, {"text": "In the sec-ond (user+domain change, \u00a76.2), training and test data are obtained from different users and domains.", "labels": [], "entities": []}, {"text": "For each setting, we compare an adaptive and an empty model against a system trained in batch mode.", "labels": [], "entities": []}, {"text": "The adaptive model is built on top of an existing model created from the training data and exploits the new test instances to refine its predictions in a stepwise manner.", "labels": [], "entities": []}, {"text": "The empty model only learns from the test set, simulating the worst condition where training data is not available.", "labels": [], "entities": []}, {"text": "The batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances.", "labels": [], "entities": []}, {"text": "Each model is also compared against a common baseline for regression tasks, which is particularly relevant in settings featuring different data distributions between training and test sets.", "labels": [], "entities": []}, {"text": "This baseline (\u00b5 henceforth) is calculated by labelling each instance of the test set with the mean HTER score of the training set.", "labels": [], "entities": [{"text": "HTER score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9243646562099457}]}, {"text": "Previous works () demonstrated that its results can be particularly hard to beat.", "labels": [], "entities": []}, {"text": "The motivations for experiments with training and test data featuring homogeneous label distributions are twofold.", "labels": [], "entities": []}, {"text": "First, since in this artificial scenario adaptation capabilities are not required for the QE component, batch methods operate in the ideal conditions (as training and test are indepen-: MAE of the best performing batch, adaptive and empty models on WMT12 data.", "labels": [], "entities": [{"text": "MAE", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.9962177872657776}, {"text": "WMT12 data", "start_pos": 249, "end_pos": 259, "type": "DATASET", "confidence": 0.9746316373348236}]}, {"text": "Training sets of different size and the test set have been arranged to reflect homogeneous label distributions.", "labels": [], "entities": []}, {"text": "dent and identically distributed).", "labels": [], "entities": []}, {"text": "This makes possible to obtain from batch models the best possible performance to compare with.", "labels": [], "entities": []}, {"text": "Second, this scenario provides the fairest conditions for such comparison because, in principle, online algorithms are not favoured by the possibility to learn from the diversity of the test instances.", "labels": [], "entities": []}, {"text": "For our controlled experiments we use the WMT12 English-Spanish corpus, which consists of 2,254 source-target pairs (1,832 for training, 422 for test).", "labels": [], "entities": [{"text": "WMT12 English-Spanish corpus", "start_pos": 42, "end_pos": 70, "type": "DATASET", "confidence": 0.9202795823415121}]}, {"text": "The HTER labels for our regression task are calculated from the post-edited version and the target sentences provided in the dataset.", "labels": [], "entities": [{"text": "HTER", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6961127519607544}]}, {"text": "To avoid biases in the label distribution, the WMT12 training and test data have been merged, shuffled, and eventually separated to generate three training sets of different size (200, 600, and 1500 instances), and one test set with 754 instances.", "labels": [], "entities": [{"text": "WMT12 training and test data", "start_pos": 47, "end_pos": 75, "type": "DATASET", "confidence": 0.7255630016326904}]}, {"text": "For each algorithm, the training sets are used for learning the QE models, optimizing parameters (i.e. C, , the kernel and its parameters for SVR and OSVR; tolerance and aggressiveness for PA) through grid search in 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "Evaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set.", "labels": [], "entities": [{"text": "MAE", "start_pos": 265, "end_pos": 268, "type": "METRIC", "confidence": 0.8878801465034485}]}, {"text": "reports the results achieved by the best performing algorithm for each type of model (batch, adaptive, empty).", "labels": [], "entities": []}, {"text": "As can be seen, close MAE values show a similar behaviour for the three types of models.", "labels": [], "entities": [{"text": "MAE", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9830930233001709}]}, {"text": "11 With the same amount of training data, the performance of the batch and the adaptive models (in this case always obtained with OSVR) is almost identical.", "labels": [], "entities": [{"text": "OSVR", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.8730880618095398}]}, {"text": "This demonstrates that, as expected, the online algorithms do not take Results marked with the \" * \" symbol are NOT statistically significant compared to the corresponding batch model.", "labels": [], "entities": []}, {"text": "The others are always statistically significant at p\u22640.005, calculated with approximate randomization advantage of test data with a label distribution similar to the training set.", "labels": [], "entities": []}, {"text": "All the models outperform the baseline, even if the minimal differences confirm the competitiveness of such a simple approach.", "labels": [], "entities": []}, {"text": "Overall, these results bring some interesting indications about the behaviour of the different online algorithms.", "labels": [], "entities": []}, {"text": "First, the good results achieved by the empty models (less than one MAE point separates them from the best ones built on the largest training set) suggest their high potential when training data are not available.", "labels": [], "entities": [{"text": "MAE", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.98918217420578}]}, {"text": "Second, our results show that OSVR is always the best performing algorithm for the adaptive and empty models.", "labels": [], "entities": [{"text": "OSVR", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.6714001893997192}]}, {"text": "This suggests a lower capability of PA to learn from instances similar to the training data.", "labels": [], "entities": []}, {"text": "To experiment with adaptive QE in more realistic conditions we used a CAT tool to collect two datasets of (source, target, post edited target) English-Italian tuples.The source sentences in the datasets come from two documents from different domains, respectively legal (L) and information technology (IT).", "labels": [], "entities": []}, {"text": "The L document, which was extracted from a European Parliament resolution published on the EUR-Lex platform, 13 contains 164 sentences.", "labels": [], "entities": [{"text": "L", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9239389300346375}, {"text": "EUR-Lex platform", "start_pos": 91, "end_pos": 107, "type": "DATASET", "confidence": 0.9096634685993195}]}, {"text": "The IT document, which was taken from a software user manual, contains 280 sentences.", "labels": [], "entities": [{"text": "IT document", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7272288650274277}]}, {"text": "The source sentences were translated with two SMT systems built by training the Moses toolkit () on parallel data from the two domains (about 2M sentences for IT and 1.5M for L).", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9825822114944458}]}, {"text": "Post-editions were collected from eight professional translators (four for each document) operating with the CAT tool in real working conditions.", "labels": [], "entities": []}, {"text": "According to the way they are created, the two datasets allow us to evaluate the adaptability of different QE models with respect to user changes  within the same domain ( \u00a76.1), as well as user and domain changes at the same time ( \u00a76.2).", "labels": [], "entities": []}, {"text": "For each document D (L or IT), these two scenarios are obtained by dividing D into two parts of equal size (80 instances for Land 140 for IT).", "labels": [], "entities": []}, {"text": "The result is one training set and one test set for each post-editor within the same domain.", "labels": [], "entities": []}, {"text": "For the user change experiments, training and test sets are selected from different post-editors within the same domain.", "labels": [], "entities": []}, {"text": "For the user+domain change experiments, training and test sets are selected from different post-editors in different domains.", "labels": [], "entities": []}, {"text": "On each combination of training and test sets, the batch, adaptive, and empty models are trained and evaluated in terms of global MAE scores on the test set.", "labels": [], "entities": [{"text": "MAE", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.8186187148094177}]}], "tableCaptions": [{"text": " Table 1: MAE of the best performing batch, adaptive and empty models on WMT12 data. Training sets  of different size and the test set have been arranged to reflect homogeneous label distributions.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9943272471427917}, {"text": "WMT12 data", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.9674261212348938}]}, {"text": " Table 2: MAE of the best performing batch, adaptive and empty models on CAT data collected from  different users in the same domain.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9750106930732727}]}, {"text": " Table 3: MAE of the best performing batch, adaptive and empty models on CAT data collected from  different users and domains.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9776048064231873}]}]}