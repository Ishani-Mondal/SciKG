{"title": [{"text": "Comparing Automatic Evaluation Measures for Image Description", "labels": [], "entities": [{"text": "Comparing Automatic Evaluation Measures", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.733938567340374}, {"text": "Image Description", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8060286939144135}]}], "abstractContent": [{"text": "Image description is anew natural language generation task, where the aim is to generate a human-like description of an image.", "labels": [], "entities": [{"text": "Image description", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8743461668491364}, {"text": "natural language generation", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.721224327882131}]}, {"text": "The evaluation of computer-generated text is a notoriously difficult problem, however , the quality of image descriptions has typically been measured using unigram BLEU and human judgements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9537039399147034}]}, {"text": "The focus of this paper is to determine the correlation of automatic measures with human judgements for this task.", "labels": [], "entities": []}, {"text": "We estimate the correlation of unigram and Smoothed BLEU, TER, ROUGE-SU4, and Meteor against human judgements on two data sets.", "labels": [], "entities": [{"text": "Smoothed", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9018551111221313}, {"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.8472121953964233}, {"text": "TER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9974954128265381}, {"text": "ROUGE-SU4", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9937620759010315}]}, {"text": "The main finding is that unigram BLEU has a weak correlation, and Meteor has the strongest correlation with human judgements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9528582692146301}, {"text": "Meteor", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9249153733253479}]}], "introductionContent": [{"text": "Recent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language.", "labels": [], "entities": []}, {"text": "State of the art visual detectors have made it possible to hypothesise what is in an image (, paving the way for automatic image description systems.", "labels": [], "entities": [{"text": "hypothesise what is in an image", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.8237122694651285}]}, {"text": "The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description.", "labels": [], "entities": []}, {"text": "An example of the type of image and gold-standard descriptions available can be seen in.", "labels": [], "entities": []}, {"text": "Recent approaches to this task have been based on slot-filling, combining web-scale ngrams ( ), syntactic tree substitution (), and description-by-retrieval.", "labels": [], "entities": [{"text": "syntactic tree substitution", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.6183975835641226}]}, {"text": "Image description has been compared to translating an image into text ( ) or summarising an image 1.", "labels": [], "entities": [{"text": "Image description", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8621210753917694}]}, {"text": "An older woman with a small dog in the snow.", "labels": [], "entities": []}, {"text": "2. A woman and a cat are outside in the snow.", "labels": [], "entities": []}], "datasetContent": [{"text": "BLEU measures the effective overlap between a reference sentence X and a candidate sentence Y . It is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor BP to penalise short translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9860801100730896}, {"text": "precision scores", "start_pos": 156, "end_pos": 172, "type": "METRIC", "confidence": 0.8953097760677338}, {"text": "brevity penalty factor BP", "start_pos": 192, "end_pos": 217, "type": "METRIC", "confidence": 0.8575076311826706}]}, {"text": "p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text.", "labels": [], "entities": [{"text": "overlap", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.5852862596511841}]}, {"text": "More formally,; to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9981891512870789}]}, {"text": "In this paper we use the smoothed BLEU implementation of Clark et al.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9918754696846008}]}, {"text": "(2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure.", "labels": [], "entities": [{"text": "BLEU measure", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.8770354986190796}, {"text": "Smoothed BLEU measure", "start_pos": 160, "end_pos": 181, "type": "METRIC", "confidence": 0.6773777405420939}]}, {"text": "We note that a higher BLEU score is better.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9706026315689087}]}, {"text": "ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9510478377342224}]}, {"text": "There is also a variant that measures the cooccurrence of pairs of tokens in both the candidate and reference (a skip-bigram): ROUGE-SU*.", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9932708144187927}]}, {"text": "The skip-bigram calculation is parameterised with d skip , the maximum number of tokens between the words in the skip-bigram.", "labels": [], "entities": []}, {"text": "Setting d skip to 0 is equivalent to bigram overlap and setting d skip to \u221e means tokens can be any distance apart.", "labels": [], "entities": []}, {"text": "If \u03b1 = |SKIP2(X,Y )| is the number of matching skip-bigrams between the reference and the candidate, then skip-bigram ROUGE is formally defined as: ROUGE has been used by only to measure the quality of generated descriptions, using a variant they describe as ROUGE-1.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.7531867623329163}]}, {"text": "We set d skip = 4 and award partial credit for unigram only matches, otherwise known as ROUGE-SU4.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9503352046012878}]}, {"text": "We use ROUGE v.1.5.5 for the analysis, and configure the evaluation script to return the result for the average score for matching between the candidate and the references.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9141964316368103}]}, {"text": "A higher ROUGE score is better.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9573467373847961}]}, {"text": "TER measures the number of modifications a human would need to make to transform a candidate Y into a reference X.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9893907904624939}]}, {"text": "The modifications available are insertion, deletion, substitute a single word, and shift a word an arbitrary distance.", "labels": [], "entities": []}, {"text": "TER is expressed as the percentage of the sentence that needs to be changed, and can be greater than 100 if the candidate is longer than the reference.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9919341802597046}]}, {"text": "More formally, TER = |edits| |reference tokens| has not yet been used to evaluate image description models.", "labels": [], "entities": [{"text": "TER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9943358302116394}]}, {"text": "We use v.0.8.0 of the TER evaluation tool, and a lower TER is better.", "labels": [], "entities": [{"text": "TER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.8234673738479614}, {"text": "TER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.998927652835846}]}, {"text": "Meteor is the harmonic mean of unigram precision and recall that allows for exact, synonym, and paraphrase matchings between candidates and references.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9660447239875793}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9969871640205383}]}, {"text": "It is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair.", "labels": [], "entities": []}, {"text": "The alignment is based on exact token matching, followed by Wordnet synonyms, and then stemmed tokens.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9230736494064331}]}, {"text": "We can calculate precision, recall, and F-measure, where m is the number of aligned unigrams between candidate and reference.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9996917247772217}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9995243549346924}, {"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9993253946304321}]}, {"text": "Meteor is defined as: the performance of different models on the image description task; a higher Meteor score is better.", "labels": [], "entities": [{"text": "image description task", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8106365005175272}]}], "tableCaptions": [{"text": " Table 1: Spearman's correlation co-efficient of au- tomatic evaluation measures against human judge- ments. All correlations are significant at p < 0.001.", "labels": [], "entities": []}]}