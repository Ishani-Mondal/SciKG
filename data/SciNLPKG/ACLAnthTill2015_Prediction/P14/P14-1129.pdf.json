{"title": [{"text": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.8339630961418152}]}], "abstractContent": [{"text": "Recent work has shown success in using neural network language models (NNLMs) as features in MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9862418174743652}]}, {"text": "Here, we present a novel formulation fora neural network joint model (NNJM), which augments the NNLM with a source context window.", "labels": [], "entities": []}, {"text": "Our model is purely lexi-calized and can be integrated into any MT decoder.", "labels": [], "entities": []}, {"text": "We also present several variations of the NNJM which provide significant additive improvements.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9656990170478821}]}, {"text": "Although the model is quite simple, it yields strong empirical results.", "labels": [], "entities": []}, {"text": "On the NIST OpenMT12 Arabic-English condition , the NNJM features produce again of +3.0 BLEU on top of a powerful, feature-rich baseline which already includes a target-only NNLM.", "labels": [], "entities": [{"text": "NIST OpenMT12 Arabic-English condition", "start_pos": 7, "end_pos": 45, "type": "DATASET", "confidence": 0.898344948887825}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9973788261413574}]}, {"text": "The NNJM features also produce again of +6.3 BLEU on top of a simpler baseline equivalent to Chi-ang's (2007) original Hiero implementation.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9030970335006714}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.997494101524353}]}, {"text": "Additionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding.", "labels": [], "entities": [{"text": "MT decoding", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.9411058127880096}]}, {"text": "These techniques speedup NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, neural network models have become increasingly popular in NLP.", "labels": [], "entities": []}, {"text": "Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7817268669605255}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7731560468673706}]}, {"text": "They have since been extended to translation modeling, parsing, and many other NLP tasks.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.9891343116760254}, {"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9483014345169067}]}, {"text": "In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature.", "labels": [], "entities": [{"text": "MT decoding", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.91780024766922}]}, {"text": "Specifically, we introduce a novel formulation fora neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window.", "labels": [], "entities": []}, {"text": "Unlike previous approaches to joint modeling (), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements than k-best rescoring only.", "labels": [], "entities": [{"text": "statistical machine translation (SMT) decoder", "start_pos": 95, "end_pos": 140, "type": "TASK", "confidence": 0.7915605774947575}]}, {"text": "Additionally, we present several variations of this model which provide significant additive BLEU gains.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.972905158996582}]}, {"text": "We also present a novel technique for training the neural network to be self-normalized, which avoids the costly step of posteriorizing over the entire vocabulary in decoding.", "labels": [], "entities": []}, {"text": "When used in conjunction with a pre-computed hidden layer, these techniques speedup NNJM computation by a factor of 10,000x, with only a small reduction on MT accuracy.", "labels": [], "entities": [{"text": "MT", "start_pos": 156, "end_pos": 158, "type": "TASK", "confidence": 0.8610894083976746}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.8418249487876892}]}, {"text": "Although our model is quite simple, we obtain strong empirical results.", "labels": [], "entities": []}, {"text": "We show primary results on the NIST OpenMT12 Arabic-English condition.", "labels": [], "entities": [{"text": "NIST OpenMT12 Arabic-English condition", "start_pos": 31, "end_pos": 69, "type": "DATASET", "confidence": 0.8619268238544464}]}, {"text": "The NNJM features produce an improvement of +3.0 BLEU on top of a baseline that is already better than the 1st place MT12 result and includes a powerful NNLM.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9048768877983093}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9988110065460205}, {"text": "MT12", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.698052167892456}, {"text": "NNLM", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.9043019413948059}]}, {"text": "Additionally, on top of a simpler decoder equivalent to original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEUas much as all of the other features in our strong baseline system combined.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8771948218345642}, {"text": "BLEUas", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9978082776069641}]}, {"text": "We also show strong improvements on the NIST OpenMT12 Chinese-English task, as well as the DARPA BOLT (Broad Operational Language Translation) Arabic-English and Chinese-English conditions.", "labels": [], "entities": [{"text": "NIST OpenMT12 Chinese-English task", "start_pos": 40, "end_pos": 74, "type": "DATASET", "confidence": 0.8340375125408173}, {"text": "DARPA", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.8959407210350037}, {"text": "BOLT", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.5400276184082031}, {"text": "Broad Operational Language Translation) Arabic-English", "start_pos": 103, "end_pos": 157, "type": "TASK", "confidence": 0.6215080966552099}]}], "datasetContent": [{"text": "We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9707032442092896}, {"text": "NIST OpenMT12", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.7806800603866577}, {"text": "DARPA", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.5608365535736084}, {"text": "BOLT", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.8060312271118164}]}, {"text": "We also present a set of auxiliary results in order to further analyze our features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of neural network likelihood  for various \u03b1 values. log(P", "labels": [], "entities": []}, {"text": " Table 2: Speed of the neural network computa- tion on a single CPU thread. \"lookups/sec\" is the  number of unique n-gram probabilities that can be  computed per second. \"sec/word\" is the amortized  cost of unique NNJM lookups in decoding, per  source word.", "labels": [], "entities": []}, {"text": " Table 3: Primary results on Arabic-English and  Chinese-English NIST MT12 Test Set. The first  section corresponds to the top and bottom ranked  systems from the evaluation, and are taken from  the NIST website. The second section corresponds  to results on top of our strongest baseline. The  third section corresponds to results on top of a  simpler baseline. Within each section, each row  includes all of the features from previous rows.  BLEU scores are mixed-case.", "labels": [], "entities": [{"text": "NIST MT12 Test Set", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.8901497274637222}, {"text": "NIST website", "start_pos": 199, "end_pos": 211, "type": "DATASET", "confidence": 0.9716390371322632}, {"text": "BLEU", "start_pos": 444, "end_pos": 448, "type": "METRIC", "confidence": 0.9984849095344543}]}, {"text": " Table 5: Comparison of our primary NNJM in de- coding vs. 1000-best rescoring.", "labels": [], "entities": []}, {"text": " Table 6: Results with different neural net- work architectures. The \"default\" NNJM in  the second row uses these parameters: SW=11,  L=192x512x512, V=32,000, A=tanh. All mod- els use a 3-word target history (i.e., 4-gram LM).  \"Layers\" refers to the size of the word embedding  followed by the hidden layers. \"Vocab\" refers to  the size of the input and output vocabularies. \"%  Gain\" is the BLEU gain over the baseline relative  to the default NNJM.", "labels": [], "entities": [{"text": "BLEU gain", "start_pos": 393, "end_pos": 402, "type": "METRIC", "confidence": 0.9773582220077515}]}]}