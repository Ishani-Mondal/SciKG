{"title": [{"text": "How Well can We Learn Interpretable Entity Types from Text?", "labels": [], "entities": []}], "abstractContent": [{"text": "Many NLP applications rely on type systems to represent higher-level classes.", "labels": [], "entities": []}, {"text": "Domain-specific ones are more informative , but have to be manually tailored to each task and domain, making them inflexible and expensive.", "labels": [], "entities": []}, {"text": "We investigate a largely unsupervised approach to learning interpretable, domain-specific entity types from unlabeled text.", "labels": [], "entities": []}, {"text": "It assumes that any common noun in a domain can function as potential entity type, and uses those nouns as hidden variables in a HMM.", "labels": [], "entities": []}, {"text": "To constrain training, it extracts co-occurrence dictionaries of entities and common nouns from the data.", "labels": [], "entities": []}, {"text": "We evaluate the learned types by measuring their prediction accuracy for verb arguments in several domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9397186636924744}]}, {"text": "The results suggest that it is possible to learn domain-specific entity types from unlabeled data.", "labels": [], "entities": []}, {"text": "We show significant improvements over an informed baseline, reducing the error rate by 56%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9915202260017395}]}], "introductionContent": [{"text": "Many NLP applications, such as question answering (QA) or information extraction (IE), use type systems to represent relevant semantic classes.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.8853280425071717}, {"text": "information extraction (IE)", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.8469174146652222}]}, {"text": "Types allow us to find similarities at a higher level to group lexically different entities together.", "labels": [], "entities": []}, {"text": "This helps to filter out candidates that violate certain constraints (e.g., in QA, if the intended answer type is PERSON, we can ignore all candidate answers with a different type), but is also used for feature generation and fact-checking.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.7871915400028229}]}, {"text": "A central question is: where do the types come from?", "labels": [], "entities": []}, {"text": "Typically, they come from a handconstructed set.", "labels": [], "entities": []}, {"text": "Domain-general types, such as named entities or WordNet supersenses, often fail to capture critical domain-specific information (in the medical domain, we might want ANTIBI-OTIC, SEDATIVE, etc., rather than just ARTI-FACT).", "labels": [], "entities": [{"text": "SEDATIVE", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.7519544959068298}]}, {"text": "Domain-specific types perform much better), but must be manually adapted to each new domain, which is expensive.", "labels": [], "entities": []}, {"text": "Alternatively, unsupervised approaches ( can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult.", "labels": [], "entities": []}, {"text": "Furthermore, it requires us to define the number of clusters beforehand.", "labels": [], "entities": []}, {"text": "Ideally, we would like to learn domain-specific types directly from data.", "labels": [], "entities": []}, {"text": "To this end, patternbased approaches have long been used to induce type systems.", "labels": [], "entities": []}, {"text": "Recently, proposed an approach that uses co-occurrence patterns to find entity type candidates, and then learns their applicability to relation arguments by using them as latent variables in a first-order HMM.", "labels": [], "entities": []}, {"text": "However, they only evaluate their method using human sensibility judgements for one domain.", "labels": [], "entities": []}, {"text": "While this shows that the types are coherent, it does not tell us much about their applicability.", "labels": [], "entities": []}, {"text": "We extend their approach with three important changes: 1.", "labels": [], "entities": []}, {"text": "we evaluate the types by measuring accuracy when using them in an extrinsic task, 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9990956783294678}]}, {"text": "we evaluate on more than one domain, and 3.", "labels": [], "entities": []}, {"text": "we explore a variety of different models.", "labels": [], "entities": []}, {"text": "We measure prediction accuracy when using the learned types in a selectional restriction task for frequent verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9503083825111389}]}, {"text": "E.g., given the relation throw(X, pass) in the football domain, we compare the model prediction to the gold data X=QUARTERBACK.", "labels": [], "entities": [{"text": "QUARTERBACK", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9442294239997864}]}, {"text": "The results indicate that the learned types can be used to in relation extraction tasks.", "labels": [], "entities": [{"text": "relation extraction tasks", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.8937564094861349}]}], "datasetContent": [{"text": "Since the labels are induced dynamically from the data, traditional precision/recall measures, which require a known ground truth, are difficult to obtain.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9989414811134338}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.940789520740509}]}, {"text": "(2011) measured sensibility by obtaining human ratings and measuring weighted accuracies overall relations.", "labels": [], "entities": []}, {"text": "While this gives an intuition of the general methodology, it is harder to put in context.", "labels": [], "entities": []}, {"text": "Here, we want to evaluate the model's performance in a downstream task.", "labels": [], "entities": []}, {"text": "We measure its ability to predict the correct types for verbal arguments.", "labels": [], "entities": []}, {"text": "We evaluate on three different domains.", "labels": [], "entities": []}, {"text": "As test case, we use a cloze test, or fill-in-theblank.", "labels": [], "entities": []}, {"text": "We select instances that contain a typecandidate word in subject or object position and replace that word with the unknown token.", "labels": [], "entities": []}, {"text": "We can then compare the model's prediction to the original word to measure accuracy., we use articles whose content metadata field contains certain labels to distinguish data from different domains.", "labels": [], "entities": [{"text": "accuracy.", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9993664622306824}]}, {"text": "We use the labels Football 2 , Law and Legislation, and Finances.", "labels": [], "entities": []}, {"text": "We run Viterbi decoding on each test set with our trained model to predict the most likely type for the unknown entities.", "labels": [], "entities": []}, {"text": "We then compare these predictions to the type in the respective gold data and compute the accuracy for each argument position.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9996641874313354}]}, {"text": "As baseline, we predict the argument types most frequently observed for the particular verb in training, e.g., predict PLAYER as subject of tackle in football.", "labels": [], "entities": [{"text": "PLAYER", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9774168729782104}]}, {"text": "We evaluate the influence of the different model structures on performance.", "labels": [], "entities": []}, {"text": "shows the accuracy of the different models in the prediction task for the three different domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999417781829834}]}, {"text": "The low results of the informed baseline indicate the task complexity.", "labels": [], "entities": []}, {"text": "We note that the original model, a bigram HMM with SVO order, fails to improve accuracy over the baseline (although its overall results were judged sensible).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994032382965088}]}, {"text": "Changing the input order to VSO) improves accuracy for both arguments over SVO order and the baseline, albeit not significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9989890456199646}]}, {"text": "The first argument gains more, since conditioning the subject type on the (unambiguous) verb is more constrained than starting outwith the subject.", "labels": [], "entities": []}, {"text": "Conditioning the object directly upon the subject creates sparser bigrams, which capture \"who does what to whom\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy for most frequent sense baseline and different models on three domains. Italic num- bers denote significant improvement over baseline (two-tailed t-test at p < 0.01). \u2206BL = difference to  baseline.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986841082572937}, {"text": "Italic num- bers", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.6779028102755547}, {"text": "BL", "start_pos": 187, "end_pos": 189, "type": "METRIC", "confidence": 0.8797709345817566}]}, {"text": " Table 2: Mean reciprocal rank for models on three domains.", "labels": [], "entities": [{"text": "Mean reciprocal rank", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9294208685557047}]}, {"text": " Table 3: Statistics for the three domains.", "labels": [], "entities": []}]}