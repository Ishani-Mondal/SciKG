{"title": [], "abstractContent": [{"text": "Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree.", "labels": [], "entities": [{"text": "Text-level discourse parsing", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6729865272839864}]}, {"text": "In this paper, we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units (EDUs).", "labels": [], "entities": [{"text": "constituency based discourse parsing", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6870475113391876}]}, {"text": "The state-of-the-art dependency parsing techniques, the Eisner algorithm and maximum spanning tree (MST) algorithm, are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7846505641937256}]}, {"text": "Experiments show that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing.", "labels": [], "entities": [{"text": "discourse dependency parsers", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6639053026835123}, {"text": "text-level discourse parsing", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.6180302401383718}]}], "introductionContent": [{"text": "It is widely agreed that no units of the text can be understood in isolation, but in relation to their context.", "labels": [], "entities": []}, {"text": "Researches in discourse parsing aim to acquire such relations in text, which is fundamental to many natural language processing applications such as question answering, automatic summarization and soon.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7213141769170761}, {"text": "question answering", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.9051215052604675}, {"text": "summarization", "start_pos": 179, "end_pos": 192, "type": "TASK", "confidence": 0.6540944576263428}]}, {"text": "One important issue behind discourse parsing is the representation of discourse structure.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6963552087545395}]}, {"text": "Rhetorical Structure Theory (RST) (, one of the most influential discourse theories, posits a hierarchical generative tree representation, as illustrated in.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.834269200762113}, {"text": "generative tree representation", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.7767565846443176}]}, {"text": "The leaves of a tree correspond to contiguous text spans called Elementary Discourse Units (EDUs) . The adjacent EDUs are combined into EDU segmentation is a relatively trivial step in discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 185, "end_pos": 202, "type": "TASK", "confidence": 0.7078187167644501}]}, {"text": "Since our work focus here is not EDU segmentation but discourse parsing.", "labels": [], "entities": [{"text": "EDU segmentation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7502669394016266}, {"text": "discourse parsing", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7166419178247452}]}, {"text": "We assume EDUs are already known.", "labels": [], "entities": []}, {"text": "the larger text spans by rhetorical relations (e.g., Contrast and Elaboration) and the larger text spans continue to be combined until the whole text constitutes a parse tree.", "labels": [], "entities": []}, {"text": "The text spans linked by rhetorical relations are annotated as either nucleus or satellite depending on how salient they are for interpretation.", "labels": [], "entities": []}, {"text": "It is attractive and challenging to parse the whole text into one tree.", "labels": [], "entities": []}, {"text": "Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing.", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 266, "end_pos": 279, "type": "TASK", "confidence": 0.764506995677948}, {"text": "discourse parsing", "start_pos": 336, "end_pos": 353, "type": "TASK", "confidence": 0.710180938243866}]}, {"text": "However, the existing approaches suffer from at least one of the following three problems.", "labels": [], "entities": []}, {"text": "First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7364150881767273}]}, {"text": "Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.7084679901599884}]}, {"text": "Third, to reduce the time complexity of the state-of-the-art constituency based parsing techniques, the approximate parsing approaches are prone to trap in local maximum.", "labels": [], "entities": []}, {"text": "In this paper, we propose to adopt the dependency structure in discourse representation to overcome the limitations mentioned above.", "labels": [], "entities": []}, {"text": "Here is the basic idea: the discourse structure consists of EDUs which are linked by the binary, asymmetrical relations called dependency relations.", "labels": [], "entities": []}, {"text": "A dependency relation holds between a subordinate EDU called the dependent, and another EDU on which it depends called the head, as illustrated in.", "labels": [], "entities": []}, {"text": "Each EDU has one head.", "labels": [], "entities": []}, {"text": "So, the dependency structure can be seen as a set of headdependent links, which are labeled by functional relations.", "labels": [], "entities": []}, {"text": "Now, we can analyze the relations between EDUs directly, without worrying about any interior text spans.", "labels": [], "entities": []}, {"text": "Since dependency trees contain much fewer nodes and on average they are simpler than constituency based trees, the current dependency parsers can have a relatively low computational complexity.", "labels": [], "entities": []}, {"text": "Moreover, concerning linearization, it is well known that dependency structures can deal with non-projective relations, while constituency-based models need the addition of complex mechanisms like transformations, movements and soon.", "labels": [], "entities": []}, {"text": "In our work, we adopt the graph based dependency parsing techniques learned from large sets of annotated dependency trees.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7439896762371063}]}, {"text": "The Eisner (1996) algorithm and maximum spanning tree (MST) algorithm are used respectively to parse the optimal projective and non-projective dependency trees with the large-margin learning technique.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to apply the dependency structure and introduce the dependency parsing techniques into discourse analysis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7600429356098175}, {"text": "discourse analysis", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7360551357269287}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 formally defines discourse dependency structure and introduces how to build a discourse dependency treebank from the existing RST corpus.", "labels": [], "entities": []}, {"text": "Section 3 presents the discourse parsing approach based on the Eisner and MST algorithms.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7100147902965546}]}, {"text": "Section 4 elaborates on the large-margin learning technique as well as the features we use.", "labels": [], "entities": []}, {"text": "Section 5 discusses the experimental results.", "labels": [], "entities": []}, {"text": "Section 6 introduces the related work and Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Coarse-grained Relation Distribution", "labels": [], "entities": [{"text": "Coarse-grained Relation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8117007613182068}]}, {"text": " Table 2: 10 Highest Distributed Fine-grained  Relations", "labels": [], "entities": []}, {"text": " Table 4: Performance Using Fine-grained Rela- tions.", "labels": [], "entities": []}, {"text": " Table 5: Top 10 Feature Weights for Coarse- grained Relation Labeling (Eisner Algorithm)", "labels": [], "entities": [{"text": "Coarse- grained Relation Labeling", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.6817356884479523}]}, {"text": " Table 6: Top 10 Feature Weights for Coarse- grained Relation Labeling (Eisner Algorithm)", "labels": [], "entities": [{"text": "Coarse- grained Relation Labeling", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.68208909034729}]}, {"text": " Table 7: Full Parser Evaluation", "labels": [], "entities": []}, {"text": " Table 8: Relation Labeling Performance", "labels": [], "entities": [{"text": "Relation Labeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9828205108642578}]}]}