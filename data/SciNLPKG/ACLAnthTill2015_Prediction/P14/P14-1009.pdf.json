{"title": [{"text": "A practical and linguistically-motivated approach to compositional distributional semantics", "labels": [], "entities": [{"text": "compositional distributional semantics", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.6298005779584249}]}], "abstractContent": [{"text": "Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences.", "labels": [], "entities": []}, {"text": "We present here anew model that, like those of Co-ecke et al.", "labels": [], "entities": []}, {"text": "(2010) and Baroni and Zam-parelli (2010), closely mimics the standard Montagovian semantic treatment of composition in distributional terms.", "labels": [], "entities": []}, {"text": "However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences.", "labels": [], "entities": []}, {"text": "We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals.", "labels": [], "entities": []}, {"text": "1 Compositional distributional semantics The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used to represent word meaning in a variety of tasks (Turney and Pantel, 2010).", "labels": [], "entities": []}, {"text": "If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors.", "labels": [], "entities": []}, {"text": "Developing a practical model of compositionality is still an open issue, which we address in this paper.", "labels": [], "entities": []}, {"text": "One approach is to use simple, parameter-free models that perform operations such as point-wise multiplication or summing (Mitchell and La-pata, 2008).", "labels": [], "entities": [{"text": "summing", "start_pos": 114, "end_pos": 121, "type": "TASK", "confidence": 0.9496251940727234}]}, {"text": "Such models turnout to be surprisingly effective in practice (Blacoe and Lap-ata, 2012), but they have obvious limitations.", "labels": [], "entities": []}, {"text": "For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition: pandas eat bamboo is identical to bamboo eats pandas.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7602488696575165}]}, {"text": "Guevara (2010), Mitchell and Lapata (2010), Socher et al.", "labels": [], "entities": []}, {"text": "(2011) and Zanzotto et al.", "labels": [], "entities": []}, {"text": "(2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model.", "labels": [], "entities": []}, {"text": "A related approach (Socher et al., 2012) assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister.", "labels": [], "entities": []}, {"text": "The training proposed in this model estimates the parameters in a supervised setting.", "labels": [], "entities": []}, {"text": "Despite positive empirical evaluation , this approach is hardly practical for general-purpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks.", "labels": [], "entities": [{"text": "general-purpose semantic language processing", "start_pos": 78, "end_pos": 122, "type": "TASK", "confidence": 0.6194766014814377}]}], "introductionContent": [], "datasetContent": [{"text": "We consider 5 different benchmarks that focus on different aspects of sentence-level semantic composition.", "labels": [], "entities": [{"text": "sentence-level semantic composition", "start_pos": 70, "end_pos": 105, "type": "TASK", "confidence": 0.6796889305114746}]}, {"text": "The first data set, created by Edward Grefenstette and Mehrnoosh Sadrzadeh and introduced in , features 200 sentence pairs that were rated for similarity by 43 annotators.", "labels": [], "entities": []}, {"text": "In this data set, sentences have fixed adjective-noun-verb-adjective-noun (anvan) structure, and they were builtin order to crucially require context-based verb disambiguation (e.g., young woman filed long nails is paired with both young woman smoothed long nails and young woman registered long nails).", "labels": [], "entities": [{"text": "context-based verb disambiguation", "start_pos": 142, "end_pos": 175, "type": "TASK", "confidence": 0.6946584184964498}]}, {"text": "We also consider a similar data set introduced by, comprising 200 sentence pairs rated by 50 annotators.", "labels": [], "entities": []}, {"text": "We will call these benchmarks anvan1 and anvan2, respectively.", "labels": [], "entities": []}, {"text": "Evaluation is carried out by computing the Spearman correlation between the annotator similarity ratings for the sentence pairs and the cosines of the vectors produced by the various systems for the same sentence pairs.", "labels": [], "entities": []}, {"text": "The benchmark introduced by The  at the TFDS workshop (tfds below) was specifically designed to test compositional methods for their sensitivity to word order and the semantic effect of determiners.", "labels": [], "entities": [{"text": "TFDS workshop", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9260545372962952}]}, {"text": "The tfds benchmark contains 157 target sentences that are matched with a set of (approximate) paraphrases (8 on av-erage), and a set of \"foils\" (17 on average).", "labels": [], "entities": [{"text": "foils", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.994337260723114}]}, {"text": "The foils have high lexical overlap with the targets but very different meanings, due to different determiners and/or word order.", "labels": [], "entities": []}, {"text": "For example, the target A man plays an acoustic guitar is matched with paraphrases such as A man plays guitar and The man plays the guitar, and foils such as The man plays no guitar and A guitar plays a man.", "labels": [], "entities": []}, {"text": "A good system should return higher similarities for the comparison with the paraphrases with respect to that with the foils.", "labels": [], "entities": [{"text": "similarities", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.9828575849533081}]}, {"text": "Performance is assessed through the t-standardized cross-target average of the difference between mean cosine with paraphrases and mean cosine with foils (Pham and colleagues, equivalently, reported non-standardized average and standard deviations).", "labels": [], "entities": []}, {"text": "The two remaining data sets are larger and more 'natural', as they were not constructed by linguists under controlled conditions to focus on specific phenomena.", "labels": [], "entities": []}, {"text": "They are aimed at evaluating systems on the sort of free-form sentences one encounters in real-life applications.", "labels": [], "entities": []}, {"text": "The msrvid data set from the SemEval-2012 Semantic Textual Similarity (STS) task consists of 750 sentence pairs that describe brief videos.", "labels": [], "entities": [{"text": "msrvid data set", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7586197853088379}, {"text": "SemEval-2012 Semantic Textual Similarity (STS) task", "start_pos": 29, "end_pos": 80, "type": "TASK", "confidence": 0.835940420627594}]}, {"text": "Sentence pairs were scored for similarity by 5 subjects each.", "labels": [], "entities": [{"text": "similarity", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9639372229576111}]}, {"text": "Following standard practice in paraphrase detection studies (e.g.,), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9309334456920624}]}, {"text": "We obtain a final similarity score by weighted addition of the 3 cues, with the optimal weights determined by linear regression on separate msrvid train data that were also provided by the SemEval task organizers (before combining, we checked that the collinearity between cues was low).", "labels": [], "entities": []}, {"text": "System scores are evaluated by their Pearson correlation with the human ratings.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.9770855903625488}]}, {"text": "The final set we use is onwn, from the *SEM-2013 STS shared task ().", "labels": [], "entities": [{"text": "SEM-2013 STS shared task", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.6259099245071411}]}, {"text": "This set contains 561 pairs of glosses (from the WordNet and OntoNotes databases), rated by 5 judges for similarity.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9739086031913757}, {"text": "OntoNotes databases", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.7974101305007935}, {"text": "similarity", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9871305823326111}]}, {"text": "Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences (consider, e.g., cause something to pass or lead somewhere; coerce by violence, fill with terror).", "labels": [], "entities": []}, {"text": "For this reason, they are very challenging for standard parsers.", "labels": [], "entities": []}, {"text": "Indeed, we estimated from a sample of 40 onwn glosses that the C&C parser (see below) has only 45% accuracy on this set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9993230104446411}]}, {"text": "Since plf needs syntactic information to construct sentence vectors compositionally, we test it on onwn to make sure that it is not overly sensitive to parser noise.", "labels": [], "entities": []}, {"text": "Evaluation proceeds as with msrvid (cue weights are determined by 10-fold cross-validation).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of function application.", "labels": [], "entities": []}]}