{"title": [{"text": "A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing", "labels": [], "entities": []}], "abstractContent": [{"text": "Text-level discourse parsing remains a challenge.", "labels": [], "entities": [{"text": "Text-level discourse parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6252889633178711}]}, {"text": "The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.999471127986908}, {"text": "relation assignment", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8949887156486511}]}, {"text": "However, their model has a high order of time complexity, and thus cannot be applied in practice.", "labels": [], "entities": []}, {"text": "In this work, we develop a much faster model whose time complexity is linear in the number of sentences.", "labels": [], "entities": []}, {"text": "Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade as local classifiers.", "labels": [], "entities": []}, {"text": "To enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9992431402206421}]}, {"text": "In addition to efficiency , our parser also significantly out-performs the state of the art.", "labels": [], "entities": []}, {"text": "Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9980389475822449}]}], "introductionContent": [{"text": "Discourse parsing is the task of identifying the presence and the type of the discourse relations between discourse units.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7780589461326599}]}, {"text": "While research in discourse parsing can be partitioned into several directions according to different theories and frameworks, Rhetorical Structure Theory (RST) () is probably the most ambitious one, because it aims to identify not only the discourse relations in a small local context, but also the hierarchical tree structure for the full text: from the relations relating the smallest discourse units (called elementary discourse units, EDUs), to the ones connecting paragraphs.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7307573854923248}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 127, "end_pos": 160, "type": "TASK", "confidence": 0.7823166251182556}]}, {"text": "For example, shows a text fragment consisting of two sentences with four EDUs in total (e 1 -e 4 ).", "labels": [], "entities": []}, {"text": "Its discourse tree representation is shown below the text, following the notation convention of RST: the two EDUs e 1 and e 2 are related by a mononuclear relation CONSEQUENCE, where e 2 is the more salient span (called nucleus, and e 1 is called satellite); e 3 and e 4 are related by another mononuclear relation CIRCUMSTANCE, withe 4 as the nucleus; the two spans e 1:2 and e 3:4 are further related by a multi-nuclear relation SE-QUENCE, with both spans as the nucleus.", "labels": [], "entities": []}, {"text": "Conventionally, there are two major sub-tasks related to text-level discourse parsing: (1) EDU segmentation: to segment the raw text into EDUs, and (2) tree-building: to build a discourse tree from EDUs, representing the discourse relations in the text.", "labels": [], "entities": [{"text": "text-level discourse parsing", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.6165323257446289}, {"text": "EDU segmentation", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.7041182219982147}]}, {"text": "Since the first sub-task is considered relatively easy, with the state-of-art accuracy at above 90% (, the recent research focus is on the second sub-task, and often uses manual EDU segmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9988400340080261}, {"text": "EDU segmentation", "start_pos": 178, "end_pos": 194, "type": "TASK", "confidence": 0.6455687135457993}]}, {"text": "The current state-of-the-art overall accuracy of the tree-building sub-task, evaluated on the RST Discourse Treebank (RST-DT, to be introduced in Section 8), is 55.73% by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9991315007209778}, {"text": "RST Discourse Treebank (RST-DT", "start_pos": 94, "end_pos": 124, "type": "DATASET", "confidence": 0.8714046716690064}]}, {"text": "However, as an optimal discourse parser, Joty et al.'s model is highly inefficient in practice, with respect to both their DCRF-based local classifiers, and their CKY-like bottom-up parsing algorithm.", "labels": [], "entities": [{"text": "discourse parser", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.6671004444360733}, {"text": "CKY-like bottom-up parsing", "start_pos": 163, "end_pos": 189, "type": "TASK", "confidence": 0.5470936497052511}]}, {"text": "DCRF (Dynamic Conditional Random Fields) is a generalization of linear-chain CRFs, in which each time slice contains a set of state variables and edges.", "labels": [], "entities": []}, {"text": "CKY parsing is a bottom-up parsing algorithm which searches all possible parsing paths by dynamic programming.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8015182614326477}]}, {"text": "Therefore, despite its superior performance, their model is infeasible inmost realistic situations.", "labels": [], "entities": []}, {"text": "The main objective of this work is to develop a more efficient discourse parser, with similar or even better performance with respect to Joty et al.'s optimal parser, but able to produce parsing results in real time.", "labels": [], "entities": [{"text": "discourse parser", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7004342973232269}]}, {"text": "First, with a Figure 1: An example text fragment composed of two sentences and four EDUs, with its RST discourse tree representation shown below.", "labels": [], "entities": []}, {"text": "greedy bottom-up strategy, we develop a discourse parser with a time complexity linear in the total number of sentences in the document.", "labels": [], "entities": []}, {"text": "As a result of successfully avoiding the expensive nongreedy parsing algorithms, our discourse parser is very efficient in practice.", "labels": [], "entities": [{"text": "discourse parser", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7147322297096252}]}, {"text": "Second, by using two linear-chain CRFs to label a sequence of discourse constituents, we can incorporate contextual information in a more natural way, compared to using traditional discriminative classifiers, such as SVMs.", "labels": [], "entities": []}, {"text": "Specifically, in the Viterbi decoding of the first CRF, we include additional constraints elicited from commonsense, to make more effective local decisions.", "labels": [], "entities": []}, {"text": "Third, after a discourse (sub)tree is fully built from bottom up, we perform a novel post-editing process by considering information from the constituents on upper levels.", "labels": [], "entities": []}, {"text": "We show that this post-editing can further improve the overall parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.96234130859375}]}], "datasetContent": [{"text": "For pre-processing, we use the Stanford CoreNLP () to syntactically parse the texts and extract coreference relations, and we use Penn2Malt 5 to lexicalize syntactic trees to extract dominance features.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9086498320102692}, {"text": "Penn2Malt 5", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.9375554919242859}]}, {"text": "For local models, our structure models are trained using MALLET) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite, which is a fast implementation of linear-chain CRFs.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.8768856525421143}]}, {"text": "The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT)), which is a large corpus annotated in the framework of RST.", "labels": [], "entities": [{"text": "RST Discourse Treebank (RST-DT))", "start_pos": 73, "end_pos": 105, "type": "DATASET", "confidence": 0.7993668168783188}]}, {"text": "The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.9594559272130331}]}, {"text": "Following previous work on the RST-DT (, we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations.", "labels": [], "entities": []}, {"text": "Non-binary relations are converted into a cascade of right-branching binary relations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of different models using  gold-standard EDU segmentation, evaluated us- ing the constituent accuracy (%) for span, nucle- arity, and relation. For relation, we also report the  macro-averaged F1-score (MAFS) for correctly  retrieved constituents (before the slash) and for  all constituents (after the slash). Statistical sig- nificance is verified using Wilcoxon's signed-rank  test.", "labels": [], "entities": [{"text": "EDU segmentation", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.6459937393665314}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.8432750701904297}, {"text": "F1-score (MAFS)", "start_pos": 215, "end_pos": 230, "type": "METRIC", "confidence": 0.9165367037057877}]}, {"text": " Table 2: Characteristics of the 38 documents in the  test data.", "labels": [], "entities": []}, {"text": " Table 3: The parsing time (in seconds) for the 38  documents in the test set of RST-DT. Time cost of  any pre-processing is excluded from the analysis.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9638524055480957}, {"text": "RST-DT", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.610130786895752}]}]}