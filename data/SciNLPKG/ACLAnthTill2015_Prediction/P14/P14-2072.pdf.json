{"title": [], "abstractContent": [{"text": "In this paper, we address the task of cross-cultural deception detection.", "labels": [], "entities": [{"text": "cross-cultural deception detection", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.8430687189102173}]}, {"text": "Using crowdsourcing, we collect three deception datasets, two in English (one originating from United States and one from India), and one in Spanish obtained from speakers from Mexico.", "labels": [], "entities": []}, {"text": "We run comparative experiments to evaluate the accuracies of deception classifiers built for each culture, and also to analyze classification differences within and across cultures.", "labels": [], "entities": []}, {"text": "Our results show that we can leverage cross-cultural information, either through translation or equivalent semantic categories, and build deception classifiers with a performance ranging between 60-70%.", "labels": [], "entities": []}], "introductionContent": [{"text": "The identification of deceptive behavior is a task that has gained increasing interest from researchers in computational linguistics.", "labels": [], "entities": [{"text": "identification of deceptive behavior", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.865296870470047}]}, {"text": "This is mainly motivated by the rapid growth of deception in written sources, and in particular in Web content, including product reviews, online dating profiles, and social networks posts.", "labels": [], "entities": []}, {"text": "To date, most of the work presented on deception detection has focused on the identification of deceit clues within a specific language, where English is the most commonly studied language.", "labels": [], "entities": [{"text": "deception detection", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8762463629245758}, {"text": "identification of deceit clues within a specific language", "start_pos": 78, "end_pos": 135, "type": "TASK", "confidence": 0.8204841762781143}]}, {"text": "However, a large portion of the written communication (e.g., e-mail, chats, forums, blogs, social networks) occurs not only between speakers of English, but also between speakers from other cultural backgrounds, which poses important questions regarding the applicability of existing deception tools.", "labels": [], "entities": []}, {"text": "Issues such as language, beliefs, and moral values may influence the way people deceive, and therefore may have implications on the construction of tools for deception detection.", "labels": [], "entities": [{"text": "deception detection", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.8970634043216705}]}, {"text": "In this paper, we explore within-and acrossculture deception detection for three different cultures, namely United States, India, and Mexico.", "labels": [], "entities": [{"text": "acrossculture deception detection", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.7515285809834799}]}, {"text": "Through several experiments, we compare the performance of classifiers that are built separately for each culture, and classifiers that are applied across cultures, by using unigrams and word categories that can act as a cross-lingual bridge.", "labels": [], "entities": []}, {"text": "Our results show that we can achieve accuracies in the range of 60-70%, and that we can leverage resources available in one language to build deception tools for another language.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9932714104652405}]}], "datasetContent": [{"text": "We collect three datasets for three different cultures: United States (English-US), India (EnglishIndia), and Mexico (Spanish-Mexico).", "labels": [], "entities": []}, {"text": "Following, we collect short deceptive and truthful essays for three topics: opinions on Abortion, opinions on Death Penalty, and feelings about a Best Friend.", "labels": [], "entities": [{"text": "Abortion", "start_pos": 88, "end_pos": 96, "type": "TASK", "confidence": 0.8944365978240967}, {"text": "Death Penalty", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.7229125499725342}]}, {"text": "For English-US and English-India, we use Amazon Mechanical Turk with a location restriction, so that all the contributors are from the country of interest (US and India).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.9413313269615173}]}, {"text": "We collect 100 deceptive and 100 truthful statements for each of the three topics.", "labels": [], "entities": []}, {"text": "To avoid spam, each contribution is manually verified by one of the authors of this paper.For SpanishMexico, while we initially attempted to collect data also using Mechanical Turk, we were notable to receive enough contributions.", "labels": [], "entities": [{"text": "SpanishMexico", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9483569264411926}]}, {"text": "We therefore created a separate web interface to collect data, and recruited participants through contacts of the paper's authors.", "labels": [], "entities": []}, {"text": "The overall process was significantly more time consuming than for the other two cultures, and resulted in fewer contributions, namely 39+39 statements for Abortion, 42+42 statements for Death Penalty, and 94+94 statements for Best Friend.", "labels": [], "entities": [{"text": "Abortion", "start_pos": 156, "end_pos": 164, "type": "TASK", "confidence": 0.8770643472671509}]}, {"text": "For all three cultures, the participants first provided their truthful responses, followed by the deceptive ones.", "labels": [], "entities": []}, {"text": "Interestingly, for all three cultures, the average number of words for the deceptive statements (62 words) is significantly smaller than for the truthful statements (81 words), which maybe explained by the added difficulty of the deceptive process, and is inline with previous observations about the cues of deception ().", "labels": [], "entities": []}, {"text": "Through our experiments, we seek answers to the following questions.", "labels": [], "entities": []}, {"text": "First, what is the performance for deception classifiers built for different cultures?", "labels": [], "entities": []}, {"text": "Second, can we use information drawn from one culture to build a deception classifier for another culture?", "labels": [], "entities": []}, {"text": "Finally, what are the psycholinguistic classes most strongly associated with deception/truth, and are there commonalities or differences among languages?", "labels": [], "entities": []}, {"text": "In all our experiments, we formulate the deception detection task in a machine learning framework, where we use an SVM classifier to discriminate between deceptive and truthful statements.", "labels": [], "entities": [{"text": "deception detection task", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8785376151402792}]}], "tableCaptions": [{"text": " Table 1: Within-culture classification, using LIWC word classes and unigrams. For LIWC, results are  shown for within-topic experiments, with ten-fold cross validation. For unigrams, both within-topic  (ten-fold cross validation on the same topic) and cross-topic (training on two topics and testing on the  third topic) results are reported.", "labels": [], "entities": [{"text": "Within-culture classification", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7790051102638245}]}, {"text": " Table 2: Cross-cultural experiments using LIWC categories and unigrams", "labels": [], "entities": []}]}