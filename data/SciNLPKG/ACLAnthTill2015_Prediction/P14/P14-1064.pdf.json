{"title": [{"text": "Graph-based Semi-Supervised Learning of Translation Models from Monolingual Data", "labels": [], "entities": [{"text": "Graph-based Semi-Supervised Learning of Translation Models", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.570388654867808}]}], "abstractContent": [{"text": "Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolin-gual evidence to construct features that rescore existing translation candidates.", "labels": [], "entities": [{"text": "Statistical phrase-based translation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6123132407665253}]}, {"text": "In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data.", "labels": [], "entities": []}, {"text": "The proposed technique first constructs phrase graphs using both source and target language mono-lingual corpora.", "labels": [], "entities": []}, {"text": "Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7307802736759186}]}, {"text": "We report results on a large Arabic-English system and a medium-sized Urdu-English system.", "labels": [], "entities": []}, {"text": "Our proposed approach significantly improves the performance of competitive phrase-based systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.998725950717926}]}], "introductionContent": [{"text": "Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities.", "labels": [], "entities": [{"text": "machine translation (SMT)", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.8529197752475739}]}, {"text": "With large amounts of data, phrase-based translation systems ( achieve state-of-the-art results in many typologically diverse language pairs ().", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7214380204677582}]}, {"text": "However, the limiting factor in the success of these techniques is parallel data availability.", "labels": [], "entities": []}, {"text": "Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial \u21e4 This work was done while the first author was interning at Microsoft Research for effective translation.", "labels": [], "entities": []}, {"text": "This problem is exacerbated in the many language pairs for which parallel resources are either limited or nonexistent.", "labels": [], "entities": []}, {"text": "While parallel data is generally scarce, monolingual resources exist in abundance and are being created at accelerating rates.", "labels": [], "entities": []}, {"text": "Can we use monolingual data to augment the phrasal translations acquired from parallel data?", "labels": [], "entities": []}, {"text": "The challenge of learning translations from monolingual data is of longstanding interest, and has been approached in several ways).", "labels": [], "entities": []}, {"text": "Our work introduces anew take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources.", "labels": [], "entities": []}, {"text": "On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes ( \u00a72.2).", "labels": [], "entities": []}, {"text": "Unlike previous work), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text.", "labels": [], "entities": []}, {"text": "This enhancement alone results in an improvement of almost 1.4 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9985740184783936}]}, {"text": "On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates ( \u00a72.1), and are embedded in a target graph.", "labels": [], "entities": []}, {"text": "We then limit the set of translation options for each unlabeled source phrase ( \u00a72.3), and using a structured graph propagation algorithm, where translation information is propagated from labeled to unlabeled phrases proportional to both source and target phrase similarities, we estimate probability distributions over translations for: Example source and target graphs used in our approach.", "labels": [], "entities": []}, {"text": "Labeled phrases on the source side are black (with their corresponding translations on the target side also black); unlabeled and generated ( \u00a72.1) phrases on the source and target sides respectively are white.", "labels": [], "entities": []}, {"text": "Labeled phrases also have conditional probability distributions defined over target phrases, which are extracted from the parallel corpora.", "labels": [], "entities": []}, {"text": "the unlabeled source phrases ( \u00a72.4).", "labels": [], "entities": []}, {"text": "The additional phrases are incorporated in the SMT system through a secondary phrase table ( \u00a72.5).", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9912146329879761}]}, {"text": "We evaluated the proposed approach on both ArabicEnglish and Urdu-English under a range of scenarios ( \u00a73), varying the amount and type of monolingual corpora used, and obtained improvements between 1 and 4 BLEU points, even when using very large language models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9992533326148987}]}], "datasetContent": [{"text": "We performed an extensive evaluation to examine various aspects of the approach along with overall system performance.", "labels": [], "entities": []}, {"text": "Two language pairs were used: Arabic-English and Urdu-English.", "labels": [], "entities": []}, {"text": "The Arabic-English evaluation was used to validate the decisions made during the development of our method and also to highlight properties of the technique.", "labels": [], "entities": []}, {"text": "With it, in \u00a73.2 we first analyzed the impact of utilizing phrases instead of words and SLP instead of LP; the latter experiment underscores the importance of generated candidates.", "labels": [], "entities": []}, {"text": "We also look at how adding morphological knowledge to the generation process can further enrich performance.", "labels": [], "entities": []}, {"text": "In \u00a73.3, we then examined the effect of using a very large 5-gram language model training on 7.5 billion English tokens to understand the nature of the improvements in \u00a73.2.", "labels": [], "entities": []}, {"text": "The Urdu to English evaluation in \u00a73.4 focuses on how noisy parallel data and completely monolingual (i.e., not even comparable) text can be used fora realistic low-resource language pair, and is evaluated with the larger language model only.", "labels": [], "entities": []}, {"text": "We also examine how our approach can learn from noisy parallel data compared to the traditional SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9894133806228638}]}, {"text": "Baseline phrasal systems are used both for comparison and for generating translation candidates for unlabeled phrases as described in \u00a72.1.", "labels": [], "entities": []}, {"text": "The baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7682229280471802}]}, {"text": "The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT, which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables).", "labels": [], "entities": [{"text": "MERT", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9417043328285217}]}, {"text": "For all systems, we use a distortion limit of 4.", "labels": [], "entities": [{"text": "distortion limit", "start_pos": 26, "end_pos": 42, "type": "METRIC", "confidence": 0.9749982953071594}]}, {"text": "We use case-insensitive BLEU () to evaluate translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9825752973556519}, {"text": "translation", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9566461443901062}]}, {"text": "Bilingual corpus statistics for both language pairs are presented in.", "labels": [], "entities": []}, {"text": "For Arabic-English, our training corpus consisted of 685k sentence pairs from standard LDC corpora 4 . The NIST MT06 and MT08 Arabic-English evaluation sets (combining the newswire and weblog domains for both sets), with four references each, were used as tuning and testing sets respectively.", "labels": [], "entities": [{"text": "NIST", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.981381893157959}, {"text": "MT06", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.5814903974533081}, {"text": "MT08 Arabic-English evaluation sets", "start_pos": 121, "end_pos": 156, "type": "DATASET", "confidence": 0.8413062393665314}]}, {"text": "For UrduEnglish, the training corpus was provided by the LDC for the NIST Urdu-English MT evaluation, and most of the data was automatically acquired from the web, making it quite noisy.", "labels": [], "entities": [{"text": "NIST Urdu-English MT evaluation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.718590185046196}]}, {"text": "After filtering, there are approximately 65k parallel sen- contains statistics for the monolingual corpora used in our experiments.", "labels": [], "entities": []}, {"text": "From these corpora, we extracted all sentences that contained at least one source or target phrase match to compute features for graph construction.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7343838810920715}]}, {"text": "For the Arabic to English experiments, the monolingual corpora are taken from the AFP Arabic and English Gigaword corpora and are of a similar date range to each other, rendering them comparable but not sentence-aligned or parallel.", "labels": [], "entities": [{"text": "AFP Arabic and English Gigaword corpora", "start_pos": 82, "end_pos": 121, "type": "DATASET", "confidence": 0.8763975103696188}]}, {"text": "For the Urdu-English experiments, completely non-comparable monolingual text was used for graph construction; we obtained the Urdu side through a web-crawler, and a subset of the AFP Gigaword English corpus was used for English.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7438718378543854}, {"text": "AFP Gigaword English corpus", "start_pos": 179, "end_pos": 206, "type": "DATASET", "confidence": 0.9279423505067825}]}, {"text": "In addition, we obtained a corpus from the ELRA 5 , which contains a mix of parallel and monolingual data; based on timestamps, we extracted a comparable English corpus for the ELRA Urdu monolingual data to form a roughly 470k-sentence \"noisy parallel\" set.", "labels": [], "entities": [{"text": "ELRA 5", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.9771034717559814}, {"text": "ELRA Urdu monolingual data", "start_pos": 177, "end_pos": 203, "type": "DATASET", "confidence": 0.9432896971702576}]}, {"text": "We used this set in two ways: either to augment the parallel data presented in, or to augment the non-comparable monolingual data in for graph construction.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7266175746917725}]}, {"text": "In our first set of experiments, we looked at the impact of choosing bigrams over unigrams as our basic unit of representation, along with performance of LP (Eq. 2) compared to SLP (Eq. 4).", "labels": [], "entities": [{"text": "LP", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9367014169692993}]}, {"text": "Recall that LP only takes into account source similarity; since the vast majority of generated candidates do not occur as labeled neighbors' labels, restricting propagation to the source graph drastically reduces the usage of generated candidates as labels, but does not completely eliminate it.", "labels": [], "entities": []}, {"text": "In these experiments, we utilize a reasonably-sized 4-gram language model trained on 900m English tokens, i.e., the English monolingual corpus.", "labels": [], "entities": []}, {"text": "presents the results of these variations; overall, by taking into account generated candidates appropriately and using bigrams (\"SLP 2-gram\"), we obtained a 1.13 BLEU gain on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.998994767665863}]}, {"text": "Using unigrams (\"SLP 1-gram\") actually does worse than the baseline, indicating the importance of focusing on translations for sparser bigrams.", "labels": [], "entities": []}, {"text": "While LP (\"LP 2-gram\") does reasonably well, its underperformance compared to SLP underlines the importance of enriching the translation space with generated candidates and handling these candidates appropriately.", "labels": [], "entities": []}, {"text": "In \"SLP-HalfMono\", we use only half of the monolingual comparable corpora, and still obtain an improvement of 0.56 BLEU points, indicating that adding more monolingual data is likely to improve the system further.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9989491105079651}]}, {"text": "Interestingly, biasing away from generated candidates using all the monolingual data (\"LP 2-gram\") performs similarly to using half the monolingual corpora and handling generated candidates properly (\"SLP-HalfMono\").", "labels": [], "entities": []}, {"text": "Additional morphologically generated candidates were added in this experiment as detailed in \u00a72.3.", "labels": [], "entities": []}, {"text": "We used a simple hand-built Arabic morphological analyzer that segments word types based on regular expressions, and an English lexiconbased morphological analyzer.", "labels": [], "entities": []}, {"text": "The morphological candidates add a small amount of improvement, primarily by targeting genuine OOVs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parameters, explanation of their function, and value chosen.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9436345100402832}]}, {"text": " Table 2: Bilingual corpus statistics for the Arabic-English  and Urdu-English datasets used.", "labels": [], "entities": []}, {"text": " Table 4: Results for the Arabic-English evaluation. The LP  vs. SLP comparison highlights the importance of target side  enrichment via translation candidate generation, 1-gram vs.  2-gram comparisons highlight the importance of emphasiz- ing phrases, utilizing half the monolingual data shows sensi- tivity to monolingual corpus size, and adding morphological  information results in additional improvement.", "labels": [], "entities": []}, {"text": " Table 5: Results with the large language model scenario. The  gains are even better than with the smaller language model.", "labels": [], "entities": []}, {"text": " Table 6. The two setups  allow us to examine how effectively our method  can learn from the noisy parallel data by treating it  as monolingual (i.e., for graph construction), com- pared to treating this data as parallel, and also ex- amines the realistic scenario of using completely  non-comparable monolingual text for graph con- struction as in the second setup.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7611128389835358}]}, {"text": " Table 6: Results for the Urdu-English evaluation evaluated  with BLEU. All experiments were conducted with the larger  language model, and generation only considered the m-best  candidates from the baseline system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9975346326828003}]}]}