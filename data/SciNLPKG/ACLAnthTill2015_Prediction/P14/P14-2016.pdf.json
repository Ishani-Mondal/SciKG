{"title": [{"text": "Automatic Detection of Multilingual Dictionaries on the Web", "labels": [], "entities": [{"text": "Automatic Detection of Multilingual Dictionaries", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.838527375459671}]}], "abstractContent": [{"text": "This paper presents an approach to query construction to detect multilingual dictionaries for predetermined language combinations on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents.", "labels": [], "entities": [{"text": "query construction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8313794434070587}]}, {"text": "We use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluate our proposed methodology in two ways: 1.", "labels": [], "entities": []}, {"text": "against a synthetic dataset, whereby we injected bilingual dictionaries into a collection of web documents, and evaluated the ability of the method to return multilingual dictionaries for individual languages; in this, we naively assume that all web documents in the background collection are not multilingual dictionaries, and as such, the results are potentially an underestimate of the true retrieval effectiveness.", "labels": [], "entities": []}, {"text": "2. against the open web via the Google search API fora given combination of languages, and hand evaluation of the returned documents: Details of the training data and queries learned for each language Note that the first evaluation with the synthetic dataset is based on monolingual dictionary retrieval effectiveness because we have very few (and often no) multilingual dictionaries fora given pairing of our target languages.", "labels": [], "entities": []}, {"text": "For a given language, we are thus evaluating the ability of our method to retrieve multilingual dictionaries containing that language (and other indeterminate languages).", "labels": [], "entities": []}, {"text": "For both the synthetic dataset and open web experiments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which returns a non-empty result set.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 89, "end_pos": 117, "type": "METRIC", "confidence": 0.9462051888306936}]}, {"text": "To train our method, we use 52 bilingual Freedict (Freedict, 2011) dictionaries and Wikipedia 1 documents for each of our target languages.", "labels": [], "entities": [{"text": "Freedict (Freedict, 2011) dictionaries", "start_pos": 41, "end_pos": 79, "type": "DATASET", "confidence": 0.9306983266557965}]}, {"text": "As there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only.", "labels": [], "entities": []}, {"text": "Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (), respectively.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8605134785175323}, {"text": "MeCab (MeCab, 2011)", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.8764905432860056}]}, {"text": "See for details of the number of Wikipedia articles and dictionaries for each language.", "labels": [], "entities": []}, {"text": "Below, we detail the construction of the synthetic dataset.", "labels": [], "entities": []}, {"text": "The synthetic dataset was constructed using a subset of ClueWeb09 ( as the background web document collection.", "labels": [], "entities": []}, {"text": "The original ClueWeb09 dataset consists of around 1 billion web pages in ten languages that were collected in January and February 2009.", "labels": [], "entities": [{"text": "ClueWeb09 dataset", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.9369719624519348}]}, {"text": "The relative proportions of documents in the different languages in the original dataset are as detailed in.", "labels": [], "entities": []}, {"text": "We randomly downsampled ClueWeb09 to 10  million documents for the 8 languages targeted in this research (the original 10 ClueWeb09 languages minus Korean and Portuguese).", "labels": [], "entities": []}, {"text": "We then sourced a random set of 246 multilingual dictionaries that were used in the construction of panlex.org, and injected them into the document collection.", "labels": [], "entities": []}, {"text": "Each of these dictionaries contains at least one of our 8 target languages, with the second language potentially being outside the 8.", "labels": [], "entities": []}, {"text": "A total of 49 languages are contained in the dictionaries.", "labels": [], "entities": []}, {"text": "We indexed the synthetic dataset using Indri.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of the training data and queries learned for each language", "labels": [], "entities": []}, {"text": " Table 3: Dictionary retrieval results over the syn- thetic dataset (\"Dicts\" = the number of dictionaries  in the document collection for that language.", "labels": [], "entities": [{"text": "Dictionary retrieval", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6347222030162811}]}, {"text": " Table 4: Dictionary retrieval results over the open  web for dictionaries containing English and each  of the indicated languages (\"Dicts\" = the number  of unique multilingual dictionaries retrieved for  that language).", "labels": [], "entities": [{"text": "Dictionary retrieval", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.621070146560669}]}]}