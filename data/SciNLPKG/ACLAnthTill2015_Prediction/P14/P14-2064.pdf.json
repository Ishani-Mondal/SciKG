{"title": [{"text": "Difficult Cases: From Data to Learning, and Back", "labels": [], "entities": []}], "abstractContent": [{"text": "This article contributes to the ongoing discussion in the computational linguistics community regarding instances that are difficult to annotate reliably.", "labels": [], "entities": []}, {"text": "Is it worthwhile to identify those?", "labels": [], "entities": []}, {"text": "What information can be inferred from them regarding the nature of the task?", "labels": [], "entities": []}, {"text": "What should be done with them when building supervised machine learning systems?", "labels": [], "entities": []}, {"text": "We address these questions in the context of a subjective semantic task.", "labels": [], "entities": []}, {"text": "In this setting, we show that the presence of such instances in training data misleads a machine learner into misclassifying clear-cut cases.", "labels": [], "entities": []}, {"text": "We also show that considering machine learning outcomes with and without the difficult cases, it is possible to identify specific weaknesses of the problem representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of cases that are difficult for annotation received recent attention from both the theoretical and the applied perspectives.", "labels": [], "entities": []}, {"text": "Such items might receive contradictory labels, without a clearway of settling the disagreement.", "labels": [], "entities": []}, {"text": "showed theoretically that hard cases -items with unreliable annotations -can lead to unfair benchmarking results when found in test data, and, in worst case, to a degradation in a machi74ne learner's performance on easy, uncontroversial instances if found in the training data.", "labels": [], "entities": []}, {"text": "provided an empirical demonstration that the presence of such difficult cases in dependency parsing evaluations leads to unstable benchmarking results, as different gold standards might provide conflicting annotations for such items.", "labels": [], "entities": [{"text": "dependency parsing evaluations", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.8659518361091614}]}, {"text": "demonstrated by simulation that systematic disagreements between annotators negatively impact generalization ability of classifiers built using data from different annotators.", "labels": [], "entities": []}, {"text": "showed that judgments of readability of the same texts by different groups of experts are sufficiently systematically different to hamper cross-expert generalization of readability classifiers trained on annotations from different groups.", "labels": [], "entities": []}, {"text": "discuss the negative impact of systematic simulated annotation inconsistencies on active learning performance on a word-sense disambiguation task.", "labels": [], "entities": [{"text": "word-sense disambiguation task", "start_pos": 115, "end_pos": 145, "type": "TASK", "confidence": 0.76719398299853}]}, {"text": "In this paper, we address the task of classifying words in a text as semantically new or old.", "labels": [], "entities": []}, {"text": "Using multiple annotators, we empirically identify instances that show substantial disagreement between annotators.", "labels": [], "entities": []}, {"text": "We then discuss those both from the linguistic perspective, identifying some characteristics of such cases, and from the perspective of machine learning, showing that the presence of difficult cases in the training data misleads the machine learner on easy, clear-cut cases -a phenomenon termed hard case bias in Beigman and Beigman . The main contribution of this paper is in providing additional empricial evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data -not only from the linguistic perspective, but also from a machine learning one.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Sizes of subsets by levels of agreement.", "labels": [], "entities": [{"text": "Sizes of subsets", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8753774960835775}]}]}