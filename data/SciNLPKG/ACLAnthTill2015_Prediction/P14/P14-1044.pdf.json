{"title": [{"text": "A Robust Approach to Aligning Heterogeneous Lexical Resources", "labels": [], "entities": [{"text": "Aligning Heterogeneous Lexical", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.862592617670695}]}], "abstractContent": [{"text": "Lexical resource alignment has been an active field of research over the last decade.", "labels": [], "entities": [{"text": "Lexical resource alignment", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8823882937431335}]}, {"text": "However, prior methods for aligning lexical resources have been either specific to a particular pair of resources, or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned.", "labels": [], "entities": []}, {"text": "Here we present a unified approach that can be applied to an arbitrary pair of lexical resources, including machine-readable dictionaries with no network structure.", "labels": [], "entities": []}, {"text": "Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources, achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources: Wikipedia, Wiktionary and OmegaWiki.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical resources are repositories of machinereadable knowledge that can be used in virtually any Natural Language Processing task.", "labels": [], "entities": []}, {"text": "Notable examples are WordNet, Wikipedia and, more recently, collaboratively-curated resources such as).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9420092105865479}, {"text": "Wikipedia", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.8898861408233643}]}, {"text": "On the one hand, these resources are heterogeneous in design, structure and content, but, on the other hand, they often provide complementary knowledge which we would like to see integrated.", "labels": [], "entities": []}, {"text": "Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms.", "labels": [], "entities": [{"text": "lexical resource alignment", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.7422732512156168}]}, {"text": "Owing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing), Semantic Role Labeling (, and Word Sense Disambiguation ().", "labels": [], "entities": [{"text": "resource alignment", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7259631752967834}, {"text": "Semantic Role Labeling", "start_pos": 215, "end_pos": 237, "type": "TASK", "confidence": 0.8082268436749777}, {"text": "Word Sense Disambiguation", "start_pos": 245, "end_pos": 270, "type": "TASK", "confidence": 0.6214210589726766}]}, {"text": "Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach) falls short because of the potential use of totally different wordings to define the same concept.", "labels": [], "entities": []}, {"text": "Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions.", "labels": [], "entities": []}, {"text": "While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another.", "labels": [], "entities": []}, {"text": "When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs.", "labels": [], "entities": []}, {"text": "However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into semantic graphs before such graph-based approaches can be applied to them.", "labels": [], "entities": []}, {"text": "To do this, recent work has proposed graph construction by monosemous linking, where a concept is linked to all the concepts associated with the monosemous words in its definition.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7368335723876953}]}, {"text": "However, this alignment method still involves tuning of parameters which are highly dependent on the characteristics of the generated graphs and, hence, requires hand-crafted sense alignments for the specific pair of resources to be aligned, a task which has to be replicated every time the resources are updated.", "labels": [], "entities": []}, {"text": "In this paper we propose a unified approach to aligning arbitrary pairs of lexical resources which is independent of their specific structure.", "labels": [], "entities": []}, {"text": "Thanks to a novel modeling of the sense entries and an effective ontologization algorithm, our approach also fares well when resources lack relational structure or pair-specific training data is absent, meaning that it is applicable to arbitrary pairs without adaptation.", "labels": [], "entities": []}, {"text": "We report state-of-the-art performance when aligning WordNet to Wikipedia, OmegaWiki and Wiktionary.", "labels": [], "entities": []}], "datasetContent": [{"text": "To enable a comparison with the state of the art, we followed and performed an alignment of WordNet synsets (WN) to three different collaboratively-constructed resources: Wikipedia (WP), Wiktionary (WT), and OmegaWiki (OW).", "labels": [], "entities": []}, {"text": "We utilized the DKPro software () to access the information in the foregoing three resources.", "labels": [], "entities": []}, {"text": "For WP, WT, OW we used the dump versions, respectively.", "labels": [], "entities": [{"text": "OW", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.8286574482917786}]}, {"text": "We followed previous work and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy.", "labels": [], "entities": [{"text": "alignment", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.9423218965530396}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9997254014015198}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9976454377174377}, {"text": "F1", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9988263249397278}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9990808963775635}]}, {"text": "Precision is the fraction of correct alignment judgments returned by the system and recall is the fraction of alignment judgments in the gold standard dataset that are correctly returned by the system.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9898349642753601}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9995478987693787}]}, {"text": "F1 is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9955728054046631}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9994508624076843}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9962784647941589}]}, {"text": "We also report results for accuracy which, in addition to true positives, takes into account true negatives, i.e., pairs which are correctly judged as unaligned.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994869232177734}]}, {"text": "Here, we describe how the four semantic graphs for our four lexical resources (i.e., WN, WP, WT, OW) were constructed.", "labels": [], "entities": []}, {"text": "As mentioned in Section 2.1.1, we build the WN graph by including all the synsets and semantic relations defined in WordNet (e.g., hypernymy and meronymy) and further populate the relation set by connecting a synset to all the other synsets that appear in its disambiguated gloss.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9447413682937622}]}, {"text": "For WP, we used the graph provided by, constructed by directly connecting an article (concept) to all the hyperlinks in its first paragraph, together with the category links.", "labels": [], "entities": []}, {"text": "Our WN and WP graphs have 118K and 2.8M nodes, respectively, with the average node degree being roughly 9 in both resources.", "labels": [], "entities": []}, {"text": "The other two resources, i.e., WT and OW, do not provide a reliable network of semantic relations, therefore we used our ontologization approach to construct their corresponding semantic graphs.", "labels": [], "entities": []}, {"text": "We report, in the following subsection, the experiments carried out to assess the accuracy of our ontologization method, together with the statistics of the obtained graphs for WT and OW.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9995183944702148}, {"text": "OW", "start_pos": 184, "end_pos": 186, "type": "DATASET", "confidence": 0.7472018599510193}]}, {"text": "For ontologizing WT and OW, the bag of content words Wis given by the content words in sense definitions and, if available, additional related words obtained from lexicon relations (see Section 3).", "labels": [], "entities": []}, {"text": "In WT, both of these are in word surface form and hence had to be disambiguated.", "labels": [], "entities": [{"text": "WT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.8446446657180786}]}, {"text": "For OW, however, the encoded relations, though rela-: The statistics of the generated graphs for WT and OW.", "labels": [], "entities": []}, {"text": "We report the distribution of the edges across types (i.e., ambiguous and unambiguous) and sources (i.e., definitions and relations) from which candidate words were obtained.", "labels": [], "entities": []}, {"text": "tively small in number, are already disambiguated and, therefore, the ontologization was just performed on the definition's content words.", "labels": [], "entities": []}, {"text": "The resulting graphs for WT and OW contain 430K and 48K nodes, respectively, each providing more than 95% coverage of concepts, with the average node degree being around 10 for both resources.", "labels": [], "entities": [{"text": "OW", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.7822971343994141}]}, {"text": "We present in, for WT and OW, the total number of edges together with their distribution across types (i.e., ambiguous and unambiguous) and sources (i.e., definitions and relations) from which candidate words were obtained.", "labels": [], "entities": []}, {"text": "The edges obtained from unambiguous entries are essentially sense disambiguated on both sides whereas those obtained from ambiguous terms area result of our similarity-based disambiguation.", "labels": [], "entities": []}, {"text": "Hence, given that a large portion of edges came from ambiguous words (see), we carried out an experiment to evaluate the accuracy of our disambiguation method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.999459445476532}]}, {"text": "To this end, we took as our benchmark the dataset provided by for evaluating relation disambiguation in WT.", "labels": [], "entities": [{"text": "relation disambiguation", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7775950133800507}]}, {"text": "The dataset contains 394 manually-disambiguated relations.", "labels": [], "entities": []}, {"text": "We compared our similarity-based disambiguation approach against the state of the art on this dataset, i.e., the WKTWSD system, which is a WT relation disambiguation algorithm based on a series of rules.", "labels": [], "entities": [{"text": "WKTWSD system", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9394162893295288}, {"text": "WT relation disambiguation", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.8283247749010721}]}, {"text": "shows the performance of our disambiguation method, together with that of WKTWSD, in terms of Precision (P), Recall (R), F1, and accuracy.", "labels": [], "entities": [{"text": "WKTWSD", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.8510753512382507}, {"text": "Precision (P)", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.9535912871360779}, {"text": "Recall (R)", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9636546969413757}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.996350884437561}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9996387958526611}]}, {"text": "The \"Human\" row corresponds to the inter-rater F1 and accuracy scores, i.e., the upperbound performance on this dataset, as calculated by.", "labels": [], "entities": [{"text": "Human\" row", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.8002143899599711}, {"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.7728353142738342}, {"text": "accuracy scores", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.979997843503952}]}, {"text": "As can be seen, our method proves to be very accurate, surpassing the performance of the WKTWSD system in terms of precision, F1, and accuracy.", "labels": [], "entities": [{"text": "WKTWSD", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.8462101221084595}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9997803568840027}, {"text": "F1", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.999722421169281}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9993367791175842}]}, {"text": "This is particularly: The performance of relation disambiguation for our similarity-based disambiguation method, as well as for the WKTWSD system.", "labels": [], "entities": [{"text": "relation disambiguation", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7809594571590424}, {"text": "WKTWSD", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.8819891214370728}]}, {"text": "interesting as the WKTWSD system uses a rulebased technique specific to relation disambiguation in WT, whereas our method is resource independent and can be applied to arbitrary words in the definition of any concept.", "labels": [], "entities": [{"text": "WKTWSD", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.932188868522644}, {"text": "relation disambiguation", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.7391596138477325}]}, {"text": "We also note that the graph constructed by had an average node degree of around 1.", "labels": [], "entities": []}, {"text": "More recently, Matuschek and Gurevych (2013) leveraged monosemous linking (cf. Section 5) in order to create denser semantic graphs for OW and WT.", "labels": [], "entities": []}, {"text": "Our approach, however, thanks to the connections obtained through ambiguous words, can provide graphs with significantly higher coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9752957224845886}]}, {"text": "As an example, for WT, generated a graph where around 30% of the nodes were in isolation, whereas this number drops to around 5% in our corresponding graph.", "labels": [], "entities": [{"text": "WT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.8947504758834839}]}, {"text": "These results show that our ontologization approach can be used to obtain dense semantic graph representations of lexical resources, while at the same time preserving a high level of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9954978227615356}]}, {"text": "Now that all the four resources are transformed into semantic graphs, we move to our alignment experiments.", "labels": [], "entities": []}, {"text": "As our benchmark we tested on the gold standard datasets used in for three alignment tasks: WordNet-Wikipedia (WN-WP), WordNetWiktionary (WN-WT), and WordNet-OmegaWiki (WN-OW).", "labels": [], "entities": []}, {"text": "However, the dataset for WN-OW was originally built for the German language and, hence, was missing many English OW concepts that could be considered as candidate target alignments.", "labels": [], "entities": [{"text": "WN-OW", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.588813841342926}]}, {"text": "We therefore fixed the dataset for the English language and reproduced the performance of previous work on the new dataset.", "labels": [], "entities": []}, {"text": "The three datasets contained 320, 484, and 315 WN concepts that were manually mapped to their corresponding concepts in WP, WT, and OW, respectively.: The performance of different systems on the task of aligning WordNet to Wikipedia (WN-WP), Wiktionary (WN-WT), and OmegaWiki (WN-OW) in terms of Precision (P), Recall (R), F1, and Accuracy (A).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 296, "end_pos": 309, "type": "METRIC", "confidence": 0.9279089421033859}, {"text": "Recall (R)", "start_pos": 311, "end_pos": 321, "type": "METRIC", "confidence": 0.9340740293264389}, {"text": "F1", "start_pos": 323, "end_pos": 325, "type": "METRIC", "confidence": 0.9884228110313416}, {"text": "Accuracy (A)", "start_pos": 331, "end_pos": 343, "type": "METRIC", "confidence": 0.9614804238080978}]}, {"text": "We present results for different configurations of our system (SemAlign), together with the state of the art in definition similarity-based alignment approaches (SB) and the best configuration of the stateof-the-art graph-based system, Dijkstra-WSA (Matuschek and Gurevych, 2013, DWSA).", "labels": [], "entities": [{"text": "definition similarity-based alignment", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.580903559923172}, {"text": "DWSA", "start_pos": 280, "end_pos": 284, "type": "DATASET", "confidence": 0.8671688437461853}]}, {"text": "Recall from Section 2 that our resource alignment technique has two parameters: the similarity threshold \u03b8 and the combination parameter \u03b2, both defined in.", "labels": [], "entities": [{"text": "resource alignment", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7463823854923248}, {"text": "similarity threshold \u03b8", "start_pos": 84, "end_pos": 106, "type": "METRIC", "confidence": 0.941104511419932}, {"text": "combination parameter \u03b2", "start_pos": 115, "end_pos": 138, "type": "METRIC", "confidence": 0.8935263951619467}]}, {"text": "We performed experiments with three different configurations: \u2022 Unsupervised, where the two parameters are set to their middle values (i.e., 0.5), hence, no tuning is performed for either of the parameters.", "labels": [], "entities": []}, {"text": "In this case, both the definitional and structural similarity scores are treated as equally important and two concepts are aligned if their overall similarity exceeds the middle point of the similarity scale.", "labels": [], "entities": []}, {"text": "\u2022 Tuning, where we follow and tune the parameters on a subset of the dataset comprising 100 items.", "labels": [], "entities": []}, {"text": "\u2022 Cross-validation, where a 5-fold cross validation is carried out to find the optimal values for the parameters, a technique used inmost of the recent alignment methods ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of the generated graphs  for WT and OW. We report the distribution of  the edges across types (i.e., ambiguous and un- ambiguous) and sources (i.e., definitions and rela- tions) from which candidate words were obtained.", "labels": [], "entities": [{"text": "OW", "start_pos": 61, "end_pos": 63, "type": "DATASET", "confidence": 0.7122052907943726}]}, {"text": " Table 2: The performance of relation disam- biguation for our similarity-based disambiguation  method, as well as for the WKTWSD system.", "labels": [], "entities": [{"text": "WKTWSD", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.889436662197113}]}, {"text": " Table 3: The performance of different systems on the task of aligning WordNet to Wikipedia (WN-WP),  Wiktionary (WN-WT), and OmegaWiki (WN-OW) in terms of Precision (P), Recall (R), F1, and Accuracy  (A). We present results for different configurations of our system (SemAlign), together with the state of  the art in definition similarity-based alignment approaches (SB) and the best configuration of the state- of-the-art graph-based system, Dijkstra-WSA (Matuschek and Gurevych, 2013, DWSA).", "labels": [], "entities": [{"text": "Precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9921310544013977}, {"text": "Recall (R)", "start_pos": 171, "end_pos": 181, "type": "METRIC", "confidence": 0.9226322323083878}, {"text": "F1", "start_pos": 183, "end_pos": 185, "type": "METRIC", "confidence": 0.9812248349189758}, {"text": "Accuracy  (A)", "start_pos": 191, "end_pos": 204, "type": "METRIC", "confidence": 0.9451695531606674}, {"text": "definition similarity-based alignment", "start_pos": 319, "end_pos": 356, "type": "TASK", "confidence": 0.6089442769686381}, {"text": "DWSA", "start_pos": 489, "end_pos": 493, "type": "DATASET", "confidence": 0.7903009057044983}]}, {"text": " Table 4: Performance of SemAlign when using only the structural similarity component (SemAlign str )  compared to the state-of-the-art graph-based alignment approach, Dijkstra-WSA (Matuschek and  Gurevych, 2013) for our three resource pairs: WordNet to Wikipedia (WN-WP), Wiktionary (WN-WT),  and OmegaWiki (WN-OW).", "labels": [], "entities": []}]}