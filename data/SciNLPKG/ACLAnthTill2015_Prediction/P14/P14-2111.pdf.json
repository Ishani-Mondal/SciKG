{"title": [{"text": "Normalizing tweets with edit scripts and recurrent neural embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Tweets often contain a large proportion of abbreviations, alternative spellings, novel words and other non-canonical language.", "labels": [], "entities": []}, {"text": "These features are problematic for standard language analysis tools and it can be desirable to convert them to canoni-cal form.", "labels": [], "entities": []}, {"text": "We propose a novel text nor-malization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embed-dings.", "labels": [], "entities": []}, {"text": "The text embeddings are generated using an Simple Recurrent Network.", "labels": [], "entities": []}, {"text": "We find that enriching the feature set with text embeddings substantially lowers word error rates on an English tweet normaliza-tion dataset.", "labels": [], "entities": [{"text": "word error rates", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.6478639046351115}, {"text": "English tweet normaliza-tion dataset", "start_pos": 104, "end_pos": 140, "type": "DATASET", "confidence": 0.5819832682609558}]}, {"text": "Our model improves on state-of-the-art with little training data and without any lexical resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "A stream of posts from Twitter contains text written in a large variety of languages and writing systems, in registers ranging from formal to internet slang.", "labels": [], "entities": []}, {"text": "Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content.", "labels": [], "entities": []}, {"text": "One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.787933737039566}]}, {"text": "A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.7114608585834503}]}, {"text": "Methods such as those of Han and Baldwin (2011),, or are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data.", "labels": [], "entities": []}, {"text": "In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources.", "labels": [], "entities": []}, {"text": "Our model learns sequences of edit operations from labeled data using a Conditional Random Field ().", "labels": [], "entities": []}, {"text": "Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation, and word and sentence boundary detection ().", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7518025934696198}, {"text": "word and sentence boundary detection", "start_pos": 121, "end_pos": 157, "type": "TASK", "confidence": 0.6319259107112885}]}, {"text": "We train a recurrent neural network language model () on a large collection of tweets.", "labels": [], "entities": []}, {"text": "When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model.", "labels": [], "entities": []}, {"text": "The principal contributions of our work are: (i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that characterlevel neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8129094541072845}, {"text": "text normalization", "start_pos": 388, "end_pos": 406, "type": "TASK", "confidence": 0.7595936954021454}]}], "datasetContent": [{"text": "We limit the size of the string alphabet by always working with UTF-8 encoded strings, and using bytes rather than characters as basic units.", "labels": [], "entities": []}, {"text": "As our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequenc\u00ea t and the target word sequence t, normalized by the total number of words in the target string.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 32, "end_pos": 53, "type": "METRIC", "confidence": 0.8646651605765024}, {"text": "Levenshtein edit distance", "start_pos": 78, "end_pos": 103, "type": "METRIC", "confidence": 0.7578177253405253}]}, {"text": "A more generally applicable metric would be character error rate, but we report WERs to make our results easily comparable with previous work.", "labels": [], "entities": [{"text": "character error rate", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.7318272988001505}, {"text": "WERs", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9915506839752197}]}, {"text": "Since the   English dataset is pre-tokenized and only covers word-to-word transformations, this choice has little importance here and character error rates show a similar pattern to word error rates.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.6902295798063278}]}, {"text": "shows the results of our development experiments.", "labels": [], "entities": []}, {"text": "NO-OP is a baseline which leaves text unchanged.", "labels": [], "entities": []}, {"text": "As expected the most constrained model OOV-ONLY outperforms the more generic models on this dataset.", "labels": [], "entities": [{"text": "OOV-ONLY", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9919835925102234}]}, {"text": "For all model variations, adding SRN features substantially improves performance: the relative error reductions range from 12% for OOV-ONLY to 30% for ALL-WORDS.", "labels": [], "entities": [{"text": "error", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9741837978363037}, {"text": "OOV-ONLY", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.8282864093780518}]}, {"text": "shows the non-unique normalizations made by the OOV-ONLY model with SRN features which were missed without them.", "labels": [], "entities": []}, {"text": "SRN features seem to be especially useful for learning long-range, multi-character edits, e.g. fb for facebook.", "labels": [], "entities": [{"text": "SRN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6440955996513367}]}, {"text": "shows the non-unique normalizations which were missed by the best model: they area mixture of relatively standard variations which happen to be infrequent in our data, like tonite or gf, and a few idiosyncratic respellings like uu or bhee.", "labels": [], "entities": []}, {"text": "Our supervised approach makes it easy to address the first type of failure by simply annotating additional training examples.", "labels": [], "entities": []}, {"text": "presents evaluation results of several approaches reported in as well as the model which did best in our development experiments.", "labels": [], "entities": []}, {"text": "HB-dict is the Internet slang dictionary from.", "labels": [], "entities": [{"text": "HB-dict", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9544076919555664}]}, {"text": "GHM-dict is the automatically constructed dictionary from 683 4 1 one 2 withh with 2 uu you 2 tonite tonight 2 thx thanks 2 thiis this 2 smh somehow 2 outta out 2 n in 2 mam 2 hmwrk homework 2 gf girlfriend 2 fxckin fucking 2 dha the 2 de the 2 d the 2 bhee be 2 bb baby 4.8: WERs compared to previous work.", "labels": [], "entities": [{"text": "GHM-dict", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.901158332824707}, {"text": "WERs", "start_pos": 276, "end_pos": 280, "type": "METRIC", "confidence": 0.9991758465766907}]}, {"text": "The WER reported for OOV-ONLY NGRAM+SRN is on the test folds only.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.998499870300293}, {"text": "OOV-ONLY NGRAM+SRN", "start_pos": 21, "end_pos": 39, "type": "METRIC", "confidence": 0.7247124761343002}]}, {"text": "The score on the full dataset is a bit better: 4.66%.", "labels": [], "entities": []}, {"text": "As can be seen our approach it the best performing approach overall and in particular it does much better than all of the single dictionary-based methods.", "labels": [], "entities": []}, {"text": "Only the combination of all the dictionaries comes close in performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: WERs compared to previous work.", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9953696131706238}]}]}