{"title": [{"text": "Adaptive HTER Estimation for Document-Specific MT Post-Editing", "labels": [], "entities": [{"text": "Adaptive HTER Estimation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6927848060925802}, {"text": "MT Post-Editing", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7603174149990082}]}], "abstractContent": [{"text": "We present an adaptive translation quality estimation (QE) method to predict the human-targeted translation error rate (HTER) fora document-specific machine translation model.", "labels": [], "entities": [{"text": "translation quality estimation (QE)", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.6586581319570541}, {"text": "human-targeted translation error rate (HTER)", "start_pos": 81, "end_pos": 125, "type": "METRIC", "confidence": 0.7603281949247632}, {"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.6607331186532974}]}, {"text": "We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis.", "labels": [], "entities": [{"text": "translation decoding process", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.9085786938667297}]}, {"text": "We show the effectiveness of such features in both classification and regression of MT quality.", "labels": [], "entities": [{"text": "classification", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.954017162322998}, {"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9890123605728149}]}, {"text": "By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and prediction quality across multiple documents, demonstrated by the higher correlation coefficient and F-scores in finding Good sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.8579787015914917}, {"text": "consistency", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.983928918838501}, {"text": "correlation coefficient", "start_pos": 185, "end_pos": 208, "type": "METRIC", "confidence": 0.9581360816955566}, {"text": "F-scores", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9928994178771973}]}, {"text": "Additionally, the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference, with a 10% increase inhuman translators' productivity.", "labels": [], "entities": [{"text": "MT post editing", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7157806555430094}]}], "introductionContent": [{"text": "Machine translation (MT) systems suffer from an inconsistent and unstable translation quality.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8858880400657654}]}, {"text": "Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system's training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled.", "labels": [], "entities": [{"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.936660647392273}]}, {"text": "As a result, users do not know whether they can trust the translation output unless they spend time to analyze * This work was done when the author was with IBM Research.", "labels": [], "entities": []}, {"text": "This shortcoming is one of the main obstacles for the adoption of MT systems, especially in machine assisted human translation: MT post-editing, where human translators have an option to edit MT proposals or translate from scratch.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9919934272766113}, {"text": "machine assisted human translation", "start_pos": 92, "end_pos": 126, "type": "TASK", "confidence": 0.6335242912173271}, {"text": "MT post-editing", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.8715792596340179}, {"text": "MT proposals", "start_pos": 192, "end_pos": 204, "type": "TASK", "confidence": 0.8939878046512604}]}, {"text": "It has been observed that human translators often discard MT proposals even if some are very accurate.", "labels": [], "entities": [{"text": "MT proposals", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.8957624435424805}]}, {"text": "If MT proposals are used properly, post-editing can increase translators productivity and lead to significant cost savings.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9689570665359497}, {"text": "translators", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9589933156967163}]}, {"text": "Therefore, it is beneficial to provide MT confidence estimation, to help the translators to decide whether to accept MT proposals, making minor modifications on MT proposals when the quality is high or translating from scratching when the quality is low.", "labels": [], "entities": [{"text": "MT confidence estimation", "start_pos": 39, "end_pos": 63, "type": "METRIC", "confidence": 0.6678997476895651}, {"text": "MT", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9420188069343567}]}, {"text": "This will save the time of reading and parsing low quality MT and improve user experience.", "labels": [], "entities": [{"text": "parsing low quality MT", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7959441542625427}]}, {"text": "In this paper we propose an adaptive quality estimation that predicts sentence-level humantargeted translation error rate (HTER)) fora document-specific MT post-editing system.", "labels": [], "entities": [{"text": "sentence-level humantargeted translation error rate (HTER))", "start_pos": 70, "end_pos": 129, "type": "METRIC", "confidence": 0.7379293441772461}, {"text": "MT post-editing", "start_pos": 153, "end_pos": 168, "type": "TASK", "confidence": 0.8543822467327118}]}, {"text": "HTER is an ideal quality measurement for MT post editing since the reference is obtained from human correction of the MT output.", "labels": [], "entities": [{"text": "HTER", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.474165141582489}, {"text": "MT post editing", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.9395076632499695}]}, {"text": "Document-specific MT model is an MT model that is specifically built for the given input document.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9572298526763916}]}, {"text": "It is demonstrated in) that document-specific MT models significantly improve the translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.917746901512146}]}, {"text": "However, this raises two issues for quality estimation.", "labels": [], "entities": [{"text": "quality estimation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.522433802485466}]}, {"text": "First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models ().", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.9262585838635763}, {"text": "MT outputs and references", "start_pos": 164, "end_pos": 189, "type": "TASK", "confidence": 0.8685621172189713}]}, {"text": "Therefore, when the MT quality estimation model is trained, it cannot be adapted to provide accurate estimates on the outputs of document-specific MT models.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.8859596252441406}]}, {"text": "Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to users.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7336012025674185}]}, {"text": "In contrast to traditional static MT quality estimation methods, our approach not only trains the MT quality estimator dynamically for each document-specific MT model to obtain higher prediction accuracy, but also achieves consistency over different document-specific MT models.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.9184638460477194}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.858776867389679}]}, {"text": "The experiments show that our MT quality estimation is highly correlated with human judgment and helps translators to increase the MT proposal adoption rate in post-editing.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.6813765366872152}, {"text": "MT proposal adoption", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.9370170632998148}]}, {"text": "We will review related work on MT quality estimation in section 2.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8514509201049805}]}, {"text": "In section 3 we will introduce the document-specific MT system built for post-editing.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.8546097874641418}]}, {"text": "We describe the static quality estimation method in section 4, and propose the adaptive quality estimation method in section 5.", "labels": [], "entities": []}, {"text": "In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.8449057539304098}]}], "datasetContent": [{"text": "In this section, we first discuss experiments that compare adaptive QE method and static QE method on a few documents, and then present results we obtained after deploying the adaptive QE method in an English-to-Japanese MT PostEditing project.", "labels": [], "entities": [{"text": "MT PostEditing", "start_pos": 221, "end_pos": 235, "type": "TASK", "confidence": 0.5338736474514008}]}, {"text": "As mentioned before, the main motivation for us to develop MT QE classification scheme is that translators often discard good MT proposals and translate the segments from scratch.", "labels": [], "entities": [{"text": "MT QE classification", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.9625958800315857}, {"text": "MT", "start_pos": 126, "end_pos": 128, "type": "TASK", "confidence": 0.9249446392059326}]}, {"text": "We would like to provide translators with some guidance on reasonably good MT proposals-the sentences with low TERs-to help them increase the leverage on MT proposals to achieve improved productivity.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9809889197349548}, {"text": "TERs-to", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9854848980903625}, {"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.9646607637405396}]}, {"text": "Our experiment and evaluation is conducted over three documents, each with about 2000 segments.", "labels": [], "entities": []}, {"text": "We first build document-specific MT model for each document, then ask human translators to correct the MT outputs and obtain the reference translation.", "labels": [], "entities": []}, {"text": "Ina typical MT QE scenario, the QE model is pre-trained and applied to various MT outputs, even though the QE training data and MT outputs are generated from different translation models.", "labels": [], "entities": [{"text": "MT QE", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.8619519174098969}, {"text": "MT outputs", "start_pos": 79, "end_pos": 89, "type": "TASK", "confidence": 0.8869588077068329}]}, {"text": "To evaluate whether such model mismatch matters, we compare the cross-model QE with the same-model QE, where the QE training data and the MT outputs are generated from the same MT model.", "labels": [], "entities": []}, {"text": "We select one document LZA with 2067 sentences.", "labels": [], "entities": []}, {"text": "We use the first 1867 sentences to train the static QE model and the remaining 200 sentences are used as test set for TER prediction.", "labels": [], "entities": [{"text": "TER prediction", "start_pos": 118, "end_pos": 132, "type": "TASK", "confidence": 0.7309159189462662}]}, {"text": "We compute the correlation coefficient (r) between each predicted TER and true TER, as shown in.", "labels": [], "entities": [{"text": "correlation coefficient (r)", "start_pos": 15, "end_pos": 42, "type": "METRIC", "confidence": 0.956013536453247}, {"text": "TER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9769151210784912}, {"text": "TER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8079650402069092}]}, {"text": "We find that the TER predictions are reasonably correct when the training and test sentences are from the same MT model (the top, with correlation coefficients around 0.5.", "labels": [], "entities": [{"text": "TER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.8870149850845337}]}, {"text": "For the crossmodel QE, we train a static QE model with 1867 sentences from another document RTW, and use it to predict the TER of the same 200 sentences from document LZA (the bottom.", "labels": [], "entities": [{"text": "RTW", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9250484108924866}, {"text": "TER", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9985027313232422}]}, {"text": "We observe significant degradation of correlation coefficient, dropping from 0.5 to 0.1.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 38, "end_pos": 61, "type": "METRIC", "confidence": 0.9844478368759155}]}, {"text": "This degradation and unstable nature is the prime motivation to develop a more robust MT quality estimation model.", "labels": [], "entities": [{"text": "MT quality estimation", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.8463015556335449}]}, {"text": "We select 1700 sentences from multiple previously translated documents as the QE training data, which are independent of the test documents.", "labels": [], "entities": [{"text": "QE training data", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.7875125805536906}]}, {"text": "We train the static QE model with this training set, including the source sentences, references and MT outputs (from multiple translation models).", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9364781379699707}]}, {"text": "To train the adaptive QE model for each test document, we build a translation model whose subsampling data includes source sentences from both the test document and the QE training data.", "labels": [], "entities": [{"text": "QE training data", "start_pos": 169, "end_pos": 185, "type": "DATASET", "confidence": 0.7362784147262573}]}, {"text": "We translate the QE source sentences with this newly built MT model, and the translation output is used to train the QE model specific to each test document.", "labels": [], "entities": []}, {"text": "We compare these two QE models on three documents, LZA, RTW and WC7, measuring rand RMSE for each QE model.", "labels": [], "entities": [{"text": "LZA", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.6885372996330261}, {"text": "RTW", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.5157209038734436}]}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "We find that the adaptive QE model demonstrates higher rand lower RMSE than the static QE model for all the test documents.", "labels": [], "entities": [{"text": "rand", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9577510356903076}, {"text": "RMSE", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9694586992263794}]}, {"text": "Besides the general correlation with human judgment, we particularly focus on those reasonably good translations, i.e., the sentences with low TERs which can help improve the translator's productivity most.", "labels": [], "entities": [{"text": "TERs", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9983201622962952}]}, {"text": "Here we report the precision, recall and F-score of finding such \"Good\" sentences (with TER \u2264 0.1) on the three documents in.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9997590184211731}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9995161294937134}, {"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9995442032814026}, {"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9984560012817383}]}, {"text": "Again, the adaptive QE model produces higher recall, mostly higher precision, and significantly improved F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9996933937072754}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9995014667510986}, {"text": "F-score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9987404942512512}]}, {"text": "The overall F-score of the adaptive QE model is 0.28 2 . Compared with the static QE model's 0.17 F-score, this is relatively 64% improvement.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9991171956062317}, {"text": "F-score", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9894267320632935}]}, {"text": "In the adaptive QE model, the source side QE training data is included in the subsampling process to build the document-specific MT model.", "labels": [], "entities": [{"text": "MT", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.9088123440742493}]}, {"text": "It would be interesting to know whether this process will negatively affect the MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9898784160614014}]}, {"text": "We evaluate the TER of MT outputs with and without the adaptive QE training on the same three documents.", "labels": [], "entities": [{"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9976851940155029}, {"text": "MT outputs", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.8809784948825836}]}, {"text": "As seen in, we do not notice translation quality degradation.", "labels": [], "entities": []}, {"text": "Instead, we observe slightly improvement on two document, with TERs reduction by 0.1-0.4 pt.", "labels": [], "entities": [{"text": "TERs", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.998995840549469}]}, {"text": "As our MT model training data include proprietary data, the MT performance is significantly better than publicly available MT software.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9629987478256226}, {"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9678153395652771}]}], "tableCaptions": [{"text": " Table 2: QE regression with static and adaptive models", "labels": [], "entities": [{"text": "QE regression", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7411936223506927}]}, {"text": " Table 3: Performance on predicting Good sentences with static and adaptive models", "labels": [], "entities": [{"text": "predicting Good sentences", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.8825366099675497}]}, {"text": " Table 4: MT Quality with and without Adaptive  QE measured by TER", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9580486416816711}, {"text": "Adaptive  QE", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.6116240918636322}, {"text": "TER", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.7970613241195679}]}, {"text": " Table 6: MT proposal usage and productivity gain in FM category.  In FM1, both Fuzzy Match and MT proposals present. In control class FM0, only Fuzzy Match proposals  present, and therefore, MT usage is not available for FM0. Strong correlation is observed between  predicted \"High\" , \"Medium\" and \"Low\" sentences with MT usage and post editing productivity.", "labels": [], "entities": [{"text": "MT proposal", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9053662419319153}]}, {"text": " Table 7: MT proposal usage and productivity gain in NP category.  In NP1, MT is the only proposal available, while in control NP0, there presents no proposal at all and  the translator has to translate from scratch. Strong correlation is observed between predicted \"High\" ,  \"Medium\" and \"Low\" sentences with MT usage and post editing productivity", "labels": [], "entities": [{"text": "MT proposal", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8975691795349121}]}, {"text": " Table 5: Performance on predicting Good sen- tences (TER \u2264 0.1) by adaptive QE model", "labels": [], "entities": [{"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9937909245491028}]}]}