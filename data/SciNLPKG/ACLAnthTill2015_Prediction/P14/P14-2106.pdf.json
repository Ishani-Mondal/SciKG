{"title": [{"text": "On WordNet Semantic Classes and Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.5316400527954102}]}], "abstractContent": [{"text": "This paper presents experiments with WordNet semantic classes to improve dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.8166808187961578}]}, {"text": "We study the effect of semantic classes in three dependency parsers, using two types of constituency-to-dependency conversions of the English Penn Treebank.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 134, "end_pos": 155, "type": "DATASET", "confidence": 0.8389424482981364}]}, {"text": "Overall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags (Agirre et al., 2011).", "labels": [], "entities": []}, {"text": "In addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion.", "labels": [], "entities": []}], "introductionContent": [{"text": "This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English.", "labels": [], "entities": [{"text": "dependency parsing of English", "start_pos": 98, "end_pos": 127, "type": "TASK", "confidence": 0.8389733880758286}]}, {"text": "Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics.", "labels": [], "entities": []}, {"text": "Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies, and systems using dynamic semantic clusters automatically acquired from corpora (.", "labels": [], "entities": []}, {"text": "Our main objective will be to determine whether static semantic knowledge can help parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.9638979434967041}]}, {"text": "We will apply different types of semantic information to three dependency parsers.", "labels": [], "entities": []}, {"text": "Specifically, we will test the following questions: \u2022 Does semantic information in WordNet help dependency parsing?", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7580389082431793}]}, {"text": "found improvements in dependency parsing using MaltParser on gold POS tags.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8602461516857147}]}, {"text": "In this work, we will investigate the effect of semantic information using predicted POS tags.", "labels": [], "entities": []}, {"text": "\u2022 Is the type of semantic information related to the type of parser?", "labels": [], "entities": []}, {"text": "We will test three different parsers representative of successful paradigms in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.851179838180542}]}, {"text": "\u2022 How does the semantic information relate to the style of dependency annotation?", "labels": [], "entities": []}, {"text": "Most experiments for English were evaluated on the Penn2Malt conversion of the constituencybased Penn Treebank.", "labels": [], "entities": [{"text": "Penn2Malt conversion", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.9616238176822662}, {"text": "constituencybased Penn Treebank", "start_pos": 79, "end_pos": 110, "type": "DATASET", "confidence": 0.8395569523175558}]}, {"text": "We will also examine the LTH conversion, with richer structure and an extended set of dependency labels.", "labels": [], "entities": [{"text": "LTH conversion", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7747621238231659}]}, {"text": "\u2022 How does WordNet compare to automatically obtained information?", "labels": [], "entities": [{"text": "WordNet", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8679894804954529}]}, {"text": "For the sake of comparison, we will also perform the experiments using syntactic/semantic clusters automatically acquired from corpora.", "labels": [], "entities": []}, {"text": "\u2022 Does parser combination benefit from semantic information?", "labels": [], "entities": []}, {"text": "Different parsers can use semantic information in diverse ways.", "labels": [], "entities": []}, {"text": "For example, while MaltParser can use the semantic information in local contexts, MST can incorporate them in global contexts.", "labels": [], "entities": []}, {"text": "We will run parser combination experiments with and without semantic information, to determine whether it is useful in the combined parsers.", "labels": [], "entities": []}, {"text": "After introducing related work in section 2, section 3 describes the treebank conversions, parsers and semantic features.", "labels": [], "entities": []}, {"text": "Section 4 presents the results and section 5 draws the main conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2).", "labels": [], "entities": [{"text": "PTBbased datasets", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.971435934305191}]}, {"text": "Finally, we will describe the different types of semantic representation that were used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: LAS results with several parsing algo- rithms, Penn2Malt conversion ( \u2020: p <0.05,  \u2021: p  <0.005). In parenthesis, difference with baseline.", "labels": [], "entities": [{"text": "parsing algo- rithms", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.807880163192749}, {"text": "Penn2Malt", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9012938737869263}]}, {"text": " Table 2: LAS results with several parsing algo- rithms in the LTH conversion ( \u2020: p <0.05,  \u2021: p  <0.005). In parenthesis, difference with baseline.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6236897706985474}]}, {"text": " Table 3: Parser combinations on Penn2Malt.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9888294339179993}]}, {"text": " Table 4: Parser combinations on LTH ( \u2020: p <0.05,   \u2021: p <0.005).", "labels": [], "entities": []}, {"text": " Table 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor- rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).", "labels": [], "entities": [{"text": "Differences in LAS (LTH)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7566419243812561}]}]}