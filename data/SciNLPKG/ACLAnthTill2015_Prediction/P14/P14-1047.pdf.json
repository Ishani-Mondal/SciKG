{"title": [{"text": "Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies", "labels": [], "entities": []}], "abstractContent": [{"text": "We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario.", "labels": [], "entities": []}, {"text": "Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from.", "labels": [], "entities": []}, {"text": "In particular, we compare the Q-learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate.", "labels": [], "entities": []}, {"text": "Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly.", "labels": [], "entities": []}, {"text": "We also show that very high gradually decreasing exploration rates are required for convergence.", "labels": [], "entities": [{"text": "convergence", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9751011729240417}]}, {"text": "We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "The dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e., dialogue context).", "labels": [], "entities": []}, {"text": "Building a dialogue policy can be a challenging task especially for complex applications.", "labels": [], "entities": []}, {"text": "For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies).", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8640968203544617}, {"text": "Reinforcement Learning (RL) of dialogue policies", "start_pos": 128, "end_pos": 176, "type": "TASK", "confidence": 0.7372240498661995}]}, {"text": "Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of areal user ;); (2) learn directly from a corpus (; or (3) learn via live interaction with human users (.", "labels": [], "entities": []}, {"text": "We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques.", "labels": [], "entities": []}, {"text": "Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus.", "labels": [], "entities": []}, {"text": "As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users.", "labels": [], "entities": []}, {"text": "Moreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU.", "labels": [], "entities": []}, {"text": "Unlike slot-filling domains, in negotiation the behaviors of the system and the user are symmetric.", "labels": [], "entities": []}, {"text": "They are both negotiators, thus building a good SU is as difficult as building a good system policy.", "labels": [], "entities": [{"text": "SU", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9463916420936584}]}, {"text": "So far research on using RL for dialogue policy learning has focused on single-agent RL techniques.", "labels": [], "entities": [{"text": "dialogue policy learning", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.90898597240448}]}, {"text": "Single-agent RL methods make the assumption that the system learns by interacting with a stationary environment, i.e., an environment that does not changeover time.", "labels": [], "entities": []}, {"text": "Here the environment is the user.", "labels": [], "entities": []}, {"text": "Generally the assumption that users do not significantly change their behavior overtime holds for simple information providing tasks (e.g., reserving a flight).", "labels": [], "entities": []}, {"text": "But this is not necessarily the case for other genres of dialogue, including negotiation.", "labels": [], "entities": []}, {"text": "Imagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her.", "labels": [], "entities": []}, {"text": "Therefore it is important to investigate RL approaches that do not make such assumptions about the user/environment.", "labels": [], "entities": [{"text": "RL", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9799364805221558}]}, {"text": "Multi-agent RL is designed to work for nonstationary environments.", "labels": [], "entities": [{"text": "Multi-agent RL", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.5909391045570374}]}, {"text": "In this case the environment of a learning agent is one or more other agents that can also be learning at the same time.", "labels": [], "entities": []}, {"text": "Therefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios.", "labels": [], "entities": []}, {"text": "This ability of multi-agent RL can also have important implications for learning via live interaction with human users.", "labels": [], "entities": []}, {"text": "Imagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants.", "labels": [], "entities": []}, {"text": "We apply multi-agent RL to a resource allocation negotiation scenario.", "labels": [], "entities": [{"text": "resource allocation negotiation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.7156923015912374}]}, {"text": "Two agents with different preferences negotiate about how to share resources.", "labels": [], "entities": []}, {"text": "We compare Q-learning (a singleagent RL algorithm) with two multi-agent RL algorithms: Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF)).", "labels": [], "entities": []}, {"text": "We vary the scenario complexity (i.e., the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate.", "labels": [], "entities": []}, {"text": "Our research contributions are as follows: (1) we propose concurrent learning using multi-agent RL as away to deal with some of the issues of current approaches to dialogue policy learning (i.e., the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior overtime, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHCWoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHCWoLF in such a variety of situations (varying a large number of parameters).", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work.", "labels": [], "entities": []}, {"text": "Section 3 provides a brief introduction to single-agent RL and multi-agent RL.", "labels": [], "entities": [{"text": "multi-agent RL", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.5808690935373306}]}, {"text": "Section 4 describes our negotiation domain and experimental setup.", "labels": [], "entities": []}, {"text": "In section 5 we present our results.", "labels": [], "entities": []}, {"text": "Finally, section 6 concludes and provides some ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our domain is a resource allocation negotiation scenario.", "labels": [], "entities": [{"text": "resource allocation negotiation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.7572723825772604}]}, {"text": "Two agents negotiate about how to share resources.", "labels": [], "entities": []}, {"text": "For the sake of readability from now on we will refer to apples and oranges.", "labels": [], "entities": []}, {"text": "The two agents have different goals.", "labels": [], "entities": []}, {"text": "Also, they have human-like constraints of imperfect information about each other; they do not know each other's reward function or degree of rationality (during learning our agents can be irrational).", "labels": [], "entities": []}, {"text": "Thus a Nash equilibrium (if there exists one) cannot be computed in advance.", "labels": [], "entities": []}, {"text": "Agent 1 cares more about apples and Agent 2 cares more about oranges.", "labels": [], "entities": []}, {"text": "shows the points that Agents 1 and 2 earn for each apple and each orange that they have at the end of the negotiation.: Points earned by Agents 1 and 2 for each apple and each orange that they have at the end of the negotiation.", "labels": [], "entities": []}, {"text": "We use a simplified dialogue model with two types of speech acts: offers and acceptances.", "labels": [], "entities": []}, {"text": "The dialogue proceeds as follows: one agent makes an offer, e.g., \"I give you 3 apples and 1 orange\", and the other agent may choose to accept it or make anew offer.", "labels": [], "entities": []}, {"text": "The negotiation finishes when one of the agents accepts the other agent's offer or time runs out.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Points earned by Agents 1 and 2 for each  apple and each orange that they have at the end of  the negotiation.", "labels": [], "entities": []}, {"text": " Table 3: State space, action space, and state-action  space sizes for different numbers of apples and or- anges to be shared (A: apples, O: oranges).", "labels": [], "entities": []}, {"text": " Table 4: Average distance from convergence re- ward over 20 runs for 100,000 episodes per epoch  and for different numbers of fruits to be shared (A:  apples, O: oranges). The best possible value is 10.", "labels": [], "entities": []}]}