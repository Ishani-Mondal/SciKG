{"title": [{"text": "Multilingual Models for Compositional Distributed Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings.", "labels": [], "entities": []}, {"text": "Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences.", "labels": [], "entities": []}, {"text": "The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7185507118701935}]}, {"text": "We extend our approach to learn semantic representations at the document level, too.", "labels": [], "entities": []}, {"text": "We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art.", "labels": [], "entities": [{"text": "document classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.7143405079841614}]}, {"text": "Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations of words provide the basis for many state-of-the-art approaches to various problems in natural language processing today.", "labels": [], "entities": []}, {"text": "Such word embeddings are naturally richer representations than those of symbolic or discrete models, and have been shown to be able to capture both syntactic and semantic information.", "labels": [], "entities": []}, {"text": "Successful applications of such models include language modelling (, paraphrase detection, and dialogue analysis.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.788168728351593}, {"text": "paraphrase detection", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.9450496435165405}, {"text": "dialogue analysis", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.8797393441200256}]}, {"text": "Within a monolingual context, the distributional hypothesis forms the basis of most approaches for learning word representations.", "labels": [], "entities": [{"text": "learning word representations", "start_pos": 99, "end_pos": 128, "type": "TASK", "confidence": 0.6448034743467966}]}, {"text": "In this work, we extend this hypothesis to multilingual data and joint-space embeddings.", "labels": [], "entities": []}, {"text": "We present a novel unsupervised technique for learning semantic representations that leverages parallel corpora and employs semantic transfer through compositional representations.", "labels": [], "entities": []}, {"text": "Unlike most methods for learning word representations, which are restricted to a single language, our approach learns to represent meaning across languages in a shared multilingual semantic space.", "labels": [], "entities": []}, {"text": "We present experiments on two corpora.", "labels": [], "entities": []}, {"text": "First, we show that for cross-lingual document classification on the Reuters RCV1/RCV2 corpora (), we outperform the prior state of the art ().", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.6895780861377716}, {"text": "Reuters RCV1/RCV2 corpora", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.9524407267570496}]}, {"text": "Second, we also present classification results on a massively multilingual corpus which we derive from the TED corpus ().", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.8787156343460083}]}, {"text": "The results on this task, in comparison with a number of strong baselines, further demonstrate the relevance of our approach and the success of our method in learning multilingual semantic representations over a wide range of languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report results on two experiments.", "labels": [], "entities": []}, {"text": "First, we replicate the cross-lingual document classification task of, learning distributed representations on the Europarl corpus and evaluating on documents from the Reuters RCV1/RCV2 corpora.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.6173474987347921}, {"text": "Europarl corpus", "start_pos": 115, "end_pos": 130, "type": "DATASET", "confidence": 0.9932602643966675}, {"text": "Reuters RCV1/RCV2 corpora", "start_pos": 168, "end_pos": 193, "type": "DATASET", "confidence": 0.9478356957435607}]}, {"text": "Subsequently, we design a multi-label classification task using the TED corpus, both for training and evaluating.", "labels": [], "entities": [{"text": "multi-label classification task", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7268866797288259}, {"text": "TED corpus", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.8796869218349457}]}, {"text": "The use of a wider range of languages in the second experiments allows us to better evaluate our models' capabilities in learning a shared multilingual semantic representation.", "labels": [], "entities": []}, {"text": "We also investigate the learned embeddings from a qualitative perspective in \u00a75.4.", "labels": [], "entities": []}, {"text": "Here we describe our experiments on the TED corpus, which enables us to scale up to multilingual learning.", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.829033225774765}]}, {"text": "Consisting of a large number of relatively short and parallel documents, this corpus allows us to evaluate the performance of the DOC model described in \u00a73.2.", "labels": [], "entities": []}, {"text": "We use the training data of the corpus to learn distributed representations across 12 languages.", "labels": [], "entities": []}, {"text": "Training is performed in two settings.", "labels": [], "entities": []}, {"text": "In the single mode, vectors are learnt from a single language pair (en-X), while in the joint mode vectorlearning is performed on all parallel sub-corpora simultaneously.", "labels": [], "entities": []}, {"text": "This setting causes words from all languages to be embedded in a single semantic space.", "labels": [], "entities": []}, {"text": "First, we evaluate the effect of the documentlevel error signal (DOC, described in \u00a73.2), as well as whether our multilingual learning method can extend to a larger variety of languages.", "labels": [], "entities": []}, {"text": "We train DOC models, using both ADD and BI as CVM (DOC/ADD, DOC/BI), both in the single and joint mode.", "labels": [], "entities": [{"text": "BI", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9747786521911621}, {"text": "BI", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.8797111511230469}]}, {"text": "For comparison, we also train ADD and DOC models without the document-level error signal.", "labels": [], "entities": []}, {"text": "The resulting document-level representations are used to train classifiers (system and settings as in \u00a75.2) for each language, which are then evaluated in the paired language.", "labels": [], "entities": []}, {"text": "In the English case we train twelve individual classifiers, each using the training data of a single language pair only.", "labels": [], "entities": []}, {"text": "As described in \u00a74, we use 15 keywords for the classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9013658761978149}]}, {"text": "Due to space limitations, we report cumulative results in the form of F1-scores throughout this paper.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9982555508613586}]}, {"text": "MT System We develop a machine translation baseline as follows.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.812258780002594}, {"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7385271787643433}]}, {"text": "We train a machine translation tool on the parallel training data, using the development data of each language pair to optimize the translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7129476070404053}]}, {"text": "We use the cdec decoder () with default settings for this purpose.", "labels": [], "entities": []}, {"text": "With this system we translate the test data, and then use a Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier for the actual experiments.", "labels": [], "entities": []}, {"text": "To exemplify, this means the de\u2192ar result is produced by training a translation system from Arabic to German.", "labels": [], "entities": []}, {"text": "The Arabic test set is translated into German.", "labels": [], "entities": [{"text": "Arabic test set", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8166199723879496}]}, {"text": "A classifier is then trained for model descriptions).", "labels": [], "entities": []}, {"text": "The left chart shows results for these models when trained on German data and evaluated on English data, the right chart vice versa.", "labels": [], "entities": []}, {"text": "on the German training data and evaluated on the translated Arabic.", "labels": [], "entities": [{"text": "German training data", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.8577370246251425}]}, {"text": "While we developed this system as a baseline, it must be noted that the classifier of this system has access to significantly more information (all words in the document) as opposed to our models (one embedding per document), and we do not expect to necessarily beat this system.", "labels": [], "entities": []}, {"text": "The results of this experiment are in.", "labels": [], "entities": []}, {"text": "When comparing the results between the ADD model and the models trained using the documentlevel error signal, the benefit of this additional signal becomes clear.", "labels": [], "entities": []}, {"text": "The joint training mode leads to a relative improvement when training on English data and evaluating in a second language.", "labels": [], "entities": []}, {"text": "This suggests that the joint mode improves the quality of the English embeddings more than it affects the L2-embeddings.", "labels": [], "entities": []}, {"text": "More surprising, perhaps, is the relative performance between the ADD and BI composition functions, especially when compared to the results in \u00a75.2, where the BI models relatively consistently performed better.", "labels": [], "entities": []}, {"text": "We suspect that the better performance of the additive composition function on this task is related to the smaller amount of training data available which could cause sparsity issues for the bigram model.", "labels": [], "entities": []}, {"text": "As expected, the MT system slightly outperforms our models on most language pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9683393836021423}]}, {"text": "However, the overall performance of the models is comparable to that of the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9028881192207336}]}, {"text": "Considering the relative amount of information available during the classifier training phase, this indicates that our learned representations are semantically useful, capturing almost the same amount of information as available to the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier.", "labels": [], "entities": []}, {"text": "We next investigate linguistic transfer across languages.", "labels": [], "entities": [{"text": "linguistic transfer across languages", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.8072195798158646}]}, {"text": "We re-use the embeddings learned with the DOC/ADD joint model from the previous experiment for this purpose, and train classifiers on all non-English languages using those embeddings.", "labels": [], "entities": [{"text": "DOC/ADD joint model", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.7953668594360351}]}, {"text": "Subsequently, we evaluate their performance in classifying documents in the remaining languages.", "labels": [], "entities": []}, {"text": "Results for this task are in.", "labels": [], "entities": []}, {"text": "While the results across language-pairs might not be very insightful, the overall good performance compared with the results in implies that we learnt semantically meaningful vectors and in fact a joint embedding space across thirteen languages.", "labels": [], "entities": []}, {"text": "Ina third evaluation, we apply the embeddings learnt without models to a monolingual classification task, enabling us to compare with prior work on distributed representation learning.", "labels": [], "entities": []}, {"text": "In this experiment a classifier is trained in one language and then evaluated in the same.", "labels": [], "entities": []}, {"text": "We again use a Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier on the raw data to establish a reasonable upper bound.", "labels": [], "entities": []}, {"text": "We compare our embeddings with the SENNA embeddings, which achieve state of the art performance on a number of tasks).", "labels": [], "entities": [{"text": "SENNA embeddings", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.8592125177383423}]}, {"text": "Additionally, we use the Polyglot embeddings of Al-Rfou, who published word embeddings across 100 languages, including all languages considered in this paper.", "labels": [], "entities": []}, {"text": "We represent each document by the mean of its word vectors and then apply the same classifier training and testing regime as with our models.", "labels": [], "entities": []}, {"text": "Even though both of these sets of embeddings were trained on much larger datasets than ours, our models outperform these baselines on all languages-even outperforming the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes system on on several  languages.", "labels": [], "entities": []}, {"text": "While this may partly be attributed to the fact that our vectors were learned on in-domain data, this is still a very positive outcome.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F1-scores for the TED document classification task for individual languages. Results are re- ported for both directions (training on English, evaluating on L2 and vice versa). Bold indicates best  result, underline best result amongst the vector-based systems.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985902905464172}, {"text": "TED document classification task", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8287755250930786}]}, {"text": " Table 4: F1-scores on the TED corpus document classification task when training and evaluating on the  same language. Baseline embeddings are Senna (Collobert et al., 2011) and Polyglot (Al-Rfou' et al.,  2013).", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9980385899543762}, {"text": "TED corpus document classification task", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.765239131450653}]}]}