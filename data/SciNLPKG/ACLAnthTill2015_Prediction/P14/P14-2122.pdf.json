{"title": [{"text": "Empirical Study of Unsupervised Chinese Word Segmentation Methods for SMT on Large-scale Corpora", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6237020393212637}, {"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9947296380996704}]}], "abstractContent": [{"text": "Unsupervised word segmentation (UWS) can provide domain-adaptive segmenta-tion for statistical machine translation (SMT) without annotated data, and bilingual UWS can even optimize segmenta-tion for alignment.", "labels": [], "entities": [{"text": "word segmentation (UWS", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7872338071465492}, {"text": "statistical machine translation (SMT)", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.7992774744828542}]}, {"text": "Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9985200762748718}]}, {"text": "This paper proposes an efficient unified PYP-based monolingual and bilingual UWS method.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed method is comparable to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain.", "labels": [], "entities": [{"text": "NIST OpenMT corpus", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.8480336666107178}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9992108345031738}, {"text": "NTCIR PatentMT corpus", "start_pos": 171, "end_pos": 192, "type": "DATASET", "confidence": 0.8450527787208557}]}], "introductionContent": [{"text": "Many languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT.", "labels": [], "entities": [{"text": "word segmentation (WS)", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.8342846870422364}, {"text": "SMT", "start_pos": 304, "end_pos": 307, "type": "TASK", "confidence": 0.9937356114387512}]}, {"text": "Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety.", "labels": [], "entities": []}, {"text": "For example, in machine translation, there are various parallel corpora such as BTEC for tourism-related dialogues and PatentMT in the patent domain , but researchers working on Chineserelated tasks often use the Stanford Chinese segmenter () which is trained on a small amount of annotated news text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7488932311534882}, {"text": "PatentMT", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.5999587774276733}]}, {"text": "In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (, relies on statistical criteria instead of manually crafted standards.", "labels": [], "entities": []}, {"text": "UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required.", "labels": [], "entities": []}, {"text": "The approaches of explicitly modeling the probability of words) significantly outperformed a heuristic approach ( on the monolingual Chinese SIGHAN-MSR corpus, which inspired the work of this paper.", "labels": [], "entities": [{"text": "Chinese SIGHAN-MSR corpus", "start_pos": 133, "end_pos": 158, "type": "DATASET", "confidence": 0.6630328396956126}]}, {"text": "However, bilingual approaches that model word probabilities suffer from computational complexity.", "labels": [], "entities": []}, {"text": "proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data.", "labels": [], "entities": [{"text": "generative", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.9596074223518372}, {"text": "BTEC data", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.9559493660926819}]}, {"text": "used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored.", "labels": [], "entities": [{"text": "speed", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9672466516494751}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9994925260543823}]}, {"text": "This paper is dedicated to bilingual UWS on large-scale corpora to support SMT.", "labels": [], "entities": [{"text": "UWS", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8439721465110779}, {"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9915602803230286}]}, {"text": "To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training.", "labels": [], "entities": [{"text": "expectation maximization (EM)", "start_pos": 147, "end_pos": 176, "type": "METRIC", "confidence": 0.8422364711761474}]}, {"text": "We aware that variational bayes (VB) maybe used for speeding up the training of DP-based or PYP-based bilingual UWS.", "labels": [], "entities": [{"text": "variational bayes (VB)", "start_pos": 14, "end_pos": 36, "type": "METRIC", "confidence": 0.9483621835708618}]}, {"text": "However, VB requires formulating them expectations of (m \u2212 1)-dimensional marginal distributions, where m is the number of hidden variables.", "labels": [], "entities": [{"text": "VB", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.8096534609794617}]}, {"text": "For UWS, the hidden variables are indicators that identify substrings of sentences in the corpus as words.", "labels": [], "entities": []}, {"text": "These variables are large in number and it is not clear how to apply VB to UWS, and as far the authors aware there is no previous work related to the application of VB to monolingual UWS.", "labels": [], "entities": []}, {"text": "Therefore, we have not explored VB methods in this paper, but we do show that our method is superior to the existing methods.", "labels": [], "entities": [{"text": "VB", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.871449887752533}]}, {"text": "The contributions of this paper include, \u2022 state-of-the-art accuracy in monolingual UWS; \u2022 the first bilingual UWS method practical for large corpora; \u2022 improvement of BLEU scores compared to supervised Stanford Chinese word segmenter.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.995151162147522}, {"text": "BLEU scores", "start_pos": 168, "end_pos": 179, "type": "METRIC", "confidence": 0.974283367395401}, {"text": "Stanford Chinese word segmenter", "start_pos": 203, "end_pos": 234, "type": "TASK", "confidence": 0.5547502934932709}]}], "datasetContent": [{"text": "In this section, the proposed method is first validated on monolingual segmentation tasks, and then evaluated in the context of SMT to study whether the translation quality, measured by BLEU, can be improved.", "labels": [], "entities": [{"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.990387499332428}, {"text": "BLEU", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9982216954231262}]}, {"text": "Two monolingual corpora and two bilingual corpora are used.", "labels": [], "entities": []}, {"text": "CHILDES is the most common test.", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7484732866287231}]}, {"text": "This data set mainly consists of news text 3 . PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences.", "labels": [], "entities": [{"text": "PatentMT9", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.9517695903778076}, {"text": "NTCIR-9 patent machine translation", "start_pos": 84, "end_pos": 118, "type": "TASK", "confidence": 0.8066779375076294}]}], "tableCaptions": [{"text": " Table 3: Results on Monolingual Corpora.", "labels": [], "entities": [{"text": "Monolingual Corpora", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7689403295516968}]}, {"text": " Table 4: Results on Bilingual Corpora.", "labels": [], "entities": [{"text": "Bilingual Corpora", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.6677750051021576}]}]}