{"title": [{"text": "A Discriminative Graph-Based Parser for the Abstract Meaning Representation", "labels": [], "entities": [{"text": "Abstract Meaning Representation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6776820619901022}]}], "abstractContent": [{"text": "Meaning Representation (AMR) is a semantic formalism for which a growing set of annotated examples is available.", "labels": [], "entities": [{"text": "Meaning Representation (AMR)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8737176775932312}]}, {"text": "We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement.", "labels": [], "entities": [{"text": "parse sentences", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8847757875919342}]}, {"text": "The method is based on a novel algorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints.", "labels": [], "entities": []}, {"text": "Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6836609542369843}]}, {"text": "Our open-source system , JAMR, is available at: http://github.com/jflanigan/jamr", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing is the problem of mapping natural language strings into meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8230627179145813}]}, {"text": "Abstract Meaning Representation (AMR) () is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8769742449124655}]}, {"text": "Nodes represent concepts, and labeled directed edges represent the relationships between them-see for an example AMR graph.", "labels": [], "entities": []}, {"text": "The formalism is based on propositional logic and neo-Davidsonian event representations.", "labels": [], "entities": []}, {"text": "Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications-in particular, machine translation-in mind.", "labels": [], "entities": [{"text": "machine translation-in", "start_pos": 171, "end_pos": 193, "type": "TASK", "confidence": 0.719855397939682}]}, {"text": "In this paper we introduce JAMR, the first published system for automatic AMR parsing.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.5191469788551331}, {"text": "AMR parsing", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9043644070625305}]}, {"text": "The system is based on a statistical model whose parameters are trained discriminatively using annotated sentences in the AMR Bank corpus ().", "labels": [], "entities": [{"text": "AMR Bank corpus", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.986953596274058}]}, {"text": "We evaluate using the Smatch score , establishing a baseline for future work.", "labels": [], "entities": [{"text": "Smatch score", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.5280269235372543}]}, {"text": "The core of JAMR is a two-part algorithm that first identifies concepts using a semi-Markov model and then identifies the relations that obtain between these by searching for the maximum spanning connected subgraph (MSCG) from an edge-labeled, directed graph representing all possible relations between the identified concepts.", "labels": [], "entities": []}, {"text": "To solve the latter problem, we introduce an apparently novel O(|V | 2 log |V |) algorithm that is similar to the maximum spanning tree (MST) algorithms that are widely used for dependency parsing ().", "labels": [], "entities": [{"text": "O", "start_pos": 62, "end_pos": 63, "type": "METRIC", "confidence": 0.9562563896179199}, {"text": "dependency parsing", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.8277687132358551}]}, {"text": "Our MSCG algorithm returns the connected subgraph with maximal sum of its edge weights from among all connected subgraphs of the input graph.", "labels": [], "entities": []}, {"text": "Since AMR imposes additional constraints to ensure semantic well-formedness, we use Lagrangian relaxation) to augment the MSCG algorithm, yielding a tractable iterative algorithm that finds the optimal solution subject to these constraints.", "labels": [], "entities": []}, {"text": "In our experiments, we have found this algorithm to converge 100% of the time for the constraint set we use.", "labels": [], "entities": []}, {"text": "The approach can be understood as an alternative to parsing approaches using graph transducers such as (synchronous) hyperedge replacement grammars (;, in much the same way that spanning tree algorithms are an alternative to using shift-reduce and dynamic programming algorithms for dependency parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9675930738449097}, {"text": "dependency parsing", "start_pos": 283, "end_pos": 301, "type": "TASK", "confidence": 0.824137955904007}]}, {"text": "While a detailed  comparison of these two approaches is beyond the scope of this paper, we emphasize that-as has been observed with dependency parsing-a diversity of approaches can shed light on complex problems such as semantic parsing.", "labels": [], "entities": [{"text": "dependency parsing-a", "start_pos": 132, "end_pos": 152, "type": "TASK", "confidence": 0.7908703088760376}, {"text": "semantic parsing", "start_pos": 220, "end_pos": 236, "type": "TASK", "confidence": 0.7694922089576721}]}], "datasetContent": [{"text": "We evaluate our parser on the newswire section of LDC2013E117 (deft-amr-release-r3-proxy.txt).", "labels": [], "entities": [{"text": "newswire section of LDC2013E117", "start_pos": 30, "end_pos": 61, "type": "DATASET", "confidence": 0.8901229947805405}]}, {"text": "Statistics about this corpus and our train/dev./test splits are given in   For the performance of concept identification, we report precision, recall, and F 1 of labeled spans using the induced labels on the training and test data as a gold standard.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.763339102268219}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9995484948158264}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9992497563362122}, {"text": "F 1", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9864296913146973}]}, {"text": "Our concept identifier achieves 84% F 1 on the test data.", "labels": [], "entities": [{"text": "F 1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9770437180995941}]}, {"text": "Precision is roughly the same between train and test, but recall is worse on test, implicating unseen concepts as a significant source of errors on test data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9896345734596252}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9988956451416016}]}, {"text": "We evaluate the performance of the full parser using Smatch v1.0 , which counts the precision, recall and F 1 of the concepts and relations together.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9995720982551575}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9988777041435242}, {"text": "F 1", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9945058822631836}]}, {"text": "Using the full pipeline (concept identification and relation identification stages), our parser achieves 58% F 1 on the test data.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.724330335855484}, {"text": "relation identification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.6691273897886276}, {"text": "F 1", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9945617616176605}]}, {"text": "Using gold concepts with the relation identification stage yields a much higher Smatch score of 80% F 1 . As a comparison, AMR Bank annotators have a consensus inter-annotator agreement Smatch score of 83% F 1 . The runtime of our system is given in.", "labels": [], "entities": [{"text": "relation identification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.8492614030838013}, {"text": "F 1", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9858879148960114}, {"text": "AMR Bank annotators", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.9313104152679443}, {"text": "F", "start_pos": 206, "end_pos": 207, "type": "METRIC", "confidence": 0.9848905205726624}]}, {"text": "The large drop in performance of 22% F 1 when moving from gold concepts to system concepts suggests that joint inference and training for the two stages might be helpful.", "labels": [], "entities": [{"text": "F 1", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9953467547893524}]}], "tableCaptions": [{"text": " Table 3. Split  Document Years Sentences Tokens  Train  1995-2006  4.0k  79k  Dev.  2007  2.1k  40k  Test  2008  2.1k  42k", "labels": [], "entities": [{"text": "Split  Document Years Sentences Tokens  Train  1995-2006  4.0k  79k  Dev.  2007  2.1k  40k  Test  2008  2.1k  42k", "start_pos": 10, "end_pos": 123, "type": "DATASET", "confidence": 0.715517219569948}]}, {"text": " Table 4: Concept identification performance.", "labels": [], "entities": [{"text": "Concept identification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8558071851730347}]}]}