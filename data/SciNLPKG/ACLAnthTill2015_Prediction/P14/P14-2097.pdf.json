{"title": [{"text": "Nonparametric Method for Data-driven Image Captioning", "labels": [], "entities": [{"text": "Image Captioning", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.6825007051229477}]}], "abstractContent": [{"text": "We present a nonparametric density estimation technique for image caption generation.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8438740173975626}]}, {"text": "Data-driven matching methods have shown to be effective fora variety of complex problems in Computer Vision.", "labels": [], "entities": [{"text": "Data-driven matching", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6635408103466034}, {"text": "Computer Vision", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.8779286444187164}]}, {"text": "These methods reduce an inference problem for an unknown image to finding an existing labeled image which is semantically similar.", "labels": [], "entities": []}, {"text": "However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are hampered by noisy estimations of visual content and poor alignment between images and human-written captions.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8483065764109293}]}, {"text": "Our work addresses this challenge by estimating a word frequency representation of the visual content of a query image.", "labels": [], "entities": []}, {"text": "This allows us to cast caption generation as an extractive summarization problem.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.900690495967865}]}, {"text": "Our model strongly outperforms two state-of-the-art caption extraction systems according to human judgments of caption relevance .", "labels": [], "entities": [{"text": "caption extraction", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9237247407436371}]}], "introductionContent": [{"text": "Automatic image captioning is a much studied topic in both the Natural Language Processing (NLP) and Computer Vision (CV) areas of research.", "labels": [], "entities": [{"text": "Automatic image captioning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7531883716583252}]}, {"text": "The task is to identify the visual content of the input image, and to output a relevant natural language caption.", "labels": [], "entities": []}, {"text": "Much prior work treats image captioning as a retrieval problem (see Section 2).", "labels": [], "entities": [{"text": "image captioning", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7905159592628479}]}, {"text": "These approaches use CV algorithms to retrieve similar images from a large database of captioned images, and then transfer text from the captions of those images to the query image.", "labels": [], "entities": []}, {"text": "This is a challenging problem for two main reasons.", "labels": [], "entities": []}, {"text": "First, visual similarity measures do not perform reliably and do not Query Image: Captioned Images: 1. 2. 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we use the SBU-Flickr dataset 2 . Ordonez et al.", "labels": [], "entities": [{"text": "SBU-Flickr dataset", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.8562992215156555}]}, {"text": "(2011) query Flickr.com using a huge number of words which describe visual entities, in order to build a corpus of one million images with captions which refer to image content.", "labels": [], "entities": [{"text": "Flickr.com", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.8403295874595642}]}, {"text": "However, further analysis by shows that many captions in SBU-Flickr (\u223c67%) describe information that cannot be obtained from the image itself, while a substantial fraction (\u223c23%) contain almost no visually relevant information.", "labels": [], "entities": [{"text": "SBU-Flickr", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.7956134676933289}]}, {"text": "Nevertheless, this dataset is the only web-scale collection of captioned images, and has enabled notable research in both CV and NLP.", "labels": [], "entities": []}, {"text": "Although BLEU () scores are widely used for image caption evaluation, we find them to be poor indicators of the quality of our model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9981661438941956}, {"text": "image caption evaluation", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8725663622220358}]}, {"text": "As shown in, our system's BLEU scores increase rapidly until about k = 25.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9753348529338837}]}, {"text": "Past this point we observe the density estimation seems to get washed out by oversmoothing, but the BLEU scores continue to improve until k = 500 but only because the generated captions become increasingly shorter.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9994692206382751}]}, {"text": "Furthermore, although we observe that our SumBasic extracted captions obtain consistently higher BLEU scores, our personal observations find KL Divergence captions to be better at balancing recall and precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9992836117744446}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9987415671348572}, {"text": "precision", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.9964878559112549}]}, {"text": "Nevertheless, BLEU scores are the accepted metric for recent work, and our KL Divergence captions with k = 25 still outperform all other previously published systems and baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9982574582099915}, {"text": "KL Divergence captions", "start_pos": 75, "end_pos": 97, "type": "METRIC", "confidence": 0.5795784592628479}]}, {"text": "We omit full results here due to space, but make our BLEU setup with captions for all systems and baselines available for documentary purposes.: Human evaluations of relevance: mean ratings and standard deviations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9782772660255432}]}, {"text": "We perform our human evaluation of caption relevance using a similar setup to that of, who have humans rate the image captions on a 1-5 scale (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 50-70% good, 1: totally bad).", "labels": [], "entities": [{"text": "caption relevance", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9151153564453125}]}, {"text": "Evaluation is performed using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9531745513280233}]}, {"text": "Evaluators are shown both the caption and the query image, and are specifically instructed to ignore errors in grammaticality and coherence.", "labels": [], "entities": []}, {"text": "We generate captions using our system with KL Divergence sentence selection and k = 25.", "labels": [], "entities": [{"text": "KL Divergence sentence selection", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.4664931446313858}]}, {"text": "We also evaluate the original HUMAN captions for the query image, as well as generated captions from two recently published caption transfer systems.", "labels": [], "entities": []}, {"text": "First, we consider the SCENE ATTRIBUTES system (), which represents both the best scene-based transfer model and a k = 1 nearest-neighbor baseline for our system.", "labels": [], "entities": [{"text": "SCENE", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.6151115894317627}, {"text": "ATTRIBUTES", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.8178684115409851}]}, {"text": "We also compare against the COLLECTIVE system (, which is the best objectbased transfer model.", "labels": [], "entities": []}, {"text": "In order to facilitate comparison, we use the same test/train split that is used in the publicly available system output for the COLLECTIVE system . However, we remove some query images which have contamination between the train and test set (this occurs when a photographer takes multiple shots of the same scene and gives all the images the exact same caption).", "labels": [], "entities": []}, {"text": "We also note that their test set is selected based on images where their object detection systems had good performance, and may not be indicative of their performance on other query images.", "labels": [], "entities": [{"text": "object detection", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7062834650278091}]}, {"text": "shows the results of our human study.", "labels": [], "entities": []}, {"text": "Captions generated by our system have 48% improvement in relevance over the SCENE AT-TRIBUTES system captions, and 34% improve-  ment over the COLLECTIVE system captions.", "labels": [], "entities": [{"text": "relevance", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9949775338172913}, {"text": "SCENE AT-TRIBUTES system captions", "start_pos": 76, "end_pos": 109, "type": "DATASET", "confidence": 0.6008115410804749}, {"text": "COLLECTIVE system captions", "start_pos": 143, "end_pos": 169, "type": "DATASET", "confidence": 0.7785536746184031}]}, {"text": "Although our system captions score lower than the human captions on average, there are some instances of our system captions being judged as more relevant than the human-written captions.", "labels": [], "entities": []}], "tableCaptions": []}