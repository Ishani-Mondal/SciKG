{"title": [{"text": "RNN-based Derivation Structure Prediction for SMT", "labels": [], "entities": [{"text": "RNN-based Derivation Structure Prediction", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6425969079136848}, {"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9831110239028931}]}], "abstractContent": [{"text": "In this paper, we propose a novel derivation structure prediction (DSP) model for SMT using recursive neural network (RNN).", "labels": [], "entities": [{"text": "derivation structure prediction (DSP)", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.8077517946561178}, {"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9951112866401672}]}, {"text": "Within the model, two steps are involved: (1) phrase-pair vector representation , to learn vector representations for phrase pairs; (2) derivation structure prediction , to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones.", "labels": [], "entities": [{"text": "phrase-pair vector representation", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6815765798091888}, {"text": "derivation structure prediction", "start_pos": 136, "end_pos": 167, "type": "TASK", "confidence": 0.8657326499621073}]}, {"text": "Final experimental results show that our DSP model can significantly improve the translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model), hierarchical phrase-based model, and syntax-based model (;.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.9598593413829803}]}, {"text": "In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as shows.", "labels": [], "entities": []}, {"text": "Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations.", "labels": [], "entities": []}, {"text": "For example in, (a) and (b) are two different derivations for Chinese sentence \"\u00d9\u0178 \u2020 \u00e29 \u00de1 \u00ac !\".", "labels": [], "entities": []}, {"text": "Comparing the two derivations, (a) is more reasonable and yields a better translation.", "labels": [], "entities": []}, {"text": "However, (b) wrongly translates phrase \" \u2020 \u00e29\" to \"and Sharon\" and combines it with incorrectly, leading to a bad translation.", "labels": [], "entities": []}, {"text": "To explore the derivation structure's potential on yielding good translations, in this paper, we propose a novel derivation structure prediction (DSP) model for SMT decoding.", "labels": [], "entities": [{"text": "derivation structure prediction (DSP)", "start_pos": 113, "end_pos": 150, "type": "TASK", "confidence": 0.8160996933778127}, {"text": "SMT decoding", "start_pos": 161, "end_pos": 173, "type": "TASK", "confidence": 0.9313835501670837}]}, {"text": "The proposed DSP model is built on recursive neural network (RNN).", "labels": [], "entities": []}, {"text": "Within the model, two steps are involved: (1) phrase-pair vector representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones.", "labels": [], "entities": [{"text": "phrase-pair vector representation", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6806346774101257}, {"text": "derivation structure prediction", "start_pos": 135, "end_pos": 166, "type": "TASK", "confidence": 0.8661376635233561}]}, {"text": "Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations.", "labels": [], "entities": []}, {"text": "We make the following contributions in this work: \u2022 We propose a novel RNN-based model to do derivation structure prediction for SMT decoding.", "labels": [], "entities": [{"text": "derivation structure prediction", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.7413804332415262}, {"text": "SMT decoding", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.9374520778656006}]}, {"text": "To our best knowledge, this is the first work on this issue in SMT community; \u2022 In current work, RNN has only been verified to be useful on monolingual structure learning ().", "labels": [], "entities": [{"text": "SMT community", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.916694700717926}]}, {"text": "We go a step further, and design a bilingual RNN to represent the derivation structure; \u2022 To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model.", "labels": [], "entities": []}], "datasetContent": [{"text": "To verify the effectiveness of our DSP model, we perform experiments on Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.6299953460693359}]}, {"text": "The training data contains about 2.1M sentence pairs with about 27.7M Chinese words and 31.9M English words 2 . We train a 5-gram language model by the Xinhua portion of Gigaword corpus and the English part of the training data.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 170, "end_pos": 185, "type": "DATASET", "confidence": 0.9140021800994873}]}, {"text": "We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7022100239992142}]}, {"text": "We use NIST MT 2003 data as the development set, and NIST MT04-08 3 as the test set.", "labels": [], "entities": [{"text": "NIST MT 2003 data", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.8911328464746475}, {"text": "NIST MT04-08 3", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.8494969606399536}]}, {"text": "We use MERT) to tune parameters.", "labels": [], "entities": [{"text": "MERT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8948360085487366}]}, {"text": "The translation quality is evaluated by case-insensitive BLEU-4 ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9466054439544678}, {"text": "BLEU-4", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9860903024673462}]}, {"text": "The statistical significance testis performed by the re-sampling approach).", "labels": [], "entities": []}, {"text": "The baseline system is our in-house BTG system.", "labels": [], "entities": [{"text": "BTG system", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.8310534358024597}]}, {"text": "To train the DSP model, we first use Word2Vec 4 toolkit to pre-train the word embedding on largescale monolingual data.", "labels": [], "entities": [{"text": "Word2Vec 4 toolkit", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.8919520576794943}]}, {"text": "The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English.", "labels": [], "entities": []}, {"text": "The dimensionality of our vectors is 50.", "labels": [], "entities": []}, {"text": "The detiled training process is as follows: (1) Using the BTG system to perform force decoding on FBIS part of the bilingual training data 5 , and collect the sentences succeeded in force decoding (86,902 sentences in total) . We then collect the corresponding force decoding derivations as gold derivations.", "labels": [], "entities": [{"text": "BTG", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.890228807926178}]}, {"text": "Here, we only use the best force decoding derivation for simple implementation.", "labels": [], "entities": []}, {"text": "In future, we will try to use multiple force decoding derivations for training.", "labels": [], "entities": []}, {"text": "(2) Collecting the bilingual phrases in the leaf nodes of gold derivations.", "labels": [], "entities": []}, {"text": "We train LNN by these phrases via L-BFGS algorithm.", "labels": [], "entities": []}, {"text": "Finally, we get 351,448 source phrases to train the source side RAE and 370,948 target phrases to train the target side RAE.", "labels": [], "entities": [{"text": "RAE", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.5774509906768799}]}, {"text": "(3) Decoding the 86902 sentences by the BTG system to get n-best translations and corresponding derivations.", "labels": [], "entities": [{"text": "BTG", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9419010281562805}]}, {"text": "The n-best derivations are used to simulate the entire derivation space.", "labels": [], "entities": []}, {"text": "We retain at most 200-best derivations for each sentence.", "labels": [], "entities": []}, {"text": "(4) Leveraging force decoding derivations and n-best derivations to train the DSP model.", "labels": [], "entities": []}, {"text": "Note that all parameters, including word embedding and parameters in LNN and DSN, are tuned together in this step.", "labels": [], "entities": []}, {"text": "It takes about 15 hours to train the entire network using a 16-core, 2.9 GHz Xeon machine.", "labels": [], "entities": []}, {"text": "We compare baseline BTG system and the DSPaugmented BTG system in this section.", "labels": [], "entities": [{"text": "DSPaugmented BTG system", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.9275476137797037}]}, {"text": "The final translation results are shown in.", "labels": [], "entities": []}, {"text": "After integrating the DSP model into BTG system, we get significant improvement on all test sets, about 1.0 BLEU points over BTG system on average.", "labels": [], "entities": [{"text": "BTG", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9083496332168579}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9995064735412598}, {"text": "BTG", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.9124137759208679}]}, {"text": "This comparison strongly demonstrates that our DSP model is useful and will be a good complement to current translation models.: Final translation results.", "labels": [], "entities": []}, {"text": "Bold numbers denote that the result is significantly better than baseline BTG system (p < 0.05).", "labels": [], "entities": []}, {"text": "Column \"Aver\" gives the average BLEU points of the 4 test sets.", "labels": [], "entities": [{"text": "Aver", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.8066664934158325}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9995372295379639}]}, {"text": "To have a better intuition for the effectiveness of our DSP model, we give a case study in.", "labels": [], "entities": []}, {"text": "It depicts two derivations built by BTG system and BTG+DSP system respectively.", "labels": [], "entities": [{"text": "BTG system", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9630900323390961}, {"text": "BTG+DSP", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.8913658261299133}]}, {"text": "From(b), we can see that BTG system yields a bad translation due to the bad derivation structure.", "labels": [], "entities": [{"text": "BTG", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.6907505989074707}]}, {"text": "In the figure, BTG system makes three mistakes.", "labels": [], "entities": [{"text": "BTG", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.6677674055099487}]}, {"text": "It attaches candidates [\u00a4\u00d2; achievements], [\u00a4 \u02c6 ; has reached] and to the big candidate [\u00d8U \u2030 n \u00a4,; cannot be regarded as a natural].", "labels": [], "entities": [{"text": "\u00d8U", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.85389643907547}]}, {"text": "Consequently, the noun phrase \"#\\\u00b7 \u00a4 \u02c6 \u00a4 \u00d2\" is translated separately, rather than as a whole, leading to a bad translation.", "labels": [], "entities": []}, {"text": "Differently, the DSP model is designed for predicting good derivations.", "labels": [], "entities": []}, {"text": "In(c), the used translation rules are actually similar to.", "labels": [], "entities": []}, {"text": "However, under a better guidance to build good derivation structure, BTG+DSP system generates a much better translation result than BTG system.", "labels": [], "entities": [{"text": "BTG+DSP", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.8038192391395569}]}], "tableCaptions": [{"text": " Table 1: Final translation results. Bold numbers  denote that the result is significantly better than  baseline BTG system (p < 0.05). Column \"Aver\"  gives the average BLEU points of the 4 test sets.", "labels": [], "entities": [{"text": "Aver", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9705945253372192}, {"text": "BLEU", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9987636804580688}]}]}