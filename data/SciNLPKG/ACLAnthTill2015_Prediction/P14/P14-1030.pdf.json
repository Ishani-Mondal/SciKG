{"title": [{"text": "Extracting Opinion Targets and Opinion Words from Online Reviews with Graph Co-ranking", "labels": [], "entities": [{"text": "Extracting Opinion Targets", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8586167494455973}]}], "abstractContent": [{"text": "Extracting opinion targets and opinion words from online reviews are two fundamental tasks in opinion mining.", "labels": [], "entities": [{"text": "Extracting opinion targets and opinion words from online reviews", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.8546361592080858}, {"text": "opinion mining", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.8104464411735535}]}, {"text": "This paper proposes a novel approach to collectively extract them with graph co-ranking.", "labels": [], "entities": []}, {"text": "First, compared to previous methods which solely employed opinion relations among words, our method constructs a heterogeneous graph to model two types of relations, including semantic relations and opinion relations.", "labels": [], "entities": []}, {"text": "Next, a co-ranking algorithm is proposed to estimate the confidence of each candidate, and the candidates with higher confidence will be extracted as opinion targets/words.", "labels": [], "entities": []}, {"text": "In this way, different relations make cooperative effects on candidates' confidence estimation.", "labels": [], "entities": []}, {"text": "Moreover, word preference is captured and incorporated into our co-ranking algorithm.", "labels": [], "entities": []}, {"text": "In this way, our co-ranking is personalized and each candi-date's confidence is only determined by its preferred collocations.", "labels": [], "entities": []}, {"text": "It helps to improve the extraction precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.988860547542572}]}, {"text": "The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "In opinion mining, extracting opinion targets and opinion words are two fundamental subtasks.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.867681473493576}]}, {"text": "Opinion targets are objects about which users' opinions are expressed, and opinion words are words which indicate opinions' polarities.", "labels": [], "entities": []}, {"text": "Extracting them can provide essential information for obtaining fine-grained analysis on customers' opinions.", "labels": [], "entities": []}, {"text": "Thus, it has attracted a lot of attentions (Hu and Liu, 2004b;.", "labels": [], "entities": []}, {"text": "To this end, previous work usually employed a collective extraction strategy (; Hu and Liu, 2004b;).", "labels": [], "entities": []}, {"text": "Their intuition is: opinion words usually co-occur with opinion targets in sentences, and there are strong modification relationship between them (called opinion relation in ( ).", "labels": [], "entities": []}, {"text": "If a word is an opinion word, other words with which that word having opinion relations will have highly probability to be opinion targets, and vice versa.", "labels": [], "entities": []}, {"text": "In this way, extraction is alternatively performed and mutual reinforced between opinion targets and opinion words.", "labels": [], "entities": []}, {"text": "Although this strategy has been widely employed by previous approaches, it still has several limitations.", "labels": [], "entities": []}, {"text": "1) Only considering opinion relations is insufficient.", "labels": [], "entities": []}, {"text": "Previous methods mainly focused on employing opinion relations among words for opinion target/word co-extraction.", "labels": [], "entities": []}, {"text": "They have investigated a series of techniques to enhance opinion relations identification performance, such as nearest neighbor rules (), syntactic patterns (, word alignment models (), etc.", "labels": [], "entities": [{"text": "opinion relations identification", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.6830654044946035}, {"text": "word alignment", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.7304006516933441}]}, {"text": "However, we are curious that whether merely employing opinion relations among words is enough for opinion target/word extraction?", "labels": [], "entities": [{"text": "opinion target/word extraction", "start_pos": 98, "end_pos": 128, "type": "TASK", "confidence": 0.5731570601463318}]}, {"text": "We note that there are additional types of relations among words.", "labels": [], "entities": []}, {"text": "For example, \"LCD\" and \"LED\" both denote the same aspect \"screen\" in TV set domain, and they are topical related.", "labels": [], "entities": []}, {"text": "We call such relations between homogeneous words as semantic relations.", "labels": [], "entities": []}, {"text": "If we have known \"LCD\" to bean opinion target, \"LED\" is naturally to bean opinion target.", "labels": [], "entities": []}, {"text": "Intuitively, besides opinion relations, semantic relations may provide additional rich clues for indicating opinion targets/words.", "labels": [], "entities": []}, {"text": "Which kind of relations is more effective for opinion targets/words extraction?", "labels": [], "entities": [{"text": "opinion targets/words extraction", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6732063949108124}]}, {"text": "Is it beneficial to consider these two types of relations together for the extraction?", "labels": [], "entities": []}, {"text": "To our best knowl-edge, these problems have seldom been studied before (see Section 2).", "labels": [], "entities": []}, {"text": "2) Ignoring word preference.", "labels": [], "entities": [{"text": "Ignoring word preference", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8392430345217387}]}, {"text": "When employing opinion relations to perform mutual reinforcing extraction between opinion targets and opinion words, previous methods depended on opinion associations among words, but seldom considered word preference.", "labels": [], "entities": [{"text": "mutual reinforcing extraction between opinion targets", "start_pos": 44, "end_pos": 97, "type": "TASK", "confidence": 0.8020229985316595}]}, {"text": "Word preference denotes a word's preferred collocations.", "labels": [], "entities": []}, {"text": "Intuitively, the confidence of a candidate being an opinion target (opinion word) should mostly be determined by its word preferences rather than all words having opinion relations with it.", "labels": [], "entities": []}, {"text": "For example \"This camera's price is expensive for me.\"", "labels": [], "entities": []}, {"text": "\"It's price is good.\"", "labels": [], "entities": []}, {"text": "\"Canon 40D has a good price.\"", "labels": [], "entities": [{"text": "Canon 40D", "start_pos": 1, "end_pos": 10, "type": "DATASET", "confidence": 0.9359129071235657}]}, {"text": "In these three sentences, \"price\" is modified by \"good\" more times than \"expensive\".", "labels": [], "entities": []}, {"text": "In traditional extraction strategy, opinion associations are usually computed based on the co-occurrence frequency.", "labels": [], "entities": []}, {"text": "Thus, \"good\" has more strong opinion association with \"price\" than \"expensive\", and it would have more contributions on determining \"price\" to bean opinion target or not.", "labels": [], "entities": []}, {"text": "\"Expensive\" actually has more relatedness with \"price\" than \"good\", and \"expensive\" is likely to be a word preference for \"price\".", "labels": [], "entities": []}, {"text": "The confidence of \"price\" being an opinion target should be influenced by \"expensive\" in greater extent than \"good\".", "labels": [], "entities": []}, {"text": "In this way, we argue that the extraction will be more precise.", "labels": [], "entities": [{"text": "extraction", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.881385087966919}]}, {"text": "\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 \u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 6 \u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 5 \u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 \u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 3 \u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 2 \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 4 \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 5 \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 6 \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u0082 1 \u00ed \u00b5\u00ed\u00b0\u00ba\u00ed \u00b5\u00ed\u00b0\u00ba \u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u00a1 \u00ed \u00b5\u00ed\u00b0\u00ba\u00ed \u00b5\u00ed\u00b0\u00ba \u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1\u009c \u00ed \u00b5\u00ed\u00b0\u00ba\u00ed \u00b5\u00ed\u00b0\u00ba \u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1\u009c Figure 1: Heterogeneous Graph: OC means opinion word candidates.", "labels": [], "entities": []}, {"text": "T C means opinion target candidates.", "labels": [], "entities": []}, {"text": "Solid curves and dotted lines respectively mean semantic relations and opinion relations between two candidates.", "labels": [], "entities": []}, {"text": "Thus, to resolve these two problems, we present a novel approach with graph co-ranking.", "labels": [], "entities": []}, {"text": "The collective extraction of opinion targets/words is performed in a co-ranking process.", "labels": [], "entities": [{"text": "collective extraction of opinion targets/words", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8319171071052551}]}, {"text": "First, we operate over a heterogeneous graph to model semantic relations and opinion relations into a unified model.", "labels": [], "entities": []}, {"text": "Specifically, our heterogeneous graph is composed of three subgraphs which model different relation types and candidates, as shown in.", "labels": [], "entities": []}, {"text": "The first subgraph G tt represents semantic relations among opinion target candidates, and the second subgraph G oo models semantic relations among opinion word candidates.", "labels": [], "entities": []}, {"text": "The third part is a bipartite subgraph G to , which models opinion relations among different candidate types and connects the above two subgraphs together.", "labels": [], "entities": []}, {"text": "Then we perform a random walk algorithm on G tt , G oo and G to separately, to estimate all candidates' confidence, and the entries with higher confidence than a threshold are correspondingly extracted as opinion targets/words.", "labels": [], "entities": []}, {"text": "The results could reflect which type of relation is more useful for the extraction.", "labels": [], "entities": []}, {"text": "Second, a co-ranking algorithm, which incorporates three separate random walks on G tt , G oo and G to into a unified process, is proposed to perform candidate confidence estimation.", "labels": [], "entities": [{"text": "candidate confidence estimation", "start_pos": 150, "end_pos": 181, "type": "TASK", "confidence": 0.6914809246857961}]}, {"text": "Different relations may cooperatively affect candidate confidence estimation and generate more global ranking results.", "labels": [], "entities": [{"text": "candidate confidence estimation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6083502968152364}]}, {"text": "Moreover, we discover each candidate's preferences through topics.", "labels": [], "entities": []}, {"text": "Such word preference will be different for different candidates.", "labels": [], "entities": []}, {"text": "We add word preference information into our algorithm and make our co-ranking algorithm be personalized.", "labels": [], "entities": []}, {"text": "A candidate's confidence would mainly absorb the contributions from its word preferences rather than its all neighbors with opinion relations, which maybe beneficial for improving extraction precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9098495244979858}]}, {"text": "We perform experiments on real-world datasets from different languages and different domains.", "labels": [], "entities": []}, {"text": "Results show that our approach effectively improves extraction performance compared to the state-of-the-art approaches.", "labels": [], "entities": [{"text": "extraction", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.9505556225776672}]}], "datasetContent": [{"text": "Datasets: To evaluate the proposed method, we used three datasets.", "labels": [], "entities": []}, {"text": "The first one is Customer Review Datasets (CRD), used in (Hu and Liu, 2004a), which contains reviews about five products.", "labels": [], "entities": [{"text": "Customer Review Datasets (CRD)", "start_pos": 17, "end_pos": 47, "type": "DATASET", "confidence": 0.6982510735591253}]}, {"text": "The second one is COAE2008 dataset2 2 , which contains Chinese reviews about four products.", "labels": [], "entities": [{"text": "COAE2008 dataset2 2", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.949623703956604}]}, {"text": "The third one is Large, also used in (, where two domains are selected (Mp3 and Hotel).", "labels": [], "entities": [{"text": "Mp3", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9344433546066284}]}, {"text": "As mentioned in ( ), Large contains 6,000 sentences for each domain.", "labels": [], "entities": []}, {"text": "Opinion targets/words are manually annotated, where three annotators were involved.", "labels": [], "entities": []}, {"text": "Two annotators were required to annotate out opinion words/targets in reviews.", "labels": [], "entities": []}, {"text": "When conflicts occur, the third annotator make final judgement.", "labels": [], "entities": []}, {"text": "In total, we respectively obtain 1,112, 1,241 opinion targets and 334, 407 opinion words in Hotel, MP3.", "labels": [], "entities": [{"text": "Hotel, MP3", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.8757915298144022}]}, {"text": "Pre-processing: All sentences are tagged to obtain words' part-of-speech tags using Stanford NLP tool 3 . And noun phrases are identified using the method in () before extraction.", "labels": [], "entities": [{"text": "Stanford NLP tool 3", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.8935495167970657}]}, {"text": "Evaluation Metrics: We select precision(P), recall(R) and f-measure(F) as metrics.", "labels": [], "entities": [{"text": "precision(P)", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.9304038733243942}, {"text": "recall(R)", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9434211850166321}, {"text": "f-measure(F)", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.8405088484287262}]}, {"text": "And a significant testis performed, i.e., a t-test with a default significant level of 0.05.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1, where  each entry denotes a SA(v i , v j ) score between two  candidates. We can see that using topics can suc- cessfully capture the preference information for  each opinion target/word.", "labels": [], "entities": [{"text": "SA", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9977115392684937}]}, {"text": " Table 2: Results of Opinion Targets Extraction on Customer Review Dataset", "labels": [], "entities": [{"text": "Customer Review Dataset", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.6026705900828043}]}, {"text": " Table 3: Results of Opinion Targets Extraction on COAE 2008 and Large", "labels": [], "entities": [{"text": "Opinion Targets Extraction", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6111993590990702}, {"text": "COAE 2008", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.9144896566867828}]}, {"text": " Table 4: Results of Opinion Words Extraction on Customer Review Dataset", "labels": [], "entities": [{"text": "Opinion Words Extraction", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6471278270085653}, {"text": "Customer Review Dataset", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.6975314517815908}]}, {"text": " Table 5: Results of Opinion Words Extraction on COAE 2008 and Large", "labels": [], "entities": [{"text": "Opinion Words Extraction", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.5954658091068268}, {"text": "COAE 2008", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.8525831699371338}]}]}