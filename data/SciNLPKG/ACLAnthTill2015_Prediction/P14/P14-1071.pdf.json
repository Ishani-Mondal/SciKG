{"title": [{"text": "Correcting Preposition Errors in Learner English Using Error Case Frames and Feedback Messages", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a novel framework called error case frames for correcting preposition errors.", "labels": [], "entities": [{"text": "correcting preposition errors", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.8480759660402933}]}, {"text": "They are case frames specially designed for describing and correcting preposition errors.", "labels": [], "entities": [{"text": "describing and correcting preposition errors", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.688040041923523}]}, {"text": "Their most distinct advantage is that they can correct errors with feedback messages explaining why the preposition is erroneous.", "labels": [], "entities": []}, {"text": "This paper proposes a method for automatically generating them by comparing learner and native corpora.", "labels": [], "entities": []}, {"text": "Experiments show (i) automatically generated error case frames achieve a performance comparable to conventional methods; (ii) error case frames are intuitively interpretable and manually modifiable to improve them; (iii) feedback messages provided by error case frames are effective in language learning assistance.", "labels": [], "entities": []}, {"text": "Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically generated rules, error case frames will likely be one of the major approaches for preposition error correction.", "labels": [], "entities": [{"text": "preposition error correction", "start_pos": 197, "end_pos": 225, "type": "TASK", "confidence": 0.8411365946133932}]}], "introductionContent": [{"text": "This paper presents a novel framework for correcting preposition errors.", "labels": [], "entities": [{"text": "correcting preposition errors", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.8815245032310486}]}, {"text": "Its most significant advantage over previous methods is that it can provide learners with feedback messages, that is, explanatory notes describing why the detected preposition is erroneous and should be corrected as indicated, as shown in.", "labels": [], "entities": []}, {"text": "Despite the fact that appropriate feedback messages are essential in language learning assistance, which is one of the immediate applications of grammatical error correc- * Part of this work was performed while the author was a visiting researcher at LIMSI, Orsay (France).", "labels": [], "entities": []}, {"text": "Target sentence: In the univerysity, I studied English in the morning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed method from two points of view: correction performance and usefulness of feedback messages.", "labels": [], "entities": []}, {"text": "We measured correction performance by recall, precision, and Fmeasure.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9995966553688049}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9995949864387512}, {"text": "Fmeasure", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9996272325515747}]}, {"text": "In the evaluation on usefulness of feedback messages, three human raters (a teacher of English at college and two who have a master degree in TESOL) separately examined whether each feedback message was useful for learning the correct usage of the preposition.", "labels": [], "entities": [{"text": "TESOL", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.6711094975471497}]}, {"text": "We defined usefulness by the ratio of feedback messages evaluated as useful to the total number of feedback messages.", "labels": [], "entities": []}, {"text": "We used the following data sets in the evaluation.", "labels": [], "entities": []}, {"text": "We selected the Konan-JIEM (KJ) learner corpus (Nagata et al., 2011) as the target texts.", "labels": [], "entities": [{"text": "Konan-JIEM (KJ) learner corpus (Nagata et al., 2011)", "start_pos": 16, "end_pos": 68, "type": "DATASET", "confidence": 0.8953178754219642}]}, {"text": "The KJ learner corpus is fully annotated with grammatical errors.", "labels": [], "entities": [{"text": "KJ learner corpus", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7282145420710245}]}, {"text": "In addition, it includes error correction results of several benchmark systems.", "labels": [], "entities": []}, {"text": "This means that one can directly compare correction results of anew method with those of the benchmark systems, which reveals where the method is strong and weak compared to the benchmark systems.", "labels": [], "entities": []}, {"text": "The KJ corpus consists of training and test sets.", "labels": [], "entities": [{"text": "KJ corpus", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7564971745014191}]}, {"text": "We used the training set to generate error case frames and evaluated correction performance on the test set.", "labels": [], "entities": []}, {"text": "In addition to these data sets, we created a development set, which we had collected to develop the proposed method.", "labels": [], "entities": []}, {"text": "We did not use it in the final evaluation.", "labels": [], "entities": []}, {"text": "As a native corpus, we used the EDR corpus (Japan electronic dictionary research institute, the Reuters-21578 corpus , and the LOCNESS corpus . We used the lexicalized dependency parser in the Stanford Statistical Natural Language Parser (ver.2.0.3) (de) to obtain parses for the data sets.", "labels": [], "entities": [{"text": "EDR corpus", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.9347770810127258}, {"text": "Japan electronic dictionary research institute", "start_pos": 44, "end_pos": 90, "type": "DATASET", "confidence": 0.8404548168182373}, {"text": "Reuters-21578 corpus", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9144522547721863}, {"text": "LOCNESS corpus", "start_pos": 127, "end_pos": 141, "type": "DATASET", "confidence": 0.8159907758235931}]}, {"text": "shows the statistics on the data sets.", "labels": [], "entities": []}, {"text": "Using these data sets, we implemented three versions of the proposed method.", "labels": [], "entities": []}, {"text": "The first one was based on error case frames generated from the training set of the KJ corpus.", "labels": [], "entities": [{"text": "KJ corpus", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.8937132358551025}]}, {"text": "The second one was the first one with active generation.", "labels": [], "entities": []}, {"text": "To implement the third one, we manually edited the error case frames of the first version to remove unnecessary error case frames and case elements (but no addition) and to add feedback messages to them.", "labels": [], "entities": []}, {"text": "After this, active generation was applied to augment the edited error case frames.", "labels": [], "entities": []}, {"text": "In implementing the proposed methods, we selected as target prepositions the ten most frequent prepositions, the same as in previous work (: about, at, by, for, from, in, of , on, to, with.", "labels": [], "entities": []}, {"text": "For comparison, we selected two conventional methods.", "labels": [], "entities": []}, {"text": "One was the best-performing system among the benchmark systems, which is the classifier-based method () which had participated in the HOO 2012 shared task ().", "labels": [], "entities": [{"text": "HOO 2012 shared task", "start_pos": 134, "end_pos": 154, "type": "DATASET", "confidence": 0.8272333443164825}]}, {"text": "The other was the SMTbased method () which was the best-performing system in preposition error correction in the CoNLL 2013 shared task ( ).", "labels": [], "entities": [{"text": "SMTbased", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.8952622413635254}, {"text": "preposition error correction", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.5740428964296976}, {"text": "CoNLL 2013 shared task", "start_pos": 113, "end_pos": 135, "type": "DATASET", "confidence": 0.8329117298126221}]}, {"text": "In addition, we evaluated performance of hybrid methods combining the correction results of the third version of the proposed method with those of the classifier-/SMT-based method; we simply took the union of the two.", "labels": [], "entities": []}, {"text": "The simple error case frame-based method achieves an Fmeasure of 0.189.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9995613694190979}]}, {"text": "It improves recall when combined with active generation, which shows the effectiveness of active generation for augmenting error case frames.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9993284940719604}]}, {"text": "It further improves precision without decreasing recall by manual editing; note that manual editing was only applied to the error case frames generated from the training data but not to those generated by active generation.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9987432360649109}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9975966811180115}]}, {"text": "The performance is comparable to both classifier-/SMT-based methods.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.8788451552391052}]}, {"text": "The hybrid methods achieve the best performances in F -measure.", "labels": [], "entities": [{"text": "F -measure", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9776827891667684}]}, {"text": "In the usefulness evaluation, the third version of the proposed method was able to provide 20 feedback messages for the target texts.", "labels": [], "entities": []}, {"text": "The three human raters evaluated 80%, 80%, and 85% of the: Correction performance in recall (R), precision (P ), and F -measure (F ).", "labels": [], "entities": [{"text": "Correction", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9863213896751404}, {"text": "recall (R)", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9572404026985168}, {"text": "precision (P )", "start_pos": 97, "end_pos": 111, "type": "METRIC", "confidence": 0.955766350030899}, {"text": "F -measure (F )", "start_pos": 117, "end_pos": 132, "type": "METRIC", "confidence": 0.9798106650511423}]}, {"text": "20 feedback messages as useful (82% on average).", "labels": [], "entities": []}, {"text": "The agreement among the raters was \u03ba = 0.67 in Fleiss's \u03ba.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the data sets for evaluation.", "labels": [], "entities": []}, {"text": " Table 2: Correction performance in recall (R),  precision (P ), and F -measure (F ).", "labels": [], "entities": [{"text": "Correction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9713418483734131}, {"text": "recall (R)", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9486019611358643}, {"text": "precision (P )", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9606622457504272}, {"text": "F -measure (F )", "start_pos": 69, "end_pos": 84, "type": "METRIC", "confidence": 0.9807609518369039}]}]}