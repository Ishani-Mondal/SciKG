{"title": [], "abstractContent": [{"text": "Widely used in speech and language processing , Kneser-Ney (KN) smoothing has consistently been shown to be one of the best-performing smoothing methods.", "labels": [], "entities": [{"text": "speech and language processing", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.605722151696682}]}, {"text": "However, KN smoothing assumes integer counts, limiting its potential uses-for example , inside Expectation-Maximization.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.8344113528728485}]}, {"text": "In this paper, we propose a generalization of KN smoothing that operates on fractional counts, or, more precisely, on distributions over counts.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8840460777282715}]}, {"text": "We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.8949795365333557}, {"text": "KN smoothing", "start_pos": 136, "end_pos": 148, "type": "TASK", "confidence": 0.8466416299343109}, {"text": "language model adaptation", "start_pos": 183, "end_pos": 208, "type": "TASK", "confidence": 0.6737662255764008}, {"text": "word alignment", "start_pos": 227, "end_pos": 241, "type": "TASK", "confidence": 0.7955722212791443}]}, {"text": "In both cases, our method improves performance significantly .", "labels": [], "entities": []}], "introductionContent": [{"text": "In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing) has consistently proven to be among the best-performing and most widely used methods.", "labels": [], "entities": []}, {"text": "However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9382184743881226}]}, {"text": "Such cases have been noted for language modeling), domain adaptation (, grapheme-tophoneme conversion, and phrase-based translation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7534299194812775}, {"text": "domain adaptation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8140420913696289}, {"text": "grapheme-tophoneme conversion", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.743499219417572}, {"text": "phrase-based translation", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.7714800834655762}]}, {"text": "For example, in Expectation-Maximization (, the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed.", "labels": [], "entities": [{"text": "Expectation (E) step", "start_pos": 48, "end_pos": 68, "type": "METRIC", "confidence": 0.7826514601707458}]}, {"text": "In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E step are fractional.", "labels": [], "entities": []}, {"text": "It is common to apply add-one smoothing to the M step, but we cannot apply KN smoothing.", "labels": [], "entities": []}, {"text": "Another example is instance weighting.", "labels": [], "entities": [{"text": "instance weighting", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.683534562587738}]}, {"text": "If we assign a weight to each training instance to indicate how important it is (say, its relevance to a particular domain), and the counts are not integral, then we again cannot train the model using KN smoothing.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 201, "end_pos": 213, "type": "TASK", "confidence": 0.7712284326553345}]}, {"text": "In this paper, we propose a generalization of KN smoothing (called expected KN smoothing) that operates on fractional counts, or, more precisely, on distributions over counts.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8109607398509979}]}, {"text": "We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.8995395302772522}]}, {"text": "We demonstrate how to apply expected KN to two tasks where KN smoothing was not applicable before.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.8134481608867645}]}, {"text": "One is language model domain adaptation, and the other is word alignment using the IBM models).", "labels": [], "entities": [{"text": "language model domain adaptation", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.7028492093086243}, {"text": "word alignment", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.8039063811302185}, {"text": "IBM models", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.9342632293701172}]}, {"text": "In both tasks, expected KN smoothing improves performance significantly.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.9286611676216125}]}], "datasetContent": [{"text": "Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data.", "labels": [], "entities": []}, {"text": "They use the in-domain training data to select a subset of the generaldomain data, build a language model on the selected subset, and evaluate its perplexity on the indomain test data.", "labels": [], "entities": []}, {"text": "Here, we follow this experimental framework and compare Moore and Lewis's unweighted method to our weighted method.", "labels": [], "entities": []}, {"text": "For our experiments, we used all the English data allowed for the BOLT Phase 1 ChineseEnglish evaluation.", "labels": [], "entities": [{"text": "BOLT Phase 1 ChineseEnglish evaluation", "start_pos": 66, "end_pos": 104, "type": "DATASET", "confidence": 0.6050137102603912}]}, {"text": "We took 60k sentences (1.7M words) of web forum data as in-domain data, further subdividing it into 54k sentences (1.5M words) for training, 3k sentences (100k words) for testing, and 3k sentences (100k words) for future use.", "labels": [], "entities": []}, {"text": "The remaining 12.7M sentences (268M words) we treated as general-domain data.", "labels": [], "entities": []}, {"text": "We trained trigram language models and compared expected KN smoothing against integral KN smoothing, fractional WB smoothing, and fractional KN smoothing, measuring perplexity across various subset sizes).", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.8471850752830505}]}, {"text": "For fractional KN, for each subset size, we optimized D to mini-mize perplexity on the test set to give it the greatest possible advantage; nevertheless, it is clearly the worst performer.", "labels": [], "entities": []}, {"text": "Expected KN consistently gives the best perplexity, and, at the optimal subset size, obtains better perplexity (148) than the other methods (156 for integral KN, 162 for fractional WB and 197 for fractional KN).", "labels": [], "entities": []}, {"text": "Finally, we note that integral KN is very sensitive to the subset size, whereas expected KN and the other methods are more robust.", "labels": [], "entities": []}, {"text": "We modified GIZA++ to perform expected KN smoothing as described above.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.8611425459384918}]}, {"text": "Smoothing is enabled or disabled with a command-line switch, making direct comparisons simple.", "labels": [], "entities": [{"text": "Smoothing", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.6390988826751709}]}, {"text": "Our implementation is publicly available as open-source software.", "labels": [], "entities": []}, {"text": "We carried out experiments on two language pairs: Arabic to English and Czech to English.", "labels": [], "entities": []}, {"text": "For Arabic-English, we used 5.4+4.3 million words of parallel text from the NIST 2009 constrained task, 2 and 346 word-aligned sentence pairs (LDC2006E86) for evaluation.", "labels": [], "entities": [{"text": "NIST 2009 constrained task", "start_pos": 76, "end_pos": 102, "type": "DATASET", "confidence": 0.8550017476081848}]}, {"text": "For CzechEnglish, we used all 2.0+2.2 million words of training data from the WMT 2009 shared task, and 515 word-aligned sentence pairs) for evaluation.", "labels": [], "entities": [{"text": "CzechEnglish", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9727169871330261}, {"text": "WMT 2009 shared task", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.8051219582557678}]}, {"text": "For all methods, we used five iterations of IBM Models 1, 2, and HMM, followed by three iterations of IBM Models 3 and 4.", "labels": [], "entities": [{"text": "IBM Models 1", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9079837004343668}]}, {"text": "We applied expected KN smoothing to all iterations of all models.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.819137454032898}]}, {"text": "We aligned in both the foreign-to-English: Expected KN (interpolating with the unigram distribution) consistently outperforms all other methods.", "labels": [], "entities": [{"text": "Expected KN", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.8557709753513336}]}, {"text": "For variational Bayes, we followed insetting \u03b1 to zero (so that the choice of p \u2032 is irrelevant).", "labels": [], "entities": []}, {"text": "For fractional KN, we chose D to maximize F1 (see. and English-to-foreign directions and then used the grow-diag-final method to symmetrize them (, and evaluated the alignments using F-measure against gold word alignments.", "labels": [], "entities": [{"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9972339272499084}, {"text": "F-measure", "start_pos": 183, "end_pos": 192, "type": "METRIC", "confidence": 0.9639490842819214}]}, {"text": "As shown in, for KN smoothing, interpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution performs the best.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.8022491633892059}, {"text": "WB smoothing", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.7210706174373627}]}, {"text": "The difference can be explained by the way the two smoothing methods estimate p \u2032 . Consider again a training example with a word e that occurs nowhere else in the training data.", "labels": [], "entities": []}, {"text": "In WB smoothing, p \u2032 ( f ) is the empirical unigram distribution.", "labels": [], "entities": [{"text": "WB smoothing", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.8709588348865509}]}, {"text": "If f contains a word that is much more frequent than the correct translation of e, then smoothing may actually encourage the model to wrongly align e with the frequent word.", "labels": [], "entities": []}, {"text": "This is much less of a problem in KN smoothing, where p \u2032 is estimated from bigram types rather than bigram tokens.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9385259449481964}]}, {"text": "We also compared with variational Bayes (  Finally, we ran MT experiments to see whether the improved alignments also lead to improved translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9622892141342163}]}, {"text": "We used the same training data as before.", "labels": [], "entities": []}, {"text": "For the Arabic-English tasks, we used the NIST 2008 test set as development data and the NIST 2009 test set as test data; for the Czech-English tasks, we used the WMT 2008 test set as development data and the WMT 2009 test set as test data.", "labels": [], "entities": [{"text": "NIST 2008 test set", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.9724164456129074}, {"text": "NIST 2009 test set", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.9676061719655991}, {"text": "WMT 2008 test set", "start_pos": 163, "end_pos": 180, "type": "DATASET", "confidence": 0.9533191025257111}, {"text": "WMT 2009 test set", "start_pos": 209, "end_pos": 226, "type": "DATASET", "confidence": 0.9670774638652802}]}, {"text": "We used the Moses toolkit () to build MT systems using various alignments (for expected KN, we used the one interpolated with the unigram distribution, and for fractional WB, we used the one interpolated with the uniform distribution).", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9837464690208435}]}, {"text": "We used a trigram language model trained on Gigaword (AFP, AP Worldstream, CNA, and Xinhua portions), and minimum error-rate training to tune the feature weights.", "labels": [], "entities": [{"text": "AP Worldstream", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.8353027701377869}]}, {"text": "shows that, although the relationship between alignment F1 and Bleu is not very consistent, expected KN smoothing achieves the best Bleu among all these methods and is significantly better than the baseline (p < 0.01).", "labels": [], "entities": [{"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.7786278128623962}, {"text": "Bleu", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9932858347892761}, {"text": "KN smoothing", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.8741109073162079}, {"text": "Bleu", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9887917041778564}]}], "tableCaptions": [{"text": " Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other  methods. For variational Bayes, we followed", "labels": [], "entities": [{"text": "Expected KN", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8203860521316528}]}, {"text": " Table 2: Smoothing more stages of training makes  alignment accuracy go up. For each row, we  smoothed all iterations of the models indicated.  Key: H = HMM model; \u2022 = smoothing enabled;  \u2022 = smoothing disabled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9786310195922852}]}]}