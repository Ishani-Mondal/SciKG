{"title": [{"text": "Zero-shot Entity Extraction from Web Pages", "labels": [], "entities": [{"text": "Zero-shot Entity Extraction from Web Pages", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.765609085559845}]}], "abstractContent": [{"text": "In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction systems rely on seed examples or redundancy across multiple web pages.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.744318425655365}]}, {"text": "In this paper, we consider anew zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page.", "labels": [], "entities": []}, {"text": "Our approach defines a log-linear model over latent extraction predicates, which select lists of entities from the web page.", "labels": [], "entities": [{"text": "latent extraction predicates", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7635170618693033}]}, {"text": "The main challenge is to define features on widely varying candidate entity lists.", "labels": [], "entities": []}, {"text": "We tackle this by abstracting list elements and using aggregate statistics to define features.", "labels": [], "entities": []}, {"text": "Finally, we created anew dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9973287582397461}]}], "introductionContent": [{"text": "We consider the task of extracting entities of a given category (e.g., hiking trails) from web pages.", "labels": [], "entities": []}, {"text": "Previous approaches either (i) assume that the same entities appear on multiple web pages, or (ii) require information such as seed examples;).", "labels": [], "entities": []}, {"text": "These approaches work well for common categories but encounter data sparsity problems for more specific categories, such as the products of a small company or the dishes at a local restaurant.", "labels": [], "entities": []}, {"text": "In this context, we may have only a single web page that contains the information we need and no seed examples.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel task, zeroshot entity extraction, where the specification of the desired entities is provided as a natural language query.", "labels": [], "entities": [{"text": "zeroshot entity extraction", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6243769029776255}]}, {"text": "Given a query (e.g., hiking Figure 1: Entity extraction typically requires additional knowledge such as a small set of seed examples or depends on multiple web pages.", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7754248976707458}]}, {"text": "In our setting, we take as input a natural language query and extract entities from a single web page.", "labels": [], "entities": []}, {"text": "trails near Baltimore) and a web page (e.g., http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.).", "labels": [], "entities": [{"text": "Avalon Super Loop", "start_pos": 188, "end_pos": 205, "type": "DATASET", "confidence": 0.9568400382995605}]}, {"text": "The task introduces two challenges.", "labels": [], "entities": []}, {"text": "Given a single web page to extract entities from, we can no longer rely on the redundancy of entities across multiple web pages.", "labels": [], "entities": []}, {"text": "Furthermore, in the zero-shot learning paradigm (, where entire categories might be unseen during training, the system must generalize to new queries and web pages without the additional aid of seed examples.", "labels": [], "entities": []}, {"text": "To tackle these challenges, we cast the task as a structured prediction problem where the input is the query and the web page, and the output is a list of entities, mediated by a latent extraction predicate.", "labels": [], "entities": []}, {"text": "To generalize across different inputs, we rely on two types of features: structural features, which look at the layout and placement of the entities being extracted; and denotation fea-tures, which look at the list of entities as a whole and assess their linguistic coherence.", "labels": [], "entities": []}, {"text": "When defining features on lists, one technical challenge is being robust to widely varying list sizes.", "labels": [], "entities": []}, {"text": "We approach this challenge by defining features over a histogram of abstract tokens derived from the list elements.", "labels": [], "entities": []}, {"text": "For evaluation, we created the OPENWEB dataset comprising natural language queries from the Google Suggest API and diverse web pages returned from web search.", "labels": [], "entities": [{"text": "OPENWEB dataset", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.7172967195510864}]}, {"text": "Despite the variety of queries and web pages, our system still achieves a test accuracy of 40.5% and an accuracy at 5 of 55.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9512250423431396}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9996212720870972}]}], "datasetContent": [{"text": "To experiment with a diverse set of queries and web pages, we created anew dataset, OPENWEB, using web pages from Google search results.", "labels": [], "entities": [{"text": "OPENWEB", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.5247662663459778}]}, {"text": "We use the method from to generate search queries by performing a breadth-first search over the query space.", "labels": [], "entities": []}, {"text": "Specifically, we use the Google Suggest API, which takes a partial query (e.g., \"list of movies\") and outputs several complete queries (e.g., \"list of horror movies\").", "labels": [], "entities": []}, {"text": "We start with seed partial queries \"list of \u2022 \" where \u2022 is one or two initial letters.", "labels": [], "entities": []}, {"text": "In each step, we call the Google Suggest API on the partial queries to obtain complete queries, The OPENWEB dataset and our code base are available for download at http://www-nlp.stanford.edu/ software/web-entity-extractor-ACL2014.", "labels": [], "entities": [{"text": "OPENWEB dataset", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.8296335935592651}]}, {"text": "In this section we evaluate our system on the OPENWEB dataset.", "labels": [], "entities": [{"text": "OPENWEB dataset", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.9502043426036835}]}, {"text": "As the main metric, we use a notion of accuracy based on compatibility; specifically, we define the accuracy as the fraction of examples where the system predicts a compatible entity list as defined in Section 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9979254007339478}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9978674650192261}]}, {"text": "We also report accuracy at 5, the fraction of examples where the top five predictions contain a compatible entity list.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996517896652222}]}, {"text": "To see how our compatibility-based accuracy tracks exact correctness, we sampled 100 web pages which have at least one valid extraction predicate and manually annotated the full list of entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9158582091331482}]}, {"text": "We found that in 85% of the examples, the longest compatible list y is the correct list of entities, and many lists in the remaining 15% miss the correct list by only a few entities.", "labels": [], "entities": []}, {"text": "In some examples, our system cannot find any list of entities that is compatible with the gold annotation.", "labels": [], "entities": []}, {"text": "The oracle score is the fraction of examples in which the system can find at least one compatible list.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top 10 path suffix patterns found by the  baseline learner in the development data. Since  we allow path entries to be permuted, each suffix  pattern is represented by a multiset of path entries.  The notation [ * ] denotes any path entry index.", "labels": [], "entities": []}, {"text": " Table 3: Main results on the OPENWEB dataset  using the default set of features. (Acc = accuracy,  A@5 = accuracy at 5)", "labels": [], "entities": [{"text": "OPENWEB dataset", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9164296984672546}, {"text": "Acc", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9958633184432983}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8827729225158691}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9751112461090088}]}, {"text": " Table 4: Breakdown of coverage errors from the development data.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.790661096572876}, {"text": "coverage errors", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.7609624564647675}]}, {"text": " Table 5: Breakdown of ranking errors from the development data.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.5794408917427063}]}]}