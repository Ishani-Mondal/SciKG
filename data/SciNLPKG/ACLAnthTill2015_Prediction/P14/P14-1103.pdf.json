{"title": [{"text": "Nonparametric Learning of Phonological Constraints in Optimality Theory", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method to jointly learn features and weights directly from distri-butional data in a log-linear framework.", "labels": [], "entities": []}, {"text": "Specifically, we propose a non-parametric Bayesian model for learning phonologi-cal markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting.", "labels": [], "entities": []}, {"text": "The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method, and is the first algorithm for learning phonological constraints without presupposing constraint structure.", "labels": [], "entities": []}, {"text": "The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis, with a violation structure corresponding to the standard constraints.", "labels": [], "entities": []}, {"text": "These results suggest an alternative data-driven source for constraints instead of a fully innate constraint set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many aspects of human cognition involve the interaction of constraints that push a decision-maker toward different options, whether in something so trivial as choosing a movie or so important as a fight-or-flight response.", "labels": [], "entities": []}, {"text": "These constraint-driven decisions can be modeled with a log-linear system.", "labels": [], "entities": []}, {"text": "In these models, a set of constraints is weighted and their violations are used to determine a probability distribution over outcomes.", "labels": [], "entities": []}, {"text": "But where do these constraints come from We consider this question by examining the dominant framework in modern phonology, Optimality Theory, implemented in a log-linear framework, MaxEnt OT (, with output forms' probabilities based on a weighted sum of constraint violations.", "labels": [], "entities": []}, {"text": "OT analyses generally assume that the constraints are innate and universal, both to obviate the problem of learning constraints' identities and to limit the set of possible languages.", "labels": [], "entities": [{"text": "OT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9113904237747192}]}, {"text": "We propose anew approach: to learn constraints with limited innate phonological knowledge by identifying sets of constraint violations that explain the observed distributional data, instead of selecting constraints from an innate set of constraint definitions.", "labels": [], "entities": []}, {"text": "Because the constraints are identified as sets of violations, this also permits constraints specific to a given language to be learned.", "labels": [], "entities": []}, {"text": "This method, which we call IBPOT, uses an Indian Buffet Process (IBP) prior to define the space of possible constraint violation matrices, and uses Bayesian reasoning to identify constraint matrices likely to have generated the observed data.", "labels": [], "entities": [{"text": "Indian Buffet Process (IBP)", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.760143851240476}]}, {"text": "In identifying constraints solely by their extensional violation profiles, this method does not directly identify the intensional definitions of the identified constraints, but to the extent that the resulting violation profiles are phonologically interpretable, we may conclude that the data themselves guide constraint identification.", "labels": [], "entities": []}, {"text": "We test IBPOT on tongue-root vowel harmony in Wolof, a West African language.", "labels": [], "entities": [{"text": "IBPOT", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9091106057167053}, {"text": "tongue-root vowel harmony", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.6687870025634766}]}, {"text": "The set of constraints learned by the model satisfy two major goals: they explain the data as well as the standard phonological analysis, and their violation structures correspond to the standard constraints.", "labels": [], "entities": []}, {"text": "This suggests an alternative data-driven genesis for constraints, rather than the traditional assumption of fully innate constraints.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run the model for 10000 iterations, using deterministic annealing through the first 2500 it-erations.", "labels": [], "entities": []}, {"text": "The model is initialized with a random markedness matrix drawn from the IBP and weights from the exponential prior.", "labels": [], "entities": [{"text": "IBP", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8405925631523132}]}, {"text": "We ran versions of the model with parameter settings between 0.01 and 1 for \u03b1, 0.05 and 0.5 for \u03b7, and 2 and 5 for K * . All these produced quantitatively similar results; we report values for \u03b1 = 1, \u03b7 = 0.5, and K * = 5, which provides the least bias toward small constraint sets.", "labels": [], "entities": []}, {"text": "To establish performance for the phonological standard, we use the IBPOT learner to find constraint weights but do not update M . The resultant learner is essentially MaxEnt OT with the weights estimated through Metropolis sampling instead of gradient ascent.", "labels": [], "entities": [{"text": "IBPOT learner", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.7069971859455109}]}, {"text": "This is done so that the IBPOT weights and phonological standard weights are learned by the same process and can be compared.", "labels": [], "entities": [{"text": "IBPOT", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.5130288600921631}]}, {"text": "We use the same parameters for this baseline as for the IBPOT tests.", "labels": [], "entities": [{"text": "IBPOT tests", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.8180926442146301}]}, {"text": "The results in this section are based on nine runs each of IBPOT and MEOT; ten MEOT runs were performed but one failed to converge and was removed from analysis.", "labels": [], "entities": [{"text": "IBPOT", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.6694727540016174}, {"text": "MEOT", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.5694167017936707}]}], "tableCaptions": []}