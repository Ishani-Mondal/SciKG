{"title": [{"text": "Bootstrapping into Filler-Gap: An Acquisition Story", "labels": [], "entities": []}], "abstractContent": [{"text": "Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives.", "labels": [], "entities": []}, {"text": "Therefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g. SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language.", "labels": [], "entities": [{"text": "filler-gap acquisition", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7973684668540955}]}, {"text": "Specifically, this model, trained on part-of-speech tags, represents the preferred locations of semantic roles relative to a verb as Gaussian mixtures over real numbers.", "labels": [], "entities": []}, {"text": "This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance.", "labels": [], "entities": [{"text": "role assignment in filler-gap constructions", "start_pos": 21, "end_pos": 64, "type": "TASK", "confidence": 0.7220118165016174}]}, {"text": "Additionally, this model is shown to be able to account fora characteristic error made by learners during this period (A and B gorped interpreted as A gorped B).", "labels": [], "entities": []}], "introductionContent": [{"text": "The phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g. [the apple] i that the boy ate ti or [what] i did the boy eat ti ), has long been an object of study for syntacticians due to its apparent processing complexity.", "labels": [], "entities": []}, {"text": "Such complexity is due, in part, to the arbitrary length of the dependency between a filler and its gap (e.g. [the apple] i that Mary said the boy ate ti ).", "labels": [], "entities": []}, {"text": "Recent studies indicate that comprehension of filler-gap constructions begins around 15 months ().", "labels": [], "entities": [{"text": "comprehension of filler-gap constructions", "start_pos": 29, "end_pos": 70, "type": "TASK", "confidence": 0.6133083254098892}]}, {"text": "This finding raises the question of how such a complex phenomenon could be acquired so early since children at that age do not yet have a very advanced grasp of language (e.g. ditransitives do not seem to be generalized until at least 31 months;).", "labels": [], "entities": []}, {"text": "This work shows that filler-gap comprehension in English maybe The developmental timeline of subject (Wh-S) and object (Wh-O) wh-clause extraction comprehension suggested by experimental results ().", "labels": [], "entities": []}, {"text": "The final row shows the timeline of 1-1 role bias errors).", "labels": [], "entities": []}, {"text": "Missing nodes denote alack of studies.", "labels": [], "entities": []}, {"text": "acquired through learning word orderings rather than relying on hierarchical syntactic knowledge.", "labels": [], "entities": []}, {"text": "This work describes a cognitive model of the developmental timecourse of filler-gap comprehension with the goal of setting a lower bound on the modeling assumptions necessary for an ideal learner to display filler-gap comprehension.", "labels": [], "entities": []}, {"text": "In particular, the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles.", "labels": [], "entities": []}, {"text": "These orderings then permit the model to successfully resolve filler-gap dependencies.", "labels": [], "entities": []}, {"text": "1 Further, the model presented here is also shown to initially reflect an idiosyncratic role assignment error observed in development (e.g. A and B kradded interpreted as A kradded B ; Gertner and Fisher, 2012), though after training, the model is able to avoid the error.", "labels": [], "entities": []}, {"text": "As such, this work maybe said to model a learner from 15 months to between 25 and 30 months.", "labels": [], "entities": []}], "datasetContent": [{"text": "The model in this work is trained using transcribed child-directed speech (CDS) from the BabySRL portions (   ing a basic noun-chunker from NLTK ().", "labels": [], "entities": [{"text": "BabySRL portions", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.9769527614116669}]}, {"text": "Based on an initial analysis of chunker performance, yes is hand-corrected to not be a noun.", "labels": [], "entities": [{"text": "chunker", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9607545137405396}]}, {"text": "Poor chunker perfomance is likely due to a mismatch in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise maybe a good estimation of learner uncertainty, so the remaining text is left uncorrected.", "labels": [], "entities": [{"text": "Wall Street Journal text", "start_pos": 93, "end_pos": 117, "type": "DATASET", "confidence": 0.928000271320343}]}, {"text": "All noun phrase chunks are then replaced with their final noun (presumed the head) to approximate the ability of children to distinguish nouns from modifiers ().", "labels": [], "entities": []}, {"text": "Finally, for each sentence, the model assigns sentence positions to each word with the final verb at zero.", "labels": [], "entities": []}, {"text": "Viterbi Expectation-Maximization is performed over each sentence in the corpus to infer the parameters of the model.", "labels": [], "entities": []}, {"text": "During the Expectation step, the model uses the current Gaussian parameters to label the nouns in each sentence with argument roles.", "labels": [], "entities": [{"text": "Expectation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9073238372802734}]}, {"text": "Since the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object.", "labels": [], "entities": []}, {"text": "The model then chooses the best label sequence for each sentence.", "labels": [], "entities": []}, {"text": "These newly labelled sentences are used during the Maximization step to determine the Gaussian parameters that maximize the likelihood of that labelling.", "labels": [], "entities": [{"text": "Maximization", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9854435324668884}]}, {"text": "The mean of each Gaussian is updated to the mean position of the words it labels.", "labels": [], "entities": []}, {"text": "Similarly, the standard deviation of each Gaussian is updated with the standard deviation of the positions it labels.", "labels": [], "entities": []}, {"text": "A learning rate of 0.3 is used to prevent large parameter jumps.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9458799660205841}]}, {"text": "The prior probability of each Gaussian is updated as the ratio of that Gaussian's labellings to the total number of labellings from that mixture in the corpus: where \u03c1 \u2208 {S, O} and \u03b8 \u2208 {C, N }.", "labels": [], "entities": []}, {"text": "Best results seem to be obtained when the skippenalty is loosened by an order of magnitude dur-  ing testing.", "labels": [], "entities": [{"text": "skippenalty", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9375573992729187}]}, {"text": "Essentially, this forces the model to tightly adhere to the perceived argument structure during training to learn more rigid parameters, but the model is allowed more leeway to skip arguments it has less confidence in during testing.", "labels": [], "entities": []}, {"text": "Convergence (see) tends to occur after four iterations but can take up to ten iterations depending on the initial parameters.", "labels": [], "entities": []}, {"text": "Since the model is unsupervised, it is trained on a given corpus (e.g. Eve) before being tested on the role annotations of that same corpus.", "labels": [], "entities": []}, {"text": "The Eve corpus was used for development purposes, 8 and the Adam data was used only for testing.", "labels": [], "entities": [{"text": "Eve corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.737302228808403}]}, {"text": "For testing, this study uses the semantic role annotations in the BabySRL corpus.", "labels": [], "entities": [{"text": "BabySRL corpus", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9855985045433044}]}, {"text": "These annotations were obtained by automatically semantic role labelling portions of CHILDES with the system of before roughly hand-correcting them ().", "labels": [], "entities": [{"text": "CHILDES", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.7952743768692017}]}, {"text": "The BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles.", "labels": [], "entities": [{"text": "BabySRL corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9829312264919281}]}, {"text": "Therefore, overall accuracy results (see) are presented both for the raw BabySRL corpus and fora collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.99937504529953}, {"text": "BabySRL corpus", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9704626202583313}, {"text": "BabySRL corpus", "start_pos": 107, "end_pos": 121, "type": "DATASET", "confidence": 0.945973128080368}]}, {"text": "Since children do not generalize above two arguments during the modelled age range (, the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers.", "labels": [], "entities": []}, {"text": "The increase inaccuracy obtained from collapsing non-agent arguments indicates that children may initially generalize incorrectly to some verbs and would need to learn lexically-specific role assignments (e.g. double-object constructions of give).", "labels": [], "entities": []}, {"text": "Since the current work is interested in general filler-gap comprehension at this age, including over unknown verbs, the remaining analyses in this paper con-: (Left) Subject-extraction accuracy and object-extraction accuracy and (Right) Wh-relative accuracy and that-relative accuracy; calculated over the Eve and Adam sections of the BabySRL corpus with non-agent roles collapsed into a single role.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9523677825927734}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9113686680793762}, {"text": "Wh-relative accuracy", "start_pos": 237, "end_pos": 257, "type": "METRIC", "confidence": 0.8945309817790985}, {"text": "accuracy", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.8241981267929077}, {"text": "BabySRL corpus", "start_pos": 335, "end_pos": 349, "type": "DATASET", "confidence": 0.9819363653659821}]}, {"text": "\u2020 p = .02 * p << .01 sider performance when non-agent arguments are collapsed.", "labels": [], "entities": [{"text": "sider", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9431114792823792}]}, {"text": "Next, a filler-gap version of the BabySRL corpus is created using a coarse filtering process: the new corpus is comprised of all sentences where an associated object precedes the final verb and all sentences where the relevant subject is not immediately followed by the final verb (see).", "labels": [], "entities": [{"text": "BabySRL corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9733273983001709}]}, {"text": "For these filler-gap evaluations, the model is trained on the full version of the corpus in question (e.g. Eve) before being tested on the filler-gap subset of that corpus.", "labels": [], "entities": []}, {"text": "The overall results of the filler-gap evaluation (see) indicate that the model improves significantly at parsing filler-gap constructions after training.", "labels": [], "entities": [{"text": "parsing filler-gap constructions", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.901434063911438}]}, {"text": "The performance of the model on roleassignment in filler-gap constructions maybe analyzed further in terms of how the model performs on subject-extractions compared with object-extractions and in terms of how the model performs on that-relatives compared with whrelatives (see).", "labels": [], "entities": []}, {"text": "The model actually performs worse at subjectextractions after training than before training.", "labels": [], "entities": []}, {"text": "This is unsurprising because, prior to training, subjects have little-to-no competition for preverbal role assignments; after training, there is a preverbal extracted object category, which the model can erroneously use.", "labels": [], "entities": []}, {"text": "This slight, though significant in Eve, deficit is counter-balanced by a very substantial and significant improvement in objectextraction labelling accuracy.", "labels": [], "entities": [{"text": "objectextraction labelling", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.7819645404815674}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9356003999710083}]}, {"text": "Similarly, training confers a large and significant improvement for role assignment in wh-relative constructions, but it yields less of an improvement for that-relative constructions.", "labels": [], "entities": [{"text": "role assignment", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.8144626021385193}]}, {"text": "This difference mimics a finding observed in the developmental literature where children seem slower to acquire comprehension of that-relatives than of whrelatives (.", "labels": [], "entities": []}, {"text": "Though performance is slightly worse when arguments are not collapsed, all the same patterns emerge.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Overall accuracy on the Eve and Adam  sections of the BabySRL corpus. Bottom rows re- flect accuracy when non-agent roles are collapsed  into a single role. Note that improvements are nu- merically slight since filler-gap is relatively rare  (Schuler, 2011).  *  p << .01", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995430707931519}, {"text": "BabySRL corpus", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9773893058300018}, {"text": "flect", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9475030899047852}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.8921278715133667}]}, {"text": " Table 4: (Above) Filters to extract filler-gap con- structions: A) the subject and verb are not ad- jacent, B) the object precedes the verb. (Below)  Filler-gap accuracy on the Eve and Adam sections  of the BabySRL corpus when non-agent roles are  collapsed into a single role.  *  p << .01", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9982072114944458}, {"text": "BabySRL corpus", "start_pos": 208, "end_pos": 222, "type": "DATASET", "confidence": 0.9723330736160278}]}, {"text": " Table 5: (Left) Subject-extraction accuracy and object-extraction accuracy and (Right) Wh-relative ac- curacy and that-relative accuracy; calculated over the Eve and Adam sections of the BabySRL corpus  with non-agent roles collapsed into a single role.  \u2020 p = .02  *  p << .01", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9894877672195435}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9810601472854614}, {"text": "Wh-relative ac- curacy", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.929584413766861}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9854961037635803}, {"text": "BabySRL corpus", "start_pos": 188, "end_pos": 202, "type": "DATASET", "confidence": 0.9827330708503723}]}]}