{"title": [], "abstractContent": [{"text": "We present a novel way of generating unseen words, which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.6157260437806448}, {"text": "optical character recognition", "start_pos": 132, "end_pos": 161, "type": "TASK", "confidence": 0.6277644038200378}]}, {"text": "We test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set.", "labels": [], "entities": [{"text": "out-of-vocabulary word rate", "start_pos": 94, "end_pos": 121, "type": "METRIC", "confidence": 0.6147411266962687}]}, {"text": "The languages we study have very different morphological properties; we show how our results differ depending on the morphological complexity of the language.", "labels": [], "entities": []}, {"text": "In our best result (on As-samese), our approach can predict 29% of the token-based out-of-vocabulary with a small amount of unlabeled training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many applications inhuman language technologies (HLT), the goal is to generate text in a target language, using its standard orthography.", "labels": [], "entities": []}, {"text": "Typical examples include automatic speech recognition (ASR, also known as STT or speech-to-text), optical character recognition (OCR), or machine translation (MT) into a target language.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.695478880405426}, {"text": "optical character recognition (OCR)", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.770614837606748}, {"text": "machine translation (MT) into a target language", "start_pos": 138, "end_pos": 185, "type": "TASK", "confidence": 0.8315820495287577}]}, {"text": "We will call such HLT applications \"target-language generation technologies\" (TLGT).", "labels": [], "entities": []}, {"text": "The best-performing systems for these applications today rely on training on large amounts of data: in the case of ASR, the data is aligned audio and transcription, plus large unannotated data for the language modeling; in the case of OCR, it is transcribed optical data; in the case of MT, it is aligned bitexts.", "labels": [], "entities": [{"text": "ASR", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9316851496696472}, {"text": "MT", "start_pos": 287, "end_pos": 289, "type": "TASK", "confidence": 0.7651079297065735}]}, {"text": "More data provides for better results.", "labels": [], "entities": []}, {"text": "For languages with rich resources, such as English, more data is often the best solution, since the required data is readily available (including bitexts), and the cost of annotating (e.g., transcribing) data is outweighed by the potential significance of the systems that the data will enable.", "labels": [], "entities": []}, {"text": "Thus, in HLT, improvements in quality are often brought about by using larger data sets).", "labels": [], "entities": []}, {"text": "When we move to low-resource languages, the solution of simply using more data becomes less appealing.", "labels": [], "entities": []}, {"text": "Unannotated data is less readily available: for example, at the time of publishing this paper, 55% of all websites are in English, the top 10 languages collectively account for 90% of web presence, and the top 36 languages have a web presence that covers at least 0.1% of web sites.", "labels": [], "entities": []}, {"text": "All other languages (and all languages considered in this paper except Persian) have a web presence of less than 0.1%.", "labels": [], "entities": []}, {"text": "Considering Wikipedia, another resource often used in HLT, English has 4.4 million articles, while only 48 other languages have more than 100,000.", "labels": [], "entities": [{"text": "HLT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9367846846580505}]}, {"text": "As attention turns to developing HLT for more languages, including lowresource languages, alternatives to \"more-data\" approaches become important.", "labels": [], "entities": []}, {"text": "At the same time, it is often not possible to use knowledge-rich approaches.", "labels": [], "entities": []}, {"text": "For low-resource languages, resources such as morphological analyzers are not usually available, and even good scholarly descriptions of the morphology (from which a tool could be built) are often not available.", "labels": [], "entities": []}, {"text": "The challenge is therefore to use data, but to make do with a small amount of data, and thus to use data better.", "labels": [], "entities": []}, {"text": "This paper is a contribution to this goal.", "labels": [], "entities": []}, {"text": "Specifically, we address TLGTs, i.e., the types of HLT mentioned above that generate target language text.", "labels": [], "entities": []}, {"text": "We propose anew approach to generating unseen words of the target language which have not been seen in the training data.", "labels": [], "entities": []}, {"text": "Our approach is entirely unsupervised.", "labels": [], "entities": []}, {"text": "It assumes that word-units are specified, typically by whitespace and punctuation.", "labels": [], "entities": []}, {"text": "Expanding the vocabulary of the target language can be useful for TLGTs in different ways.", "labels": [], "entities": [{"text": "TLGTs", "start_pos": 66, "end_pos": 71, "type": "TASK", "confidence": 0.9386100769042969}]}, {"text": "For ASR and OCR, which can compose words from smaller units (phones or graphically recognized letters), an expanded target language vocabulary can be directly exploited without the need for changing the technology at all: the new words need to be inserted into the relevant resources (lexicon, language model) etc, with appropriately estimated probabilities.", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9668025970458984}]}, {"text": "In the case of MT into morphologically rich low-resource languages, morphological segmentation is typically used in developing the translation models to reduce sparsity, but this does not guarantee against generating wrong word combinations.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9909352660179138}]}, {"text": "The expanded word combinations can be used to extend the language models used for MT to bias against incoherent hypothesized new sequences of segmented words.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9951867461204529}]}, {"text": "Our approach relies on unsupervised morphological segmentation.", "labels": [], "entities": []}, {"text": "We do not in this paper contribute to research in unsupervised morphological segmentation; we only use it.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6794528961181641}]}, {"text": "The contribution of this paper lies in proposing how to use the results of unsupervised morphological segmentation in order to generate unseen words of the language.", "labels": [], "entities": []}, {"text": "We investigate several ways of doing so, and we test them on seven low-resource languages.", "labels": [], "entities": []}, {"text": "These languages have very different morphological properties, and we show how our results differ depending on the morphological complexity of the language.", "labels": [], "entities": []}, {"text": "In our best result (on Assamese), we show that our approach can predict 29% of the tokenbased out-of-vocabulary with a small amount of unlabeled training data.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first discuss related work in Section 2.", "labels": [], "entities": []}, {"text": "We then present our method in Section 3, and present experimental results in Section 4.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation Data The IARPA Babel program is a research program for developing rapid spoken detection systems for under-resourced languages.", "labels": [], "entities": [{"text": "rapid spoken detection", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7304013172785441}]}, {"text": "We use the IARPA Babel program limited language pack data which consists of 20 hours of telephone speech with transcription.", "labels": [], "entities": [{"text": "IARPA Babel program limited language pack data", "start_pos": 11, "end_pos": 57, "type": "DATASET", "confidence": 0.8355445350919452}]}, {"text": "We use six languages which are known to have rich morphology: Assamese (IARPAbabel102b-v0.5a), Bengali (IARPA-babel103b-v0.4b), Pashto (IARPA-babel104b-v0.4bY), Tagalog (IARPA-babel106-v0.2g), Turkish (IARPAbabel105b-v0.4) and Zulu (IARPA-babel206b-v0.1e).", "labels": [], "entities": []}, {"text": "Speech annotation such as silences and hesitations are removed from transcription and all words are turned into lower-case (for languages using the Roman script -Tagalog, Turkish and Zulu).", "labels": [], "entities": []}, {"text": "Moreover, in order to be able to perform a manual error analysis, we include a language that has rich morphology and of which the first author is a native speaker: Persian.", "labels": [], "entities": []}, {"text": "We sampled data from the training and development set of the Persian dependency treebank () to create a comparable seventh dataset in Persian.", "labels": [], "entities": [{"text": "Persian dependency treebank", "start_pos": 61, "end_pos": 88, "type": "DATASET", "confidence": 0.8234160343805949}]}, {"text": "Statistics about the datasets are shown in.", "labels": [], "entities": []}, {"text": "We also conduct further experiments on just verbs and nouns in the data set for Persian (Persian-N and Persian V).", "labels": [], "entities": []}, {"text": "As shown in, the training data is very small and the OOV rate is high especially in terms of types.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9863821864128113}]}, {"text": "For some languages that have richer morphology such as Turkish and Zulu, the OOV rate is much higher than other languages.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.981296956539154}]}, {"text": "Word Generation Tools and Settings For unsupervised learning of morphology, we use Morfessor CAT-MAP (v.", "labels": [], "entities": [{"text": "Word Generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5967948734760284}, {"text": "Morfessor CAT-MAP", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.8294457495212555}]}, {"text": "0.9.2) which was shown to be a very accurate morphological analyzer for morphologically rich languages and thus we also have a morphological analyzer that can give all possible segmentations fora given word.", "labels": [], "entities": []}, {"text": "By running the morphological analyzer on the OOVs, we can have the potential upper bound of OOV reduction by the system (labeled \"\u221e\" in).", "labels": [], "entities": [{"text": "OOV", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.8646051287651062}]}], "tableCaptions": [{"text": " Table 1: Statistics of training and development data for morphology-based unsupervised word generation  experiments.", "labels": [], "entities": [{"text": "morphology-based unsupervised word generation", "start_pos": 58, "end_pos": 103, "type": "TASK", "confidence": 0.6075870618224144}]}, {"text": " Table 2: Type-based expansion results for the 50k-best list for different models. Tr. WFST stands for  trigraph WFST, NRR for no reranking, W\u2022Tr for trigraph reweighting, TRR for trigraph-based rereank- ing, BRR for reranking morpheme boundary, and \u221e for the upper bound of OOV reduction via lexicon  expansion if we produce all words. FP (full-pack data) shows the effect of using bigger data with the size  of about seven times larger than our data set, instead of using our unsupervised approach.", "labels": [], "entities": [{"text": "Tr", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9445008635520935}, {"text": "BRR", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.9955317974090576}, {"text": "OOV reduction", "start_pos": 275, "end_pos": 288, "type": "TASK", "confidence": 0.7608976364135742}, {"text": "FP", "start_pos": 337, "end_pos": 339, "type": "METRIC", "confidence": 0.9170887470245361}]}, {"text": " Table 3: Token-based expansion results for the 50k-best list for different models. Abbreviations are the  same as", "labels": [], "entities": [{"text": "Token-based expansion", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9110070168972015}, {"text": "Abbreviations", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.9978633522987366}]}, {"text": " Table 4: Information about the number of unique  morphemes in the Fixed Affix model for each  dataset including empty affixes. |L| shows the  upper bound of the number of possible unique  words that can be generated from the word gener- ation model. |If | is the average number of unique  prefix-suffix pairs (including empty pairs) for each  stem.", "labels": [], "entities": []}, {"text": " Table 5: Results from running a hand-crafted  Turkish morphological analyzer", "labels": [], "entities": [{"text": "Turkish morphological analyzer", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.5447932283083597}]}]}