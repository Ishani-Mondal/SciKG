{"title": [{"text": "Learning Bilingual Word Representations by Marginalizing Alignments", "labels": [], "entities": [{"text": "Learning Bilingual Word Representations", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5634083151817322}, {"text": "Marginalizing Alignments", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8232444226741791}]}], "abstractContent": [{"text": "We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data.", "labels": [], "entities": []}, {"text": "By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.6804632395505905}]}, {"text": "The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.", "labels": [], "entities": [{"text": "cross-lingual classification task", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.7589049537976583}]}], "introductionContent": [{"text": "Distributed representations have become an increasingly important tool in machine learning.", "labels": [], "entities": [{"text": "Distributed representations", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.840705543756485}, {"text": "machine learning", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7517243027687073}]}, {"text": "Such representations-typically continuous vectors learned in an unsupervised setting-can frequently be used in place of hand-crafted, and thus expensive, features.", "labels": [], "entities": []}, {"text": "By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas.", "labels": [], "entities": []}, {"text": "This includes AI and reinforcement learning, image retrieval (, language modelling (, sentiment analysis, framesemantic parsing (, and document classification (.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7907949388027191}, {"text": "language modelling", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.6936734467744827}, {"text": "sentiment analysis", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.8012543022632599}, {"text": "framesemantic parsing", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.6959191113710403}, {"text": "document classification", "start_pos": 135, "end_pos": 158, "type": "TASK", "confidence": 0.7598297595977783}]}, {"text": "In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7375559906164805}]}, {"text": "The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also and).", "labels": [], "entities": []}, {"text": "While most work employing distributed representations has focused on monolingual tasks, multilingual representations would also be useful for several NLP-related tasks.", "labels": [], "entities": []}, {"text": "Such problems include document classification, machine translation, and cross-lingual information retrieval, where multilingual data is frequently the norm.", "labels": [], "entities": [{"text": "document classification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8026270270347595}, {"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8069277703762054}, {"text": "cross-lingual information retrieval", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.6887529790401459}]}, {"text": "Furthermore, learning multilingual representations can also be useful for cross-lingual information transfer, that is exploiting resource-fortunate languages to generate supervised data in resource-poor ones.", "labels": [], "entities": [{"text": "cross-lingual information transfer", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.6492261290550232}]}, {"text": "We propose a probabilistic model that simultaneously learns word alignments and bilingual distributed word representations.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7101061046123505}]}, {"text": "As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (), we marginalize out the alignments, thus capturing more bilingual semantic context.", "labels": [], "entities": []}, {"text": "Further, this results in our distributed word alignment (DWA) model being the first probabilistic account of bilingual word representations.", "labels": [], "entities": [{"text": "distributed word alignment (DWA)", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7332439919312795}]}, {"text": "This is desirable as it allows better reasoning about the derived representations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.8257136940956116}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "We present anew probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages.", "labels": [], "entities": []}, {"text": "Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (.", "labels": [], "entities": [{"text": "document classification task", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.788551131884257}]}, {"text": "As a by-product we develop a distributed version of FASTALIGN (, which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations.", "labels": [], "entities": [{"text": "FASTALIGN", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.8129672408103943}]}], "datasetContent": [{"text": "We first evaluate the alignment error rate of our approach, which establishes the model's ability to both learn alignments as well as word representations that explain these alignments.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 22, "end_pos": 42, "type": "METRIC", "confidence": 0.6275669535001119}]}, {"text": "Next, we use a cross-lingual document classification task to verify that the representations are semantically useful.", "labels": [], "entities": [{"text": "cross-lingual document classification task", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.6918862834572792}]}, {"text": "We also inspect the embedding space qualitatively to get some insight into the learned structure.", "labels": [], "entities": []}, {"text": "We compare the alignments learned herewith those of the FASTALIGN model which produces very good alignments and translation BLEU scores.", "labels": [], "entities": [{"text": "FASTALIGN", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.6115055680274963}, {"text": "translation", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.8101005554199219}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.8427740335464478}]}, {"text": "We use the same language pairs and datasets as in, that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (.", "labels": [], "entities": [{"text": "FBIS Chinese-English corpus", "start_pos": 63, "end_pos": 90, "type": "DATASET", "confidence": 0.9483030438423157}, {"text": "Europarl corpus", "start_pos": 130, "end_pos": 145, "type": "DATASET", "confidence": 0.9941723942756653}]}, {"text": "We used the preprocessing tools from CDEC and further replaced all unique tokens with UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8615727424621582}]}, {"text": "We trained our models with 100 dimensional representations for up to 40 iterations, and the FA model for 5 iterations as is the default.", "labels": [], "entities": [{"text": "FA", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9155437350273132}]}, {"text": "shows that our model learns alignments on part with those of the FA model.", "labels": [], "entities": []}, {"text": "This is inline with expectation as our model was trained using the FA expectations.", "labels": [], "entities": [{"text": "FA expectations", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.8643004596233368}]}, {"text": "However, it confirms that the learned word representations are able to explain translation probabilities.", "labels": [], "entities": []}, {"text": "Surprisingly, context seems to have little impact on the alignment error, suggesting that the model receives sufficient information from the aligned words themselves.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Alignment error rate (AER) compar- ison, in both directions, between the FASTAL- IGN (FA) alignment model and our model (DWA)  with k context words (see Equation 1). Lower  numbers indicate better performance.", "labels": [], "entities": [{"text": "Alignment error rate (AER) compar- ison", "start_pos": 10, "end_pos": 49, "type": "METRIC", "confidence": 0.9417028625806173}, {"text": "FASTAL- IGN (FA", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.6710489511489868}]}, {"text": " Table 2: Document classification accuracy when  trained on 1,000 training examples of the RCV1/2  corpus (train\u2192test). Baselines are the majority  class, glossed, and MT (Klementiev et al., 2012).  Further, we are comparing to Klementiev et al.  (2012), BiCVM ADD (", "labels": [], "entities": [{"text": "Document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8143265545368195}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.972412645816803}, {"text": "RCV1/2  corpus", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.8375426828861237}, {"text": "MT", "start_pos": 168, "end_pos": 170, "type": "METRIC", "confidence": 0.9963395595550537}, {"text": "BiCVM ADD", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.8645514249801636}]}]}