{"title": [], "abstractContent": [{"text": "This paper addresses the problem of EM-based decipherment for large vocabularies.", "labels": [], "entities": [{"text": "EM-based decipherment", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.8997983634471893}]}, {"text": "Here, decipherment is essentially a tagging problem: Every cipher token is tagged with some plaintext type.", "labels": [], "entities": []}, {"text": "As with other tagging problems, this one can be treated as a Hidden Markov Model (HMM), only here, the vocabularies are large, so the usual O(N V 2) exact EM approach is infeasible.", "labels": [], "entities": []}, {"text": "When faced with this situation, many people turn to sampling.", "labels": [], "entities": []}, {"text": "However, we propose to use a type of approximate EM and show that it works well.", "labels": [], "entities": []}, {"text": "The basic idea is to collect fractional counts only over a small subset of links in the forward-backward lattice.", "labels": [], "entities": []}, {"text": "The subset is different for each iteration of EM.", "labels": [], "entities": []}, {"text": "One option is to use beam search to do the subsetting.", "labels": [], "entities": []}, {"text": "The second method restricts the successor words that are looked at, for each hypothesis.", "labels": [], "entities": []}, {"text": "It does this by consulting pre-computed tables of likely n-grams and likely substitutions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The decipherment of probabilistic substitution ciphers (ciphers in which each plaintext token can be substituted by any cipher token, following a distribution p(f |e), cf.) can be seen as an important step towards decipherment for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 231, "end_pos": 233, "type": "TASK", "confidence": 0.9935523271560669}]}, {"text": "This problem has not been studied explicitly before.", "labels": [], "entities": []}, {"text": "Scaling to larger vocabularies for probabilistic substitution ciphers decipherment is a difficult problem: The algorithms for 1:1 or homophonic substitution ciphers are not applicable, and standard algorithms like EM training become intractable when vocabulary sizes go beyond a few hundred words.", "labels": [], "entities": []}, {"text": "In this paper we present an efficient EM based training procedure for probabilistic substitution ciphers which provides high decipherment accuracies while having low computational requirements.", "labels": [], "entities": []}, {"text": "The proposed approach allows using high order n-gram language models, and is scalable to large vocabulary sizes.", "labels": [], "entities": []}, {"text": "We show improvements in decipherment accuracy in a variety of experiments (including MT) while being computationally more efficient than previous published work on EM-based decipherment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9845959544181824}, {"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9494025111198425}]}], "datasetContent": [{"text": "We first show experiments for data in which the underlying model is an actual 1:1 substitution cipher.", "labels": [], "entities": []}, {"text": "In this case, we report the word accuracy of the final decipherment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9700677990913391}]}, {"text": "We then show experiments fora simple machine translation task.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.7982257604598999}]}, {"text": "Here we report translation quality in BLEU.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9666723608970642}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9798473119735718}]}, {"text": "The corpora used in this paper are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics of the copora used in this pa- per: The VERBMOBIL corpus is used to conduct  experiments on simple substitution ciphers, while  the OPUS corpus is used in our Machine Transla- tion experiments.", "labels": [], "entities": [{"text": "VERBMOBIL corpus", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.7370258569717407}, {"text": "OPUS corpus", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.8122029006481171}]}, {"text": " Table 4: Results for simple substitution ciphers  based on the VERBMOBIL corpus using exact,  beam, and preselection EM. Exact EM is not  tractable for vocabulary sizes above 200.", "labels": [], "entities": [{"text": "VERBMOBIL corpus", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.9561062753200531}, {"text": "exact", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.95059734582901}]}, {"text": " Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on  the Spanish/English OPUS corpus using only non-parallel corpora for training.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9447453022003174}, {"text": "BLEU scores)", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9421643416086832}, {"text": "Spanish/English OPUS corpus", "start_pos": 104, "end_pos": 131, "type": "DATASET", "confidence": 0.5784464359283448}]}]}