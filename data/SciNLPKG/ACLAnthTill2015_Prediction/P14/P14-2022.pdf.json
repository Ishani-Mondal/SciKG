{"title": [{"text": "Faster Phrase-Based Decoding by Refining Feature State", "labels": [], "entities": [{"text": "Phrase-Based Decoding", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.9206474423408508}]}], "abstractContent": [{"text": "We contribute a faster decoding algorithm for phrase-based machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6769210199515024}]}, {"text": "Translation hypotheses keep track of state, such as context for the language model and coverage of words in the source sentence.", "labels": [], "entities": []}, {"text": "Most features depend upon only part of the state, but traditional algorithms, including cube pruning, handle state atomically.", "labels": [], "entities": []}, {"text": "For example, cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage , despite the fact that source coverage is irrelevant to the language model.", "labels": [], "entities": []}, {"text": "Our key contribution avoids this behavior by placing hypotheses into equivalence classes, masking the parts of state that matter least to the score.", "labels": [], "entities": []}, {"text": "Moreover, we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically.", "labels": [], "entities": []}, {"text": "Since our algorithm and cube pruning are both approximate, improvement can be used to increase speed or accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9788768887519836}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9969521760940552}]}, {"text": "When tuned to attain the same accuracy, our algorithm is 4.0-7.7 times as fast as the Moses decoder with cube pruning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9990589022636414}]}], "introductionContent": [{"text": "Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile devices without Internet connectivity.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9453567266464233}]}, {"text": "We contribute a fast decoding algorithm for phrase-based machine translation along with an implementation in anew open-source (LGPL) decoder available at http://kheafield.com/code/.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6638673345247904}]}, {"text": "Phrase-based decoders () keep track of several types of state with translation hypotheses: coverage of the source sentence thus far, context for the language model, the last position for the distortion model, and anything else features need.", "labels": [], "entities": []}, {"text": "Existing decoders handle state atomically: hypotheses that have exactly the same state can be recombined and efficiently handled via dynamic programming, but there is no special handling for partial agreement.", "labels": [], "entities": []}, {"text": "Therefore, features are repeatedly consulted regarding hypotheses that differ only in ways irrelevant to their score, such as coverage of the source sentence.", "labels": [], "entities": []}, {"text": "Our decoder bundles hypotheses into equivalence classes so that features can focus on the relevant parts of state.", "labels": [], "entities": []}, {"text": "We pay particular attention to the language model because it is responsible for much of the hypothesis state.", "labels": [], "entities": []}, {"text": "As the decoder builds translations from left to right, it records the last N \u2212 1 words of each hypothesis so that they can be used as context to score the first N \u2212 1 words of a phrase, where N is the order of the language model.", "labels": [], "entities": []}, {"text": "Traditional decoders) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes.", "labels": [], "entities": []}, {"text": "Our algorithm instead discovers good combinations in a coarse-to-fine manner.", "labels": [], "entities": []}, {"text": "The algorithm exploits the fact that hypotheses often share the same suffix and phrases often share the same prefix.", "labels": [], "entities": []}, {"text": "These shared suffixes and prefixes allow the algorithm to coarsely reason over many combinations at once.", "labels": [], "entities": []}, {"text": "Our primary contribution is anew search algorithm that exploits the above observations, namely that state can be divided into pieces relevant to each feature and that language model state can be further subdivided.", "labels": [], "entities": []}, {"text": "The primary claim is that our algorithm is faster and more accurate than the popular cube pruning algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "The primary claim is that our algorithm performs better than cube pruning in terms of the trade-off between time and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9899090528488159}]}, {"text": "We compare our new decoder implementation with Moses () by translating 1677 sentences from Chinese to English.", "labels": [], "entities": []}, {"text": "These sentences area deduplicated subset of the NIST Open MT 2012 test set and were drawn from Chinese online text sources, such as discussion forums.", "labels": [], "entities": [{"text": "NIST Open MT 2012 test set", "start_pos": 48, "end_pos": 74, "type": "DATASET", "confidence": 0.9215273956457773}]}, {"text": "We trained our phrase table using a bitext of 10.8 million sentence pairs, which after tokenization amounts to approximately 290 million words on the English side.", "labels": [], "entities": []}, {"text": "The bitext contains data from several sources, including news articles, UN proceedings, Hong Kong government documents, online forum data, and specialized sources such as an idiom translation table.", "labels": [], "entities": []}, {"text": "We also trained our language model on the English half of this bitext using unpruned interpolated modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "The system has standard phrase table, length, distortion, and language model features.", "labels": [], "entities": []}, {"text": "We plan to implement lexicalized reordering in future work; without this, the test system is 0.53 BLEU () point behind a state-of-theart system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9990800619125366}]}, {"text": "We set the reordering limit to R = 15.", "labels": [], "entities": [{"text": "R", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9808256030082703}]}, {"text": "The phrase table was pre-pruned by applying the same heuristic as Moses: select the top 20 target phrases by score, including the language model.", "labels": [], "entities": []}, {"text": "Moses () revision d6df825 was compiled with all optimizations recommended in the documentation.", "labels": [], "entities": [{"text": "Moses () revision d6df825", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8660902380943298}]}, {"text": "We use the inmemory phrase table for speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9898689985275269}]}, {"text": "Tests were run on otherwise-idle identical machines with 32 GB RAM; the processes did not come close to running out of memory.", "labels": [], "entities": []}, {"text": "The language model was compiled into KenLM probing format and placed in RAM while text phrase tables were forced into the disk cache before each run.", "labels": [], "entities": []}, {"text": "Timing is based on CPU usage (user plus system) minus loading time, as measured by running on empty input; our decoder is also faster at loading.", "labels": [], "entities": []}, {"text": "Model score is comparable across decoders and averaged overall 1677 sentences; higher is better.", "labels": [], "entities": []}, {"text": "The relationship between model score and uncased BLEU () is noisy, so peak BLEU is not attained by the highest search accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9829810857772827}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9921122789382935}]}, {"text": "shows the results for pop limits k ranging from 5 to 10000 while shows select results.", "labels": [], "entities": []}, {"text": "For Moses, we also set the stack size to k to disable a second pruning pass, as is common.", "labels": [], "entities": []}, {"text": "Because Moses is slower, we also ran our decoder with higher beam sizes to fill in the graph.", "labels": [], "entities": []}, {"text": "Our decoder is more accurate, but mostly faster.", "labels": [], "entities": []}, {"text": "We can interpret accuracy improvments as speed improvements by asking how much time is required to attain the same accuracy as the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9925938248634338}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9977206587791443}]}, {"text": "By this metric, our decoder is 4.0 to 7.7 times as fast as Moses, depending on k.: Results for select stack sizes k.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for select stack sizes k.", "labels": [], "entities": []}]}