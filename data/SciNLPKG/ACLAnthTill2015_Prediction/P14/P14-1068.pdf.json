{"title": [{"text": "Learning Grounded Meaning Representations with Autoencoders", "labels": [], "entities": [{"text": "Learning Grounded Meaning Representations", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5464812144637108}]}], "abstractContent": [{"text": "In this paper we address the problem of grounding distributional representations of lexical meaning.", "labels": [], "entities": []}, {"text": "We introduce anew model which uses stacked autoencoders to learn higher-level embeddings from tex-tual and visual input.", "labels": [], "entities": []}, {"text": "The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively.", "labels": [], "entities": []}, {"text": "We evaluate our model on its ability to simulate similarity judgments and concept categorization.", "labels": [], "entities": []}, {"text": "On both tasks, our approach outperforms baselines and related models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen a surge of interest in single word vector spaces and their successful use in many natural language applications.", "labels": [], "entities": []}, {"text": "Examples include information retrieval (, search query expansions (), document classification), and question answering (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8202236592769623}, {"text": "search query expansions", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.6647774974505106}, {"text": "document classification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7540745437145233}, {"text": "question answering", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8978867530822754}]}, {"text": "Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 175, "end_pos": 192, "type": "TASK", "confidence": 0.9242739379405975}, {"text": "similarity judgments", "start_pos": 198, "end_pos": 218, "type": "TASK", "confidence": 0.6406445801258087}]}, {"text": "In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis: words that appear in similar linguistic contexts are likely to have related meanings.", "labels": [], "entities": []}, {"text": "Word meaning, however, is also tied to the physical world.", "labels": [], "entities": [{"text": "Word meaning", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6941475570201874}]}, {"text": "Words are grounded in the external environment and relate to sensorimotor experience.", "labels": [], "entities": []}, {"text": "To account for this, new types of perceptually grounded distributional models have emerged.", "labels": [], "entities": []}, {"text": "These models learn the meaning of words based on textual and perceptual input.", "labels": [], "entities": []}, {"text": "The latter is approximated by feature norms elicited from humans (, visual information extracted automatically from images, or a combination of both).", "labels": [], "entities": []}, {"text": "Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.736933708190918}]}, {"text": "These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images).", "labels": [], "entities": []}, {"text": "In this work, we introduce a model, illustrated in, which learns grounded meaning representations by mapping words and images into a common embedding space.", "labels": [], "entities": []}, {"text": "Our model uses stacked autoencoders ( to induce semantic representations integrating visual and textual information.", "labels": [], "entities": []}, {"text": "The literature describes several successful approaches to multimodal learning using different variants of deep networks) and data sources including text, images, audio, and video.", "labels": [], "entities": []}, {"text": "Unlike most previous work, our model is defined at a finer level of granularity -it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities.", "labels": [], "entities": []}, {"text": "We follow in arguing that an attribute-centric representation is expedient for several reasons.", "labels": [], "entities": []}, {"text": "Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g.,) where humans often employ attributes when asked to describe a concept.", "labels": [], "entities": []}, {"text": "Secondly, from a modeling perspective, attributes allow for easier integration of different modalities, since these are rendered in the same medium, namely, language.", "labels": [], "entities": []}, {"text": "Thirdly, attributes are well-suited to describing visual phenomena (e.g., objects, scenes, actions).", "labels": [], "entities": []}, {"text": "They allow to generalize to new instances for which there are no training examples available and to transcend category and task boundaries whilst offering a generic description of visual data.", "labels": [], "entities": []}, {"text": "Our model learns multimodal representations from attributes which are automatically inferred from text and images.", "labels": [], "entities": []}, {"text": "We evaluate the embeddings it produces on two tasks, namely word similarity and categorization.", "labels": [], "entities": []}, {"text": "In the first task, model estimates of word similarity (e.g., gem-jewel are similar but glass-magician are not) are compared against elicited similarity ratings.", "labels": [], "entities": []}, {"text": "We performed a large-scale evaluation on anew dataset consisting of human similarity judgments for 7,576 word pairs.", "labels": [], "entities": []}, {"text": "Unlike previous efforts such as the widely used WordSim353 collection (), our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning representation) together and in isolation.", "labels": [], "entities": [{"text": "WordSim353 collection", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9614834487438202}, {"text": "meaning representation", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.7121988534927368}]}, {"text": "We also assess whether the learnt representations are appropriate for categorization, i.e., grouping a set of objects into meaningful semantic categories (e.g., peach and apple are members of FRUIT, whereas chair and table are FURNITURE).", "labels": [], "entities": [{"text": "grouping a set of objects into meaningful semantic categories", "start_pos": 92, "end_pos": 153, "type": "TASK", "confidence": 0.7049642271465726}, {"text": "FRUIT", "start_pos": 192, "end_pos": 197, "type": "METRIC", "confidence": 0.5910401940345764}]}, {"text": "On both tasks, our model outperforms baselines and related models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental setup for assessing the performance of our model.", "labels": [], "entities": []}, {"text": "We give details on the tasks and datasets used for evaluation, we explain how the textual and visual inputs were constructed, how the SAE model was trained, and describe the approaches used for comparison with our own work.", "labels": [], "entities": []}, {"text": "Word Similarity We first evaluated how well our model predicts word similarity ratings.", "labels": [], "entities": [{"text": "Word Similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.604836493730545}]}, {"text": "Although several relevant datasets exist, such as the widely used WordSim353 () or the more recent Rel-122 norms, they contain many abstract words, (e.g., love-sex or arrest-detention) which are not covered in.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.9819231629371643}, {"text": "Rel-122 norms", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9203304350376129}]}, {"text": "This is fora good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon.", "labels": [], "entities": []}, {"text": "We thus created anew dataset consisting exclusively of nouns which we hope will be useful for the development and evaluation of grounded semantic space models.", "labels": [], "entities": []}, {"text": "Initially, we created all possible pairings over nouns and computed their semantic relatedness using Patwardhan and Pedersen (2006)'s WordNet-based measure.", "labels": [], "entities": [{"text": "WordNet-based measure", "start_pos": 134, "end_pos": 155, "type": "DATASET", "confidence": 0.8427540957927704}]}, {"text": "We opted for this specific measure as it achieves high correlation with human ratings and has a high coverage on our nouns.", "labels": [], "entities": [{"text": "coverage", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9942049384117126}]}, {"text": "Next, for each word we randomly selected 30 pairs under the assumption that they are representative of the full variation of semantic similarity.", "labels": [], "entities": []}, {"text": "This resulted in 7,576 word pairs for which we obtained similarity ratings using Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 81, "end_pos": 109, "type": "DATASET", "confidence": 0.8878298799196879}]}, {"text": "Participants were asked to rate a pair on two dimensions, visual and semantic similarity using a Likert scale of 1 (highly dissimilar) to 5 (highly similar).", "labels": [], "entities": []}, {"text": "Each task consisted of 32 pairs covering examples of weak to very strong semantic relatedness.", "labels": [], "entities": []}, {"text": "Two control pairs from were included in each task to potentially help identify and eliminate data from participants who assigned random scores.", "labels": [], "entities": []}, {"text": "Examples of the stimuli and mean ratings are shown in.", "labels": [], "entities": []}, {"text": "The elicitation study comprised overall 255 tasks, each task was completed by five volunteers.", "labels": [], "entities": []}, {"text": "The similarity data was post-processed so as to identify and remove outliers.", "labels": [], "entities": []}, {"text": "We considered an outlier to be any individual whose mean pairwise correlation fell outside two standard deviations from the mean correlation.", "labels": [], "entities": []}, {"text": "11.5% of the annotations were detected as outliers and removed.", "labels": [], "entities": []}, {"text": "After outlier removal, we further examined how well the participants agreed in their similarity judgments.", "labels": [], "entities": []}, {"text": "We measured inter-subject agreement as the average pairwise correlation coefficient (Spearman's \u03c1) between the ratings of all annotators for each task.", "labels": [], "entities": [{"text": "pairwise correlation coefficient", "start_pos": 51, "end_pos": 83, "type": "METRIC", "confidence": 0.8104642033576965}, {"text": "Spearman's \u03c1)", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.7602558881044388}]}, {"text": "For semantic similarity, the mean correlation was 0.76  =0.97, StD =0.11) and for visual similarity 0.63 (Min =0.19, Max =0.90, SD =0.14).", "labels": [], "entities": [{"text": "mean correlation", "start_pos": 29, "end_pos": 45, "type": "METRIC", "confidence": 0.8191255629062653}, {"text": "StD", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9632353782653809}, {"text": "Max", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9495261907577515}]}, {"text": "These results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency.", "labels": [], "entities": [{"text": "similarity", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9711545705795288}, {"text": "consistency", "start_pos": 146, "end_pos": 157, "type": "METRIC", "confidence": 0.9866541028022766}]}, {"text": "For comparison, measure achieved a coefficient of 0.56 on the dataset for semantic similarity and 0.48 for visual similarity.", "labels": [], "entities": []}, {"text": "The correlation between the average ratings of the AMT annotators and the Miller and Charles (1991) dataset was \u03c1 = 0.91.", "labels": [], "entities": [{"text": "AMT annotators", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8108369410037994}, {"text": "Miller and Charles (1991) dataset", "start_pos": 74, "end_pos": 107, "type": "DATASET", "confidence": 0.5997580928461892}, {"text": "\u03c1", "start_pos": 112, "end_pos": 113, "type": "METRIC", "confidence": 0.9581997990608215}]}, {"text": "In our experiments (see Section 5), we correlate modelbased cosine similarities with mean similarity ratings (again using Spearman's \u03c1).", "labels": [], "entities": []}, {"text": "Categorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language.", "labels": [], "entities": [{"text": "grouping objects into meaningful categories)", "start_pos": 49, "end_pos": 93, "type": "TASK", "confidence": 0.5827483137448629}]}, {"text": "We evaluated model output against a gold standard set of categories created by.", "labels": [], "entities": []}, {"text": "The dataset contains a classification, produced by human participants, of McRae et al.'s (2005) nouns into (possibly multiple) semantic categories (40 in total).", "labels": [], "entities": []}, {"text": "To obtain a clustering of nouns, we used Chinese Whispers), a randomized graph-clustering algorithm.", "labels": [], "entities": []}, {"text": "In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor- The dataset can be downloaded from http: //homepages.inf.ed.ac.uk/s0897549/data/.", "labels": [], "entities": []}, {"text": "respond to words and edges to cosine similarity scores between vectors representing their meaning.", "labels": [], "entities": []}, {"text": "CW is a non-parametric model, it induces the number of clusters (i.e., categories) from the data as well as which nouns belong to these clusters.", "labels": [], "entities": []}, {"text": "In our experiments, we initialized Chinese Whispers with different graphs resulting from different vector-based representations of the nouns.", "labels": [], "entities": [{"text": "initialized Chinese Whispers", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.516786257425944}]}, {"text": "We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see.", "labels": [], "entities": []}, {"text": "CW can optionally apply a minimum weight threshold which we optimized using the categorization dataset from.", "labels": [], "entities": []}, {"text": "The latter contains a classification of 82 nouns into 10 categories.", "labels": [], "entities": []}, {"text": "These nouns were excluded from the gold standard) in our final evaluation.", "labels": [], "entities": []}, {"text": "We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task; it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively.", "labels": [], "entities": [{"text": "F-score measure", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.973711371421814}, {"text": "SemEval 2007 task", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.6391372283299764}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9870647192001343}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9982958436012268}]}], "tableCaptions": [{"text": " Table 1: Examples of attribute-based representations provided as input to our autoencoders.", "labels": [], "entities": []}, {"text": " Table 2: Mean semantic and visual similarity rat- ings for the McRae et al. (2005) nouns using a  scale of 1 (highly dissimilar) to 5 (highly similar).", "labels": [], "entities": [{"text": "McRae et al. (2005) nouns", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.8829548433423042}]}, {"text": " Table 3: Correlation of model predictions against  similarity ratings for McRae et al. (2005) noun  pairs (using Spearman's \u03c1).", "labels": [], "entities": []}, {"text": " Table 5: F-score results on concept categorization.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9970601201057434}]}]}