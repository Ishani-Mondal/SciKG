{"title": [{"text": "Automatic Keyphrase Extraction: A Survey of the State of the Art", "labels": [], "entities": [{"text": "Automatic Keyphrase Extraction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6430625716845194}]}], "abstractContent": [{"text": "While automatic keyphrase extraction has been examined extensively, state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7284434288740158}]}, {"text": "We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.", "labels": [], "entities": [{"text": "automatic keyphrase extraction", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6589266359806061}]}], "introductionContent": [{"text": "Automatic keyphrase extraction concerns \"the automatic selection of important and topical phrases from the body of a document\").", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7310300320386887}, {"text": "automatic selection of important and topical phrases from the body of a document", "start_pos": 45, "end_pos": 125, "type": "TASK", "confidence": 0.6972096470686105}]}, {"text": "In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document.", "labels": [], "entities": []}, {"text": "Document keyphrases have enabled fast and accurate searching fora given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (), text categorization (), opinion mining, and document indexing ( ).", "labels": [], "entities": [{"text": "natural language processing (NLP) and information retrieval (IR)", "start_pos": 164, "end_pos": 228, "type": "TASK", "confidence": 0.6959460228681564}, {"text": "text summarization", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.7203937768936157}, {"text": "text categorization", "start_pos": 267, "end_pos": 286, "type": "TASK", "confidence": 0.7414880692958832}, {"text": "opinion mining", "start_pos": 291, "end_pos": 305, "type": "TASK", "confidence": 0.8263078033924103}, {"text": "document indexing", "start_pos": 311, "end_pos": 328, "type": "TASK", "confidence": 0.7229729890823364}]}, {"text": "Owing to its importance, automatic keyphrase extraction has received a lot of attention.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8005777597427368}]}, {"text": "However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.8345652520656586}]}, {"text": "Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8626835942268372}]}], "datasetContent": [{"text": "In this section, we describe metrics for evaluating keyphrase extraction systems as well as state-ofthe-art results on commonly-used datasets.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8207449615001678}]}, {"text": "Designing evaluation metrics for keyphrase extraction is by no means an easy task.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8776984512805939}]}, {"text": "To score the output of a keyphrase extraction system, the typical approach, which is also adopted by the SemEval-2010 shared task on keyphrase extraction, is (1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match, and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score (F).", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7549164295196533}, {"text": "keyphrase extraction", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.7396972626447678}, {"text": "precision (P)", "start_pos": 339, "end_pos": 352, "type": "METRIC", "confidence": 0.952223002910614}, {"text": "recall (R)", "start_pos": 354, "end_pos": 364, "type": "METRIC", "confidence": 0.9576861709356308}, {"text": "F-score (F)", "start_pos": 370, "end_pos": 381, "type": "METRIC", "confidence": 0.9587427377700806}]}, {"text": "Conceivably, exact match is an overly strict condition, considering a predicted keyphrase incorrect even if it is a variant of a gold keyphrase.", "labels": [], "entities": [{"text": "exact match", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9316842257976532}]}, {"text": "For instance, given the gold keyphrase \"neural network\", exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase (\"artificial neural network\") or one of its morphological (\"neural networks\") or lexical (\"neural net\") variants.", "labels": [], "entities": []}, {"text": "While morphological variations can be handled using a stemmer, other variations may not be handled easily and reliably.", "labels": [], "entities": []}, {"text": "Human evaluation has been suggested as a possibility (), but it is timeconsuming and expensive.", "labels": [], "entities": [{"text": "Human evaluation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6206028163433075}]}, {"text": "For this reason, researchers have experimented with two types of automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "The first type of metrics addresses the problem with exact match.", "labels": [], "entities": []}, {"text": "These metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e., overlapping n-grams) and are commonly used in machine translation (MT) and summarization evaluations.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.8312964081764221}, {"text": "summarization evaluations", "start_pos": 170, "end_pos": 195, "type": "TASK", "confidence": 0.8568151891231537}]}, {"text": "They include BLEU, METEOR, NIST, and ROUGE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9987408518791199}, {"text": "METEOR", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.938976526260376}, {"text": "NIST", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.4840448200702667}, {"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9764117002487183}]}, {"text": "Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses ().", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9796867370605469}]}, {"text": "The second type of metrics focuses on how a system ranks its predictions.", "labels": [], "entities": []}, {"text": "Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) ( will award more credit to A than to B if the ranks of the correct predictions in A's output are higher than those in B's output.", "labels": [], "entities": [{"text": "binary preference measure (Bpref)", "start_pos": 76, "end_pos": 109, "type": "METRIC", "confidence": 0.9205436110496521}, {"text": "mean reciprocal rank (MRR)", "start_pos": 114, "end_pos": 140, "type": "METRIC", "confidence": 0.9413505295912424}]}, {"text": "R-precision (R p ) is an IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates (.", "labels": [], "entities": [{"text": "R-precision (R p )", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9068606495857239}, {"text": "IR", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9843850135803223}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9881618618965149}]}, {"text": "The motivation behind the design of R p is simple: a system will achieve a perfect R p value if it ranks all the keyphrases above the non-keyphrases.", "labels": [], "entities": []}, {"text": "lists the best scores on some popular evaluation datasets and the corresponding systems.", "labels": [], "entities": []}, {"text": "For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9941802024841309}, {"text": "Inspec test set", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9757398168245951}, {"text": "DUC-2001 dataset", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.9896558821201324}, {"text": "SemEval-2010 test set", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.897797962029775}]}, {"text": "First, F-scores decrease as document length increases.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9984557628631592}]}, {"text": "These results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents.", "labels": [], "entities": []}, {"text": "Second, recent unsupervised approaches have rivaled their supervised counterparts in performance).", "labels": [], "entities": []}, {"text": "For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5.", "labels": [], "entities": [{"text": "SemEval-2010 shared task", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7524830301602682}, {"text": "F-score", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9988793730735779}]}], "tableCaptions": [{"text": " Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk ( * ).", "labels": [], "entities": []}, {"text": " Table 2: Best scores achieved on various datasets.", "labels": [], "entities": []}]}