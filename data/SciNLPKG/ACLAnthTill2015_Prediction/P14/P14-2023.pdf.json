{"title": [{"text": "Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models", "labels": [], "entities": [{"text": "Decoder Integration", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8016656041145325}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.8642271757125854}]}], "abstractContent": [{"text": "Neural network language models are often trained by optimizing likelihood, but we would prefer to optimize fora task specific metric, such as BLEU in machine translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9979621171951294}, {"text": "machine translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.690617099404335}]}, {"text": "We show how a recurrent neural network language model can be optimized towards an expected BLEU loss instead of the usual cross-entropy criterion.", "labels": [], "entities": [{"text": "BLEU loss", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9792830646038055}]}, {"text": "Furthermore , we tackle the issue of directly integrating a recurrent network into first-pass decoding under an efficient approximation.", "labels": [], "entities": []}, {"text": "Our best results improve a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the expected BLEU objective improves over a cross-entropy trained model by up to 0.6 BLEU in a single reference setup.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 27, "end_pos": 71, "type": "TASK", "confidence": 0.56147750467062}, {"text": "WMT 2012 French-English data", "start_pos": 90, "end_pos": 118, "type": "DATASET", "confidence": 0.954804077744484}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9967877864837646}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9984194040298462}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9873237013816833}]}], "introductionContent": [{"text": "Neural network-based language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9985021352767944}, {"text": "statistical machine translation", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.6453841825326284}]}, {"text": "In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling () with several subsequent applications in machine translation ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7299741506576538}, {"text": "machine translation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.7799021601676941}]}, {"text": "Recurrent models have the potential to capture long-span dependencies since their predictions are based on an unbounded history of previous words ( \u00a72).", "labels": [], "entities": []}, {"text": "In practice, neural network models for machine translation are usually trained by maximizing the likelihood of the training data, either via a crossentropy objective) or more recently, noise-contrastive estimation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7969367802143097}]}, {"text": "However, it is widely appreciated that directly optimizing fora task-specific metric often leads to better performance).", "labels": [], "entities": []}, {"text": "The expected BLEU objective provides an efficient way of achieving this for machine translation () instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation ( \u00a73).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.994769275188446}, {"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8367998003959656}, {"text": "Minimum Error Rate Training (MERT)", "start_pos": 159, "end_pos": 193, "type": "METRIC", "confidence": 0.7889699254717145}, {"text": "machine translation", "start_pos": 287, "end_pos": 306, "type": "TASK", "confidence": 0.8079335391521454}]}, {"text": "Most previous work on neural networks for machine translation is based on a rescoring setup (, thereby sidestepping the algorithmic and engineering challenges of direct decoder-integration.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7931457459926605}]}, {"text": "One recent exception is who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring.", "labels": [], "entities": []}, {"text": "Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice.", "labels": [], "entities": [{"text": "Decoder integration", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.832029789686203}]}, {"text": "Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language models.", "labels": [], "entities": []}, {"text": "However, for recurrent networks we have to deal with the unbounded history, which breaks the usual dynamic programming assumptions for efficient search.", "labels": [], "entities": []}, {"text": "We show how a simple but effective approximation can sidestep this issue and we empirically demonstrate its effectiveness ( \u00a74).", "labels": [], "entities": []}, {"text": "We test the expected BLEU objective by training a recurrent neural network language model and obtain substantial improvements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9961918592453003}]}, {"text": "We also find that our efficient approximation for decoder integration is very accurate, clearly outperforming a rescoring setup ( \u00a75).", "labels": [], "entities": [{"text": "decoder integration", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8785451948642731}]}], "datasetContent": [{"text": "We use a phrase-based system similar to) based on a set of common features including maximum likelihood estimates p M L (e|f ) and p M L (f |e), lexically weighted estimates p LW (e|f ) and p LW (f |e), word and phrase-penalties, a hierarchical reordering model (, a linear distortion feature, and a modified Kneser-Ney language model trained on the target-side of the parallel data.", "labels": [], "entities": []}, {"text": "Log-linear weights are tuned with MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.917770504951477}]}, {"text": "We use training and test data from the WMT 2012 campaign and report results on French-English and German-English.", "labels": [], "entities": [{"text": "WMT 2012 campaign", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.72116619348526}]}, {"text": "Translation models are estimated on 102M words of parallel data for French-English, and 99M words for German-English; about 6.5M words for each language pair are newswire, the remainder are parliamentary proceedings.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9543741345405579}]}, {"text": "We evaluate on six newswire domain test sets from 2008 to 2013 containing between 2034 to 3003 sentences.", "labels": [], "entities": [{"text": "newswire domain test sets", "start_pos": 19, "end_pos": 44, "type": "DATASET", "confidence": 0.7955679297447205}]}, {"text": "Loglinear weights are estimated on the 2009 data set comprising 2525 sentences.", "labels": [], "entities": [{"text": "Loglinear weights", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.7734678089618683}, {"text": "2009 data set", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.8792335192362467}]}, {"text": "We evaluate accuracy in terms of BLEU with a single reference.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9994592070579529}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9988002777099609}]}, {"text": "For rescoring we use ei- Recombination only retains the highest scoring state if there are multiple identical states, that is, they cover the same source span, the same translation phrase and contexts.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9639456868171692}]}, {"text": "ther lattices or the unique 100-best output of the phrase-based decoder and re-estimate the loglinear weights by running a further iteration of MERT on the n-best list of the development set, augmented by scores corresponding to the neural network models.", "labels": [], "entities": []}, {"text": "At test time we rescore n-best lists with the new weights.", "labels": [], "entities": []}, {"text": "All neural network models are trained on the news portion of the parallel data, corresponding to 136K sentences, which we found to be most useful in initial experiments.", "labels": [], "entities": []}, {"text": "As training data we use unique 100-best lists generated by the baseline system.", "labels": [], "entities": []}, {"text": "We use the same data both for training the phrase-based system as well as the language model but find that the resulting bias did not hurt end-to-end accuracy ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.948945939540863}]}, {"text": "The vocabulary consists of words that occur in at least two different sentences, which is 31K words for both language pairs.", "labels": [], "entities": []}, {"text": "We tuned the learning rate \u00b5 of our mini-batch SGD trainer as well as the probability scaling parameter \u03b3 (3) on a held-out set and found simple settings of \u00b5 = 0.1 and \u03b3 = 1 to be good choices.", "labels": [], "entities": [{"text": "learning rate \u00b5", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.8445918560028076}, {"text": "probability scaling parameter \u03b3 (3)", "start_pos": 74, "end_pos": 109, "type": "METRIC", "confidence": 0.7616146206855774}]}, {"text": "To prevent over-fitting, we experimented with L2 regularization, but found no accuracy improvements, probably because SGD regularizes enough.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9991735816001892}]}, {"text": "We evaluate performance on a held-out set during training and stop whenever the objective changes less than 0.0003.", "labels": [], "entities": []}, {"text": "The hidden layer uses 100 neurons unless otherwise stated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: French-English accuracy of decoder integration of a recurrent neural network language model  (RNN decode) compared to n-best and lattice rescoring as well as the output of a phrase-based system  using an n-gram model (Baseline); Alltest is the corpus-weighted average BLEU across all test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9892622828483582}, {"text": "Alltest", "start_pos": 239, "end_pos": 246, "type": "METRIC", "confidence": 0.9915511608123779}, {"text": "BLEU", "start_pos": 278, "end_pos": 282, "type": "METRIC", "confidence": 0.9554188251495361}]}, {"text": " Table 2: German-English results of direct decoder integration (cf. Table 1).", "labels": [], "entities": []}, {"text": " Table 3: French-English accuracy of a decoder integrated cross-entropy recurrent neural network model  (CE RNN) and a combination with an expected BLEU trained model (xBLEU RNN). Results are not  comparable to Table 1 since a smaller hidden layer was used to keep training times manageable ( \u00a75.2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.99402916431427}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9963078498840332}]}]}