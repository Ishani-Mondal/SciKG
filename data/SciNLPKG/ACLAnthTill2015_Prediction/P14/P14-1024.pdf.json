{"title": [{"text": "Metaphor Detection with Cross-Lingual Model Transfer", "labels": [], "entities": [{"text": "Metaphor Detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8611971437931061}]}], "abstractContent": [{"text": "We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction.", "labels": [], "entities": []}, {"text": "Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language.", "labels": [], "entities": []}, {"text": "Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages.", "labels": [], "entities": []}, {"text": "We provide results on three new test sets in Span-ish, Farsi, and Russian.", "labels": [], "entities": []}, {"text": "The results support the hypothesis that metaphors are conceptual, rather than lexical, in nature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lakoff and Johnson (1980) characterize metaphor as reasoning about one thing in terms of another, i.e., a metaphor is a type of conceptual mapping, where words or phrases are applied to objects and actions in ways that do not permit a literal interpretation.", "labels": [], "entities": []}, {"text": "They argue that metaphors play a fundamental communicative role in verbal and written interactions, claiming that much of our everyday language is delivered in metaphorical terms.", "labels": [], "entities": []}, {"text": "There is empirical evidence supporting the claim: recent corpus studies have estimated that the proportion of words used metaphorically ranges from 5% to 20%, and Thibodeau and Boroditsky (2011) provide evidence that a choice of metaphors affects decision making.", "labels": [], "entities": [{"text": "decision making", "start_pos": 247, "end_pos": 262, "type": "TASK", "confidence": 0.8826095163822174}]}, {"text": "Given the prevalence and importance of metaphoric language, effective automatic detection of metaphors would have a number of benefits, both practical and scientific.", "labels": [], "entities": [{"text": "automatic detection of metaphors", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.7528500854969025}]}, {"text": "Language processing applications that need to understand language or preserve meaning (information extraction, machine translation, dialog systems, sentiment analysis, and text analytics, etc.) would have access to a potentially useful high-level bit of information about whether something is to be understood literally or not.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.761695384979248}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7498087286949158}, {"text": "sentiment analysis", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.935910165309906}, {"text": "text analytics", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.6851329952478409}]}, {"text": "Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation.", "labels": [], "entities": []}, {"text": "However, metaphor detection is a hard problem.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9745424389839172}]}, {"text": "On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language.", "labels": [], "entities": []}, {"text": "On the other, metaphors can be domain-and contextdependent.", "labels": [], "entities": []}, {"text": "Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources;) and corpus-based approaches.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.9143052399158478}]}, {"text": "We build on this foundation and also extend metaphor detection into other languages in which few resources may exist.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9782146215438843}]}, {"text": "Our work makes the following contributions: (1) we develop anew state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses; 2 (2) we create new metaphor-annotated corpora for Russian and English; 3 (3) using a paradigm of model transfer, we provide support for the hypothesis that metaphors are concep-tual (rather than lexical) in nature by showing that our English-trained model can detect metaphors in Spanish, Farsi, and Russian.", "labels": [], "entities": [{"text": "English metaphor detection", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.6865558524926504}]}], "datasetContent": [{"text": "In this section we describe a training and testing dataset as well a data collection procedure.", "labels": [], "entities": []}, {"text": "Our task, as defined in Section 2, is to classify SVO and AN relations as either metaphoric or literal.", "labels": [], "entities": []}, {"text": "We first conduct a 10-fold cross-validation experiment on the training set defined in Section 4.1.", "labels": [], "entities": []}, {"text": "We represent each candidate relation using the features described in Section 3.2, and evaluate performance of the three feature categories and their combinations.", "labels": [], "entities": []}, {"text": "This is done by computing an accuracy in the 10-fold cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9995868802070618}]}, {"text": "Experimental results are given in, where we also provide the number of features in each feature set.", "labels": [], "entities": []}, {"text": "These results show superior performance over previous state-of-the-art results, confirming our hypothesis that conceptual features are effective in metaphor classification.", "labels": [], "entities": [{"text": "metaphor classification", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.9124757945537567}]}, {"text": "For the SVO task, the cross-validation accuracy is about 10% better than that of.", "labels": [], "entities": [{"text": "SVO task", "start_pos": 8, "end_pos": 16, "type": "TASK", "confidence": 0.558517724275589}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.979079008102417}]}, {"text": "For the AN task, the cross validation accuracy is better by 8% than the result of Turney et al.", "labels": [], "entities": [{"text": "AN task", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9249123334884644}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9785879850387573}]}, {"text": "(2011) (two baseline methods are described in Section 5.2).", "labels": [], "entities": []}, {"text": "We can see that all types of features have good performance on their own (VSM is the strongest feature type).", "labels": [], "entities": [{"text": "VSM", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.45856115221977234}]}, {"text": "Noun supersense features alone allows us to achieve an accuracy of 75%, i.e., adjective supersense features contribute 4% to adjective-noun supersense feature combination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9993354678153992}]}, {"text": "Experiments with the pairs of features yield better results than individual features, implying that the feature categories are not redundant.", "labels": [], "entities": []}, {"text": "Yet, combining all features leads to even higher accuracy during crossvalidation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9986818432807922}]}, {"text": "In the case of the AN task, a difference between the All feature combination and any other combination of features listed in is statistically significant (p < 0.01 for both the sign and the permutation test).", "labels": [], "entities": [{"text": "AN task", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.8811197578907013}]}, {"text": "Although the first experiment shows very high scores, the 10-fold cross-validation cannot fully reflect the generality of the model, because all folds are parts of the same corpus.", "labels": [], "entities": []}, {"text": "They are collected by the same human judges and belong to the same domain.", "labels": [], "entities": []}, {"text": "Therefore, experiments on out-ofdomain data are crucial.", "labels": [], "entities": []}, {"text": "We carryout such experiments using held-out SVO and AN EN test sets, described in Section 4.2 and.", "labels": [], "entities": [{"text": "SVO", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8556801080703735}, {"text": "AN EN test sets", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.7937695533037186}]}, {"text": "In this experiment, we measure the f -score.", "labels": [], "entities": [{"text": "f -score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.988610029220581}]}, {"text": "We classify SVO and AN relations using a classifier trained on the All feature combination and balanced thresholds.", "labels": [], "entities": []}, {"text": "The values of the f -score are 0.76, both for SVO and AN tasks.", "labels": [], "entities": [{"text": "f -score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9855187932650248}, {"text": "AN", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.6756247878074646}]}, {"text": "This out-of-domain experiment suggests that our classifier is portable across domains and genres.", "labels": [], "entities": []}, {"text": "However, (1) different application may have different requirements for recall/precision, and (2) classification results maybe skewed towards having high precision and low recall (or vice versa).", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9987574815750122}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9555187225341797}, {"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9941237568855286}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9973748922348022}]}, {"text": "It is possible to trade precision for recall by choosing a different threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.999315619468689}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9987371563911438}]}, {"text": "Thus, in addition to giving a single f -score value for balanced thresholds, we present a Receiver Operator Characteristic (ROC) curve, where we plot a fraction of true positives against the fraction of false positives for 100 threshold values in the range from zero to one.", "labels": [], "entities": []}, {"text": "The area under the ROC curve (AUC) can be interpreted as the probability that a classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example.", "labels": [], "entities": [{"text": "ROC curve (AUC)", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.9455438852310181}]}, {"text": "For a randomly guessing classifier, the ROC curve is a dashed diagonal line.", "labels": [], "entities": [{"text": "ROC", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9151983261108398}]}, {"text": "A bad classi- Assuming that positive examples are labeled by ones, and negative examples are labeled by zeros.", "labels": [], "entities": []}, {"text": "fier has an ROC curve that goes close to the dashed diagonal or even below it.", "labels": [], "entities": [{"text": "ROC", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9872176051139832}]}, {"text": "According to ROC plots in, all three feature sets are effective, both for SVO and for AN tasks.", "labels": [], "entities": [{"text": "AN tasks", "start_pos": 86, "end_pos": 94, "type": "TASK", "confidence": 0.9135535955429077}]}, {"text": "Abstractness and Imageability features work better for adjectives and nouns, which is inline with previous findings.", "labels": [], "entities": [{"text": "Abstractness", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9788545370101929}]}, {"text": "It can be also seen that VSM features are very effective.", "labels": [], "entities": [{"text": "VSM", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.81089186668396}]}, {"text": "This is inline with results of, who found that it is hard to improve over the classifier that uses only VSM features.", "labels": [], "entities": []}, {"text": "In the next experiment we corroborate the main hypothesis of this paper: a model trained on En-  Experimental results for all four languages, are given in.", "labels": [], "entities": []}, {"text": "The ROC curves for SVO and AN tasks are plotted in and, respectively.", "labels": [], "entities": [{"text": "ROC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9803978800773621}]}, {"text": "Each curve corresponds to a test set described in.", "labels": [], "entities": []}, {"text": "In addition, we perform an oracle experiment, to obtain actual f -score values for best thresholds.", "labels": [], "entities": [{"text": "f -score", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.8207334081331888}]}, {"text": "Detailed results are shown in.", "labels": [], "entities": [{"text": "Detailed", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.6172933578491211}]}, {"text": "Consistent results with high f -scores are obtained across all four languages.", "labels": [], "entities": [{"text": "f -scores", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9542787472407023}]}, {"text": "Note that higher scores are obtained for the Russian test set.", "labels": [], "entities": [{"text": "Russian test set", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.958000918229421}]}, {"text": "We hypothesize that this happens due to a higher-quality translation dictionary (which allows a more accurate model transfer).", "labels": [], "entities": []}, {"text": "Relatively lower (yet reasonable) results for Farsi can be explained by a smaller size of the bilingual dictionary (thus, fewer feature projections can be obtained).", "labels": [], "entities": []}, {"text": "Also note that, in our experience, most of Farsi metaphors are adjective-noun constructions.", "labels": [], "entities": []}, {"text": "This is why the AN FA dataset in is significantly larger than SVO FA.", "labels": [], "entities": [{"text": "AN FA dataset", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.7454013427098592}]}, {"text": "In that, for the AN Farsi task we observe high performance scores. and confirm, that we obtain similar, robust results on four very different languages, using the same English classifiers.", "labels": [], "entities": [{"text": "AN Farsi task", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.5842229425907135}]}, {"text": "We view this result as a strong evidence of language-independent nature of our metaphor detection method.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8614104986190796}]}, {"text": "In particular, this shows that proposed conceptual features can be used to detect selectional preferences violation across languages.", "labels": [], "entities": [{"text": "selectional preferences violation", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.6173165440559387}]}, {"text": "To summarize the experimental section, our metaphor detection approach obtains state-of-the- art performance in English, is effective when applied to out-of-domain English data, and works cross-lingually.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8741188943386078}]}], "tableCaptions": [{"text": " Table 1: Sizes of the eight test sets. Each dataset is  balanced, i.e., it has an equal number of metaphors  and non-metaphors. For example, English SVO  dataset has 222 relations: 111 metaphoric and 111  literal.", "labels": [], "entities": [{"text": "English SVO  dataset", "start_pos": 142, "end_pos": 162, "type": "DATASET", "confidence": 0.703493614991506}]}, {"text": " Table 2: 10-fold cross validation results for three  feature categories and their combination, for clas- sifiers trained on English SVO and AN training  sets. # FEAT column shows a number of features.  ACC column reports an accuracy score in the 10- fold cross validation. Statistically significant dif- ferences (p < 0.01) from the all-feature combina- tion are marked with a star.", "labels": [], "entities": [{"text": "English SVO and AN training  sets", "start_pos": 125, "end_pos": 158, "type": "DATASET", "confidence": 0.6045352816581726}, {"text": "FEAT", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9977260231971741}, {"text": "ACC", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.6044374108314514}, {"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9992238283157349}]}, {"text": " Table 4: Comparing AN metaphor detection  method to the baselines: accuracy of the 10- fold cross validation on annotations of five human  judges.", "labels": [], "entities": [{"text": "AN metaphor detection", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.9271959066390991}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9996823072433472}]}, {"text": " Table 5: Cross-lingual experiment: f -scores for  classifiers trained on the English data using a com- bination of all features, and applied, with optimal  thresholds, to SVO and AN metaphoric and literal  relations in four test languages: English, Russian,  Spanish, and Farsi.", "labels": [], "entities": []}]}