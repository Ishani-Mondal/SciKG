{"title": [{"text": "Approximation Strategies for Multi-Structure Sentence Compression", "labels": [], "entities": [{"text": "Approximation", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9693958759307861}, {"text": "Multi-Structure Sentence Compression", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.650303453207016}]}], "abstractContent": [{"text": "Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9384116232395172}]}, {"text": "We explore instead the use of Lagrangian relaxation to decou-ple the two subproblems and solve them separately.", "labels": [], "entities": []}, {"text": "While dynamic programming is viable for bigram-based sentence compression , finding optimal compressed trees within graphs is NP-hard.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7064926028251648}]}, {"text": "We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms, yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers.", "labels": [], "entities": []}, {"text": "Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression is a text-to-text generation task in which an input sentence must be transformed into a shorter output sentence which accurately reflects the meaning in the input and also remains grammatically well-formed.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9187361896038055}, {"text": "text-to-text generation task", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7993219296137491}]}, {"text": "The compression task has received increasing attention in recent years, in part due to the availability of datasets such as the Ziff-Davis corpus) and the Edinburgh compression corpora), from which the following example is drawn.", "labels": [], "entities": [{"text": "Edinburgh compression corpora", "start_pos": 155, "end_pos": 184, "type": "DATASET", "confidence": 0.9397932489713033}]}, {"text": "Chapman, who had cultivated a conventional image with his ubiquitous tweed jacket and pipe, by his own later admission stunned a party attended by his friends and future Python colleagues by coming out as a homosexual.", "labels": [], "entities": []}, {"text": "Compressed: In 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual.", "labels": [], "entities": []}, {"text": "Following an assumption often used in compression systems, the compressed output in this corpus is constructed by dropping tokens from the input sentence without any paraphrasing or reordering.", "labels": [], "entities": []}, {"text": "A number of diverse approaches have been proposed for deletion-based sentence compression, including techniques that assemble the output text under an n-gram factorization over the input text or an arc factorization over input dependency parses.", "labels": [], "entities": [{"text": "deletion-based sentence compression", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.8045686483383179}]}, {"text": "Joint methods have also been proposed that invoke integer linear programming (ILP) formulations to simultaneously consider multiple structural inference problems-both over n-grams and input dependencies  or n-grams and all possible dependencies.", "labels": [], "entities": []}, {"text": "However, it is wellestablished that the utility of ILP for optimal inference in structured problems is often outweighed by the worst-case performance of ILP solvers on large problems without unique integral solutions.", "labels": [], "entities": []}, {"text": "Furthermore, approximate solutions can often be adequate for real-world generation systems, particularly in the presence of linguisticallymotivated constraints such as those described by, or domain-specific pruning strategies such as the use of sentence templates to constrain the output.", "labels": [], "entities": []}, {"text": "In this work, we develop approximate inference strategies to the joint approach of which trade the optimality guarantees of exact ILP for faster inference by separately solving the n-gram and dependency subproblems and using Lagrange multipliers to enforce consistency between their solutions.", "labels": [], "entities": []}, {"text": "However, while the former problem can be solved efficiently using the dynamic programming approach of, there are no efficient algorithms to recover maximum weighted nonprojective subtrees in a general directed graph.", "labels": [], "entities": []}, {"text": "Maximum spanning tree algorithms, commonly used in non-projective dependency parsing), are not easily adaptable to this task since the maximum-weight subtree is not necessarily apart of the maximum spanning tree.", "labels": [], "entities": []}, {"text": "We therefore consider methods to recover approximate solutions for the subproblem of finding the maximum weighted subtree in a graph, common among which is the use of a linear programming relaxation.", "labels": [], "entities": []}, {"text": "This linear program (LP) appears empirically tight for compression problems and our experiments indicate that simply using the non-integral solutions of this LP in Lagrangian relaxation can empirically lead to reasonable compressions.", "labels": [], "entities": []}, {"text": "In addition, we can recover approximate solutions to this problem by using the ChuLiu Edmonds algorithm for recovering maximum spanning trees ( over the relatively sparse subgraph defined by a solution to the relaxed LP.", "labels": [], "entities": []}, {"text": "Our proposed approximation strategies are evaluated using automated metrics in order to address the question: under what conditions should a real-world sentence compression system implementation consider exact inference with an ILP or approximate inference?", "labels": [], "entities": []}, {"text": "The contributions of this work include: \u2022 An empirically-useful technique for approximating the maximum-weight subtree in a weighted graph using LP-relaxed inference.", "labels": [], "entities": []}, {"text": "\u2022 Multiple approaches to generate good approximate solutions for joint multi-structure compression, based on Lagrangian relaxation to enforce equality between the sequential and syntactic inference subproblems.", "labels": [], "entities": [{"text": "joint multi-structure compression", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.6295342346032461}]}, {"text": "\u2022 An analysis of the tradeoffs incurred by joint approaches with regard to runtime as well as performance under automated measures.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran compression experiments over the newswire (NW) and broadcast news transcription (BN) corpora compiled by which contain gold compressions produced by human annotators using only word deletion.", "labels": [], "entities": [{"text": "newswire (NW) and broadcast news transcription (BN) corpora compiled", "start_pos": 40, "end_pos": 108, "type": "DATASET", "confidence": 0.6833680822299077}]}, {"text": "The datasets were filtered to eliminate instances with less than 2 and more than 110 tokens for parser compatibility and divided into training/development/test sections following the splits from, yielding 953/63/603 instances for the NW corpus and 880/78/404 for the BN corpus.", "labels": [], "entities": [{"text": "NW corpus", "start_pos": 234, "end_pos": 243, "type": "DATASET", "confidence": 0.9329247772693634}, {"text": "BN corpus", "start_pos": 267, "end_pos": 276, "type": "DATASET", "confidence": 0.9795236587524414}]}, {"text": "Gold dependency parses were approximated by running the Stanford dependency parser 7 over reference compressions.", "labels": [], "entities": [{"text": "Gold dependency parses", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.4529146154721578}]}, {"text": "Following evaluations in machine translation as well as previous work in sentence compression (, we evaluate system performance using F 1 metrics over n-grams and dependency edges produced by parsing system output with RASP () and the Stanford parser.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7984355390071869}, {"text": "sentence compression", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7645113170146942}, {"text": "F 1 metrics", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.8633497953414917}]}, {"text": "All ILPs and LPs were solved using Gurobi, 8 a high-performance commercialgrade solver.", "labels": [], "entities": []}, {"text": "Following a recent analysis of compression evaluations () which revealed a strong correlation between system compression rate and human judgments of compression quality, we constrained all systems to produce compressed output at a specific rate-determined by the the gold compressions available for each instance-to ensure that the reported differences between the systems understudy are meaningful.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results for the BN corpus, averaged over 3 gold compressions per instance. All  systems were restricted to compress to the size of the median gold compression yielding an average  compression rate of 77.26%.", "labels": [], "entities": [{"text": "BN corpus", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.9663177728652954}]}, {"text": " Table 2: Experimental results for the NW corpus with all systems compressing to the size of the gold  compression, yielding an average compression rate of 70.24%. In both tables, bold entries show signifi- cant gains within a column under the paired t-test (p < 0.05) and Wilcoxon's signed rank test (p < 0.01).", "labels": [], "entities": [{"text": "NW corpus", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.8763676881790161}, {"text": "Wilcoxon's signed rank test", "start_pos": 273, "end_pos": 300, "type": "METRIC", "confidence": 0.6048690736293793}]}]}