{"title": [{"text": "An Empirical Study on the Effect of Negation Words on Sentiment", "labels": [], "entities": [{"text": "Sentiment", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.862523078918457}]}], "abstractContent": [{"text": "Negation words, such as no and not, play a fundamental role in modifying sentiment of textual expressions.", "labels": [], "entities": []}, {"text": "We will refer to a negation word as the negator and the text span within the scope of the negator as the argument.", "labels": [], "entities": []}, {"text": "Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument (and not on the negator or the argument itself).", "labels": [], "entities": []}, {"text": "We use a sentiment tree-bank to show that these existing heuristics are poor estimators of sentiment.", "labels": [], "entities": []}, {"text": "We then modify these heuristics to be dependent on the negators and show that this improves prediction.", "labels": [], "entities": []}, {"text": "Next, we evaluate a recently proposed composition model (Socher et al., 2013) that relies on both the negator and the argument.", "labels": [], "entities": []}, {"text": "This model learns the syntax and semantics of the negator's argument with a recursive neural network.", "labels": [], "entities": []}, {"text": "We show that this approach performs better than those mentioned above.", "labels": [], "entities": []}, {"text": "In addition , we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors .", "labels": [], "entities": []}], "introductionContent": [{"text": "Morante and Sporleder (2012) define negation to be \"a grammatical category that allows the changing of the truth value of a proposition\".", "labels": [], "entities": []}, {"text": "Negation is often expressed through the use of negative signals or negators-words like isn't and never, and it can significantly affect the sentiment of its scope.", "labels": [], "entities": [{"text": "Negation", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9714320302009583}]}, {"text": "Understanding the impact of negation on sentiment is essential in automatic analysis of sentiment.", "labels": [], "entities": [{"text": "automatic analysis of sentiment", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.7537085860967636}]}, {"text": "The literature contains interesting research attempting to model and understand the behavior (reviewed in Section 2).", "labels": [], "entities": []}, {"text": "For example, a simple yet influential hypothesis posits that a negator reverses the sign of the sentiment value of the modified text ().", "labels": [], "entities": []}, {"text": "The shifting hypothesis), however, assumes that negators change sentiment values by a constant amount.", "labels": [], "entities": []}, {"text": "In this paper, we refer to a negation word as the negator (e.g., isn't), a text span being modified by and composed with a negator as the argument (e.g., very good), and entire phrase (e.g., isn't very good) as the negated phrase.", "labels": [], "entities": []}, {"text": "The recently available Stanford Sentiment Treebank () renders manually annotated, real-valued sentiment scores for all phrases in parse trees.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 23, "end_pos": 50, "type": "DATASET", "confidence": 0.8637668887774149}]}, {"text": "This corpus provides us with the data to further understand the quantitative behavior of negators, as the effect of negators can now be studied with arguments of rich syntactic and semantic variety.", "labels": [], "entities": []}, {"text": "illustrates the effect of a common list of negators on sentiment as observed on the Stanford Sentiment Treebank.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 84, "end_pos": 111, "type": "DATASET", "confidence": 0.912044902642568}]}, {"text": "Each dot in the figure corresponds to a negated phrase in the treebank.", "labels": [], "entities": []}, {"text": "The x-axis is the sentiment score of its argument s( w) and y-axis the sentiment score of the entire negated phrase s(w n , w).", "labels": [], "entities": []}, {"text": "We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather roughly.", "labels": [], "entities": []}, {"text": "Moreover, the figure shows that same or similar s( w) scores (x-axis) can correspond to very different s(w n , w) scores (y-axis), which, to some degree, suggests the potentially complicated behavior of negators.", "labels": [], "entities": []}, {"text": "This paper describes a quantitative study of the effect of a list of frequent negators on sentiment.", "labels": [], "entities": []}, {"text": "We regard the negators' behavior as an underlying function embedded in annotated data; we aim to model this function from different aspects.", "labels": [], "entities": []}, {"text": "By examining sentiment compositions of negators and arguments, we model the quantitative behavior of negators in changing sentiment.", "labels": [], "entities": []}, {"text": "That is, given a negated phrase (e.g., isn't very good) and the sentiment score of its argument (e.g., s(\"very good \u2032\u2032 ) = 0.5), we focus on understanding the negator's quantitative behavior in yielding the sentiment score of the negated phrase s(\"isn \u2032 t very good \u2032\u2032 ).", "labels": [], "entities": []}, {"text": "We first evaluate the modeling capabilities of two influential heuristics and show that they capture only very limited regularity of negators' effect.", "labels": [], "entities": []}, {"text": "We then extend the models to be dependent on the negators and demonstrate that such a simple extension can significantly improve the performance of fitting to the human annotated data.", "labels": [], "entities": []}, {"text": "Next, we evaluate a recently proposed composition model) that relies on both the negator and the argument.", "labels": [], "entities": []}, {"text": "This model learns the syntax and semantics of the negator's argument with a recursive neural network.", "labels": [], "entities": []}, {"text": "This approach performs significantly better than those mentioned above.", "labels": [], "entities": []}, {"text": "In addition, we explicitly incorporate the prior sentiment of the argument and observe that this information helps reduce fitting errors.", "labels": [], "entities": []}, {"text": "The sentiment values have been linearly rescaled from the original range to; in the figure a negative or positive value corresponds to a negative or a positive sentiment respectively; zero means neutral.", "labels": [], "entities": []}, {"text": "The negator list will be discussed later in the paper.", "labels": [], "entities": []}, {"text": "Similar distribution is observed in other data such as Tweets ().", "labels": [], "entities": [{"text": "Tweets", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8968493938446045}]}], "datasetContent": [{"text": "Data As described earlier, the Stanford Sentiment Treebank () has manually annotated, real-valued sentiment values for all phrases in parse trees.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 31, "end_pos": 58, "type": "DATASET", "confidence": 0.8840177853902181}]}, {"text": "This provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.", "labels": [], "entities": []}, {"text": "The data contain around 11,800 sentences from movie reviews that were originally collected by.", "labels": [], "entities": []}, {"text": "The sentences were parsed with the Stanford parser (.", "labels": [], "entities": []}, {"text": "The phrases at all tree nodes were manually annotated with one of 25 sentiment values that uniformly span between the positive and negative poles.", "labels": [], "entities": []}, {"text": "The values are normalized to the range of.", "labels": [], "entities": []}, {"text": "In this paper, we use a list of most frequent negators that include the words not, no, never, and their combinations with auxiliaries (e.g., didn't).", "labels": [], "entities": []}, {"text": "Each occurrence of a negator and the phrase it is directly composed within the treebank, i.e., w n , w, is considered a data point in our study.", "labels": [], "entities": []}, {"text": "In total, we collected 2,261 pairs, including 1,845 training and 416 test cases.", "labels": [], "entities": []}, {"text": "The split of training and test data is same as specified in.", "labels": [], "entities": []}, {"text": "Evaluation metrics We use the mean absolute error (MAE) to evaluate the models, which measures the averaged absolute offsets between the predicted sentiment values and the gold standard.", "labels": [], "entities": [{"text": "mean absolute error (MAE)", "start_pos": 30, "end_pos": 55, "type": "METRIC", "confidence": 0.9435405731201172}]}, {"text": "More specifically, MAE is calculated as: wher\u00ea s(w n , w) denotes the gold sentiment value and s(w n , w) the predicted one for the pair w n , w, and N is the total number of test instances.", "labels": [], "entities": [{"text": "MAE", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9111517667770386}]}, {"text": "Note that mean square error (MSE) is another widely used measure for regression, but it is less intuitive for out task here.", "labels": [], "entities": [{"text": "mean square error (MSE)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.9549339512983958}]}, {"text": "Overall regression performance shows the overall fitting performance of all models.", "labels": [], "entities": []}, {"text": "The first row of the table is a random baseline, which simply guesses the sentiment value for each test case randomly in the range.", "labels": [], "entities": []}, {"text": "The table shows that the basic reversing and shifting heuristics do capture negators' behavior to some degree, as their MAE scores are higher than that of the baseline.", "labels": [], "entities": [{"text": "reversing and shifting heuristics", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.779541552066803}, {"text": "MAE", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9483042359352112}]}, {"text": "Making the basic shifting model to be dependent on the negators (model 4) reduces the prediction error significantly as compared with the error of the basic shifting (model 3).", "labels": [], "entities": [{"text": "prediction error", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9054388403892517}]}, {"text": "The same is true for the polarity-based shifting (model 5), reflecting that the roles of negators are different when modifying positive and negative phrases.", "labels": [], "entities": []}, {"text": "Merging these two models yields additional improvement (model 6).", "labels": [], "entities": []}, {"text": "Models marked with an asterisk (*) are statistically significantly better than the random baseline.", "labels": [], "entities": []}, {"text": "Models with a dagger sign ( \u2020) significantly outperform model (3).", "labels": [], "entities": []}, {"text": "Double asterisks ** indicates a statistically significantly different from model (6), and the model with the double dagger \u2020 \u2020is significantly better than model (7).", "labels": [], "entities": [{"text": "double dagger \u2020 \u2020", "start_pos": 109, "end_pos": 126, "type": "METRIC", "confidence": 0.8301879912614822}]}, {"text": "One-tailed paired t-test with a 95% significance level is used here.", "labels": [], "entities": [{"text": "significance level", "start_pos": 36, "end_pos": 54, "type": "METRIC", "confidence": 0.9562350511550903}]}], "tableCaptions": []}