{"title": [{"text": "Discourse Complements Lexical Semantics for Non-factoid Answer Reranking", "labels": [], "entities": [{"text": "Answer Reranking", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.8682320713996887}]}], "abstractContent": [{"text": "We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse: a shallow representation centered around discourse markers, and a deep one based on Rhetorical Structure Theory.", "labels": [], "entities": []}, {"text": "We evaluate the proposed model on two corpora from different genres and domains: one from Yahoo!", "labels": [], "entities": []}, {"text": "Answers and one from the biology domain, and two types of non-factoid questions: manner and reason.", "labels": [], "entities": []}, {"text": "We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer, improving performance up to 24% (relative) over a state-of-the-art model that exploits lexical semantic similarity alone.", "labels": [], "entities": []}, {"text": "We further demonstrate excellent domain transfer of discourse information, suggesting these discourse features have general utility to non-factoid question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7302601039409637}]}], "introductionContent": [{"text": "Driven by several international evaluations and workshops such as the Text REtrieval Conference (TREC) and the Cross Language Evaluation Forum (CLEF), the task of question answering (QA) has received considerable attention.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.8017172863086065}, {"text": "question answering (QA)", "start_pos": 163, "end_pos": 186, "type": "TASK", "confidence": 0.8805952429771423}]}, {"text": "However, most of this effort has focused on factoid questions rather than more complex non-factoid (NF) questions, such as manner, reason, or causation questions.", "labels": [], "entities": []}, {"text": "Moreover, the vast majority of QA models explore only local linguistic structures, such as syntactic dependencies or semantic role frames, which are generally restricted to individual sentences.", "labels": [], "entities": []}, {"text": "This is problematic for NF QA, where questions are answered not by atomic facts, but by larger cross-sentence conceptual structures that convey the desired answers.", "labels": [], "entities": [{"text": "NF QA", "start_pos": 24, "end_pos": 29, "type": "TASK", "confidence": 0.5511889755725861}]}, {"text": "Thus, to answer NF questions, one needs a model of what these answer structures look like.", "labels": [], "entities": []}, {"text": "Driven by this observation, our main hypothesis is that the discourse structure of NF answers provides complementary information to state-ofthe-art QA models that measure the similarity (either lexical and/or semantic) between question and answer.", "labels": [], "entities": []}, {"text": "We propose a novel answer reranking (AR) model that combines lexical semantics (LS) with discourse information, driven by two representations of discourse: a shallow representation centered around discourse markers and surface text information, and a deep one based on the Rhetorical Structure Theory (RST) discourse framework (.", "labels": [], "entities": [{"text": "answer reranking (AR)", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.789402425289154}, {"text": "Rhetorical Structure Theory (RST) discourse", "start_pos": 273, "end_pos": 316, "type": "TASK", "confidence": 0.7268690722329276}]}, {"text": "To the best of our knowledge, this work is the first to systematically explore within-and cross-sentence structured discourse features for NF AR.", "labels": [], "entities": [{"text": "NF AR", "start_pos": 139, "end_pos": 144, "type": "TASK", "confidence": 0.6247780919075012}]}, {"text": "The contributions of this work are: 1.", "labels": [], "entities": []}, {"text": "We demonstrate that modeling discourse is greatly beneficial for NF AR for two types of NF questions, manner (\"how\") and reason (\"why\"), across two large datasets from different genres and domains -one from the community question-answering (CQA) site of Yahoo!", "labels": [], "entities": [{"text": "NF AR", "start_pos": 65, "end_pos": 70, "type": "TASK", "confidence": 0.8553012609481812}]}, {"text": "Answers 3 , and one from a biology textbook.", "labels": [], "entities": []}, {"text": "Our results show statistically significant improvements of up to 24% on top of state-of-the-art LS models (.", "labels": [], "entities": []}, {"text": "2. We demonstrate that both shallow and deep discourse representations are useful, and, in general, their combination performs best.", "labels": [], "entities": []}, {"text": "3. We show that discourse-based QA models using inter-sentence features considerably out-perform single-sentence models when answers span multiple sentences.", "labels": [], "entities": []}, {"text": "4. We demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to NF QA.", "labels": [], "entities": []}], "datasetContent": [{"text": "For YA, we used the standard implementations for P@1 and mean reciprocal rank (MRR) ().", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 57, "end_pos": 83, "type": "METRIC", "confidence": 0.9423247675100962}]}, {"text": "In the Bio corpus, because answer candidates are not guaranteed to match gold annotations exactly, these metrics do not immediately apply.", "labels": [], "entities": [{"text": "Bio corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8319808542728424}]}, {"text": "We adapted them to this dataset by weighing each answer by its overlap with gold answers, where overlap is measured as the highest F1 score between the candidate and a gold answer.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.983961433172226}]}, {"text": "Thus, P@1 reduces to this F1 score for the top answer.", "labels": [], "entities": [{"text": "P@1", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9495685497919718}, {"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9863802790641785}]}, {"text": "For MRR, we used the rank of the candidate with the highest overlap score, weighed by the inverse of the rank.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9677870869636536}, {"text": "overlap score", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9512848854064941}]}, {"text": "For example, if the best answer fora question appears at rank 2 with an F1 score of 0.3, the corresponding MRR score is 0.3/2.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.983384758234024}, {"text": "MRR score", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9740523993968964}]}, {"text": "analyzes the performance of the proposed reranking model on the three datasets and against two baselines.", "labels": [], "entities": []}, {"text": "The first baseline sorts the candidate answers in descending order of the scores produced by the candidate retrieval (CR) module.", "labels": [], "entities": []}, {"text": "The second baseline (CR + LS) trains a reranking model without discourse, using just the CR and LS features.", "labels": [], "entities": []}, {"text": "For YA, we include an additional baseline that selects an answer randomly.", "labels": [], "entities": []}, {"text": "We list multiple versions of the proposed reranking model, broken down by the features used.", "labels": [], "entities": []}, {"text": "For Bio, we retrieved the top 20 answer candidates in CR.", "labels": [], "entities": [{"text": "CR", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.874218225479126}]}, {"text": "At this setting, the oracle performance (i.e., the performance with perfect reranking of the 20 candidates) was 69.6% P@1 for Bio HOW, and 72.3% P@1 for Bio WHY.", "labels": [], "entities": [{"text": "Bio HOW", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.7148496210575104}, {"text": "Bio WHY", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.8727657794952393}]}, {"text": "These relatively low oracle scores, which serve as a performance ceiling for our approach, highlight the difficulty of the task.", "labels": [], "entities": []}, {"text": "For YA, we used all answers provided for each given question.", "labels": [], "entities": [{"text": "YA", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8214840292930603}]}, {"text": "For all experiments we used a linear SVM kernel.", "labels": [], "entities": []}, {"text": "Examining, several trends are clear.", "labels": [], "entities": []}, {"text": "Both discourse models significantly increase both P@1 and MRR performance overall baselines broadly across genre, domain, and question types.", "labels": [], "entities": [{"text": "MRR", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.5408880710601807}]}, {"text": "More specifically, DMM and DPM show similar performance benefits when used individually, but their combination generally outperforms the individual models, illustrating the fact that the two models capture related but different discourse information.", "labels": [], "entities": []}, {"text": "This is a motivating result for discourse analysis, especially considering that the discourse parser was trained on a domain different from the corpora used here.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8084006905555725}]}], "tableCaptions": [{"text": " Table 1: Overall results across three datasets. The improve-", "labels": [], "entities": []}, {"text": " Table 4: Percentage of top features with the highest SVM", "labels": [], "entities": [{"text": "SVM", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.357162743806839}]}, {"text": " Table 5: Transfer performance from YA to Bio HOW for", "labels": [], "entities": [{"text": "Bio HOW", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.5613068491220474}]}, {"text": " Table 6: YA results with integrated discourse and LS.", "labels": [], "entities": [{"text": "YA", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.5665122866630554}]}]}