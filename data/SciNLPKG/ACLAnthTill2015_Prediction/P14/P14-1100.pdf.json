{"title": [{"text": "Spectral Unsupervised Parsing with Additive Tree Metrics", "labels": [], "entities": [{"text": "Spectral Unsupervised Parsing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8749705950419108}]}], "abstractContent": [{"text": "We propose a spectral approach for un-supervised constituent parsing that comes with theoretical guarantees on latent structure recovery.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7022729516029358}, {"text": "latent structure recovery", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.6676773031552633}]}, {"text": "Our approach is grammar-less-we directly learn the bracketing structure of a given sentence without using a grammar model.", "labels": [], "entities": []}, {"text": "The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples.", "labels": [], "entities": []}, {"text": "Although finding the \"minimal\" latent tree is NP-hard in general, for the case of pro-jective trees we find that it can be found using bilexical parsing algorithms.", "labels": [], "entities": []}, {"text": "Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Solutions to the problem of grammar induction have been long sought after since the early days of computational linguistics and are interesting both from cognitive and engineering perspectives.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8660181164741516}, {"text": "computational linguistics", "start_pos": 98, "end_pos": 123, "type": "TASK", "confidence": 0.7296967804431915}]}, {"text": "Cognitively, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees.", "labels": [], "entities": []}, {"text": "This means the unsupervised setting is a better model for studying language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.6945378333330154}]}, {"text": "From the engineering perspective, training data for unsupervised parsing exists in abundance (i.e. sentences and part-of-speech tags), and is much cheaper than the syntactically annotated data required for supervised training.", "labels": [], "entities": []}, {"text": "Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars, and the constituent context model ().", "labels": [], "entities": []}, {"text": "Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood () or a variant of it (.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.957112729549408}]}, {"text": "Unfortunately, finding the global maximum for these objective functions is usually intractable) which often leads to severe local optima problems (but see).", "labels": [], "entities": []}, {"text": "Thus, strong experimental results are often achieved by initialization techniques (, incremental dataset use () and other specialized techniques to avoid local optima such as count transforms.", "labels": [], "entities": []}, {"text": "These approaches, while empirically promising, generally lack theoretical justification.", "labels": [], "entities": []}, {"text": "On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model () or matrix completion ().", "labels": [], "entities": [{"text": "PCFG model", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.9292175769805908}]}, {"text": "These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results.", "labels": [], "entities": []}, {"text": "In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap.", "labels": [], "entities": []}, {"text": "More specifically, we approach unsupervised constituent parsing from the perspective of structure learning as opposed to parameter learning.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.6942892670631409}]}, {"text": "We associate each sentence with an undirected latent tree graphical model, which is a tree consisting of both observed variables (corresponding to the words in the sentence) and an additional set of latent variables that are unobserved in the data.", "labels": [], "entities": []}, {"text": "This undirected latent tree is then directed via a direction mapping to give the final constituent parse.", "labels": [], "entities": []}, {"text": "In our framework, parsing reduces to finding the best latent structure fora given sentence.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9747406244277954}]}, {"text": "However, due to the presence of latent variables, structure learning of latent trees is substantially more complicated than in observed models.", "labels": [], "entities": [{"text": "structure learning", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6835195571184158}]}, {"text": "As before, one solution would be local search heuristics.", "labels": [], "entities": []}, {"text": "Intuitively, however, latent tree models encode low rank dependencies among the observed variables permitting the development of \"spec-tral\" methods that can lead to provably correct solutions.", "labels": [], "entities": []}, {"text": "In particular we leverage the concept of additive tree metrics) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies.", "labels": [], "entities": []}, {"text": "Additive tree metrics can be leveraged by \"meta-algorithms\" such as neighbor-joining ( and recursive grouping) to provide consistent learning algorithms for latent trees.", "labels": [], "entities": []}, {"text": "Moreover, we show that it is desirable to learn the \"minimal\" latent tree based on the tree metric (\"minimum evolution\" in phylogenetics).", "labels": [], "entities": []}, {"text": "While this criterion is in general NP-hard), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently.", "labels": [], "entities": []}, {"text": "Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree.", "labels": [], "entities": []}, {"text": "This leads to a severe data sparsity problem even for moderately long sentences.", "labels": [], "entities": []}, {"text": "To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community ().", "labels": [], "entities": [{"text": "kernel smoothing", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7557645440101624}]}, {"text": "This allows principled sharing of samples from different but similar underlying distributions.", "labels": [], "entities": []}, {"text": "We provide theoretical guarantees on the recovery of the correct underlying latent tree and characterize the associated sample complexity under our technique.", "labels": [], "entities": []}, {"text": "Empirically we evaluate our method on data in English, German and Chinese.", "labels": [], "entities": []}, {"text": "Our algorithm performs favorably to constituent-context model (CCM), without the need for careful initialization.", "labels": [], "entities": []}, {"text": "In addition, we also analyze CCM's sensitivity to initialization, and compare our results to Seginer's algorithm).", "labels": [], "entities": [{"text": "initialization", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9698426127433777}]}], "datasetContent": [{"text": "We report results on three different languages: English, German, and Chinese.", "labels": [], "entities": []}, {"text": "For English we use the Penn treebank (, with sections 2-21 for training and section 23 for final testing.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.993475615978241}]}, {"text": "For German and Chinese we use the Negra treebank and the Chinese treebank respectively and the first 80% of the sentences are used for training and the last 20% for testing.", "labels": [], "entities": [{"text": "Negra treebank", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.98309525847435}, {"text": "Chinese treebank", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.9789767861366272}]}, {"text": "All punctuation from the data is removed.", "labels": [], "entities": []}, {"text": "We primarily compare our method to the constituent-context model (CCM) of.", "labels": [], "entities": []}, {"text": "We also compare our method to the algorithm of Seginer (2007).", "labels": [], "entities": []}, {"text": "Top bracket heuristic Our algorithm requires the top bracket in order to direct the latent tree.", "labels": [], "entities": []}, {"text": "In practice, we employ the following heuristic to find the bracket using the following three steps: \u2022 If there exists a comma/semicolon/colon at index i that has at least a verb before i and both a noun followed by a verb after i, then return \u2022 Otherwise find the first non-participle verb (say at index j) and return Word embeddings As mentioned earlier, each w i can bean arbitrary feature vector.", "labels": [], "entities": []}, {"text": "For all languages we use Brown clustering to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity.", "labels": [], "entities": []}, {"text": "For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings of length 50.", "labels": [], "entities": []}, {"text": "We also explored two types of CCA embeddings: OSCCA and TSCCA, given in.", "labels": [], "entities": [{"text": "OSCCA", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8207500576972961}, {"text": "TSCCA", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.6906481385231018}]}, {"text": "The OSCCA embeddings behaved better, so we only report its results.", "labels": [], "entities": [{"text": "OSCCA", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.839862585067749}]}, {"text": "Choice of kernel For our experiments, we use the kernel where \u03b3 denotes the user-specified bandwidth, The kernel is non-zero if and only if the tags at position j and kin x are identical to the ones in position j and kin x , and if the direction between j and k is identical to the one between j and k . Note that the kernel is not binary, as opposed to the theoretical kernel in the supplementary material.", "labels": [], "entities": []}, {"text": "Our experiments show that using a non-zero value different than 1 that is a function of the distance between j and k compared to the distance between j and k does better in practice.", "labels": [], "entities": []}, {"text": "Choice of data For CCM, we found that if the full dataset (all sentence lengths) is used in training, then performance degrades when evaluating on sentences of length \u2264 10.", "labels": [], "entities": []}, {"text": "We therefore restrict the data used with CCM to sentences of length \u2264 , where is the maximal sentence length being evaluated.", "labels": [], "entities": []}, {"text": "This does not happen with our algorithm, which manages to leverage lexical information whenever more data is available.", "labels": [], "entities": []}, {"text": "We therefore use the full data for our method for all lengths.", "labels": [], "entities": []}, {"text": "We also experimented with the original POS tags and the universal POS tags of.", "labels": [], "entities": []}, {"text": "Here, we found out that our method does better with the universal part of speech tags.", "labels": [], "entities": []}, {"text": "For CCM, we also experimented with the original parts of speech, universal tags (CCM-U), the cross-product of the original parts of speech with the Brown clusters (CCM-OB), and the crossproduct of the universal tags with the Brown clusters (CCM-UB).", "labels": [], "entities": []}, {"text": "The results in indicate that the vanilla setting is the best for CCM.", "labels": [], "entities": [{"text": "CCM", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8727878928184509}]}, {"text": "Thus, for all results, we use universal tags for our method and the original POS tags for CCM.", "labels": [], "entities": []}, {"text": "We believe that our approach substitutes the need for fine-grained POS tags with the lexical information.", "labels": [], "entities": []}, {"text": "CCM, on the other hand, is fully unlexicalized.", "labels": [], "entities": [{"text": "CCM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9078331589698792}]}, {"text": "Parameter Selection Our method requires two parameters, the latent dimension m and the bandwidth \u03b3.", "labels": [], "entities": []}, {"text": "CCM also has two parameters, the number of extra constituent/distituent counts used for smoothing.", "labels": [], "entities": [{"text": "CCM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9118403792381287}]}, {"text": "For both methods we chose the best parameters for sentences of length \u2264 10 on the English Penn Treebank (training) and used this set for all other experiments.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.9663680593172709}]}, {"text": "This resulted in m = 7, \u03b3 = 0.4 for our method and 2, 8 for CCM's extra constituent/distituent counts respectively.", "labels": [], "entities": []}, {"text": "We also tried letting CCM choose different hyperparameters for different sentence lengths based on dev-set likelihood, but this gave worse results than holding them fixed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different CCM variants on English  (training). U stands for universal POS tagset, OB stands for  conjoining original POS tags with Brown clusters and UB  stands for conjoining universal POS tags with Brown clusters.  The best setting is just the vanilla setting, CCM.", "labels": [], "entities": [{"text": "OB", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9427405595779419}]}, {"text": " Table 2: F1 bracketing measure for the test sets and train sets in three languages. NN, CC, and BC indicate the performance of  our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for h dir described  in  \u00a7 4.1. NN-O, CC-O, and BC-O indicate that the oracle (i.e. true top bracket) was used for h dir .", "labels": [], "entities": []}]}