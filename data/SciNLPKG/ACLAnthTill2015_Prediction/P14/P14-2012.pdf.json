{"title": [{"text": "Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction", "labels": [], "entities": [{"text": "Employing Word Representations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.724729080994924}, {"text": "Domain Adaptation of Relation Extraction", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.8722100496292114}]}], "abstractContent": [{"text": "Relation extraction suffers from a performance loss when a model is applied to out-of-domain data.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9473611116409302}]}, {"text": "This has fostered the development of domain adaptation techniques for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.9550102055072784}]}, {"text": "This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7345999330282211}]}, {"text": "We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7220591157674789}]}], "introductionContent": [{"text": "The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8402103960514069}]}, {"text": "Recent research in this area, whether feature-based) or kernelbased (, attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources.", "labels": [], "entities": [{"text": "RE", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9330236911773682}]}, {"text": "The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution.", "labels": [], "entities": []}, {"text": "When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically.", "labels": [], "entities": []}, {"text": "This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into anew model which can perform well on new domains (the target domains).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7318856418132782}]}, {"text": "The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (), named entity recognition and sentiment analysis), etc.", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.7885007858276367}, {"text": "named entity recognition and sentiment analysis", "start_pos": 203, "end_pos": 250, "type": "TASK", "confidence": 0.6407288362582525}]}, {"text": "Unfortunately, there is very little work on domain adaptation for RE.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7244637310504913}, {"text": "RE", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.8139951229095459}]}, {"text": "The only study explicitly targeting this problem so far is by who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels.", "labels": [], "entities": [{"text": "kernel-based relation extractors", "start_pos": 109, "end_pos": 141, "type": "TASK", "confidence": 0.6721556981404623}, {"text": "word clustering", "start_pos": 218, "end_pos": 233, "type": "TASK", "confidence": 0.6840093433856964}, {"text": "latent semantic analysis (LSA)", "start_pos": 238, "end_pos": 268, "type": "TASK", "confidence": 0.7449658811092377}]}, {"text": "Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity.", "labels": [], "entities": []}, {"text": "In fact, only use the 10-bit cluster prefix in their study.", "labels": [], "entities": []}, {"text": "We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7195397913455963}]}, {"text": "+ It is unclear if this approach can encode realvalued features of words (such as word embeddings () effectively.", "labels": [], "entities": []}, {"text": "As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.7112590372562408}]}, {"text": "In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu-rally and effectively.", "labels": [], "entities": [{"text": "RE", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9495644569396973}]}, {"text": "The application of word representations such as word clusters in domain adaptation of RE) is motivated by its successes in semi-supervised methods) where word representations help to reduce data-sparseness of lexical information in the training data.", "labels": [], "entities": []}, {"text": "In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains.", "labels": [], "entities": []}, {"text": "The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains.", "labels": [], "entities": [{"text": "RE", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.7593269348144531}]}, {"text": "We extend this motivation by further evaluating word embeddings () on feature-based methods to adapt RE systems to new domains.", "labels": [], "entities": []}, {"text": "We explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE.", "labels": [], "entities": [{"text": "domain adaptation of RE", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.6404801160097122}]}, {"text": "More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors.", "labels": [], "entities": [{"text": "relation extractors", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.7234937101602554}]}], "datasetContent": [{"text": "We investigate the effectiveness of word embeddings on lexical features by following the procedure described in Section 5.2.", "labels": [], "entities": []}, {"text": "We test our system on two scenarios: In-domain: the system is trained and evaluated on the source domain (bn+nw, 5-fold cross validation); Out-of-domain: the system is trained on the source domain and evaluated on the target development set of bc (bc dev).", "labels": [], "entities": []}, {"text": "suffix ED in lexical group names is to indicate the embedding features).", "labels": [], "entities": [{"text": "ED", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9702184796333313}]}, {"text": "From the tables, we find that for C&W and HLBL embeddings of 50 and 100 dimensions, the most effective way to introduce word embeddings is to add embeddings to the heads of the two mentions (row 2; both in-domain and out-of-domain) although it is less pronounced for HLBL embedding with 50 dimensions.", "labels": [], "entities": []}, {"text": "Interestingly, for C&W embedding with 25 dimensions, adding the embedding to both heads and words of the two mentions (row 6) performs the best for both in-domain and out-of-domain scenarios.", "labels": [], "entities": []}, {"text": "This is new compared to the word cluster features where the heads of the two mentions are always the best places for augmentation.", "labels": [], "entities": []}, {"text": "It suggests that a suitable amount of embeddings for words in the mentions might be useful for the augmentation of the heads and inspires further exploration.", "labels": [], "entities": []}, {"text": "Introducing embeddings to words of mentions alone has mild impact while it is generally a bad idea to augment chunk heads and words in the contexts.", "labels": [], "entities": []}, {"text": "Comparing C&W and HLBL embeddings is somehow more complicated.", "labels": [], "entities": []}, {"text": "For both in-domain and out-of-domain settings with different numbers of dimensions, C&W embedding outperforms HLBL embedding when only the heads of the mentions are augmented while the degree of negative impact of HLBL embedding on chunk heads as well as context words seems less serious than C&W's.", "labels": [], "entities": []}, {"text": "Regarding the incremental addition of features (rows 6, 7, 8), C&W is better for the outof-domain performance when 50 dimensions are used, whereas HLBL (with both 50 and 100 dimensions) is more effective for the in-domain setting.", "labels": [], "entities": []}, {"text": "For the next experiments, we will apply the C&W embedding of 50 dimensions to the heads of the mentions for its best out-of-domain performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: In-domain and Out-of-domain performance for different embedding features. The cells in bold are the best results.", "labels": [], "entities": []}, {"text": " Table 3: Domain Adaptation Results with Word Represen-", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8505002558231354}]}, {"text": " Table 4: Domain Adaptation Results with Regularization.", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8364412784576416}]}]}