{"title": [{"text": "Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.9553425312042236}]}], "abstractContent": [{"text": "We investigate different ways of learning structured perceptron models for coref-erence resolution when using non-local features and beam search.", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.8136095106601715}]}, {"text": "Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features.", "labels": [], "entities": []}, {"text": "By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline.", "labels": [], "entities": []}, {"text": "Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper studies and extends previous work using the structured perceptron) for complex NLP tasks.", "labels": [], "entities": []}, {"text": "We show that for the task of coreference resolution the straightforward combination of beam search and early update () falls short of more limited feature sets that allow for exact search.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9867275655269623}, {"text": "early update", "start_pos": 103, "end_pos": 115, "type": "METRIC", "confidence": 0.9529135227203369}]}, {"text": "This contrasts with previous work on, e.g., syntactic parsing () and linearization, and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging) and named entity recognition.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7619074881076813}, {"text": "part-of-speech tagging", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.6917599588632584}, {"text": "named entity recognition", "start_pos": 213, "end_pos": 237, "type": "TASK", "confidence": 0.694676528374354}]}, {"text": "The main reason why early updates underperform in our setting is that the task is too difficult and that the learning algorithm is notable to profit from all training data.", "labels": [], "entities": []}, {"text": "Put another way, early updates happen too early, and the learning algorithm rarely reaches the end of the instances as it halts, updates, and moves onto the next instance.", "labels": [], "entities": []}, {"text": "An alternative would be to continue decoding the same instance after the early updates, which is equivalent to Learning as Search Optimization ().", "labels": [], "entities": []}, {"text": "The learning task we are tackling is however further complicated since the target structure is under-determined by the gold standard annotation.", "labels": [], "entities": []}, {"text": "Coreferent mentions in a document are usually annotated as sets of mentions, where all mentions in a set are coreferent.", "labels": [], "entities": []}, {"text": "We adopt the recently popularized approach of inducing a latent structure within these sets.", "labels": [], "entities": []}, {"text": "This approach provides a powerful boost to the performance of coreference resolvers, but we find that it does not combine well with the LaSO learning strategy.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.9351049959659576}]}, {"text": "We therefore propose a modification to LaSO, which delays updates until after each instance.", "labels": [], "entities": [{"text": "LaSO", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8574374318122864}]}, {"text": "The combination of this modification with non-local features leads to further improvements in the clustering accuracy, as we show in evaluation results on all languages from the CoNLL 2012 Shared TaskArabic, Chinese, and English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9700559377670288}, {"text": "CoNLL 2012 Shared TaskArabic", "start_pos": 178, "end_pos": 206, "type": "DATASET", "confidence": 0.8953630477190018}]}, {"text": "We obtain the best results to date on these data sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply our model to the CoNLL 2012 Shared Task data, which includes a training, development, and test set split for three languages: Arabic, Chinese and English.", "labels": [], "entities": [{"text": "CoNLL 2012 Shared Task data", "start_pos": 26, "end_pos": 53, "type": "DATASET", "confidence": 0.9283776760101319}]}, {"text": "We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by.", "labels": [], "entities": [{"text": "English gender and number data compiled", "start_pos": 126, "end_pos": 165, "type": "DATASET", "confidence": 0.5697616438070933}]}, {"text": "We use automatically extracted mentions using the same mention extraction procedure as.", "labels": [], "entities": []}, {"text": "We evaluate our system using the CoNLL 2012 scorer, which computes several coreference metrics: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAF e and CEAF m ().", "labels": [], "entities": [{"text": "CoNLL 2012 scorer", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.903554360071818}, {"text": "MUC", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9503834247589111}, {"text": "CEAF e", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.870695948600769}, {"text": "CEAF m", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.6428048312664032}]}, {"text": "We also report the CoNLL average (also known as MELA; Denis and Baldridge), i.e., the arithmetic mean of MUC, B 3 , and CEAF e . It should be noted that for B 3 and the CEAF metrics, multiple ways of handling twinless mentions have been proposed).", "labels": [], "entities": [{"text": "CoNLL average", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.8897649347782135}, {"text": "MELA", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9223599433898926}, {"text": "arithmetic mean", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.9614545106887817}, {"text": "MUC", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.6642714738845825}, {"text": "B 3", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9137314558029175}, {"text": "CEAF e", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9320043921470642}, {"text": "CEAF metrics", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.8796803951263428}]}, {"text": "We use the most recent version of the CoNLL scorer (version 7), which implements the original definitions of these metrics.", "labels": [], "entities": [{"text": "CoNLL scorer", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.8378917872905731}]}, {"text": "Our system is evaluated on the version of the data with automatic preprocessing information (e.g., predicted parse trees).", "labels": [], "entities": []}, {"text": "Unless otherwise stated we use 25 iterations of perceptron training and abeam size of 20.", "labels": [], "entities": []}, {"text": "We did not attempt to tune either of these parameters.", "labels": [], "entities": []}, {"text": "We experiment with two feature sets for each language: the optimized local feature sets (denoted local), and the optimized local feature sets extended with non-local features (denoted non-local).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of local and non-local fea- ture sets on the development sets.", "labels": [], "entities": []}, {"text": " Table 2: Comparison with other systems on the test sets. Bold numbers indicate significance at the  p < 0.05 level between the best and the second best systems (according to the CoNLL average) using  a Wilcoxon signed rank sum test. We refrain from significance tests on the CoNLL average, as it is an  average over other F-measures.", "labels": [], "entities": [{"text": "significance", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.9696720838546753}, {"text": "CoNLL average", "start_pos": 179, "end_pos": 192, "type": "DATASET", "confidence": 0.7802442312240601}, {"text": "CoNLL average", "start_pos": 276, "end_pos": 289, "type": "DATASET", "confidence": 0.9309847354888916}]}]}