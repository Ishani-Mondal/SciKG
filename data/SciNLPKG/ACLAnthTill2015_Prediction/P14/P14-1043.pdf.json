{"title": [{"text": "Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.6280244886875153}]}], "abstractContent": [{"text": "This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6960261166095734}]}, {"text": "Instead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data.", "labels": [], "entities": []}, {"text": "With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings.", "labels": [], "entities": []}, {"text": "This framework offers two promising advantages.", "labels": [], "entities": []}, {"text": "1) ambiguity encoded in parse forests compromises noise in 1-best parse trees.", "labels": [], "entities": []}, {"text": "During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves.", "labels": [], "entities": []}, {"text": "2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser.", "labels": [], "entities": []}, {"text": "Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised dependency parsing has made great progress during the past decade.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8337604701519012}]}, {"text": "However, it is very difficult to further improve performance * Correspondence author of supervised parsers.", "labels": [], "entities": []}, {"text": "For example, and show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 122, "end_pos": 129, "type": "TASK", "confidence": 0.9683310985565186}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8743774890899658}]}, {"text": "In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest.", "labels": [], "entities": []}, {"text": "Previously, unlabeled data is explored to derive useful local-context features such as word clusters (, subtree frequencies (, and word co-occurrence counts (.", "labels": [], "entities": []}, {"text": "A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8436041474342346}]}, {"text": "All above work leads to significant improvement on parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.979721188545227}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9452654719352722}]}, {"text": "Another line of research is to pickup some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training, co-training, and tri-training (Zhou and).", "labels": [], "entities": []}, {"text": "However, these methods gain limited success in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.882144957780838}]}, {"text": "Although working well on constituent parsing, self-training is shown unsuccessful for dependency parsing).", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7214650213718414}, {"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7679138481616974}]}, {"text": "The reason maybe that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8295486271381378}]}, {"text": "apply a variant of co-training to dependency parsing and report positive results on out-of-domain text.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8814595639705658}]}, {"text": "combine tri-training and parser ensemble to boost parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.951068639755249}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9460186958312988}]}, {"text": "Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical.", "labels": [], "entities": []}, {"text": "In this way, the autoparsed unlabeled data becomes more reliable.", "labels": [], "entities": []}, {"text": "w 0 He 1 saw 2 a 3 deer 4 riding 5 a 6 bicycle 7 in 8 the 9 park 10 . 11: An example sentence with an ambiguous parse forest.", "labels": [], "entities": []}, {"text": "However, one obvious drawback of these methods is that they are unable to exploit unlabeled data with divergent outputs from different parsers.", "labels": [], "entities": []}, {"text": "Our experiments show that unlabeled data with identical outputs from different parsers tends to be short (18.25 words per sentence on average), and only has a small proportion of 40% (see).", "labels": [], "entities": []}, {"text": "More importantly, we believe that unlabeled data with divergent outputs is equally (if not more) useful.", "labels": [], "entities": []}, {"text": "Intuitively, an unlabeled sentence with divergent outputs should contain some ambiguous syntactic structures (such as preposition phrase attachment) that are very hard to resolve and lead to the disagreement of different parsers.", "labels": [], "entities": [{"text": "preposition phrase attachment", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.7165194948514303}]}, {"text": "Such sentences can provide more discriminative instances for training which maybe unavailable in labeled data.", "labels": [], "entities": []}, {"text": "To solve above issues, this paper proposes a more general and effective framework for semi-supervised dependency parsing, referred to as ambiguity-aware ensemble training.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.6891892403364182}]}, {"text": "Different from traditional self/co/tri-training which only use 1-best parse trees on unlabeled data, our approach adopts ambiguous labelings, represented by parse forest, as gold-standard for unlabeled sentences.", "labels": [], "entities": []}, {"text": "shows an example sentence with an ambiguous parse forest.", "labels": [], "entities": []}, {"text": "The forest is formed by two parse trees, respectively shown at the upper and lower sides of the sentence.", "labels": [], "entities": []}, {"text": "The differences between the two parse trees are highlighted using dashed arcs.", "labels": [], "entities": []}, {"text": "The upper tree take \"deer\" as the subject of \"riding\", whereas the lower one indicates that \"he\" rides the bicycle.", "labels": [], "entities": []}, {"text": "The other difference is where the preposition phrase (PP) \"in the park\" should be attached, which is also known as the PP attachment problem, a notorious challenge for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.9705883860588074}]}, {"text": "Reserving such uncertainty has three potential advantages.", "labels": [], "entities": []}, {"text": "First, noise in unlabeled data is largely alleviated, since parse forest encodes only a few highly possible parse trees with high oracle score.", "labels": [], "entities": []}, {"text": "Please note that the parse forest in contains four parse trees after combination of the two different choices.", "labels": [], "entities": []}, {"text": "Second, the parser is able to learn useful features from the unambiguous parts of the parse forest.", "labels": [], "entities": []}, {"text": "Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees.", "labels": [], "entities": []}, {"text": "To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser (, and a generative constituent parser.", "labels": [], "entities": []}, {"text": "The 1-best parse trees of these three parsers are aggregated in different ways.", "labels": [], "entities": []}, {"text": "Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9879613518714905}]}, {"text": "Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings.", "labels": [], "entities": []}, {"text": "Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ensemble training outperforms other entire-tree based methods such as self/co/tri-training.", "labels": [], "entities": []}, {"text": "In summary, we make following contributions.", "labels": [], "entities": []}, {"text": "1. We propose a generalized ambiguity-aware ensemble training framework for semisupervised dependency parsing, which can make better use of unlabeled data, especially when parsers from different views produce divergent syntactic structures.", "labels": [], "entities": [{"text": "semisupervised dependency parsing", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.6191368997097015}]}, {"text": "2. We first employ a generative constituent parser for semi-supervised dependency parsing.", "labels": [], "entities": [{"text": "generative constituent parser", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.8342195749282837}, {"text": "dependency parsing", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.6913502514362335}]}, {"text": "Experiments show that the constituent parser is very helpful since it produces more divergent structures for our semi-supervised parser than discriminative dependency parsers.", "labels": [], "entities": []}, {"text": "3. We build the first state-of-the-art CRF-based dependency parser.", "labels": [], "entities": [{"text": "CRF-based dependency parser", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.7971624533335367}]}, {"text": "Using the probabilistic parser, we benchmark and conduct systematic comparisons among ours and all previous bootstrapping methods, including self/co/tritraining.", "labels": [], "entities": []}], "datasetContent": [{"text": "To verify the effectiveness of our proposed approach, we conduct experiments on Penn Treebank (PTB) and Penn Chinese Treebank 5.1 (CT-B5).", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.9652965188026428}, {"text": "Penn Chinese Treebank 5.1 (CT-B5)", "start_pos": 104, "end_pos": 137, "type": "DATASET", "confidence": 0.9477962936673846}]}, {"text": "For English, we follow the popular practice to split data into training (sections 2-21), development (section 22), and test (section 23).", "labels": [], "entities": []}, {"text": "For CTB5, we adopt the data split of ( shows the data statistics.", "labels": [], "entities": [{"text": "CTB5", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9205055832862854}]}, {"text": "We measure parsing performance using the standard unlabeled attachment score (UAS), excluding punctuation marks.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 50, "end_pos": 82, "type": "METRIC", "confidence": 0.8059477359056473}]}, {"text": "For significance test, we adopt Dan Bikel's randomized parsing evaluation comparator.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data sets (in sentence number).", "labels": [], "entities": []}, {"text": " Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.", "labels": [], "entities": []}, {"text": " Table 5: UAS comparison on Chinese test data.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.3541438579559326}, {"text": "Chinese test data", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.8358832399050394}]}, {"text": " Table 6: Performance of our semi-supervised  GParser with different sets of \"Unlabeled \u2190  B+Z\" on English test set. \"Len\" means averaged  sentence length.", "labels": [], "entities": [{"text": "English test set", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.7489383220672607}]}]}