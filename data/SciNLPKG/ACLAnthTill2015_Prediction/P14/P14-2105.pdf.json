{"title": [{"text": "Semantic Parsing for Single-Relation Question Answering", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8482738137245178}, {"text": "Single-Relation Question Answering", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.7105973660945892}]}], "abstractContent": [{"text": "We develop a semantic parsing framework based on semantic similarity for open domain question answering (QA).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.731569916009903}, {"text": "open domain question answering (QA)", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.7663144384111676}]}, {"text": "We focus on single-relation questions and decompose each question into an entity mention and a relation pattern.", "labels": [], "entities": []}, {"text": "Using convo-lutional neural network models, we measure the similarity of entity mentions with entities in the knowledge base (KB) and the similarity of relation patterns and relations in the KB.", "labels": [], "entities": []}, {"text": "We score relational triples in the KB using these measures and select the top scoring relational triple to answer the question.", "labels": [], "entities": []}, {"text": "When evaluated on an open-domain QA task, our method achieves higher precision across different recall points compared to the previous approach , and can improve F 1 by 7 points.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9989575147628784}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9961575865745544}, {"text": "F 1", "start_pos": 162, "end_pos": 165, "type": "METRIC", "confidence": 0.9920811355113983}]}], "introductionContent": [{"text": "Open-domain question answering (QA) is an important and yet challenging problem that remains largely unsolved.", "labels": [], "entities": [{"text": "Open-domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8184617360432943}]}, {"text": "In this paper, we focus on answering single-relation factual questions, which are the most common type of question observed in various community QA sites, as well as in search query logs.", "labels": [], "entities": []}, {"text": "We assumed such questions are answerable by issuing a singlerelation query that consists of the relation and an argument entity, against a knowledge base (KB).", "labels": [], "entities": []}, {"text": "Example questions of this type include: \"Who is the CEO of Tesla?\" and \"Who founded Paypal?\"", "labels": [], "entities": []}, {"text": "While single-relation questions are easier to handle than questions with more complex and multiple relations, such as \"When was the child of the former Secretary of State in Obama's administration born?\", single-relation questions are still far from completely solved.", "labels": [], "entities": []}, {"text": "Even in this restricted domain there area large number of paraphrases of the same question.", "labels": [], "entities": []}, {"text": "That is to say that the problem of mapping from a question to a particular relation and entity in the KB is non-trivial.", "labels": [], "entities": []}, {"text": "In this paper, we propose a semantic parsing framework tailored to single-relation questions.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7184899747371674}]}, {"text": "At the core of our approach is a novel semantic similarity model using convolutional neural networks.", "labels": [], "entities": []}, {"text": "Leveraging the question paraphrase data mined from the WikiAnswers corpus by, we train two semantic similarity models: one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation.", "labels": [], "entities": [{"text": "WikiAnswers corpus", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.8621135652065277}]}, {"text": "The answer to the question can thus be derived by finding the relation-entity triple r(e 1 , e 2 ) in the KB and returning the entity not mentioned in the question.", "labels": [], "entities": []}, {"text": "By using a general semantic similarity model to match patterns and relations, as well as mentions and entities, our system outperforms the existing rule learning system, PARALEX, with higher precision at all the recall points when answering the questions in the same test set.", "labels": [], "entities": [{"text": "PARALEX", "start_pos": 170, "end_pos": 177, "type": "METRIC", "confidence": 0.9811896085739136}, {"text": "precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9965298771858215}, {"text": "recall", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.9824460744857788}]}, {"text": "The highest achievable F 1 score of our system is 0.61, versus 0.54 of PARALEX.", "labels": [], "entities": [{"text": "achievable", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9886899590492249}, {"text": "F 1 score", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9170197447141012}, {"text": "PARALEX", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9279700517654419}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first survey related work in Sec.", "labels": [], "entities": []}, {"text": "2, followed by the problem definition and the high-level description of our approach in Sec.", "labels": [], "entities": []}, {"text": "4 details our semantic models and Sec.", "labels": [], "entities": []}, {"text": "5 shows the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to provide a fair comparison to previous work, we experimented with our approach using the PARALAX dataset, which consists of paraphrases of questions mined from WikiAnswers and answer triples from ReVerb.", "labels": [], "entities": [{"text": "PARALAX dataset", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.9180110991001129}, {"text": "ReVerb", "start_pos": 207, "end_pos": 213, "type": "DATASET", "confidence": 0.9360848069190979}]}, {"text": "In this section, we briefly introduce the dataset, describe the system training and evaluation processes and, finally, present our experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of two variations of our sys- tems, compared with the PARALEX system.", "labels": [], "entities": [{"text": "PARALEX", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8728470802307129}]}]}