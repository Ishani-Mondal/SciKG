{"title": [{"text": "Sprinkling Topics for Weakly Supervised Text Classification", "labels": [], "entities": [{"text": "Sprinkling Topics", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8809464871883392}, {"text": "Weakly Supervised Text Classification", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.6397000327706337}]}], "abstractContent": [{"text": "Supervised text classification algorithms require a large number of documents labeled by humans, that involve a labor-intensive and time consuming process.", "labels": [], "entities": [{"text": "text classification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7263472676277161}]}, {"text": "In this paper, we propose a weakly supervised algorithm in which supervision comes in the form of labeling of Latent Dirichlet Allocation (LDA) topics.", "labels": [], "entities": [{"text": "labeling of Latent Dirichlet Allocation (LDA) topics", "start_pos": 98, "end_pos": 150, "type": "TASK", "confidence": 0.7154382599724664}]}, {"text": "We then use this weak supervision to \"sprin-kle\" artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations.", "labels": [], "entities": []}, {"text": "We evaluate this approach to improve performance of text classification on three real world datasets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8230326771736145}]}], "introductionContent": [{"text": "In supervised text classification learning algorithms, the learner (a program) takes human labeled documents as input and learns a decision function that can classify a previously unseen document to one of the predefined classes.", "labels": [], "entities": [{"text": "text classification learning", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.8130810558795929}]}, {"text": "Usually a large number of documents labeled by humans are used by the learner to classify unseen documents with adequate accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9905411601066589}]}, {"text": "Unfortunately, labeling a large number of documents is a labor-intensive and time consuming process.", "labels": [], "entities": [{"text": "labeling a large number of documents", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.8111096421877543}]}, {"text": "In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) () which does not need labeled documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.768377423286438}, {"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 67, "end_pos": 100, "type": "METRIC", "confidence": 0.885350356499354}]}, {"text": "LDA is an unsupervised probabilistic topic model and it is widely used to discover latent semantic structure of a document collection by modeling words in the documents.", "labels": [], "entities": []}, {"text": "used LDA topics as features in text classification, but they use labeled documents while learning a classifier.", "labels": [], "entities": [{"text": "text classification", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7166610360145569}]}, {"text": "sLDA, DiscLDA () and MedLDA () are few extensions of LDA which model both class labels and words in the documents.", "labels": [], "entities": [{"text": "MedLDA", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.7580254077911377}]}, {"text": "These models can be used for text classification, but they need expensive labeled documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.863012433052063}]}, {"text": "An approach that is less demanding in terms of knowledge engineering is ClassifyLDA.", "labels": [], "entities": []}, {"text": "In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words.", "labels": [], "entities": []}, {"text": "These labeled topics are used to create anew topic model such that in the new model topics are better aligned to class labels.", "labels": [], "entities": []}, {"text": "A class label is assigned to a test document on the basis of its most prominent topics.", "labels": [], "entities": []}, {"text": "We extend ClassifyLDA algorithm by \"sprinkling\" topics to unlabeled documents.", "labels": [], "entities": []}, {"text": "Sprinkling () integrates class labels of documents into Latent Semantic Indexing (LSI)).", "labels": [], "entities": []}, {"text": "The basic idea involves encoding of class labels as artificial words which are \"sprinkled\" (appended) to training documents.", "labels": [], "entities": []}, {"text": "As LSI uses higher order word associations (), sprinkling of artificial words gives better and class-enriched latent semantic structure.", "labels": [], "entities": []}, {"text": "However, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents.", "labels": [], "entities": [{"text": "Sprinkled LSI", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.7804055511951447}]}, {"text": "The paper revolves around the idea of labeling topics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these labeled topic for sprinkling.", "labels": [], "entities": []}, {"text": "As in ClassifyLDA, we ask an annotator to assign class labels to a set of topics inferred on the unlabeled training documents.", "labels": [], "entities": []}, {"text": "We use the labeled topics to find probability distribution of each training document over the class labels.", "labels": [], "entities": []}, {"text": "We create a set of artificial words corresponding to a class label and add (or sprinkle) them to the document.", "labels": [], "entities": []}, {"text": "The number of such artificial terms is propor-tional to the probability of generating the document by the class label.", "labels": [], "entities": []}, {"text": "We then infer a set of topics on the sprinkled training documents.", "labels": [], "entities": []}, {"text": "As LDA uses higher order word associations () while discovering topics, we hypothesize that sprinkling will improve text classification performance of ClassifyLDA.", "labels": [], "entities": [{"text": "text classification", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7245421707630157}]}, {"text": "We experimentally verify this hypothesis on three real world datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We determine the effectiveness of our algorithm in relation to ClassifyLDA algorithm proposed in (.", "labels": [], "entities": []}, {"text": "We evaluate and compare our text classification algorithm by computing Macro averaged F1.", "labels": [], "entities": [{"text": "text classification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8117403984069824}, {"text": "Macro averaged F1", "start_pos": 71, "end_pos": 88, "type": "METRIC", "confidence": 0.4848230481147766}]}, {"text": "As the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average Macro-F1.", "labels": [], "entities": []}, {"text": "Similar to () we also learn supervised SVM classifier (LDA-SVM) for each dataset using topics as features and report average Macro-F1.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7813469469547272}]}, {"text": "We use the following datasets in our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results of text classification on various datasets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.8294580280780792}]}]}