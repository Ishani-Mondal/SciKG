{"title": [{"text": "Using Discourse Structure Improves Machine Translation Evaluation", "labels": [], "entities": [{"text": "Discourse Structure Improves Machine Translation", "start_pos": 6, "end_pos": 54, "type": "TASK", "confidence": 0.6540838241577148}]}], "abstractContent": [{"text": "We present experiments in using discourse structure for improving machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.8808784087498983}]}, {"text": "We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory.", "labels": [], "entities": []}, {"text": "Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment-and at the system-level.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.855919361114502}]}, {"text": "Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should betaken into account in the development of future richer evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "From its foundations, Statistical Machine Translation (SMT) had two defining characteristics: first, translation was modeled as a generative process at the sentence-level.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.8612403372923533}]}, {"text": "Second, it was purely statistical over words or word sequences and made little to no use of linguistic information.", "labels": [], "entities": []}, {"text": "Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.991865336894989}]}, {"text": "Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (), hierarchical structures), and semantic roles (, and (b) by going beyond the sentence-level, e.g., translating at the document level ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9968999624252319}]}, {"text": "Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text.", "labels": [], "entities": []}, {"text": "Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards.", "labels": [], "entities": []}, {"text": "The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts.", "labels": [], "entities": []}, {"text": "Note that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations.", "labels": [], "entities": []}, {"text": "Thus, in a coherent text, discourse units (sentences or clauses) are logically connected: the meaning of a unit relates to that of the previous and the following units.", "labels": [], "entities": []}, {"text": "Discourse analysis seeks to uncover this coherence structure underneath the text.", "labels": [], "entities": [{"text": "Discourse analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8081915080547333}]}, {"text": "Several formal theories of discourse have been proposed to describe the coherence structure).", "labels": [], "entities": []}, {"text": "For example, the Rhetorical Structure Theory (, or RST, represents text by labeled hierarchical structures called Discourse Trees (DTs), which can incorporate several layers of other linguistic information, e.g., syntax, predicate-argument structure, etc.", "labels": [], "entities": []}, {"text": "Modeling discourse brings together the above research directions (a) and (b), which makes it an attractive goal for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9891093969345093}]}, {"text": "This is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (, collocated with the 2013 annual meeting of the Association of Computational Linguistics.", "labels": [], "entities": [{"text": "Discourse in Machine Translation", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.6565049737691879}]}, {"text": "The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7165622264146805}, {"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9597461223602295}, {"text": "SMT", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.9933534860610962}, {"text": "machine translation evaluation", "start_pos": 165, "end_pos": 195, "type": "TASK", "confidence": 0.8117780884106954}]}, {"text": "One possible reason could be the unavailability of accurate discourse parsers.", "labels": [], "entities": []}, {"text": "However, this situation is likely to change given the most recent advances in automatic discourse analysis).", "labels": [], "entities": [{"text": "automatic discourse analysis", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.5978109935919443}]}, {"text": "We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics.", "labels": [], "entities": [{"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9296426177024841}, {"text": "MT evaluation", "start_pos": 194, "end_pos": 207, "type": "TASK", "confidence": 0.9159703850746155}]}, {"text": "While in this work we focus on the latter, we think that the former is also within reach, and that SMT systems would benefit from preserving the coherence relations in the source language when generating target-language translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9867026209831238}]}, {"text": "In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9106347858905792}]}, {"text": "We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (; then, we show that they can help improve a number of MT evaluation metrics at the segment-and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.905179888010025}, {"text": "WMT11", "start_pos": 260, "end_pos": 265, "type": "DATASET", "confidence": 0.9537296295166016}, {"text": "WMT12 metrics shared tasks", "start_pos": 274, "end_pos": 300, "type": "DATASET", "confidence": 0.8277667015790939}]}, {"text": "These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties.", "labels": [], "entities": []}, {"text": "Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses.", "labels": [], "entities": []}, {"text": "Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9636085629463196}]}, {"text": "Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO) and MIRA (, which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses.", "labels": [], "entities": [{"text": "MT tuning", "start_pos": 163, "end_pos": 172, "type": "TASK", "confidence": 0.8939788341522217}, {"text": "MIRA", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.8499484062194824}, {"text": "reranking n-best lists of translation hypotheses", "start_pos": 276, "end_pos": 324, "type": "TASK", "confidence": 0.748340884844462}]}], "datasetContent": [{"text": "In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9702432751655579}, {"text": "WMT11 metrics shared tasks", "start_pos": 69, "end_pos": 95, "type": "DATASET", "confidence": 0.9018985778093338}, {"text": "translations into English", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.8722328941027323}]}, {"text": "This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation campaigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English (CS-EN), French-English (FR-EN), German-English (DE-EN), and Spanish-English (ES-EN); as well as a dataset with the English references.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9070641398429871}, {"text": "WMT11 MT evaluation", "start_pos": 81, "end_pos": 100, "type": "DATASET", "confidence": 0.7732757329940796}]}, {"text": "We measured the correlation of the metrics with the human judgments provided by the organizers.", "labels": [], "entities": []}, {"text": "The judgments represent rankings of the output of five systems chosen at random, fora particular sentence, also chosen at random.", "labels": [], "entities": []}, {"text": "Note that each judgment effectively constitutes 10 pairwise system rankings.", "labels": [], "entities": []}, {"text": "The overall coverage, i.e. the number of unique sentences that were evaluated, was only a fraction of the total; the total number of judgments, along with other information of the datasets are shown in.", "labels": [], "entities": []}, {"text": "In this study, we evaluate to what extent existing evaluation metrics can benefit from additional discourse information.", "labels": [], "entities": []}, {"text": "To do so, we contrast different MT evaluation metrics with and without discourse information.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8982114493846893}]}, {"text": "The evaluation metrics we used are described below.", "labels": [], "entities": []}, {"text": "We used the freely available version of the ASIYA toolkit 4 in order to extend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task.", "labels": [], "entities": [{"text": "ASIYA toolkit 4", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.7842238148053488}, {"text": "WMT12 metrics task", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.6477266152699789}]}, {"text": "ASIYA () is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic information.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9860945045948029}]}, {"text": "For reproducibility, below we explain the individual metrics with the exact names required by the toolkit to calculate them.", "labels": [], "entities": []}, {"text": "First, we used ASIYA's ULC (Gim\u00e9nez and M` arquez, 2010b), which was the best performing metric at the system and the segment levels at the WMT08 and WMT09 metrics tasks.", "labels": [], "entities": [{"text": "ULC", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.4888370633125305}, {"text": "WMT08 and WMT09 metrics tasks", "start_pos": 140, "end_pos": 169, "type": "DATASET", "confidence": 0.7337527513504029}]}, {"text": "This is a uniform linear combination of 12 individual metrics.", "labels": [], "entities": []}, {"text": "From the original ULC, we only replaced TER and Meteor individual metrics by newer versions taking into account synonymy lookup and paraphrasing: TERp-A and METEOR-pa in ASIYA's terminology.", "labels": [], "entities": [{"text": "TER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.985565721988678}, {"text": "TERp-A", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9597020745277405}, {"text": "METEOR-pa", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9762775301933289}]}, {"text": "We will call this combined metric Asiya-0809 in our experiments.", "labels": [], "entities": []}, {"text": "To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (), NIST), TER), ROUGE-W (, and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stem-ming) and METEOR-sy (+synonyms).", "labels": [], "entities": [{"text": "WMT12 metrics task", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7607637643814087}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9988077878952026}, {"text": "TER", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9937161803245544}, {"text": "ROUGE-W", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9943749904632568}, {"text": "METEOR-sy", "start_pos": 299, "end_pos": 308, "type": "METRIC", "confidence": 0.7942272424697876}]}, {"text": "The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section.", "labels": [], "entities": [{"text": "Asiya-0809", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.9123566150665283}]}, {"text": "The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores.", "labels": [], "entities": []}, {"text": "We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group: 1.", "labels": [], "entities": []}, {"text": "Combination of five metrics based on lexical similarity: BLEU, NIST, METEOR-ex, ROUGE-W, and TERp-A.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9990596175193787}, {"text": "NIST", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.6779608726501465}, {"text": "METEOR-ex", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9738040566444397}, {"text": "ROUGE-W", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9885457158088684}, {"text": "TERp-A", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9748959541320801}]}, {"text": "Combination of four metrics ba-sed on syntactic information from constituency and dependency parse trees: 'CP-STM-4', 'DP-HWCM c-4', 'DP-HWCM r-4', and 'DP-Or(*)'.", "labels": [], "entities": []}, {"text": "In this section, we explore how discourse information can be used to improve machine translation evaluation metrics.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.8801454703013102}]}, {"text": "Below we present the evaluation results at the system-and segment-level, using our two basic metrics on discourse trees (Section 3.1), which are referred to as DR and DR-LEX.", "labels": [], "entities": []}, {"text": "In our experiments, we only consider translation into English, and use the data described in.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9767529368400574}]}, {"text": "For evaluation, we follow the setup of the metrics task of WMT12): at the system-level, we use the official script from WMT12 to calculate the Spearman's correlation, where higher absolute values indicate better metrics performance; at the segment-level, we use Kendall's Tau for measuring correlation, where negative values are worse than positive ones.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9772236347198486}, {"text": "WMT12", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.9731937646865845}, {"text": "Spearman's correlation", "start_pos": 143, "end_pos": 165, "type": "METRIC", "confidence": 0.7165220181147257}, {"text": "correlation", "start_pos": 290, "end_pos": 301, "type": "METRIC", "confidence": 0.9712473750114441}]}, {"text": "In our experiments, we combine DR and DR-LEX to other metrics in two different ways: using uniform linear interpolation (at system-and segment-level), and using a tuned linear interpolation for the segment-level.", "labels": [], "entities": [{"text": "DR", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.8581898212432861}]}, {"text": "We only present the average results overall four language pairs.", "labels": [], "entities": []}, {"text": "For simplicity, in our tables we show results divided into evaluation groups:  and DR-LEX.", "labels": [], "entities": [{"text": "DR-LEX", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9121063947677612}]}, {"text": "2. Group II: includes the metrics that participated in the WMT12 metrics task, excluding metrics which did not have results for all language pairs.", "labels": [], "entities": [{"text": "WMT12 metrics task", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6349071860313416}]}, {"text": "3. Group III: contains other important evaluation metrics, which were not considered in the WMT12 metrics task: NIST and ROUGE for both system-and segment-level, and BLEU and TER at segment-level.", "labels": [], "entities": [{"text": "WMT12 metrics task", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.5033113559087118}, {"text": "NIST", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9005929231643677}, {"text": "ROUGE", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9947651624679565}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9988504648208618}, {"text": "TER", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9792832732200623}]}, {"text": "4. Group IV: includes the metric combinations calculated with ASIYA and described in Section 4.", "labels": [], "entities": [{"text": "ASIYA", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.7667603492736816}]}, {"text": "For each metric in groups II, III and IV, we present the results for the original metric as well for the linear interpolation of that metric with DR and with DR-LEX.", "labels": [], "entities": []}, {"text": "The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that degrade are in italic.", "labels": [], "entities": [{"text": "DR", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9085158705711365}]}, {"text": "Furthermore, we also present overall results for: (i) the average score overall metrics, excluding DR and DR-LEX, and (ii) the differences in the correlations for the DR/DR-LEX-combined and the original metrics.", "labels": [], "entities": []}, {"text": "Results on WMT12 at the system-level.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9050337672233582}]}, {"text": "Spearman's correlation with human judgments.", "labels": [], "entities": []}, {"text": "shows the system-level experimental results for WMT12.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.6183298826217651}]}, {"text": "We can see that DR is already competitive by itself: on average, it has a correlation of .807, very close to BLEU and TER scores (.810 and .812, respectively).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9989184141159058}, {"text": "TER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.988339900970459}]}, {"text": "Moreover, DR yields improvements when combined with 15 of the 19 metrics; worsening only four of the metrics.", "labels": [], "entities": [{"text": "DR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.997112512588501}]}, {"text": "Overall, we observe an average improvement of +.024, in the correlation with the human judgments.", "labels": [], "entities": []}, {"text": "This suggests that DR contains information that is complementary to that used by the other metrics.", "labels": [], "entities": []}, {"text": "Note that this is true both for the individual metrics from groups II and III, as well as for the metric combinations in group IV.", "labels": [], "entities": []}, {"text": "Combinations in the last group involve several metrics that already use linguistic information at different levels and are hard to improve over; yet, adding DR does improve, which shows that it has some complementary information to offer.", "labels": [], "entities": []}, {"text": "As expected, DR-LEX performs better than DR since it is lexicalized (at the unigram level), and also gives partial credit to correct structures.", "labels": [], "entities": []}, {"text": "Individually, DR-LEX outperforms most of the metrics from group II, and ranks as the second best metric in that group.", "labels": [], "entities": [{"text": "DR-LEX", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9479948282241821}]}, {"text": "Furthermore, when combined with individual metrics in group II, DR-LEX is able to improve consistently over each one of them.", "labels": [], "entities": [{"text": "DR-LEX", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.7816750407218933}]}, {"text": "Note that, even though DR-LEX has better individual performance than DR, it does not yield improvements when combined with most of the metrics in group IV.", "labels": [], "entities": []}, {"text": "8 However, overall metrics and all language pairs, DR-LEX is able to obtain an average improvement in correlation of +.035, which is remarkably higher than that of DR.", "labels": [], "entities": [{"text": "correlation", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9969651103019714}]}, {"text": "Thus, we can conclude that at the system-level, adding discourse information to a metric, even using the simplest of the combination schemes, is a good idea for most of the metrics, and can help to significantly improve the correlation with human judgments.", "labels": [], "entities": []}, {"text": "shows the results for WMT12 at the segment-level.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9102128148078918}]}, {"text": "We can see that DR performs badly, with a high negative Kendall's Tau of -.433.", "labels": [], "entities": [{"text": "Kendall's Tau", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.8579620520273844}]}, {"text": "This should not be surprising: (a) the discourse tree structure alone does not contain enough information fora good evaluation at the segment-level, and (b) this metric is more sensitive to the quality of the DT, which can be wrong or void.", "labels": [], "entities": []}, {"text": "Additionally, DR is more likely to produce a high number of ties, which is harshly penalized by WMT12's definition of Kendall's Tau.", "labels": [], "entities": [{"text": "WMT12's definition of Kendall's Tau", "start_pos": 96, "end_pos": 131, "type": "DATASET", "confidence": 0.8908477681023734}]}, {"text": "Conversely, ties and incomplete discourse analysis were not a problem at the system-level, where evidence from all 3,003 test sentences is aggregated, and allows to rank systems more precisely.", "labels": [], "entities": []}, {"text": "Due to the low score of DR as an individual metric, it fails to yield improvements when uniformly combined with other metrics.", "labels": [], "entities": [{"text": "DR", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9759395122528076}]}, {"text": "Again, DR-LEX is better than DR; with a positive Tau of +.133, yet as an individual metric, it ranks poorly compared to other metrics in group II.", "labels": [], "entities": [{"text": "DR-LEX", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.8639276623725891}, {"text": "DR", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9724686741828918}, {"text": "Tau", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9294705390930176}]}, {"text": "However, when linearly combined with other metrics, DR-LEX outperforms 14 of the 19 metrics in.", "labels": [], "entities": [{"text": "DR-LEX", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.8703011274337769}]}, {"text": "Across all metrics, DR-LEX yields an average Tau improvement of +.026, i.e. from .165 to .190.", "labels": [], "entities": [{"text": "DR-LEX", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8804708123207092}, {"text": "Tau improvement", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.9786581993103027}]}, {"text": "This is a large improvement, taking into account that the combinations are just uniform linear combinations.", "labels": [], "entities": []}, {"text": "In subsection 5.4, we present the results of tuning the linear combination in a discriminative way.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of systems (systs), judgments  (ranks), unique sentences (sents), and different  judges (judges) for the different language pairs, for  the human evaluation of the WMT12 and WMT11  shared tasks.", "labels": [], "entities": [{"text": "WMT12 and WMT11  shared tasks", "start_pos": 181, "end_pos": 210, "type": "DATASET", "confidence": 0.732690155506134}]}, {"text": " Table 3: Results on WMT12 at the segment-level.  Kendall's Tau with human judgments.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.701398491859436}]}, {"text": " Table 4: Results on WMT12 at the segment- level: tuning with cross-validation on WMT12.  Kendall's Tau with human judgments.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8769145011901855}, {"text": "WMT12", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.9727482199668884}]}, {"text": " Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.9710156321525574}, {"text": "WMT12", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9812840819358826}]}]}