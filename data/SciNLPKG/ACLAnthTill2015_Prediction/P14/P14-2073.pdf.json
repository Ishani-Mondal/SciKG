{"title": [{"text": "Particle Filter Rejuvenation and Latent Dirichlet Allocation", "labels": [], "entities": [{"text": "Particle Filter Rejuvenation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7161334951718649}]}], "abstractContent": [{"text": "Previous research has established several methods of online learning for latent Dirichlet allocation (LDA).", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.7386587162812551}]}, {"text": "However , streaming learning for LDA-allowing only one pass over the data and constant storage complexity-is not as well explored.", "labels": [], "entities": []}, {"text": "We use reservoir sampling to reduce the storage complexity of a previously-studied online algorithm, namely the particle filter, to constant.", "labels": [], "entities": []}, {"text": "We then show that a simpler particle filter implementation performs just as well, and that the quality of the initialization dominates other factors of performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "We extend a popular model, latent Dirichlet allocation (LDA), to unbounded streams of documents.", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.7019182691971461}]}, {"text": "In order for inference to be practical in this setting it must use constant space asymptotically and run in pseudo-linear time, perhaps O(n) or O(n log n).", "labels": [], "entities": []}, {"text": "presented a method for LDA inference based on particle filters, where a sample set of models is updated online with each new token observed from a stream.", "labels": [], "entities": [{"text": "LDA inference", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9663362205028534}]}, {"text": "In general, these models should be regularly resampled and rejuvenated using Markov Chain Monte Carlo (MCMC) steps over the history in order to improve the efficiency of the particle filter ().", "labels": [], "entities": []}, {"text": "The particle filter of rejuvenates over independent draws from the history by storing all past observations and states.", "labels": [], "entities": []}, {"text": "This algorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense).", "labels": [], "entities": []}, {"text": "In the current work we propose using reservoir sampling in the rejuvenation step to reduce the storage complexity of the particle filter to O(1).", "labels": [], "entities": [{"text": "O", "start_pos": 140, "end_pos": 141, "type": "METRIC", "confidence": 0.9931636452674866}]}, {"text": "This improvement is practically useful in the large-data setting and is also scientifically interesting in that it recovers some of the cognitive plausibility which originally motivated.", "labels": [], "entities": []}, {"text": "However, in experiments on the dataset studied by, we show that rejuvenation does not benefit the particle filter's performance.", "labels": [], "entities": []}, {"text": "Rather, performance is dominated by the effects of random initialization (a problem for which we provide a correction while abiding by the same constraints as).", "labels": [], "entities": []}, {"text": "This result re-opens the question of whether rejuvenation is of practical importance in online learning for static Bayesian models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our particle filter on three datasets studied in: diff3, rel3, and sim3.", "labels": [], "entities": []}, {"text": "Each of these datasets is a collection of posts under three categories from the 20 Newsgroups dataset.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.7126893401145935}]}, {"text": "We use a 60% training/40% testing split of this data that is available online.", "labels": [], "entities": []}, {"text": "We preprocess the data by splitting each line on non-alphabet characters, converting the resulting tokens to lower-case, and filtering out any tokens that appear in a list of common English stop words.", "labels": [], "entities": []}, {"text": "In addition, we remove the header of every file and filter every line that does not contain a non-trailing space (which removes embedded ASCII-encoded attachments).", "labels": [], "entities": []}, {"text": "Finally, we shuffle the order of the documents.", "labels": [], "entities": []}, {"text": "After these steps, we compute the vocabulary for each dataset as the set of all non-singleton types in the training data augmented with a special out-of-vocabulary symbol.", "labels": [], "entities": []}, {"text": "During training we report the out-of-sample NMI, calculated by holding the word proportions \u03c6 fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document.", "labels": [], "entities": [{"text": "NMI", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.8023290634155273}]}, {"text": "Two Gibbs sweeps have been shown to yield good performance in practice (; we increase the number of sweeps to five after inspecting the stability on our dataset.", "labels": [], "entities": []}, {"text": "The variance of the particle filter is often large, so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction.", "labels": [], "entities": []}, {"text": "Our first set of experiments has a similar parameterization 3 to the experiments of except we draw the rejuvenation sequence from a reservoir.", "labels": [], "entities": []}, {"text": "We initialize the particle filter with 200 Gibbs sweeps on the first 10% of each dataset.", "labels": [], "entities": []}, {"text": "Then, for each dataset, for rejuvenation disabled, rejuvenation based on a reservoir of size 1000, and rejuvenation based on the entire history (in turn), we perform 30 runs of the particle filter from that fixed initial model.", "labels": [], "entities": []}, {"text": "Our results) resemble those of; we believe the discrepancies are mostly attributable to differences in preprocessing.", "labels": [], "entities": []}, {"text": "In these experiments, the initial model was not chosen arbitrarily.", "labels": [], "entities": []}, {"text": "Rather, an initial model that yielded out-of-sample NMI close to the initial outof-sample NMI scores reported in the previous  study was chosen from a set of 100 candidates.", "labels": [], "entities": []}, {"text": "We now investigate the significance of the initial model selection step used in the previous experiments.", "labels": [], "entities": []}, {"text": "We run anew set of experiments in which the reservoir size is held fixed at 1000 and the size of the initialization sample is varied.", "labels": [], "entities": []}, {"text": "Specifically, we vary the size of the initialization sample, in documents, between zero (corresponding to no Gibbs initialization), 30, 100, and 300, and also perform a run of batch Gibbs sampling (with no particle filter).", "labels": [], "entities": []}, {"text": "In each case, 2000 Gibbs sweeps are performed.", "labels": [], "entities": []}, {"text": "In these experiments, the initial models are not held fixed; for each of the 30 runs for each dataset, the initial model was generated by a different Gibbs chain.", "labels": [], "entities": []}, {"text": "The results for these experiments, depicted in, indicate that the size of the initialization sample improves mean NMI and reduces variance, and that the variance of the particle filter itself is dominated by the variance of the initial model.", "labels": [], "entities": [{"text": "NMI", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.8411698341369629}, {"text": "variance", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9670947790145874}]}, {"text": "We observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm, as measured by NMI.", "labels": [], "entities": [{"text": "NMI", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.870802104473114}]}, {"text": "With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model.", "labels": [], "entities": []}, {"text": "Thus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set, setting the particle filter's initial model to the model out of these 20 with the highest in-sample NMI.", "labels": [], "entities": []}, {"text": "This procedure is performed independently for each run of the particle filter.", "labels": [], "entities": []}, {"text": "We may not always have labeled data for initialization, so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80% of the initialization sample, heldout perplexity (per word) is estimated on the remaining 20%, using a first-moment particle learning approximation, and the particle filter is started from the model out of these 20 with the lowest held-out perplexity.", "labels": [], "entities": [{"text": "initialization", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.9628707766532898}]}, {"text": "The results, shown in, show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity.", "labels": [], "entities": []}], "tableCaptions": []}