{"title": [{"text": "Simple extensions fora reparameterised IBM Model 2", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9369897445042928}]}], "abstractContent": [{"text": "A modification of a reparameterisation of IBM Model 2 is presented, which makes the model more flexible, and able to model a preference for aligning to words to either the right or left, and take into account POS tags on the target side of the corpus.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9159452716509501}]}, {"text": "We show that this extension has a very small impact on training times, while obtaining better alignments in terms of BLEU scores.", "labels": [], "entities": [{"text": "alignments", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.97173011302948}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9994008541107178}]}], "introductionContent": [{"text": "Word alignment is at the basis of most statistical machine translation.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7672418057918549}, {"text": "statistical machine translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6197420756022135}]}, {"text": "The models that are generally used are often slow to train, and have a large number of parameters.", "labels": [], "entities": []}, {"text": "present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.941249152024587}]}, {"text": "While this model is very effective, it also has a very low number of parameters, and as such doesn't have a large amount of expressive power.", "labels": [], "entities": []}, {"text": "For one thing, it forces the model to consider alignments on both sides of the diagonal equally likely.", "labels": [], "entities": []}, {"text": "However, it isn't clear that this is the case, as for some languages an alignment to earlier or later in the sentence (above or below the diagonal) could be common, due to word order differences.", "labels": [], "entities": []}, {"text": "For example, when aligning to Dutch, it maybe common for one verb to be aligned near the end of the sentence that would beat the beginning in English.", "labels": [], "entities": []}, {"text": "This would mean most of the other words in the sentence would also align slightly away from the diagonal in one direction.", "labels": [], "entities": []}, {"text": "shows an example sentence in which this happens.", "labels": [], "entities": []}, {"text": "Here, a circle denotes an alignment, and darker squares are more likely under the alignment model.", "labels": [], "entities": []}, {"text": "In this case the modified Model 2 would simply make both directions equally likely, where we would really like for only one direction to be more likely.", "labels": [], "entities": []}, {"text": "In some cases it could be that the prior probability fora word alignment should be off the diagonal.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.688005730509758}]}, {"text": "Furthermore, it is common in word alignment to take word classes into account.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7808753252029419}]}, {"text": "This is commonly implemented for the HMM alignment model as well as Models 4 and 5.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.8391249179840088}]}, {"text": "show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER).", "labels": [], "entities": [{"text": "Alignment Error Rate (AER)", "start_pos": 64, "end_pos": 90, "type": "METRIC", "confidence": 0.9689332147439321}]}, {"text": "This is not implemented for Model 2, as it already has an alignment model that is dependent on both source and target length, and the position in both sentences, and adding a dependency to word classes would make the the Model even more prone to overfitting than it already is.", "labels": [], "entities": []}, {"text": "However, using the reparameterization in) would leave the model simple enough even with a relatively large amount of word classes.", "labels": [], "entities": []}, {"text": "shows an example of how the model extensions could benefit word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7815369963645935}]}, {"text": "In the example, all the Dutch words have a different word class, and so can have different gradients for alignment probability over the english words.", "labels": [], "entities": []}, {"text": "If the model has learned that prepositions and nouns are more likely to align to words later in the sentence, it could have a lower lambda for both word classes, resulting in a less steep slope.", "labels": [], "entities": []}, {"text": "If we also split lambda into two variables, we can get alignment probabilities as shown above for the Dutch word 'de', where aligning to one side of the diagonal is made more likely for some word classes.", "labels": [], "entities": []}, {"text": "Finally, instead of just having one side of the diagonal less steep than the other, it maybe useful to instead move the peak of the alignment probability function off the diagonal, while keeping it equally likely.", "labels": [], "entities": []}, {"text": "In, this is done for the past participle 'gezien'.", "labels": [], "entities": []}, {"text": "We will present a simple model for adding the above extensions to achieve the above (splitting the parameter, adding an offset and conditioning the parameters on the POS tag of the target word) in section 2, results on a set of experiments in section 3 and present our conclusions in section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Token counts and average amount of time  to train models (and separately training time for  Model 4) on original corpora in one direction in  hours, by corpus.", "labels": [], "entities": []}, {"text": " Table 2: AER results on Chinese-English and  French-English data sets", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9898110628128052}, {"text": "French-English data sets", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.7811263203620911}]}, {"text": " Table 3: BLEU results on Chinese-English and  French-English data sets", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975663423538208}, {"text": "French-English data sets", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.7756606141726176}]}]}