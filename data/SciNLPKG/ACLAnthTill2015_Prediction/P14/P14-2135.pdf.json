{"title": [{"text": "Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More", "labels": [], "entities": [{"text": "Improving Multi-Modal Representations", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8923876086870829}, {"text": "Image Dispersion", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6799227595329285}]}], "abstractContent": [{"text": "Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition.", "labels": [], "entities": []}, {"text": "However, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others.", "labels": [], "entities": []}, {"text": "We propose an unsupervised method to determine whether to include perceptual input fora concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings.", "labels": [], "entities": []}, {"text": "The method relies solely on image data, and can be applied to a variety of other NLP tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system ().", "labels": [], "entities": []}, {"text": "Such models extract information about the perceptible characteristics of words from data collected in property norming experiments or directly from 'raw' data sources such as images).", "labels": [], "entities": []}, {"text": "This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning.", "labels": [], "entities": []}, {"text": "Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (.", "labels": [], "entities": [{"text": "predicting compositionality", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.930169403553009}]}, {"text": "Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity.", "labels": [], "entities": []}, {"text": "Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (), it can in fact be detrimental to representations of abstract concepts ().", "labels": [], "entities": []}, {"text": "Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts).", "labels": [], "entities": []}, {"text": "Indeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory, posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded only in the linguistic modality.", "labels": [], "entities": []}, {"text": "Existing multi-modal architectures generally extract and process all the information from their specified sources of perceptual input.", "labels": [], "entities": []}, {"text": "Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types.", "labels": [], "entities": []}, {"text": "The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts.", "labels": [], "entities": []}, {"text": "For instance, 72% of word tokens in the British National Corpus ( were rated by contributors to the University of South Florida dataset (USF) () as more abstract than the noun war, a concept that many would consider quite abstract.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.9401117364565531}, {"text": "University of South Florida dataset (USF)", "start_pos": 100, "end_pos": 141, "type": "DATASET", "confidence": 0.8547568023204803}]}, {"text": "In light of these considerations, we propose a novel algorithm for approximating conceptual concreteness.", "labels": [], "entities": [{"text": "approximating conceptual concreteness", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.8652087251345316}]}, {"text": "Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in captur-ing the semantic similarity of concepts.", "labels": [], "entities": []}, {"text": "Further, our algorithm constitutes the first means of quantifying conceptual concreteness that does not rely on labor-intensive experimental studies or annotators.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate the application of this unsupervised concreteness metric to the semantic classification of adjective-noun pairs, an existing NLP task to which concreteness data has proved valuable previously.", "labels": [], "entities": [{"text": "semantic classification of adjective-noun pairs", "start_pos": 88, "end_pos": 135, "type": "TASK", "confidence": 0.804647022485733}]}], "datasetContent": [{"text": "Our experiments focus on multi-modal models that extract their perceptual input automatically from images.", "labels": [], "entities": []}, {"text": "Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation.", "labels": [], "entities": [{"text": "human concept acquisition", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.799863338470459}]}, {"text": "They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets.", "labels": [], "entities": []}, {"text": "We use Google Images as our image source, and extract the first n image results for each concept word.", "labels": [], "entities": []}, {"text": "It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr ().", "labels": [], "entities": []}, {"text": "Other potential sources, such as ImageNet () or the ESP Game Dataset (Von), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets.", "labels": [], "entities": [{"text": "ESP Game Dataset (Von)", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.8853796621163686}]}, {"text": "We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity -a standard measure for evaluating the quality of representations (see e.g.).", "labels": [], "entities": []}, {"text": "To test the ability of our model to capture concept similarity, we measure correlations with WordSim353 (), a selection of 353 concept pairs together with a similarity rating provided by human annotators.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 93, "end_pos": 103, "type": "DATASET", "confidence": 0.9507169127464294}]}, {"text": "WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. ().", "labels": [], "entities": [{"text": "WordSim", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9233877062797546}]}, {"text": "As a complementary gold-standard, we use the University of South Florida Norms (USF) ().", "labels": [], "entities": [{"text": "University of South Florida Norms (USF)", "start_pos": 45, "end_pos": 84, "type": "DATASET", "confidence": 0.814644355326891}]}, {"text": "This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs.", "labels": [], "entities": []}, {"text": "The USF norms have been used in many previous studies to evaluate semantic representations).", "labels": [], "entities": [{"text": "USF norms", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8701708614826202}]}, {"text": "The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators.", "labels": [], "entities": [{"text": "USF evaluation set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9077471494674683}]}, {"text": "We create a representative evaluation set of USF pairs as follows.", "labels": [], "entities": [{"text": "USF", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.7149937152862549}]}, {"text": "We randomly sample 100 concepts from the upper quartile and 100 concepts from the lower quartile of a list of all USF concepts ranked by concreteness.", "labels": [], "entities": [{"text": "USF", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8545053601264954}]}, {"text": "We denote these sets C, for concrete, and A for abstract respectively.", "labels": [], "entities": []}, {"text": "We then extract all pairs (w 1 , w 2 ) in the USF dataset such that both w 1 and w 2 are in A\u222aC.", "labels": [], "entities": [{"text": "USF dataset", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9852786064147949}]}, {"text": "This yields an evaluation set of 903 pairs, of which 304 are such that w 1 , w 2 \u2208 C and 317 are such that w 1 , w 2 \u2208 A.", "labels": [], "entities": []}, {"text": "The images used in our experiments and the evaluation gold-standards can be downloaded from http://www.cl.cam.ac.uk/ \u02dc dk427/dispersion.html.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Concepts with highest and lowest image  dispersion scores in our evaluation set, and con- creteness ratings from the USF dataset.", "labels": [], "entities": [{"text": "USF dataset", "start_pos": 127, "end_pos": 138, "type": "DATASET", "confidence": 0.9889300167560577}]}]}