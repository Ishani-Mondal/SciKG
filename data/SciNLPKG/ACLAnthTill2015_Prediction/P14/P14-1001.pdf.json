{"title": [{"text": "Learning Ensembles of Structured Prediction Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is assumed.", "labels": [], "entities": []}, {"text": "This includes a number of ran-domized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels.", "labels": [], "entities": []}, {"text": "We also report the results of extensive experiments with these algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "We study the problem of learning accurate ensembles of structured prediction experts.", "labels": [], "entities": [{"text": "learning accurate ensembles of structured prediction experts", "start_pos": 24, "end_pos": 84, "type": "TASK", "confidence": 0.6312195020062583}]}, {"text": "Ensemble methods are widely used in machine learning and have been shown to be often very effective).", "labels": [], "entities": [{"text": "machine learning", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.779113233089447}]}, {"text": "However, ensemble methods and their theory have been developed primarily for binary classification or regression tasks.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.7040647864341736}]}, {"text": "Their techniques do not readily apply to structured prediction problems.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.6574799716472626}]}, {"text": "While it is straightforward to combine scalar outputs fora classification or regression problem, it is less clear how to combine structured predictions such as phonemic pronunciation hypotheses, speech recognition lattices, parse trees, or alternative machine translations.", "labels": [], "entities": []}, {"text": "Consider for example the problem of devising an ensemble method for pronunciation, a critical component of modern speech recognition ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7284421026706696}]}, {"text": "Often, several pronunciation models or experts are available for transcribing words into sequences of phonemes.", "labels": [], "entities": []}, {"text": "These models may have been derived using other machine learning algorithms or they maybe based on carefully hand-crafted rules.", "labels": [], "entities": []}, {"text": "In general, none of these pronunciation experts is fully accurate and each expert maybe making mistakes at different positions along the output sequence.", "labels": [], "entities": []}, {"text": "One can hope that a model that patches together the pronunciation of different experts could achieve a superior performance.", "labels": [], "entities": []}, {"text": "Similar ensemble structured prediction problems arise in other tasks, including machine translation, part-of-speech tagging, optical character recognition and computer vision, with structures or substructures varying with each task.", "labels": [], "entities": [{"text": "ensemble structured prediction", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.7418019572893778}, {"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8278254270553589}, {"text": "part-of-speech tagging", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7105920761823654}, {"text": "optical character recognition", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.6023282011349996}, {"text": "computer vision", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.7413675487041473}]}, {"text": "We seek to tackle all of these problems simultaneously and consider the general setting where the label or output associated to an input x \u2208 X is a structure y \u2208 Y that can be decomposed and represented by l substructures y 1 , . .", "labels": [], "entities": []}, {"text": ", y l . For the pronunciation example just discussed, x is a specific word or word sequence and y its phonemic transcription.", "labels": [], "entities": []}, {"text": "A natural choice for the substructures y k is then the individual phonemes forming y.", "labels": [], "entities": []}, {"text": "Other possible choices include n-grams of consecutive phonemes or more general subsequences.", "labels": [], "entities": []}, {"text": "We will assume that the loss function considered admits an additive decomposition over the substructures, as is common in structured prediction.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7283120453357697}]}, {"text": "We also assume access to a set of structured prediction experts h 1 , . .", "labels": [], "entities": []}, {"text": ", hp that we treat as black boxes.", "labels": [], "entities": []}, {"text": "Given an input x \u2208 X , each expert predicts a structure h j (x) = (h 1 j (x), . .", "labels": [], "entities": []}, {"text": ", h l j (x)).", "labels": [], "entities": []}, {"text": "The hypotheses h j maybe the output of a structured prediction algorithm such as Conditional Random Fields (), Averaged Perceptron),), Max Margin Markov Networks () or the Regression Technique for), or some other algorithmic or human expert.", "labels": [], "entities": []}, {"text": "Given a labeled training sample (x 1 , y 1 ), . .", "labels": [], "entities": []}, {"text": ", (x m , y m ), our objective is to use the predictions of these experts 1 to form an accurate ensemble.", "labels": [], "entities": []}, {"text": "Most of the references just mentioned do not give a rigorous theoretical justification for the techniques proposed.", "labels": [], "entities": []}, {"text": "We are not aware of any prior theoretical analysis for the ensemble structured prediction problem we consider.", "labels": [], "entities": [{"text": "ensemble structured prediction", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6781400839487711}]}, {"text": "Here, we present two families of algorithms for learning ensembles of structured prediction rules that both perform well in practice and enjoy strong theoretical guarantees.", "labels": [], "entities": []}, {"text": "In Section 3, we develop ensemble methods based on on-line algorithms.", "labels": [], "entities": []}, {"text": "To do so, we extend existing on-line-to-batch conversions to our more general setting.", "labels": [], "entities": []}, {"text": "In Section 4, we present anew boosting-style algorithm which is applicable even with a large set of classes as in the problem we consider, and for which we present margin-based learning guarantees.", "labels": [], "entities": []}, {"text": "Section 5 reports the results of our extensive experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a number of artificial and real-world data sets for our experiments.", "labels": [], "entities": []}, {"text": "For each data set, we performed 10-fold cross-validation with disjoint training sets.", "labels": [], "entities": []}, {"text": "We report the average error for each task.", "labels": [], "entities": []}, {"text": "In addition to the H MVote , H Rand and H ESPBoost hypotheses, we experimented with two algorithms discussed in more detail in): a cross-validation on-line-tobatch conversion of the WMWP algorithm, H CV , a majority-vote on-line-to-batch conversion with FPL, H FPL , and a cross-validation on-line-tobatch conversion with FPL, H FPL-CV . Finally, we compare with the H SLE algorithm of.", "labels": [], "entities": [{"text": "H MVote", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.8630666136741638}, {"text": "FPL", "start_pos": 261, "end_pos": 264, "type": "METRIC", "confidence": 0.9271965622901917}, {"text": "FPL", "start_pos": 322, "end_pos": 325, "type": "DATASET", "confidence": 0.5924152731895447}]}], "tableCaptions": [{"text": " Table 1: Average Normalized Hamming Loss,  ADS1 and ADS2. \u03b2 ADS1 = 0.95, \u03b2 ADS2 = 0.95,  T SLE = 100, \u03b4 = 0.05.", "labels": [], "entities": [{"text": "Average Normalized Hamming Loss", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.7924726009368896}, {"text": "T SLE", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9217941761016846}, {"text": "\u03b4", "start_pos": 103, "end_pos": 104, "type": "METRIC", "confidence": 0.945935070514679}]}, {"text": " Table 4: Average edit distance, PDS1 and PDS2.  \u03b2 P DS1 = 0.85, \u03b2 P DS2 = 0.97, T SLE = 100,  \u03b4 = 0.05.", "labels": [], "entities": [{"text": "Average edit distance", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.870419979095459}, {"text": "T SLE", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9196420311927795}, {"text": "\u03b4", "start_pos": 95, "end_pos": 96, "type": "METRIC", "confidence": 0.9360336065292358}]}]}