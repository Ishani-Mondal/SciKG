{"title": [{"text": "Linguistic Considerations in Automatic Question Generation", "labels": [], "entities": [{"text": "Linguistic Considerations in Automatic Question Generation", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.6320209751526514}]}], "abstractContent": [{"text": "As students read expository text, comprehension is improved by pausing to answer questions that reinforce the material.", "labels": [], "entities": []}, {"text": "We describe an automatic question generator that uses semantic pattern recognition to create questions of varying depth and type for self-study or tutoring.", "labels": [], "entities": []}, {"text": "Throughout, we explore how linguistic considerations inform system design.", "labels": [], "entities": []}, {"text": "In the described system , semantic role labels of source sentences are used in a domain-independent manner to generate both questions and answers related to the source sentence.", "labels": [], "entities": []}, {"text": "Evaluation results show a 44% reduction in the error rate relative to the best prior systems, averaging overall metrics, and up to 61% reduction in the error rate on grammatical-ity judgments.", "labels": [], "entities": [{"text": "error rate", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9656303226947784}, {"text": "error rate", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9569419920444489}]}], "introductionContent": [{"text": "Studies of student learning show that answering questions increases depth of student learning, facilitates transfer learning, and improves students' retention of material (.", "labels": [], "entities": []}, {"text": "The aim of this work is to automatically generate questions for such pedagogical purposes.", "labels": [], "entities": []}], "datasetContent": [{"text": "This evaluation was conducted with one file (Chemistry: Bonds) which had 59 sentences, from which the system generated 142 questions.", "labels": [], "entities": []}, {"text": "The purpose of this evaluation was to determine if any patterns consistently produce poor questions.", "labels": [], "entities": []}, {"text": "The average linguistics score per pattern in this evaluation was 5.0 to 4.18.", "labels": [], "entities": []}, {"text": "We were also interested to know if first predicates make better questions than later ones.", "labels": [], "entities": []}, {"text": "The average score by predicate position is shown in  Based on this sample of questions there is no significant difference in linguistic scores for questions generated at various predicate positions.", "labels": [], "entities": []}, {"text": "Some question generation systems simplify complex sentences in initial stages of their system.", "labels": [], "entities": [{"text": "question generation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7291637361049652}]}, {"text": "In our approach this is unnecessary, and simplifying could miss many valid questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Note that the Rating column  gives the average of the grammaticality, clarity and  naturalness scores.", "labels": [], "entities": [{"text": "clarity", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.998496413230896}]}, {"text": " Table 3: Predicate depth and question quality", "labels": [], "entities": [{"text": "Predicate depth", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7086732089519501}]}, {"text": " Table 4: Comparison with Heilman and Smith", "labels": [], "entities": []}, {"text": " Table 5: Comparison with Lindberg et al.", "labels": [], "entities": []}]}