{"title": [{"text": "Combining Multiple Knowledge Sources for Discourse Segmentation", "labels": [], "entities": [{"text": "Discourse Segmentation", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7217368632555008}]}], "abstractContent": [{"text": "We predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data.", "labels": [], "entities": []}, {"text": "We present two methods for developing seg-mentation algorithms from training data: hand tuning and machine learning.", "labels": [], "entities": [{"text": "hand tuning", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.7792060375213623}]}, {"text": "When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning).", "labels": [], "entities": []}], "introductionContent": [{"text": "Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. ().", "labels": [], "entities": []}, {"text": "However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another.", "labels": [], "entities": []}, {"text": "We have been engaged in a study addressing this gap.", "labels": [], "entities": []}, {"text": "In previous work (, we reported on a method for empirically validating global discourse units, and on our evaluation of algorithms to identify these units.", "labels": [], "entities": []}, {"text": "We found significant agreement among naive subjects on a discourse segmentation task, which suggests that global discourse units have some objective reality.", "labels": [], "entities": [{"text": "discourse segmentation task", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.7740780015786489}]}, {"text": "However, we also found poor correlation of three untuned algorithms (based on features of referential noun phrases, cue words, and pauses, respectively) with the subjects' segmentations.", "labels": [], "entities": []}, {"text": "In this paper, we discuss two methods for developing segmentation algorithms using multiple know-*Bellcore did not support the second author's work.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.9666805267333984}, {"text": "Bellcore", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9318098425865173}]}, {"text": "In section 2, we give a brief overview of related work and summarize our previous results.", "labels": [], "entities": []}, {"text": "In section 3, we discuss how linguistic features are coded and describe our evaluation.", "labels": [], "entities": []}, {"text": "In section 4, we present our analysis of the errors made by the best performing untuned algorithm, and anew algorithm that relies on enriched input features and multiple knowledge sources.", "labels": [], "entities": []}, {"text": "In section 5, we discuss our use of machine learning tools to automatically construct decision trees for segmentation from a large set of input features.", "labels": [], "entities": []}, {"text": "Both the hand tuned and automatically derived algorithms improve over our previous algorithms.", "labels": [], "entities": []}, {"text": "The primary benefit of the hand tuning is to identify new input features for improving performance.", "labels": [], "entities": [{"text": "hand tuning", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.750234991312027}]}, {"text": "Machine learning tools make it convenient to perform numerous experiments, to use large feature sets, and to evaluate results using crossvalidation.", "labels": [], "entities": []}, {"text": "We discuss the significance of our results and briefly compare the two methods in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The segmentation algorithms presented in the next two sections were developed by examining only a training set of narratives.", "labels": [], "entities": []}, {"text": "The algorithms are then evaluated by examining their performance in predicting segmentation on a separate test set.", "labels": [], "entities": [{"text": "predicting segmentation", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8610016107559204}]}, {"text": "We currently use 10 narratives for training and 5 narratives for testing.", "labels": [], "entities": []}, {"text": "(The remaining 5 narratives are reserved for future research.)", "labels": [], "entities": []}, {"text": "The 10 training narratives, as detailed in Section 5.", "labels": [], "entities": []}, {"text": "To quantify algorithm performance, we use the information retrieval metrics shown in.", "labels": [], "entities": []}, {"text": "Recall is the ratio of correctly hypothesized boundaries to target boundaries.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9771085977554321}]}, {"text": "Precision is the ratio of hypothesized boundaries that are correct to the total hypothesized boundaries.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.990375816822052}]}, {"text": "(Cf. for fallout and error.)", "labels": [], "entities": [{"text": "error", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.93172687292099}]}, {"text": "Ideal behavior would be to identify all and only the target boundaries: the values for band c in would thus both equal O, representing no errors.", "labels": [], "entities": [{"text": "O", "start_pos": 119, "end_pos": 120, "type": "METRIC", "confidence": 0.9873129725456238}]}, {"text": "The ideal values for recall, precision, fallout, and error are 1, 1, 0, and 0, while the worst values are 0, 0, 1, and 1.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993355870246887}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9992395639419556}, {"text": "error", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.988437831401825}]}, {"text": "To get an intuitive summary of overall performance, we also sum the deviation of the observed value from the ideal value for each metric: (1-recall) + (1-precision) + fallout + error.", "labels": [], "entities": [{"text": "error", "start_pos": 177, "end_pos": 182, "type": "METRIC", "confidence": 0.8760965466499329}]}, {"text": "The summed deviation for perfect performance is thus 0.", "labels": [], "entities": []}, {"text": "Finally, to interpret our quantitative results, we use the performance of our human subjects as a target goal for the performance of our algorithms (.", "labels": [], "entities": []}, {"text": "shows the average human performance for both the training and test sets of narratives.", "labels": [], "entities": []}, {"text": "Note that human performance is basically the same for both sets of narratives.", "labels": [], "entities": []}, {"text": "However, two factors prevent this performance from being closer to ideal (e.g., recall and precision of 1).", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9997879862785339}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9985070824623108}]}, {"text": "The first is the wide variation in the number of boundaries that subjects used, as discussed above.", "labels": [], "entities": []}, {"text": "The second is the inherently fuzzy nature of boundary location.", "labels": [], "entities": []}, {"text": "We discuss this second issue at length in (Passonnean and Litman, to appear), and present relaxed IR metrics that penalize near misses less heavily in (Litman and Passonneau, 1995).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average human performance.", "labels": [], "entities": []}, {"text": " Table 2. This is strong  evidence that the tuned algorithm is a better pre- dictor of segment boundaries than the original NP  algorithm. Nevertheless, the test results of condition  2 are much worse than the corresponding training re- sults, particularly for precision", "labels": [], "entities": []}, {"text": " Table 3: Performance on test set.", "labels": [], "entities": []}, {"text": " Table 4: Performance on training set.", "labels": [], "entities": []}, {"text": " Table 5: Performance on test set.", "labels": [], "entities": []}, {"text": " Table 6: Using 10-fold cross-validation.", "labels": [], "entities": []}]}