{"title": [{"text": "Bayesian Grammar Induction for Language Modeling", "labels": [], "entities": [{"text": "Bayesian Grammar Induction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5786821643511454}, {"text": "Language Modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7131758779287338}]}], "abstractContent": [{"text": "We describe a corpus-based induction algorithm for probabilistic context-free grammars.", "labels": [], "entities": []}, {"text": "The algorithm employs a greedy heuristic search within a Bayesian framework , and a post-pass using the Inside-Outside algorithm.", "labels": [], "entities": []}, {"text": "We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks.", "labels": [], "entities": []}, {"text": "In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques.", "labels": [], "entities": []}, {"text": "The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.", "labels": [], "entities": []}, {"text": "1 Introduction In applications such as speech recognition, handwriting recognition, and spelling correction, performance is limited by the quality of the language model utilized (7; 7; 7; 7).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8099177777767181}, {"text": "handwriting recognition", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.9302300810813904}, {"text": "spelling correction", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.9600888192653656}]}, {"text": "However, static language modeling performance has remained basically unchanged since the advent of n-gram language models forty years ago (7).", "labels": [], "entities": [{"text": "static language modeling", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.663823535044988}]}, {"text": "Yet, n-gram language models can only capture dependencies within an n-word window, where currently the largest practical n for natural language is three, and many dependencies in natural language occur beyond a three-word window.", "labels": [], "entities": []}, {"text": "In addition, n-gram models are extremely large, thus making them difficult to implement efficiently in memory-constrained applications.", "labels": [], "entities": []}, {"text": "An appealing alternative is grammar-based language models.", "labels": [], "entities": []}, {"text": "Language models expressed as a pro-babilistic grammar tend to be more compact than n-gram language models, and have the ability to model long-distance dependencies (7; 7; 7).", "labels": [], "entities": []}, {"text": "However, to date there has been little success in constructing grammar-based language models competitive with n-gram models in problems of any magnitude.", "labels": [], "entities": []}, {"text": "In this paper, we describe a corpus-based indue-tion algorithm for probabilistic context-free grammars that outperforms n-gram models and the Inside-Outside algorithm (7) in medium-sized domains.", "labels": [], "entities": []}, {"text": "This result marks the first time a grammar-based language model has surpassed n-gram mode-ling in a task of at least moderate size.", "labels": [], "entities": []}, {"text": "The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm.", "labels": [], "entities": []}, {"text": "2 Grammar Induction as Search Grammar induction can be framed as a search problem , and has been framed as such almost without exception in past research (7).", "labels": [], "entities": [{"text": "Search Grammar induction", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6740642289320627}]}, {"text": "The search space is taken to be some class of grammars; for example, in our work we search within the space of probabilistic context-free grammars.", "labels": [], "entities": []}, {"text": "The objective function is ta-ken to be some measure dependent on the training data; one generally wants to find a grammar that in some sense accurately models the training data.", "labels": [], "entities": []}, {"text": "Most work in language modeling, including n-gram models and the Inside-Outside algorithm, falls under the maximum-likelihood paradigm, where one takes the objective function to be the likelihood of the training data given the grammar.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7215331494808197}]}, {"text": "However, the optimal grammar under this objective function is one which generates only strings in the training data and no other strings.", "labels": [], "entities": []}, {"text": "Such grammars are poor language models, as they overfit the training data and do not model the language at large.", "labels": [], "entities": []}, {"text": "In n-gram models and the Inside-Outside algorithm, this issue is evaded by bounding the size and form of the grammars considered, so that the \"optimal\" grammar cannot be expressed.", "labels": [], "entities": []}, {"text": "However, in our work we do not wish to limit the size of the grammars considered.", "labels": [], "entities": []}, {"text": "The basic shortcoming of the maximum-likelihood objective function is that it does not encompass the compelling intuition behind Occam's Razor, that simpler (or smaller) grammars are preferable over complex (or larger) grammars.", "labels": [], "entities": [{"text": "Occam's Razor", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.7237970034281412}]}, {"text": "A factor in the objective function that favors smaller grammars over 228", "labels": [], "entities": []}], "introductionContent": [{"text": "In applications such as speech recognition, handwriting recognition, and spelling correction, performance is limited by the quality of the language model utilized (7; 7; 7; 7).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7998929619789124}, {"text": "handwriting recognition", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.9342531859874725}, {"text": "spelling correction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.9643527865409851}]}, {"text": "However, static language modeling performance has remained basically unchanged since the advent of n-gram language models forty years ago.", "labels": [], "entities": [{"text": "static language modeling", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.6655498147010803}]}, {"text": "Yet, n-gram language models can only capture dependencies within an nword window, where currently the largest practical n for natural language is three, and many dependencies in natural language occur beyond a three-word window.", "labels": [], "entities": []}, {"text": "In addition, n-gram models are extremely large, thus making them difficult to implement efficiently in memory-constrained applications.", "labels": [], "entities": []}, {"text": "An appealing alternative is grammar-based language models.", "labels": [], "entities": []}, {"text": "Language models expressed as a probabilistic grammar tend to be more compact than n-gram language models, and have the ability to model long-distance dependencies.", "labels": [], "entities": []}, {"text": "However, to date there has been little success in constructing grammar-based language models competitive with n-gram models in problems of any magnitude.", "labels": [], "entities": []}, {"text": "In this paper, we describe a corpus-based induetion algorithm for probabilistic context-free grammars that outperforms n-gram models and the Inside-Outside algorithm in medium-sized domains.", "labels": [], "entities": []}, {"text": "This result marks the first time a grammarbased language model has surpassed n-gram modeling in a task of at least moderate size.", "labels": [], "entities": []}, {"text": "The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: English-like artificial grammar", "labels": [], "entities": []}, {"text": " Table 3: Wall Street Journal-like artificial grammar", "labels": [], "entities": [{"text": "Wall Street Journal-like", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.951394776503245}]}, {"text": " Table 4: English sentence part-of-speech sequences", "labels": [], "entities": []}, {"text": " Table 5: Parameters and Training Time", "labels": [], "entities": [{"text": "Parameters", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8804547786712646}]}]}