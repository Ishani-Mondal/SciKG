{"title": [], "abstractContent": [{"text": "We introduce three new techniques for statistical language models: extension mod-eling, nonmonotonic contexts, and the divergence heuristic.", "labels": [], "entities": []}, {"text": "Together these techniques result in language models that have few states, even fewer parameters, and low message entropies.", "labels": [], "entities": []}, {"text": "1 Introduction Current approaches to automatic speech and handwriting transcription demand a strong language model with a small number of states and an even smaller number of parameters.", "labels": [], "entities": [{"text": "handwriting transcription", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7660643458366394}]}, {"text": "If the model entropy is high, then transcription results are abysmal.", "labels": [], "entities": []}, {"text": "If there are too many states, then transcription becomes computationally infeasible.", "labels": [], "entities": []}, {"text": "And if there are too many parameters; then \"overfitting\" occurs and predictive performance degrades.", "labels": [], "entities": []}, {"text": "In this paper we introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuris-tic.", "labels": [], "entities": [{"text": "extension modeling", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7727193534374237}]}, {"text": "Together these techniques result in language models that have few states, even fewer parameters, and low message entropies.", "labels": [], "entities": []}, {"text": "For example, our techniques achieve a message entropy of 1.97 bits/char on the Brown corpus using only 89,325 parameters.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9297690093517303}]}, {"text": "By modestly increasing the number of model parameters in a principled manner, our techniques are able to further reduce the message entropy of the Brown Corpus to 1.91 bits/char.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.8804923295974731}]}, {"text": "1 In contrast, the character 4-gram model requires 250 times as many parameters in order to achieve a message entropy of only 2.47 bits/char.", "labels": [], "entities": []}, {"text": "Given the logarithmic nature of codelengths, a savings of 0.5 bits/char is quite significant.", "labels": [], "entities": []}, {"text": "The fact that our model performs significantly better using vastly fewer parameters argues 1The only change to our model selection procedure is to replace the incremental cost formula ALe(w, ~', a) with a constant cost of 2 bits/extension.", "labels": [], "entities": [{"text": "ALe", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.8466756343841553}]}, {"text": "This small change reduces the test message entropy from 1.97 to 1.91 bits/char but it also quadruples the number of model parameters and triples the total codelength. that it is a much better probability model of natural language text.", "labels": [], "entities": []}, {"text": "Our first two techniques-nonmonolonic contexts and exlension modeling-are generalizations of the traditional context model (Cleary and Witten 1984; Rissanen 1983,1986).", "labels": [], "entities": [{"text": "exlension modeling-are", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7674126923084259}]}, {"text": "Our third technique-the divergence heuristic-is an incremental model selection criterion based directly on Rissanen's (1978) minimum description length (MDL) principle.", "labels": [], "entities": [{"text": "divergence heuristic-is", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.8970424830913544}, {"text": "minimum description length (MDL)", "start_pos": 125, "end_pos": 157, "type": "METRIC", "confidence": 0.6976632078488668}]}, {"text": "The MDL principle states that the best model is the simplest model that provides a compact description of the observed data.", "labels": [], "entities": []}, {"text": "In the traditional context model, every prefix and every suffix of a context is also a context.", "labels": [], "entities": []}, {"text": "Three consequences follow from this property.", "labels": [], "entities": []}, {"text": "The first consequence is that the context dictionary is unnecessarily large because most of these contexts are redundant.", "labels": [], "entities": []}, {"text": "The second consequence is to attenuate the benefits of context blending, because most contexts are equivalent to their maximal proper suffixes.", "labels": [], "entities": [{"text": "context blending", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.718546524643898}]}, {"text": "The third consequence is that the length of the longest candidate context can increase by at most one symbol at each time step, which impairs the model's ability to model complex sources.", "labels": [], "entities": []}, {"text": "Ina non-monotonic model, this constraint is relaxed to allow compact dictionaries, discontinuous backoff, and arbitrary context switching.", "labels": [], "entities": []}, {"text": "The traditional context model maps every history to a unique context.", "labels": [], "entities": []}, {"text": "All symbols are predicted using that context, and those predictions are estimated using the same set of histories.", "labels": [], "entities": []}, {"text": "In contrast, an extension model maps every history to a sel of contexts, one for each symbol in the alphabet.", "labels": [], "entities": []}, {"text": "Each symbol is predicted in its own context, and the model's current predictions need not be estimated using the same set of histories.", "labels": [], "entities": []}, {"text": "This is a form of parameter tying that increases the accuracy of the model's predictions while reducing the number of free parameters in the model.", "labels": [], "entities": [{"text": "parameter tying", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7361629605293274}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9990174770355225}]}, {"text": "As a result of these two generalizations, nonmono-tonic extension models can outperform their equivalent context models using significantly fewer parameters.", "labels": [], "entities": []}, {"text": "For example, an order 3 n-gram (ie., the 4-gram) requires more than 51 times as many con-220", "labels": [], "entities": []}], "introductionContent": [{"text": "Current approaches to automatic speech and handwriting transcription demand a strong language model with a small number of states and an even smaller number of parameters.", "labels": [], "entities": [{"text": "handwriting transcription", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7609542906284332}]}, {"text": "If the model entropy is high, then transcription results are abysmal.", "labels": [], "entities": []}, {"text": "If there are too many states, then transcription becomes computationally infeasible.", "labels": [], "entities": []}, {"text": "And if there are too many parameters; then \"overfitting\" occurs and predictive performance degrades.", "labels": [], "entities": []}, {"text": "In this paper we introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic.", "labels": [], "entities": [{"text": "statistical language models", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7713777820269266}, {"text": "extension modeling", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8018467426300049}]}, {"text": "Together these techniques result in language models that have few states, even fewer parameters, and low message entropies.", "labels": [], "entities": []}, {"text": "For example, our techniques achieve a message entropy of 1.97 bits/char on the Brown corpus using only 89,325 parameters.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9297690093517303}]}, {"text": "By modestly increasing the number of model parameters in a principled manner, our techniques are able to further reduce the message entropy of the Brown Corpus to 1.91 bits/char.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.8804923295974731}]}, {"text": "1 In contrast, the character 4-gram model requires 250 times as many parameters in order to achieve a message entropy of only 2.47 bits/char.", "labels": [], "entities": []}, {"text": "Given the logarithmic nature of codelengths, a savings of 0.5 bits/char is quite significant.", "labels": [], "entities": []}, {"text": "The fact that our model performs significantly better using vastly fewer parameters argues 1The only change to our model selection procedure is to replace the incremental cost formula ALe(w, ~', a) with a constant cost of 2 bits/extension.", "labels": [], "entities": [{"text": "ALe", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.8466756343841553}]}, {"text": "This small change reduces the test message entropy from 1.97 to 1.91 bits/char but it also quadruples the number of model parameters and triples the total codelength. that it is a much better probability model of natural language text.", "labels": [], "entities": []}, {"text": "Our first two techniques -nonmonolonic contexts and exlension modeling -are generalizations of the traditional context model.", "labels": [], "entities": [{"text": "exlension modeling", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8121385276317596}]}, {"text": "Our third technique -the divergence heuristic -is an incremental model selection criterion based directly on minimum description length (MDL) principle.", "labels": [], "entities": [{"text": "divergence heuristic", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.891075611114502}, {"text": "minimum description length (MDL)", "start_pos": 109, "end_pos": 141, "type": "METRIC", "confidence": 0.7467451393604279}]}, {"text": "The MDL principle states that the best model is the simplest model that provides a compact description of the observed data.", "labels": [], "entities": []}, {"text": "In the traditional context model, every prefix and every suffix of a context is also a context.", "labels": [], "entities": []}, {"text": "Three consequences follow from this property.", "labels": [], "entities": []}, {"text": "The first consequence is that the context dictionary is unnecessarily large because most of these contexts are redundant.", "labels": [], "entities": []}, {"text": "The second consequence is to attenuate the benefits of context blending, because most contexts are equivalent to their maximal proper suffixes.", "labels": [], "entities": [{"text": "context blending", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.718546524643898}]}, {"text": "The third consequence is that the length of the longest candidate context can increase by at most one symbol at each time step, which impairs the model's ability to model complex sources.", "labels": [], "entities": []}, {"text": "Ina nonmonotonic model, this constraint is relaxed to allow compact dictionaries, discontinuous backoff, and arbitrary context switching.", "labels": [], "entities": []}, {"text": "The traditional context model maps every history to a unique context.", "labels": [], "entities": []}, {"text": "All symbols are predicted using that context, and those predictions are estimated using the same set of histories.", "labels": [], "entities": []}, {"text": "In contrast, an extension model maps every history to a sel of contexts, one for each symbol in the alphabet.", "labels": [], "entities": []}, {"text": "Each symbol is predicted in its own context, and the model's current predictions need not be estimated using the same set of histories.", "labels": [], "entities": []}, {"text": "This is a form of parameter tying that increases the accuracy of the model's predictions while reducing the number of free parameters in the model.", "labels": [], "entities": [{"text": "parameter tying", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7361629605293274}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9990174770355225}]}, {"text": "As a result of these two generalizations, nonmonotonic extension models can outperform their equivalent context models using significantly fewer parameters.", "labels": [], "entities": []}, {"text": "For example, an order 3 n-gram (ie., the 4-gram) requires more than 51 times as many con-texts and 787 times as many parameters as the order 3 nonmonotonic extension model, yet already performs worse on the Brown corpus by 0.08 bits/char.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 207, "end_pos": 219, "type": "DATASET", "confidence": 0.8298327326774597}]}, {"text": "Our third contribution is the divergence heuristic, which adds a more specific context to the model only when it reduces the codelength of the past data more than it increases the codelength of the model.", "labels": [], "entities": [{"text": "divergence heuristic", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.8941130936145782}]}, {"text": "In contrast, the traditional selection heuristic adds a more specific context to the model only if it's entropy is less than the entropy of the more general context (.", "labels": [], "entities": []}, {"text": "The traditional minimum entropy heuristic is a special case of the more effective and more powerful divergence heuristic.", "labels": [], "entities": []}, {"text": "The divergence heuristic allows our models to generalize from the training corpus to the testing corpus, even for nonstationary sources such as the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 148, "end_pos": 160, "type": "DATASET", "confidence": 0.8741710484027863}]}, {"text": "The remainder of our article is organized into three sections.", "labels": [], "entities": []}, {"text": "In section 2, we formally define the class of extension models and present a heuristic model selection algorithm for that model class based on the divergence criterion.", "labels": [], "entities": []}, {"text": "Next, in section 3, we demonstrate the efficacy of our techniques on the Brown Corpus, an eclectic collection of English prose containing approximately one million words of text.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9860883057117462}]}, {"text": "Section 4 discusses possible improvements to the model class.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for the nonmonotonic extension  model (NEM), the nonmonotonic context model  (NCM),", "labels": [], "entities": []}]}