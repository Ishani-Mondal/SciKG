{"title": [], "abstractContent": [{"text": "A technique for reducing a tagset used for n-gram part-of-speech disambiguation is introduced and evaluated in an experiment.", "labels": [], "entities": [{"text": "n-gram part-of-speech disambiguation", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.6571661432584127}]}, {"text": "The technique ensures that all information that is provided by the original tagset can be restored from the reduced one.", "labels": [], "entities": []}, {"text": "This is crucial, since we are interested in the linguistically motivated tags for part-of-speech disambiguation.", "labels": [], "entities": []}, {"text": "The reduced tagset needs fewer parameters for its statistical model and allows more accurate parameter estimation.", "labels": [], "entities": []}, {"text": "Additionally, there is a slight but not significant improvement of tagging accuracy.", "labels": [], "entities": [{"text": "tagging", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.9641125202178955}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9674689769744873}]}, {"text": "1 Motivation Statistical part-of-speech disambiguation can be efficiently done with n-gram models (Church, 1988; Cutting et al., 1992).", "labels": [], "entities": [{"text": "Motivation Statistical part-of-speech disambiguation", "start_pos": 2, "end_pos": 54, "type": "TASK", "confidence": 0.6749884188175201}]}, {"text": "These models are equivalent to Hidden Markov Models (HMMs) (Rabiner, 1989) of order n-1.", "labels": [], "entities": []}, {"text": "The states represent parts of speech (categories, tags), there is exactly one state for each category, and each state outputs words of a particular category.", "labels": [], "entities": []}, {"text": "The transition and output probabilities of the HMM are derived from smoothed frequency counts in a text corpus.", "labels": [], "entities": []}, {"text": "Generally, the categories for part-of-speech tagging are linguistically motivated and do not reflect the probability distributions or co-occurrence probabilities of words belonging to that category.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7088909447193146}]}, {"text": "It is an implicit assumption for statistical part-of-speech tagging that words belonging to the same category have similar probability distributions.", "labels": [], "entities": [{"text": "statistical part-of-speech tagging", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.6188270847002665}]}, {"text": "But this assumption does not hold in many of the cases.", "labels": [], "entities": []}, {"text": "Take for example the word cliff which could be a proper (NP) 1 or a common noun (NN) (ignoring capitalization of proper nouns for the moment).", "labels": [], "entities": []}, {"text": "The two previous words area determiner (AT) and an 1All tag names used in this paper are inspired by those used for the LOB Corpus (Garside et al., 1987).", "labels": [], "entities": [{"text": "area determiner (AT)", "start_pos": 23, "end_pos": 43, "type": "METRIC", "confidence": 0.7062154710292816}, {"text": "LOB Corpus", "start_pos": 120, "end_pos": 130, "type": "DATASET", "confidence": 0.9684883654117584}]}, {"text": "adjective (J J).", "labels": [], "entities": []}, {"text": "The probability of cliff being a common noun is the product of the respective contextual and lexical probabilities p(N N ]AT, JJ) \u2022 p(c//fflN N), regardless of other information provided by the actual words (a sheer cliff vs. the wise Cliff).", "labels": [], "entities": [{"text": "AT", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9579052329063416}]}, {"text": "Obviously , information useful for probability estimation is not encoded in the tagset.", "labels": [], "entities": [{"text": "probability estimation", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.6970573663711548}]}, {"text": "On the other hand, in some cases information not needed for probability estimation is encoded in the tagset.", "labels": [], "entities": [{"text": "probability estimation", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.616564467549324}]}, {"text": "The distributions for comparative and superlative forms of adjectives in the Susanne Corpus (Sampson, 1995) are very similar.", "labels": [], "entities": [{"text": "Susanne Corpus (Sampson, 1995)", "start_pos": 77, "end_pos": 107, "type": "DATASET", "confidence": 0.9542593104498727}]}, {"text": "The number of correct tag assignments is not affected when we combine the two categories.", "labels": [], "entities": []}, {"text": "However, it does not suffice to assign the combined tag, if we are interested in the distinction between comparative and superlative form for further processing.", "labels": [], "entities": []}, {"text": "We have to ensure that the original (interesting) tag can be restored.", "labels": [], "entities": []}, {"text": "There are two contradicting requirements.", "labels": [], "entities": []}, {"text": "On the one hand, more tags mean that there is more information about a word at hand, on the other hand, the more tags, the severer the sparse-data problem is and the larger the corpora that are needed for training.", "labels": [], "entities": []}, {"text": "This paper presents away to modify a given tag-set, such that categories with similar distributions in a corpus are combined without losing information provided by the original tagset and without losing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9950833320617676}]}, {"text": "2 Clustering of Tags The aim of the presented method is to reduce a tag-set as much as possible by combining (clustering) two or more tags without losing information and without losing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9967568516731262}]}, {"text": "The fewer tags we have, the less parameters have to be estimated and stored, and the less severe is the sparse data problem.", "labels": [], "entities": []}, {"text": "Incoming text will be disambiguated with the new reduced tagset, but we ensure that the original tag is still uniquely ide:.ltified by the new tag.", "labels": [], "entities": []}, {"text": "The basic idea is to exploit the fact that some of the categories have a very similar frequency distribution in a corpus.", "labels": [], "entities": []}, {"text": "If we combine categories with 287", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}