{"title": [{"text": "UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS", "labels": [], "entities": [{"text": "UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS", "start_pos": 0, "end_pos": 66, "type": "METRIC", "confidence": 0.584104482616697}]}], "abstractContent": [{"text": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7943824827671051}]}, {"text": "The algorithm is based on two powerful constraints-that words tend to have one sense per discourse and one sense per collocation-exploited in an iterative bootstrapping procedure.", "labels": [], "entities": []}, {"text": "Tested accuracy exceeds 96%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9997358918190002}]}], "introductionContent": [{"text": "This paper presents an unsupervised algorithm that can accurately disambiguate word senses in a large, completely untagged corpus) The algorithm avoids the need for costly hand-tagged training data by exploiting two powerful properties of human language: 1.", "labels": [], "entities": []}, {"text": "One sense per collocation: 2 Nearby words provide strong and consistent clues to the sense of a target word, conditional on relative distance, order and syntactic relationship.", "labels": [], "entities": []}, {"text": "2. One sense per discourse: The sense of a target word is highly consistent within any given document.", "labels": [], "entities": []}, {"text": "Moreover, language is highly redundant, so that the sense of a word is effectively overdetermined by (1) and (2) above.", "labels": [], "entities": []}, {"text": "The algorithm uses these properties to incrementally identify collocations for target senses of a word, given a few seed collocations 1Note that the problem here is sense disambiguation: assigning each instance of a word to established sense definitions (such as in a dictionary).", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 165, "end_pos": 185, "type": "TASK", "confidence": 0.7285982966423035}]}, {"text": "This differs from sense induction: using distributional similarity to partition word instances into clusters that may have no relation to standard sense partitions.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7578550577163696}]}, {"text": "2Here I use the traditional dictionary definition of collocation -\"appearing in the same location; a juxtaposition of words\".", "labels": [], "entities": []}, {"text": "No idiomatic or non-compositional interpretation is implied.", "labels": [], "entities": []}, {"text": "for each sense, This procedure is robust and selfcorrecting, and exhibits many strengths of supervised approaches, including sensitivity to word-order information lost in earlier unsupervised algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "The words used in this evaluation were randomly selected from those previously studied in the literature.", "labels": [], "entities": []}, {"text": "They include words where sense differences are realized as differences in French translation (drug --* drogue/m~dicament, and duty --~ devoir/droit), a verb (poach) and words used in Schiitze's 1992 disambiguation experiments (tank, space, motion, plant) J \u00b0 The data were extracted from a 460 million word corpus containing news articles, scientific abstracts, spoken transcripts, and novels, and almost certainly constitute the largest training/testing sets used in the sense-disambiguation literature.", "labels": [], "entities": []}, {"text": "Columns 6-8 illustrate differences in seed training options.", "labels": [], "entities": []}, {"text": "Using only two words as seeds does surprisingly well (90.6 %).", "labels": [], "entities": []}, {"text": "This approach is least successful for senses with a complex concept space, which cannot be adequately represented by single words.", "labels": [], "entities": []}, {"text": "Using the salient words of a dictionary definition as seeds increases the coverage of the concept space, improving accuracy (94.8%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9993588328361511}]}, {"text": "However, spurious words in example sentences can be a source of noise.", "labels": [], "entities": []}, {"text": "Quick hand tagging of a list of algorithmically-identified salient collocates appears to be worth the effort, due to the increa3ed accuracy (95.5%) and minimal cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9586235880851746}]}, {"text": "Columns 9 and 10 illustrate the effect of adding the probabilistic one-sense-per-discourse constraint to collocation-based models using dictionary entries as training seeds.", "labels": [], "entities": []}, {"text": "Column 9 shows its effectiveness 1\u00b0The number of words studied has been limited hereby the highly time-consuming constraint that full hand tagging is necessary for direct comparison with supervised training.", "labels": [], "entities": [{"text": "hand tagging", "start_pos": 134, "end_pos": 146, "type": "TASK", "confidence": 0.6734287589788437}]}, {"text": "Although apparently small in absolute terms, on average this represents a 27% reduction in error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9878853559494019}]}, {"text": "11 When applied at each iteration, this process reduces the training noise, yielding the optimal observed accuracy in column 10.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9684785604476929}]}, {"text": "Comparative performance: Column 5 shows the relative performance of supervised training using the decision list algorithm, applied to the same data and not using any discourse information.", "labels": [], "entities": []}, {"text": "Unsupervised training using the additional one-sense-per-discourse constraint frequently exceeds this value.", "labels": [], "entities": []}, {"text": "Column 11 shows the performance of Schiitze's unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus.", "labels": [], "entities": [{"text": "New York Times News Service corpus", "start_pos": 114, "end_pos": 148, "type": "DATASET", "confidence": 0.7592172573010126}]}, {"text": "Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9991357922554016}]}], "tableCaptions": []}