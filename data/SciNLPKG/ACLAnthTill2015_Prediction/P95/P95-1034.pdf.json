{"title": [], "abstractContent": [{"text": "Large-scale natural language generation requires the integration of vast mounts of knowledge: lexical, grammatical, and conceptual.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7564630707105001}]}, {"text": "A robust generator must be able to operate well even when pieces of knowledge axe missing.", "labels": [], "entities": []}, {"text": "It must also be robust against incomplete or inaccurate inputs.", "labels": [], "entities": []}, {"text": "To attack these problems, we have built a hybrid generator , in which gaps in symbolic knowledge are filled by statistical methods.", "labels": [], "entities": []}, {"text": "We describe algorithms and show experimental results.", "labels": [], "entities": []}, {"text": "We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.", "labels": [], "entities": []}], "introductionContent": [{"text": "A large-scale natural language generation (NLG) system for unrestricted text should be able to operate in an environment of 50,000 conceptual terms and 100,000 words or phrases.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.8328566153844198}]}, {"text": "Turning conceptual expressions into English requires the integration of large knowledge bases (KBs), including grammar, ontology, lexicon, collocations, and mappings between them.", "labels": [], "entities": []}, {"text": "The quality of an NLG system depends on the quality of its inputs and knowledge bases.", "labels": [], "entities": []}, {"text": "Given that perfect KBs do not yet exist, an important question arises: can we build high-quality NLG systems that are robust against incomplete KBs and inputs?", "labels": [], "entities": []}, {"text": "Although robustness has been heavily studied in natural language understanding, it has received much less attention in NLG.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6512357294559479}]}, {"text": "We describe a hybrid model for natural language generation which offers improved performance in the presence of knowledge gaps in the generator (the grammar and the lexicon), and of errors in the semantic input.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.688706616560618}]}, {"text": "The model comes out of our practical experience in building a large Japanese-English newspaper machine translation system, JAPAN-GLOSS ( . This system translates Japanese into representations whose terms are drawn from the SENSUS ontology ( , a 70,000-node knowledge base skeleton derived from resources like WordNet), Longman's Dictionary, and the PENMAN Upper Model.", "labels": [], "entities": [{"text": "Japanese-English newspaper machine translation", "start_pos": 68, "end_pos": 114, "type": "TASK", "confidence": 0.6279195845127106}, {"text": "WordNet", "start_pos": 309, "end_pos": 316, "type": "DATASET", "confidence": 0.9375418424606323}, {"text": "Longman's Dictionary", "start_pos": 319, "end_pos": 339, "type": "DATASET", "confidence": 0.8769638538360596}, {"text": "PENMAN Upper Model", "start_pos": 349, "end_pos": 367, "type": "DATASET", "confidence": 0.7595983346303304}]}, {"text": "These representations are turned into English during generation.", "labels": [], "entities": []}, {"text": "Because we are processing unrestricted newspaper text, all modules in JAPAN-GLOSS must be robust.", "labels": [], "entities": [{"text": "JAPAN-GLOSS", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9080004096031189}]}, {"text": "In addition, we show how the model is useful in simplifying the design of a generator and its knowledge bases even when perfect knowledge is available.", "labels": [], "entities": []}, {"text": "This is accomplished by relegating some aspects of lexical choice (such as preposition selection and noncompositional interlexical constraints) to a statistical component.", "labels": [], "entities": []}, {"text": "The generator can then use simpler rules and combine them more freely; the price of this simplicity is that some of the output maybe invalid.", "labels": [], "entities": []}, {"text": "At this point, the statistical component intervenes and filters from the output all but the fluent expressions.", "labels": [], "entities": []}, {"text": "The advantage of this two-level approach is that the knowledge bases in the generator become simpler, easier to develop, more portable across domains, and more accurate and robust in the presence of knowledge gaps.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first goal was to integrate the symbolic knowledge in the PENMAN system with the statistical knowledge in our language model.", "labels": [], "entities": [{"text": "PENMAN system", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.8414284288883209}]}, {"text": "We took a semantic representation generated automatically from a short Japanese sentence.", "labels": [], "entities": []}, {"text": "We then used PEN-MAN to generate 3,456 English sentences corresponding to the 3,456 (= 2'.", "labels": [], "entities": [{"text": "PEN-MAN", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.7262449860572815}]}, {"text": "33) possible combinations of the values of seven binary and three ternary features that were unspecified in the semantic input.", "labels": [], "entities": []}, {"text": "These features were relevant to the semantic representation but their values were not extractable from the Japanese sentence, and thus each of their combinations corresponded to a particular interpretation among the many possible in the presence of incompleteness in the semantic input.", "labels": [], "entities": []}, {"text": "Specifying a feature forced PENMAN to make a particular linguistic decision.", "labels": [], "entities": []}, {"text": "For example, adding (:identifiability-q t) forces the choice of determiner, while the :lex feature offers explicit control over the selection of open-class words.", "labels": [], "entities": []}, {"text": "A literal translation of the input sentence was something like As for new company, there is plan to establish in February.", "labels": [], "entities": []}, {"text": "Here are three randomly selected translations; note that the object of the \"establishing\" action is unspecified in the Japanese input, but PEN-MAN supplies a placeholder it when necessary, to ensure grammaticality: SAvailable from the ACL Data Collection Initiative, as CD ROM 1.", "labels": [], "entities": [{"text": "ACL Data Collection Initiative", "start_pos": 235, "end_pos": 265, "type": "DATASET", "confidence": 0.9238901287317276}]}, {"text": "A new company will have in mind that it is establishing it on February.", "labels": [], "entities": []}, {"text": "The new company plans the launching on February.", "labels": [], "entities": []}, {"text": "New companies will have as a goal the launching at February.", "labels": [], "entities": []}, {"text": "We then ranked the 3,456 sentences using the bigram version of our statistical language model, with the hope that good renditions would come out on top.", "labels": [], "entities": []}, {"text": "Here is an abridged list of outputs, loglikelihood scores heuristically corrected for length, and rankings: While this experiment shows that statistical models can help make choices in generation, it fails as a computational strategy.", "labels": [], "entities": []}, {"text": "Running PENMAN 3,456 times is expensive, but nothing compared to the cost of exhaustively exploring all combinations in larger input representations corresponding to sentences typically found in newspaper text.", "labels": [], "entities": []}, {"text": "Twenty or thirty choice points typically multiply into millions or billions of potential sentences, and it is infeasible to generate them all independently.", "labels": [], "entities": []}, {"text": "This leads us to consider other algorithms.", "labels": [], "entities": []}], "tableCaptions": []}