{"title": [{"text": "Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies", "labels": [], "entities": [{"text": "Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.6072910163137648}]}], "abstractContent": [{"text": "We describe one approach to build an automatically trainable anaphora resolution system.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.780659407377243}]}, {"text": "In this approach, we use Japanese newspaper articles tagged with discourse information as training examples fora machine learning algorithm which employs the C4.5 decision tree algorithm by Quin-lan (Quinlan, 1993).", "labels": [], "entities": []}, {"text": "Then, we evaluate and compare the results of several variants of the machine learning-based approach with those of our existing anaphora resolution system which uses manually-designed knowledge sources.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7468415796756744}]}, {"text": "Finally, we compare our algorithms with existing theories of anaphora, in particular, Japanese zero pronouns .", "labels": [], "entities": []}], "introductionContent": [{"text": "Anaphora resolution is an important but still difficult problem for various large-scale natural language processing (NLP) applications, such as information extraction and machine tr~slation.", "labels": [], "entities": [{"text": "Anaphora resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8196464776992798}, {"text": "information extraction", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.8477862179279327}]}, {"text": "Thus far, no theories of anaphora have been tested on an empirical basis, and therefore there is no answer to the \"best\" anaphora resolution algorithm.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7424785196781158}]}, {"text": "I Moreover, an anaphora resolution system within an NLP system for real applications must handle: \u2022 degraded or missing input (no NLP system has complete lexicons, grammars, or semantic knowledge and outputs perfect results), and \u2022 different anaphoric phenomena in different domains, languages, and applications.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7518593668937683}]}, {"text": "Thus, even if there exists a perfect theory, it might notwork well with noisy input, or it would not coverall the anaphoric phenomena.", "labels": [], "entities": []}, {"text": "1Walker compares Brennan, Friedman a~ad Pollard's centering approach ( with Hobbs' algorithm) on a theoretical basis.", "labels": [], "entities": [{"text": "1Walker", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9445459246635437}]}, {"text": "These requirements have motivated us to develop robust, extensible, and trainable anaphora resolution systems.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7772179245948792}]}, {"text": "Previously, we reported our data-driven multilingual anaphora resolution system, which is robust, exteusible, and manually trainable.", "labels": [], "entities": [{"text": "multilingual anaphora resolution", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.6447006563345591}]}, {"text": "It uses discourse knowledge sources (KS's) which are manually selected and ordered.", "labels": [], "entities": []}, {"text": "(Henceforth, we call the system the Manually-Designed Resolver, or MDR.)", "labels": [], "entities": []}, {"text": "We wanted to develop, however, truly automatically trainable systems, hoping to improve resolution performance and reduce the overhead of manually constructing and arranging such discourse data.", "labels": [], "entities": []}, {"text": "In this paper, we first describe one approach we are taking to build an automatically trainable anaphora resolution system.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7757497727870941}]}, {"text": "In this approach, we tag corpora with discourse information, and use them as training examples fora machine learning algorithm.", "labels": [], "entities": []}, {"text": "(Henceforth, we call the system the Machine Learning-based Resolver, or MLR.)", "labels": [], "entities": [{"text": "Machine Learning-based Resolver", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.5660927494366964}]}, {"text": "Specifically, we have tagged Japanese newspaper articles about joint ventures and used the C4.5 decision tree algorithm by Quinlan.", "labels": [], "entities": []}, {"text": "Then, we evaluate and compare the results of the MLR with those produced by the MDR.", "labels": [], "entities": [{"text": "MLR", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7882720232009888}, {"text": "MDR", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9053123593330383}]}, {"text": "Finally, we compare our algorithms with existing theories of anaphora, in particular, Japanese zero pronouns.", "labels": [], "entities": []}], "datasetContent": [{"text": "Changing the three parameters in the MLRs caused changes in anaphora resolution performance.", "labels": [], "entities": [{"text": "MLRs", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.8290459513664246}]}, {"text": "As shows, using anaphoric chains without anaphoric type identification helped improve the MLRs.", "labels": [], "entities": [{"text": "MLRs", "start_pos": 90, "end_pos": 94, "type": "TASK", "confidence": 0.969212532043457}]}, {"text": "Our experiments with the confidence factor parameter indicates the trade off between recall and precision.", "labels": [], "entities": [{"text": "confidence factor", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.9447695910930634}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.999180018901825}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9974669218063354}]}, {"text": "With 100% confidence factor, which means no pruning of the tree, the tree overfits the examples, and leads to spurious uses of features such as the number of sentences between an anaphor and an antecedent near the leaves of the generated tree.", "labels": [], "entities": []}, {"text": "This causes the system to attempt more anaphor resolutions albeit with lower precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9977557063102722}]}, {"text": "Conversely, too much pruning can also yield poorer results.", "labels": [], "entities": []}, {"text": "MLR-5 illustrates that when anaphoric type identification is turned on the MLR's performance drops SF-measure is calculated by: where P is precision, R is recall, and /3 is the relative importance given to recall over precision.", "labels": [], "entities": [{"text": "MLR-5", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.927474319934845}, {"text": "anaphoric type identification", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6935989856719971}, {"text": "SF-measure", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9760622382164001}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9990955591201782}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9920517206192017}, {"text": "recall", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9920810461044312}, {"text": "precision", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9953858256340027}]}, {"text": "In this case, = 1.0.", "labels": [], "entities": []}, {"text": "but still exceeds that of the MDR.", "labels": [], "entities": [{"text": "MDR", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8517813086509705}]}, {"text": "MLR-6 shows the effect of not training on anaphoric chains.", "labels": [], "entities": [{"text": "MLR-6", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7990128993988037}]}, {"text": "It results in poorer performance than the MLR-1, 2, 3, 4, and 5 configurations and the MDR.", "labels": [], "entities": [{"text": "MLR-1", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.7070670127868652}, {"text": "MDR", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9093175530433655}]}, {"text": "One advantage of the MDR is that a tagged training corpus is not required for hand-coding the resolution algorithms.", "labels": [], "entities": []}, {"text": "Of course, such a tagged corpus is necessary to evaluate system performance quantitatively and is also useful to consult with during algorithm construction.", "labels": [], "entities": []}, {"text": "However, the MLR results seem to indicate the limitation of the MDR in the way it uses orderer KS's.", "labels": [], "entities": []}, {"text": "Currently, the MDR uses an ordered list of multiple orderer KS's for each anaphoric type (cf.), where the first applicable orderer KS in the list is used to pick the best antecedent when there is more than one possibility.", "labels": [], "entities": []}, {"text": "Such selection ignores the fact that even anaphora of the same type may use different orderers (i.e. have different preferences), depending on the types of possible antecedents and on the context in which the particular anaphor was used in the text.", "labels": [], "entities": []}, {"text": "indicates that with even 50 training texts, the MLR achieves better performance than the MDR.", "labels": [], "entities": [{"text": "MLR", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.770117998123169}]}, {"text": "Performance seems to reach a plateau at about 250 training examples with a F-measure of around 77.4.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9997081160545349}]}], "tableCaptions": [{"text": " Table 3: Six Configurations of MLRs", "labels": [], "entities": [{"text": "MLRs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.7725110054016113}]}, {"text": " Table 6: Recall and Precision of the MLRs and the MDR  NAME-ORG  631  R  P  84.79  92.24  84.79  93.04  83.20  94.09  83.84  94.30  85.74  92.80  68.30  91.70  76.39  90.09", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9953527450561523}, {"text": "Precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9934588074684143}, {"text": "MDR  NAME-ORG  631  R  P  84.79  92.24  84.79  93.04  83.20  94.09  83.84  94.30  85.74  92.80  68.30  91.70  76.39  90.09", "start_pos": 51, "end_pos": 173, "type": "DATASET", "confidence": 0.9144789764755651}]}, {"text": " Table 7:MLR-2 Configuration with Varied Training Data Sizes", "labels": [], "entities": [{"text": "MLR-2 Configuration", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.6081577241420746}]}]}