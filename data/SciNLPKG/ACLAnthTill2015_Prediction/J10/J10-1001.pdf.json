{"title": [{"text": "Broad-Coverage Parsing Using Human-Like Memory Constraints", "labels": [], "entities": [{"text": "Broad-Coverage Parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5207397639751434}]}], "abstractContent": [{"text": "Human syntactic processing shows many signs of taking place within a general-purpose short-term memory.", "labels": [], "entities": [{"text": "syntactic processing", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.6888603419065475}]}, {"text": "But this kind of memory is known to have a severely constrained storage capacity-possibly constrained to as few as three or four distinct elements.", "labels": [], "entities": []}, {"text": "This article describes a model of syntactic processing that operates successfully within these severe constraints, by recognizing constituents in a right-corner transformed representation (a variant of left-corner parsing) and mapping this representation to random variables in a Hierarchical Hidden Markov Model, a factored time-series model which probabilistically models the contents of a bounded memory store overtime.", "labels": [], "entities": [{"text": "syntactic processing", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7541617453098297}]}, {"text": "Evaluations of the coverage of this model on a large syntactically annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy based on this model, suggest this model maybe cognitively plausible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9989494681358337}, {"text": "bounded-memory parsing", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7043401896953583}]}], "introductionContent": [{"text": "It is an interesting possibility that human syntactic processing may occur entirely within a general-purpose short-term memory.", "labels": [], "entities": [{"text": "syntactic processing", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7343794405460358}]}, {"text": "Like other short-term memory processes, syntactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading; and memory of words and syntax degrades overtime within and across sentences, unlike semantics and discourse information about referents from other sentences.", "labels": [], "entities": []}, {"text": "But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements).", "labels": [], "entities": []}, {"text": "These limits may seem too austere to process the rich tree-like phrase structure commonly invoked to explain word-order regularities in natural language.", "labels": [], "entities": []}, {"text": "This article aims to show that they are not.", "labels": [], "entities": []}, {"text": "The article describes a comprehension model, based on a right-corner transform-a reversible tree transform related to the left-corner transform of Johnson (1998a)-that associates familiar phrase structure trees with the contents of a memory store of three to four partially completed constituents overtime.", "labels": [], "entities": []}, {"text": "Coverage results on the large syntactically annotated Penn Treebank corpus show avast majority of naturally occurring sentences can be recognized using a memory store containing a maximum of only three incomplete constituents, and nearly all sentences can be recognized using four, consistent with estimates of human short-term memory capacity.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.9933354059855143}]}, {"text": "This transform reduces memory usage in incremental (left to right) processing by transforming right-branching constituent structures into left-branching structures, allowing child constituents to be composed with parent constituents before either have been completely recognized.", "labels": [], "entities": []}, {"text": "But because this composition identifies an incomplete child as the awaited portion of an incomplete parent, it implicitly predicts that this child constituent will be the rightmost (i.e., last) child of the parent, before this child has been completely recognized.", "labels": [], "entities": []}, {"text": "Parsing accuracy results on the Penn Treebank using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)-essentially a probabilistic pushdown automaton with a bounded pushdown store-show that this prediction can be reliably learned from training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9757500290870667}, {"text": "Penn Treebank", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9948887228965759}]}, {"text": "The remainder of this article is organized as follows: Section 2 describes some related models of human syntactic processing using a bounded memory store; Section 3 describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical parsing using this bounded store of incomplete constituents; Section 4 describes the right-corner transform and how it relates conventional phrase structure to incomplete constituents in a bounded memory store; Section 5 describes an experiment to estimate the level of coverage of the Penn Treebank corpus that can be achieved using this transform with various memory limits, given a linguistically motivated binarization of this corpus; and Section 6 gives accuracy results of this bounded-memory model trained on this corpus, given that some amount of incremental prediction (as described earlier) must be involved.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 231, "end_pos": 250, "type": "TASK", "confidence": 0.7543378174304962}, {"text": "Penn Treebank corpus", "start_pos": 529, "end_pos": 549, "type": "DATASET", "confidence": 0.9937829772631327}, {"text": "accuracy", "start_pos": 702, "end_pos": 710, "type": "METRIC", "confidence": 0.9991757273674011}]}], "datasetContent": [{"text": "In order to determine whether a memory-preserving parsing strategy, like the optionally arc-eager strategy, can be reliably learned, a baseline Cocke-Kasami-Younger (CKY) parser and bounded-memory right-corner HHMM parser were evaluated on the standard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set described in Section 5.2 (WSJ Sections 2-21) as training data.", "labels": [], "entities": [{"text": "Penn Treebank WSJ Section 23", "start_pos": 253, "end_pos": 281, "type": "DATASET", "confidence": 0.9519949793815613}, {"text": "parsing task", "start_pos": 282, "end_pos": 294, "type": "TASK", "confidence": 0.5469389259815216}, {"text": "WSJ Sections 2-21)", "start_pos": 351, "end_pos": 369, "type": "DATASET", "confidence": 0.9264523684978485}]}, {"text": "Training examples requiring more than four stack elements were excluded from training, in order to avoid generating inconsistent model probabilities (e.g., from expansions that could not be re-composed within the bounded memory store).", "labels": [], "entities": []}, {"text": "Most likely sequences of HHMM stack configurations are evaluated by reversing the binarization, right-corner, and time-series mapping transforms described in Sections 4 and 5.", "labels": [], "entities": []}, {"text": "But some of the binarization rewrites cannot be completely reversed, because they cannot be unambiguously matched to output trees.", "labels": [], "entities": []}, {"text": "Automatically derived lexical projections below the annotated phrase level (e.g., binarizations of base noun phrases) can be completely reversed, because the derived categories are characteristically labeled with terminal symbols.", "labels": [], "entities": []}, {"text": "So, too, can the conjunction and \"nominal\" binarizations described in Section 5.1, because they can be identified by characteristic \"-LIST\" and underscore delimiters.", "labels": [], "entities": []}, {"text": "But automatically derived projections above the annotated phrase level cannot be reliably identified in parser output (for example, an intermediate projection \"S \ud97b\udf59 PP S\" mayor may not be annotated in the corpus).", "labels": [], "entities": []}, {"text": "In order to isolate the evaluation from the effects of these ambiguous matchings, the evaluation was performed using trees in a partially binarized format, obtained by reversing only those rewrites that result in unambiguous matches.", "labels": [], "entities": []}, {"text": "Evaluating on this partially binarized data does not seem to unfairly increase parsing performance compared to other published results-quite the contrary: an evaluation using the state-of-the-art Charniak (2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when its hypotheses and gold standard trees are converted into this format.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9588703513145447}, {"text": "F-score", "start_pos": 262, "end_pos": 269, "type": "METRIC", "confidence": 0.8683492541313171}]}, {"text": "Both CKY baseline and HHMM test systems were run with a simple part of speech ( POS) model using relative frequency estimates from the training set, backed off to a discriminative (decision tree) model conditioned on the last five letters of each word, normalized over unigram POS probabilities.", "labels": [], "entities": [{"text": "CKY baseline", "start_pos": 5, "end_pos": 17, "type": "DATASET", "confidence": 0.912513256072998}, {"text": "HHMM test", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.8180898129940033}]}, {"text": "The CKY baseline and HHMM results were obtained by training and evaluating on binarized trees, which is a necessary condition for the right-corner transform.", "labels": [], "entities": [{"text": "CKY baseline", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9006092548370361}, {"text": "HHMM", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.772598922252655}]}, {"text": "The CKY baseline results appear to be better than those fora baseline probabilistic context-free grammar (PCFG) system reported by using no modifications to the corpus, and no parent or sibling conditioning (see, top) because the binarization process allows the parser to avoid some sparse data effects due to large flat branching structures in the Treebank, resulting in improved parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 381, "end_pos": 388, "type": "TASK", "confidence": 0.9474937915802002}, {"text": "accuracy", "start_pos": 389, "end_pos": 397, "type": "METRIC", "confidence": 0.8603625297546387}]}, {"text": "Klein and Manning note that applying linguistically motivated binarization transforms can yield substantial improvements in accuracy-as much as nine points, in their study (in comparison, binarization only seems to improve accuracy by about seven points above an unmodified baseline in the present study).", "labels": [], "entities": [{"text": "accuracy-as", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.9986849427223206}, {"text": "accuracy", "start_pos": 223, "end_pos": 231, "type": "METRIC", "confidence": 0.9972406625747681}]}, {"text": "But the Klein and Manning results for binarization are provided only for models already augmented with Markov dependencies (that is, conditioning on parent and sibling categories, analogous to HHMM dependencies), so it was not possible to compare to a binarized and un-Markovized benchmark.", "labels": [], "entities": []}, {"text": "The results for HHMM parsing, training, and evaluating on these same binarized trees (modulo right-corner and variable-mapping transforms) were substantially better than binarized CKY, most likely due to the expanded HHMM dependencies on previous (q d t\u22121 ) and parent (q d\u22121 t ) variables at each q d t . For example, binarized PCFG probabilities maybe defined in terms of three category symbols A, B, and C: P(A \ud97b\udf59 BC | A); whereas some of the HHMM probabilities are defined in terms of five category Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure (% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on unmodified and binarized WSJ Sections 22 (sentences 1-393: \"devset\") and 23-24 (all sentences).", "labels": [], "entities": [{"text": "HHMM parsing", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.8608515858650208}, {"text": "category Labeled recall (LR)", "start_pos": 493, "end_pos": 521, "type": "METRIC", "confidence": 0.7406547218561172}, {"text": "labeled precision (LP)", "start_pos": 523, "end_pos": 545, "type": "METRIC", "confidence": 0.8166620612144471}, {"text": "weighted average (F-score)", "start_pos": 547, "end_pos": 573, "type": "METRIC", "confidence": 0.8711371779441833}, {"text": "WSJ Sections 22", "start_pos": 707, "end_pos": 722, "type": "DATASET", "confidence": 0.9020357926686605}]}, {"text": "Results are shown with and without punctuation, compared to) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R'01) using parent+sibling conditioning.", "labels": [], "entities": []}, {"text": "Baseline CKY and test (parent+sibling) cases for the HHMM system start out at a higher accuracy than for the Klein-Manning system because the HHMM system requires binarization of trees, which removes some data sparsity in the raw Treebank annotation, whereas the Klein-Manning results are computed prior to binarization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9968289732933044}]}, {"text": "Because it is incremental, the parser occasionally eliminates all continuable analyses from the beam, and therefore fails to find a parse.", "labels": [], "entities": []}, {"text": "HHMM parse failures are accounted as zeros in the recall statistics, but are also listed separately, because in principle it might be possible to recover useful syntactic structure from partial sequences.", "labels": [], "entities": [{"text": "HHMM parse", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.6893061697483063}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9963733553886414}]}, {"text": "but apparently not to the point of sparsity; this is similar to the effect of horizontal Markovization (conditioning on the sibling category immediately previous to an expanded category) and vertical Markovization (conditioning on the parent of an expanded category) commonly used in PCFG parsing models).", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 284, "end_pos": 296, "type": "TASK", "confidence": 0.5875579416751862}]}, {"text": "The improvement due to HHMM parsing over the PCFG baseline (18.6% reduction in error) is comparable to that reported by Klein and Manning for parent and sibling dependencies (first-order vertical and horizontal Markovization) over a baseline PCFG without binarization (17.5% reduction in error).", "labels": [], "entities": [{"text": "HHMM parsing", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.6740320473909378}, {"text": "PCFG baseline", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9133324325084686}, {"text": "error", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.8988289833068848}]}, {"text": "However, because it is not possible to run the HHMM parser without binarization, and because Klein and Manning do not report results for binarization transforms in the absence of parent and sibling Markovization, it is potentially misleading to compare the results directly.", "labels": [], "entities": []}, {"text": "For example, it is possible that the binarization transforms described here may have performanceoptimizing effects that are latent in the binarized PCFG, but are brought out in HHMM parsing.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.883356511592865}, {"text": "HHMM parsing", "start_pos": 177, "end_pos": 189, "type": "TASK", "confidence": 0.7655295133590698}]}, {"text": "Results on Section 23 of this corpus show close to 84% recall and precision, comparable to that reported for state-of-the-art cubic-time parsers (with no constant bounds on processing storage) using similar configurations of conditioning information, that is, without lexicalization or smoothing.", "labels": [], "entities": [{"text": "Section 23 of this corpus", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.6737334430217743}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9994632601737976}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9971264004707336}]}, {"text": "Roark (2001) describes a similar incremental parser based on left-corner transformed grammars, and also reports results for parsing with and without parent and sibling Markovization.", "labels": [], "entities": []}, {"text": "Again the performance is comparable under similar conditions, bottom).", "labels": [], "entities": []}, {"text": "This system was run with abeam width of 2,000 hypotheses.", "labels": [], "entities": [{"text": "width", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9396923184394836}]}, {"text": "This beam width was selected in order to compare the performance of the bounded-memory model, which predicts in-element or cross-element composition, with that of conventional broad-coverage parsers, which also maintain large beams.", "labels": [], "entities": []}, {"text": "With better modeling and vastly more data from which to learn, it is possible that the human processor may need to maintain far fewer alternative analyses, or perhaps only one, conditioned on a lookahead window of observations).", "labels": [], "entities": []}, {"text": "These experiments used a maximum stack depth of four, and conditioned expansion and transition probabilities for each q d ton only the portion of the parent category following the slash (that is, only A 2 of A 1 /A 2 ), in order to avoid sparse data effects.", "labels": [], "entities": []}, {"text": "Examples requiring more than four stack elements were excluded from training.", "labels": [], "entities": []}, {"text": "This is because in the basic relative frequency estimation used here, training examples are depth-specific.", "labels": [], "entities": [{"text": "relative frequency estimation", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.56928750872612}]}, {"text": "Because the (unpunctuated) training set contains only about a dozen sentences requiring more than four depth levels, each occupying that level for only a few words, the data on which the fifth level of this model would be trained are very sparse.", "labels": [], "entities": []}, {"text": "Models at greater stack depths, and models depending on complete parent categories (or grandparent categories, etc., as in state-of-the-art parsers) could be developed using smoothing and backoff techniques or feature-based log-linear models, but this is left for later work (see Section 7).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Percent coverage of right-corner transformed Switchboard Treebank Sections 2-3.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8267621994018555}, {"text": "Switchboard Treebank Sections", "start_pos": 55, "end_pos": 84, "type": "DATASET", "confidence": 0.8186241189638773}]}, {"text": " Table 2  Percent coverage of left-and right-corner transformed WSJ Treebank Sections 2-21.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8685005307197571}, {"text": "WSJ Treebank Sections 2-21", "start_pos": 64, "end_pos": 90, "type": "DATASET", "confidence": 0.9750797152519226}]}]}