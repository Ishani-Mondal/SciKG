{"title": [{"text": "Automatically Identifying the Source Words of Lexical Blends in English", "labels": [], "entities": [{"text": "Automatically Identifying the Source Words of Lexical Blends", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7617329619824886}]}], "abstractContent": [{"text": "Newly coined words pose problems for natural language processing systems because they are not in a system's lexicon, and therefore no lexical information is available for such words.", "labels": [], "entities": []}, {"text": "A common way to form new words is lexical blending, as in cosmeceutical, a blend of cosmetic and pharmaceutical.", "labels": [], "entities": []}, {"text": "We propose a statistical model for inferring a blend's source words drawing on observed linguistic properties of blends; these properties are largely based on the recognizability of the source words in a blend.", "labels": [], "entities": []}, {"text": "We annotate a set of 1,186 recently coined expressions which includes 515 blends, and evaluate our methods on a 324-item subset.", "labels": [], "entities": []}, {"text": "In this first study of novel blends we achieve an accuracy of 40% on the task of inferring a blend's source words, which corresponds to a reduction in error rate of 39% over an informed baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9994893074035645}, {"text": "error rate", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9862382709980011}]}, {"text": "We also give preliminary results showing that our features for source word identification can be used to distinguish blends from other kinds of novel words.", "labels": [], "entities": [{"text": "source word identification", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6391709844271342}]}, {"text": "1. Lexical Blends Neologisms-newly coined words or new senses of an existing word-are constantly being introduced into a language (Algeo 1980; Lehrer 2003), often for the purpose of naming anew concept.", "labels": [], "entities": [{"text": "Lexical Blends Neologisms-newly coined words or new senses of an existing word-are", "start_pos": 3, "end_pos": 85, "type": "TASK", "confidence": 0.7850523243347803}]}, {"text": "Domains that are culturally prominent or that are rapidly advancing, such as electronic communication and the Internet, often contain many ne-ologisms, although novel words arise throughout a language (Ayto 1990, 2006; Knowles and Elliott 1997).", "labels": [], "entities": []}, {"text": "Consequently, any natural language processing (NLP) system operating on recently produced text will encounter new words.", "labels": [], "entities": []}, {"text": "Because lexical resources are often a key component of an NLP system, performance of the entire system will likely suffer due to missing lexical information for neologisms.", "labels": [], "entities": []}, {"text": "Ideally, an NLP system could identify neologisms as such, and then infer various aspects of their syntactic or semantic properties necessary for the computational task at hand.", "labels": [], "entities": []}, {"text": "Recent approaches to this kind of lexical acquisition task typically infer the target lexical information from statistical distributional properties of the terms.", "labels": [], "entities": [{"text": "lexical acquisition task", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8692431052525839}]}, {"text": "However, this technique is generally not", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The data set used in our previous study of blends consisted of expressions from the Macquarie Dictionary (Delbridge 1981) with an etymology entry indicating that they are blends.", "labels": [], "entities": [{"text": "Macquarie Dictionary (Delbridge 1981)", "start_pos": 84, "end_pos": 121, "type": "DATASET", "confidence": 0.9765775005022684}]}, {"text": "All of our statistical features were devised using the development portion of this data set, enabling us to use the full WORDSPLEND data set for testing.", "labels": [], "entities": [{"text": "WORDSPLEND data set", "start_pos": 121, "end_pos": 140, "type": "DATASET", "confidence": 0.8899293144543966}]}, {"text": "To compare our results to those in our earlier study, we also perform experiments on a subset of the previous data set.", "labels": [], "entities": []}, {"text": "We are uncertain as to whether a number of the blends from the Macquarie Dictionary are in fact blends.", "labels": [], "entities": [{"text": "Macquarie Dictionary", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.9696848690509796}]}, {"text": "For example, it does not match our intuition that clash is a blend of clap and dash.", "labels": [], "entities": []}, {"text": "We created a second data set of confirmed blends, MAC-CONF, consisting of only those blends from Macquarie that are found in at least one of two additional dictionaries with an etymology entry indicating that they are blends.", "labels": [], "entities": [{"text": "Macquarie", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.9813963770866394}]}, {"text": "We report results on the 30 expressions in the unseen test portion of MAC-CONF.", "labels": [], "entities": [{"text": "MAC-CONF", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8035691380500793}]}, {"text": "We generate candidate sets using two different lexicons: the CELEX lexicon, and a wordlist created from the Web 1T 5-gram Corpus (Brants and Franz 2006).", "labels": [], "entities": [{"text": "CELEX lexicon", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.973849356174469}, {"text": "Web 1T 5-gram Corpus", "start_pos": 108, "end_pos": 128, "type": "DATASET", "confidence": 0.5718834027647972}]}, {"text": "These are discussed further herein.", "labels": [], "entities": []}, {"text": "The frequency information needed to calculate the frequency features is extracted from the Web 1T 5-gram Corpus.", "labels": [], "entities": [{"text": "Web 1T 5-gram Corpus", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.8981572836637497}]}, {"text": "The length, contribution, and phonology features, as well as the syllable structure filter, are calculated on the basis of the source words themselves, or are derived from information in CELEX (when CELEX is the lexicon in use).", "labels": [], "entities": [{"text": "CELEX", "start_pos": 187, "end_pos": 192, "type": "DATASET", "confidence": 0.884213924407959}]}, {"text": "We compute semantic similarity between the source words using Jiang and Conrath's (1997) measure in the WordNet::Similarity package (Pedersen, Patwardhan, and Michelizzi 2004), and we compute semantic relatedness of the pair using the cosine between word co-occurrence vectors using software provided by.", "labels": [], "entities": []}, {"text": "We conduct separate experiments with the two different lexicons for candidate set creation.", "labels": [], "entities": [{"text": "candidate set creation", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.6519997318585714}]}, {"text": "We began by using CELEX, because it contains rich phonological information that some of our features draw on.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.8156871795654297}]}, {"text": "However, in our analysis of the results, we noted that for many expressions the correct candidate pair is not in the candidate set.", "labels": [], "entities": []}, {"text": "Many of the blends in WORDSPLEND are formed from words which are themselves new words, often coined for concepts related to the Internet, such as download, for example; such words are not listed in CELEX.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 198, "end_pos": 203, "type": "DATASET", "confidence": 0.9677160382270813}]}, {"text": "This motivated us to create a lexicon from a 2 Note that it would be possible to automatically infer the phonological and syllabic information required for our features using automatic approaches for text-to-phoneme conversion and syllabification (Bartlett, Kondrak, and Cherry 2008, for example).", "labels": [], "entities": [{"text": "text-to-phoneme conversion", "start_pos": 200, "end_pos": 226, "type": "TASK", "confidence": 0.7218808084726334}]}, {"text": "Although such techniques currently provide noisy information, phonological and syllabic information for the blend itself could also be inferred, allowing the development of features that exploit this information.", "labels": [], "entities": []}, {"text": "We leave exploring such possibilities for future work.", "labels": [], "entities": []}, {"text": "recent data set (the Web 1T 5-gram Corpus) that would be expected to contain many of these new coinages.", "labels": [], "entities": [{"text": "Web 1T 5-gram Corpus", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.8319070935249329}]}, {"text": "To form a lexicon from this corpus, we extract the 100K most frequent words, restricted to lowercase and all-alphabetic forms.", "labels": [], "entities": []}, {"text": "Using this lexicon we expect the correct source word pair to be in the candidate set for more expressions.", "labels": [], "entities": []}, {"text": "However, this comes at the expense of potentially larger candidate sets, due to the larger lexicon size.", "labels": [], "entities": []}, {"text": "Furthermore, since this lexicon does not contain phonological or syllabic representations of each word, we cannot extract three features: the feature for the syllable heuristic, and the two features that capture the tendency for the second source word to be longer than the first in terms of phonemes and syllables.", "labels": [], "entities": []}, {"text": "(We do calculate the phonological similarity between the two candidate source words, in terms of graphemes.)", "labels": [], "entities": []}, {"text": "Because each of our features is designed to have a high value fora correct source word pair and a low value otherwise, we can simply sum the features for each candidate pair to get a score for each pair indicating its degree of goodness as a source word pair for the blend under consideration.", "labels": [], "entities": []}, {"text": "However, because our various features have values falling on differing ranges, we first normalize the feature values by subtracting the mean of that feature within that candidate set and dividing by the corresponding standard deviation.", "labels": [], "entities": []}, {"text": "We also take the arctan of each resulting feature value to reduce the influence of outliers.", "labels": [], "entities": [{"text": "arctan", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9746161699295044}]}, {"text": "We then sum the feature values for each candidate pair, and order the pairs within each candidate set according to this sum.", "labels": [], "entities": []}, {"text": "This ranks the pairs in terms of decreasing degree of goodness as a source word pair.", "labels": [], "entities": []}, {"text": "We refer to this method as the feature ranking approach.", "labels": [], "entities": [{"text": "feature ranking", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7540897727012634}]}, {"text": "We also use a machine learning approach applied to the features in a training regimen.", "labels": [], "entities": []}, {"text": "Our task can be viewed as a classification problem in which each candidate pair is either a positive instance (the correct source word pair) or a negative instance (an incorrect source word pair).", "labels": [], "entities": []}, {"text": "However, a standard machine learning algorithm does not directly apply because of the structure of the problem space.", "labels": [], "entities": []}, {"text": "In classification, we typically look fora hyperplane that separates the positive and negative training examples.", "labels": [], "entities": [{"text": "classification", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.9636756777763367}]}, {"text": "In the context of our problem, this corresponds to separating all the correct candidate pairs (for all blends in our data set) from all the incorrect candidate pairs.", "labels": [], "entities": []}, {"text": "However, such an approach is undesirable as it ignores the structure of the candidate sets; it is only necessary to separate the correct source word pair fora given blend from the corresponding incorrect candidate pairs (i.e., for the same blend).", "labels": [], "entities": []}, {"text": "This is also inline with the formulation of our features, which are designed to give relatively higher values to correct candidate pairs than incorrect candidate pairs within the candidate set fora given blend; it is not necessarily the case that the feature values for the correct candidate pair fora given blend will be higher than those for an incorrect candidate pair for another blend.", "labels": [], "entities": []}, {"text": "In other words, the features are designed to give values that are relative to the candidates fora particular blend.", "labels": [], "entities": []}, {"text": "To address this issue, we use aversion of the perceptron algorithm similar to that proposed by.", "labels": [], "entities": []}, {"text": "In this approach, the classifier is trained by only adjusting the perceptron weight vector when the correct candidate pair is not scored higher than the incorrect pairs for the target blend (not across all the candidate pairs for all blends).", "labels": [], "entities": []}, {"text": "Furthermore, to accommodate for the large variation in candidate set size we use an uneven margin-in this case the distance between the weighted sum of the feature vector fora correct and incorrect candidate pair-of 1 #correct cand.", "labels": [], "entities": []}, {"text": "pairs \u00d7 #incorrect cand.", "labels": [], "entities": [{"text": "incorrect", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9663228392601013}]}, {"text": "pairs . We therefore learn a single weight vector such that, within each candidate set, the correct candidate pairs are scored higher than the incorrect candidate pairs by a factor of this margin.", "labels": [], "entities": []}, {"text": "When updating the weight vector, we multiply the update that we add to the weight vector by a factor of this margin to prevent the classifier from being overly influenced by large candidate sets.", "labels": [], "entities": []}, {"text": "During testing, each candidate pair is ranked according to the weighted sum of its feature vector.", "labels": [], "entities": []}, {"text": "To evaluate this approach, on each of WORDSPLEND and MAC-CONF we perform 10-fold cross-validation with 10 random restarts.", "labels": [], "entities": [{"text": "WORDSPLEND", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.6784588694572449}]}, {"text": "In these experiments, we use our syllable heuristic as a feature, rather than as a filter, to allow the learner to weight it appropriately.", "labels": [], "entities": []}, {"text": "We evaluate our methods according to two measures: accuracy and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9997010827064514}, {"text": "mean reciprocal rank (MRR)", "start_pos": 64, "end_pos": 90, "type": "METRIC", "confidence": 0.9113140205542246}]}, {"text": "Under the accuracy measure, the system is scored as correct if it ranks one of the correct source word pairs fora given blend first, and as incorrect otherwise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994970560073853}]}, {"text": "The MRR gives the mean of the rank of the highest ranked correct source word pair for each blend.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8733161687850952}]}, {"text": "Although accuracy is more stringent than MRR, we are interested in MRR to see where the system ranks the correct source word pair in the case that it is not ranked first.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9992057681083679}, {"text": "MRR", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.7817620038986206}, {"text": "MRR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.6224734783172607}]}, {"text": "We compare the accuracy of our system against a chance (random) baseline, and an informed baseline in which the feature ranking approach is applied using just two of our features, the frequency of each candidate source word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9991440773010254}]}], "tableCaptions": [{"text": " Table 4  Percent of expressions (% exps) with their source words in each lexical resource and candidate  set (CS), and after applying the syllable heuristic filter on the CELEX CS, as well as median CS  size, for both the WORDSPLEND and MAC-CONF data sets.", "labels": [], "entities": [{"text": "CELEX CS", "start_pos": 172, "end_pos": 180, "type": "DATASET", "confidence": 0.6954531967639923}, {"text": "WORDSPLEND and MAC-CONF data sets", "start_pos": 223, "end_pos": 256, "type": "DATASET", "confidence": 0.7179800271987915}]}, {"text": " Table 5  Percent accuracy on blends in WORDSPLEND and MAC-CONF using the feature ranking  approach. The size of each data set is given in parentheses. The lexicon employed (CELEX or  WEB 1T) is indicated. The best accuracy obtained using this approach for each data set and  lexicon is shown in boldface. * = results that are significantly better than the informed baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9991509914398193}, {"text": "CELEX", "start_pos": 174, "end_pos": 179, "type": "METRIC", "confidence": 0.8691498637199402}, {"text": "WEB 1T)", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.6660062174002329}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9949984550476074}]}, {"text": " Table 6  Percent accuracy on blends in WORDSPLEND and MAC-CONF using the modified perceptron  algorithm. The size of each data set is given in parentheses. The lexicon employed (CELEX or  WEB 1T) is indicated. * = results that are significantly better than the informed baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9993481040000916}, {"text": "CELEX", "start_pos": 179, "end_pos": 184, "type": "METRIC", "confidence": 0.7435264587402344}, {"text": "WEB 1T)", "start_pos": 189, "end_pos": 196, "type": "METRIC", "confidence": 0.5666302839914957}]}]}