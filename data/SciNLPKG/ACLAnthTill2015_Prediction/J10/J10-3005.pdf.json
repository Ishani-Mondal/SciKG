{"title": [], "abstractContent": [{"text": "Sentence compression holds promise for many applications ranging from summarization to subtitle generation.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9333585202693939}, {"text": "summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9839645624160767}, {"text": "subtitle generation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.6977230161428452}]}, {"text": "The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents.", "labels": [], "entities": []}, {"text": "In this article we present a discourse-informed model which is capable of producing document compressions that are coherent and informative.", "labels": [], "entities": [{"text": "document compressions", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.702694371342659}]}, {"text": "Our model is inspired by theories of local coherence and formulated within the framework of integer linear programming.", "labels": [], "entities": []}, {"text": "Experimental results show significant improvements over a state-of-the-art discourse agnostic approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have witnessed increasing interest in sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.8160467743873596}]}, {"text": "The task encompasses automatic methods for shortening sentences with minimal information loss while preserving their grammaticality.", "labels": [], "entities": []}, {"text": "The popularity of sentence compression is largely due to its relevance for applications.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.859397292137146}]}, {"text": "Summarization is a casein point here.", "labels": [], "entities": []}, {"text": "Most summarizers to date aim to produce informative summaries at a given compression rate.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9726841449737549}]}, {"text": "If we can have a compression component that reduces sentences to a minimal length and still retains the most important content, then we should be able to pack more information content into a fixed size summary.", "labels": [], "entities": []}, {"text": "In other words, sentence compression would allow summarizers to increase the overall amount of information extracted without increasing the summary length).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7424508333206177}]}, {"text": "It could also be used as a post-processing step in order to render summaries more coherent and less repetitive.", "labels": [], "entities": []}, {"text": "Beyond summarization, a sentence compression module could be used to display text on small screen devices such as PDAs (Corston-Oliver 2001) or as a reading aid for the blind).", "labels": [], "entities": [{"text": "summarization", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.9794146418571472}, {"text": "sentence compression", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7284287959337234}]}, {"text": "Sentence compression could also benefit information retrieval by eliminating extraneous information from the documents indexed by the retrieval engine.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9240664839744568}, {"text": "information retrieval", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.764203667640686}]}, {"text": "This way it would be possible to store less information in the index without dramatically affecting retrieval performance.", "labels": [], "entities": []}, {"text": "In theory, sentence compression may involve several rewrite operations such as deletion, substitution, insertion, and word reordering.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.7791638374328613}]}, {"text": "In practice, however, the task is commonly defined as a word deletion problem: Given an input sentence of words x = x 1 , x 2 , . .", "labels": [], "entities": []}, {"text": ", x n , the aim is to produce a compression by removing any subset of these words.", "labels": [], "entities": []}, {"text": "Many sentence compression models aim to learn deletion rules from a parsed parallel corpus of source sentences and their target compressions.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.7148882746696472}]}, {"text": "For example, learn asynchronous context-free grammar (Aho and Ullman 1969) from such a corpus.", "labels": [], "entities": []}, {"text": "The grammar rules have weights (essentially probabilities estimated using maximum likelihood) and are used to find the best compression from the set of all possible compressions fora given sentence.", "labels": [], "entities": []}, {"text": "Other approaches exploit syntactic information without making explicit use of a parallel grammar-for example, by learning which words or constituents to delete from a parse tree (.", "labels": [], "entities": []}, {"text": "Despite differences in formulation and training requirements (some approaches require a parallel corpus, whereas others do not), existing models are similar in that they compress sentences in isolation without taking their surrounding context into account.", "labels": [], "entities": []}, {"text": "This is in marked contrast with common practice in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9852929711341858}]}, {"text": "Professional abstractors often rely on contextual cues while creating summaries.", "labels": [], "entities": []}, {"text": "This is true of automatic summarization systems too, which consider the position of a sentence in a document and how it relates to its surrounding sentences.", "labels": [], "entities": [{"text": "summarization", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.7721021175384521}]}, {"text": "Determining which information is important in a sentence is not merely a function of its syntactic position (e.g., deleting the verb or the subject of a sentence is less likely).", "labels": [], "entities": []}, {"text": "A variety of contextual factors can play a role, such as the discourse topic, whether the sentence introduces new entities or events that have not been mentioned before, or the reader's background knowledge.", "labels": [], "entities": []}, {"text": "A sentence-centric view of compression is also at odds with most relevant applications which aim to create a shorter document rather than a single sentence.", "labels": [], "entities": []}, {"text": "The resulting document must not only be grammatical but also coherent if it is to function as a replacement for the original.", "labels": [], "entities": []}, {"text": "However, this cannot be guaranteed without knowledge of how the discourse progresses from sentence to sentence.", "labels": [], "entities": []}, {"text": "To give a simple example, a contextually aware compression system could drop a word or phrase from the current sentence, simply because it is not mentioned anywhere else in the document and is therefore deemed unimportant.", "labels": [], "entities": []}, {"text": "Or it could decide to retain it for the sake of topic continuity.", "labels": [], "entities": [{"text": "topic continuity", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.8479906916618347}]}, {"text": "In this article we are interested in creating a compression model that is appropriate for both documents and sentences.", "labels": [], "entities": []}, {"text": "Luckily, a variety of discourse theories have been developed over the years (e.g., and have found application in summarization ( and other text generation applications (Scott and de Souza 1990;).", "labels": [], "entities": [{"text": "summarization", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.9920966625213623}, {"text": "text generation", "start_pos": 139, "end_pos": 154, "type": "TASK", "confidence": 0.7108395993709564}]}, {"text": "In creating a contextsensitive compression model we are faced with three important questions: (1) Which type of discourse information is useful for compression?", "labels": [], "entities": []}, {"text": "(2) Is it amenable to automatic processing (there is little hope for interfacing our compression model with applications if discourse-level cues cannot be identified robustly)? and (3) How are sentence-and document-based information best integrated in a unified modeling framework?", "labels": [], "entities": []}, {"text": "In building our compression model we borrow insights from two popular models of discourse, Centering Theory ( and lexical chains.", "labels": [], "entities": []}, {"text": "Both approaches capture local coherence-the way adjacent sentences bind together to form a larger discourse.", "labels": [], "entities": []}, {"text": "They also both share the view that discourse coherence revolves around discourse entities and the way they are introduced and discussed.", "labels": [], "entities": []}, {"text": "We first automatically augment our documents with annotations pertaining to centering and lexical chains, which we subsequently use to inform our compression model.", "labels": [], "entities": []}, {"text": "The latter is an extension of the integer linear programming formulation proposed by.", "labels": [], "entities": []}, {"text": "Ina nutshell, sentence compression is modeled as an optimization problem.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.8450535833835602}]}, {"text": "Given along sentence, a compression is formed by retaining the words that maximize a scoring function coupled with a small number of constraints ensuring that the resulting output is grammatical.", "labels": [], "entities": []}, {"text": "The constraints are encoded as linear inequalities whose solution is found using integer linear programming (ILP;.", "labels": [], "entities": []}, {"text": "Discourse-level information can be straightforwardly incorporated by slightly changing the compression objectivewe now wish to compress entire documents rather than isolated sentences-and augmenting the constraint set with discourse-specific constraints.", "labels": [], "entities": []}, {"text": "We use our model to compress whole documents (rather than sentences sequentially) and evaluate whether the resulting text is understandable and informative using a question-answering task.", "labels": [], "entities": []}, {"text": "We show that our method yields significant improvements over discourse agnostic state-of-the-art compression models.", "labels": [], "entities": [{"text": "discourse agnostic state-of-the-art compression", "start_pos": 61, "end_pos": 108, "type": "TASK", "confidence": 0.629434622824192}]}, {"text": "The remainder of this article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of related work.", "labels": [], "entities": []}, {"text": "In Section 3 we present the ILP framework and compression model we employ in our experiments.", "labels": [], "entities": []}, {"text": "We introduce our discourse-related extensions in Sections 4 and 5.", "labels": [], "entities": []}, {"text": "Section 6 discusses our experimental set-up and evaluation methodology.", "labels": [], "entities": []}, {"text": "Our results are presented in Section 7.", "labels": [], "entities": []}, {"text": "Discussion of future work concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental set-up for assessing the performance of the compression model.", "labels": [], "entities": []}, {"text": "We describe the compression corpus used in our study, briefly introduce the model used for comparison with our approach, and explain how system output was evaluated.", "labels": [], "entities": []}, {"text": "Previous studies evaluate the well-formedness of automatically generated compressions out of context.", "labels": [], "entities": []}, {"text": "The target sentences are typically rated by naive subjects on two dimensions, grammaticality and importance.", "labels": [], "entities": []}, {"text": "Automatic evaluation measures have also been proposed.", "labels": [], "entities": []}, {"text": "compare the grammatical relations found in the system output against those found in a gold standard using F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9884191751480103}]}, {"text": "Although F1 conflates grammaticality and importance into a single score, it nevertheless has been shown to correlate reliably with human judgments.", "labels": [], "entities": [{"text": "F1", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9983933568000793}]}, {"text": "The aims of our evaluation study were twofold.", "labels": [], "entities": []}, {"text": "Firstly, we wanted to examine whether our discourse constraints improve the compressions for individual sentences.", "labels": [], "entities": []}, {"text": "There is no hope for generating shorter documents if the compressed sentences are either too wordy or too ungrammatical.", "labels": [], "entities": []}, {"text": "Secondly and more importantly, our goal was to evaluate the compressed documents as a whole by examining whether they are readable and the degree to which they retain key information when compared to the originals.", "labels": [], "entities": []}, {"text": "We evaluated sentence-based compressions automatically using F1 and the grammatical relations annotations provided by RASP (.", "labels": [], "entities": [{"text": "sentence-based compressions", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6845442056655884}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9974532723426819}, {"text": "RASP", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.864170253276825}]}, {"text": "This parser is suited to the compression task as it provides parses for both full sentences and sentence fragments and is generally robust enough to analyze semi-grammatical sentences.", "labels": [], "entities": []}, {"text": "We computed F1 overall the relations provided by RASP (e.g., subject, direct/indirect object, modifier; 17 in total).", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9946562051773071}, {"text": "RASP", "start_pos": 49, "end_pos": 53, "type": "TASK", "confidence": 0.6391434073448181}]}, {"text": "We compared the output of our discourse system on the test set (31 documents, 604 sentences) against the sentence-based ILP model and.", "labels": [], "entities": []}, {"text": "Our document-level evaluation was motivated by two questions: (1) Are the compressed documents readable? and (2) How much key information is preserved between the source document and its target compression?", "labels": [], "entities": []}, {"text": "The readability of a document is fairly straightforward to measure by asking participants to provide a rating (e.g., on a seven-point scale).", "labels": [], "entities": []}, {"text": "Measuring how much information is preserved in the compressed document is more involved.", "labels": [], "entities": []}, {"text": "Under the assumption that the target document is to function as a replacement for the source, we can measure the extent to which the compressed version can be used to find answers for questions which have been derived from the source and are representative of its core content.", "labels": [], "entities": []}, {"text": "We thus created questions from the source and then determined whether it was possible to find their answers by reading the compressed target.", "labels": [], "entities": []}, {"text": "The more questions a hypothetical compression system can answer, the better it is at compressing the document as a whole.", "labels": [], "entities": []}, {"text": "A question-answering (Q&A) paradigm has been used previously to evaluate summaries and text compression.", "labels": [], "entities": [{"text": "summaries", "start_pos": 73, "end_pos": 82, "type": "TASK", "confidence": 0.9695340991020203}, {"text": "text compression", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7484259009361267}]}, {"text": "performed one of the first Q&A evaluations to investigate the degree to which documents could be summarized before reading comprehension diminished.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8217377265294393}]}, {"text": "Their corpus consisted of four passages randomly selected from a set of sample Graduate Management Aptitude Test (GMAT) reading comprehension tests.", "labels": [], "entities": [{"text": "Graduate Management Aptitude Test (GMAT) reading comprehension", "start_pos": 79, "end_pos": 141, "type": "TASK", "confidence": 0.5802634788884057}]}, {"text": "The texts covered a range of topics including medieval literature, 18th-century Japan, minority-operated businesses, and Florentine art.", "labels": [], "entities": []}, {"text": "Accompanying each text were eight multiple-choice questions, each containing five possible answers.", "labels": [], "entities": []}, {"text": "The questions were provided by the Educational Testing Service and were designed to measure the subjects' reading comprehension.", "labels": [], "entities": [{"text": "Educational Testing Service", "start_pos": 35, "end_pos": 62, "type": "DATASET", "confidence": 0.9187651872634888}]}, {"text": "Subjects were given various textual treatments: the full text, a human-authored abstract, three systemgenerated extracts, and a final treatment where merely the questions were presented without any text.", "labels": [], "entities": []}, {"text": "The questions-only treatment was used as a control to investigate if subjects could answer questions without any source material.", "labels": [], "entities": []}, {"text": "Subjects were instructed to read the passage (if provided) and answer the multiple choice questions.", "labels": [], "entities": []}, {"text": "The advantage of using standardized tests, such as the GMAT reading comprehension test, is that Q&A pairs are provided along with a method for scoring answers (the correct answer is one among five possible choices).", "labels": [], "entities": [{"text": "GMAT reading comprehension", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.8362177610397339}]}, {"text": "However, our corpora do not contain ready prepared Q&A pairs; thus we require a methodology for constructing questions and their answers and scoring documents against the answers.", "labels": [], "entities": []}, {"text": "One such methodology is presented in the TIPSTER Text Summarization Evaluation (SUMMAC;.", "labels": [], "entities": [{"text": "TIPSTER Text Summarization Evaluation", "start_pos": 41, "end_pos": 78, "type": "TASK", "confidence": 0.662567749619484}]}, {"text": "SUMMAC was concerned with producing summaries tailored to specific topics.", "labels": [], "entities": []}, {"text": "The Q&A task involved an evaluation where a topic-related summary fora document was evaluated in terms of its \"informativeness,\" namely, the degree to which it contained answers found in the source document to a set of topic-related questions.", "labels": [], "entities": [{"text": "Q&A task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.890232041478157}]}, {"text": "For each topic (three in total), 30 relevant documents were chosen to generate a single summary.", "labels": [], "entities": []}, {"text": "One annotator per topic came up with no more than five questions relating to the obligatory aspects of the topic.", "labels": [], "entities": []}, {"text": "An obligatory aspect of a topic was defined as information that must be present in the document for the document to be relevant to the topic.", "labels": [], "entities": []}, {"text": "The annotators then created an answer key for their topic by annotating the passages and phrases from the documents which provided the answers to the questions.", "labels": [], "entities": []}, {"text": "In the SUMMAC evaluation, the annotator for each topic was tasked with scoring the system summaries.", "labels": [], "entities": [{"text": "SUMMAC evaluation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.6421205997467041}]}, {"text": "Scoring involved comparing the summaries against the answer key (annotated passages from the source documents) while judging whether the summary provided a Correct, Partially Correct, or Missing answer.", "labels": [], "entities": [{"text": "Scoring", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9723460078239441}]}, {"text": "If a summary contained an answer key and sufficient context the summary was deemed correct; however, summaries would be considered partially correct if the answer key was present but with insufficient context.", "labels": [], "entities": []}, {"text": "If context was completely missing, misleading, or the answer key was absent then the summary was judged missing.", "labels": [], "entities": []}, {"text": "Our methodology for constructing Q&A pairs and for scoring documents is inspired by the SUMMAC evaluation exercise (.", "labels": [], "entities": []}, {"text": "Rather than creating questions for document sets (or topics) our questions were derived from individual documents.", "labels": [], "entities": []}, {"text": "Two annotators were independently instructed to read the documents from our (test) corpus and create Q&A pairs.", "labels": [], "entities": []}, {"text": "Each annotator drafted no more than ten questions and answers per document, related to its content.", "labels": [], "entities": []}, {"text": "Annotators were asked to create fact-based questions which required an unambiguous answer; these were typically who, what, where, when, and how-style questions.", "labels": [], "entities": []}, {"text": "The purpose of using two annotators per document was to allow annotators to compare and revise their Q&A pairs; this process was repeated until a common agreed-upon set of questions was reached.", "labels": [], "entities": []}, {"text": "Revisions typically involved merging and simplifying questions to make them clearer, and in some cases splitting a question into multiple questions.", "labels": [], "entities": []}, {"text": "Documents for which too few questions were agreed upon and for which the questions and answers were too ambiguous were removed.", "labels": [], "entities": []}, {"text": "This left an evaluation set of six documents with between five to eight concise questions per document.", "labels": [], "entities": []}, {"text": "shows a document from our test set and the questions and answers our annotators created for it.", "labels": [], "entities": []}, {"text": "For scoring our documents we adopt a more objective method than SUMMAC.", "labels": [], "entities": []}, {"text": "Instead of asking the annotator who constructed the questions to check the document compressions for the answers, we ask naive participants to read the compressed documents and answer the questions as best as they can.", "labels": [], "entities": []}, {"text": "During evaluation, the source document is not shown to our subjects; thus, if the compression is difficult to read, the Example document from our test set and questions with answer key created for this document.", "labels": [], "entities": []}, {"text": "participants have no point of reference to help them understand the compression.", "labels": [], "entities": []}, {"text": "This is a departure from previous evaluations within text generation tasks, where the source text is available at judgment time; in our case only the system output is available.", "labels": [], "entities": [{"text": "text generation tasks", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8061520258585612}]}, {"text": "The document-based evaluation was conducted remotely over the Internet using a custom-built Web interface.", "labels": [], "entities": []}, {"text": "Upon loading the Web interface, participants were presented with a set of instructions that explained the Q&A task and provided examples.", "labels": [], "entities": [{"text": "Q&A task", "start_pos": 106, "end_pos": 114, "type": "TASK", "confidence": 0.7734591960906982}]}, {"text": "Subjects were first asked to read the compressed document and then rate its readability on a seven-point scale where 7 = excellent, and 1 = terrible.", "labels": [], "entities": []}, {"text": "Next, questions were presented one at a time (the order being is defined by the annotators) and participants were encouraged to consult the document for the answer.", "labels": [], "entities": []}, {"text": "Answers were written directly into a text field on the Web interface which allowed free-form text to be submitted.", "labels": [], "entities": []}, {"text": "Once a participant provided an answer and confirmed the answer, the interface locked the answer to ensure it was not modified later.", "labels": [], "entities": []}, {"text": "This was necessary because later questions could reveal information which would help answer previous questions.", "labels": [], "entities": []}, {"text": "We elicited answers for six documents in four compression conditions: gold standard, using the ILP sentence-based model, the ILP discourse model, and model.", "labels": [], "entities": []}, {"text": "A Latin square design was used to prevent participants from seeing multiple treatments (compressions) of the same document thus removing any learning effect.", "labels": [], "entities": []}, {"text": "A total of 116 unpaid volunteers completed the experiment.", "labels": [], "entities": []}, {"text": "They were recruited through student mailing lists and the Language Experiments Web site.", "labels": [], "entities": []}, {"text": "The answers provided by our subjects were scored against an answer key.", "labels": [], "entities": []}, {"text": "A correct answer was marked with a score of one, and zero otherwise.", "labels": [], "entities": []}, {"text": "In cases where two answers were required, a score of 0.5 was awarded to each correct answer.", "labels": [], "entities": []}, {"text": "The score fora compressed document is the average of its question scores.", "labels": [], "entities": []}, {"text": "All subsequent tests and comparisons are performed on the document score.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Compression results: compression rate and relation-based F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9892064332962036}]}]}