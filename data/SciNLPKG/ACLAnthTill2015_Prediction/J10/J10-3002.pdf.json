{"title": [{"text": "Discriminative Word Alignment by Linear Modeling", "labels": [], "entities": [{"text": "Discriminative Word Alignment", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6627922157446543}]}], "abstractContent": [{"text": "Word alignment plays an important role in many NLP tasks as it indicates the correspondence between words in a parallel text.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7157835215330124}]}, {"text": "Although widely used to align large bilingual corpora, gen-erative models are hard to extend to incorporate arbitrary useful linguistic information.", "labels": [], "entities": []}, {"text": "This article presents a discriminative framework for word alignment based on a linear model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7944507598876953}]}, {"text": "Within this framework, all knowledge sources are treated as feature functions, which depend on a source language sentence, a target language sentence, and the alignment between them.", "labels": [], "entities": []}, {"text": "We describe a number of features that could produce symmetric alignments.", "labels": [], "entities": []}, {"text": "Our model is easy to extend and can be optimized with respect to evaluation metrics directly.", "labels": [], "entities": []}, {"text": "The model achieves state-of-the-art alignment quality on three word alignment shared tasks for five language pairs with varying divergence and richness of resources.", "labels": [], "entities": []}, {"text": "We further show that our approach improves translation performance for various statistical machine translation systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9646219611167908}, {"text": "statistical machine translation", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6259716053803762}]}], "introductionContent": [{"text": "Word alignment, which can be defined as an object for indicating the corresponding words in a parallel text, was first introduced as an intermediate result of statistical machine translation (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7230198979377747}, {"text": "statistical machine translation", "start_pos": 159, "end_pos": 190, "type": "TASK", "confidence": 0.6245342691739401}]}, {"text": "Consider the following Chinese sentence and its English translation: translation assessment and critiquing tools, text generation, bilingual lexigraphy, and word sense disambiguation.", "labels": [], "entities": [{"text": "translation assessment", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.9480101764202118}, {"text": "text generation", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7617383599281311}, {"text": "word sense disambiguation", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.6748392979303995}]}, {"text": "Various methods have been proposed for finding word alignments between parallel texts.", "labels": [], "entities": [{"text": "finding word alignments between parallel texts", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.7806285818417867}]}, {"text": "Among them, generative alignment models ( have been widely used to produce word alignments for large bilingual corpora.", "labels": [], "entities": [{"text": "generative alignment", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.9664375483989716}, {"text": "word alignments", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.7342535108327866}]}, {"text": "Describing the relationship of a bilingual sentence pair, a generative model treats word alignment as a hidden process and maximizes the likelihood of a training corpus using the expectation maximization (EM) algorithm.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.752147912979126}]}, {"text": "After the maximization process is complete, the unknown model parameters are determined and the word alignments are set to the maximum posterior predictions of the model.", "labels": [], "entities": []}, {"text": "However, one drawback of generative models is that they are hard to extend.", "labels": [], "entities": [{"text": "generative", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9636074900627136}]}, {"text": "Generative models usually impose strong independence assumptions between sub-models, making it very difficult to incorporate arbitrary features explicitly.", "labels": [], "entities": []}, {"text": "For example, when considering whether to align two words, generative models cannot include information about lexical and syntactic features such as part of speech and orthographic similarity in an easy way.", "labels": [], "entities": []}, {"text": "Such features would allow for more effective use of sparse data and result in a model that is more robust in the presence of unseen words.", "labels": [], "entities": []}, {"text": "Extending a generative model requires that the interdependence of information sources be modeled explicitly, which often makes the resulting system quite complex.", "labels": [], "entities": []}, {"text": "In this article, we introduce a discriminative framework for word alignment based on the linear modeling approach.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.8029553294181824}]}, {"text": "Within this framework, we treat all knowledge sources as feature functions that depend on a source sentence, a target sentence, and the alignment between them.", "labels": [], "entities": []}, {"text": "Each feature function is associated with a feature weight.", "labels": [], "entities": []}, {"text": "The linear combination of features gives an overall score to each candidate alignment.", "labels": [], "entities": []}, {"text": "The best alignment is the one with the highest overall score.", "labels": [], "entities": []}, {"text": "A linear model not only allows for easy integration of new features, but also admits optimizing feature weights directly with respect to evaluation metrics.", "labels": [], "entities": []}, {"text": "Experimental results show that our approach improves both alignment quality and translation performance significantly.", "labels": [], "entities": [{"text": "alignment", "start_pos": 58, "end_pos": 67, "type": "TASK", "confidence": 0.9468374848365784}, {"text": "translation", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9253087043762207}]}, {"text": "This article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a formal description of our model.", "labels": [], "entities": []}, {"text": "We show how to train feature weights by taking evaluation metrics into account and how to find the most probable alignment in an exponential search space efficiently.", "labels": [], "entities": []}, {"text": "Section 3 describes a number of features used in our experiments, focusing on the features that produce symmetric alignments.", "labels": [], "entities": []}, {"text": "In Section 4, we evaluate our model in both alignment and translation tasks.", "labels": [], "entities": [{"text": "translation", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.8602867722511292}]}, {"text": "Section 5 reviews previous work related to our approach and the article closes with a conclusion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first metric is alignment error rate (AER), proposed by.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 20, "end_pos": 46, "type": "METRIC", "confidence": 0.8940116663773855}]}, {"text": "AER has been used as official evaluation criterion inmost word alignment shared tasks.", "labels": [], "entities": [{"text": "AER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9805693626403809}, {"text": "word alignment shared tasks", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.7905781120061874}]}, {"text": "Och and Ney define two kinds of links in hand-aligned alignments: sure links for alignments that are unambiguous and possible links for ambiguous alignments.", "labels": [], "entities": []}, {"text": "Sure links usually connect content words such as Zhongguo and China.", "labels": [], "entities": [{"text": "China", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.8938065767288208}]}, {"text": "In contrast, possible links often align words within idiomatic expressions and free translations.", "labels": [], "entities": []}, {"text": "An AER score is given by AER(S, P, A) = 1 \u2212 |A \u2229 S| + |A \u2229 P| |A| + |S| where S is a set of sure links in a reference alignment that is hand-aligned by human experts, P is a set of possible links in the reference alignment, and A is a candidate alignment.", "labels": [], "entities": [{"text": "AER score", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9777646958827972}, {"text": "AER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9979673027992249}]}, {"text": "Note that S is a subset of P: S \u2286 P.", "labels": [], "entities": []}, {"text": "The lower the AER score is, the better the alignment quality is.", "labels": [], "entities": [{"text": "AER score", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9880044758319855}]}, {"text": "Although widely used, AER has been criticized for correlating poorly with translation quality.", "labels": [], "entities": [{"text": "AER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9845355153083801}]}, {"text": "In other words, lower AER scores do not necessarily lead to better translation quality.", "labels": [], "entities": [{"text": "AER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9969533681869507}, {"text": "translation", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.954986035823822}]}, {"text": "1 argue that reference alignments should consist of only sure links.", "labels": [], "entities": []}, {"text": "They propose anew measure called the balanced F-measure: where \u03b1 is a parameter that sets the trade-off between precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.8801712989807129}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9985843896865845}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9882651567459106}]}, {"text": "Higher F-measure means better alignment quality.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9989256262779236}, {"text": "alignment", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.9395378232002258}]}, {"text": "Obviously, \u03b1 less than 0.5 weights recall higher, whereas \u03b1 greater than 0.5 weights precision higher.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9879255294799805}]}, {"text": "We use both AER and F-measure in our experiments.", "labels": [], "entities": [{"text": "AER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9993507266044617}, {"text": "F-measure", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9957661628723145}]}, {"text": "AER is used in experiments evaluating alignment quality (Section 4.1) and F-measure is used in experiments evaluating translation performance (Section 4.2).", "labels": [], "entities": [{"text": "AER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.984943687915802}, {"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9961109757423401}, {"text": "translation", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.950107991695404}]}, {"text": "In this section, we try to answer two questions: 1.", "labels": [], "entities": []}, {"text": "Does the proposed approach achieve higher alignment quality than generative alignment models?", "labels": [], "entities": [{"text": "generative alignment", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.933284729719162}]}, {"text": "2. Do statistical machine translation systems produce better translations if we replace generative alignment models with the proposed approach?", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.6221831142902374}, {"text": "generative alignment", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.9376255869865417}]}, {"text": "In Section 4.1, we evaluate our approach on three word alignment shared tasks for five language pairs with varying divergence and richness of resources.", "labels": [], "entities": []}, {"text": "Experimental return {g o2o , g o2m , g m2o , g m2m } \ud97b\udf59 return the four feature gains 39: end procedure results show that our system outperforms systems participating in the three shared tasks significantly and achieves comparable results with other state-of-the-art discriminative alignment models.", "labels": [], "entities": []}, {"text": "In Section 4.2, we investigate the effect of our model on translation quality.", "labels": [], "entities": []}, {"text": "By training feature weights with respect to F-measure instead of AER, our model results in superior translation quality over generative methods for phrase-based, hierarchical phrase-based, and tree-to-string SMT systems.", "labels": [], "entities": [{"text": "AER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9966917037963867}, {"text": "SMT", "start_pos": 208, "end_pos": 211, "type": "TASK", "confidence": 0.8713670372962952}]}, {"text": "In this section, we present results of experiments on three word alignment shared tasks: 1.", "labels": [], "entities": [{"text": "word alignment shared tasks", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.8146913647651672}]}, {"text": "HLT Among these, we choose two tasks, English-French and Chinese-English, to report detailed experimental results.", "labels": [], "entities": [{"text": "HLT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7180710434913635}]}, {"text": "Results for the other tasks can also be found in.", "labels": [], "entities": []}, {"text": "Corpus statistics for the English-French and Chinese-English tasks are shown in.", "labels": [], "entities": []}, {"text": "The English-French data from the HLT/NAACL 2003 shared task consist of a training corpus of 1,130,104 sentence pairs, a development corpus of 37 sentence pairs, and a test corpus of 447 sentence pairs.", "labels": [], "entities": [{"text": "HLT/NAACL 2003 shared task", "start_pos": 33, "end_pos": 59, "type": "DATASET", "confidence": 0.7864808638890585}]}, {"text": "The development and test sets are manually aligned and marked with both sure and possible labels.", "labels": [], "entities": []}, {"text": "Although the Canadian Hansard bilingual corpus is widely used in the community, direct comparisons are difficult due to the differences in splitting of training data, development data, and test data.", "labels": [], "entities": [{"text": "Canadian Hansard bilingual corpus", "start_pos": 13, "end_pos": 46, "type": "DATASET", "confidence": 0.8311717361211777}]}, {"text": "To make our results more comparable to previous work, we followed Lacoste-  Julien et al. splitting the original test set into two parts: the first 200 sentences as the development set and the remaining 247 sentences as the test set.", "labels": [], "entities": []}, {"text": "To compare with systems participating in the 2003 NAACL shared task, we also used the small development set of 37 sentences to optimize feature weights, and ran our system on the original test set of 447 sentences.", "labels": [], "entities": [{"text": "NAACL shared task", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.7176825205485026}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The Chinese-English data from the HTRDP 2005 shared task contains a development corpus of 502 sentence pairs and a test corpus of 505 sentence pairs.", "labels": [], "entities": [{"text": "HTRDP 2005 shared task", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.8003890216350555}]}, {"text": "We use a training corpus of 837, 594 sentence pairs available from Chinese Linguistic Data Consortium and a bilingual dictionary containing 415, 753 entries.", "labels": [], "entities": [{"text": "Chinese Linguistic Data Consortium", "start_pos": 67, "end_pos": 101, "type": "DATASET", "confidence": 0.911157950758934}]}, {"text": "In this section, we report on experiments with Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6699273884296417}]}, {"text": "To investigate the effect of our discriminative model on translation performance, we used three translation systems:", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9628748297691345}]}], "tableCaptions": [{"text": " Table 1  Example feature values and alignment error rates.", "labels": [], "entities": [{"text": "alignment error rates", "start_pos": 37, "end_pos": 58, "type": "METRIC", "confidence": 0.8569820523262024}]}, {"text": " Table 4  Corpus characteristics of the English-French task.", "labels": [], "entities": []}, {"text": " Table 5  Corpus characteristics of the Chinese-English task.", "labels": [], "entities": []}, {"text": " Table 6  Comparison of AER scores for various IBM models in GIZA++ and Vigne. These models are  trained only on development and test sets. The pruning setting for Vigne is \u03b2 = 0 and b = 1. All  differences are not statistically significant.", "labels": [], "entities": [{"text": "AER", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9980936646461487}]}, {"text": " Table 7  Comparison of GIZA++, Cross-EM, and Vigne on both tasks. Note that Vigne yields only  one-to-one alignments if both \"Model 4 s2t\" and \"Model 4 t2s\" features are used. The pruning  setting for Vigne is \u03b2 = 0 and b = 1. While the final results of our system are better than the best  baseline generative models significantly at p < 0.01, adding a single feature will not always  produce a significant improvement, especially for English-French.", "labels": [], "entities": []}, {"text": " Table 9  AER scores achieved by the symmetric alignment model on both tasks. The pruning setting for  Vigne is \u03b2 = 0 and b = 1. Although the final model obviously outperforms the initial model  significantly at p < 0.01, adding a single feature will not always result in a significant  improvement, especially for English-French.", "labels": [], "entities": [{"text": "AER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9994762539863586}, {"text": "Vigne", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.8232718110084534}]}, {"text": " Table 10  Comparison of aligning speed (words per second) and AER score with varying beam widths for  the Chinese-English task. We fix \u03b2 = 0.01. Bold numbers refer to the results that are better than  the baseline but not significantly so. We use \"+\" to denote the results that outperform the best  baseline (b = 1) and are statistically significant at p < 0.05. Similarly, we use \"++\" to denote  significantly better than baseline at p < 0.01.", "labels": [], "entities": [{"text": "AER score", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.976604163646698}]}, {"text": " Table 12  Comparison of some word alignment systems on the Canadian Hansard data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.746210902929306}, {"text": "Canadian Hansard data", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.8717806537946066}]}, {"text": " Table 13  Maximization of F-measure with different settings of \u03b1 (the weighting factor in the balanced  F-measure). We use IBM Model 4 and HMM as baseline systems. Our system restricts the search  space by exploring only the union of baseline predictions. We compute the \"oracle\" alignments  by intersecting the union with reference alignments. We use \"+\" to denote the result that  outperforms the best baseline result with statistical significance at p < 0.05. Similarly, we use  \"++\" to denote significantly better than baseline at p < 0.01.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.9370723366737366}]}, {"text": " Table 14  BLEU scores on the devtest set. We use \"+\" to denote the result that outperforms the best  baseline result (highlighted in bold) statistically significantly at p < 0.05. Similarly, we use \"++\"  to denote significantly better than baseline at p < 0.01.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9991119503974915}]}]}