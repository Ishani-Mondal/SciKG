{"title": [{"text": "Learning Tractable Word Alignment Models with Complex Constraints", "labels": [], "entities": [{"text": "Learning Tractable Word Alignment", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5193842947483063}]}], "abstractContent": [{"text": "Word-level alignment of bilingual text is a critical resource fora growing variety of tasks.", "labels": [], "entities": [{"text": "Word-level alignment of bilingual text", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7739450752735137}]}, {"text": "Proba-bilistic models for word alignment present a fundamental trade-off between richness of captured constraints and correlations versus efficiency and tractability of inference.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7822544872760773}]}, {"text": "In this article, we use the Posterior Regularization framework (Gra\u00e7a, Ganchev, and Taskar 2007) to incorporate complex constraints into probabilistic models during learning without changing the efficiency of the underlying model.", "labels": [], "entities": []}, {"text": "We focus on the simple and tractable hidden Markov model, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints.", "labels": [], "entities": []}, {"text": "Models estimated with these constraints produce a significant boost in performance as measured by both precision and recall of manually annotated alignments for six language pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9993501305580139}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9982105493545532}]}, {"text": "We also report experiments on two different tasks where word alignments are required: phrase-based machine translation and syntax transfer, and show promising improvements over standard methods.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7432048916816711}, {"text": "phrase-based machine translation", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.6351470152537028}, {"text": "syntax transfer", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.7123497724533081}]}], "introductionContent": [{"text": "The seminal work of introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.7140062550703684}, {"text": "word-byword\" alignment", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.6580139001210531}]}, {"text": "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and, are still widely used for word alignment.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9443102180957794}, {"text": "word alignment", "start_pos": 168, "end_pos": 182, "type": "TASK", "confidence": 0.8601235449314117}]}, {"text": "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases and rules [) as well as for MT system combination).", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6737111210823059}, {"text": "extracting minimal translation", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.7888100544611613}, {"text": "machine translation (MT)", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.8314442992210388}, {"text": "MT system combination", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.9258684118588766}]}, {"text": "But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (; discovery of paraphrases (; and joint unsupervised POS and parser induction across languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.756222665309906}]}, {"text": "IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation.", "labels": [], "entities": []}, {"text": "IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.", "labels": [], "entities": []}, {"text": "Many researchers use the GIZA++ software package as a black box, selecting IBM Model 4 as a compromise between alignment quality and efficiency.", "labels": [], "entities": [{"text": "GIZA++ software package", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.8535536825656891}]}, {"text": "All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).", "labels": [], "entities": []}, {"text": "Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).", "labels": [], "entities": []}, {"text": "This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results.", "labels": [], "entities": []}, {"text": "In this article we focus on the HMM word alignment model, using a novel unsupervised learning framework that significantly boosts its performance.", "labels": [], "entities": [{"text": "HMM word alignment", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8425345222155253}]}, {"text": "The new training framework, called Posterior Regularization, incorporates prior knowledge in the form of constraints on the model's posteriors.", "labels": [], "entities": [{"text": "Posterior Regularization", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.5928372591733932}]}, {"text": "The constraints are expressed as inequalities on the expected value under the posterior distribution of user-defined features.", "labels": [], "entities": []}, {"text": "Although the base model remains unchanged, learning guides the model to satisfy these constraints.", "labels": [], "entities": []}, {"text": "We propose two such constraints: (i) bijectivity: one word should not translate to many words; and (ii) symmetry: directional alignments should agree.", "labels": [], "entities": []}, {"text": "Both of these constraints significantly improve the performance of the model both in precision and recall, with the symmetry constraint generally producing more accurate alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995892643928528}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9987444877624512}]}, {"text": "Section 3 presents the Posterior Regularization (PR) framework and describes how to encode such constraints in an efficient manner, requiring only repeated inference in the original model to enforce the constraints.", "labels": [], "entities": [{"text": "Posterior Regularization (PR)", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7379488468170166}]}, {"text": "Section 4 presents a detailed evaluation of the alignments produced.", "labels": [], "entities": []}, {"text": "The constraints over posteriors consistently and significantly outperform the unconstrained HMM model, evaluated against manual annotations.", "labels": [], "entities": []}, {"text": "Moreover, this training procedure outperforms the more complex IBM Model 4 nine times out of 12.", "labels": [], "entities": []}, {"text": "We examine the influence of constraints on the resulting posterior distributions and find that they are especially effective for increasing alignment accuracy for rare words.", "labels": [], "entities": [{"text": "alignment", "start_pos": 140, "end_pos": 149, "type": "TASK", "confidence": 0.935880720615387}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.8652685284614563}]}, {"text": "We also demonstrate anew methodology to avoid overfitting using a small development corpus.", "labels": [], "entities": []}, {"text": "Section 5 evaluates the new framework on two different tasks that depend on word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6941406726837158}]}, {"text": "Section 5.1 focuses on MT and shows that the better alignments also lead to better translation systems, adding to similar evidence presented in.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9900792837142944}]}, {"text": "Section 5.2 shows that the alignments we produce are better suited for transfer of syntactic dependency parse annotations.", "labels": [], "entities": [{"text": "transfer of syntactic dependency parse annotations", "start_pos": 71, "end_pos": 121, "type": "TASK", "confidence": 0.7252048750718435}]}, {"text": "An implementation of this work) is available under a GPL license.", "labels": [], "entities": []}], "datasetContent": [{"text": "We begin with a comparison of word alignment quality evaluated against manually annotated alignments as measured by precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9991167187690735}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9901154041290283}]}, {"text": "We use the six parallel corpora with gold annotations described in the beginning of Section 2.", "labels": [], "entities": []}, {"text": "We discarded all training data sentence pairs where one of the sentences contained more than 40 words.", "labels": [], "entities": []}, {"text": "Following common practice, we added the unlabeled development and test data sets to the pool of unlabeled sentences.", "labels": [], "entities": []}, {"text": "We initialized the IBM Model 1 translation table with uniform probabilities over word pairs that occur together in the same sentence and trained the IBM Model 1 for 5 iterations.", "labels": [], "entities": [{"text": "IBM Model 1 translation", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7009938359260559}, {"text": "IBM Model 1", "start_pos": 149, "end_pos": 160, "type": "DATASET", "confidence": 0.944374700387319}]}, {"text": "All HMM alignment models were initialized with the translation table from IBM Model 1 and uniform distortion probabilities.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8890341818332672}]}, {"text": "We run each training procedure until the area under the precision/recall curve measured on a development corpus stops increasing (see for an example of such a curve).", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9991542100906372}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.80951327085495}]}, {"text": "Using the precision/recall curve gives a broader sense of the model's performance than using a single point (by tuning a threshold fora particular metric).", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990355968475342}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9278577566146851}]}, {"text": "In most cases this meant four iterations for normal EM training and two iterations using posterior regularization.", "labels": [], "entities": [{"text": "EM training", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9152651429176331}]}, {"text": "We suspect that the constraints make the space easier to search.", "labels": [], "entities": []}, {"text": "The convergence criterion for the projection algorithm was the normalized l 2 norm of the gradient (gradient norm divided by number of constraints) being smaller than \u03b7 (see Algorithm 1).", "labels": [], "entities": []}, {"text": "For bijective constraints, we set \u03b7 to 0.005 and used zero slack.", "labels": [], "entities": []}, {"text": "For symmetric constraints, \u03b7 and slack were set to 0.001.", "labels": [], "entities": []}, {"text": "We chose \u03b7 aggressively and lower values did not significantly increase performance.", "labels": [], "entities": []}, {"text": "Less aggressive settings cause degradation of performance: For example, for En-Fr using 10k sentences, and running four iterations of constrained EM, the area under the precision/recall curve for the symmetric model changed from 70% with \u03b7 = 0.1 to 85% using \u03b7 = 0.001.", "labels": [], "entities": [{"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9987258315086365}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.841579020023346}]}, {"text": "On the other hand, the number of iterations required to project the constraints increases for smaller values of \u03b7.", "labels": [], "entities": []}, {"text": "The number of forward-backward calls for normal HMM is 40k (one for each sentence and EM iteration), for the symmetric model using \u03b7 = 0.1 was around 41k and using \u03b7 = 0.001 was around 26M (14 minutes to 4 hours 14 minutes of training time, 17 times slower, for the different settings of \u03b7).", "labels": [], "entities": []}, {"text": "We note that better optimization methods, such as L-BFGS, or using a warm start for the parameters at each EM iteration (parameters from the previous iteration), or training the models online, would potentially decrease the running time of our method.", "labels": [], "entities": []}, {"text": "The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints.", "labels": [], "entities": []}, {"text": "We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.6983493864536285}]}, {"text": "However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8822744290033976}]}, {"text": "Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.", "labels": [], "entities": []}, {"text": "We used a standard implementation of IBM Model 4 (Och and Ney 2003) and because changing the existing code is not trivial, we could not use the same stopping criterion to avoid overfitting and we are notable to produce precision/recall curves.", "labels": [], "entities": [{"text": "precision", "start_pos": 219, "end_pos": 228, "type": "METRIC", "confidence": 0.9990173578262329}, {"text": "recall", "start_pos": 229, "end_pos": 235, "type": "METRIC", "confidence": 0.9237040281295776}]}, {"text": "We trained IBM Model 4 using the default configuration of the  In this section we evaluate the alignments resulting from using the proposed constraints in two different tasks: Statistical machine translation where alignments are used to restrict the number of possible minimal translation units; and syntax transfer, where alignments are used to decide how to transfer dependency links.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 176, "end_pos": 207, "type": "TASK", "confidence": 0.7666487892468771}, {"text": "syntax transfer", "start_pos": 300, "end_pos": 315, "type": "TASK", "confidence": 0.7828985452651978}]}], "tableCaptions": [{"text": " Table 1  Test corpora statistics: English-French, English-Spanish, English-Portuguese,  Portuguese-Spanish, Portuguese-French, and Spanish-French.", "labels": [], "entities": []}, {"text": " Table 2  BLEU scores for all language pairs. The best threshold was selected according to the  development set after the last MERT iteration. Bold denotes the best score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989469647407532}]}]}