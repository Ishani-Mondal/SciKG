{"title": [{"text": "Distributional Memory: A General Framework for Corpus-Based Semantics", "labels": [], "entities": [{"text": "Distributional Memory", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.942701131105423}]}], "abstractContent": [{"text": "Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus.", "labels": [], "entities": []}, {"text": "As an alternative to this \"one task, one model\" approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.", "labels": [], "entities": []}, {"text": "Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.", "labels": [], "entities": []}, {"text": "In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.", "labels": [], "entities": [{"text": "word similarity judgments", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.6732513010501862}]}, {"text": "Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.", "labels": [], "entities": [{"text": "Distributional Memory", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.8701306283473969}]}, {"text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multipurpose nature.", "labels": [], "entities": [{"text": "Distributional Memory", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8406213819980621}]}], "introductionContent": [{"text": "The last two decades have seen a rising wave of interest among computational linguists and cognitive scientists in corpus-based models of semantic representation.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 138, "end_pos": 161, "type": "TASK", "confidence": 0.7368321716785431}]}, {"text": "These models, variously known as vector spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term we will adopt, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis, stating that the degree of semantic similarity between two words (or other linguistic units) can be modeled as a function of the degree of overlap among their linguistic contexts.", "labels": [], "entities": []}, {"text": "Conversely, the format of distributional representations greatly varies depending on the specific aspects of meaning they are designed to model.", "labels": [], "entities": []}, {"text": "The most straightforward phenomenon tackled by DSMs is what Turney (2006b) calls attributional similarity, which encompasses standard taxonomic semantic relations such as synonymy, co-hyponymy, and hypernymy.", "labels": [], "entities": []}, {"text": "Words like dog and puppy, for example, are attributionally similar in the sense that their meanings share a large number of attributes: They are animals, they bark, and soon.", "labels": [], "entities": []}, {"text": "Attributional similarity is typically addressed by DSMs based on word collocates.", "labels": [], "entities": [{"text": "Attributional similarity", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9319775998592377}]}, {"text": "These collocates are seen as proxies for various attributes of the concepts that the words denote.", "labels": [], "entities": []}, {"text": "Words that share many collocates denote concepts that share many attributes.", "labels": [], "entities": []}, {"text": "Both dog and puppy may occur near owner, leash, and bark, because these words denote properties that are shared by dogs and puppies.", "labels": [], "entities": []}, {"text": "The attributional similarity between dog and puppy, as approximated by their contextual similarity, will be very high.", "labels": [], "entities": [{"text": "attributional similarity", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.7608672976493835}]}, {"text": "DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) or concept categorization) because such tasks require a measure of attributional similarity that favors concepts that share many properties, such as synonyms and co-hyponyms.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.9424744546413422}]}, {"text": "However, many other tasks require detecting different kinds of semantic similarity.", "labels": [], "entities": []}, {"text": "Turney (2006b) defines relational similarity as the property shared by pairs of words (e.g, dog-animal and car-vehicle) linked by similar semantic relations (e.g., hypernymy), despite the fact that the words in one pair might not be attributionally similar to those in the other pair (e.g., dog is not attributionally similar to car, nor is animal to vehicle).", "labels": [], "entities": []}, {"text": "Turney generalizes DSMs to tackle relational similarity and represents pairs of words in the space of the patterns that connect them in the corpus.", "labels": [], "entities": []}, {"text": "Pairs of words that are connected by similar patterns probably hold similar relations, that is, they are relationally similar.", "labels": [], "entities": []}, {"text": "For example, we can hypothesize that dog-tail is more similar to car-wheel than to dog-animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car-wheel than like those of dog-animal (is a, such as, etc.).", "labels": [], "entities": []}, {"text": "Turney uses the relational space to implement tasks such as solving analogies and harvesting instances of relations.", "labels": [], "entities": []}, {"text": "Although they are not explicitly expressed in these terms, relation extraction algorithms) also rely on relational similarity, and focus on learning one relation type at a time (e.g., finding parts).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.874584287405014}]}, {"text": "Although semantic similarity, either attributional or relational, has the lion's share in DSMs, similarity is not the only aspect of meaning that is addressed by distributional approaches.", "labels": [], "entities": []}, {"text": "For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties.", "labels": [], "entities": []}, {"text": "In this case, the task is not to find out that dog is similar to puppy or cat, but that it has a tail, it is used for hunting, and so on., , and use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects.", "labels": [], "entities": []}, {"text": "instead focus on that subset of noun properties known in lexical semantics as qualia roles, and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 240, "end_pos": 259, "type": "TASK", "confidence": 0.7751031816005707}]}, {"text": "The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences, argument alternations (, event types (, and so forth.", "labels": [], "entities": []}, {"text": "Finally, some DSMs capture a sort of \"topical\" relatedness between words: They might find, for example, a relation between dog and fidelity.", "labels": [], "entities": []}, {"text": "Topical relatedness, addressed by DSMs based on document distributions such as LSA and Topic Models, is not further discussed in this article.", "labels": [], "entities": [{"text": "Topical relatedness", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8509069085121155}]}, {"text": "DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction).", "labels": [], "entities": [{"text": "automatic thesaurus construction", "start_pos": 80, "end_pos": 112, "type": "TASK", "confidence": 0.6481900115807852}]}, {"text": "Corpus-based semantic models have also attracted the attention of lexical semanticists as away to provide the notion of synonymy with a more robust empirical foundation).", "labels": [], "entities": []}, {"text": "Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies.", "labels": [], "entities": []}, {"text": "Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semantic change, lexical variation (, and for the analysis of multiword expressions.", "labels": [], "entities": []}, {"text": "The wealth and variety of semantic issues that DSMs are able to tackle confirms the importance of looking at distributional data to explore meaning, as well as the maturity of this research field.", "labels": [], "entities": []}, {"text": "However, if we looked from a distance at the whole field of DSMs we would see that, besides the general assumption shared by all models that information about the context of a word is an important key in grasping its meaning, the elements of difference overcome the commonalities.", "labels": [], "entities": []}, {"text": "For instance, DSMs geared towards attributional similarity represent words in the contexts of other (content) words, thereby looking very different from models that represent word pairs in terms of patterns linking them.", "labels": [], "entities": []}, {"text": "In turn, both these models differ from those used to explore concept properties or argument alternations.", "labels": [], "entities": []}, {"text": "The typical approach in the field has been a local one, in which each semantic task (or set of closely related tasks) is treated as a separate problem, that requires its own corpus-derived model and algorithm, both optimized to achieve the best performance in a given task, but lacking generality, since they resort to task-specific distributional representations, often complemented by additional taskspecific resources.", "labels": [], "entities": []}, {"text": "As a consequence, the landscape of DSMs looks more like a jigsaw puzzle in which different parts have been completed and the whole figure starts to emerge from the fragments, but it is not clear yet how to put everything together and compose a coherent picture.", "labels": [], "entities": []}, {"text": "We argue that the \"one semantic task, one distributional model\" approach represents a great limit of the current state of the art.", "labels": [], "entities": []}, {"text": "From a theoretical perspective, corpusbased models hold promise as large-scale simulations of how humans acquire and use conceptual and linguistic information from their environment.", "labels": [], "entities": []}, {"text": "However, existing DSMs lack exactly the multi-purpose nature that is a hallmark of human semantic competence.", "labels": [], "entities": []}, {"text": "The common view in cognitive (neuro)science is that humans resort to a single semantic memory, a relatively stable long-term knowledge database, adapting the information stored thereto the various tasks at hand).", "labels": [], "entities": []}, {"text": "The fact that DSMs need to go back to their environment (the corpus) to collect ad hoc statistics for each semantic task, and the fact that different aspects of meaning require highly different distributional representations, cast many shadows on the plausibility of DSMs as general models of semantic memory.", "labels": [], "entities": []}, {"text": "From a practical perspective, going back to the corpus to train a different model for each application is inefficient, and it runs the risk of overfitting the model to a specific task, while losing sight of its adaptivity-a highly desirable feature for any intelligent system.", "labels": [], "entities": []}, {"text": "Think, by contrast, of WordNet), a single, general purpose network of semantic information that has been adapted to all sorts of tasks, many of them certainly not envisaged by the resource creators.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.938771665096283}]}, {"text": "We think that it is not by chance that no comparable resource has emerged from DSM development.", "labels": [], "entities": []}, {"text": "In this article, we want to show that a unified approach is not only a desirable goal, but it is also a feasible one.", "labels": [], "entities": []}, {"text": "With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics.", "labels": [], "entities": []}, {"text": "Differently from other current proposals that share similar aims, we believe that the lack of generalization in corpus-based semantics stems from the choice of representing co-occurrence statistics directly as matrices-geometrical objects that model distributional data in terms of binary relations between target items (the matrix rows) and their contexts (the matrix columns).", "labels": [], "entities": []}, {"text": "This results in the development of ad hoc models that lose sight of the fact that different semantic spaces actually rely on the same kind of underlying distributional information.", "labels": [], "entities": []}, {"text": "DM instead represents corpus-extracted co-occurrences as a third-order tensor, a ternary geometrical object that models distributional data in terms of wordlink-word tuples.", "labels": [], "entities": [{"text": "DM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.814437747001648}]}, {"text": "Matrices are then generated from the tensor in order to perform semantic tasks in the spaces they define.", "labels": [], "entities": []}, {"text": "Crucially, these on-demand matrices are derived from the same underlying resource (the tensor) and correspond to different \"views\" of the same data, extracted once and for all from a corpus.", "labels": [], "entities": []}, {"text": "DM is tested hereon what we believe to be the most varied array of semantic tasks ever addressed by a single distributional model.", "labels": [], "entities": []}, {"text": "In all cases, we compare the performance of several DM implementations to state-of-the-art results.", "labels": [], "entities": []}, {"text": "While some of the ad hoc models that were developed to tackle specific tasks do outperform our most successful DM implementation, the latter is never too far from the top, without any task-specific tuning.", "labels": [], "entities": []}, {"text": "We think that the advantage of having a general model that does not need to be retrained for each new task outweighs the (often minor) performance advantage of the task-specific models.", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "After framing our proposal within the general debate on co-occurrence modeling in distributional semantics (Section 2), we introduce the DM framework in Section 3 and compare it to other unified approaches in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 pertains to the specific implementations of the DM framework we will test experimentally.", "labels": [], "entities": []}, {"text": "The experiments are reported in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8171754777431488}]}, {"text": "Section 7 concludes by summarizing what we have achieved, and discussing the implications of these results for corpus-based distributional semantics.", "labels": [], "entities": []}], "datasetContent": [{"text": "As we saw in Section 3, labeled matricization generates four distinct semantic spaces from the third-order tensor.", "labels": [], "entities": []}, {"text": "For each space, we have selected a set of semantic experiments that we model by applying some combination of the vector manipulation operations of Section 5.2.", "labels": [], "entities": []}, {"text": "The experiments correspond to key semantic tasks in computational linguistics and/or cognitive science, typically addressed by distinct DSMs so far.", "labels": [], "entities": []}, {"text": "We have also aimed at maximizing the variety of aspects of meaning covered by the experiments, ranging from synonymy detection to argument structure and concept properties, and encompassing all the major lexical classes.", "labels": [], "entities": [{"text": "synonymy detection", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.709452822804451}]}, {"text": "Both these facts support the view of DM as a generalized model that is able to overtake state-of-the-art DSMs in the number and types of semantic issues addressed, while being competitive in each specific task.", "labels": [], "entities": []}, {"text": "The choice of the DM semantic space to tackle a particular task is essentially based on the \"naturalness\" with which the task can be modeled in that space.", "labels": [], "entities": []}, {"text": "However, alternatives are conceivable, both with respect to space selection, and to the operations performed on the space.", "labels": [], "entities": []}, {"text": "For instance, Turney (2008) models synonymy detection with a DSM that closely resembles our W 1 W 2 \u00d7L space, whereas we tackle this task under the more standard W 1 \u00d7LW 2 view.", "labels": [], "entities": [{"text": "synonymy detection", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9036283195018768}]}, {"text": "It is an open question whether there are principled ways to select the optimal space configuration fora given semantic task.", "labels": [], "entities": []}, {"text": "In this article, we limit ourselves to proving that each space derived through tensor matricization is semantically interesting in the sense that it provides the proper ground to address some semantic task.", "labels": [], "entities": []}, {"text": "Feature selection/reweighting and dimensionality reduction have been shown to improve DSM performance.", "labels": [], "entities": [{"text": "DSM", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9169859886169434}]}, {"text": "For instance, the feature bootstrapping method proposed by boosts the precision of a DSM in lexical entailment recognition.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9991463422775269}, {"text": "lexical entailment recognition", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.6777451237042745}]}, {"text": "Even if these methods can be applied to DM as well, we did not use them in our experiments.", "labels": [], "entities": [{"text": "DM", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9517859220504761}]}, {"text": "The results presented subsequently should be regarded as a \"baseline\" performance that could be enhanced in future work by exploring various task-specific parameters (we will comeback in the conclusion to the role of parameter tuning in DM).", "labels": [], "entities": [{"text": "DM", "start_pos": 237, "end_pos": 239, "type": "TASK", "confidence": 0.9268457293510437}]}, {"text": "This is consistent with our current aim of focusing on the generality and adaptivity of DM, rather than on task-specific optimization.", "labels": [], "entities": [{"text": "DM", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9435832500457764}]}, {"text": "As a first, important step in this latter direction, however, we conclude the empirical evaluation in Section 6.5 by replicating one experiment using tensor-decomposition-based smoothing, a form of optimization that can only be performed within the tensor-based approach to DSMs.", "labels": [], "entities": []}, {"text": "In order to maximize coverage of the experimental test sets, they are pre-processed with a mixture of manual and heuristic procedures to assign a POS to the words they contain, lemmatize, convert some multiword forms to single words, and turn some adverbs into adjectives (our models do not contain multiwords or adverbs).", "labels": [], "entities": [{"text": "POS", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9688320159912109}]}, {"text": "Nevertheless, some words (or word pairs) are unrecoverable, and in such cases we make a random guess (in cases where we do not have full coverage of a data set, the reported results are averages across repeated experiments, to account for the variability in random guesses).", "labels": [], "entities": []}, {"text": "In many of the experiments herein, DM is not only compared to the results available in the literature, but also to our implementation of state-of-the-art DSMs.", "labels": [], "entities": [{"text": "DM", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9299384355545044}]}, {"text": "These alternative models have been trained on the same corpus (with the same linguistic preprocessing) used to build the DM tuple tensors.", "labels": [], "entities": []}, {"text": "This way, we aim at achieving a fairer comparison with alternative approaches in distributional semantics, abstracting away from the effects induced by differences in the training data.", "labels": [], "entities": []}, {"text": "Most experiments report global (micro-averaged) test set accuracy (alone, or combined with other measures) to assess the performance of the algorithms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8983737230300903}]}, {"text": "The number of correctly classified items among all test elements can be seen as a binomially distributed random variable, and we follow the ACL Wiki state-of-the-art site 7 in reporting also Clopper-Pearson binomial 95% confidence intervals around the accuracies (binomial intervals and other statistical quantities were computed using the R package; 8 where no further references are given, we used the standard R functions for the relevant analysis).", "labels": [], "entities": [{"text": "ACL Wiki state-of-the-art site 7", "start_pos": 140, "end_pos": 172, "type": "DATASET", "confidence": 0.92281174659729}, {"text": "Clopper-Pearson binomial 95% confidence intervals", "start_pos": 191, "end_pos": 240, "type": "METRIC", "confidence": 0.692826971411705}]}, {"text": "The binomial confidence intervals give a sense of the spread of plausible population values around the test-set-based point estimates of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9980111122131348}]}, {"text": "Where appropriate and interesting, we compare the accuracy of two specific models statistically with an exact Fisher test on the contingency table of correct and wrong responses given by the two models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9991040825843811}]}, {"text": "This approach to significance testing is problematic in many respects, the most important being that we ignore dependencies incorrect and wrong counts due to the fact that the algorithms are evaluated on the same test set).", "labels": [], "entities": [{"text": "significance testing", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9444268047809601}]}, {"text": "More appropriate tests, however, would require access to the fully itemized results from the compared algorithms, whereas inmost cases we only know the point estimate reported in the earlier literature.", "labels": [], "entities": []}, {"text": "For similar reasons, we do not make significance claims regarding other performance measures, such as macro-averaged F.", "labels": [], "entities": [{"text": "F", "start_pos": 117, "end_pos": 118, "type": "METRIC", "confidence": 0.9332769513130188}]}, {"text": "Other forms of statistical analysis of the results are introduced herein when they are used; they are mostly limited to the models for which we have full access to the results.", "labels": [], "entities": []}, {"text": "Note that we are interested in whether DM performance is overall within state-of-the-art range, and not on making precise claims about the models it outperforms.", "labels": [], "entities": [{"text": "DM performance", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.9044424891471863}]}, {"text": "In this respect, we think that our general results are clear even where they are not supported by statistical inference, or interpretation of the latter is problematic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  A toy weighted tuple structure.", "labels": [], "entities": []}, {"text": " Table 2  A labeled third-order tensor of dimensionality 3 \u00d7 2 \u00d7 3 representing the weighted tuple  structure of Table 1.", "labels": [], "entities": []}, {"text": " Table 3  Labeled mode-1, mode-2, and mode-3 matricizations of the tensor in Table 2.", "labels": [], "entities": []}, {"text": " Table 4  Percentage Pearson correlation with the Rubenstein and Goodenough (1965) similarity ratings.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 21, "end_pos": 40, "type": "METRIC", "confidence": 0.8922483325004578}]}, {"text": " Table 5  Percentage accuracy in TOEFL synonym detection with 95% binomial confidence intervals (CI).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9914473295211792}, {"text": "TOEFL synonym detection", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8531644145647684}, {"text": "binomial confidence intervals (CI)", "start_pos": 66, "end_pos": 100, "type": "METRIC", "confidence": 0.8827981849511465}]}, {"text": " Table 6  Purity in noun clustering with bootstrapped 95% confidence intervals (CI).", "labels": [], "entities": [{"text": "noun clustering", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8708214461803436}, {"text": "bootstrapped 95% confidence intervals (CI)", "start_pos": 41, "end_pos": 83, "type": "METRIC", "confidence": 0.9052699431777}]}, {"text": " Table 7  Correlation with verb-argument plausibility judgments.", "labels": [], "entities": []}, {"text": " Table 8  Percentage accuracy in solving SAT analogies with 95% binomial confidence intervals (CI).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9874535799026489}, {"text": "binomial confidence intervals (CI)", "start_pos": 64, "end_pos": 98, "type": "METRIC", "confidence": 0.8142925401528677}]}, {"text": " Table 9  Relation classification performance; all measures macro-averaged, except accuracy in the NS and  OC data sets, where we also report the accuracy 95% confidence intervals (CI).", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8964509069919586}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9994542002677917}, {"text": "NS and  OC data sets", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.7916136384010315}, {"text": "accuracy 95% confidence intervals (CI)", "start_pos": 146, "end_pos": 184, "type": "METRIC", "confidence": 0.9498132169246674}]}, {"text": " Table 11  Average qualia extraction performance.", "labels": [], "entities": [{"text": "qualia extraction", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.784723699092865}]}, {"text": " Table 12  Average percentage overlap with subject-generated properties and standard deviation.", "labels": [], "entities": []}, {"text": " Table 13  Verb classification performance (precision, recall, and F for MS are macro-averaged). Global  accuracy supplemented by 95% binomial confidence intervals (CI).", "labels": [], "entities": [{"text": "Verb classification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6756982207298279}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9994267225265503}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9981496334075928}, {"text": "F", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.9985418319702148}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8878607749938965}, {"text": "binomial confidence intervals (CI)", "start_pos": 134, "end_pos": 168, "type": "METRIC", "confidence": 0.8455221007267634}]}, {"text": " Table 15  Average qualia extraction performance with automatically harvested links (compare to Table 11).", "labels": [], "entities": [{"text": "qualia extraction", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7114068865776062}]}, {"text": " Table 16  Purity in Almuhareb-Poesio concept clustering with rank reduction of the APTypeDM tensor;  95% confidence intervals (CI) obtained by bootstrapping.", "labels": [], "entities": [{"text": "95% confidence intervals (CI)", "start_pos": 102, "end_pos": 131, "type": "METRIC", "confidence": 0.7574655328478131}]}]}