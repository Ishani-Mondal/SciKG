{"title": [{"text": "A Graph-Theoretic Framework for Semantic Distance", "labels": [], "entities": [{"text": "Semantic Distance", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7518707811832428}]}], "abstractContent": [{"text": "Many NLP applications entail that texts are classified based on their semantic distance (how similar or different the texts are).", "labels": [], "entities": []}, {"text": "For example, comparing the text of anew document to that of documents of known topics can help identify the topic of the new text.", "labels": [], "entities": []}, {"text": "Typically, a distributional distance is used to capture the implicit semantic distance between two pieces of text.", "labels": [], "entities": []}, {"text": "However, such approaches do not take into account the semantic relations between words.", "labels": [], "entities": []}, {"text": "In this article, we introduce an alternative method of measuring the semantic distance between texts that integrates distributional information and ontological knowledge within a network flow formalism.", "labels": [], "entities": []}, {"text": "We first represent each text as a collection of frequency-weighted concepts within an ontology.", "labels": [], "entities": []}, {"text": "We then make use of a network flow method which provides an efficient way of explicitly measuring the frequency-weighted ontological distance between the concepts across two texts.", "labels": [], "entities": []}, {"text": "We evaluate our method in a variety of NLP tasks, and find that it performs well on two of three tasks.", "labels": [], "entities": []}, {"text": "We develop anew measure of semantic coherence that enables us to account for the performance difference across the three data sets, shedding light on the properties of a data set that lends itself well to our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language tasks can be cast as a problem of comparing texts in terms of their semantic distance.", "labels": [], "entities": []}, {"text": "For example, given a suitable text distance measure, document classification can be performed by comparing the text of anew document to the text of various documents whose topics are known.", "labels": [], "entities": [{"text": "document classification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7685901522636414}]}, {"text": "The new document is then labelled with the topic of the document whose text is most similar to it.", "labels": [], "entities": []}, {"text": "In general, the texts to be compared maybe full documents, as in this example, or maybe portions of documents, or even collections of documents.", "labels": [], "entities": []}, {"text": "Using text comparison to perform semantic classification has been adopted in a variety of natural language processing (NLP) tasks, from document classification, to prepositional phrase attachment (, to spelling correction (.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7405603528022766}, {"text": "document classification", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.783933162689209}, {"text": "prepositional phrase attachment", "start_pos": 164, "end_pos": 195, "type": "TASK", "confidence": 0.5972977081934611}, {"text": "spelling correction", "start_pos": 202, "end_pos": 221, "type": "TASK", "confidence": 0.835315465927124}]}, {"text": "Distributional methods for semantic distance are widely used and highly successful in comparing texts that are represented as bags of words with associated frequencies of occurrence.", "labels": [], "entities": [{"text": "semantic distance", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8847314119338989}]}, {"text": "In document classification, for example, the text of a document maybe represented as a word frequency vector, which is compared using a distributional distance measure to each of the word frequency vectors of the texts of the documents of known topics.", "labels": [], "entities": [{"text": "document classification", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7137348502874374}]}, {"text": "In this way, distributional distance between word vectors captures the semantic distance between two texts that is implicitly encoded in the set of words used in each.", "labels": [], "entities": []}, {"text": "Semantic distance can also be measured more explicitly, by using the relations in an ontology as the direct encoding of semantic association.", "labels": [], "entities": []}, {"text": "However, such approaches have generally been limited to calculating the distance between two individual concepts, rather than capturing the distance between two sets of concepts corresponding to two texts.", "labels": [], "entities": []}, {"text": "Numerous measures have been proposed, for example, for capturing the distance between two concepts in WordNet, typically relying on the synonymy (synset) and hyponymy (is-a) relations (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.9321772456169128}]}, {"text": "Using such an ontological measure to compare two texts (collections of words instead of single words) might involve mapping each word of a text to its appropriate concept(s) in the ontology, and then calculating the aggregate distance between the two resulting sets of concepts across the ontological relations.", "labels": [], "entities": []}, {"text": "For example, one might calculate the semantic distance between the two texts as the average, minimum, maximum, or summed ontological distance between the individual elements of the two sets of concepts.", "labels": [], "entities": []}, {"text": "Observe that each of these approaches to text comparison-distributional and ontological-encodes information not contained in the other.", "labels": [], "entities": [{"text": "text comparison-distributional", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.6912214010953903}]}, {"text": "Distributional distance captures important information about frequency of occurrence of the words that constitute the target text, whereas ontological distance captures essential semantic knowledge that has been encoded in the relations of an ontology.", "labels": [], "entities": []}, {"text": "In response, previous work has attempted to combine distributional and ontological information in computing semantic distance.", "labels": [], "entities": []}, {"text": "For example, researchers have developed measures of semantic distance between texts that apply distributional distances to concept vectors of frequencies rather than to word vectors).", "labels": [], "entities": []}, {"text": "However, these approaches only make pairwise comparisions between the elements of the concept vectors, and do not take into account the important ontological relations among the concepts.", "labels": [], "entities": []}, {"text": "In order to capture such relations, other methods have instead integrated distributional information into an ontological method.", "labels": [], "entities": []}, {"text": "However, such approaches have heretofore been limited to measuring distance between two individual concepts.", "labels": [], "entities": []}, {"text": "For example, some ontological measures use corpus frequencies of words to yield concept weights that are taken into account in measuring the distance between two concepts.", "labels": [], "entities": []}, {"text": "What has been missing is an approach to semantic distance between two texts-two sets of words-that can truly integrate distributional and ontological (relational) information, drawing more fully on their complementary advantages for text comparison.", "labels": [], "entities": [{"text": "semantic distance between two texts-two sets of words-that", "start_pos": 40, "end_pos": 98, "type": "TASK", "confidence": 0.7636433131992817}, {"text": "text comparison", "start_pos": 233, "end_pos": 248, "type": "TASK", "confidence": 0.6726793497800827}]}, {"text": "In this article, we describe anew graph-based distance measure that achieves the desired integration of distributional and ontological factors in measuring semantic distance between two sets of concepts (mapped from two texts).", "labels": [], "entities": []}, {"text": "An ontology is treated as a graph in the usual manner, in which the concepts are nodes and the relations are edges.", "labels": [], "entities": []}, {"text": "A text is represented as a subgraph of the ontology, by mapping the words in the text into their corresponding concepts, which are weighted according to the word frequencies.", "labels": [], "entities": []}, {"text": "We call the resulting set of frequency-weighted concepts a semantic profile.", "labels": [], "entities": []}, {"text": "By exploiting the relational structure of the ontology, we can explicitly measure the ontological distance over the paths between two profiles.", "labels": [], "entities": []}, {"text": "Using the frequencies on the concept nodes, we weight these paths according to the frequency distribution of words in the two texts.", "labels": [], "entities": []}, {"text": "The resulting calculation yields a frequency-weighted ontological distance between the two sets of concepts.", "labels": [], "entities": []}, {"text": "Thus, we view a text not as a set of items to be compared individually to those in another set (with those individual distances then somehow combined, e.g., as in Corley and Mihalcea), but rather as a distribution of \"mass\" within a graph that encodes the semantic relations across the two sets, and use a weighted graph-based approach that captures the aggregate distance between the two frequency masses.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first method to integrate ontological and distributional information in the graphical calculation of text distance.", "labels": [], "entities": [{"text": "text distance", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.6496696025133133}]}, {"text": "This article describes the use of the new measure in several different types of NLP text comparison tasks, in order to explore the situations in which such an approach can be effective.", "labels": [], "entities": [{"text": "NLP text comparison tasks", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.8343471735715866}]}, {"text": "Given the novelty of the approach, the task-based evaluation is not intended as the last word on the usefulness of the method, but rather as a first suite of experiments across different types of text comparison tasks to illuminate some of the strengths and weaknesses of such an approach to text distance.", "labels": [], "entities": [{"text": "text comparison tasks", "start_pos": 196, "end_pos": 217, "type": "TASK", "confidence": 0.755516767501831}, {"text": "text distance", "start_pos": 292, "end_pos": 305, "type": "TASK", "confidence": 0.788664311170578}]}, {"text": "We thus analyze the results in detail to identify future directions for further illuminating when and to what extent the method might be useful.", "labels": [], "entities": []}, {"text": "The analysis reveals that our method is not consistently successful across our sample tasks.", "labels": [], "entities": []}, {"text": "We hypothesize that, because ontological relations play an integral role in our semantic distance measure, the measure is less effective when the semantic profile fora text (the set of corresponding concepts) lacks semantic coherence.", "labels": [], "entities": []}, {"text": "Other work has explored ways to measure the semantic coherence of a set of concepts in terms of their connectedness within an ontology ().", "labels": [], "entities": []}, {"text": "Because a semantic profile in our work includes both ontological (relational) and distributional (frequency) knowledge, we require a measure of semantic coherence that takes both into account.", "labels": [], "entities": []}, {"text": "We develop a novel measure of semantic coherence called profile density that captures both the ontological and distributional coherence of a set of frequencyweighted concepts, and apply it to the data sets used in the different tasks to better understand the performance of our semantic distance measure.", "labels": [], "entities": []}, {"text": "Our distance measure is cast as a graphical text comparison task within a network flow framework as described in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we give an overview of our exploration of the method on three types of text comparison problems.", "labels": [], "entities": [{"text": "text comparison", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.6965055018663406}]}, {"text": "The following three sections present experimental results and analysis of applying our method to the various tasks: verb alternation detection (Section 4), name disambiguation (Section 5), and document classification (Section 6).", "labels": [], "entities": [{"text": "verb alternation detection", "start_pos": 116, "end_pos": 142, "type": "TASK", "confidence": 0.833264430363973}, {"text": "name disambiguation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.8275970816612244}, {"text": "document classification", "start_pos": 193, "end_pos": 216, "type": "TASK", "confidence": 0.7908353805541992}]}, {"text": "In Section 7, we describe our profile density measure and use it to analyze the properties of the data sets that lead to the performance differential across the tasks.", "labels": [], "entities": []}, {"text": "We conclude the paper with a description of related work in text comparison and graph-theoretic NLP approaches (Section 8) and a discussion of some future directions for our research (Section 9).", "labels": [], "entities": [{"text": "text comparison", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.715813010931015}]}], "datasetContent": [{"text": "We select three different NLP tasks that can be formulated as text classification problems based on semantic distance between the texts.", "labels": [], "entities": [{"text": "text classification", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7342753857374191}]}, {"text": "In each case, the texts to be compared are treated as bags of words with associated frequencies.", "labels": [], "entities": []}, {"text": "The tasks are chosen to reflect different types of relations used to extract the relevant words, to see if a varying amount of constraint on the words comprising a text influences the performance of our method.", "labels": [], "entities": []}, {"text": "In verb alternation detection (Section 4), we identify which verbs, out of a set of target and filler verbs, allow a certain variation in the syntactic expression of their underlying argument structure.", "labels": [], "entities": [{"text": "verb alternation detection", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.837269107500712}]}, {"text": "The task is achieved by comparing the set of head words that occur with the verb in each of two different syntactic positions (e.g., subject of intransitive and object of transitive).", "labels": [], "entities": []}, {"text": "In this task, the words that makeup the texts to be compared have a particular syntactic relation to the verb under consideration.", "labels": [], "entities": []}, {"text": "In proper name disambiguation (Section 5), we classify the sense of an ambiguous name according to its local context.", "labels": [], "entities": [{"text": "proper name disambiguation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.8035366535186768}]}, {"text": "This task is similar to word sense disambiguation (WSD), in picking the intended sense of a term, but also has similarities to topic identification, since the proper name delineates a particular domain of discourse.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.8006800611813863}, {"text": "picking the intended sense of a term", "start_pos": 60, "end_pos": 96, "type": "TASK", "confidence": 0.7874237724712917}, {"text": "topic identification", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.7904959321022034}]}, {"text": "In this task, we compare the text constituting the ambiguous instance to texts representing each of the known referents of the name.", "labels": [], "entities": []}, {"text": "Here, the words of a text are extracted from a small window of occurrence around the target name token (25 words on each side), regardless of the syntactic relations among the words.", "labels": [], "entities": []}, {"text": "For the known referents, the words from these windows are aggregated across a small set of labelled instances.", "labels": [], "entities": []}, {"text": "In document classification (Section 6), a text is classified into one of a restricted number of topic categories.", "labels": [], "entities": [{"text": "document classification", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.756534993648529}]}, {"text": "The text to be classified consists of all the words in a document; for each topic, it is compared to a set of words corresponding to a small set of known documents for that topic.", "labels": [], "entities": []}, {"text": "The extracted words are not constrained by syntactic relation (as in verb alternation) or even by distance to a target element (as in name disambiguation).", "labels": [], "entities": [{"text": "name disambiguation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7306865006685257}]}, {"text": "In each case, the resulting bag of words fora text must be mapped into a semantic profile-a frequency-weighted set of concepts in an ontology.", "labels": [], "entities": []}, {"text": "Because all three of our tasks involve general domain text, we use WordNet as our ontology.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9664433002471924}]}, {"text": "(A domain-restricted task may motivate the use of a domain-specific ontology, such as UMLS for comparing medical texts as in Bodenreider.)", "labels": [], "entities": [{"text": "Bodenreider", "start_pos": 125, "end_pos": 136, "type": "DATASET", "confidence": 0.910830557346344}]}, {"text": "Because the noun hierarchy of the WordNet ontology is most developed, we restrict our semantic profiles to use only the nouns from the bag of words corresponding to a text: Any word in the text that appears in the noun hierarchy of WordNet is included in the bag of nouns.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 232, "end_pos": 239, "type": "DATASET", "confidence": 0.9560055136680603}]}, {"text": "The bag of nouns with their associated frequencies must be mapped to the appropriate concepts in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9732832908630371}]}, {"text": "Given the current state of unsupervised WSD, there is generally no attempt to disambiguate the words of a text when performing this kind of mapping-that is, there is no selection of the most appropriate concept or set of concepts to map the words to, given the context of their use.", "labels": [], "entities": [{"text": "WSD", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9367464780807495}]}, {"text": "The simplest method is to distribute the frequency of each word uniformly to its corresponding concepts.", "labels": [], "entities": []}, {"text": "For example, maps the word frequency to the most specific concept(s) for the word, including all of the possible synsets for the word, but not their hypernyms.", "labels": [], "entities": []}, {"text": "also distributes the word frequency uniformly, but does so across the most specific concept(s) and all of their hypernyms.", "labels": [], "entities": []}, {"text": "Other approaches, although still avoiding the difficulties of WSD, do try to capture the overall semantic \"tendencies\" of the set of words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.930982768535614}]}, {"text": "Such methods estimate the appropriate probability distribution over a set of concepts to represent a given bag of nouns as a whole (.", "labels": [], "entities": []}, {"text": "However, such techniques still start with a mapping of each word to all of its immediate concepts.", "labels": [], "entities": []}, {"text": "For all three of our tasks, we take the simple approach of mapping each noun individually to its most specific concepts (not their hypernyms), uniformly dividing the word frequency among them.", "labels": [], "entities": []}, {"text": "In verb alternation, we also experiment with the possibility of finding the best set of frequency-weighted concepts for the full bag of nouns (using the techniques of Li and Abe and), to see if this affects the performance of our method.", "labels": [], "entities": [{"text": "verb alternation", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7230471521615982}]}, {"text": "The precise classification experiment performed using these semantic profiles is described in detail subsequently in the section for each task.", "labels": [], "entities": []}, {"text": "In each case, we compare the performance of our MCF method on the semantic profiles to one or more purely distributional methods using the original word frequency vectors.", "labels": [], "entities": []}, {"text": "We adopt the data set from an investigation of a semantic distance measure that was a precursor to our network flow method.", "labels": [], "entities": []}, {"text": "The selection of these verbs and extraction of their arguments are discussed in the following two sections; we then describe our evaluation methodology.", "labels": [], "entities": []}, {"text": "We evaluate our method on the causative alternation.", "labels": [], "entities": []}, {"text": "As noted previously, in this alternation the target syntactic slots for comparison are the subject of the intransitive (Subj-Intrans) and the object of the transitive (Obj-Trans).", "labels": [], "entities": []}, {"text": "(These are the positions of the chocolate in Examples (1a) and (1b), respectively.)", "labels": [], "entities": []}, {"text": "To identify verbs undergoing this alternation, we randomly selected verbs from among Levin classes that are indicated to allow the causative alternation.", "labels": [], "entities": []}, {"text": "This allows us to test the ability of a distance measure to detect alternation behavior among verbs from a range of semantic classes which may differ in other respects.", "labels": [], "entities": []}, {"text": "We refer to the verbs that are expected to undergo the causative alternation as causative verbs.", "labels": [], "entities": []}, {"text": "For comparison, we randomly selected an equal number of filler verbs, subject to the constraint that their Levin classes do not allow a causative alternation.", "labels": [], "entities": []}, {"text": "(Specifically, none of the classes containing a filler verb allows an alternation in which the same underlying argument appears in the Subj-Intrans slot as well as the Obj-Trans slot.)", "labels": [], "entities": []}, {"text": "The full set of potential causative and filler verbs were filtered according to corpus counts, as described next.", "labels": [], "entities": []}, {"text": "We use the accuracy of labelling all instances as our evaluation measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9992640614509583}]}, {"text": "To compare to prior results using F-measure, we report that in some tables.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9800602197647095}]}, {"text": "Because we label all instances, accuracy and F-measure are equivalent in our method, using 2rp/(r + p) as the definition of F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9994103908538818}, {"text": "F-measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9556835293769836}]}, {"text": "The random baseline for our task is the accuracy of labelling all instances with the predominant name, as shown in the \"Majority\" column of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9992321729660034}]}, {"text": "Because we use the data set of Pedersen, Purandare, and, we compare our performance to their distributional method (reporting their best results both with and without singular value decomposition).", "labels": [], "entities": []}, {"text": "Because their method is an unsupervised one, we also train and test a supervised learner using distributional data (LIBSVM by Chang and Lin).", "labels": [], "entities": []}, {"text": "For each set of training data, we remove stopwords and use the remaining words (with their frequencies) as input features for the SVM.", "labels": [], "entities": []}, {"text": "We then obtain the optimal parameters (i.e., optimal values for cost and gamma in LIBSVM) by using 10-fold cross-validation over the training data.", "labels": [], "entities": []}, {"text": "Finally, we perform classification on the test data using those parameters.", "labels": [], "entities": []}, {"text": "This enables us to compare our results to a purely distributional method with access to the same training data.", "labels": [], "entities": []}, {"text": "Because our method is supervised, it is important to minimize the amount of annotated data required to build the gold-standard profiles.", "labels": [], "entities": []}, {"text": "(Lengthy training time can also bean issue fora supervised method, but here \"training\" is the straightforward task of building an aggregate semantic profile.)", "labels": [], "entities": []}, {"text": "Because it is unclear a priori what amount of training data is sufficient, we experiment with several quantities.", "labels": [], "entities": []}, {"text": "We initially select 200 random instances per pair of names, respecting the relative proportions of the two names overall.", "labels": [], "entities": []}, {"text": "(Two hundred instances constitute about 0.1-10% of the data per pair of names.)", "labels": [], "entities": []}, {"text": "Subsequently, we decrease the quantity further, to one-half and onequarter the original amount (100 and 50 instances, respectively) to observe how the Average results for the network flow (NF) results using 200 instances per gold-standard profile, SVM using 200 training vectors, and Ped05 and Ped05 SVD (the best results without and with SVD, respectively).", "labels": [], "entities": []}, {"text": "All results are F-measure (the same as accuracy for our method and SVM).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.998236894607544}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9994474053382874}]}, {"text": "The weighted average is calculated based on the number of instances in each pair of names.", "labels": [], "entities": []}, {"text": "The best result for each name pair is indicated in boldface.", "labels": [], "entities": []}, {"text": "performance is influenced by the amount of data used to construct the gold standard profiles.", "labels": [], "entities": []}, {"text": "9 To reduce the impact of possible skewed sampling of training data, we repeat the random sampling five times, with no overlap between the random samples.", "labels": [], "entities": []}, {"text": "We report the performance of each sample set as well as the average over the five samples.", "labels": [], "entities": []}, {"text": "On dataset1, our network flow distance performs better than or as well as all other measures on the individual frequency bands, as shown in.", "labels": [], "entities": []}, {"text": "On all verbs combined (the \"All\" column) the performance of our method is not the best, although the Wilcoxon test shows no significant difference between the rankings of NF and the best measure (Manhattan).", "labels": [], "entities": [{"text": "Manhattan)", "start_pos": 196, "end_pos": 206, "type": "METRIC", "confidence": 0.9587500989437103}]}, {"text": "(The difference in rankings between NF and all other measures is significant.)", "labels": [], "entities": [{"text": "NF", "start_pos": 36, "end_pos": 38, "type": "DATASET", "confidence": 0.7287690043449402}]}, {"text": "Interestingly, we find that the \"All Verbs\" performance of NF (and that of several other methods) is indeed worse than the performance on the individual frequency bands.", "labels": [], "entities": [{"text": "All Verbs\"", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.8570334315299988}]}, {"text": "We examined the distance values across the frequency bands to determine the cause for this pattern.", "labels": [], "entities": []}, {"text": "We found that low frequency verbs tend to have smaller distances Accuracies on dataset1 by the network flow method (NF), cosine, Manhattan distance, Euclidean distance, skew divergence (skew div), and Jensen-Shannon divergence (JS).", "labels": [], "entities": []}, {"text": "Best accuracies in each condition are shown in boldface.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9729841351509094}]}, {"text": "Our data is a corpus of articles from 20 different Usenet newsgroups released by.", "labels": [], "entities": []}, {"text": "Because each newsgroup corresponds to a topic, the articles can be classified using the (single) newsgroup label.", "labels": [], "entities": []}, {"text": "We use the collection maintained by, in which all the duplicates (cross-posts) are removed, resulting in 18,828 articles.", "labels": [], "entities": []}, {"text": "The articles are approximately evenly distributed among the 20 newsgroups.", "labels": [], "entities": []}, {"text": "Stopwords and article headers are removed before processing each text.", "labels": [], "entities": []}, {"text": "Work that relies on word frequency vectors to represent the texts in document classification has revealed the importance of preprocessing the word frequency data to emphasize those terms that are likely to be most meaningful.", "labels": [], "entities": [{"text": "document classification", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7387610971927643}]}, {"text": "For example, word frequencies have typically been weighted by inverse document frequencies (tf \u00b7 idf ) to lessen the impact of very common but less distinguishing words.", "labels": [], "entities": []}, {"text": "According to, their best system on the same corpus uses the log tf +1 log idf weighting scheme.", "labels": [], "entities": []}, {"text": "In order to compare our system to theirs, we use this same word weighting scheme in the creation of the word vectors that are used to produce our semantic profiles.", "labels": [], "entities": []}, {"text": "(We have experimented with using raw word frequencies as well as tf \u00b7 idf to produce profiles.", "labels": [], "entities": []}, {"text": "Both methods yield approximately the same results as the log tf +1 log idf frequency weighting scheme.)", "labels": [], "entities": []}, {"text": "As mentioned before, we treat the classification task similarly to name disambiguation, taking a minimally supervised approach.", "labels": [], "entities": [{"text": "classification task", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.9010157883167267}, {"text": "name disambiguation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8702282309532166}]}, {"text": "We randomly select a small number of documents as training data for creating the gold-standard semantic profiles.", "labels": [], "entities": []}, {"text": "We use 10 or 30 documents per newsgroup, or approximately 1-3% of the documents.", "labels": [], "entities": []}, {"text": "The remaining documents are used as testing data.", "labels": [], "entities": []}, {"text": "Again, we use a random sample of documents for each gold-standard profile, repeated five times to minimize the impact of a possible skewed sampling.", "labels": [], "entities": []}, {"text": "We report the average accuracy over the five samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9989657402038574}]}, {"text": "Because there are 20 possible topic labels, the random baseline is very low, at 5%.", "labels": [], "entities": []}, {"text": "(Using the predominant label raises this only slightly.)", "labels": [], "entities": []}, {"text": "A more informative evaluation of our method is to compare to a state-of-the-art approach that is purely distributional.", "labels": [], "entities": []}, {"text": "A comparison to is natural, since we use the same data set.", "labels": [], "entities": []}, {"text": "However, they trained an SVM on 30 documents per class and tested on 10% of the documents, repeated 10 times.", "labels": [], "entities": []}, {"text": "Because our training approach differs somewhat (training on 10 or 30 documents per class, testing on all remaining documents, repeated 5 times), we also replicate their SVM experiment using our training and test sets.", "labels": [], "entities": []}, {"text": "As in the name disambiguation task, we use the LIBSVM software package (Chang and Lin 2001) and tune the classifier in the training phase for the best SVM parameters prior to the testing.", "labels": [], "entities": [{"text": "name disambiguation task", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8287962675094604}, {"text": "LIBSVM software package", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.8733917474746704}]}, {"text": "Also as in our name disambiguation task, we additionally train and test the SVM on just the nouns in a document (rather than all words), and also on the nouns mapped to concepts (with the relevant frequencies as the feature values in both cases).", "labels": [], "entities": [{"text": "name disambiguation task", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.8506044149398804}]}, {"text": "Thus we report results of the SVM on three different types of input frequency vectors: all words, nouns, and concepts.", "labels": [], "entities": [{"text": "SVM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9389131665229797}]}], "tableCaptions": [{"text": " Table 1  Accuracies on dataset1 by the network flow method (NF), cosine, Manhattan distance, Euclidean  distance, skew divergence (skew div), and Jensen-Shannon divergence (JS). Best accuracies in  each condition are shown in boldface.", "labels": [], "entities": []}, {"text": " Table 2  Accuracies on dataset2 by the network flow method (NF), cosine, Manhattan distance, Euclidean  distance, skew divergence (skew div), and Jensen-Shannon divergence (JS). Best accuracies in  each condition are shown in boldface.", "labels": [], "entities": []}, {"text": " Table 3  Average accuracies by the network flow method (NF), Manhattan distance (Man), skew  divergence (skew div), and Jensen-Shannon divergence (JS) on different profiles: original  (\"raw\"), Li and Abe, and Clark and Weir profiles. Best accuracies in each condition are shown in  boldface.", "labels": [], "entities": [{"text": "Manhattan distance (Man)", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.8433287382125855}, {"text": "Jensen-Shannon divergence (JS)", "start_pos": 121, "end_pos": 151, "type": "METRIC", "confidence": 0.6091230869293213}]}, {"text": " Table 4  The pairs to be identified, the raw frequencies, and the relative frequency of the majority name.", "labels": [], "entities": []}, {"text": " Table 5  Network flow results using 200 training instances on the random samples and their average  performance.", "labels": [], "entities": []}, {"text": " Table 6  Average results for the network flow (NF) results using 200 instances per gold-standard profile,  SVM using 200 training vectors, and Ped05 and Ped05 SVD (the best results without and with  SVD, respectively). All results are F-measure (the same as accuracy for our method and SVM).  The weighted average is calculated based on the number of instances in each pair of names. The  best result for each name pair is indicated in boldface.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 236, "end_pos": 245, "type": "METRIC", "confidence": 0.9903520941734314}, {"text": "accuracy", "start_pos": 259, "end_pos": 267, "type": "METRIC", "confidence": 0.9975619316101074}]}, {"text": " Table 7  Average classification results of the network flow method using 200, 100, and 50 training data  per classification task. The weighted average is calculated based on the number of test instances  per task.", "labels": [], "entities": []}, {"text": " Table 8  Average classification results using 10 and 30 training documents per newsgroup.", "labels": [], "entities": []}, {"text": " Table 11  The profile density scores for each data set at five different values of \u03b1, as well as the average  scores across the \u03b1 values.", "labels": [], "entities": []}]}