{"title": [{"text": "Re-structuring, Re-labeling, and Re-aligning for Syntax-Based Machine Translation", "labels": [], "entities": [{"text": "Syntax-Based Machine Translation", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.714205821355184}]}], "abstractContent": [{"text": "This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "parsing and alignment", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.6912140448888143}, {"text": "statistical machine translation (SMT)", "start_pos": 143, "end_pos": 180, "type": "TASK", "confidence": 0.8210681478182474}]}, {"text": "We present three modifications to the MT training data to improve the accuracy of a state-of-the-art syntax MT system: restructuring changes the syntactic structure of training parse trees to enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context; and realigning unifies word alignment across sentences to remove bad word alignments and refine good ones.", "labels": [], "entities": [{"text": "MT training", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9086739420890808}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9985659718513489}]}, {"text": "Better structures, labels, and word alignments are learned by the EM algorithm.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.6986025422811508}]}, {"text": "We show that each individual technique leads to improvement as measured by BLEU, and we also show that the greatest improvement is achieved by combining them.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.999163031578064}]}, {"text": "We report an overall 1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9979435801506042}, {"text": "NIST08 evaluation", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.9493762850761414}]}, {"text": "1. Background Syntactic methods have recently proven useful in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.8278294404347738}]}, {"text": "In this article, we explore different ways of exploiting the structure of bilingual material for syntax-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.8965933322906494}]}, {"text": "In particular, we ask what kinds of tree structures, tree labels, and word alignments are best suited for improving end-to-end translation accuracy.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7205884754657745}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.754922091960907}]}, {"text": "We begin with structures from standard parsing and alignment tools, then use the EM algorithm to revise these structures in light of the translation task.", "labels": [], "entities": [{"text": "parsing and alignment", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7280313074588776}]}, {"text": "We report an overall +1.48 BLEU improvement on a standard Chinese-to-English test.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9978392720222473}]}], "introductionContent": [], "datasetContent": [{"text": "For our experiments, we use a 245 million word Chinese/English bitext, available from LDC.", "labels": [], "entities": [{"text": "LDC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9755815267562866}]}, {"text": "A re-implementation of the Collins (1997) parser runs on the English half of the bitext to produce parse trees, and GIZA runs on the entire bitext to produce M4 word alignments.", "labels": [], "entities": [{"text": "Collins (1997) parser", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.894218361377716}, {"text": "M4 word alignments", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.6272043983141581}]}, {"text": "We extract a subset of 36 million words from the entire bitext, by selecting only sentences in the mainland news domain.", "labels": [], "entities": []}, {"text": "We extract translation rules from these selected 36 million words.", "labels": [], "entities": []}, {"text": "Experiments show that our Chinese/English syntax MT systems built from this selected bitext give as high BLEU scores as from the entire bitext.", "labels": [], "entities": [{"text": "Chinese/English syntax MT", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.4255958318710327}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9991446733474731}]}, {"text": "Our development set consists of 1,453 lines and is extracted from the NIST02-NIST05 evaluation sets, for tuning of feature weights.", "labels": [], "entities": [{"text": "NIST02-NIST05 evaluation sets", "start_pos": 70, "end_pos": 99, "type": "DATASET", "confidence": 0.942072868347168}]}, {"text": "The development set is from the newswire domain, and we chose it to represent a wide period of time rather than a single year.", "labels": [], "entities": []}, {"text": "We use the NIST08 evaluation set as our test set.", "labels": [], "entities": [{"text": "NIST08 evaluation set", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.9611304998397827}]}, {"text": "Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion.", "labels": [], "entities": [{"text": "NIST08 evaluation set", "start_pos": 12, "end_pos": 33, "type": "DATASET", "confidence": 0.9148551821708679}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9995294809341431}]}, {"text": "We use two 5-gram language models.", "labels": [], "entities": []}, {"text": "One is trained on the English half of the bitext.", "labels": [], "entities": [{"text": "English half of the bitext", "start_pos": 22, "end_pos": 48, "type": "DATASET", "confidence": 0.919685709476471}]}, {"text": "The other is trained on one billion words of monolingual data.", "labels": [], "entities": []}, {"text": "Kneser-Ney smoothing is applied to both language models.", "labels": [], "entities": []}, {"text": "Language models are represented using randomized data structures similar to those of in decoding for efficient RAM usage.", "labels": [], "entities": []}, {"text": "To test the significance of improvements over the baseline, we compute paired bootstrap p-values) for BLEU between the baseline system and each improved system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9989935755729675}]}, {"text": "The end-to-end MT experiment used as baseline and described in Sections 3.5 and 4.3 was trained on IBM Model 4 word alignments, obtained by running GIZA, as described in Section 2.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9674744606018066}, {"text": "IBM Model 4 word alignments", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6169753074645996}]}, {"text": "We compared this baseline to an MT system that used alignments obtained by re-aligning the GIZA alignments using the method of Section 5.2 with the 36 million word subset of the training corpus used for re-alignment learning.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9489402174949646}]}, {"text": "We next compared the baseline to an MT system that used re-alignments obtained by also incorporating the size prior described in Section 5.4.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9607800245285034}]}, {"text": "As can be seen by the results in, the size prior method is needed to obtain reasonable improvement in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9959020018577576}]}, {"text": "These results are consistent with those reported in, where gains in Chinese and Arabic MT systems were observed, though over a weaker baseline and with less training data than is used in this work.", "labels": [], "entities": [{"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9004542231559753}]}], "tableCaptions": [{"text": " Table 1  Translation accuracy versus binarization algorithms. In this and all other tables reporting BLEU  performance, statistically significant improvements over the baseline are highlighted. p = the  paired bootstrap p-value computed between each system and the baseline, showing the level at  which the two systems are significantly different.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9551261067390442}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7319849133491516}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9273947477340698}]}, {"text": " Table 3  Binarization bias learned by the EM re-structuring method on the model 4 word alignments.", "labels": [], "entities": []}, {"text": " Table 4  Grammar size vs. re-labeling methods. Re-labeling does not explode the grammar size.", "labels": [], "entities": []}, {"text": " Table 5  Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was  carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values  are computed against Baseline1.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9903316497802734}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9057005047798157}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9966268539428711}, {"text": "Four-way splitting", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6468679010868073}]}, {"text": " Table 6  Translation performance, grammar size versus the re-alignment algorithm proposed in  Section 5.2, and re-alignment as modified in Section 5.4.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9708007574081421}]}, {"text": " Table 7  Summary of experiments in this article, including a combined experiment with re-alignment,  re-structuring, and re-labeling.", "labels": [], "entities": []}]}