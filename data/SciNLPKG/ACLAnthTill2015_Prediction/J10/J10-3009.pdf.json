{"title": [{"text": "Linguistically Annotated Reordering: Evaluation and Analysis", "labels": [], "entities": [{"text": "Linguistically Annotated Reordering", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.48845183849334717}]}], "abstractContent": [{"text": "Linguistic knowledge plays an important role in phrase movement in statistical machine translation.", "labels": [], "entities": [{"text": "phrase movement", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.8310621678829193}, {"text": "statistical machine translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7124532461166382}]}, {"text": "To efficiently incorporate linguistic knowledge into phrase reordering, we propose anew approach: Linguistically Annotated Reordering (LAR).", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7544476389884949}]}, {"text": "In LAR, we build hard hierarchical skeletons and inject soft linguistic knowledge from source parse trees to nodes of hard skeletons during translation.", "labels": [], "entities": []}, {"text": "The experimental results on large-scale training data show that LAR is comparable to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very competitive lexicalized reordering approach.", "labels": [], "entities": []}, {"text": "When combined with BWR, LAR provides complementary information for phrase reordering, which collectively improves the BLEU score significantly.", "labels": [], "entities": [{"text": "BWR", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.5142071843147278}, {"text": "LAR", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9018154740333557}, {"text": "phrase reordering", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8441740274429321}, {"text": "BLEU score", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9808516502380371}]}, {"text": "To further understand the contribution of linguistic knowledge in LAR to phrase reordering, we introduce a syntax-based analysis method to automatically detect constituent movement in both reference and system translations, and summarize syntactic reordering patterns that are captured by reordering models.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.774337649345398}]}, {"text": "With the proposed analysis method, we conduct a comparative analysis that not only provides the insight into how linguistic knowledge affects phrase movement but also reveals new challenges in phrase reordering.", "labels": [], "entities": [{"text": "phrase movement", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.7505073249340057}, {"text": "phrase reordering", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.8045016229152679}]}], "introductionContent": [{"text": "The phrase-based approach is a widely accepted formalism in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.8026689489682516}]}, {"text": "It segments the source sentence into a sequence of phrases (not necessarily syntactic phrases), then translates and reorders these phrases in the target.", "labels": [], "entities": []}, {"text": "The reason for the popularity of phrasal SMT is its capability of non-compositional translations and local word reorderings within phrases.", "labels": [], "entities": [{"text": "phrasal SMT", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.5496410131454468}]}, {"text": "Unfortunately, reordering at the phrase level is still problematic for phrasal SMT.", "labels": [], "entities": [{"text": "phrasal SMT", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.5925496816635132}]}, {"text": "The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases.", "labels": [], "entities": []}, {"text": "In order to utilize lexical information for phrase reordering, Tillman (2004) and propose lexicalized reordering models which directly condition phrase movement on phrases themselves.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7603326439857483}]}, {"text": "One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data.", "labels": [], "entities": []}, {"text": "To eliminate this restriction, Xiong, Liu, and suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence.", "labels": [], "entities": []}, {"text": "Although these lexicalized reordering models significantly outperform the distortion-based reordering model as reported, only using lexical information (e.g., boundary words) is not adequate to move phrases to appropriate positions.", "labels": [], "entities": []}, {"text": "Consider the following Chinese example with its English translation: In this example, boundary words \ud97b\udf59 and \ud97b\udf59 are able to decide that the translation of the PP phrase \ud97b\udf59...\ud97b\udf59 should be postponed until some phrase that succeeds it is translated.", "labels": [], "entities": []}, {"text": "But they cannot provide further information about exactly which succeeding phrase should be translated first.", "labels": [], "entities": []}, {"text": "If high-level linguistic knowledge, such as the syntactic context VP\u2192PP VP, is given, the position of the PP phrase can be easily determined since the pre-verbal modifier PP in Chinese is frequently translated into a post-verbal counterpart in English.", "labels": [], "entities": []}, {"text": "In this article, we focus on linguistically motivated phrase reordering, which integrates high-level linguistic knowledge in phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7596122622489929}, {"text": "phrase reordering", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7649661898612976}]}, {"text": "We adopt a two-step strategy.", "labels": [], "entities": []}, {"text": "In the first step, we establish a hierarchical skeleton in phrasal SMT by incorporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.6639172434806824}, {"text": "Bracketing Transduction Grammar (BTG) (Wu 1997)", "start_pos": 88, "end_pos": 135, "type": "TASK", "confidence": 0.6733054667711258}, {"text": "phrasal SMT", "start_pos": 141, "end_pos": 152, "type": "TASK", "confidence": 0.5613763630390167}]}, {"text": "In the second step, we inject soft linguistic information into nodes of the skeleton.", "labels": [], "entities": []}, {"text": "There are two significant advantages to using BTG in phrasal SMT.", "labels": [], "entities": [{"text": "phrasal SMT", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.5637038052082062}]}, {"text": "First, BTG is able to generate hierarchical structures.", "labels": [], "entities": [{"text": "BTG", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.6376764178276062}]}, {"text": "This not only enhances phrasal SMT's capability for hierarchical and long-distance reordering but also establishes a platform for phrasal SMT to incorporate knowledge from linguistic structure.", "labels": [], "entities": [{"text": "phrasal SMT", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.6229281425476074}, {"text": "phrasal SMT", "start_pos": 130, "end_pos": 141, "type": "TASK", "confidence": 0.5517583191394806}]}, {"text": "Second, phrase reordering is restricted by the ITG constraint (.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7651323676109314}]}, {"text": "Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it is broadly verified that the ITG constraint has good coverage of word reorderings on various language pairs).", "labels": [], "entities": []}, {"text": "This makes phrase reordering in phrasal SMT a more tractable task.", "labels": [], "entities": [{"text": "phrase reordering in phrasal SMT", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.5798863470554352}]}, {"text": "After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject soft linguistic information into the nodes of the skeleton.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8317787647247314}]}, {"text": "We annotate each BTG node with syntactic and lexical elements by projecting the source parse tree onto the BTG binary tree.", "labels": [], "entities": [{"text": "BTG binary tree", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.9195947448412577}]}, {"text": "The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree.", "labels": [], "entities": []}, {"text": "To address this issue, we propose an annotation algorithm.", "labels": [], "entities": []}, {"text": "The algorithm is able to label any BTG nodes during decoding with very little overhead, regardless of whether the BTG nodes are aligned with syntactic constituent nodes in the source parse tree.", "labels": [], "entities": []}, {"text": "The annotated linguistic elements are then used to guide phrase reordering under the ITG constraint.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7017473578453064}]}, {"text": "We call this two-step phrase reordering strategy linguistically annotated reordering (LAR)).", "labels": [], "entities": []}, {"text": "Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG.", "labels": [], "entities": [{"text": "BTG", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9356692433357239}]}, {"text": "However, they use boundary words as reordering features at the second step.", "labels": [], "entities": []}, {"text": "To distinguish this from our work, we call their approach boundary word-based reordering (BWR).", "labels": [], "entities": [{"text": "word-based reordering (BWR)", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7226587295532226}]}, {"text": "LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures.", "labels": [], "entities": [{"text": "BWR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9773861169815063}, {"text": "BTG-based phrasal SMT", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.5699033737182617}]}, {"text": "Furthermore, they can be combined.", "labels": [], "entities": []}, {"text": "We evaluate LAR vs. BWR using the automatic metric BLEU (.", "labels": [], "entities": [{"text": "LAR", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9132130742073059}, {"text": "BWR", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9027783870697021}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.7844690084457397}]}, {"text": "The BLEU scores show that LAR is comparable to BWR and significantly improves phrase reordering when combined with BWR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9985401630401611}, {"text": "LAR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9847219586372375}, {"text": "phrase reordering", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.8415878117084503}]}, {"text": "We want to further study what happens when we combine BWR with LAR.", "labels": [], "entities": [{"text": "BWR", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.5822632908821106}]}, {"text": "In particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place.", "labels": [], "entities": [{"text": "SMT system", "start_pos": 137, "end_pos": 147, "type": "TASK", "confidence": 0.9241667985916138}]}, {"text": "The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT.", "labels": [], "entities": [{"text": "phrase movement", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.6972458511590958}, {"text": "phrasal SMT", "start_pos": 199, "end_pos": 210, "type": "TASK", "confidence": 0.6204633116722107}]}, {"text": "Because syntactic constituents are often moved together across languages during translation, we particularly study how linguistic knowledge affects syntactic constituent movement.", "labels": [], "entities": [{"text": "syntactic constituent movement", "start_pos": 148, "end_pos": 178, "type": "TASK", "confidence": 0.6488155921300253}]}, {"text": "To that end, we introduce a syntax-based analysis method.", "labels": [], "entities": []}, {"text": "We parse source sentences, and align the parse trees with reference translations as well as system translations.", "labels": [], "entities": []}, {"text": "We then summarize syntactic reordering patterns using contextfree grammar (CFG) rules from the obtained tree-to-string alignments.", "labels": [], "entities": [{"text": "summarize syntactic reordering patterns", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.8015055805444717}]}, {"text": "The extracted reordering patterns clearly show the trace of syntactic constituent movement in both reference translations and system translations.", "labels": [], "entities": []}, {"text": "With the proposed analysis method, we analyze the combination of BWR and LAR vs. BWR alone.", "labels": [], "entities": [{"text": "BWR", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.778003454208374}, {"text": "BWR", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.654285728931427}]}, {"text": "There are essentially three issues that are addressed in this syntax-based comparative analysis.", "labels": [], "entities": [{"text": "syntax-based comparative analysis", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.6614049275716146}]}], "datasetContent": [{"text": "Our system is a BTG-based phrasal SMT system, developed following Section 2.", "labels": [], "entities": [{"text": "BTG-based phrasal SMT", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.6414275765419006}]}, {"text": "We integrate the boundary word-based reordering model and the linguistically annotated reordering model into our system according to our reordering configuration.", "labels": [], "entities": []}, {"text": "We carried out various experiments to evaluate the reordering example extraction algorithms of Section 3, the linguistically annotated reordering model vs. boundary word-based reordering model, and the effects of linguistically annotated features on the Chinese-toEnglish translation task of the NIST MT-05 using large scale training data.", "labels": [], "entities": [{"text": "reordering example extraction", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.6458178261915842}, {"text": "Chinese-toEnglish translation task", "start_pos": 254, "end_pos": 288, "type": "TASK", "confidence": 0.6802095174789429}, {"text": "NIST MT-05", "start_pos": 296, "end_pos": 306, "type": "DATASET", "confidence": 0.922580748796463}]}, {"text": "We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M Chinese words and 112.78M English words) listed in in both directions and then applied the \"grow-diag-final\" refinement rule (Koehn, Och, and Marcu 2003) to obtain many-to-many word alignments.", "labels": [], "entities": []}, {"text": "From the word-aligned corpora, we extracted bilingual phrases.", "labels": [], "entities": []}, {"text": "We used all corpora listed in except for the United Nations corpus to train our reordering models, which consist of 33.3M Chinese words and 35.79M English words.", "labels": [], "entities": [{"text": "United Nations corpus", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.933758536974589}]}, {"text": "We ran the reordering example extractor AExtractor and TExtractor of Section 3 on the chosen word-aligned corpora.", "labels": [], "entities": [{"text": "AExtractor", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9772540330886841}, {"text": "TExtractor", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9959127306938171}]}, {"text": "We then extracted boundary word features from the reordering examples.", "labels": [], "entities": []}, {"text": "To extract linguistically annotated features, we parsed the Chinese side of the chosen parallel text using a Chinese parser (Xiong, Liu, and Lin 2005) which was trained on the Penn Chinese Treebank with an F 1 -score of 79.4%.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 176, "end_pos": 197, "type": "DATASET", "confidence": 0.9820073445638021}, {"text": "F 1 -score", "start_pos": 206, "end_pos": 216, "type": "METRIC", "confidence": 0.9851494133472443}]}, {"text": "We ran the off-the-shelf MaxEnt toolkit 8 to tune the reordering feature weights with the iteration number set to 100 and Gaussian prior to 1 to avoid overfitting.", "labels": [], "entities": [{"text": "MaxEnt toolkit 8", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9227098623911539}]}, {"text": "We built our 4-gram language model using the SRILM toolkit (Stolcke 2002), which was trained on the Xinhua section of the English Gigaword corpus (181.1M words).", "labels": [], "entities": [{"text": "SRILM toolkit (Stolcke 2002)", "start_pos": 45, "end_pos": 73, "type": "DATASET", "confidence": 0.8702405293782552}, {"text": "English Gigaword corpus", "start_pos": 122, "end_pos": 145, "type": "DATASET", "confidence": 0.8247117201487223}]}, {"text": "We selected 580 short sentences (not exceeding 50 characters per sentence) from the NIST MT-02 evaluation test data as our development set (18 words/31 characters per sentence).", "labels": [], "entities": [{"text": "NIST MT-02 evaluation test data", "start_pos": 84, "end_pos": 115, "type": "DATASET", "confidence": 0.9288285851478577}]}, {"text": "The NIST MT-05 test set includes 1,082 sentences with an average of 27.4 words/47.6 characters per sentence.", "labels": [], "entities": [{"text": "NIST MT-05 test set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9109944850206375}]}, {"text": "The reference corpus for the NIST MT-05 test set contains four translations per source sentence.", "labels": [], "entities": [{"text": "NIST MT-05 test set", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.8805211335420609}]}, {"text": "Both the development and test sets were also parsed using the parser mentioned above.", "labels": [], "entities": []}, {"text": "Our evaluation metric is the case-insensitive BLEU-4 () using the shortest reference sentence length for the brevity penalty.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9868205189704895}]}, {"text": "The model feature weights are tuned on the development set to maximize BLEU using MERT ).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.999235987663269}, {"text": "MERT", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9853317141532898}]}, {"text": "Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn 2004).", "labels": [], "entities": [{"text": "BLEU score differences", "start_pos": 28, "end_pos": 50, "type": "METRIC", "confidence": 0.9486690560976664}]}], "tableCaptions": [{"text": " Table 3  Comparison of reordering example extraction algorithms and selection rules. We only use BWR  as the reordering model for this comparison.", "labels": [], "entities": [{"text": "reordering example extraction", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6749210158983866}, {"text": "BWR", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9570667743682861}]}, {"text": " Table 4  BLEU scores for LAR, BWR, and their combinations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993377327919006}, {"text": "LAR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9787868857383728}, {"text": "BWR", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9719176888465881}]}, {"text": " Table 5  BLEU scores on different training data sets. Large1 refers to the corpora listed in Table 2 except  for the United Nations corpus. Large2 includes all corpora listed in Table 2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995007514953613}, {"text": "United Nations corpus", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.9344368378321329}]}, {"text": " Table 7  Statistics of multi-branching and REF/SYS-reorderable nodes per sentence.", "labels": [], "entities": []}, {"text": " Table 8  Distribution of number of different orders by which syntactic constituents are translated in  references.", "labels": [], "entities": []}, {"text": " Table 9  Two-order translation distribution of 4 NP-related constituents.", "labels": [], "entities": []}, {"text": " Table 10  F 1 -scores ( BWR+LAR vs. BWR) for the 13 most frequent constituents in the test corpus.  Constituents indicated in bold have relatively lower F 1 score for reordering.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9878694266080856}, {"text": "BWR+LAR", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8785674174626669}, {"text": "BWR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.7238789796829224}, {"text": "F 1 score", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9871053894360861}]}, {"text": " Table 11  Syntactic reordering precision and recall of BWR+LAR vs. BWR on the test corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9876375198364258}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9991704225540161}, {"text": "BWR+LAR", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.7434535225232443}, {"text": "BWR", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8539085388183594}]}, {"text": " Table 12  Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider  the gap in syntactic reordering patterns.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9983958601951599}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.999384880065918}, {"text": "BWR+LAR", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.7570108771324158}, {"text": "BWR", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9208213686943054}]}]}