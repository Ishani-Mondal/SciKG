{"title": [{"text": "Hierarchical Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars", "labels": [], "entities": [{"text": "Hierarchical Phrase-Based Translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6538186967372894}]}], "abstractContent": [{"text": "In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation and alignment.", "labels": [], "entities": [{"text": "phrase-based translation and alignment", "start_pos": 76, "end_pos": 114, "type": "TASK", "confidence": 0.7374838590621948}]}, {"text": "The decoder is implemented with standard Weighted Finite-State Transducer (WFST) operations as an alternative to the well-known cube pruning procedure.", "labels": [], "entities": []}, {"text": "We find that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, better parameter optimization, and improved translation performance.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8554346561431885}, {"text": "translation search", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.9554480016231537}]}, {"text": "The direct generation of translation lattices in the target language can improve subsequent rescoring procedures, yielding further gains when applying long-span language models and Minimum Bayes Risk decoding.", "labels": [], "entities": [{"text": "Minimum Bayes Risk decoding", "start_pos": 181, "end_pos": 208, "type": "TASK", "confidence": 0.6268975883722305}]}, {"text": "We also provide insights as to how to control the size of the search space defined by hierarchical rules.", "labels": [], "entities": []}, {"text": "We show that shallow-n grammars, low-level rule catenation, and other search constraints can help to match the power of the translation system to specific language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hierarchical phrase-based translation) is one of the current promising approaches to statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Hierarchical phrase-based translation)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7689544036984444}, {"text": "statistical machine translation (SMT)", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.8154177566369375}]}, {"text": "Hiero SMT systems are based on probabilistic synchronous context-free grammars (SCFGs) whose translation rules can be extracted automatically from word-aligned parallel text.", "labels": [], "entities": [{"text": "Hiero SMT", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.6149724572896957}]}, {"text": "These grammars can produce a very rich space of candidate translations and, relative to simpler phrasebased systems, the power of Hiero is most evident in translation between dissimilar languages, such as English and Chinese.", "labels": [], "entities": []}, {"text": "Hiero is able to learn and apply complex patterns in movement and translation that are not possible with simpler systems.", "labels": [], "entities": []}, {"text": "Hiero can also be used to good effect on \"simpler\" problems, such as translation between English and Spanish (), even though there is not the same need for the full complexity of movement and translation.", "labels": [], "entities": [{"text": "translation between English and Spanish", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.8841344833374023}]}, {"text": "If gains in using Hiero are small, however, the computational and modeling complexity involved are difficult to justify.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.7905415296554565}]}, {"text": "Such concerns would vanish if there were reliable methods to match Hiero complexity for specific translation problems.", "labels": [], "entities": [{"text": "Hiero complexity", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.7851432263851166}]}, {"text": "Loosely put, it would be a good thing if the complexity of a system was somehow proportional to the improvement in translation quality the system delivers.", "labels": [], "entities": []}, {"text": "Another notable current trend in SMT is system combination.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9966939687728882}, {"text": "system combination", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7759934365749359}]}, {"text": "Minimum Bayes Risk decoding is widely used to rescore and improve hypotheses produced by individual systems, and more aggressive system combination techniques which synthesize entirely new hypotheses from those of contributing systems can give even greater translation improvements (.", "labels": [], "entities": []}, {"text": "It is now commonplace to note that even the best available individual SMT system can be significantly improved upon by such techniques.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9916247725486755}]}, {"text": "This puts a burden on the underlying SMT systems which is somewhat unusual in NLP.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9890700578689575}]}, {"text": "The requirement is not merely to produce a single hypothesis that is as good as possible.", "labels": [], "entities": []}, {"text": "Ideally, the SMT systems should generate large collections of candidate hypotheses that are simultaneously diverse and of good quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9905130863189697}]}, {"text": "Relative to these concerns, previously published descriptions of Hiero have noted certain limitations.", "labels": [], "entities": []}, {"text": "Spurious ambiguity) was described as a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation.", "labels": [], "entities": []}, {"text": "This can result in n-best lists with very few different translations which is problematic for the minimum-error-rate training algorithm ...", "labels": [], "entities": []}, {"text": "This is due in part to the cube pruning procedure, which enumerates all distinct hypotheses to a fixed depth by means of k-best hypothesis lists.", "labels": [], "entities": []}, {"text": "If enumeration was not necessary, or if the lists could be arbitrarily deep, there might still be many duplicate derivations, but at least the hypothesis space would not be impoverished.", "labels": [], "entities": []}, {"text": "Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu 1997;).", "labels": [], "entities": []}, {"text": "For our purposes we say that overgeneration occurs when different derivations based on the same set of rules give rise to different translations.", "labels": [], "entities": []}, {"text": "An example is given in.", "labels": [], "entities": []}, {"text": "This process is not necessarily a bad thing in that it allows new translations to be synthesized from rules extracted from training data; a strong target language model, such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses.", "labels": [], "entities": []}, {"text": "Overgeneration does complicate translation, however, in that many hypotheses are introduced only to be subsequently discarded.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9815736413002014}]}, {"text": "The situation is further complicated by search errors.", "labels": [], "entities": []}, {"text": "Any search procedure which relies on pruning during search is at risk of search errors and the risk is made worse if the grammars tend to introduce many similar scoring hypotheses.", "labels": [], "entities": []}, {"text": "In particular we have found that cube pruning is very prone to search errors, that is, the hypotheses produced by cube pruning are not the top scoring hypotheses which should be found under the Hiero grammar ().", "labels": [], "entities": [{"text": "Hiero grammar", "start_pos": 194, "end_pos": 207, "type": "DATASET", "confidence": 0.9170898199081421}]}, {"text": "These limitations are clearly related to each other.", "labels": [], "entities": []}, {"text": "Moreover, they become more problematic as the amount of parallel text grows.", "labels": [], "entities": []}, {"text": "As the number of rules in the grammar increases, the grammars become more expressive, but the ability to search them does not improve.", "labels": [], "entities": []}, {"text": "This leads to a widening gap between the expressive power of the grammar and the ability to search it to find good and diverse hypotheses.", "labels": [], "entities": []}, {"text": "In this article we describe the following two refinements to Hiero which are intended to address some of the limitations in its original formulation.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.7384335398674011}]}, {"text": "Lattice-based hierarchical translation We describe how the cube pruning procedure can be replaced by standard operations with Weighted Finite State Transducers (WFSTs) so that Hiero uses translation lattices rather than n-best lists in search.", "labels": [], "entities": [{"text": "Lattice-based hierarchical translation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6595942775408427}]}, {"text": "We find that keeping partial translation hypotheses in lattice form greatly reduces search errors.", "labels": [], "entities": []}, {"text": "In some instances it is possible to perform translation without any pruning at all so that search errors are completely eliminated.", "labels": [], "entities": [{"text": "translation", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9764760732650757}]}, {"text": "Consistent with the observation by, this leads to improvements in minimum error rate training.", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 66, "end_pos": 93, "type": "METRIC", "confidence": 0.7821831405162811}]}, {"text": "Furthermore, the direct generation of translation lattices can improve gains from subsequent language model and Minimum Bayes Risk (MBR) rescoring.", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR) rescoring", "start_pos": 112, "end_pos": 146, "type": "METRIC", "confidence": 0.8934657233101981}]}, {"text": "Shallow-n grammars and additional nonterminal categories Nonterminals can be incorporated into hierarchical translation rules for the purpose of tuning the size of the Hiero search space for individual language pairs.", "labels": [], "entities": []}, {"text": "Shallow-n grammars are described and shown to control the level of rule nesting, low-level rule catenation, and the minimum and maximum spans of individual translation rules.", "labels": [], "entities": [{"text": "rule nesting", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.7403012663125992}]}, {"text": "In translation experiments we find that a shallow-1 grammar (one level of rule nesting) is sufficiently expressive for Arabic-to-English translation, but that a shallow-3 grammar is required in Chinese-to-English translation to match the performance of a full Hiero system that allows arbitrary rule nesting.", "labels": [], "entities": [{"text": "rule nesting", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.74318066239357}]}, {"text": "These nonterminals are introduced to control the Hiero search space and do not require estimation from annotated-or parsed-parallel text, as can be required by translation systems based on linguistically motivated grammars.", "labels": [], "entities": []}, {"text": "We use this approach as the basis of a general approach to SMT modeling.", "labels": [], "entities": [{"text": "SMT modeling", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.991144984960556}]}, {"text": "To control overgeneration, we revisit the synchronous context-free grammar defined by hierarchical rules and take a shallow-1 grammar as a starting point.", "labels": [], "entities": []}, {"text": "We then increase the complexity of the rules until the desired translation quality is found.", "labels": [], "entities": []}, {"text": "With these refinements we find that hierarchical phrase-based translation can be efficiently carried outwith no (or minimal) search errors in large-data tasks and can achieve state-of-the-art translation performance.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.6920195370912552}]}, {"text": "There are many benefits to formulating Hiero translation in terms of WFSTs.", "labels": [], "entities": [{"text": "formulating Hiero translation", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.72252290447553}, {"text": "WFSTs", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8462098836898804}]}, {"text": "Following the manner in which, elucidate other machine translation models, we can use WFST operations to make the operations of the Hiero decoder very clear.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7388083636760712}, {"text": "Hiero decoder", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9545694291591644}]}, {"text": "The simplicity of the analysis makes it possible to focus on the underlying grammars and avoid the complexities of heuristic search procedures.", "labels": [], "entities": []}, {"text": "Once the decoder is formulated, implementation is mostly straightforward using standard WFST techniques developed for language processing.", "labels": [], "entities": [{"text": "WFST", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8273332118988037}]}, {"text": "What difficulties arise are due to using finite state techniques with grammars which are not themselves finite state.", "labels": [], "entities": []}, {"text": "We will show, however, that the basic operations which need to be performed, such as extracting sufficient statistics for minimum error rate training, can be done relatively easily and naturally.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now define the hierarchical grammars for the translation experiments which we describe next.", "labels": [], "entities": []}, {"text": "Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3.", "labels": [], "entities": []}, {"text": "These grammars do not incorporate any monotonicity constraints, that is K 1 = K 2 = 1.", "labels": [], "entities": []}, {"text": "Shallow-1, K 1 = 1, K 2 = 3 : hierarchical rules with one nonterminal can reorder a monotonic production of up to three target language phrases of level 0.", "labels": [], "entities": []}, {"text": "Shallow-1, K 1 = 1, K 2 = 3, vo : hierarchical rules with one nonterminal can reorder a monotonic catenation of up to three target language phrases of level 0, but only if one of the source terminals is tagged as a verb.", "labels": [], "entities": []}, {"text": "Shallow-2, K 1 = 2, K 2 = 3, vo : two levels of reordering with monotonic production of up to three target language phrases of level 1, but only if one of the source terminals is tagged as a verb.", "labels": [], "entities": []}, {"text": "In this section we report on hierarchical phrase-based translation experiments with WFSTs.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.5939984818299612}, {"text": "WFSTs", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.875408411026001}]}, {"text": "We focus mainly on the NIST Arabic-to-English and Chinese-to-English translation tasks; some results for other language pairs are summarized in Section 4.", "labels": [], "entities": [{"text": "NIST", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.8469705581665039}, {"text": "Chinese-to-English translation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.5956544727087021}]}, {"text": "For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08 (and MT09) Arabic Constrained Data track (\u223c150M words per language).", "labels": [], "entities": [{"text": "NIST", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9769622087478638}, {"text": "MT08", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.49759441614151}, {"text": "MT09) Arabic Constrained Data track", "start_pos": 92, "end_pos": 127, "type": "DATASET", "confidence": 0.8510809342066447}]}, {"text": "In addition to reporting results on the MT08 set itself, we make use of a development set mt02-05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form a validation set mt02-05-test.", "labels": [], "entities": [{"text": "MT08 set", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9184349477291107}, {"text": "NIST MT02 through MT05 evaluation sets", "start_pos": 149, "end_pos": 187, "type": "DATASET", "confidence": 0.8655782242616018}]}, {"text": "The mt02-05-tune set has 2,075 sentences.", "labels": [], "entities": [{"text": "mt02-05-tune set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9382177293300629}]}, {"text": "For Chinese-to-English translation we use all available parallel text for the GALE 2008 evaluation; 2 this is approximately 250M words per language.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6014768332242966}, {"text": "GALE 2008 evaluation", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.9033096234003702}]}, {"text": "We report translation results on the NIST MT08 set, a development set tune-nw, and a validation set test-nw.", "labels": [], "entities": [{"text": "NIST MT08 set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8827168742815653}]}, {"text": "These tuning and test sets contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT05.", "labels": [], "entities": [{"text": "GALE program", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.8620032668113708}, {"text": "MT02", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.9432440400123596}, {"text": "MT05", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.753616452217102}]}, {"text": "The tune-nw set has 1,755 sentences, and test-nw set is similar.", "labels": [], "entities": []}, {"text": "The parallel texts for both language pairs are aligned using MTTK.", "labels": [], "entities": [{"text": "MTTK", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.871207058429718}]}, {"text": "We extract hierarchical rules from the aligned parallel texts using the constraints developed by.", "labels": [], "entities": []}, {"text": "We further filter the extracted rules by count and pattern as described by.", "labels": [], "entities": [{"text": "count", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9820244312286377}]}, {"text": "The following features are extracted from the parallel data and used to assign scores to translation rules: source-to-target and targetto-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by.", "labels": [], "entities": []}, {"text": "We use two types of language model in translation.", "labels": [], "entities": []}, {"text": "In first-pass translation we use 4-gram language models estimated over the English side of the parallel text (for each language pair) and a 965 million word subset of monolingual data from the English Gigaword Third Edition (LDC2007T07).", "labels": [], "entities": [{"text": "English Gigaword Third Edition (LDC2007T07)", "start_pos": 193, "end_pos": 236, "type": "DATASET", "confidence": 0.8157688208988735}]}, {"text": "These are the language models used if pruning is needed during search.", "labels": [], "entities": []}, {"text": "The main language model is a zero-cutoff stupid-backoff ( 5-gram language model, estimated using 6.6B words of English text from the English Gigaword corpus.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 133, "end_pos": 156, "type": "DATASET", "confidence": 0.825722316900889}]}, {"text": "These language models are converted to WFSTs as needed; failure transitions are used for correct application of back-off weights.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8127161264419556}]}, {"text": "In tuning the systems, standard MERT (Och 2003) iterative parameter estimation under IBM BLEU is performed on the development sets.", "labels": [], "entities": [{"text": "MERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9677984714508057}, {"text": "Och 2003)", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.6691292027632395}, {"text": "IBM", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.4817533791065216}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9097678065299988}]}], "tableCaptions": [{"text": " Table 2  Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) after first-pass  decoding and subsequent rescoring steps. Decoding time reported for mt02-05-tune is in seconds  per word. Both systems are optimized using MERT over the k-best lists generated by HCP.", "labels": [], "entities": [{"text": "IBM BLEU", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.6523436009883881}, {"text": "MERT", "start_pos": 240, "end_pos": 244, "type": "METRIC", "confidence": 0.8719375729560852}]}, {"text": " Table 3  Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-pass  decoding and subsequent rescoring steps. The MERT k-best column indicates which decoder  generated the k-best lists used in MERT optimization. The mt08 set contains 691 sentences of  newswire and 666 sentences of Web text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.6386362314224243}, {"text": "MERT optimization", "start_pos": 226, "end_pos": 243, "type": "TASK", "confidence": 0.970348060131073}, {"text": "mt08 set", "start_pos": 249, "end_pos": 257, "type": "DATASET", "confidence": 0.8747219741344452}]}, {"text": " Table 4  Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) with various  grammar configurations. Decoding time reported in seconds per word for mt02-05-tune.", "labels": [], "entities": [{"text": "Contrastive Arabic-to-English translation", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.6864420572916666}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.6233211159706116}]}, {"text": " Table 5  Contrastive Chinese-to-English translation results (lower-cased IBM BLEU ) with various  grammar configurations and search parameters. Decoding time is reported in sec/word for  tune-nw.", "labels": [], "entities": [{"text": "Contrastive Chinese-to-English translation", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.6476593911647797}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.6368070244789124}]}, {"text": " Table 6  Arabic-to-English results (lower-cased IBM BLEU) when determinizing the lattice at the  upper-most CYK cell with alternative semirings.", "labels": [], "entities": [{"text": "IBM", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.4494406282901764}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7799190282821655}]}, {"text": " Table 7  Arabic-to-English results (lower-cased IBM BLEU) when using alternative Arabic  decompositions, and their combination with k-best-based and lattice-based MBR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7222439050674438}]}]}