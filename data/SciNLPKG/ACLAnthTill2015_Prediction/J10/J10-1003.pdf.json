{"title": [], "abstractContent": [{"text": "We present an approach to the automatic creation of extractive summaries of literary short stories.", "labels": [], "entities": [{"text": "automatic creation of extractive summaries of literary short stories", "start_pos": 30, "end_pos": 98, "type": "TASK", "confidence": 0.7447861797279782}]}, {"text": "The summaries are produced with a specific objective in mind: to help a reader decide whether she would be interested in reading the complete story.", "labels": [], "entities": []}, {"text": "To this end, the summaries give the user relevant information about the setting of the story without revealing its plot.", "labels": [], "entities": []}, {"text": "The system relies on assorted surface indicators about clauses in the short story, the most important of which are those related to the aspectual type of a clause and to the main entities in a story.", "labels": [], "entities": []}, {"text": "Fifteen judges evaluated the summaries on a number of extrinsic and intrinsic measures.", "labels": [], "entities": []}, {"text": "The outcome of this evaluation suggests that the summaries are helpful in achieving the original objective.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last decade, automatic text summarization has become a popular research topic with a curiously restricted scope of applications.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6915018955866495}]}, {"text": "A few innovative research directions have emerged, including headline generation (Soricut and Marcu 2007), summarization of books (, personalized summarization (, generation of tables-of-contents, summarization of speech (), dialogues (Zechner 2002), evaluative text, and biomedical documents.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8157512545585632}, {"text": "summarization of books", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.9024436473846436}, {"text": "summarization of speech", "start_pos": 197, "end_pos": 220, "type": "TASK", "confidence": 0.9078563849131266}]}, {"text": "In addition, more researchers have been venturing past purely extractive summarization).", "labels": [], "entities": []}, {"text": "By and large, however, most research in text summarization still revolves around texts characterized by rigid structure.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7454540431499481}]}, {"text": "The better explored among such texts are news articles (, medical documents (), legal documents, and papers in the area of computer science.", "labels": [], "entities": []}, {"text": "Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically.", "labels": [], "entities": [{"text": "summarizing these genres", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.8829873204231262}]}, {"text": "Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7135965824127197}]}, {"text": "We attempt to make a step in this direction by devising an approach to summarizing a relatively unexplored genre: literary short stories.", "labels": [], "entities": [{"text": "literary short stories", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.6243410408496857}]}, {"text": "Well-structured documents, such as news articles, exhibit a number of characteristics that help identify some of the important passages without performing in-depth semantic analysis.", "labels": [], "entities": []}, {"text": "These characteristics include predictable location of typical items in a document and in its well-delineated parts, cue words, and template-like structure that often characterizes a genre (e.g., scientific papers).", "labels": [], "entities": []}, {"text": "This is not the casein literature.", "labels": [], "entities": []}, {"text": "Quite the contrary-to write fiction in accordance with a template is a sure way to write poor prose.", "labels": [], "entities": []}, {"text": "One also cannot expect to find portions of text that summarize the main idea behind a story, and even less so to find them in the same location.", "labels": [], "entities": []}, {"text": "In addition, the variety of literary devices (the widespread use of metaphor and figurative language, leaving things unsaid and relying on the reader's skill of reading between the lines, frequent use of dialogue, etc.) makes summarizing fiction a very distinct task.", "labels": [], "entities": [{"text": "summarizing fiction", "start_pos": 226, "end_pos": 245, "type": "TASK", "confidence": 0.9620973467826843}]}, {"text": "It is a contribution of this work to demonstrate that summarizing short fiction is feasible using state-of-the-art tools in natural language technology.", "labels": [], "entities": [{"text": "summarizing short fiction", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.9374662637710571}]}, {"text": "In the case of our corpus, this is also done without deep semantic resources or knowledge bases, although such resources would be of great help.", "labels": [], "entities": []}, {"text": "We leverage syntactic information and shallow semantics (provided by a gazetteer) to produce indicative summaries of short stories that people find helpful and that outperform naive baselines and two state-of-the-art generic summarizers.", "labels": [], "entities": []}, {"text": "We have restricted the scope of this potentially vast project in several ways.", "labels": [], "entities": []}, {"text": "In the course of this work we concentrate on producing summaries of short stories suitable fora particular purpose: to help a reader form adequate expectations about the complete story and decide whether she would be interested in reading it.", "labels": [], "entities": []}, {"text": "To this end, the summary includes important elements of the setting of a story, such as the place and the main characters, presented as excerpts from the complete story.", "labels": [], "entities": []}, {"text": "The assumption behind this definition is this: If a reader knows when and where the story takes place and who its main characters are, she should be able to make informed decisions about it.", "labels": [], "entities": []}, {"text": "With such a definition of short story summaries, re-telling the plot in the summary is not among the objectives of this work; in fact, doing so is undesirable.", "labels": [], "entities": []}, {"text": "We have introduced this limitation for two reasons.", "labels": [], "entities": []}, {"text": "There is an \"ideological\" side of the decision: Not many people want to know what happens in a story before reading it, even if this may help them decide that the story is worth reading.", "labels": [], "entities": []}, {"text": "There also is a practical side, namely the complexity of the problem: Summarizing the plot would be considerably more difficult (see Section 2 fora review of related work).", "labels": [], "entities": []}, {"text": "We hope to tackle this issue in the future.", "labels": [], "entities": []}, {"text": "For now, creating indicative summaries of short stories is challenge enough.", "labels": [], "entities": [{"text": "summaries of short stories", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.8093362301588058}]}, {"text": "The summaries in Figures 1-3 illustrate our approach in the context of a naive lead baseline and a ceiling.", "labels": [], "entities": []}, {"text": "shows an example of an automatically produced summary that meets the aforementioned criteria.", "labels": [], "entities": []}, {"text": "A reader can see that the story is set in a restaurant where the customers are tended to by two waitresses: the fair Aileen who \"wins hearts\" and \"the-bag-o'-meal\" plain-faced Tildy.", "labels": [], "entities": [{"text": "Aileen", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.96319979429245}, {"text": "Tildy", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.7844019532203674}]}, {"text": "If the reader chooses to pursue the story, she will find the description of an accident of paramount importance to Tildy: One day she is kissed by a customer in public!", "labels": [], "entities": [{"text": "Tildy", "start_pos": 115, "end_pos": 120, "type": "TASK", "confidence": 0.5365017056465149}]}, {"text": "The event is more than flattering to usually under-appreciated Tildy.", "labels": [], "entities": [{"text": "Tildy", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9129959940910339}]}, {"text": "It causes a complete change in how she views herself.", "labels": [], "entities": []}, {"text": "The story then unfolds to reveal that the customer was drunk on the day in question and that he returned to apologize several days later.", "labels": [], "entities": []}, {"text": "This apology is a severe blow to Tildy and an abrupt end of many a dream that the incident had spurred in her head.", "labels": [], "entities": [{"text": "Tildy", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9157763123512268}]}, {"text": "The story ends with Aileen trying to comfort her crying friend by saying \"He ain't anything Example of a summary produced by the system. of a gentleman or he wouldn't ever of apologized.\"", "labels": [], "entities": [{"text": "Aileen", "start_pos": 20, "end_pos": 26, "type": "TASK", "confidence": 0.7470155358314514}]}, {"text": "Yet, the summary in does not reveal these facts.", "labels": [], "entities": []}, {"text": "For comparison, shows a summary obtained by taking the same number of sentences from the beginning of the story.", "labels": [], "entities": []}, {"text": "As the reader can see, such a trivial approach is not sufficient to create a useful summary.", "labels": [], "entities": []}, {"text": "shows a manually created \"ideal\" summary.", "labels": [], "entities": []}, {"text": "We experimented with a corpus of 47 stories from the 19th and early 20th century written by renowned writers, including O.", "labels": [], "entities": [{"text": "O.", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.8100122213363647}]}, {"text": "Henry, Jerome K. Jerome, Anton Chekhov, and Guy de Maupassant.", "labels": [], "entities": []}, {"text": "The stories, with the exception of a few fairy tales, are classical examples of short social fiction.", "labels": [], "entities": []}, {"text": "The corpus was collected from Project Gutenberg (www.gutenberg.org) and only contains stories in English.", "labels": [], "entities": []}, {"text": "The average length of a story is 3,333 tokens and the target compression rate expressed in the number of sentences is 94%.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.9456752240657806}]}, {"text": "In order to create summaries of short stories that satisfy our stated criteria (henceforth indicative summaries), the system searches each story for sentences that focus on important entities and relate the background of the story (as opposed to events).", "labels": [], "entities": []}, {"text": "Correspondingly, processing has two stages.", "labels": [], "entities": []}, {"text": "Initially, the summarizer identifies two types of important entities: main characters and locations.", "labels": [], "entities": []}, {"text": "This is achieved using a gazetteer, resolving anaphoric expressions and then identifying frequently mentioned", "labels": [], "entities": []}], "datasetContent": [{"text": "The final version of the summarizer proceeds as follows.", "labels": [], "entities": []}, {"text": "First of all, the stories are parsed with the Connexor parser and named entities are recognized using the GATE Gazetteer.", "labels": [], "entities": [{"text": "GATE Gazetteer", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.9701071977615356}]}, {"text": "Then the system resolves anaphoric references and identifies important characters and locations.", "labels": [], "entities": []}, {"text": "During the next stage, the summarizer splits all source sentences into clauses and creates coarse-and fine-grained representations for each clause.", "labels": [], "entities": []}, {"text": "A clause is modeled as a vector of character-, location-and verb-related features.", "labels": [], "entities": []}, {"text": "Finally, the system employs two alternative procedures to select summary-worthy sentences: manually designed rules and machine learning.", "labels": [], "entities": []}, {"text": "We performed a number of experiments to find out how successful our system is in creating summaries of short stories.", "labels": [], "entities": [{"text": "summaries of short stories", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.8943403363227844}]}, {"text": "The experimental corpus consisted of 47 short stories split into a training set of 27 stories and a test set of 20 stories.", "labels": [], "entities": []}, {"text": "The average length of a story in the corpus was 3,333 tokens, 244 sentences, or approximately 4.5 U.S.-lettersized pages.", "labels": [], "entities": []}, {"text": "The corpus contains stories written by 17 different authors.", "labels": [], "entities": []}, {"text": "It was split manually so that its training and test portions contained approximately an equal number of stories by the same writer.", "labels": [], "entities": []}, {"text": "The first author of this paper annotated each clause of every story for summary-worthiness and achieved the compression rate of 6%, counted in sentences.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 108, "end_pos": 124, "type": "METRIC", "confidence": 0.9863787293434143}]}, {"text": "This rate was the target compression rate in all further experiments.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9560870826244354}]}, {"text": "The training data set consisted of 10,525 clauses, 506 of which were annotated as summary-worthy and all others as not summary-worthy.", "labels": [], "entities": [{"text": "training data set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8018930951754252}]}, {"text": "The test data set contained 7,890 clauses, 406 of them summary-worthy.", "labels": [], "entities": [{"text": "test data set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8113827506701151}]}, {"text": "We fine-tuned the system and used the training portion of the data set to identify the best settings.", "labels": [], "entities": []}, {"text": "Then we ran two sets of experiments on the test portion.", "labels": [], "entities": []}, {"text": "In the first set of experiments, we applied a manually designed set of rules that select sentences for possible inclusion in summaries.", "labels": [], "entities": []}, {"text": "These experiments are described in Section 5.2.", "labels": [], "entities": []}, {"text": "The second set of experiments relied on using machine-learning techniques to create summaries.", "labels": [], "entities": []}, {"text": "It is described in Section 5.3.", "labels": [], "entities": [{"text": "Section 5.3", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.8295781910419464}]}, {"text": "After the completion of the experiments, the summaries were evaluated by six judges.", "labels": [], "entities": []}, {"text": "They were also compared against extractive summaries produced by three people.", "labels": [], "entities": []}, {"text": "Section 6 discusses the evaluation procedures in detail and reports the results.", "labels": [], "entities": []}, {"text": "The first classification procedure applies manually designed rules to a clause-level representation of the original stories to produce descriptive summaries.", "labels": [], "entities": []}, {"text": "The rules are designed using the same features as those used for machine learning and described in Section 4.3 and in Appendix A. The first author created two sets of rules to guide the sentence classification process: one for the coarse-grained and another for the fine-grained representation.", "labels": [], "entities": [{"text": "sentence classification process", "start_pos": 186, "end_pos": 217, "type": "TASK", "confidence": 0.775969018538793}]}, {"text": "The rules operate at clause level.", "labels": [], "entities": []}, {"text": "If a clause is deemed summary-worthy, the complete parent sentence is included in the summary.", "labels": [], "entities": []}, {"text": "displays a few examples of rules for the fine-grained data set (a clause is considered to be summary-worthy if a rule returns True).", "labels": [], "entities": []}, {"text": "The first rule attempts to select clauses that talk about one of the main characters and contain temporal expressions of type enactment.", "labels": [], "entities": []}, {"text": "The rationale for this rule is that such clauses are likely to describe habitual activities of protagonists (e.g., He always smoked.)", "labels": [], "entities": []}, {"text": "The second rule follows the same rationale but the stativity of the situation is signaled by the main stative verb.", "labels": [], "entities": []}, {"text": "The third rule rejects clauses in progressive tense because such clauses are unlikely to contain background information.", "labels": [], "entities": []}, {"text": "The set of rules for the fine-grained representation has a tree-like structure.", "labels": [], "entities": []}, {"text": "It processes the features of a clause and outputs a binary prediction.", "labels": [], "entities": []}, {"text": "The rules for the coarse-grained representation function differently.", "labels": [], "entities": []}, {"text": "Each clause is assigned a score based on the values of its features.", "labels": [], "entities": []}, {"text": "The system then selects 6% of sentences that contain clauses with the highest scores.", "labels": [], "entities": []}, {"text": "The scores attributed to the particular feature values were assigned and fine-tuned manually using linguistic knowledge described in Section 4.3.", "labels": [], "entities": []}, {"text": "The reasons why the procedures for the two data sets differ are as follows.", "labels": [], "entities": []}, {"text": "Assigning and fine-tuning the scores is a more flexible process and it is easier to perform manually.", "labels": [], "entities": [{"text": "Assigning", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9720815420150757}]}, {"text": "Ideally, we would apply score-based rules to both representations, but assigning and fine-tuning the scores manually for the fine-grained data set is excessively laborintensive: there are too many features with too many values.", "labels": [], "entities": []}, {"text": "For instance, one may want to reward clauses in simple pastor present tenses, reflecting the fact that such clauses are more likely to be descriptive than those in perfect or progressive tenses.", "labels": [], "entities": []}, {"text": "This information is expressed in the coarse-grained data set using one binary feature simple past present and fine-tuning the score is trivial.", "labels": [], "entities": []}, {"text": "On the other hand, the same information in the fine-grained data set is distributed over three features with a total of seven values: is perf (yes, no), is progressive (yes, no), and tense (past, present. future).", "labels": [], "entities": []}, {"text": "Distributing the \"reward\" among three independent features is far less obvious.", "labels": [], "entities": []}, {"text": "The rules in both data sets, as well as the set of weights used for the coarse-grained representation, were selected and fine-tuned empirically using the training portion of the corpus as a guide.", "labels": [], "entities": []}, {"text": "Once the parameters had been adjusted, the system produced two sets of summaries for the test portion of the corpus (one for each representation).", "labels": [], "entities": []}, {"text": "The detailed algorithms for both data sets are too long for inclusion in this article.", "labels": [], "entities": []}, {"text": "show the rationale for the algorithms.", "labels": [], "entities": []}, {"text": "The interested reader is referred to for pseudo-code.", "labels": [], "entities": []}, {"text": "As an alternative to rule construction, in the second set of experiments we performed decision tree induction with C5.0 (Quinlan 1992) to select salient descriptive sentences.", "labels": [], "entities": [{"text": "rule construction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7741610109806061}, {"text": "decision tree induction", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.7175024151802063}]}, {"text": "C5.0 was our choice mainly because of the readability of its output.", "labels": [], "entities": []}, {"text": "The training and test data sets exhibited an almost 1:17 class imbalance (i.e., only 6% of all annotated clauses belonged to the positive class).", "labels": [], "entities": []}, {"text": "Because the corpus was rather small, we applied a number of techniques to correct class imbalance in the training data set.", "labels": [], "entities": []}, {"text": "These techniques included classification costs, undersampling (randomly removing instances of the majority class), oversampling (randomly duplicating instances of the minority class), and synthetic example generation (.", "labels": [], "entities": [{"text": "synthetic example generation", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.6831707954406738}]}, {"text": "Using tenfold cross-validation on the training data set and original annotations by the first author, High-level overview of the rules for the coarse-grained data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.8067140777905782}]}, {"text": "we selected the best class-imbalance correction techniques for each representation and also fine-tuned the learning parameters available in C5.0.", "labels": [], "entities": []}, {"text": "These experiments brought the best results when using classification costs for the coarse-grained data set and undersampling for the fine-grained data set.", "labels": [], "entities": []}, {"text": "In order to see what features were most informative in each data set, we conducted a small experiment.", "labels": [], "entities": []}, {"text": "We removed one feature at a time from the training set and used the decrease in F-score as a measure of informativeness.", "labels": [], "entities": [{"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9982268214225769}]}, {"text": "The experiment showed that in the coarse-grained data set the following features were the most informative: the presence of a character in a clause, the difference between the index of the current sentence and the sentence where the character was first mentioned, syntactic function of a character mention, index of the sentence, and tense.", "labels": [], "entities": []}, {"text": "In the fine-grained data set the findings are similar: the index of the sentence, whether a character mention is a subject, the presence of a character mention in the clause, and whether the character mention is a pronoun are more important than the other features.", "labels": [], "entities": []}, {"text": "After selecting the best parameters on the training data set using tenfold crossvalidation, the system produced two sets of summaries for the test data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Results of anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8119421601295471}]}, {"text": " Table 2  Description of the features in both data sets.", "labels": [], "entities": []}, {"text": " Table 4  Answers to factual questions.", "labels": [], "entities": []}, {"text": " Table 7  Sentence co-selection between computer-and human-made summaries. Majority gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.706586092710495}]}, {"text": " Table 8  Sentence co-selection between computer-and human-made summaries. Union gold standard.", "labels": [], "entities": [{"text": "Union gold standard", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.7535306215286255}]}, {"text": " Table 9  Sentence co-selection between computer-and human-made summaries. Intersection gold  standard.", "labels": [], "entities": [{"text": "Intersection gold  standard", "start_pos": 75, "end_pos": 102, "type": "METRIC", "confidence": 0.9130013386408488}]}, {"text": " Table 10  ROUGE-2 recall scores.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.997267484664917}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9097960591316223}]}, {"text": " Table 11  ROUGE-SU4 recall scores.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9951424598693848}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8898782134056091}]}]}