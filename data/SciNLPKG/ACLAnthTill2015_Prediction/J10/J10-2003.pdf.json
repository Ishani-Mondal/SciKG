{"title": [{"text": "What Is Not in the Bag of Words for Why-QA?", "labels": [], "entities": []}], "abstractContent": [{"text": "While developing an approach to why-QA, we extended a passage retrieval system that uses off-the-shelf retrieval technology with a re-ranking step incorporating structural information.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.8582067787647247}]}, {"text": "We get significantly higher scores in terms of MRR@150 (from 0.25 to 0.34) and success@10.", "labels": [], "entities": [{"text": "MRR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9905331134796143}, {"text": "success", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9895538091659546}]}, {"text": "The 23% improvement that we reach in terms of MRR is comparable to the improvement reached on different QA tasks by other researchers in the field, although our re-ranking approach is based on relatively lightweight overlap measures incorporating syntactic constituents, cue words, and document structure.", "labels": [], "entities": [{"text": "MRR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.6597024202346802}]}], "introductionContent": [{"text": "About 5% of all questions asked to QA systems are why-questions.", "labels": [], "entities": []}, {"text": "Why-questions need a different approach than factoid questions, because their answers are explanations that usually cannot be stated in a single phrase.", "labels": [], "entities": []}, {"text": "Recently, research has been directed at QA for why-questions (why-QA).", "labels": [], "entities": []}, {"text": "In earlier work on answering why-questions on the basis of Wikipedia, we found that the answers to most why-questions are passages of text that are at least one sentence and at most one paragraph in length ().", "labels": [], "entities": []}, {"text": "Therefore, we aim at developing a system that takes as input a whyquestion and gives as output a ranked list of candidate answer passages.", "labels": [], "entities": []}, {"text": "In the current article, we propose a three-step setup fora why-QA system: (1) a question-processing module that transforms the input question to a query; (2) an offthe-shelf retrieval module that retrieves and ranks passages of text that share content with the input query; and (3) a re-ranking module that adapts the scores of the retrieved passages using structural information from the input question and the retrieved passages.", "labels": [], "entities": []}, {"text": "In the first part of this article, we focus on step 2, namely, passage retrieval.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9311186671257019}]}, {"text": "The classic approach to finding passages in a text collection that share content with an input query is retrieval using a bag-of-words (BOW) model (.", "labels": [], "entities": []}, {"text": "BOW models are based on the assumption that text can be represented as an unordered collection of words, disregarding grammatical structure.", "labels": [], "entities": []}, {"text": "Most BOW-based models use statistical weights based on term frequency, document frequency, passage length, and term density (.", "labels": [], "entities": []}, {"text": "Because BOW approaches disregard grammatical structure, systems that rely on a BOW model have their limitations in solving problems where the syntactic relation between words or word groups is crucial.", "labels": [], "entities": []}, {"text": "The importance of syntax for QA is sometimes illustrated by the sentence Ruby killed Oswald, which is not an answer to the question Who did Oswald kill?).", "labels": [], "entities": []}, {"text": "Therefore, a number of researchers in the field investigated the use of structural information on top of a BOW approach for answer retrieval and ranking.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.8981211483478546}]}, {"text": "These studies show that although the BOW model makes the largest contribution to the QA system results, adding structural (syntactic information) can give a significant improvement.", "labels": [], "entities": [{"text": "BOW", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.911915123462677}]}, {"text": "In the current article, we hypothesize that for the relatively complex problem of why-QA, a significant improvement-at least comparable to the improvement gained for factoid QA-can be gained from the addition of structural information to the ranking component of the QA system.", "labels": [], "entities": []}, {"text": "We first evaluate a passage retrieval system for why-QA based on standard BOW ranking (step 1 and 2 in our set-up).", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8629553616046906}, {"text": "BOW", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.7819027304649353}]}, {"text": "Then we perform an analysis of the strengths and weaknesses of the BOW model for retrieving and ranking candidate answers.", "labels": [], "entities": [{"text": "BOW", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.4706099331378937}]}, {"text": "In view of the observed weaknesses of the BOW model, we choose our feature set to be applied to the set of candidate answer passages in the re-ranking module (step 3 in our set-up).", "labels": [], "entities": []}, {"text": "The structural features that we propose are based on the idea that some parts of the question and the answer passage are more important for relevance ranking than other parts.", "labels": [], "entities": [{"text": "relevance ranking", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.8536536693572998}]}, {"text": "Therefore, our re-ranking features are overlap-based: They tell us which parts of a why-question and its candidate answers are the most salient for ranking the answers.", "labels": [], "entities": []}, {"text": "We evaluate our initial and adapted ranking strategies using a set of why-questions and a corpus of Wikipedia documents, and we analyze the contribution of both the BOW model and the structural features.", "labels": [], "entities": [{"text": "BOW", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.49091172218322754}]}, {"text": "The main contributions of this article are: (1) we address the relatively new problem of why-QA and (2) we analyze the contribution of overlap-based structural information to the problem of answer ranking.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 190, "end_pos": 204, "type": "TASK", "confidence": 0.9301739633083344}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, related work is discussed.", "labels": [], "entities": []}, {"text": "Section 3 presents the BOW-based passage retrieval method for why-QA, followed by a discussion of the strengths and weaknesses of the approach in Section 4.", "labels": [], "entities": [{"text": "BOW-based passage retrieval", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.5919178128242493}]}, {"text": "In Section 5, we extend our system with a re-ranking component based on structural overlap features.", "labels": [], "entities": []}, {"text": "A discussion of the results and our conclusions are presented in Sections 6 and 7, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we use the Wikipedia INEX corpus).", "labels": [], "entities": [{"text": "Wikipedia INEX corpus", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.8884889682133993}]}, {"text": "This corpus consists of all 659,388 articles from the online Wikipedia in the summer of 2006 in XML format.", "labels": [], "entities": []}, {"text": "For development and testing purposes, we exploit the Webclopedia question set, which contains questions asked to the online QA system answers.com.", "labels": [], "entities": [{"text": "Webclopedia question set", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.9738936622937521}]}, {"text": "Of these questions, 805 (5% of the total set) are why-questions.", "labels": [], "entities": []}, {"text": "For 700 randomly selected why-questions, we manually searched for an answer in the Wikipedia XML corpus, saving the remaining 105 questions for future testing purposes.", "labels": [], "entities": [{"text": "Wikipedia XML corpus", "start_pos": 83, "end_pos": 103, "type": "DATASET", "confidence": 0.9390191833178202}]}, {"text": "186 of these 700 questions have an answer in the corpus.", "labels": [], "entities": []}, {"text": "3 Extraction of one relevant answer for each of these questions resulted in a set of 186 why-questions and their reference answers.", "labels": [], "entities": []}, {"text": "Two examples illustrate the type of data we are working with: 1.", "labels": [], "entities": []}, {"text": "\"Why didn't Socrates leave Athens after he was convicted?\"", "labels": [], "entities": []}, {"text": "-\"Socrates considered it hypocrisy to escape the prison: he had knowingly agreed to live under the city's laws, and this meant the possibility of being judged guilty of crimes by a large jury.\"", "labels": [], "entities": []}, {"text": "2. \"Why do most cereals crackle when you add milk?\"", "labels": [], "entities": []}, {"text": "-\"They are made of a sugary rice mixture which is shaped into the form of rice kernels and toasted.", "labels": [], "entities": []}, {"text": "These kernels bubble and rise in a manner which forms very thin walls.", "labels": [], "entities": []}, {"text": "When the cereal is exposed to milk or juices, these walls tend to collapse suddenly, creating the famous 'Snap, crackle and pop' sounds.\"", "labels": [], "entities": []}, {"text": "To be able to do fast evaluation without elaborate manual assessments, we manually created one answer pattern for each of the questions in our set.", "labels": [], "entities": []}, {"text": "The answer pattern is a regular expression that defines which of the retrieved passages are considered a relevant answer to the input question.", "labels": [], "entities": []}, {"text": "The first version of the answer patterns was directly based on the corresponding reference answer, but in the course of the development and evaluation process, we extended the patterns in order to cover as many as possible of the Wikipedia passages that contain an answer.", "labels": [], "entities": []}, {"text": "In fact, answer judgment is a complex task due to the presence of multiple answer variants in the corpus.", "labels": [], "entities": [{"text": "answer judgment", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.9846557378768921}]}, {"text": "It is a time-consuming process because of the large number of candidate answers that need to be judged when long lists of answers are retrieved per question.", "labels": [], "entities": []}, {"text": "In future work, we will comeback to the assessment of relevant and irrelevant answers.", "labels": [], "entities": []}, {"text": "After applying our answer patterns to the passages retrieved, we count the questions that have at least one relevant answer in the top n results.", "labels": [], "entities": []}, {"text": "This number divided by the total number of questions in a test set gives the measure success@n.", "labels": [], "entities": []}, {"text": "In Section 3.2, we explain the levels for n that we use for evaluation.", "labels": [], "entities": []}, {"text": "For the highest ranked relevant answer per question, we determine the RR.", "labels": [], "entities": [{"text": "RR", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9957356452941895}]}, {"text": "Questions for which the system did not retrieve an answer in the list of 150 results get an RR of 0.", "labels": [], "entities": [{"text": "RR", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9990455508232117}]}, {"text": "Over all questions, we calculate the mean reciprocal rank MRR.", "labels": [], "entities": [{"text": "mean reciprocal rank MRR", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7039616256952286}]}], "tableCaptions": [{"text": " Table 2  Features that significantly contribute to the re-ranking score (p < 0.05), ranked by their  coefficient in the logistic regression model (representing their importance).", "labels": [], "entities": [{"text": "re-ranking score", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.8689960837364197}]}]}