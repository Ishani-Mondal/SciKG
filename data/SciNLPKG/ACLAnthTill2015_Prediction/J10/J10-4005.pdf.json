{"title": [], "abstractContent": [{"text": "We propose a novel string-to-dependency algorithm for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.7555835445721945}]}, {"text": "This algorithm employs a target dependency language model during decoding to exploit long distance word relations, which cannot be modeled with a traditional n-gram language model.", "labels": [], "entities": []}, {"text": "Experiments show that the algorithm achieves significant improvement in MT performance over a state-of-the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9936115145683289}, {"text": "NIST MT06", "start_pos": 151, "end_pos": 160, "type": "DATASET", "confidence": 0.7855739891529083}, {"text": "MT08 newswire evaluation sets", "start_pos": 165, "end_pos": 194, "type": "DATASET", "confidence": 0.8611566424369812}]}], "introductionContent": [{"text": "n-gram Language Models (LMs) have been widely used in current Statistical Machine Translation (SMT) systems.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.8017387390136719}]}, {"text": "Because they treat a sentence as a flat string of tokens, a drawback of traditional n-gram LMs is that they cannot model long range word relations, such as predicate-argument attachments, that are critical to translation quality.", "labels": [], "entities": []}, {"text": "We propose a hierarchical string-to-dependency translation model that exploits a dependency LM while decoding (as opposed to during reranking n-best output) to score alternative translations based on their structural soundness.", "labels": [], "entities": []}, {"text": "In order to generate the structured output (dependency trees) required for dependency LM scoring, translation rules in our system represent the target side as dependency structures.", "labels": [], "entities": [{"text": "dependency LM scoring", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7416905363400778}]}, {"text": "We restrict the target side of the rules to well-formed dependency structures to weed out bad translation rules and enable efficient decoding through dynamic programming.", "labels": [], "entities": []}, {"text": "Due to the flexibility of well-formed dependency structures, such structures can cover a large set of non-constituent transfer rules () that have been shown useful for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 168, "end_pos": 170, "type": "TASK", "confidence": 0.9962109327316284}]}, {"text": "For comparison purposes, as our baseline, we replicated the Hiero decoder), a state-of-the-art hierarchical string-to-string model.", "labels": [], "entities": [{"text": "Hiero decoder)", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9409395257631937}]}, {"text": "Our experiments show that the string-to-dependency decoder significantly improves MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9934338331222534}]}], "datasetContent": [{"text": "We experimented with four models: r baseline: hierarchical string to string translation, using our own replication of the Hiero system r filtered: like the baseline, it uses string to string rules, except that rules whose target side does not correspond to a well-formed structure in rule extraction are excluded.", "labels": [], "entities": [{"text": "hierarchical string to string translation", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.694758927822113}, {"text": "rule extraction", "start_pos": 284, "end_pos": 299, "type": "TASK", "confidence": 0.7553668320178986}]}, {"text": "No dependency LM is used in decoding r str-dep: string-to-dependency system.", "labels": [], "entities": []}, {"text": "It uses rules with target dependency structures and a dependency LM in decoding r labeled: an enhanced str-dep model with POS tags as labels We use the Hiero model as our baseline because it is the closest to our string-todependency model.", "labels": [], "entities": []}, {"text": "They use similar rule extraction and decoding algorithms.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7529730200767517}]}, {"text": "The major difference is in the representation of target structures.", "labels": [], "entities": []}, {"text": "We use dependency structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding.", "labels": [], "entities": []}, {"text": "All models were tuned on BLEU, and evaluated on BLEU, TER (), and METEOR (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9978502988815308}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9985209107398987}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9978413581848145}, {"text": "METEOR", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9919324517250061}]}, {"text": "It is well known that all automatic scores are crude approximations of translation quality.", "labels": [], "entities": []}, {"text": "It is not uncommon fora technique to improve the metric that is used for tuning but hurt other metrics.", "labels": [], "entities": []}, {"text": "The use of multiple metrics helps us avoid drawing false conclusions based on metric-specific improvements.", "labels": [], "entities": []}, {"text": "For both Arabic-to-English and Chinese-to-English MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.896435558795929}, {"text": "NIST", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9790624380111694}, {"text": "MT02-05", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.5001329183578491}, {"text": "MT06", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9451534152030945}, {"text": "MT08 newswire sets", "start_pos": 102, "end_pos": 120, "type": "DATASET", "confidence": 0.9284659226735433}]}, {"text": "The training data for Arabic-to-English MT contains around 1.9 million pairs of bi-lingual sentences from ten corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC-2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, and SSUSAC27 (Sakhr Arabic-English Parallel Corpus).", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.8516582250595093}]}, {"text": "The training data for Chinese-toEnglish MT contains around 1.0 million pairs of bi-lingual sentences from eight corpora: LDC2002E18, LDC2005T06, LDC2005T10, LDC2006E26, LDC2006G05, LDC2002L27, LDC2005T34, and LDC2003E07.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.6560667753219604}]}, {"text": "The dependency LMs were trained on the same parallel training data.", "labels": [], "entities": []}, {"text": "For that purpose, we parsed the English side of the parallel data.", "labels": [], "entities": [{"text": "parsed", "start_pos": 21, "end_pos": 27, "type": "TASK", "confidence": 0.9685497879981995}]}, {"text": "Two separate models were trained: one for Arabic from the Arabic training data and the other for Chinese from the Chinese training data.", "labels": [], "entities": []}, {"text": "Traditional tri-gram and 5-gram string LMs were trained on the English side of the parallel data as well as the English Gigaword corpus V3.0 in away described by. shows the number of transfer rules extracted from the training data for the tuning and test sets.", "labels": [], "entities": [{"text": "English Gigaword corpus V3.0", "start_pos": 112, "end_pos": 140, "type": "DATASET", "confidence": 0.773569256067276}]}, {"text": "The constraint of well-formed dependency structures greatly reduced the size of the rule set.", "labels": [], "entities": []}, {"text": "Although the rule size increased a little bit after incorporating dependency structures and labels in rules, the size of string-to-dependency rule set is about 10% to 20% of the baseline.", "labels": [], "entities": []}, {"text": "show the BLEU, TER, and METEOR scores on MT06 and MT08 for Arabic-to-English MT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9994329810142517}, {"text": "TER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9959879517555237}, {"text": "METEOR", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9958920478820801}, {"text": "MT06", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9533281326293945}, {"text": "MT08", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9244369268417358}, {"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.8542774319648743}]}, {"text": "show the scores for Chinese-to-English MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.7423115372657776}]}, {"text": "For system comparison, we primarily rely on the lower-cased BLEU score of the decoding output because it is the metric on which all systems were tuned.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9789863526821136}]}, {"text": "We measured the significance of BLEU, TER, and METEOR with paired bootstrap resampling as proposed by.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9992189407348633}, {"text": "TER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.996485710144043}, {"text": "METEOR", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9938194155693054}]}, {"text": "In, (+/-) represent being better/worse than the baseline at 95% confidence level, respectively, and (*) represents insignificant difference from the baseline.", "labels": [], "entities": []}, {"text": "For Arabic-to-English MT, the str-dep model decoder improved BLEU by 1.3 on MT06 and 1.2 on MT08 before 5-gram rescoring.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.8756619691848755}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9995213747024536}, {"text": "MT06", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9327659606933594}, {"text": "MT08", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9175289869308472}]}, {"text": "For Chinese-to-English MT, the improvements in BLEU were 1.0 on MT06 and 1.4 on MT08.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.7925911545753479}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9988284707069397}, {"text": "MT06", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.8889256119728088}, {"text": "MT08", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.920314610004425}]}, {"text": "After rescoring, the improvements became smaller, ranging from 0.8 to 1.3.", "labels": [], "entities": []}, {"text": "All the BLEU improvements on 5-gram scores are statistically significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9976084232330322}]}, {"text": "The use of POS labels in transfer rules further improves the BLEU score by about 0.7 points on average.", "labels": [], "entities": [{"text": "POS", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.8860861659049988}, {"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9811252653598785}]}, {"text": "The overall BLEU improvement on lower-cased decoding output  is 1.8 points on MT06 and 2.1 points on MT08 for Arabic-to-English translation, and 2.0 points on MT06 and 1.6 points on MT08 for Chinese-to-English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9993878602981567}, {"text": "MT06", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9085764288902283}, {"text": "MT08", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.9248633980751038}, {"text": "MT06", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.9142691493034363}, {"text": "MT08", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.9374184608459473}, {"text": "Chinese-to-English translation", "start_pos": 191, "end_pos": 221, "type": "TASK", "confidence": 0.6877108663320541}]}, {"text": "METEOR scores became significantly better for all conditions.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8933830857276917}]}, {"text": "TER improved significantly for Arabic-to-English but marginally on Chinese-to-English tasks.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8560487031936646}]}, {"text": "The results on METEOR and TER suggested that the new model did improve translation accuracy.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.7401188015937805}, {"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9634842872619629}, {"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9628490805625916}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8358357548713684}]}, {"text": "The filtered string-to-string rules can be viewed as the string projection of stringto-dependency rules.", "labels": [], "entities": []}, {"text": "It shows the performance of using dependency structure for rule filtering only.", "labels": [], "entities": [{"text": "rule filtering", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.7637408375740051}]}, {"text": "The results are very interesting.", "labels": [], "entities": []}, {"text": "On Arabic-to-English, the filtered model was significantly worse, which means that many useful rules were lost due to the structural constraints.", "labels": [], "entities": []}, {"text": "On Chinese-to-English, the tri-gram scores of the filtered model were a little bit worse.", "labels": [], "entities": []}, {"text": "However, after 5-gram rescoring, the BLEU scores became higher than the baseline, and METEOR scores were even significantly better.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9996073842048645}, {"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9933390021324158}]}, {"text": "We suspect that the different performance that we observed is due to the difference in source languages and their tokenization methods.", "labels": [], "entities": []}, {"text": "In any case, the purpose of the filtered model is not to propose the use of structural constraints for rule filtering, although it greatly reduced the rule size and allowed the use of more useful training data potentially.", "labels": [], "entities": [{"text": "rule filtering", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.8968209028244019}]}, {"text": "The use of structural constraints is compulsory for the introduction of dependency LMs and non-terminal labels, which compensated for the loss of rule filtering, and led to significant overall improvement., showed that, for the purpose of representing word relations, dependency structures are advantageous over CFG structures because they do not require complete constituents.", "labels": [], "entities": []}, {"text": "A number of techniques have been proposed to improve rule coverage. and introduced artificial constituent nodes dominating the phrase of interest.", "labels": [], "entities": [{"text": "rule coverage.", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.8480360209941864}]}, {"text": "The binarization method used by can cover many non-constituent rules also, but not all of them.", "labels": [], "entities": []}, {"text": "showed that the best results were obtained by combining these methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Number of transfer rules.", "labels": [], "entities": []}, {"text": " Table 2  BLEU, TER, and METEOR percentage scores on MT06 Arabic-to-English newswire set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9996745586395264}, {"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9981632828712463}, {"text": "METEOR percentage", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.9701299071311951}, {"text": "MT06 Arabic-to-English newswire set", "start_pos": 53, "end_pos": 88, "type": "DATASET", "confidence": 0.913693368434906}]}, {"text": " Table 3  BLEU, TER, and METEOR percentage scores on MT08 Arabic-to-English newswire set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9996844530105591}, {"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9981619715690613}, {"text": "METEOR percentage", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.9697623550891876}, {"text": "MT08 Arabic-to-English newswire set", "start_pos": 53, "end_pos": 88, "type": "DATASET", "confidence": 0.9121735543012619}]}, {"text": " Table 4  BLEU, TER, and METEOR percentage scores on MT06 Chinese-to-English newswire set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999629020690918}, {"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9981841444969177}, {"text": "METEOR percentage", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.9701540470123291}, {"text": "MT06 Chinese-to-English newswire set", "start_pos": 53, "end_pos": 89, "type": "DATASET", "confidence": 0.9311171323060989}]}, {"text": " Table 5  BLEU, TER, and METEOR percentage scores on MT08 Chinese-to-English newswire set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9996360540390015}, {"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9982407093048096}, {"text": "METEOR percentage", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.9703151285648346}, {"text": "MT08 Chinese-to-English newswire set", "start_pos": 53, "end_pos": 89, "type": "DATASET", "confidence": 0.9296136349439621}]}]}