{"title": [], "abstractContent": [{"text": "When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately.", "labels": [], "entities": []}, {"text": "We refer to this task as disentanglement.", "labels": [], "entities": [{"text": "disentanglement", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.9722327589988708}]}, {"text": "We present a corpus of Internet Relay Chat dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability.", "labels": [], "entities": []}, {"text": "We propose a graph-based clustering model for disentanglement, using lexical, timing, and discourse-based features.", "labels": [], "entities": []}, {"text": "The model's predicted disentanglements are highly correlated with manual annotations.", "labels": [], "entities": []}, {"text": "We conclude by discussing two extensions to the model, specificity tuning and conversation start detection, both of which are promising but do not currently yield practical improvements.", "labels": [], "entities": [{"text": "specificity tuning", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.9665118455886841}, {"text": "conversation start detection", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.8409799536069235}]}], "introductionContent": [], "datasetContent": [{"text": "We annotate the 800-line test transcript using our system.", "labels": [], "entities": []}, {"text": "The annotation obtained has 62 conversations, with mean length 12.90.", "labels": [], "entities": []}, {"text": "The average density of conversations is 2.86, and the entropy is 3.72.", "labels": [], "entities": []}, {"text": "This places it within the bounds of our human annotations (see), toward the more general end of the spectrum.", "labels": [], "entities": []}, {"text": "As a standard of comparison for our system, we provide results for several baselines-trivial systems which any useful annotation should outperform.", "labels": [], "entities": []}, {"text": "All different Each utterance is a separate conversation.", "labels": [], "entities": []}, {"text": "All same The whole transcript is a single conversation.", "labels": [], "entities": []}, {"text": "Blocks of k Each consecutive group of k utterances is a conversation.", "labels": [], "entities": []}, {"text": "Pause of k Each pause of k seconds or more separates two conversations.", "labels": [], "entities": [{"text": "Pause of k Each pause of k seconds", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.8611691445112228}]}, {"text": "Speaker Each speaker's utterances are treated as a monologue.", "labels": [], "entities": []}, {"text": "For each particular metric, we calculate the best baseline result among all of these.", "labels": [], "entities": []}, {"text": "To find the best block size or pause length, we search over multiples of five between 5 and 300.", "labels": [], "entities": []}, {"text": "This makes these baselines appear better than they really are, because their performance is optimized with respect to the test data.", "labels": [], "entities": []}, {"text": "baseline results is shown in.)", "labels": [], "entities": []}, {"text": "We also calculate results for two more systems.", "labels": [], "entities": []}, {"text": "One is a non-trivial baseline: Time/mention Our system, using only time gap and mention-based features.", "labels": [], "entities": []}, {"text": "The other is an oracle, designed to test how well a segmentation system designed for meeting or lecture data might possibly do on this task.", "labels": [], "entities": []}, {"text": "If no conversation were ever interrupted, such a system would be perfect (up to the limit of annotator agreement).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Statistics on 6 annotations of 800 utterances of chat transcript. Inter-annotator agreement metrics  (below the line) are calculated between distinct pairs of annotations.", "labels": [], "entities": []}, {"text": " Table 3  Metric values between proposed annotations and human annotations. Model scores typically fall  between inter-annotator agreement and baseline performance.", "labels": [], "entities": []}, {"text": " Table 4  Results reported by others on the same task.", "labels": [], "entities": []}, {"text": " Table 5  Metric values between proposed annotations and human annotations on test data. The tuned  model (evaluated at the entropy of the human annotations) improves on one-to-one accuracy but  not on loc 3 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9910030364990234}]}, {"text": " Table 6  Metric values using an oracle new-conversation detector on test data.", "labels": [], "entities": []}, {"text": " Table 7  Precision, recall, and F-score of the new conversation class on test data (average 81  conversations).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9972291588783264}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9980456829071045}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9996808767318726}]}]}