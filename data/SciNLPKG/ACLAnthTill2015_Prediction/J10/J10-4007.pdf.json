{"title": [{"text": "A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a vector space-based model for selectional preferences that predicts plausibility scores for argument headwords.", "labels": [], "entities": []}, {"text": "It does not require any lexical resources (such as WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9652949571609497}]}, {"text": "It can be trained either on one corpus with syntactic annotation, or on a combination of a small semantically annotated primary corpus and a large, syntactically analyzed generalization corpus.", "labels": [], "entities": []}, {"text": "Our model is able to predict inverse selectional preferences, that is, plausibility scores for predicates given argument heads.", "labels": [], "entities": []}, {"text": "We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task (prediction of human plausibility judgments), gauging the influence of different parameters and comparing our model against other model classes.", "labels": [], "entities": [{"text": "prediction of human plausibility judgments)", "start_pos": 86, "end_pos": 129, "type": "TASK", "confidence": 0.7840025921662649}]}, {"text": "We obtain consistent benefits from using the disambiguation and semantic role information provided by a semantically tagged primary corpus.", "labels": [], "entities": []}, {"text": "As for parameters, we identify settings that yield good performance across a range of experimental conditions.", "labels": [], "entities": []}, {"text": "However, frequency remains a major influence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items.", "labels": [], "entities": [{"text": "prediction", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.9606959819793701}]}], "introductionContent": [{"text": "Selectional preferences or selectional constraints describe knowledge about possible and plausible fillers fora predicate's argument positions.", "labels": [], "entities": []}, {"text": "They model the fact that there is often a semantically coherent set of concepts that can fill a given argument position.", "labels": [], "entities": []}, {"text": "Selectional preferences can help for many text analysis tasks which involve comparing different attachment decisions.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.8295641243457794}]}, {"text": "Examples include syntactic disambiguation), word sense disambiguation (WSD,, semantic role labeling (SRL,, and characterizing the conditions under which entailment holds between two predicates).", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.7180157154798508}, {"text": "word sense disambiguation (WSD", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6512407779693603}, {"text": "semantic role labeling (SRL", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.6685195147991181}]}, {"text": "Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate-argument combinations, for example in compositionality assessment or the detection of diathesis alternations.", "labels": [], "entities": [{"text": "compositionality assessment", "start_pos": 158, "end_pos": 185, "type": "TASK", "confidence": 0.9324603378772736}]}, {"text": "In psycholinguistics, selectional preferences predict human plausibility judgments for predicate-argument combinations) and effects inhuman sentence reading times.", "labels": [], "entities": []}, {"text": "All these applications rely on the availability of broad-coverage, reliable selectional preferences for predicates and their argument positions.", "labels": [], "entities": []}, {"text": "Given the immense effort necessary for manual semantic lexicon building and its associated reliability problems (see, e.g.,, all contemporary models of selectional preferences acquire selectional preferences automatically from large corpora.", "labels": [], "entities": [{"text": "manual semantic lexicon building", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.6836318522691727}]}, {"text": "The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument headword (or filler) from a corpus, and then to compute selectional preference as relative frequencies.", "labels": [], "entities": []}, {"text": "However, due to the Zipfian nature of word frequencies, the first step on its own results in a very sparse list of headwords, in particular for less frequent predicates.", "labels": [], "entities": []}, {"text": "As an example, the verb anglicize only appears with nine direct objects in the 100-million word British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.9422594308853149}]}, {"text": "Only one of them, name, appears more than once.", "labels": [], "entities": [{"text": "name", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.5107682943344116}]}, {"text": "Many highly plausible fillers are missing from the list, such as word or spelling.", "labels": [], "entities": [{"text": "word or spelling", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.6927290558815002}]}, {"text": "In order to make sensible predictions for triples that are unseen at training time, it is crucial to add a generalization step that infers a degree of preference for new, unseen headwords fora given predicate and role.", "labels": [], "entities": []}, {"text": "The result is, in the ideal case, an assignment to every possible headword of some degree of compatibility (or plausibility) with the predicate's preferences.", "labels": [], "entities": []}, {"text": "In the case of anglicize, the desired result would be a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine.", "labels": [], "entities": []}, {"text": "The predominant approach to generalizing over headwords, first introduced by, is based on semantic hierarchies such as WordNet ().", "labels": [], "entities": [{"text": "generalizing over headwords", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.8751750985781351}]}, {"text": "The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy.", "labels": [], "entities": [{"text": "WordNet noun hierarchy", "start_pos": 144, "end_pos": 166, "type": "DATASET", "confidence": 0.9046570460001627}]}, {"text": "This can be achieved in many different ways.", "labels": [], "entities": []}, {"text": "The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English ().", "labels": [], "entities": []}, {"text": "An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach.", "labels": [], "entities": [{"text": "generalization", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9738112688064575}]}, {"text": "These models, which avoid dependence on lexical resources, use corpus data for generalization.", "labels": [], "entities": []}, {"text": "In this article, we present a lightweight model for the acquisition and representation of selectional preferences.", "labels": [], "entities": [{"text": "acquisition and representation of selectional preferences", "start_pos": 56, "end_pos": 113, "type": "TASK", "confidence": 0.7993105252583822}]}, {"text": "Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be identified with reasonable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9778997898101807}]}, {"text": "Its key point is to use vector space similarity to generalize from seen to unseen headwords.", "labels": [], "entities": []}, {"text": "The vector space representations which serve as a basis for computing similarity can in principle be computed from any arbitrary corpus, given that it is large enough.", "labels": [], "entities": []}, {"text": "In particular, this need not be the same corpus as the one on which we observe predicate-headword co-occurrences.", "labels": [], "entities": []}, {"text": "Our model thus distinguishes between a primary corpus, from which the predicate-role-headword triples are extracted, and a generalization corpus for computing the vector space representations.", "labels": [], "entities": []}, {"text": "This distinction makes it possible to apply our model to primary corpora with rich information that are too small for efficient generalization, such as domain-specific corpora or corpora with deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization corpus is available.", "labels": [], "entities": []}, {"text": "We empirically demonstrate the benefit of this distinction.", "labels": [], "entities": []}, {"text": "We use FrameNet as primary corpus and the BNC as generalization corpus, modeling selectional preferences for semantic roles with nearperfect coverage and low error rate.", "labels": [], "entities": [{"text": "BNC", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8789730072021484}]}, {"text": "We evaluate our model on two tasks.", "labels": [], "entities": []}, {"text": "The first task is pseudo-disambiguation), where the model decides which of two randomly chosen words is a better filler for the given argument position.", "labels": [], "entities": []}, {"text": "This task tests model properties that are needed for concrete semantic analysis tasks, most notably word sense disambiguation, but also for semantic role labeling.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.7141286134719849}, {"text": "semantic role labeling", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7073974212010702}]}, {"text": "The second task is the prediction of human plausibility ratings, which is a standard task-independent benchmark for the quality of selectional preferences.", "labels": [], "entities": [{"text": "prediction of human plausibility ratings", "start_pos": 23, "end_pos": 63, "type": "TASK", "confidence": 0.8293509602546691}]}, {"text": "We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional models on both tasks.", "labels": [], "entities": [{"text": "WordNetbased", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.9528110027313232}]}, {"text": "Finally, we investigate inverse preferences, that is, preferences that arguments have for their predicates.", "labels": [], "entities": []}, {"text": "Although there is ample cognitive evidence for the existence of such preferences (e.g.,), to our knowledge, they have not been investigated systematically in linguistics.", "labels": [], "entities": []}, {"text": "However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g.,).", "labels": [], "entities": []}, {"text": "We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well.", "labels": [], "entities": []}, {"text": "The model that we discuss in this article, EPP, was first introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by (evaluating against human plausibility judgments).", "labels": [], "entities": []}, {"text": "In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse preferences, and we introduce anew similarity measure, nGCM, which achieves excellent performance in many settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we give a high-level overview over the experiments and experimental settings we will use subsequently.", "labels": [], "entities": []}, {"text": "Details will be provided in the following sections.", "labels": [], "entities": []}, {"text": "We evaluate the EPP model in three ways: We test the prediction of verbal selectional preference models with a pseudo-disambiguation task (Experiment 1).", "labels": [], "entities": []}, {"text": "Then, we address the task of predicting human verb-argument plausibility ratings (Experiment 2).", "labels": [], "entities": [{"text": "predicting human verb-argument plausibility ratings", "start_pos": 29, "end_pos": 80, "type": "TASK", "confidence": 0.8664855122566223}]}, {"text": "Finally, we investigate inverse selectional preferences-preferences of nouns for the predicates that they co-occur with-again using pseudo-disambiguation (Experiment 3).", "labels": [], "entities": []}, {"text": "We compare the EPP model to models from the three model categories presented in Section 2: RESNIK as a hierarchical model; ROOTH ET AL.", "labels": [], "entities": [{"text": "RESNIK", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9253577589988708}, {"text": "ROOTH ET AL", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.931102991104126}]}, {"text": "as a distributional model; and PADO ET AL.", "labels": [], "entities": [{"text": "PADO ET AL", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.8457734783490499}]}, {"text": "as a semantic role-based model.", "labels": [], "entities": []}, {"text": "As both and  have argued, no WordNet-based model systematically outperforms the others, and the RESNIK model shows the most consistent behavior across different scenarios.", "labels": [], "entities": []}, {"text": "Among the distributional models, we choose ROOTH ET AL.", "labels": [], "entities": [{"text": "ROOTH ET AL", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.8793008128801981}]}, {"text": "as a model that performs soft clustering and thus shows a marked difference to the EPP model.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first comparison of all three generalization paradigms: semantic hierarchy-based, distributional, and semantic role-based.", "labels": [], "entities": []}, {"text": "As mentioned earlier, we employ two tasks to evaluate the four models: pseudodisambiguation and the prediction of human plausibility ratings.", "labels": [], "entities": [{"text": "prediction of human plausibility ratings", "start_pos": 100, "end_pos": 140, "type": "TASK", "confidence": 0.804776418209076}]}, {"text": "The pseudodisambiguation task) has become a standard evaluation measure for selectional preference models).", "labels": [], "entities": []}, {"text": "Given a choice of two potential headwords, the task of a selectional preference model is to pick the more plausible one to fill a particular argument position of a given predicate.", "labels": [], "entities": []}, {"text": "Pseudo-disambiguation can be viewed as a word sense disambiguation task in which the two potential headwords together form a \"pseudo-word,\" for example herb/struggle from the original words herb and struggle.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.7221460938453674}]}, {"text": "The task is to \"disambiguate\" the pseudoword to the word that fits better in the given context.", "labels": [], "entities": []}, {"text": "It can also be viewed as an in vitro version of semantic role labeling and dependency parsing (depending on whether the relations are semantic roles or grammatical functions).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6588760117689768}, {"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7285434305667877}]}, {"text": "In this case, the scenario is that of a sentence containing a predicate and two words that could potentially fill an argument position of that predicate, for example, the predicate recommend with the potential headwords herb and struggle for the grammatical relation of direct object.", "labels": [], "entities": []}, {"text": "The task is to decide which of the two potential headwords is better suited to fill the argument position.", "labels": [], "entities": []}, {"text": "Human plausibility ratings, on the other hand, make considerably more finegrained distinctions than those occurring in pseudo-disambiguation tasks.", "labels": [], "entities": []}, {"text": "Here, models predict the exact human ratings for verb-argument-role triples.", "labels": [], "entities": []}, {"text": "Ratings are collected to further control carefully selected experimental items for psycholinguistic studies, or are solicited for corpus-derived triples specifically to create evaluation data for plausibility models (.", "labels": [], "entities": []}, {"text": "We contrast two different levels of semantic analysis for the predicates and argument positions.", "labels": [], "entities": []}, {"text": "In the SEM PRIMARY setting, the predicates are FrameNet frames, each of them potentially instantiated by multiple different verbs.", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.867937296628952}]}, {"text": "The argument positions in these settings are frame-semantic roles.", "labels": [], "entities": []}, {"text": "This setting most closely matches the notion of selectional preferences as characterizations of semantic arguments of an event.", "labels": [], "entities": []}, {"text": "In addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument positions are grammatical functions (subject and direct object).", "labels": [], "entities": []}, {"text": "Viewing grammatical functions as shallow approximations of semantic roles, we can expect the selectional preference models for this setting to yield noisier estimates than in the SEM PRIMARY setting.", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 179, "end_pos": 190, "type": "TASK", "confidence": 0.8550980091094971}]}, {"text": "The two settings will differ only in the choice of primary corpus, but will use the same generalization corpus.", "labels": [], "entities": []}, {"text": "illustrates the difference between the SEM PRIMARY setting and the SYN PRIMARY setting on an example from a pseudo-disambiguation task: The SEM PRIMARY setting has predicates like the FrameNet frame (predicate sense) ADORNING, with the semantic role THEME as argument position.", "labels": [], "entities": [{"text": "ADORNING", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.8883166909217834}, {"text": "THEME", "start_pos": 250, "end_pos": 255, "type": "METRIC", "confidence": 0.9701207876205444}]}, {"text": "In contrast, the SYN PRIMARY setting has predicates that are verb lemmas, such as cause, and argument positions that are grammatical functions (subj).", "labels": [], "entities": []}, {"text": "In both settings, the two potential headwords (here called headword and confounder, to be explained in more detail in the next section) to be distinguished in the pseudo-disambiguation task are noun lemmas.", "labels": [], "entities": []}, {"text": "The verb-dependency-headword tuples of the SYN PRIMARY setting yield much more coarse-grained and noisy characterizations of selectional preferences; however, they can be extracted from corpora with only syntactic annotation.", "labels": [], "entities": []}, {"text": "We are therefore able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this setting by parsing it with the Minipar dependency parser.", "labels": [], "entities": [{"text": "BNC (Burnard 1995)", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.8584013462066651}]}, {"text": "Minipar could parse almost all of the corpus, resulting in 6,005,130 parsed sentences.", "labels": [], "entities": []}, {"text": "For the SEM PRIMARY setting, we require a primary corpus with role-semantic annotation.", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.887668251991272}]}, {"text": "We use the much smaller FrameNet corpus.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.9247301816940308}]}, {"text": "FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists fine-grained semantic argument roles for each frame.", "labels": [], "entities": []}, {"text": "Ambiguity is expressed by membership of a word in multiple frames.", "labels": [], "entities": []}, {"text": "Each frame is exemplified with annotated example sentences extracted from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.964246928691864}]}, {"text": "The FrameNet release 1.2 comprises 131,582 annotated sentences (roughly three million words).", "labels": [], "entities": [{"text": "FrameNet release 1.2", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9132958849271139}]}, {"text": "To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.", "labels": [], "entities": [{"text": "Collins (1997) parser", "start_pos": 78, "end_pos": 99, "type": "DATASET", "confidence": 0.8362568855285645}]}, {"text": "As generalization corpus, we use the Minipar-parsed BNC in both settings.", "labels": [], "entities": [{"text": "Minipar-parsed BNC", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.8643674552440643}]}, {"text": "The experimentation with two different primary corpora allows us to directly study the influence of the disambiguation of predicates and the semantic characterization of argument positions on the performance of selectional preference models.", "labels": [], "entities": []}, {"text": "Note, however, that the comparison is complicated by differences between the two corpora: The primary corpus for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the determination of predicates, grammatical functions, and headwords.", "labels": [], "entities": []}, {"text": "The primary corpus for the SEM PRIMARY setting is manually annotated for semantics but is parsed automatically to determine headwords.", "labels": [], "entities": [{"text": "SEM PRIMARY setting", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8314392566680908}]}, {"text": "This can introduce noise in the headwords, but not in the determination of predicates and semantic roles.", "labels": [], "entities": []}, {"text": "Also, the primary corpus for the SYN PRIMARY setting is much larger than the one used in the SEM PRIMARY setting.", "labels": [], "entities": [{"text": "SEM PRIMARY setting", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8189445932706197}]}, {"text": "The first experiment uses a pseudo-disambiguation task to evaluate the models' performance on modeling the plausibility of nouns as headwords of argument positions of verbal predicates.", "labels": [], "entities": []}, {"text": "Experimental psycholinguistics affords a second perspective on selectional preferences: The plausibility of verb-argument pairs has been shown to have an important effect on human sentence processing (e.g.,.", "labels": [], "entities": [{"text": "human sentence processing", "start_pos": 174, "end_pos": 199, "type": "TASK", "confidence": 0.6945506731669108}]}, {"text": "In these studies, plausibility was operationalized as the thematic fit or selectional preference between a verb and its argument in a specific argument position.", "labels": [], "entities": []}, {"text": "Models of human sentence processing therefore need selectional preference models.", "labels": [], "entities": []}, {"text": "Conversely, psycholinguistic plausibility judgments can be used to evaluate computational models of selectional preferences.", "labels": [], "entities": []}, {"text": "We present evaluations on two plausibility judgment data sets used in recent studies.", "labels": [], "entities": []}, {"text": "The first data set consists of 100 data points 10 from McRae,.", "labels": [], "entities": [{"text": "McRae", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9740041494369507}]}, {"text": "Our example in, which is taken from this data set, was elicited by asking study participants to rate the plausibility of, for example, a hunter shooting (AGENT) or being shot (PATIENT).", "labels": [], "entities": [{"text": "AGENT", "start_pos": 154, "end_pos": 159, "type": "METRIC", "confidence": 0.8971691727638245}, {"text": "PATIENT)", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9209927320480347}]}, {"text": "The data point demonstrates the McRae set's balanced structure: 25 verbs are paired with two argument headwords in two argument positions each, such that each argument is highly plausible in one argument position but implausible in the other (hunters shoot, but are seldom shot, and vice versa for deer).", "labels": [], "entities": [{"text": "McRae set", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.9261887967586517}]}, {"text": "The resulting distribution of ratings is thus highly bimodal.", "labels": [], "entities": []}, {"text": "Models can only reliably predict the human ratings in this data set if they can capture the difference between verb argument positions as well as between individual fillers.", "labels": [], "entities": []}, {"text": "However, because the verb-argument pairs were created by hand and with strict requirements, many of the arguments are infrequent in standard corpora (e.g., wimp, bellboy, or knight).", "labels": [], "entities": []}, {"text": "When FrameNet is used to annotate senses for the verbs, no appropriate senses are available for 28 of the 100 verb-argument pairs, reducing the test set to 72 data points.", "labels": [], "entities": []}, {"text": "The second, larger data set addresses this sparseness issue.", "labels": [], "entities": []}, {"text": "Its triples are constructed on the basis of corpus co-occurrences . Eighteen verbs are combined with their three most frequent subjects and objects found in the Penn Treebank and FrameNet corpora, respectively, up to a total of 12 arguments.", "labels": [], "entities": [{"text": "Penn Treebank and FrameNet corpora", "start_pos": 161, "end_pos": 195, "type": "DATASET", "confidence": 0.8240019679069519}]}, {"text": "Each verb-argument pair was rated both as an agent and as a patient (i.e., both in the observed and an unobserved argument position), which leads to a total of 24 rated triples per verb.", "labels": [], "entities": []}, {"text": "The data set contains ratings for 414 triples.", "labels": [], "entities": []}, {"text": "The resulting judgments show a more even distribution of data.", "labels": [], "entities": []}, {"text": "With FrameNet annotation for the verbs, appropriate senses are not attested for six verb-argument pairs, reducing the test set to 408 data points.", "labels": [], "entities": []}, {"text": "We evaluate our models by correlating the predicted plausibility values with the human judgments, which range between 1 and 7.", "labels": [], "entities": []}, {"text": "Because we do not assume a priori that there The original data set has 60 data points more, which were used as the development set for the ET AL. model.", "labels": [], "entities": [{"text": "ET AL. model", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.9501770585775375}]}, {"text": "11 The PADO ET AL.", "labels": [], "entities": [{"text": "PADO ET AL", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.515833298365275}]}, {"text": "model now uses automatically induced verb clusters instead of FrameNet frames. is a linear correlation between the two variables, we do not use Pearson's productmoment correlation, but instead Spearman's \u03c1, a non-parametric rank-order correlation coefficient.", "labels": [], "entities": [{"text": "Pearson's productmoment correlation", "start_pos": 144, "end_pos": 179, "type": "METRIC", "confidence": 0.5762814655900002}]}, {"text": "Note that significance is harder to reach the smaller the number of data points is.", "labels": [], "entities": [{"text": "significance", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9802242517471313}]}, {"text": "In line with Experiment 1, we include a simple frequency baseline FREQ, which predicts the plausibility of each item as its frequency in the BNC (SYN) and in FrameNet (SEM), respectively.", "labels": [], "entities": [{"text": "FREQ", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9095119833946228}]}, {"text": "With regard to an upper bound, we assume that automatic models of plausibility should not be expected to surpass the typical human agreement on the plausibility judgment.", "labels": [], "entities": []}, {"text": "This is roughly \u03c1 \u2248 0.7 for the Pado data set.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9867951273918152}, {"text": "Pado data set", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9711390733718872}]}, {"text": "focuses on EPP variants with unreduced DEPSPACE for the McRae data set.", "labels": [], "entities": [{"text": "DEPSPACE", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.626626193523407}, {"text": "McRae data set", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9899289409319559}]}, {"text": "We see that this data set is rather difficult to model.", "labels": [], "entities": []}, {"text": "None of the models trained in the SEM PRIMARY setting achieves a significant correlation.", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9104909002780914}]}, {"text": "Apparently, the FrameNet corpus is too small to acquire selectional preferences that generalize well to the infrequent items that makeup the McRae data set.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.8826271891593933}, {"text": "McRae data set", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.9609009623527527}]}, {"text": "In the SYN PRIMARY setting, the nGCM model's predictions reach significance.", "labels": [], "entities": []}, {"text": "shows results on the McRae data set for all selectional preference models that we are considering.", "labels": [], "entities": [{"text": "McRae data set", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.9766488075256348}]}, {"text": "For EPP, we only show nGCM as the best-performing similarity measure from the pseudo-disambiguation task, and Cosine as a widely used vanilla measure.", "labels": [], "entities": []}, {"text": "The results for the SEM PRIMARY setting (left-hand side) mirror the results for the SEM PRIMARY setting in Experiment 1: The deep PADO ET AL.", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.719538152217865}, {"text": "SEM PRIMARY", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.786058098077774}, {"text": "PADO ET AL", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.5569300850232443}]}, {"text": "model shows the best correlation (it is the only model to predict human judgments significantly).", "labels": [], "entities": [{"text": "correlation", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9799631237983704}]}, {"text": "It overcomes the sparseness in the FrameNet corpus by using semantic verb classes that are particularly geared towards grouping the existing verb occurrences in the way that is most meaningful for this task.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9055755734443665}]}, {"text": "It covers about 80% of the test data.", "labels": [], "entities": []}, {"text": "EPP has full coverage, and although it does not make statistically significant predictions, it shows substantially higher correlation coefficients than ROOTH ET AL. and RESNIK.", "labels": [], "entities": [{"text": "EPP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9544827342033386}, {"text": "correlation", "start_pos": 122, "end_pos": 133, "type": "METRIC", "confidence": 0.9678599834442139}, {"text": "ROOTH ET AL.", "start_pos": 152, "end_pos": 164, "type": "METRIC", "confidence": 0.9250353574752808}, {"text": "RESNIK", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.6247014403343201}]}, {"text": "The DEPSPACE and WORDSPACE variants of EPP perform similarly here, and the simple frequency baseline has very low coverage and correlation.", "labels": [], "entities": [{"text": "DEPSPACE", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.5747162103652954}, {"text": "WORDSPACE", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.7260439991950989}, {"text": "EPP", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.8949750065803528}, {"text": "coverage", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9659931063652039}]}, {"text": "The term selectional preference is typically used to describe the semantic constraints that predicates place on their arguments.", "labels": [], "entities": []}, {"text": "In this section, we will investigate how nominal arguments place semantic constraints or expectations on the predicates with which they occur.", "labels": [], "entities": []}, {"text": "Such expectations can bethought of as typical events that involve the given object.", "labels": [], "entities": []}, {"text": "For example, a noun like apple could be said to have preferences about its inverse subject position, that is, the verbs that can take it as a plausible subject.", "labels": [], "entities": []}, {"text": "Examples might be verbs like grow or fall; for its inverse object position, apple probably prefers verbs like eat, cut, or plant.", "labels": [], "entities": []}, {"text": "We will use the term inverse selectional preference to refer to preferences of nouns for their predicates, distinguishing them from regular selectional preferences.", "labels": [], "entities": []}, {"text": "It is clear that not all verbs will be equally likely to occur with a given nounrole pair.", "labels": [], "entities": []}, {"text": "Still, inverse selectional preferences warrant a closer look: To what extent do inverse selectional preferences differ from regular ones?", "labels": [], "entities": []}, {"text": "And are the tasks of predicting regular and inverse selectional preferences equally difficult?", "labels": [], "entities": []}, {"text": "We start in Section 7.2 with an exploratory data analysis of inverse selectional preferences, which shows that inverse selectional preferences show semantically coherent patterns like regular selectional preferences, but that, in contrast to most verbs, nouns tend to occur with multiple semantic groups of verbs.", "labels": [], "entities": []}, {"text": "In Sections 7.3-7.5, we test the EPP model on a pseudodisambiguation task for inverse selectional preferences.", "labels": [], "entities": []}, {"text": "We evaluate inverse selectional preferences on a pseudo-disambiguation task that is setup completely analogously to our experiments on regular preferences in Section 5: given a noun, an inverse argument position, one verb observed in this position, and a confounder verb, distinguish between the two verbs.", "labels": [], "entities": []}, {"text": "We use the 100 nouns sampled across five frequency bands that we already used in Section 7.2.", "labels": [], "entities": []}, {"text": "We experiment with both WORDSPACE and DEPSPACE models, but restrict our attention to DISCR weighting, which showed good results in Experiment 1.", "labels": [], "entities": [{"text": "DEPSPACE", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7331260442733765}, {"text": "DISCR weighting", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.7445172071456909}]}, {"text": "In Section 5, we experimented on two different primary corpora, the BNC (SYN PRIMARY setting) and FrameNet (SEM PRIMARY setting).", "labels": [], "entities": [{"text": "BNC", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8298084735870361}]}, {"text": "Subsequently, we will use the SYN PRIMARY setting again, but not the SEM PRIMARY setting.", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.7402850687503815}]}, {"text": "In the SEM PRIMARY setting, the roles are FrameNet frame elements (semantic roles).", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.8798910975456238}]}, {"text": "However, frame elements are specific to a single frame, for example, the frame element ROPE belongs to the frame ROPE_MANIPULATION.", "labels": [], "entities": []}, {"text": "15 It would thus be pointless to predict a verb frame given a noun and a frame element name, as the frame element already gives away the frame.", "labels": [], "entities": []}, {"text": "shows the results of testing the EPP model for inverse selectional preferences on pseudo-disambiguation.", "labels": [], "entities": []}, {"text": "Coverage is very good for all model variants, similarly to Experiment 1.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9743753671646118}]}, {"text": "The error rates, as well, are close to those for the regular preferences in the SYN PRIMARY setting (cf.).", "labels": [], "entities": [{"text": "error", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9845618605613708}, {"text": "SYN PRIMARY setting", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.6475058595339457}]}, {"text": "The best model there (DEPSPACE, PCA, nGCM with DISCR weighting) achieved an error rate of 25.6%, and the best model for inverse preferences (WORDSPACE, Lin with DISCR weighting) reaches an error rate of 27.2% here.", "labels": [], "entities": [{"text": "DEPSPACE", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8389444351196289}, {"text": "error rate", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9797983467578888}]}, {"text": "Lin shows the best error rates in all conditions, closely followed by nGCM (the difference is significant in WORDSPACE and the reduced DEPSPACE, but not significant in the unreduced DEPSPACE).", "labels": [], "entities": [{"text": "error", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9589507579803467}, {"text": "DEPSPACE", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.6709176301956177}]}, {"text": "The Hindle similarity measure again brings up the rear.", "labels": [], "entities": [{"text": "Hindle similarity measure", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.7715555826822916}]}, {"text": "In PCA-transformed spaces, the error rates are similar across all similarity measures except for Hindle, as in Experiment 1.", "labels": [], "entities": [{"text": "error rates", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9800350964069366}]}], "tableCaptions": [{"text": " Table 4  SYN PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.", "labels": [], "entities": [{"text": "Pseudo-disambiguation", "start_pos": 31, "end_pos": 52, "type": "METRIC", "confidence": 0.9538528323173523}]}, {"text": " Table 5  SEM PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.", "labels": [], "entities": [{"text": "SEM PRIMARY", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6658133268356323}]}, {"text": " Table 7  Comparison of EPP DEPSPACE models on McRae data. Unreduced spaces, DISCR weighting.  ***p < 0.001.", "labels": [], "entities": [{"text": "EPP DEPSPACE", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.6784169971942902}, {"text": "McRae data", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.9654149413108826}, {"text": "DISCR weighting", "start_pos": 77, "end_pos": 92, "type": "METRIC", "confidence": 0.8928948938846588}]}, {"text": " Table 8  Comparison across models on McRae data. **p < 0.01, ***p < 0.001.", "labels": [], "entities": [{"text": "McRae data", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9476576447486877}]}, {"text": " Table 9  Comparison of EPP DEPSPACE parametrizations on Pad\u00f3 data. Unreduced spaces, DISCR  weighting. **p < 0.01; ***p < 0.001.", "labels": [], "entities": [{"text": "Pad\u00f3 data", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9538591802120209}, {"text": "DISCR  weighting", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9040524363517761}]}, {"text": " Table 10  Comparison across models on Pad\u00f3 data. ***p < 0.001.", "labels": [], "entities": [{"text": "Pad\u00f3 data", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.9652354121208191}]}, {"text": " Table 11  Minimal, median, and maximal selectional preference strength (measured in terms of KL  divergence) in a sample of 100 verbs and 100 nouns (20 lemmas each per frequency band).", "labels": [], "entities": [{"text": "Minimal", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.912109911441803}, {"text": "maximal selectional preference strength", "start_pos": 32, "end_pos": 71, "type": "METRIC", "confidence": 0.8808510601520538}, {"text": "KL  divergence", "start_pos": 94, "end_pos": 108, "type": "METRIC", "confidence": 0.9162372648715973}]}, {"text": " Table 12  Examples of regular and inverse selectional preferences from different frequency bands for  argument positions of nouns and verbs: overall selectional preference strength SelStr and most  highly associated fillers with association strengths SelAssoc.", "labels": [], "entities": []}, {"text": " Table 13  Pseudo-disambiguation results for inverse selectional preferences (BNC as primary and  secondary corpus, DISCR weighting). ER = Error rate; Cov = Coverage.", "labels": [], "entities": [{"text": "ER", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9951568245887756}, {"text": "Error rate", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9160015881061554}]}]}