{"title": [{"text": "The Noisy Channel Model for Unsupervised Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.6711321771144867}]}], "abstractContent": [{"text": "We introduce a generative probabilistic model, the noisy channel model, for unsupervised word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.6601459085941315}]}, {"text": "In our model, each context C is modeled as a distinct channel through which the speaker intends to transmit a particular meaning S using a possibly ambiguous word W.", "labels": [], "entities": []}, {"text": "To reconstruct the intended meaning the hearer uses the distribution of possible meanings in the given context P(S|C) and possible words that can express each meaning P(W|S).", "labels": [], "entities": []}, {"text": "We assume P(W|S) is independent of the context and estimate it using WordNet sense frequencies.", "labels": [], "entities": [{"text": "WordNet sense frequencies", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.8080725272496542}]}, {"text": "The main problem of unsupervised WSD is estimating context-dependent P(S|C) without access to any sense-tagged text.", "labels": [], "entities": [{"text": "WSD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9478833079338074}]}, {"text": "We show one way to solve this problem using a statistical language model based on large amounts of untagged text.", "labels": [], "entities": []}, {"text": "Our model uses coarse-grained semantic classes for S internally and we explore the effect of using different levels of granularity on WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9646504521369934}]}, {"text": "The system outputs fine-grained senses for evaluation, and its performance on noun disambiguation is better than most previously reported unsupervised systems and close to the best supervised systems.", "labels": [], "entities": [{"text": "noun disambiguation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7950868606567383}]}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is the task of identifying the correct sense of an ambiguous word in a given context.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8331400553385416}, {"text": "identifying the correct sense of an ambiguous word in a given context", "start_pos": 47, "end_pos": 116, "type": "TASK", "confidence": 0.5392408072948456}]}, {"text": "An accurate WSD system would benefit applications such as machine translation and information retrieval.", "labels": [], "entities": [{"text": "WSD", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9534125328063965}, {"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8256572782993317}, {"text": "information retrieval", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.8113016188144684}]}, {"text": "The most successful WSD systems to date are based on supervised learning and trained on sense-tagged corpora.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9808889031410217}]}, {"text": "In this article we present an unsupervised WSD algorithm that can leverage untagged text and can perform at the level of the best supervised systems for the allnouns disambiguation task.", "labels": [], "entities": [{"text": "WSD", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8906601667404175}, {"text": "allnouns disambiguation task", "start_pos": 157, "end_pos": 185, "type": "TASK", "confidence": 0.7893350919087728}]}, {"text": "The main drawback of the supervised approach is the difficulty of acquiring considerable amounts of training data, also known as the knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.7451796531677246}]}, {"text": "report that each successive doubling of the training data for WSD only leads to a 3-4% error reduction within their experimental range.", "labels": [], "entities": [{"text": "WSD", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9332902431488037}, {"text": "error", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.978536069393158}]}, {"text": "experiment with the problem of selection among confusable words and show that the learning curves do not converge even after a billion words of training data.", "labels": [], "entities": []}, {"text": "They suggest unsupervised, semi-supervised, or active learning to take advantage of large data sets when labeling is expensive.", "labels": [], "entities": []}, {"text": "observes that in a supervised naive Bayes WSD system trained on SemCor, approximately half of the test instances do not contain any of the contextual features (e.g., neighboring content words or local collocation patterns) observed in the training data.", "labels": [], "entities": []}, {"text": "SemCor is the largest publicly available corpus of sense-tagged text, and has only about a quarter million sense-tagged words.", "labels": [], "entities": []}, {"text": "In contrast, our unsupervised system uses the Web1T data set) for unlabeled examples, which contains counts from a 10 12 word corpus derived from publicly-available Web pages.", "labels": [], "entities": [{"text": "Web1T data set", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.9542836546897888}]}, {"text": "A note on the term \"unsupervised\" maybe appropriate here.", "labels": [], "entities": []}, {"text": "In the WSD literature \"unsupervised\" is typically used to describe systems that do not directly use sensetagged corpora for training.", "labels": [], "entities": [{"text": "WSD", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9342488050460815}]}, {"text": "However, many of these unsupervised systems, including ours, use sense ordering or sense frequencies from WordNet) or other dictionaries.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9485305547714233}]}, {"text": "Thus it might be more appropriate to call them weakly supervised or semi-supervised.", "labels": [], "entities": []}, {"text": "More specifically, context-sense pairs or context-word-sense triples are not observed in the training data, but context-word frequencies (from untagged text) and word-sense frequencies (from dictionaries or other sources) are used in model building.", "labels": [], "entities": []}, {"text": "One of the main problems we explore in this study is the estimation of contextdependent sense probabilities when no context-sense pairs have been observed in the training data.", "labels": [], "entities": []}, {"text": "The first contribution of this article is a probabilistic generative model for word sense disambiguation that seamlessly integrates unlabeled text data into the model building process.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.7564155757427216}]}, {"text": "Our approach is based on the noisy channel model, which has been an essential ingredient in fields such as speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.852484941482544}, {"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8230821192264557}]}, {"text": "In this study we demonstrate that the noisy channel model can also be the key component for unsupervised word sense disambiguation, provided we can solve the context-dependent sense distribution problem.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.6460894048213959}]}, {"text": "In Section 2.1 we show one way to estimate the context-dependent sense distribution without using any sense-tagged data.", "labels": [], "entities": []}, {"text": "Section 2.2 outlines the complete unsupervised WSD algorithm using this model.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9337857365608215}]}, {"text": "We estimate the distribution of coarse-grained semantic classes rather than fine-grained senses.", "labels": [], "entities": []}, {"text": "The solution uses the two distributions for which we do have data: the distribution of words used to express a given sense, and the distribution of words that appear in a given context.", "labels": [], "entities": []}, {"text": "The first can be estimated using WordNet sense frequencies, and the second can be estimated using an n-gram language model as described in Section 2.3.", "labels": [], "entities": []}, {"text": "The second contribution of this article is an exploration of semantic classes at different levels of granularity for word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.7084398965040842}]}, {"text": "Using fine-grained senses for model building is inefficient both computationally and from a learning perspective.", "labels": [], "entities": [{"text": "model building", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7306054383516312}]}, {"text": "The noisy channel model can take advantage of the close distribution of similar senses if they are grouped into semantic classes.", "labels": [], "entities": []}, {"text": "We take semantic classes to be groups of WordNet synsets defined using the hypernym hierarchy.", "labels": [], "entities": []}, {"text": "In each experiment we designate a number of synsets high in the WordNet hypernym hierarchy as \"head synsets\" and use their descendants to partition the senses into separate semantic classes.", "labels": [], "entities": [{"text": "WordNet hypernym hierarchy", "start_pos": 64, "end_pos": 90, "type": "DATASET", "confidence": 0.9292686978975931}]}, {"text": "In Section 3 we present performance bounds for such class-based WSD and describe our method of exploring the different levels of granularity.", "labels": [], "entities": []}, {"text": "In Section 4 we report on our actual experiments and compare our results with the best supervised and unsupervised systems from SensEval-2 (,.", "labels": [], "entities": []}, {"text": "Section 5 discusses these results and the idiosyncrasies of the data sets, baselines, and evaluation metrics used.", "labels": [], "entities": []}, {"text": "Section 6 presents related work, and Section 7 summarizes our contributions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran three experiments with the noisy channel model using different sets of semantic classes.", "labels": [], "entities": []}, {"text": "The first experiment uses the 25 WordNet semantic categories for nouns, the second experiment looks at what happens when we group all the senses to just two or three semantic classes, and the final experiment optimizes the number of semantic classes using one data set (which gives 135 classes) and reports the out-of-sample result using another data set.", "labels": [], "entities": []}, {"text": "The noun instances from the last three SensEval/SemEval English all-words tasks are used for evaluation.", "labels": [], "entities": [{"text": "SensEval/SemEval English all-words tasks", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.6773069302241007}]}, {"text": "We focus on the disambiguation of nouns for several reasons.", "labels": [], "entities": []}, {"text": "Nouns constitute the largest portion of content words (48% of the content words in the Brown corpus are nouns).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9174112677574158}]}, {"text": "For many tasks and applications (e.g., Web queries) nouns are the most frequently encountered and important part of speech.", "labels": [], "entities": []}, {"text": "Finally, WordNet has a more complete coverage of noun semantic relations than other parts of speech, which is important for our experiments with semantic classes.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.9547140002250671}, {"text": "noun semantic relations", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7867597341537476}]}, {"text": "As described in Section 2.2 we use the model to assign each ambiguous word to its most likely semantic class in all the experiments.", "labels": [], "entities": []}, {"text": "The lowest numbered sense in that class is taken as the fine-grained answer.", "labels": [], "entities": []}, {"text": "Finally we apply the one sense per discourse heuristic: If the same word has been assigned more than one sense within the same document, we take a majority vote and use sense numbers to break the ties.", "labels": [], "entities": []}, {"text": "gives some baselines for comparison.", "labels": [], "entities": []}, {"text": "The performance of the best supervised and unsupervised systems on noun disambiguation for each data set are given.", "labels": [], "entities": [{"text": "noun disambiguation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7989248335361481}]}, {"text": "The first-sense baseline (FSB) is obtained by always picking the lowest numbered sense for the word in the appropriate WordNet version.", "labels": [], "entities": [{"text": "first-sense baseline (FSB)", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.8636854648590088}, {"text": "WordNet version", "start_pos": 119, "end_pos": 134, "type": "DATASET", "confidence": 0.9298044443130493}]}, {"text": "We prefer the FSB baseline over the commonly used most-frequent-sense baseline because the tie breaking is unambiguous.", "labels": [], "entities": [{"text": "FSB baseline", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.5672015100717545}]}, {"text": "All the results reported are for fine-grained sense disambiguation.", "labels": [], "entities": []}, {"text": "The top three systems given in the table for each task are all supervised systems; the result for the best Baselines for the three SensEval English all-words tasks; the WordNet version used (WN); number of noun instances (Nouns); percentage accuracy of the first-sense baseline (FSB); the top three supervised systems; and the best unsupervised system (Unsup).", "labels": [], "entities": [{"text": "accuracy of the first-sense baseline (FSB)", "start_pos": 241, "end_pos": 283, "type": "METRIC", "confidence": 0.6681096665561199}]}, {"text": "The last row gives the total score of the best systems on the three tasks.", "labels": [], "entities": []}, {"text": "unsupervised system is given in the last column.", "labels": [], "entities": []}, {"text": "The reported unsupervised systems douse the sense ordering and frequency information from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9841212034225464}]}, {"text": "In previous work, descendants of 25 special WordNet synsets (known as the unique beginners) have been used as the coarse-grained semantic classes for nouns (Crestan, El-B` eze, and De Loupy 2001; Kohomban and Lee 2005).", "labels": [], "entities": []}, {"text": "These unique beginners were used to organize the nouns into 25 lexicographer files based on their semantic category during WordNet development.", "labels": [], "entities": []}, {"text": "shows the synsets at the top of the noun hierarchy in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.962405264377594}]}, {"text": "The 25 unique beginners have been shaded, and the two graphics show how the hierarchy evolved between the two WordNet versions used in this study.", "labels": [], "entities": []}, {"text": "Increasing the number of semantic classes has two opposite effects on WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9838320016860962}]}, {"text": "The higher the number, the finer distinctions we can make, and the maximum possible fine-grained accuracy goes up.", "labels": [], "entities": [{"text": "finer", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9726579785346985}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9350488185882568}]}, {"text": "However, the more semantic classes we define, the more difficult it becomes to distinguish them from one another.", "labels": [], "entities": []}, {"text": "For an empirical analysis of the effect of semantic class granularity on the fine-grained WSD accuracy, we generated different sets of semantic classes using the following algorithm.", "labels": [], "entities": [{"text": "WSD", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.8777817487716675}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.585494339466095}]}], "tableCaptions": [{"text": " Table 1  Baselines for the three SensEval English all-words tasks; the WordNet version used (WN);  number of noun instances (Nouns); percentage accuracy of the first-sense baseline (FSB); the top  three supervised systems; and the best unsupervised system (Unsup). The last row gives the  total score of the best systems on the three tasks.", "labels": [], "entities": [{"text": "SensEval English all-words tasks", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7930228263139725}, {"text": "accuracy of the first-sense baseline (FSB)", "start_pos": 145, "end_pos": 187, "type": "METRIC", "confidence": 0.7905995473265648}]}, {"text": " Table 3  Confusion matrix for Senseval2 data with the 25 WordNet noun classes. The rows are actual  classes, the columns are predicted classes. Column names have been abbreviated to save space.  The last two columns give the frequency of the class (F) and the accuracy of the class (A).", "labels": [], "entities": [{"text": "WordNet noun classes", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9025988380114237}, {"text": "accuracy", "start_pos": 261, "end_pos": 269, "type": "METRIC", "confidence": 0.9994937181472778}]}, {"text": " Table 5  Result summary for the three data sets. The columns give the data set, the results of the three  experiments, best reported result, the first-sense baseline, and the number of instances.", "labels": [], "entities": []}]}