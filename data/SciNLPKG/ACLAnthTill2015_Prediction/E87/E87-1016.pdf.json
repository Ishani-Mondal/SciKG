{"title": [{"text": "STOCHASTIC MODELING OF LANGUAGE VIA SENTENCE SPACE PARTITIONING", "labels": [], "entities": [{"text": "LANGUAGE VIA SENTENCE SPACE PARTITIONING", "start_pos": 23, "end_pos": 63, "type": "METRIC", "confidence": 0.6676456689834595}]}], "abstractContent": [{"text": "In some computer applications of linguistics (such as maximum-likelihood decoding of speech or handwriting), the purpose of the language-handling component (Language Model) is to estimate the linguistic (a priori) probability of arbitrary natural-language sentences.", "labels": [], "entities": []}, {"text": "This paper discusses theoretical and practical issues regarding an approach to building such a language model based on any equivalence criterion defined on incomplete sentences, and experimental results and measurements performed on such a model of the Italian language, which is apart of the prototype for the recognition of spoken Italian built at the IBM Rome Scintific Center.", "labels": [], "entities": [{"text": "recognition of spoken Italian", "start_pos": 311, "end_pos": 340, "type": "TASK", "confidence": 0.8314034789800644}, {"text": "IBM Rome Scintific Center", "start_pos": 354, "end_pos": 379, "type": "DATASET", "confidence": 0.8159738779067993}]}, {"text": "STOCHASTIC MODELS OF LANGUAGE In some computer applications, it is necessary to have away to estimate the probability of any arbitrary natural-language sentence.", "labels": [], "entities": []}, {"text": "A prominent example is maximum-likelihood speech recognition (as discussed in [1], [4], [7]), whose underlying mathematical approach can be generalized to recognition of natural language \"encoded\" in any medium (e.g. handwriting).", "labels": [], "entities": [{"text": "maximum-likelihood speech recognition", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.6075009902318319}]}, {"text": "The subsystem which estimates this probability can be called a stochastic model of the target language.", "labels": [], "entities": []}, {"text": "If the sentence is to be recognized while it is being produced (as necessary fora real-time application), the computation of its probability should proceed \"left-to-right,\" i.e. word byword from the beginning towards the end of the sentence, allowing application of fast tree-search algorithms such as stack decoding[5] Left-to-right computation of the probability of any word string is made possible by a formal manipulation based on the definition of condit__ional probability: if W i is the i-th word in the sequence 14' of length N, then: N e(W)= 1-IP(EI w,t , ~_~ .....", "labels": [], "entities": []}, {"text": "~'t) i=1 In other terms, the probability of a sequence of words is the product of the conditional probability of each word, given all of the previous ones.", "labels": [], "entities": []}, {"text": "As a formal step, this holds for full sentences as well as for any subsequence within a sentence, and also for multi-sentence pieces of text, as long as sentence boundaries are explicitly accounted for (typically by introducing a pseudo-word as sentence boundary marker).", "labels": [], "entities": []}, {"text": "We shall apply this equation only to subsequences occurring at the start of sentences (i.e. \"incomplete\" sentences); thus, the unconditional probability P(WI) can meaningfully be read as the probability that the particular word WI, rather than any other word, will be the one starting a sentence.", "labels": [], "entities": [{"text": "unconditional probability P(WI)", "start_pos": 127, "end_pos": 158, "type": "METRIC", "confidence": 0.889561136563619}]}, {"text": "The language model will thus consist essentially of away to compute the conditional probability of any (target) word given all of the words that precede it in the sentence.", "labels": [], "entities": []}, {"text": "For brevity, we shall call this (possibly empty) subsequence of the sentence to the left of the target word its prefix, using this term intcrchangeably with incomplete sentence, and we shall refer to the operation of conditional probability estimation given an incomplete sentence as predicang the next word in the sentence.", "labels": [], "entities": []}, {"text": "A stochastic language model in this form maybe said to be in predictive normal form [2].", "labels": [], "entities": []}, {"text": "The predictive power of two language models in predictive normal form can always be compared on an empirical basis, no matter how different their internal structures maybe, by using the perplexity statistic introduced in [6]; the perplexity, computed by applying a language model in predictive normal form to an arbitrary body of text, can be interpreted as the average number of words among which the model is \"in doubt\" at every context along the text (this can be made rigorous along the lines of the argument in [13]).", "labels": [], "entities": []}, {"text": "TRAINING THE MODEL A naive statistical approach to the estimation of the conditional probabilities of words given prefixes, to build a language model in predictive normal form, would simply collect occurrences of each prefix in a large corpus, using the relative frequencies of following words as estimates of probability.", "labels": [], "entities": [{"text": "TRAINING THE MODEL", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.8295002182324728}]}, {"text": "'l'i~is is clearly unfeasible: no matter how large the available corpus, the possible prefixes will be yet more numerous; thus, most of them will not be observed in the corpus, and those which are observed will only be seen followed by a very limited and unrepresentative subset of the words that can come after them.", "labels": [], "entities": []}, {"text": "This problem stems directly from the fact that the number of elements in the set (\"space\") of different possible (incomplete) sentences is too high; thus, it can be met head-on by simply reducing the number of incomplete sentences which are deemed to differ significantly for predictinn purposes, i.e. by passing to the quotient space of the sentence space on a suitable equivalence relation; in other words, by using as, contexts of the language model, the equivalence classes in a partition of the set of all prefixes, rather than the prefixes themselves.", "labels": [], "entities": []}, {"text": "The equivalence classification of prefixes can be based on any kind of linguistical knowledge, as long as it can be applied to two prefixes to judge if they can be deemed \"similar enough\" to allow us to expect that they should lead to the same prediction regarding the next word to Le expected in the sentence.", "labels": [], "entities": []}, {"text": "Indeed, the knowledge embodied in the equivalence classification need not be of the kind that would be commonly labeled \"[inguistical\"; the equivalence criterion 91", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}