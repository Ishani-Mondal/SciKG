{"title": [{"text": "Text Segmentation Using Exponential Models*", "labels": [], "entities": [{"text": "Text Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7075453102588654}]}], "abstractContent": [{"text": "This paper introduces anew statistical approach to partitioning text automatically into coherent segments.", "labels": [], "entities": [{"text": "partitioning text automatically into coherent segments", "start_pos": 51, "end_pos": 105, "type": "TASK", "confidence": 0.8338955442110697}]}, {"text": "Our approach enlists both short-range and long-range language models to help it sniff out likely sites of topic changes in text.", "labels": [], "entities": []}, {"text": "To aid its search, the system consults a set of simple lexical hints it has learned to associate with the presence of boundaries through inspection of a large corpus of annotated data.", "labels": [], "entities": []}, {"text": "We also propose anew probabilistically motivated error metric for use by the natural language processing and information retrieval communities, intended to supersede precision and recall for appraising segmen-tation algorithms.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9991251826286316}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9982824325561523}]}, {"text": "Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the effectiveness of our approach in two very different domains, Wall Street Journal articles and the TDT Corpus, a collection of newswire articles and broadcast news transcripts.", "labels": [], "entities": [{"text": "Wall Street Journal articles", "start_pos": 161, "end_pos": 189, "type": "DATASET", "confidence": 0.9512239992618561}, {"text": "TDT Corpus", "start_pos": 198, "end_pos": 208, "type": "DATASET", "confidence": 0.9206676483154297}]}], "introductionContent": [{"text": "The task we address in this paper might seem on the face of it rather elementary: identify where one region of text ends and another begins.", "labels": [], "entities": []}, {"text": "This work was motivated by the observations that such a seemingly simple problem can actually prove quite difficult to automate, and that a tool for partitioning a stream of undifferentiated text (or multimedia) into coherent regions would be of great benefit to a number of existing applications.", "labels": [], "entities": [{"text": "partitioning a stream of undifferentiated text (or multimedia) into coherent regions", "start_pos": 149, "end_pos": 233, "type": "TASK", "confidence": 0.8052761646417471}]}, {"text": "The task itself is ill-defined: what exactly is meant by a \"region\" of text?", "labels": [], "entities": []}, {"text": "We confront this issue by *Research supported in part by NSF grant IRI-9314969, DARPA AASERT award DAAH04-95-1-0475, and the ATR Interpreting Telecommunications Research Laboratories.", "labels": [], "entities": [{"text": "NSF grant IRI-9314969", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.6377757589022318}, {"text": "DARPA", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.5461549758911133}, {"text": "AASERT award DAAH04-95-1-0475", "start_pos": 86, "end_pos": 115, "type": "METRIC", "confidence": 0.6324108839035034}, {"text": "ATR Interpreting Telecommunications Research Laboratories", "start_pos": 125, "end_pos": 182, "type": "DATASET", "confidence": 0.9152298092842102}]}, {"text": "adopting an empirical definition of segment.", "labels": [], "entities": []}, {"text": "At our disposal is a collection of online data (38 million words of Wall Street Journal archives and another 150 million words from selected news broadcasts) annotated with the boundaries between regions--articles or news reports, respectively.", "labels": [], "entities": [{"text": "Wall Street Journal archives", "start_pos": 68, "end_pos": 96, "type": "DATASET", "confidence": 0.9634960740804672}]}, {"text": "Given this input, the task of constructing a segmenter maybe cast as a problem in machine learning: glean from the data a set of hints about where boundaries occur, and use these hints to inform a decision on whereto place breaks in unsegmented data.", "labels": [], "entities": []}, {"text": "A general-purpose tool for partitioning expository text or multimedia data into coherent regions would have a number of immediate practical uses.", "labels": [], "entities": [{"text": "partitioning expository text or multimedia data", "start_pos": 27, "end_pos": 74, "type": "TASK", "confidence": 0.8517305354277293}]}, {"text": "In fact, this research was inspired by a problem in information retrieval: given a large unpartitioned collection of expository text and a user's query, return a collection of coherent segments matching the query.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7605648338794708}]}, {"text": "Lacking a segmenting tool, an II:t application maybe able to locate positions in its database which are strong matches with the user's query, but be unable to determine how much of the surrounding data to provide to the user.", "labels": [], "entities": []}, {"text": "This can manifest itself in quite unfortunate ways.", "labels": [], "entities": []}, {"text": "For example, a video-on-demand application (such as the one described in () responding to a query about a recent news event may provide the user with a news clip related to the event, followed or preceded by part of an unrelated story or even a commercial.", "labels": [], "entities": []}, {"text": "Document summarization is another fertile area for an automatic segmenter.", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9232636094093323}]}, {"text": "Summarization tools often work by breaking the input into \"topics\" and then summarizing each topic independently.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9829967021942139}]}, {"text": "A segmentation tool has obvious applications to the first of these tasks.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 2, "end_pos": 14, "type": "TASK", "confidence": 0.9658221006393433}]}, {"text": "The output of a segmenter could also serve as input to various language-modeling tools.", "labels": [], "entities": []}, {"text": "For instance, one could envision segmenting a corpus, classifying the segments by topic, and then constructing topic-dependent language models from the generated classes.", "labels": [], "entities": []}, {"text": "The paper will proceed as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we very briefly review some previous approaches to the text segmentation problem.", "labels": [], "entities": [{"text": "text segmentation problem", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.8644887208938599}]}, {"text": "In Section 3 we describe our model, including the type of linguistic clues it looks for in deciding when placing a partition is appropriate.", "labels": [], "entities": []}, {"text": "In Section 4 we describe a feature induction algorithm that automatically constructs a set of the most informative clues.", "labels": [], "entities": [{"text": "feature induction", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7014960050582886}]}, {"text": "Section 5 shows examples of the feature induction algorithm inaction.", "labels": [], "entities": []}, {"text": "In Section 6 we introduce anew, probabilistically motivated way to evaluate a text segmenter.", "labels": [], "entities": [{"text": "text segmenter", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7016350775957108}]}, {"text": "Finally, in Section 7 we demonstrate our model's effectiveness on two distinct domains.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: A sample of the 84,694 word pairs from  the BN domain. Roughly speaking, after seeing an  \"s\" word, the empirical probability of witnessing the  corresponding \"t\" in the next N words is boosted by  the factor in the third column. In the experiments  described herein, N = 500. A separate set of (s, t)  pairs were extracted from the WSJ corpus.", "labels": [], "entities": [{"text": "BN domain", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.9257113933563232}, {"text": "WSJ corpus", "start_pos": 343, "end_pos": 353, "type": "DATASET", "confidence": 0.9881246089935303}]}]}