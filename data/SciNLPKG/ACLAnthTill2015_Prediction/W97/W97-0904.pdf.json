{"title": [{"text": "ESTIMATING THE TRUE PERFORMANCE OF CLASSIFICATION-BASED NLP TECHNOLOGY", "labels": [], "entities": [{"text": "PERFORMANCE OF CLASSIFICATION-BASED NLP TECHNOLOGY", "start_pos": 20, "end_pos": 70, "type": "METRIC", "confidence": 0.5630810856819153}]}], "abstractContent": [{"text": "Many of the tasks associated with natural language processing (NLP) can be viewed as classification problems.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.8051750461260477}]}, {"text": "Examples are the computer grading of student writing samples and speech recognition systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7418569326400757}]}, {"text": "If we accept this view, then the objective of learning classifications from sample text is to classify and predict successfully on new text.", "labels": [], "entities": []}, {"text": "While success in the marketplace can be said to be the ultimate test of validation for NLP systems, this success is not likely to be achieved unless appropriate techniques are used to validate the prototype.", "labels": [], "entities": []}, {"text": "This paper discusses useful validation techniques for classification-based NLP systems and how these techniques maybe used to estimate the true performance of the system.", "labels": [], "entities": [{"text": "classification-based NLP", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.7971913516521454}]}], "introductionContent": [{"text": "The objective of learning classifications from sample text is to classify and predict successfidly on new text.", "labels": [], "entities": []}, {"text": "For example, in developing a system for grading student writing samples, the objective is to learn how to classify student writing samples into grade categories so that we may use the system to predict successfully the grade categories for new samples of student writing.", "labels": [], "entities": []}, {"text": "The most commonly used measure of successor failure is a classifier's error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.8982721865177155}]}, {"text": "Each .time the classifier is presented with a case, it makes a decision about the appropriate class for the case.", "labels": [], "entities": []}, {"text": "Sometimes it is right; sometimes it is wrong.", "labels": [], "entities": []}, {"text": "The true error rate is statistically defined as the error rate of the classifier on a large number of new cases that converge in the limit to the actual population distribution.", "labels": [], "entities": [{"text": "true error rate", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7207454939683279}, {"text": "error rate", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9569781720638275}]}, {"text": "If we were given an unlimited number of cases, the true error rate could be readily computed as the number of samples approached infinity.", "labels": [], "entities": [{"text": "error rate", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9459184110164642}]}, {"text": "In the real world, the number of samples is always finite, and typically relatively small.", "labels": [], "entities": []}, {"text": "The major question is then whether it is possible to extrapolate from empirical error rates calculated from small sample results to the true error rate.", "labels": [], "entities": []}, {"text": "It turns out that there area number of ways of presenting sample cases to a classifier to get better estimates of the true error rate.", "labels": [], "entities": []}, {"text": "Some of these techniques are better than others.", "labels": [], "entities": []}, {"text": "In statistical terms, some estimators of the true error rate are considered biased.", "labels": [], "entities": [{"text": "true error rate", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.6954766313234965}]}, {"text": "They tend to estimate too low, i.e., on the optimistic side, or too high, i.e., on the pessimistic side.", "labels": [], "entities": []}, {"text": "In the next section, we will define just what an error is when using classification systems for natural language processing.", "labels": [], "entities": []}, {"text": "The apparent error rate will be contrasted with the true error rate.", "labels": [], "entities": [{"text": "apparent error rate", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.7507158716519674}]}, {"text": "The effect of classifier complexity and feature dimensionality on classification results will be followed by conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}