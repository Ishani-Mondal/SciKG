{"title": [{"text": "Lexicon Effects on Chinese Information Retrieval", "labels": [], "entities": [{"text": "Chinese Information Retrieval", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6034613251686096}]}], "abstractContent": [{"text": "We investigate the effects of lexicon size and stopwords on Chinese information retrieval using our method of short-word segmentation based on simple language usage rules and statistics.", "labels": [], "entities": [{"text": "Chinese information retrieval", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.596055289109548}]}, {"text": "These rules allow us to employ a small lexicon of only 2,175 entries and provide quite admirable retrieval results.", "labels": [], "entities": []}, {"text": "It is noticed that accurate segmentation is not essential for good retrieval.", "labels": [], "entities": [{"text": "accurate", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.954129159450531}, {"text": "segmentation", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.8318806290626526}]}, {"text": "Larger lexicons can lead to incremental improvements.", "labels": [], "entities": []}, {"text": "The presence of stopwords do not contribute much noise to IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9842575788497925}]}, {"text": "Their removal risks elimination of crucial words in a query and adversely affect retrieval, especially when the queries are short.", "labels": [], "entities": []}, {"text": "Short queries of a few words perform more than 10% worse than paragraph-size queries.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is well known that a sentence in Chinese (or several other oriental languages) consists of a continuous string of 'characters' without delimiting white spaces to identify words.", "labels": [], "entities": []}, {"text": "In Chinese, the characters are called ideographs.", "labels": [], "entities": []}, {"text": "This makes it difficult to do machine studies on these languages since isolated words are needed for many purposes, such as linguistic analysis, machine translation, etc.", "labels": [], "entities": [{"text": "linguistic analysis", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7152260988950729}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7543646097183228}]}, {"text": "Automatic methods for correctly isolating words in a sentence --a process called word segmentation --is therefore an important and necessary first step to betaken before other analysis can begin.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7068996429443359}, {"text": "betaken", "start_pos": 155, "end_pos": 162, "type": "TASK", "confidence": 0.9269393086433411}]}, {"text": "Many researchers have proposed practical methods to resolve this problem such as (.", "labels": [], "entities": []}, {"text": "Information retrieval (IR) deals with the problem of selecting relevant documents fora user need that is expressed in free text.", "labels": [], "entities": [{"text": "Information retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8772660553455353}]}, {"text": "The document collection is usually huge, of gigabyte size, and both queries and documents are domain unrestricted and unpredictable.", "labels": [], "entities": []}, {"text": "When one does IR in the Chinese language with its peculiar property, then one would assume that accurate word segmentation is also a crucial first step before other processing can begin.", "labels": [], "entities": [{"text": "IR", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.981681764125824}, {"text": "accurate word segmentation", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.5665905276934305}]}, {"text": "However, in the recent 5th Text REtrieval Conference (TREC-5) where a fairly large scale Chinese IR experiment was performed, we have demonstrated that a simple word segmentation method, couple with a powerful retrieval algorithm, is sufficient to provide quite good retrieval results.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC-5)", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7745445569356283}, {"text": "word segmentation", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7049439400434494}]}, {"text": "Moreover, experiments by others using even simpler bigram representation of text (i.e. all consecutive overlapping two characters), both within and outside the TREC environment, also produce good results.", "labels": [], "entities": []}, {"text": "This is a bit counter-intuitive because the bigram method leads to three times as large an indexing feature space compared with our segmentation (approximately 1.5 million vs 0.5 million), and one would expect that there are many random, non-content matchings between queries and documents that may adversely affect precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 316, "end_pos": 325, "type": "METRIC", "confidence": 0.9933106899261475}]}, {"text": "Apparently, this is not so.", "labels": [], "entities": []}, {"text": "Based on this observation, we made some adjustments to our lexicon, and provide some experimental results of the lexicon effects on retrieval effectiveness.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Bigram and Short-Word Segmentation  Retrieval Results Averaged over 28 Queries", "labels": [], "entities": [{"text": "Short-Word Segmentation  Retrieval", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.6889474093914032}]}, {"text": " Table 2: Effect of Lexicon-based and Rule-based Stopwords on Long Query Retrieval using L0 & L01", "labels": [], "entities": []}, {"text": " Table 3. There is incremental  improvements in average precision by using the larger  lexicon: e.g. for ExpTyp.1, from 0.455 (L0) to 0.463  (Lll), about 2%. The removal of stopwords for Lll  (ExpTyp.4 vs 1) does not lead to much difference,", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9522321224212646}, {"text": "ExpTyp.1", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.90091472864151}]}, {"text": " Table 3: Effect of Lexicon-based and Rule-based  Stopwords on Long Query Retrieval using L1 and  Lll", "labels": [], "entities": []}, {"text": " Table 4: Effect of Lexicon-based and Rule-based Stopwords on Short Query Retrieval using L00, L01, L1  & Lll.", "labels": [], "entities": []}]}