{"title": [{"text": "Collocation Lattices and Maximum Entropy Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Maximum entropy framework proved to be expressive and powerful for the statistical language modelling, but it suffers from the computational expensiveness of the model building.", "labels": [], "entities": [{"text": "statistical language modelling", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.7544570565223694}]}, {"text": "The iterative scaling algorithm that is used for the parameter estimation is computationally expensive while the feature selection process requires to estimate parameters of the model for many candidate features many times.", "labels": [], "entities": []}, {"text": "In this paper we present a novel approach for building maximum entropy models.", "labels": [], "entities": []}, {"text": "Our approach uses a features collocation lattice and selects the atomic features without resorting to iterative scaling.", "labels": [], "entities": []}, {"text": "After the atomic features have been selected we, using the iterative scaling, compile a fully saturated model for the maximal constraint space and then start to eliminate the most specific constraints.", "labels": [], "entities": []}, {"text": "Since during constraint deselection at every point we have a fully fit maximum entropy model, we rank the constraints on the basis of their weights in the model.", "labels": [], "entities": []}, {"text": "Therefore we don't have to use the iterative scaling during constraint ranldng and apply it only for linear model regression.", "labels": [], "entities": []}, {"text": "Another important improvement is that since the simplified model deviates from the previous larger model only in a small number of constraints, we use the parameters of the old model as the initial values of the parameters for the iterative scaling of the new one.", "labels": [], "entities": []}, {"text": "This proved to decrease the number of required iterations by about tenfold.", "labels": [], "entities": []}, {"text": "As practical results we discuss how our method has been applied to several tasks of language modelling such as sentence boundary disambiguation, part-of-speech tagging and automatic document abstracting.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7214750796556473}, {"text": "sentence boundary disambiguation", "start_pos": 111, "end_pos": 143, "type": "TASK", "confidence": 0.6399611532688141}, {"text": "part-of-speech tagging", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.7831535637378693}, {"text": "automatic document abstracting", "start_pos": 172, "end_pos": 202, "type": "TASK", "confidence": 0.5811261832714081}]}], "introductionContent": [{"text": "Maximum entropy modelling has been recently introduced to the NLP community and proved to bean expressive and powerful framework.", "labels": [], "entities": [{"text": "Maximum entropy modelling", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6436872084935507}]}, {"text": "The maximum entropy model is a model which fits to a set of pre-defined constraints and assumes m~ximum ignorance about everything which is not subject to its constraints thus assigning such cases with the most uniform distribution.", "labels": [], "entities": []}, {"text": "The most uniform distribution will have the entropy on its maxim~n and the model is chosen accorrllng to:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}