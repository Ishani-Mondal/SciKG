{"title": [{"text": "Towards Generation of Fluent Referring Action in Multimodal Situations", "labels": [], "entities": [{"text": "Generation of Fluent Referring Action", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.7639891624450683}]}], "abstractContent": [{"text": "Referring actions in multimodal situations can bethought of as linguistic expressions well coordinated with several physical actions.", "labels": [], "entities": []}, {"text": "In this paper, what patterns of linguistic expressions are commonly used and how physical actions are temporally coordinated to them are reported based on corpus examinations.", "labels": [], "entities": []}, {"text": "In particular, by categorizing objects according to two features, visibility and membership, the schematic patterns of referring expressions are derived.", "labels": [], "entities": []}, {"text": "The difference between the occurrence frequencies of those patterns in a multimodal situation and a spoken-mode situation explains the findings of our previous research.", "labels": [], "entities": []}, {"text": "Implementation based on these results is ongoing.", "labels": [], "entities": []}], "introductionContent": [{"text": "A lot of active studies have been conducted on the temporal coordination of natural language and visual information.", "labels": [], "entities": []}, {"text": "The visual information considered includes pointing gestures, facial expressions and iconic gestures, and graphical effects such as highlighting and blinking ().", "labels": [], "entities": []}, {"text": "Among those we have been focusing on generating effective explanations by using natural language temporally coordinated with pictures and gestures.", "labels": [], "entities": []}, {"text": "The experimental system we implemented is for explaining the installation and operation of a telephone with an answering machine feature, and simulates instruction dialogues performed by an expert in a face-to-face situation with a telephone in front of her ().", "labels": [], "entities": []}, {"text": "The system explains by using synthesized speech coordinated with pointing gestures from a caricatured agent and simulated operations implemented by the switching of figures.", "labels": [], "entities": []}, {"text": "One of the important issues for enhancing this type of system is to shed light on what makes referring actions fluent in multimodal situations and to build a mechanism to generate such fluent actions.", "labels": [], "entities": []}, {"text": "We also empirically investigated how communicative modes influence the content and style of referring actions made in dialogues.", "labels": [], "entities": []}, {"text": "Experiments were conducted to obtain a corpus consisting of human-to-human instruction dialogues on telephone installation in two settings.", "labels": [], "entities": []}, {"text": "One is a spoken-mode dialogue situation (SMD hereafter), in which explanations are given using just voice.", "labels": [], "entities": []}, {"text": "The other is a multimodal dialogue situation (MMD hereafter), in which both voice and visual information, mainly the current state and outlook of the expert's telephone and her pointing gestures to it, can be communicated.", "labels": [], "entities": []}, {"text": "Detailed analysis of the referring actions observed in that corpus revealed the following two properties.", "labels": [], "entities": []}, {"text": "PI: The availability of pointing, communication through the visual channel reduces the amount of information conveyed through the speech or linguistic channel.", "labels": [], "entities": [{"text": "PI", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8706633448600769}]}, {"text": "In initial identification, the usage of linguistic expressions on shape/size, characters/marks, and related objects decreases in MMD, while the usage of position information does not decrease.", "labels": [], "entities": [{"text": "initial identification", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.7524363696575165}, {"text": "MMD", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.915638267993927}]}, {"text": "P2: In SMD, referring actions tend to be realized to an explicit goal and divided into a series of finegrained steps.", "labels": [], "entities": [{"text": "SMD", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9800951480865479}]}, {"text": "The participants try to achieve them step by step with many confirmations.", "labels": [], "entities": []}, {"text": "Although our findings were very suggestive for analyzing the properties of referring actions in multimodal situations, they were still descriptive and not sufficient to allow their use in designing referring action generation mechanisms.", "labels": [], "entities": [{"text": "referring action generation", "start_pos": 198, "end_pos": 225, "type": "TASK", "confidence": 0.6496706008911133}]}, {"text": "Then, as the next step, we have been examining that corpus closer and trying to derive some schemata of referring actions, which would be useful for implementation of multimodal dialogue systems.", "labels": [], "entities": []}, {"text": "This paper reports the results of these activities.", "labels": [], "entities": []}, {"text": "Two short comments must be made to make our research standpoint clearer.", "labels": [], "entities": []}, {"text": "First, our purpose is to generate referring actions that model human referring actions in mundane situations.", "labels": [], "entities": []}, {"text": "Theoretically speaking, as Appelt pointed out, it is enough for referring to provide sufficient description to distin-guish one object from the other candidates.", "labels": [], "entities": []}, {"text": "For example, a pointing action to the object must be enough, or description of the object's position, such as \"the upper left button of the dial buttons\" also must be considered sufficient.", "labels": [], "entities": []}, {"text": "However, we often observe referring actions that consist of a linguistic expression, \"a small button with the mark of a handset above and to the left of the dial buttons\", accompanied with a pointing gesture.", "labels": [], "entities": []}, {"text": "Such a referring action is familiar to us even though it is redundant from a theoretical viewpoint.", "labels": [], "entities": []}, {"text": "Such familiar actions that the recipient does not perceive as awkward is called fluent in this paper.", "labels": [], "entities": []}, {"text": "Our objective is to generate such fluent referring actions, and is rather different from those of and.", "labels": [], "entities": []}, {"text": "Second, in our research, a referring action is considered as the entire sequence of actions needed for allowing the addressee to identify the intended object and incorporating its achievement into part of the participants' shared knowledge.", "labels": [], "entities": []}, {"text": "In order to refer to an object in a box, an imperative sentence such as \"Open the box, and look inside\" maybe used.", "labels": [], "entities": []}, {"text": "Such a request shifts the addressee's attention, and to see it as apart of the referring action maybe problematic.", "labels": [], "entities": []}, {"text": "It is, however, reasonable to think that both the request for looking into the box and the assertion of the fact that an object is in the box come from different plans for achieving the same goal, identifying the object.", "labels": [], "entities": []}, {"text": "As Cohen claimed that it is useful to understand referring expressions from the viewpoint of speech act planning, it is not so ridiculous to go one step further and to consider the entire sequence of actions, including attention shifts, as an instance of a plan for object referring.", "labels": [], "entities": [{"text": "speech act planning", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.706663191318512}, {"text": "object referring", "start_pos": 266, "end_pos": 282, "type": "TASK", "confidence": 0.7272356599569321}]}, {"text": "Moreover, this approach better suits implementing a referring action generation mechanism as a planner.", "labels": [], "entities": [{"text": "referring action generation", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6757629116376241}]}, {"text": "The next section describes what kinds of linguistic expression are used for referring actions in MMD and compares them with those in SMD.", "labels": [], "entities": [{"text": "MMD", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.8977896571159363}]}, {"text": "In particular, by categorizing objects according to two features: visibility and membership, schemata for object referring expressions of each category are derived.", "labels": [], "entities": []}, {"text": "In the third section, how several kinds of actions such as pointing gestures are accompanied by such expressions is reported.", "labels": [], "entities": []}, {"text": "In the fourth section, implementation of referring action generation is discussed based on our findings described thus far.", "labels": [], "entities": [{"text": "referring action generation", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7762951652208964}]}, {"text": "Finally, in the last section, our findings are summarized and future work is discussed.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The schemata for referring to visible isolated objects and their occurrence frequency", "labels": [], "entities": []}, {"text": " Table 2: The schemata for referring to invisible objects and their occurrence frequency", "labels": [], "entities": []}, {"text": " Table 3: The schemata for referring to group members and their occurrence frequency", "labels": [], "entities": []}]}