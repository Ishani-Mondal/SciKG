{"title": [], "abstractContent": [{"text": "This paper presents anew view of ExplanatiomBased Learning (EBL) of natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6416538059711456}]}, {"text": "Rather than employing EBL for specializing parsers by inferring new ones, this paper suggests employing EBL for learning how to reduce ambiguity only partially.We exemplify this by presenting anew EBL method that learns parsers that avoid spurious over-generation, and we show how the same method can be used for reducing the sizes of stochastic: grammars learned from tree-banks, e.g. (Bod, 1995, Charniak, 1996, Sekine and Grishman, 1995).", "labels": [], "entities": []}, {"text": "The present method consists of an EBL algorithm for learning partial-parsers, and a parsing algorithm which combines partial-parsers with existing \"full-parsers\".", "labels": [], "entities": []}, {"text": "The learned partial-parsers, implementable as Cascades of Finite State Transducers (CF-STs), recognize and combine constituents efficiently, prohibiting spurious overgener-ation.", "labels": [], "entities": []}, {"text": "The parsing algorithm combines a learned partial-parser with a given full-parser such that the role of the full-parser is limited tO combining the constituents, recognized by the partial-parser, and to recognizing unrecognized portions of the input sentence.", "labels": [], "entities": []}, {"text": "Besides the reduction of the parse-space prior to disambiguation, the present method provides away for refining existing disambiguation models that learn stochastic grammars from tree-banks e.g. (Bod, 1995, Charniak, 1996, Sekine and Grishman, 1995).", "labels": [], "entities": []}, {"text": "We exhibit encouraging empirical results using a pilot implementation: parse-space is reduced substantially with minimal loss of coverage.", "labels": [], "entities": []}, {"text": "The speedup gain for disam-biguation models is exemplified by experiments with the DOP model (Bod, 1995).", "labels": [], "entities": [{"text": "Bod, 1995)", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8304917514324188}]}], "introductionContent": [{"text": "Current work on natural language parsing is in large part directed towards eliminating overgeneration of grammars by employing stochastic models for disambiguation (e.g.).", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.6491324305534363}]}, {"text": "For many applications (e.g. Speech Understanding), probabilistic evaluation of the full parse-space using such models is NPhard, and even when it is deterministic polynomial-time, then grammar size is prohibitive.", "labels": [], "entities": [{"text": "Speech Understanding", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8197129368782043}]}, {"text": "Therefore, it is necessary to develop methods that, on the one hand, reduce the space of analyses, as much as possible prior to disambiguation, and on the other hand, reduce the sizes of grammars used for disambiguation.", "labels": [], "entities": []}, {"text": "This paper presents a method aimed at these two forms of reduction of time and space costs.", "labels": [], "entities": []}, {"text": "In recent work on speeding up parsing, effort is directed towards specializing broad-coverage grammar by EBL (e.g.).", "labels": [], "entities": [{"text": "speeding up parsing", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.6909185250600179}]}, {"text": "Grammar-specialization, in these works, amounts to replacing a given parser by afresh efficient parser learned from the tree-bank.", "labels": [], "entities": []}, {"text": "The learned parser trades coverage for efficiency.", "labels": [], "entities": [{"text": "coverage", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9490011930465698}]}, {"text": "Inspired by these works, we present anew method based on EBL for learning efficient parsers.", "labels": [], "entities": []}, {"text": "Rather than specializing a given full-parser by inferring anew one, the present method learns a partial-parser and combines it with the full-parser in away that reduces ambiguity.", "labels": [], "entities": []}, {"text": "The combination is a serial construction in which the partial-parser is employed first for recognizing and combining constituents.", "labels": [], "entities": []}, {"text": "The partialparser is learned such that it parses only those portions of the sentence that are \"safe\" to parse, i.e. at the points where there is clear bias in the tree- In T.M.", "labels": [], "entities": []}, {"text": "Ellison (ed.) bank.", "labels": [], "entities": [{"text": "Ellison (ed.) bank.", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9252451757589976}]}, {"text": "These constituents are then passed through, together with unrecognized portions of the input, to the full-parser, that completes the space only where necessary.", "labels": [], "entities": []}, {"text": "For disambiguation models such as, the present method refines the cutting criteria which these models employ for inferring stochastic grammars.", "labels": [], "entities": []}, {"text": "This refinement results in the inference of smaller, yet no less powerful, statistical grammars.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments concern both coverage as well assize of parse-space.", "labels": [], "entities": []}, {"text": "We employ the T-parser underlying the tree-bank (CFG) as a full-parser.", "labels": [], "entities": []}, {"text": "In table 1 we list the results often independent experiments, each obtained by a random split of 4500 training-set and 500 test-set.", "labels": [], "entities": []}, {"text": "Since the domain contains many (easy for parsing) one word utterances (e.g. \"yes\" or \"no\"), we exclude one word utterances from the results.", "labels": [], "entities": [{"text": "parsing) one word utterances", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.8774435877799988}]}, {"text": "On average, the ten test-sets contained 337.2 (of 500) utterances longer than one word.", "labels": [], "entities": []}, {"text": "shows the results on utterances longer than one word, with mean length of 5.57 words per utterance.", "labels": [], "entities": [{"text": "mean length", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.7949883937835693}]}, {"text": "For training the EBL learning algorithm we set a threshold on the frequency of SSFs: 0.3% of the size of the training-set (i.e. 14).", "labels": [], "entities": []}, {"text": "To avoid problems of unknown words, we allowed the words of the test-set to be included with all postags with: Means and STDs often experiments (OVIS): Par denotes Partial-Parser which they appear in the whole tree-bank (for both parsers).", "labels": [], "entities": [{"text": "Means", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9478521347045898}]}, {"text": "shows the statistical means and (in brackets) the standard deviations of the ten experiments (always for sentences longer than 1 word).", "labels": [], "entities": []}, {"text": "Right parse (also structural consistency) denotes the percentage of test sentences for which the parser's chart contains the right parse (i.e. test-set parse).", "labels": [], "entities": []}, {"text": "Any parse (also coverage) denotes the percentage of test sentences for which the parser's chart contained a parse.", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9879827499389648}]}, {"text": "Precision denotes the ratio (Right parse/Any parse), which expresses the precision of the parser as a parse-space generator.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9866980910301208}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9970577955245972}]}, {"text": "And active nodes denotes the mean number of active items in a CYK parser implementation; active items are those items that participate in a full parse of the sentence.", "labels": [], "entities": []}, {"text": "On average the partial-parser reduces the space by 4.33 times on all sentence lengths.", "labels": [], "entities": []}, {"text": "The reduction of space reaches a mean of 7 times on sentences longer than 6.", "labels": [], "entities": []}, {"text": "The degradation in precision (4%) is due to several reasons.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996484518051147}]}, {"text": "Firstly, the fact that the partial-parser is currently implemented as a contextfree recognizer clearly contributes to this degradation.", "labels": [], "entities": []}, {"text": "Secondly, after analyzing the test-results of one experiment, we found out that about half of the errors are due to deeper structures assigned by the Partial-Parser rather than really wrong structures; typically those were compound NPs which received shallow annotations in the tree-bank.", "labels": [], "entities": []}, {"text": "Thirdly, part of the errors is due to tree-bank annotation mistakes.", "labels": [], "entities": [{"text": "errors", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9611473083496094}]}, {"text": "And finally, there is a remaining part of errors which is due to the assumptions of the EBL method; these are harder to solve than the previous three.", "labels": [], "entities": []}, {"text": "In and 5 we show the learning curves of the present method for six sizes of training-sets; five of the six training-sets were obtained randomly from a set of 4500 trees, and the sixth consisted of the whole set.", "labels": [], "entities": []}, {"text": "For these experiments we employed the same set of 500 test-trees randomly chosen (all length sentences).", "labels": [], "entities": []}, {"text": "The experiments were repeated twice: once allowing \"retreating\" on localcontext (as explained earlier), and once not allowing that, during the learning phase (the two versions are denoted \"Retreating\" and \"No-Retreating\" respectively).", "labels": [], "entities": []}, {"text": "The learning curves of the Retreating partial-parser, show that from a certain point on there is some deterioration of precision but further gain of space-reduction.", "labels": [], "entities": [{"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9993929862976074}]}, {"text": "The situation is different  with the No-Retreating version.", "labels": [], "entities": []}, {"text": "The explanation for the loss of precision is that when the training-set is smaller, less PA-SSFs are learned, which implies a larger role for the T-Parser.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.999404788017273}]}, {"text": "This situation is magnified by the fact that the coverage of the T-Parser is lower on smaller training-sets.", "labels": [], "entities": [{"text": "coverage", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9891911745071411}]}, {"text": "The deterioration of precision of the Retreating version compared to the No-Retreating version is due to the fact that the number of learned local-context PA-SSFs becomes much larger; this implies reduction of parse-space but also some loss of precision (since the partialparser does not employ the local-context).", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9990087747573853}, {"text": "precision", "start_pos": 244, "end_pos": 253, "type": "METRIC", "confidence": 0.9991050362586975}]}, {"text": "To test the present method together with DOP we employed the same 10 random splits which we employed in the previous experiments.", "labels": [], "entities": [{"text": "DOP", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.7342228889465332}]}, {"text": "This time we did not include anything about unknown words in the test-sets (i.e. a sentence that includes an unknown word is not parsable).", "labels": [], "entities": []}, {"text": "DOP and EBL+DOP were trained employing the following parameter setting for partial-trees (cf. (Sima'an, 1996a)): for each projected partial-tree, a maximum was set on its depth (D), number of substitution-sites (N) on its frontier, number of words (W) and number of consecutive words (C) on its frontier.", "labels": [], "entities": []}, {"text": "The setting was D=4, N=2, W=7 and C=2.", "labels": [], "entities": [{"text": "setting", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9661518931388855}, {"text": "D", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9798892140388489}]}, {"text": "This reduces the number of elementary-trees which DOP projects drastically without loss of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9948298335075378}]}, {"text": "Furthermore, the EBL algorithm was trained with a threshold on the frequency of SSFs equal to 14.", "labels": [], "entities": []}, {"text": "The EBL method is used for both specializing the T-parser, which DOP, and for specifying the cut-nodes for DOP.", "labels": [], "entities": [{"text": "EBL", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.797193169593811}]}, {"text": "Another set of experiments on the same 10 random splits (denoted EBL0.75 in table 2) was conducted where the threshold on 0 was set at 0.75, i.e. a sequence of grammar symbols was allowed to be learned if it was for at least 75% of the time an SSF.", "labels": [], "entities": [{"text": "EBL0.75", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.5544432401657104}]}, {"text": "This was achieved by allowing the learning algorithm to change the threshold (0) on the definition of PA-SSF; each time there are no more PA-SSFs to learn, 0 was reduced by 0.03 and learning went on. lists the means and standard deviation for the 10 experiments for all sentences of length larger or equal to 2 words.", "labels": [], "entities": []}, {"text": "The average (std of) percentage of the sentences that included an unknown word is 2.56% (0.93%).", "labels": [], "entities": []}, {"text": "The measures which the table lists are coverage and accuracy, where coverage is the percentage of sentences that received a parse, and accuracy is the percentage of parsable sentences that received exactly the same parse as the test-set counterpart.", "labels": [], "entities": [{"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9988754391670227}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992856383323669}, {"text": "coverage", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9846766591072083}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9994277358055115}]}, {"text": "The precision of a method is equal to the multiplication of the two previous measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994619488716125}]}, {"text": "On average, DOP \"guesses\" in 88.82% (i.e. precision) of the cases exactly the same test-set parse; with EBL this becomes 86.77%, i.e. a loss of 2.05%.", "labels": [], "entities": [{"text": "DOP", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.7783806324005127}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9981581568717957}, {"text": "EBL", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.6758730411529541}]}, {"text": "The speedup is on average 3.1 times but, more importantly, the standard-deviation in processing time is less than a fifth.", "labels": [], "entities": [{"text": "speedup", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9728983640670776}]}, {"text": "On longer sentences, the speedup exceeds 6 times.", "labels": [], "entities": [{"text": "speedup", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.5077288746833801}]}, {"text": "shows the accumulative frequency of sentences to CPU-time: for x secs., the figure shows the number of sentences that take at least x secs.", "labels": [], "entities": []}, {"text": "If a deadline of 5 secs. is set beforehand, DOP misses around the 600 cases (of 3372) while the EBL misses less than 100 cases.", "labels": [], "entities": [{"text": "DOP", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.8950771689414978}, {"text": "EBL", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.6049557328224182}]}, {"text": "the figures are 263 to 23, and at 20 secs.", "labels": [], "entities": []}, {"text": "it's 116 to 6 cases respectively.", "labels": [], "entities": []}, {"text": "The version EBL0.75 shows similar learning capabilities to the EBL, (i.e. EBL1.0) version.", "labels": [], "entities": [{"text": "EBL0.75", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9702972769737244}, {"text": "EBL", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.8769652843475342}]}, {"text": "Its precision is slightly better with 87.26% and its coverage is virtually the same as DOP's.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9997296929359436}, {"text": "coverage", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9978048205375671}]}, {"text": "The EBL0.75 does not improve speedup though (actually it's slightly slower).", "labels": [], "entities": [{"text": "EBL0.75", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8636953830718994}, {"text": "speedup", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9845665097236633}]}, {"text": "The explanation to this behavior is simple: EBL0.75 does not seem to learn significantly many more rules than EBL1.0 and, during parsing, it gives up the assumption that PA-SSF borders are trustworthy.", "labels": [], "entities": [{"text": "EBL0.75", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8344413638114929}, {"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9645705819129944}]}, {"text": "This way it takes less risk but then it slightly loses speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9710243940353394}]}, {"text": "Again we conjecture that EBL0.75 would provide more speedup if local-context would be used during partial-parsing.", "labels": [], "entities": [{"text": "EBL0.75", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.7979495525360107}]}, {"text": "shows also the sizes of grammars which DOP projects with and without EBL.", "labels": [], "entities": []}, {"text": "The number of elementary-trees in the table for the Partial-Parser does not include the lexicon.", "labels": [], "entities": []}, {"text": "The sizes of the statistical grammars of DOP with EBL is about 1.2 times smaller than DOP's.", "labels": [], "entities": [{"text": "EBL", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7741295695304871}]}, {"text": "This is not the reduction which we hoped for, but it is quite evident that this is due to constraining the EBL mechanism; currently learning takes place only where local-context can be assumed of minor importance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Means and STDs of ten experiments (OVIS): Par denotes Partial-Parser", "labels": [], "entities": [{"text": "Means", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9921451210975647}, {"text": "Par", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9504943490028381}]}, {"text": " Table 2: Means and STDs of ten experiments (OVIS), ParPar denotes Partial-Parser", "labels": [], "entities": [{"text": "Means and STDs", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.7556380828221639}, {"text": "ParPar", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.951784610748291}]}]}