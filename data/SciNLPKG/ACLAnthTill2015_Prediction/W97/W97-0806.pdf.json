{"title": [{"text": "Integrating a Lexical Database and a Training Collection for Text Categoriza tion", "labels": [], "entities": [{"text": "Text Categoriza tion", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7463361620903015}]}], "abstractContent": [{"text": "Automatic text categorization is a complex and useful task for manynatural language processing applications.", "labels": [], "entities": [{"text": "Automatic text categorization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.65607221921285}]}, {"text": "Recent approaches to textcategorization focus more on algorithms than on resources involved in thisoperation.", "labels": [], "entities": []}, {"text": "In contrast to this trend, we present an approach based on the integration of widely available resources aslexical databases and training collections to overcome current limitationsof the task.", "labels": [], "entities": []}, {"text": "Our approach ~ makes use of Word-Net synonymy information toincrease evidence for bad trained categories.", "labels": [], "entities": []}, {"text": "When testing a direct categorization, a WordNet basedone, a training algorithm, and our integrated approach , the latter exhibitsa better perfomance than any of the others.", "labels": [], "entities": []}, {"text": "Incidentally, WordNet based approach perfomance is comparable with the trainingapproach one.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text categorization (TC) is the classification ofdocuments with respect to a set of one or more pre-existing categories.", "labels": [], "entities": [{"text": "Text categorization (TC)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8606364846229553}]}, {"text": "TCis a hard and very useful operation frequently applied to the assignment of subject categories to documents, toroute and filter texts, or as apart of natural language processingsystems.", "labels": [], "entities": []}, {"text": "In this paper we present an automatic TC approach based on theuse of several linguistic resources.", "labels": [], "entities": [{"text": "TC", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9808714985847473}]}, {"text": "Nowadays, many resources like trainingcollections and lexical databases have been successfully employed for text classificationtasks, but always in an isolated way.", "labels": [], "entities": [{"text": "text classificationtasks", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.8060738146305084}]}, {"text": "Thecurrent trend in the TC field is to pay more attention to algorithms thanto resources.", "labels": [], "entities": [{"text": "TC", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9887421727180481}]}, {"text": "We believe that the key idea for the improvement of text categorization is increasing theamount of information a system makes use of, through the integration ofseveral resources.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7208945751190186}]}, {"text": "We have chosen the Information Retrieval vector space model for ourapproach.", "labels": [], "entities": [{"text": "Information Retrieval vector space", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.8369351178407669}]}, {"text": "Term weight vectors are computed for documents and categoriesemploying the lexical database WordNet and the training subset of the testcollection Reuters-22173.", "labels": [], "entities": [{"text": "Term weight vectors", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9274921218554179}, {"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9617635011672974}, {"text": "Reuters-22173", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.76951003074646}]}, {"text": "We calculate the weight vectors for: 1 This research is supported by the Spanish Commttee of Sctence andTechnology (CICYT TIC94-0187).", "labels": [], "entities": [{"text": "Spanish Commttee of Sctence andTechnology (CICYT TIC94-0187)", "start_pos": 73, "end_pos": 133, "type": "DATASET", "confidence": 0.8655014832814535}]}, {"text": "_ A direct approach, _ a Wordnet based approach, _ a training collection approach, _ and finally, a technique for integrating WordNet and a training collection.", "labels": [], "entities": []}, {"text": "Later, we compare document-category similarity by means of a cosine-basedfunction.", "labels": [], "entities": []}, {"text": "We have driven a series of experiments on the test subset of Reuters-22173, which yields two conclusions.", "labels": [], "entities": [{"text": "test subset of Reuters-22173", "start_pos": 46, "end_pos": 74, "type": "DATASET", "confidence": 0.664861798286438}]}, {"text": "First, the integrated approach performs better than any of the other ones, confirming thehypothesis that the more informed a text classification system is, thebetter it performs.", "labels": [], "entities": [{"text": "text classification", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7623764872550964}]}, {"text": "Secondly, the lexical database oriented technique can rival with the training approach, avoiding the necessity ofcost-expensive building of training collections for any domain andclassification task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of TC and other text classification operations exhibits greatheterogeneity.", "labels": [], "entities": [{"text": "TC", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9340762495994568}, {"text": "text classification", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7045796662569046}]}, {"text": "Several metrics and test collections have been used fordifferent approaches or works.", "labels": [], "entities": []}, {"text": "This results in alack of comparability among the approaches,forcing to replicate experiments from other researchers.", "labels": [], "entities": []}, {"text": "Trying to minimize this problem, we havechosen a set of very extended metrics and a frequently used free testcollection for our work.", "labels": [], "entities": []}, {"text": "The metrics are recall and precision, and the testcollection is, as introduced before, Reuters-22173.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9995052814483643}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993138313293457}, {"text": "Reuters-22173", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9788140654563904}]}, {"text": "Before stepping into the actual results, we provide acloser look to these elements.", "labels": [], "entities": []}, {"text": "The VSM promotes recall and precision based evaluation, but there are several ways of calculating or even defining them.", "labels": [], "entities": [{"text": "VSM", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7838059663772583}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9983647465705872}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.994931161403656}]}, {"text": "Wefocus on recall, being the discussion analogous for precismn.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9889629483222961}]}, {"text": "First,definition can be given regarding categories or documents.", "labels": [], "entities": []}, {"text": "Second, computation can be done macroaveraging or micro-averaging.", "labels": [], "entities": []}, {"text": "_ Recall can be defined as the number of correctly assigned documents to a category over the number of documents to becorrectly assigned to the category.", "labels": [], "entities": [{"text": "Recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9417730569839478}]}, {"text": "But a document-oriented definition is also possible: the number of correctly assigned categories to adocument over the number of correct categories to be assigned to thedocument.", "labels": [], "entities": []}, {"text": "This later definition is more coherent with the task, but theformer allows to identify the most problematic categories.", "labels": [], "entities": []}, {"text": "_ Macro-averaging consists of computing recall and precision for every item (document or category) in one of both previous ways, and averaging aRer it.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9985268115997314}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9038066267967224}]}, {"text": "Micro-averaging is adding up all numbers of correctly assigned items, items assigned, and items to be assigned, and calculate only one value of recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9995525479316711}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9880536198616028}]}, {"text": "When micro-averaging, no distinction about document or category orientation can be made.", "labels": [], "entities": [{"text": "document or category orientation", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6461035460233688}]}, {"text": "Macro-averaging assigns equal weight to every category, while micro-averaging is influenced by most frequent categories.", "labels": [], "entities": []}, {"text": "Evaluation depends finally on the category assignment strategy: probabihty thresholding, k-per-doe assignment, etc.", "labels": [], "entities": []}, {"text": "Strategies define the way to produce recall/precision tables.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9934698343276978}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.8498415350914001}]}, {"text": "For instance, if similarities are normalized to the interval, eleven levels of prob- ROME, June 18 -Italy's overall balance of payments showed a deficit of 3,211 bllllon izre in May compared with a surplus of 2,040 billion in April, provxsional Bank of Italy figures how.", "labels": [], "entities": [{"text": "prob- ROME", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.575812021891276}, {"text": "provxsional Bank of Italy", "start_pos": 233, "end_pos": 258, "type": "DATASET", "confidence": 0.9179976433515549}]}, {"text": "The May deflclt compares with a surplus of 1,555 billion lire an the corresponding month of 1986.", "labels": [], "entities": [{"text": "May", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.5210724472999573}, {"text": "deflclt", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.41417407989501953}]}, {"text": "For the flrst five months of 1987, the overall balance of payments showed a surplus of 299 billlon lire agalnst a deficit of 2,854 billlon in the corresponding 1986 perlod.", "labels": [], "entities": []}, {"text": "REUTER ability threshold can beset to0.0, 0.1, and so.", "labels": [], "entities": [{"text": "REUTER ability threshold", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.9416345159212748}]}, {"text": "When the system performs k-per-doe assignment, the value of k is ranged from 1 to a reasonable maximum.", "labels": [], "entities": []}, {"text": "We must assign an unknown number of categories to each document in Reuters.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9053782224655151}]}, {"text": "So, the probabdity thresholding approach seems the most sensible one.", "labels": [], "entities": [{"text": "probabdity thresholding", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.6851753890514374}]}, {"text": "We have then computed recall and precision for eleven ,levels of threshold, both macro and micro-averaging.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9991205334663391}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9975419044494629}]}, {"text": "When macro-averaging, we have used the category-oriented definition of recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9991825222969055}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9909052848815918}]}, {"text": "After that, we have calculated averages of those eleven values in order to get single figures for comparison.", "labels": [], "entities": []}], "tableCaptions": []}