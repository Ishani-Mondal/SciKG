{"title": [], "abstractContent": [{"text": "Stochastic categorial grammars (SCGs) are introduced as a more appropriate formalism for statistical language learners to estimate than stochastic context free grammars.", "labels": [], "entities": [{"text": "Stochastic categorial grammars (SCGs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7495177835226059}]}, {"text": "As a vehicle for demonstrating SCG estimation, we show, in terms of crossing rates and in coverage, that when training material is limited, SCG estimation using the Minimum Description Length Principle is preferable to SCG estimation using an indifferent prior.", "labels": [], "entities": [{"text": "SCG estimation", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.9582429528236389}, {"text": "coverage", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9955490827560425}, {"text": "SCG estimation", "start_pos": 140, "end_pos": 154, "type": "TASK", "confidence": 0.918543815612793}]}], "introductionContent": [{"text": "Stochastic context free grammars (SCFGs), which are standard context free grammars extended with a probabilistic interpretation of the generation of strings, have been shown to model some sources with hidden branching processes more efficiently than stochastic regular grammars.", "labels": [], "entities": []}, {"text": "Furthermore, SCFGs can be automatically estimated using the Inside-Outside algorithm, which is guaranteed to produce a SCFG that is (locally) optimal.", "labels": [], "entities": []}, {"text": "Hence, SCFGs appear to be suitable formalisms for the estimation of widecovering grammars, capable of being used as part of a system that assigns logical forms to sentences.", "labels": [], "entities": []}, {"text": "Unfortunately, from a Natural Language Processing perspective, SCFGs are not appropriate grammars to learn.", "labels": [], "entities": []}, {"text": "Firstly, as Collins demonstrates, accurate parse selection, which is important for ambiguity resolution, requires lexical statistics.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.8442704677581787}, {"text": "ambiguity resolution", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7176375687122345}]}, {"text": "SCFGs, as standardly used in the InsideOutside algorithm, are in Chomsky Normal Form (CNF), which restricts rules to being at most binary branching.", "labels": [], "entities": []}, {"text": "Such rules are not lexicalised, and hence, to lexicalise (CNF) CFGs requires adding a complex statistical model that simulates the projection of head items up the parse tree.", "labels": [], "entities": []}, {"text": "Given the embryonic status of grammatical statistical models and the difficulties of accurately estimating the parameters of such a model, it seems more prudent to prefer whenever possible simpler statistical models with fewer parameters, and treat lexicalisation as part of the grammatical formalism, and not as part of the statistical framework (for example).", "labels": [], "entities": []}, {"text": "Secondly, (stochastic) CFGs are wellknown as being linguistically inadequate formalisms for problems such as non-constituent coordination.", "labels": [], "entities": []}, {"text": "Hence, a learner using a SCFG will not have an appropriate formalism with which to construct an adequate grammar.", "labels": [], "entities": []}, {"text": "Stochastic categorial grammars (SCGs), which are classical categorial grammars extended with a probabilistic component, by contrast, have a grammatical component that is naturally lexicalised.", "labels": [], "entities": [{"text": "Stochastic categorial grammars (SCGs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7538605431715647}]}, {"text": "Furthermore, Combinatory Categorial Grammars have been shown to account elegantly for problematic areas of syntax such as non-constituent co-ordination, and so it seems likely that SCGs, when suitably extended, will be able to inherit this linguistic adequacy.", "labels": [], "entities": []}, {"text": "We therefore believe that SCGs are more useful formalisms for statistical language learning than SCFGs.", "labels": [], "entities": []}, {"text": "Future work will reinforce the differences between SCFGs and SCGS, but in this paper, we instead concentrate upon the estimation of SCGs.", "labels": [], "entities": []}, {"text": "Stochastic grammars (of all varieties) are usually estimated using the Maximum Likelihood Principle, which assumes an indifferent prior probability distribution.", "labels": [], "entities": []}, {"text": "When there is sufficient training material, Maximum Likelihood Estimation (MLE) produces good results.", "labels": [], "entities": [{"text": "Maximum Likelihood Estimation (MLE", "start_pos": 44, "end_pos": 78, "type": "METRIC", "confidence": 0.7590024650096894}]}, {"text": "More usually however, with many thousands of parameters to estimate, there will be insufficient training material for MLE to produce an optimal solution.", "labels": [], "entities": [{"text": "MLE", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9710578918457031}]}, {"text": "If, instead, an informative prior is used in place of the indifferent prior, better results can be achieved.", "labels": [], "entities": []}, {"text": "In this paper we show how using an informative prior probability distribution We use the Minimum Description Length Principle (MDL) as the basis of our informative prior.", "labels": [], "entities": [{"text": "Minimum Description Length Principle (MDL)", "start_pos": 89, "end_pos": 131, "type": "METRIC", "confidence": 0.7404345571994781}]}, {"text": "To our knowledge, we know of no other papers comparing MDL to MLE using naturally occurring data and learning probabilistic grammars.", "labels": [], "entities": [{"text": "MLE", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.8432124257087708}]}, {"text": "For example, Stolcke's MDL-based learner was trained using artificial data; Chen's similar learner mixes smoothing techniques with MDL, thereby obfuscating the difference between MDL and MLE.", "labels": [], "entities": []}, {"text": "The structure of the rest of this paper is as follows.", "labels": [], "entities": []}, {"text": "In section 2 we introduce SCGs.", "labels": [], "entities": []}, {"text": "We then in section 3 present a problem facing most statistical learners known as over]itting.", "labels": [], "entities": []}, {"text": "Section 4 gives an overview of the MDL principle, which we use to deal with overfitting1; in section 5 we present our learner.", "labels": [], "entities": []}, {"text": "Following this, in section 6 we give some experiments comparing use of MDL, with a MLE-style learner.", "labels": [], "entities": []}, {"text": "The paper ends with some brief comments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here, we report on a number of experiments showing that when there is a danger of overfitting taking place, MDL produces a quantitatively better SCG than does MLE.", "labels": [], "entities": []}, {"text": "To evaluate the various lexica produced, we used the following metrics: \u2022 To measure a grammar's coverage, we note the number of tag sequences, drawn from a corpus of naturally occurring language, some grammar generates.", "labels": [], "entities": []}, {"text": "The higher the number, the better the grammar.", "labels": [], "entities": []}, {"text": "\u2022 To measure a grammar's overgeneration, we note the number of ungrammatical strings, drawn from a source that generates all strings up to some length randomly, a grammar generates.", "labels": [], "entities": []}, {"text": "The lower the number, the better the grammar.", "labels": [], "entities": []}, {"text": "That is, random sequences of tags, of a sufficient length, will have a low probability of being grammatically well-formed.", "labels": [], "entities": []}, {"text": "\u2022 To measure the accuracy of the parses pro-.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9995330572128296}]}, {"text": "duced, we use the Grammar Evaluation Interest Group scheme (GEIG) (.", "labels": [], "entities": []}, {"text": "This compares unlabelled, manually produced parses with automatically produced parses in terms of recall (the ratio of matched brackets overall brackets in the manually produced parses), precision (the ratio of matched brackets in the manually produced parse overall brackets found by the parser) and crossing rates (the number of times a bracketed sequence produced by the parser overlaps with one in the manually produced parse, but neither is properly contained in the other).", "labels": [], "entities": [{"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9994718432426453}, {"text": "precision", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9995567202568054}]}, {"text": "The higher the precision and recall, and the lower the crossing rates, the better the grammar.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996988773345947}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9993416666984558}]}, {"text": "Throughout our experiments, we used the Brill part-of-speech tagger to create testing and training material.", "labels": [], "entities": []}, {"text": "Our trigram model was created using seven million words of tagged material drawn from the British National Corpus (BNC); training material consisted of 43,000 tagged sentences also taken from the BNC.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 90, "end_pos": 119, "type": "DATASET", "confidence": 0.9700267414251963}, {"text": "BNC", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.9860111474990845}]}, {"text": "For test material, we took 429 sentences taken from the Spoken English Corpus (SEC).", "labels": [], "entities": [{"text": "Spoken English Corpus (SEC)", "start_pos": 56, "end_pos": 83, "type": "DATASET", "confidence": 0.7933357904354731}]}, {"text": "To compute crossing rates, recall and precision figures, we used a program called Parseval to compare most probable parses with manually produced parses (232 trees in total taken from the SEC) (.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9991180300712585}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9980619549751282}]}, {"text": "To measure overgeneration, we randomly generated 250 strings.", "labels": [], "entities": []}, {"text": "From a manual inspection, these do appear to be ungrammatical.", "labels": [], "entities": []}, {"text": "Here is an example randomly generated tag sequence:", "labels": [], "entities": []}], "tableCaptions": []}