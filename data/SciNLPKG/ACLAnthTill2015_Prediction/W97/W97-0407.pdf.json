{"title": [{"text": "Using Categories in the EUTRANS System", "labels": [], "entities": [{"text": "EUTRANS", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.918881893157959}]}], "abstractContent": [{"text": "The EUTRANS project, aims at developing Machine Translation systems for limited domain applications.", "labels": [], "entities": [{"text": "EUTRANS", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9354171752929688}, {"text": "Machine Translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8524966537952423}]}, {"text": "These systems accept speech and text input, and are trained using an example based approach.", "labels": [], "entities": []}, {"text": "The translation model used in this project is the Subsequential Transducer , which is easily integrable in conventional speech recognition systems.", "labels": [], "entities": []}, {"text": "In addition, Subsequential Transducers can be automatically learned from corpora.", "labels": [], "entities": []}, {"text": "This paper describes the use of categories for improving the EUTRANS translation systems.", "labels": [], "entities": [{"text": "EUTRANS translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7708660364151001}]}, {"text": "Experimental results with the task defined in the project show that this approach reduces the number of examples required for achieving good models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The EUTRANS project 1 (), funded by the European Union, aims at developing Machine Translation systems for limited domain applications.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8544895946979523}]}, {"text": "These systems accept speech and text input, and are trained using an example based approach.", "labels": [], "entities": []}, {"text": "The translation model used in this project is the Subsequential Transducer (SST), which is easily integrable in conventional speech recognition systems by using it both as language and translation model (.", "labels": [], "entities": [{"text": "Subsequential Transducer (SST", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6906831040978432}]}, {"text": "In addition, SSTs can be automatically learned from sentence aligned bilingual corpora.", "labels": [], "entities": [{"text": "SSTs", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9878204464912415}]}, {"text": "This paper describes the use of categories both in the training and translation processes for improving the EUTRANS translation systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9586925506591797}, {"text": "EUTRANS translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7457356154918671}]}, {"text": "The approach presented here improves that in, the integration of categories within the systems is simpler, and it allows for categories grouping units larger than a word.", "labels": [], "entities": []}, {"text": "Experimental results with the Traveler Task, defined in, show that this method reduces the number of examples required for achieving good models.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2 some basic concepts and the notation are introduced.", "labels": [], "entities": []}, {"text": "The technique used for integrating categories in the system is detailed in section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents the speech translation system.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7743116617202759}]}, {"text": "Both speech and text input experiments are described in section 5.", "labels": [], "entities": []}, {"text": "Finally, section 6 presents some conclusions and new directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our approach was tested with the three text corpora.", "labels": [], "entities": []}, {"text": "Each one was divided in training and test sets, with 490,000 and 10,000 pairs, respectively.", "labels": [], "entities": []}, {"text": "A sequence of models was trained with increasing subsets of the training set.", "labels": [], "entities": []}, {"text": "Each model was tested using only those sentences in the test set that were not seen in training.", "labels": [], "entities": []}, {"text": "This has been done because a model trained with OSTIA-DR is guaranteed to reproduce exactly those sentences it has seen during learning.", "labels": [], "entities": []}, {"text": "The performance was evaluated in terms of Word Error Rate (WER), which is the percentage of output words that has to be inserted, deleted and substituted for they to exactly match the corresponding expected translations.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.8823504249254862}]}, {"text": "The results for the three corpora can be seen on.", "labels": [], "entities": []}, {"text": "The columns labeled as \"Different\" and \"Categ.\", refer to the number of different sentences in the training set and the number of different sentences after categorization.", "labels": [], "entities": []}, {"text": "Graphical representations of the same results are on and 5.", "labels": [], "entities": []}, {"text": "As expected, the use of lexical categories had a major impact on the learning algorithm.", "labels": [], "entities": []}, {"text": "The differences in WER attributable to the use of lexical categories can be as high as about a 40% in the early stages of the learning process and decrease when the number of examples grows.", "labels": [], "entities": [{"text": "WER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7889489531517029}]}, {"text": "The large increase in performance is a natural consequence of the fact that the categories help in reducing the total variability that can be found in the corpora (although sentences do exhibit a great deal of variability, the underlying syntactic structure is actually much less diverse).", "labels": [], "entities": []}, {"text": "They also have the advantage of allowing an easier extension in the vocabulary of the task without having a negative effect on the performance of the models so obtained ().", "labels": [], "entities": []}, {"text": "A set of Spanish to English speaker independent translation experiments were performed integrating in our speech input system (as described in Figure 3: Evolution of translation WER with the size of the training set: Spanish to English text corpus.", "labels": [], "entities": [{"text": "Spanish to English speaker independent translation", "start_pos": 9, "end_pos": 59, "type": "TASK", "confidence": 0.6002330829699835}, {"text": "WER", "start_pos": 178, "end_pos": 181, "type": "METRIC", "confidence": 0.8527135848999023}]}, {"text": "The sizes in the horizontal axis refer to the first three columns in(a).", "labels": [], "entities": []}, {"text": "The phones were represented by context-independent continuousdensity HMMs.", "labels": [], "entities": []}, {"text": "Each HMM consisted of six states following a left-to-right topology with loops and skips.", "labels": [], "entities": []}, {"text": "The emission distribution of each state was modeled by a mixture of Gaussians.", "labels": [], "entities": []}, {"text": "Actually, there were only three emi~.", "labels": [], "entities": []}, {"text": "sion distributions per HMM since the states were tied in pairs (the first with the second, the third with the fourth, and the fifth with the sixth).", "labels": [], "entities": []}, {"text": "Details about the corpus used in training these models and its parametrization can be found in ().", "labels": [], "entities": []}, {"text": "\u2022 LEXICAL LEVEL Spanish Phonetics allows the representation of each word as a sequence of phones that can be derived from standard rules.", "labels": [], "entities": [{"text": "LEXICAL LEVEL", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.783207505941391}]}, {"text": "This sequence can be represented by a simple chain.", "labels": [], "entities": []}, {"text": "There were a total of 31 phones, including stressed and unstressed vowels plus two types of silence.", "labels": [], "entities": []}, {"text": "We used the best of the transducers obtained in the Spanish to English text experiments.", "labels": [], "entities": []}, {"text": "It was enriched with probabilities estimated by parsing the same training data with the final model and using relative frequencies of use as probability estimates.", "labels": [], "entities": []}, {"text": "The Viterbi search for the most likely path was speeded up by using beam search at two levels: independent beam widths were used in the states of the SST (empirically fixed to 300) and in the states of the HMMs.", "labels": [], "entities": []}, {"text": "Other details of the experiments can be found in ().", "labels": [], "entities": []}, {"text": "shows that good translation results (a WER of 6.4%) can be achieved with a Real Time Factor (RTF) of just 2.2.", "labels": [], "entities": [{"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9643275141716003}, {"text": "WER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9991294741630554}, {"text": "Real Time Factor (RTF)", "start_pos": 75, "end_pos": 97, "type": "METRIC", "confidence": 0.9128887355327606}]}, {"text": "It is worth noting that these results were obtained in a HP-9735 workstation without resorting to any type of specialised hardware or signal processing device.", "labels": [], "entities": []}, {"text": "When translation accuracy is the main concern, a more detailed acoustic model and a wider beam in the search can be used to achieve a WER of 1.9%, but with a RTF of 11.3.", "labels": [], "entities": [{"text": "translation", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.964852511882782}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.8618727922439575}, {"text": "WER", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9983343482017517}, {"text": "RTF", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.9976717829704285}]}], "tableCaptions": [{"text": " Table 2: Main features of the Spanish to English, Spanish to German and Spanish to Italian text  corpora.", "labels": [], "entities": []}, {"text": " Table 3: Text input results: Translation word error rates (WER) and sizes of the transducers for different  number of training pairs.", "labels": [], "entities": [{"text": "Translation word error rates (WER)", "start_pos": 30, "end_pos": 64, "type": "METRIC", "confidence": 0.859497036252703}]}, {"text": " Table 4: Speech input results:Translation word er- ror rates (WER) and real time factor (RTF) for  the best Spanish to English transducer.", "labels": [], "entities": [{"text": "Translation word er- ror rates (WER)", "start_pos": 31, "end_pos": 67, "type": "METRIC", "confidence": 0.8452017770873176}, {"text": "real time factor (RTF)", "start_pos": 72, "end_pos": 94, "type": "METRIC", "confidence": 0.924262116352717}]}]}