{"title": [{"text": "The Effects of Corpus Size and Homogeneity on Language Model Quality", "labels": [], "entities": [{"text": "Corpus Size", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.6259822696447372}]}], "abstractContent": [{"text": "Generic speech recognition systems typically use language models that are trained to cope with abroad variety of input.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7437709271907806}]}, {"text": "However, many recognition applications are more constrained, often to a specific topic or domain.", "labels": [], "entities": []}, {"text": "In cases such as these, a knowledge of the particular topic can be used to advantage.", "labels": [], "entities": []}, {"text": "This report describes the development of a number of techniques for augmenting domain-specific language models with data from a more general source.", "labels": [], "entities": []}, {"text": "The first concerns the problem of acquiring a suitable sample of the domain-specific language data from which to train the models.", "labels": [], "entities": []}, {"text": "The issue here is essentially one of quality, since it is shown that not all domain-specific corpora are equal.", "labels": [], "entities": []}, {"text": "Moreover, they can display significantly different characteristics that affect the quality of any language models built therefrom.", "labels": [], "entities": []}, {"text": "These characteristics are defined using a number of statistical measures, and their significance for language modelling is discussed.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.8356396853923798}]}, {"text": "The second investigation concerns the empirical development and evaluation of a set of language models for the task of email speech-u>-text dictation.", "labels": [], "entities": [{"text": "email speech-u>-text dictation", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.685714316368103}]}, {"text": "The issue here is essentially one of quantity, since it is shown that effective language models can be built from very modestly sized corpora, providing the training data matches the target appfication.", "labels": [], "entities": []}, {"text": "Evaluations show that a language model trained on only 2 million words can perform better than one trained on a corpus of over 100 times that size.", "labels": [], "entities": []}], "introductionContent": [{"text": "The development of robust speech recognition technology offers great potential for the design of improved interfaces to a wide range of applications.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.753931850194931}]}, {"text": "The current project concerns the development of one such application: the speech-to-text dictation of email messages.", "labels": [], "entities": [{"text": "speech-to-text dictation of email messages", "start_pos": 74, "end_pos": 116, "type": "TASK", "confidence": 0.8043596625328064}]}, {"text": "The work makes use of the Abbot recogniser, which is a eonnectionist/HMM continuous speech recognition system developed by the Connectionist Speech Group at Cambridge University.", "labels": [], "entities": [{"text": "Abbot recogniser", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.8140015304088593}, {"text": "HMM continuous speech recognition", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.48065541684627533}]}, {"text": "It is designed to recognise British English and American English, clearly spoken in a quiet acoustic environment).", "labels": [], "entities": []}, {"text": "The Abbot system is available with a vocabulary of 20,000 words, which means that anything spoken outside this vocabulary cannot be recognised (and therefore will be recognised as another word or string of words).", "labels": [], "entities": [{"text": "Abbot system", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9393999874591827}]}, {"text": "The vocabulary and grammar 0-aM) were optimised for the task of reading from a North American Business newspaper, in this case the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 131, "end_pos": 150, "type": "DATASET", "confidence": 0.9436120986938477}]}, {"text": "Some 227 million words of training text were used in building this LM and it is widely used throughout the speech community.", "labels": [], "entities": []}, {"text": "However, despite the size of the original training corpus, this LM was dearly not designed for the specific task of email dictation, so its performance is likely to be sub-optimal.", "labels": [], "entities": [{"text": "email dictation", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.7643801271915436}]}, {"text": "However, anew vocabulary and LM can This work was completed at HP Labs during a previous appoimmcnt funded by the Royal Academy of Engineering.", "labels": [], "entities": [{"text": "LM", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.954271137714386}]}, {"text": ".I I easily be created and then substituted for the one supplied.", "labels": [], "entities": []}, {"text": "The LMs described in this paper were all 'back-off trigram' LMs (, built using the CMU SLM toolkit.", "labels": [], "entities": [{"text": "CMU SLM toolkit", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.9328898787498474}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Similarity and homogenei W of BNC domains and email", "labels": [], "entities": []}, {"text": " Table 2. %Correct, accuracy, and perplexity of the language models", "labels": [], "entities": [{"text": "Correct", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9754059314727783}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9988239407539368}]}]}