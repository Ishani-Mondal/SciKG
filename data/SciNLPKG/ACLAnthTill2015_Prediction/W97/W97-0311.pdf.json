{"title": [{"text": "Automatic Discovery of Non-Compositional Compounds in Parallel Data *", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic segmentation of text into minimal content-bearing units is an unsolved problem even for languages like English.", "labels": [], "entities": [{"text": "Automatic segmentation of text", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7471928298473358}]}, {"text": "Spaces between words offer an easy first approximation , but this approximation is not good enough for machine translation (MT), where many word sequences are not translated word-for-word.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.811526745557785}]}, {"text": "This paper presents an efficient automatic method for discovering sequences of words that are translated as a unit.", "labels": [], "entities": []}, {"text": "The method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages.", "labels": [], "entities": []}, {"text": "It can discover hundreds of non-compositional compounds on each iteration , and constructs longer compounds out of shorter ones.", "labels": [], "entities": []}, {"text": "Objective evaluation on a simple machine translation task has shown the method's potential to improve the quality of MT output.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.7877263327439626}, {"text": "MT output", "start_pos": 117, "end_pos": 126, "type": "TASK", "confidence": 0.9209201037883759}]}, {"text": "The method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations .", "labels": [], "entities": [{"text": "word spellings and pronunciations", "start_pos": 122, "end_pos": 155, "type": "TASK", "confidence": 0.7447975128889084}]}], "introductionContent": [{"text": "The optimal way to analyze linguistic data into its primitive elements is rarely obvious but often crucial.", "labels": [], "entities": []}, {"text": "Identifying phones and words in speech has been a major focus of research.", "labels": [], "entities": [{"text": "Identifying phones and words in speech", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8537463049093882}]}, {"text": "Automatically finding words in text, the problem addressed here, is largely unsolved for languages such as Chinese and Thai, which are written without spaces * Many thanks to Mike Collins, Jason Eisner, Mitch Marcus and two anonymous reviewers for their feedback on earlier drafts of this paper.", "labels": [], "entities": []}, {"text": "This research was supported by an equipment grant from Sun MicroSystems and by ARPA Contract #N66001-94C-6043.", "labels": [], "entities": [{"text": "ARPA Contract #N66001-94C-6043", "start_pos": 79, "end_pos": 109, "type": "DATASET", "confidence": 0.6719879359006882}]}, {"text": "Spaces in texts of languages like English offer an easy first approximation to minimal content-bearing units.", "labels": [], "entities": []}, {"text": "However, this approximation mis-analyzes non-compositional compounds (NCCs) such as \"kick the bucket\" and \"hot dog.\"", "labels": [], "entities": []}, {"text": "NCCs are compound words whose meanings area matter of convention and cannot be synthesized from the meanings of their space-delimited components.", "labels": [], "entities": []}, {"text": "Treating NCCs as multiple words degrades the performance of machine translation (MT), information retrieval, natural language generation, and most other NLP applications.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8336596012115478}, {"text": "information retrieval", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.8372072577476501}, {"text": "natural language generation", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.7098347743352255}]}, {"text": "NCCs are usually not translated literally to other languages.", "labels": [], "entities": []}, {"text": "Therefore, one way to discover NCCs is to induce and analyze a translation model between two languages.", "labels": [], "entities": []}, {"text": "This paper is about an informationtheoretic approach to this kind of ontological discovery.", "labels": [], "entities": [{"text": "ontological discovery", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7935672402381897}]}, {"text": "The method is based on the insight that treatment of NCCs as multiple words reduces the predictive power of translation models.", "labels": [], "entities": []}, {"text": "Whether a given sequence of words is an NCC can be determined by comparing the predictive power of two translation models that differ on whether they treat the word sequence as an NCC.", "labels": [], "entities": []}, {"text": "Searching a space of data models in this manner has been proposed before, e.g. by and, but their particular methods have been limited by the computational expense of inducing data models and the typically vast number of potential NCCs that need to be tested.", "labels": [], "entities": []}, {"text": "The method presented here overcomes this limitation by making independence assumptions that allow hundreds of NCCs to be discovered from each pair of induced translation models.", "labels": [], "entities": []}, {"text": "It is further accelerated by heuristics for gauging the a priori likelihood of validation for each candidate NCC.", "labels": [], "entities": []}, {"text": "The predictive power of a translation model depends on what the model is meant to predict.", "labels": [], "entities": []}, {"text": "This paper considers two different applications of trans-lation models, and their corresponding objective functions.", "labels": [], "entities": []}, {"text": "The different objective functions lead to different mathematical formulations of predictive power, different heuristics for estimating predictive power, and different classifications of word sequences with respect to compositionality.", "labels": [], "entities": []}, {"text": "Monolingual properties of NCCs are not considered by either objective function.", "labels": [], "entities": []}, {"text": "So, the method will not detect phrases that are translated word-for-word despite non-compositional semantics, such as the English metaphors \"ivory tower\" and \"banana republic,\" which translate literally into French.", "labels": [], "entities": []}, {"text": "On the other hand, the method will detect word sequences that are often paraphrased in translation, but have perfectly compositional meanings in the monolingual sense.", "labels": [], "entities": []}, {"text": "For example, \"tax system\" is most often translated into French as \"r6gime fiscale.\"", "labels": [], "entities": []}, {"text": "Each new batch of validated NCCs raises the value of the objective function for the given application, as demonstrated in Section 8.", "labels": [], "entities": []}, {"text": "You can skip ahead to fora random sample of the NCCs that the method validated for use in a machine translation task.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8295996189117432}]}, {"text": "The NCC detection method makes some assumptions about the properties of statistical translation models, but no assumptions about the data from which the models are constructed.", "labels": [], "entities": [{"text": "NCC detection", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7142727971076965}]}, {"text": "Therefore, the method is applicable to parallel data other than parallel texts.", "labels": [], "entities": []}, {"text": "For example, Section 8 applies the method to orthographic and phonetic representations of English words to discover the NCCs of English orthography.", "labels": [], "entities": []}], "datasetContent": [{"text": "To demonstrate the method's applicability to data other than parallel texts, and to illustrate some of its interesting properties, I describe my last experiment first.", "labels": [], "entities": []}, {"text": "I applied the mutual information objective function and its associated predictive value function to a data set consisting of spellings and pronunciations of 17381 English words.", "labels": [], "entities": []}, {"text": "the NCCs of English spelling that the algorithm discovered on the first 10 iterations.", "labels": [], "entities": []}, {"text": "The table reveals some interesting behavior of the algorithm.", "labels": [], "entities": []}, {"text": "The NCCs \"er,\" \"ng\" and \"ow\" were validated because this data set represents the sounds usually produced by these letter combinations with one phoneme.", "labels": [], "entities": [{"text": "er", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.6078559160232544}]}, {"text": "The NCC \"es\" most often appears in word-final position, where the \"e\" is silent.", "labels": [], "entities": [{"text": "NCC \"es\"", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7904179245233536}]}, {"text": "However, when \"es\" is not word-final, the \"e\" is usually not silent, and the most frequent following letter is \"s\", which is why the NCC \"ess\" was validated.", "labels": [], "entities": [{"text": "NCC \"ess\"", "start_pos": 133, "end_pos": 142, "type": "DATASET", "confidence": 0.8399701714515686}]}, {"text": "NCCs like \"tio\" and \"ough\" are built up over multiple iterations, sometimes out of pairs of previously discovered NCCs.", "labels": [], "entities": []}, {"text": "The other two experiments were carried out on transcripts of Canadian parliamentary debates, known as the Hansards.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.975016176700592}]}, {"text": "French and English versions of these texts were aligned by sentence using the method of.", "labels": [], "entities": []}, {"text": "Morphological variants in both languages were stemmed to a canonical form.", "labels": [], "entities": []}, {"text": "Thirteen million words (in both languages combined) were used for training and another two and a half million were used for testing.", "labels": [], "entities": []}, {"text": "All translation models were induced using the method of.", "labels": [], "entities": []}, {"text": "Six iterations of the NCC discovery algorithm were run in \"two-sided\" mode, using the objective function I, and five iterations were run using the objective function V. chart the NCC discovery process.", "labels": [], "entities": [{"text": "NCC discovery", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.8031546175479889}, {"text": "NCC discovery", "start_pos": 179, "end_pos": 192, "type": "TASK", "confidence": 0.7877383530139923}]}, {"text": "The NCCs proposed for the V objective function were much more likely to be validated than those proposed for I, because the predictive value function v ~ is much easier to estimate a priori than the predictive value function iq In 3 iterations on the English side of the bitext, 192 NCCs were validated for I and 1432 were validated for V.", "labels": [], "entities": []}, {"text": "Of the 1432 NCCs validated for V, 84 NCCs consisted of 3 words, 3 consisted of 4 words and 2 consisted of 5 words.", "labels": [], "entities": [{"text": "V", "start_pos": 31, "end_pos": 32, "type": "TASK", "confidence": 0.9529210925102234}]}, {"text": "The French NCCs were longer on average, due to the frequent \"N de N\" construction for noun compounds.", "labels": [], "entities": [{"text": "French NCCs", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9331678748130798}]}, {"text": "The first experiment on the Hansards involved the mutual information objective function I and its associated predictive value function in Equation 3.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9557514190673828}]}, {"text": "The first step in the experiment was the construction of 5 new versions of the test data, in addition to the original version.", "labels": [], "entities": []}, {"text": "Version k of the test data was constructed by fusing all NCCs validated up to iteration k on the training data.", "labels": [], "entities": []}, {"text": "The second step was to induce a translation model from each version of the test data.", "labels": [], "entities": []}, {"text": "There was no opportunity to measure the impact of NCC recognition under the objective function I on any real application, but shows that the mutual information of successive test translation models rose as desired.", "labels": [], "entities": [{"text": "NCC recognition", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8580646812915802}]}, {"text": "The recognition on the bag-of-words translation task was measured directly, using Bitext-Based Lexicon Evaluation (BIBLE:).", "labels": [], "entities": [{"text": "bag-of-words translation task", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7432852188746134}, {"text": "BIBLE", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.634906530380249}]}, {"text": "BIBLE is a family of evaluation algorithms for comparing different translation methods objectively and automatically.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7621205449104309}]}, {"text": "The algorithms are based on the observation that if translation method A is better than translation method B, and each method produces a translation from one half of a held-out test bitext, then the other half of that bitext will be more similar to the translation produced by A than to the translation produced by B. In the present experiment, the trans- lation method was always bag-of-words translation, but using different translation models.", "labels": [], "entities": []}, {"text": "The similarity of two texts was measured in terms of word precision and word recall in aligned sentence pairs, ignoring word order.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.811355471611023}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9292282462120056}]}, {"text": "I compared the 6 base translation models induced in 6 iterations of the algorithm in Section 5. 5 The first model is numbered 0, to indicate that it did not recognize any NCCs.", "labels": [], "entities": []}, {"text": "The 6 translation models were evaluated on the test bitext (E, F) using the following BIBLE algorithm: 1.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9930247068405151}]}, {"text": "Fuse all word sequences in E that correspond to NCCs recognized by the translation model.", "labels": [], "entities": []}, {"text": "2. Initialize the counters a and c to zero.", "labels": [], "entities": []}, {"text": "3. Let b be the number of words in F.", "labels": [], "entities": []}, {"text": "5The entire algorithm was only run six times, but Steps 2 and 3 were run a seventh time.", "labels": [], "entities": []}, {"text": "The BIBLE algorithm compared the 6 models in both directions of translation.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.7716672420501709}]}, {"text": "The results are detailed in Figures 4 and 5.", "labels": [], "entities": []}, {"text": "shows F-measures that are standard in the information retrieval literature: 2 * precision * recall The absolute recall and precision values in these figures are quite low, but this is not a reflection of the quality of the translation models.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9827690720558167}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9989308714866638}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9471501708030701}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.8039935827255249}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9927330017089844}]}, {"text": "Rather, it is an expected outcome of BIBLE evaluation, which is quite harsh.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.6295803189277649}]}, {"text": "Many translations are not word for word in real bitexts and BIBLE does not even give credit for synonyms.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.7078288197517395}]}, {"text": "The best possible performance on this 6Removing words from fin Step 3(c) is necessary to ensure that no target word gives credit to more than one source word translation, and thereby to foil a simple method of cheating: If matched words inf are not removed, then a trivial translation model where all source words translate to the most frequent target word would score surprisingly high!", "labels": [], "entities": []}, {"text": "E.g. a French to English translation method that outputs \"the the the the...\" would recall more than 6% of English words.", "labels": [], "entities": [{"text": "French to English translation", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.7124767154455185}]}, {"text": "kind of BIBLE evaluation has been estimated at 62% precision and 60% recall.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.8980877995491028}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9997401833534241}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9993964433670044}]}, {"text": "The purpose of BIBLE is internally valid comparison, rather than externally valid benchmarking.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.7459172606468201}]}, {"text": "On a sufficiently large test bitext, BIBLE can expose the slightest differences in translation quality.", "labels": [], "entities": [{"text": "BIBLE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9535946249961853}]}, {"text": "The number of NCCs validated on each iteration was nevermore than 2.5% of the vocabulary size.", "labels": [], "entities": []}, {"text": "Thus, the curves in have a very small range, but the trends are clear.", "labels": [], "entities": []}, {"text": "A qualitative assessment of the NCC discovery method can be made by looking at.", "labels": [], "entities": [{"text": "NCC discovery", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.6997271478176117}]}, {"text": "It contains a random sample of 50 of the English NCCs accumulated in the first five iterations of the algorithm in Section 5, using the simpler objective function V.", "labels": [], "entities": [{"text": "English NCCs", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8857668936252594}]}, {"text": "All of the NCCs in the table are noncompositional with respect to the objective function V.", "labels": [], "entities": []}, {"text": "Many of the NCCs, like \"red tape\" and \"blaze the trail,\" are true idioms.", "labels": [], "entities": []}, {"text": "E.g. \"flow-\" has not yet been recognized as a non-compositional part of \"flow-through share,\" and likewise for \"head\" in \"rear its ugly head.\"", "labels": [], "entities": []}, {"text": "These NCCs would likely be completed if the algorithm were allowed to run for more iterations.", "labels": [], "entities": []}, {"text": "Some of the other entries deserve more explanation.", "labels": [], "entities": []}, {"text": "First, \"Della Noce\" is the last name of a Canadian Member of Parliament.", "labels": [], "entities": []}, {"text": "Every occurrence of this name in the French training text was tokenized as \"Della noce\" with a lowercase \"n,\" because \"noce\" is a common noun in French meaning \"marriage,\" and the tokenization algorithm lowercases all capitalized words that are found in the lexicon.", "labels": [], "entities": []}, {"text": "When this word occurs in the French text without \"Della,\" its English translation is \"marriage,\" but when it occurs as part of the name, its translation is \"Noce.\"", "labels": [], "entities": []}, {"text": "So, the French bigram \"Della Noce\" is noncompositional with respect to the objective function V.", "labels": [], "entities": [{"text": "French bigram \"Della Noce", "start_pos": 8, "end_pos": 33, "type": "DATASET", "confidence": 0.8178494334220886}]}, {"text": "It was validated as an NCC.", "labels": [], "entities": [{"text": "NCC", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8882529735565186}]}, {"text": "On a subsequent iteration, the algorithm found that the English bigram \"Della Noce\" was always linked to one French word, the NCC \"Dellamoce,\" so it decided that the English \"Della Noce\" must also bean NCC.", "labels": [], "entities": [{"text": "NCC", "start_pos": 202, "end_pos": 205, "type": "DATASET", "confidence": 0.9761255383491516}]}, {"text": "This is one of the few non-compositional personal names in the Hansards.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9264205098152161}]}, {"text": "Another interesting entry in the table is the last one.", "labels": [], "entities": []}, {"text": "The capitalized English words \"Generic\" and \"Association\" are translated with perfect consistency to \"Generic\" and \"association,\" respectively, in the training text.", "labels": [], "entities": [{"text": "consistency", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.97881019115448}]}, {"text": "The translation of the middle two words, however, is non-compositional.", "labels": [], "entities": []}, {"text": "When \"Pharmaceutical\" and \"Industry\" occur together, they are rendered in the French text without translation as \"Pharmaceutical Industry.\"", "labels": [], "entities": []}, {"text": "When they occur separately, they are translated into \"pharmaceutique\" and \"industrie.\"", "labels": [], "entities": []}, {"text": "Thus, the English bigram \"Pharmaceutical Industry\" is an NCC, but the words that always occur around it are not part of the NCC.", "labels": [], "entities": [{"text": "NCC", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.9471940398216248}]}, {"text": "Similar reasoning applies to \"ship unprocesseduranium.\"", "labels": [], "entities": []}, {"text": "The bigram < ship, unprocessed > is an NCC because its components are translated noncompositionally whenever they co-occur.", "labels": [], "entities": []}, {"text": "However, \"uranium\" is always translated as \"uranium,\" so it is not apart of the NCC.", "labels": [], "entities": [{"text": "NCC", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9808583855628967}]}, {"text": "This NCC demonstrates that valid NCCs may cross the boundaries of grammatical constituents.", "labels": [], "entities": []}, {"text": "The heuristics in Section 6 are designed specifically to find the interesting features in that featureless desert.", "labels": [], "entities": []}, {"text": "Furthermore, translational equivalence relations involving explicit representations of targetlanguage NCCs are more useful than fertility distributions for applications that do translation by table lookup.", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.9077125191688538}]}, {"text": "Many authors (e.g.) define \"collocations\" in terms of monolingual frequency and part-of-speech patterns.", "labels": [], "entities": []}, {"text": "Markedly high frequency is a necessary property of NCCs, because otherwise they would fallout of use.", "labels": [], "entities": []}, {"text": "However, at least for translationrelated applications, it is not a sufficient property.", "labels": [], "entities": [{"text": "translationrelated", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9851322770118713}]}, {"text": "Non-compositional translation cannot be detected reliably without looking at translational distributions.", "labels": [], "entities": []}, {"text": "The deficiency of criteria that ignore translational distributions is illustrated by their propensity to validate most personal names as \"collocations.\"", "labels": [], "entities": []}, {"text": "At least among West European languages, translations of the vast majority of personal names are perfectly compositional.", "labels": [], "entities": []}, {"text": "Several authors have used mutual information and similar statistics as an objective function for word clustering, for automatic determination of phonemic baseforms, and for language modeling for speech recognition.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.775383323431015}, {"text": "speech recognition", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.7955266535282135}]}, {"text": "Although the applications considered in this paper are different, the strategy is similar: search a space of data models for the one with maximum predictive power.", "labels": [], "entities": []}, {"text": "also employ parallel texts and independence assumptions that are similar to those described in Section 6.", "labels": [], "entities": []}, {"text": "Like, they report a modest improvement in model perplexity and encouraging qualitative results.", "labels": [], "entities": []}, {"text": "Unfortunately, their estimation method cannot propose more than tenor so word-pair clusters before the translation model must be re-estimated.", "labels": [], "entities": []}, {"text": "Also, the particular clustering method that they hoped to improve using parallel data is not very robust for low frequencies.", "labels": [], "entities": []}, {"text": "So, like Smadja et al., they were forced to ignore all words that occur less than five times.", "labels": [], "entities": []}, {"text": "If appropriate objective functions and predictive value functions can be found for these other tasks, then the method in this paper might be applied to them.", "labels": [], "entities": []}, {"text": "There has been some research into matching compositional phrases across bitexts.", "labels": [], "entities": [{"text": "matching compositional phrases across bitexts", "start_pos": 34, "end_pos": 79, "type": "TASK", "confidence": 0.830623722076416}]}, {"text": "For example, presented a method for finding translations of whole noun phrases.", "labels": [], "entities": [{"text": "finding translations of whole noun phrases", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.8016615211963654}]}, {"text": "showed how to use an existing translation lexicon to populate a database of \"phrasal correspondences\" for use in example-based MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.8988096117973328}]}, {"text": "These compositional translation patterns enable more sophisticated approaches to MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9962466359138489}]}, {"text": "However, they are only useful if they can be discovered reliably and efficiently.", "labels": [], "entities": []}, {"text": "Their time may come when we have a better understanding of how to model the human translation process.", "labels": [], "entities": [{"text": "human translation process", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.675536056359609}]}], "tableCaptions": [{"text": " Table 2: NCCs proposed and accepted, using the mutual information objective function I.", "labels": [], "entities": []}, {"text": " Table 3: NCCs proposed and accepted, using the simpler objective function V.", "labels": [], "entities": []}]}