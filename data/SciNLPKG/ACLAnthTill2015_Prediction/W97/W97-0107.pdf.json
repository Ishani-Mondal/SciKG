{"title": [{"text": "Reestimation and Best-First Parsing Algorithm for Probabilistic Dependency Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a reesthnation algorithm and a best-first parsing (BFP) algorithm for probabilistic dependency grummars (PDG).", "labels": [], "entities": [{"text": "best-first parsing (BFP)", "start_pos": 51, "end_pos": 75, "type": "METRIC", "confidence": 0.5814171731472015}]}, {"text": "The proposed reestimation algorithm is a variation of the inside-outside algorithm adapted to probabilistic dependency grammars.", "labels": [], "entities": []}, {"text": "The inside-outside algorithm is a probabilistic parameter reestimation algorithm for phrase structure grammars in Chomsky Normal Form (CNF).", "labels": [], "entities": [{"text": "phrase structure grammars in Chomsky Normal Form (CNF)", "start_pos": 85, "end_pos": 139, "type": "TASK", "confidence": 0.690468966960907}]}, {"text": "Dependency grammar represents a sentence structure as a set of dependency links between arbitrary two words in the sentence, and cannot be reestimated by the inside-outside algorithm directly.", "labels": [], "entities": [{"text": "Dependency grammar", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7618158161640167}]}, {"text": "In this paper, non-constituent objects, complete-llnk and complete-sequence are defined as basic units of dependency structure, and the probabilities of them are rees-timated.", "labels": [], "entities": []}, {"text": "The reestimation and BFP algorithms utilize CYK-style chart and the non-constituent objects as chart entries.", "labels": [], "entities": [{"text": "BFP", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9901594519615173}]}, {"text": "Both algoritbrn~ have O(n s) time complexities.", "labels": [], "entities": [{"text": "O", "start_pos": 22, "end_pos": 23, "type": "METRIC", "confidence": 0.9839527010917664}]}], "introductionContent": [{"text": "There have been many efforts to induce grammars automatically from corpus by utilizing the vast amount of corpora with various degrees of annotations.", "labels": [], "entities": []}, {"text": "Corpus-based, stochastic grammar induction has many profitable advantages such as simple acquisition and extension of linguistic knowledges, easy treatment of ambiguities by virtue of its innate scoring mechanism, and fail-soi~ reaction to ill-formed or extra-grammatical sentences.", "labels": [], "entities": [{"text": "stochastic grammar induction", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.7062986890474955}]}, {"text": "Most of corpus-based grammar inductions have concentrated on phrase structure gram\u00b0 mars.", "labels": [], "entities": [{"text": "corpus-based grammar inductions", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6662493546803793}]}, {"text": "The typical works on phrase structure grammar induction are as follows): (1) generating all the possible rules, (2) reestimating the probabilities of rules using the inside-outside algorithm, and (3) finally finding a stable grammar by eliminating the rules which have probability values close to 0.", "labels": [], "entities": [{"text": "phrase structure grammar induction", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.8521291017532349}]}, {"text": "Generating all the rules is done by restricting the number of nonterminals and/or the number of the right hand side symbols in the rules and enumerating all the possible combinations.", "labels": [], "entities": []}, {"text": "Chen extracts rules by some heuristics and reestimates the probabilities of rules using the inside-outside algorithm.", "labels": [], "entities": []}, {"text": "The inside-outside algorithm learns a grammar by iteratively adjusting the rule probabilities to minimize the training corpus entropy.", "labels": [], "entities": []}, {"text": "It is extensively used as reestimation algorithm for phrase structure grammars.", "labels": [], "entities": [{"text": "phrase structure grammars", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.8326995968818665}]}, {"text": "Most of the works on phrase structure grammar induction, however, have partially succeeded.", "labels": [], "entities": [{"text": "phrase structure grammar induction", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.8959862738847733}]}, {"text": "Estimating phrase structure grammars by minimizing the training corpus on-tropy does not lead to the desired grammars which is consistent with human intuitions (.", "labels": [], "entities": [{"text": "phrase structure grammars", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.6991316676139832}]}, {"text": "To increase the correctness of the learned grammar, Marcken proposed to include lexical information to the phrase structure grammar.", "labels": [], "entities": [{"text": "phrase structure grammar", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.747857411702474}]}, {"text": "A recent trend of parsing is also to include lexiccal information to increase the correctness.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9871854186058044}]}, {"text": "This means that the lack of lexical information in phrase structure grammar is a major weak point for syntactic disambiguation.", "labels": [], "entities": [{"text": "phrase structure grammar", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.8184865514437357}, {"text": "syntactic disambiguation", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.8617497682571411}]}, {"text": "Besides the lack of lexical information, the induction of phrase structure grnmmar may suffer from structural data sparseness with medium sized training corpus.", "labels": [], "entities": []}, {"text": "The structural data sparseness means the lack of information on the grammar rules.", "labels": [], "entities": []}, {"text": "An approach to increase the correctness of grammar induction is to learn a grammar from a tree-tagged corpus or bracketed corpus.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7557671964168549}]}, {"text": "But the construction of vast sized tree-corpus or bracketed corpus is very labour-intensive and manual construction of such corpus may produce serious inconsistencies.", "labels": [], "entities": []}, {"text": "And the structural-data sparseness problem still remains.", "labels": [], "entities": []}, {"text": "The problems of structural-data sparseness and lack of lexical information can be lessened with PDG.", "labels": [], "entities": []}, {"text": "Dependency grammar defines a language as a set of dependency relations between any two words.", "labels": [], "entities": [{"text": "Dependency grammar", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8592656850814819}]}, {"text": "The basic units of sentence structure in DG, the dependency relations are much simpler than the rules in phrase structure grnmmar.", "labels": [], "entities": []}, {"text": "So, the search space of dependency grammar maybe smaller and the grammar induction maybe less affected by the structural-data sparseness.", "labels": [], "entities": []}, {"text": "Dependency grammar induction has been studied by).", "labels": [], "entities": [{"text": "Dependency grammar induction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8519001007080078}]}, {"text": "In the works, however, the dependency grammar was rather a restricted form of phrase structure grarnrnarss.", "labels": [], "entities": []}, {"text": "Accordingly, they extensively used the inside-outside algorithm to reestimate the grnmmnr and have the same problem of structural-data sparseness.", "labels": [], "entities": []}, {"text": "In this paper, we propose a reestimation algorithm and a best-first parsing algorithm for PDG.", "labels": [], "entities": []}, {"text": "The reestimation algorithm is a variation of the inside-outside algorithm adapted to PDG.", "labels": [], "entities": [{"text": "reestimation", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.8735314607620239}]}, {"text": "The inside-outside algorithm is a probabilistic parameter reestimation algorithm for phrase structure grammars in CNF and thus cannot be directly used for reestimation of probabilistic dependency grammnrs.", "labels": [], "entities": []}, {"text": "We define non-constituent objects, complete-link and complete-sequence as basic units of dependency structure.", "labels": [], "entities": []}, {"text": "Both of reestimation algorithm and best-first parsing algorithm utilize a CYK-style chart and the non-constituent objects as chart entries.", "labels": [], "entities": []}, {"text": "Both algorithms have O(n s) time complexities.", "labels": [], "entities": [{"text": "O", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.9767389297485352}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 defines the basic units and describes best-first parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.8930656909942627}]}, {"text": "Section 3 describes the reestimation algorithm.", "labels": [], "entities": []}, {"text": "Section 4 shows the experimental results of reestimation algorithm on Korean and finally section 5 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiment of the reestimation algorithm on Korean language.", "labels": [], "entities": []}, {"text": "Korean is a partially ordered language in which the head words are always placed to the right of their dependent words.", "labels": [], "entities": []}, {"text": "Under such restriction on word order, an abstract dependency structure for Korean is depicted in.", "labels": [], "entities": []}, {"text": "Korean sentence is spaced by \"word-phrases\" which is a sequence of content word and its functional words.", "labels": [], "entities": []}, {"text": "In this experiment, the final part of speech(POS) of a \"word-phrase\" is selected as the representative POS and the inter-word-phrase dependencies are reestimated.", "labels": [], "entities": []}, {"text": "We used 54 POS set.", "labels": [], "entities": []}, {"text": "The initial probabilities of all the possible dependencies were set equal.", "labels": [], "entities": []}, {"text": "The experiment was performed on three kinds of training and test sets extracted from The experiment result shows that the proposed reestimation algorithm converges to a (local) minimum entropy.", "labels": [], "entities": []}, {"text": "It shows also that the train and test entropies are not affected much by the domain nor by the size of training corpus.", "labels": [], "entities": []}, {"text": "It maybe because the reestimation was done on inter-POS dependencies, which is relatively simple.", "labels": [], "entities": []}, {"text": "If the reestimation would be done on the dependencies between POS sequences for ~word-phrase\" or on the dependencies between lexical entities~ the entropies maybe affected much by the domain and the size of corpus.", "labels": [], "entities": []}, {"text": "Below we show the parsing results of two example Korean sentences.", "labels": [], "entities": []}, {"text": "We used the proposed best-first parsing algorithm to find the most probable parse of each sentence.", "labels": [], "entities": []}, {"text": "The inter-word-phrase probabilities used for parsing are the reestimated ones for the training set-3.", "labels": [], "entities": [{"text": "parsing", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9697059392929077}]}, {"text": "To the right of each Korean word-phrase, the meaning of it in English is given in the square brackets.", "labels": [], "entities": []}, {"text": "In the parse representations, each individual inter-word-phrase probability is given to the right of the dependent word-phrase.", "labels": [], "entities": []}, {"text": "The probability of each parse is the product of all the inter-word-phrase probabilities in the parse and is given on the top of each parse.", "labels": [], "entities": []}, {"text": "Input Sentence: PDG induction.", "labels": [], "entities": [{"text": "PDG induction", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.5948702394962311}]}, {"text": "By experiment on Korean: we have shown that the reestimation algorithm converges to a local minimum and constitute a stable grammar.", "labels": [], "entities": []}, {"text": "Compared to phrase structure grammars, PDG can be a useful and practical scheme for parsing model and language model.", "labels": [], "entities": [{"text": "phrase structure grammars", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.7534375985463461}, {"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9707528948783875}]}, {"text": "It is because dependency tree is much simpler and easily understood than the structure constructed by the phrase structure grammars.", "labels": [], "entities": []}, {"text": "Besides the search space of the grammar maybe smaller and the effect of structural data sparseness maybe less.", "labels": [], "entities": []}, {"text": "This also means that the reestimation algorithm for PDG can converge with smaller training corpus.", "labels": [], "entities": []}, {"text": "We are planning to evaluate the parsing model based on the reestimated PDG and the PDG-based language model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9737352132797241}, {"text": "reestimated", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.963074803352356}, {"text": "PDG", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.5392519235610962}]}], "tableCaptions": [{"text": " Table 1: Train and test entropies", "labels": [], "entities": [{"text": "entropies", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.5200093388557434}]}]}