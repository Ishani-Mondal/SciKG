{"title": [{"text": "I I I I I, I l Grammar Acquisition Based on Clustering Analysis and Its Application to Statistical Parsing", "labels": [], "entities": [{"text": "Grammar", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9892309308052063}, {"text": "Acquisition", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.5074568390846252}, {"text": "Statistical Parsing", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8248862326145172}]}], "abstractContent": [{"text": "This paper proposes anew method for learning a context-sensitive conditional probability context-free grammar from an unlabeled bracketed corpus based on clustering analysis and describes a natural language parsing model which uses a probability-based scoring function of the grammar to rank parses of a sentence.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 190, "end_pos": 214, "type": "TASK", "confidence": 0.6985190908114115}]}, {"text": "By grouping brackets in s corpus into a number of sire;far bracket groups based on their local contextual information, the corpus is automatically labeled with some nonterm~=a] labels, and consequently a grammar with conditional probabilities is acquired.", "labels": [], "entities": []}, {"text": "The statistical parsing model provides a framework for finding the most likely parse of a sentence based on these conditional probabilities.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6720572113990784}]}, {"text": "Experiments using Wall Street Journal data show that our approach achieves a relatively high accuracy: 88 % recaJ1, 72 % precision and 0.7 crossing brackets per sentence for sentences shorter than 10 words, and 71 ~ recall, 51 ~0 precision and 3.4 crossing brackets for sentences between 10-19 words.", "labels": [], "entities": [{"text": "Wall Street Journal data", "start_pos": 18, "end_pos": 42, "type": "DATASET", "confidence": 0.9661414474248886}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9991864562034607}, {"text": "recaJ1", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9113538265228271}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9977973699569702}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.997805655002594}, {"text": "precision", "start_pos": 230, "end_pos": 239, "type": "METRIC", "confidence": 0.9642608761787415}]}, {"text": "This result supports the assumption that local contextual statistics obtained from an unlabeled bracketed corpus are effective for learnln~ a useful grammar and parsing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most natural language processing systems utilize grammars for parsing sentences in order to recognize their structure and finally to understand their meaning.", "labels": [], "entities": []}, {"text": "Due to the ,l~mculty and complexity of constructing a grammar by hand, there were several approaches developed fora, uton~tically training grammars from a large corpus with some probabilistic models.", "labels": [], "entities": []}, {"text": "These methods can be characterized by properties of the corpus they used, such as whether it includes information of brackets, lexical ]abels, nontermlnsl labels and soon.", "labels": [], "entities": []}, {"text": "Recently several parsed corpora which include full bracketing, tagging and nonterm~l labels have been available for researchers to use for constructing a probaMlistic grammar.", "labels": [], "entities": []}, {"text": "Most researches on these grammars calcuLzte statistics of a grammar from a fullyparsed corpus with nonterm;nal lsbeis and apply them to rank the possible parses of a sentence.", "labels": [], "entities": []}, {"text": "While these researches report some promising results, due to the cost of corpus construction, it still seems worth inferring a probabilistic grammar from corpora with less information, such as ones without bracketing and/or nonterm~al labels, and use it for parsing.", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7340769171714783}]}, {"text": "Unlike the way to annotate bracketings for corpora by hand, the hand-annotation of nonterm~nal labels need a process that a corpus builder have to determine types of nonterm~nal labels and their number.", "labels": [], "entities": []}, {"text": "This process is, in some senses, arbitrary and most of such corpora occupy a set of very coarse-grained nonterminsl labels.", "labels": [], "entities": []}, {"text": "Moreover, compared with corpora including nonterm~sl labels, there are more existing corpora which include bracketings without nonterm~nal labels such as EDR corpus and ATIS spoken language corpns.", "labels": [], "entities": [{"text": "EDR corpus", "start_pos": 154, "end_pos": 164, "type": "DATASET", "confidence": 0.9063624441623688}, {"text": "ATIS spoken language corpns", "start_pos": 169, "end_pos": 196, "type": "DATASET", "confidence": 0.8457026779651642}]}, {"text": "The well-known standard method to infer a prohabilistic conte.xt-free grammar from a bracketed/unbracketed corpus without nonterminal labels is so-called I inside-outside algorithm which w~ originally proposed by and was implemented as applications for speech and language in ~Lar90], and.", "labels": [], "entities": []}, {"text": "Although encouraging results were shown in these works, the derived grammars were restricted to Chomsky normal-form CFGs and there were problems of the small size of acceptable trai=~ng corpora and the relatively high computation time required for training the grandams.", "labels": [], "entities": []}, {"text": "Towards the problems, this paper proposes anew method which can learn a standard CFG with less computational cost by adopting techniques of clustering analysis to construct a contextsensitive probab'distic grammar from a bracketed corpus where nontermlnal labels are not annotated.", "labels": [], "entities": []}, {"text": "Another claim of this paper is that statistics from a large bracketed corpus without nonterminal labels combined with clustering techniques can help us construct a probabilistic grammar which produces an accurate natural language statistical parser.", "labels": [], "entities": []}, {"text": "In this method, nonterminal labels for brackets in a bracketed corpus can be automatically assigned by making use of local contextual information which is defined as a set of category pairs of left and right words of a constituent in the phrase structure of a sentence.", "labels": [], "entities": []}, {"text": "In this research, based on the assumption that not all contexts are useful in every case, effectiveness of contexts is also investigated.", "labels": [], "entities": []}, {"text": "By using only effective contexts, it is possible for us to improve training speed and memory space without a sacrifice of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9972301125526428}]}, {"text": "Finally, a statistical parsing model bawd on the acquired grammar is provided and the performance is shown through some experiments using the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 142, "end_pos": 152, "type": "DATASET", "confidence": 0.9812111258506775}]}], "datasetContent": [{"text": "To give some support to our su~ested grammar acquisition metllod and statistical parsing model, three following evaluation experiments are made.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7068930268287659}]}, {"text": "The experiments use texts from the Wall Street Journal (WSJ) Corpus and its bracketed version provided by the Penn 'rreebank.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) Corpus", "start_pos": 35, "end_pos": 67, "type": "DATASET", "confidence": 0.9446620004517692}, {"text": "Penn 'rreebank", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.9657641847928365}]}, {"text": "Out of nearly 48,000 sentences(i,222,065 words), we extracted 46,000 sentences(I,172,710 words) as possible material source for traiuing a grammar and 2000 sentences(49,355 words) as source for testing.", "labels": [], "entities": []}, {"text": "The first experiment involves an evaluation of performance of our proposed grammar learning method shown in the section 2.", "labels": [], "entities": []}, {"text": "In this prp]imi~ary experiment, only rules which have lexical categories as their right hand side are considered and the acquired nontermlnal labels are compared with those assigned in the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 189, "end_pos": 199, "type": "DATASET", "confidence": 0.9633106589317322}]}, {"text": "The second experiment stands for investigating effectiveness of contexts described in section 3.", "labels": [], "entities": []}, {"text": "The purpose is to find out useful contexts and use them instead of all contexts based on the assumption that not all contexts are useful for clustering brackets in grammar acquisition.", "labels": [], "entities": [{"text": "grammar acquisition", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.6887347251176834}]}, {"text": "Reducing the number of contexts will help us to improve the computation time and space.", "labels": [], "entities": []}, {"text": "The last experiment is carried out for evaluating the whole grammar which is learned based on local contextual information and indicating the performance of our statistical parsing model using the acquired grammar.", "labels": [], "entities": []}, {"text": "The measures used for this evaJuation are bracketing recall, precision and crossing.", "labels": [], "entities": [{"text": "bracketing", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.933904767036438}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9784975647926331}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996787309646606}, {"text": "crossing", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.896697998046875}]}, {"text": "This subsection shows some results of our preliminary experiments to confirm effectiveness of the proposed grammar acquisition techniques.", "labels": [], "entities": [{"text": "grammar acquisition", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7510429322719574}]}, {"text": "The grammar is learned from the WSJ bracketed corpus where all nonterm~nals are omitted.", "labels": [], "entities": [{"text": "WSJ bracketed corpus", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.8797601858774821}]}, {"text": "In this experiment, we focus on only the rules with I~ c~egories as th~ ~ght h~d ~de.", "labels": [], "entities": []}, {"text": "For ~tance, ci -~ (~J)(~N), c2 -~ (DT)(NN) and Cs --~ (P.RP$)(N.N') in.", "labels": [], "entities": []}, {"text": "Due to the reason of computation time and space, we use the rule tokens which appear more than 500 times in the corpus.", "labels": [], "entities": []}, {"text": "The number of initial rules is 51.", "labels": [], "entities": []}, {"text": "From these rules, the most similar pair is calculated and merged to anew label The merging process is cached out in an iterative way.", "labels": [], "entities": []}, {"text": "In each iterative step of the merging process, differential entropies are calculated.", "labels": [], "entities": []}, {"text": "During the merging process, there are some sharp pealr~ indicating the rapid fluctuation of entropy.", "labels": [], "entities": []}, {"text": "These sharp peaks can be used as a step to terrnln~te the merging process.", "labels": [], "entities": []}, {"text": "In the experhnents, a peak with .DE ~> 0.12 is applied.", "labels": [], "entities": [{"text": "DE", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9957471489906311}]}, {"text": "As the result, the process is halted up at the 45th step and 6 groups are obtained.", "labels": [], "entities": []}, {"text": "This result is evaluated by comparing the system's result with nontermlnal symbols given in the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9778991937637329}]}, {"text": "The evaluation method utilizes a contingency table model which is introduced in and widely used in Information Retrieval and Psychology[lwa95].", "labels": [], "entities": [{"text": "Information Retrieval and Psychology", "start_pos": 99, "end_pos": 135, "type": "TASK", "confidence": 0.8499157428741455}]}, {"text": "The following measures are considered.", "labels": [], "entities": []}, {"text": "The F-measure is used as a combined measure of recall and precision, where /3 is the weight of recall relative to precision.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9951555728912354}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9993464350700378}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9994876384735107}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9853305816650391}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9959527254104614}]}, {"text": "Here, we use/5 ----1.0, equal weight.", "labels": [], "entities": []}, {"text": "The result shows 0.93 ~o PR, 0.93 ~o PP, 0.92 ~0 ~ 0.92 % I~P and 0.93 % FM, which are all relativeiy good va/ues.", "labels": [], "entities": [{"text": "PR", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.8806458711624146}, {"text": "FM", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9941157102584839}]}, {"text": "Especially, PP shows that almost all same labels in the WSJ are assigned in same groups.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8464812636375427}]}, {"text": "In order to investigate whether the application of differentia/entropy to cutoff the merging process is appropriate, we plot values of these measures at all merging steps as shown in.", "labels": [], "entities": []}, {"text": "From the graphs, we found out that the best solution is located at around 44th-45th merging steps.", "labels": [], "entities": []}, {"text": "This is consistent with the grouping result of our approach.", "labels": [], "entities": []}, {"text": "Moreover, the precision equals 100 % from 1st-38nd steps, indicating that the merging process is suitable.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9997335076332092}]}], "tableCaptions": [{"text": " Table 1: Parsing accuracy using the WSJ corpus", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8958608508110046}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9860338568687439}, {"text": "WSJ", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8535854816436768}]}]}