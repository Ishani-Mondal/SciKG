{"title": [{"text": "Integration of Hand-Crafted and Statistical Resources in Measuring Word Similarity", "labels": [], "entities": [{"text": "Measuring Word Similarity", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.7753661274909973}]}], "abstractContent": [{"text": "This paper proposes anew approach for word similarity measurement.", "labels": [], "entities": [{"text": "word similarity measurement", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.8277148604393005}]}, {"text": "The statistics-based computation of word similarity has been popular in recent research, but is associated with a significant computational cost.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.6853087246417999}]}, {"text": "On the other hand, the use of hand-crafted thesauri as semantic resources is simple to implement, but lacks mathematical rigor.", "labels": [], "entities": []}, {"text": "To integrate the advantages of these two approaches, we aim at calculating a statistical weight for each branch of a thesaurus, so that we can measure word similarity simply based on the length of the path between two words in the thesaurus.", "labels": [], "entities": []}, {"text": "Our experiment on Japanese nouns shows that this framework upheld the inequality of statistics-based word similarity with an accuracy of more than 70%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9991483688354492}]}, {"text": "We also report on the effectivity of our framework in the task of word sense disam-biguation.", "labels": [], "entities": [{"text": "word sense disam-biguation", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7166654666264852}]}], "introductionContent": [{"text": "This paper proposes anew approach for word similarity measurement, as has been variously used in such NLP applications as smoothing and word clustering.", "labels": [], "entities": [{"text": "word similarity measurement", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.8471354444821676}, {"text": "word clustering", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.8096111714839935}]}, {"text": "Previous methods for word similarity measurement can be divided into two categories: statistics-based approaches and hand-crafted thesaurus-based approaches.", "labels": [], "entities": [{"text": "word similarity measurement", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.848617156346639}]}, {"text": "In statistics-based approaches, and namely the \"vector space model\", each word is generally represented by a vector consisting of co-occurrence statistics (such as frequency) with respect to other words.", "labels": [], "entities": []}, {"text": "The similarity between two given words is then computationally measured using two vectors representing those words.", "labels": [], "entities": []}, {"text": "One typical implementation computes the relative similarity as the cosine of the angle between two vectors, a method which is also commonly used in information retrieval and text categorization systems to measure the similarity between documents.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.7526393234729767}]}, {"text": "Since it is based on mathematical methods, this type of similarity measurement has been popular.", "labels": [], "entities": [{"text": "similarity measurement", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.8145515620708466}]}, {"text": "Besides this, since the similarity is computed based on given co-occurrence data, word similarity can easily be adjusted according to the domain.", "labels": [], "entities": []}, {"text": "However, data sparseness is an inherent problem.", "labels": [], "entities": []}, {"text": "This fact was observed in our preliminary experiment, despite using statistical information taken from news articles as many as 4 years.", "labels": [], "entities": []}, {"text": "Furthermore, in this approach, vectors require O(N 2) memory space, given that N is the number of words, and therefore, large data sizes can prove prohibitive.", "labels": [], "entities": []}, {"text": "Note that even if one statically stores possible word similarity combinations, O(N 2) space is required.", "labels": [], "entities": []}, {"text": "The other category of word similarity approaches uses semantic resources, that is, hand-cra/ted thesauri (such as the Roget's thesaurus or WordNet in the case of English, and Bunruigoihyo or EDR in the case of Japanese), based on the intuitively feasible assumption that words located near each other within the structure of a thesaurus have similar meaning.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.70257368683815}]}, {"text": "Therefore, the similarity between two given words is represented by the length of the path between them in the thesaurus structure.", "labels": [], "entities": []}, {"text": "Unlike the former approach, the required memory space can be restricted to O(N) because only a list of semantic codes for each word is required.", "labels": [], "entities": [{"text": "O", "start_pos": 75, "end_pos": 76, "type": "METRIC", "confidence": 0.9581554532051086}]}, {"text": "For example, the commonly used Japanese Bunrsigoihyo thesaurus represents each semantic code with only 8 digits.", "labels": [], "entities": []}, {"text": "However, computationally speaking, the relation between the similarity (namely the semantic length of the path), and the physical length of the path is not clear 1.", "labels": [], "entities": []}, {"text": "Furthermore, since most thesauri aim at a general word hierarchy, the similarity between words used in specific domains (technical terms) cannot be measured to the desired level of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9935364127159119}]}, {"text": "IMost researchers heuristicallydefine functions between the similarity and physical path length.", "labels": [], "entities": [{"text": "IMost", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8213919997215271}]}, {"text": "In this paper, we aim at intergrating the advantages of the two above methodological types, or more precisely, realizing statistics-based word similarity based on the length of the thesaurus path.", "labels": [], "entities": []}, {"text": "The crucial concern in this process is how to determine the statistics-based length of each branch in a thesanrus.", "labels": [], "entities": []}, {"text": "We tentatively use the Bunruigoihyo thesaurus, in which each word corresponds to a leaf in the tree structure.", "labels": [], "entities": [{"text": "Bunruigoihyo thesaurus", "start_pos": 23, "end_pos": 45, "type": "DATASET", "confidence": 0.9032288491725922}]}, {"text": "Let us take figure 1, which shows a fragment of the thesaurus.", "labels": [], "entities": []}, {"text": "In this figure, w,'s denote words and x,'s denote the statistics-based length (SBL, for short) of each branch i.", "labels": [], "entities": [{"text": "statistics-based length (SBL", "start_pos": 54, "end_pos": 82, "type": "METRIC", "confidence": 0.7411569952964783}]}, {"text": "Let the statistics-based (vector space model) word similarity between wl and w2 be vsm(wl, w2).", "labels": [], "entities": []}, {"text": "We hope to estimate this similarity by the length of the path through branches 3 and 4, and derive an equation \"xs + x4 = sirn(wl, w2)\".", "labels": [], "entities": [{"text": "sirn", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.914272665977478}]}, {"text": "Intuitively speaking, any combination of xs and x4 which satisfies this equation can constitute the SBLs for branches 3 and 4.", "labels": [], "entities": []}, {"text": "Formalizing equations for other pairs of words in the same manner, we can derive the simultaneous equation shown in.", "labels": [], "entities": []}, {"text": "That is, we can assign the SBL for each branch byway of finding answers for each x~.", "labels": [], "entities": [{"text": "SBL", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9929678440093994}]}, {"text": "This method is expected to excel in the following aspects.", "labels": [], "entities": []}, {"text": "First, this method allows us to measure the statisticsbased word similarity, while retaining the optimal required memory space (O(N)).", "labels": [], "entities": []}, {"text": "One may argue that statistics-based automatic thesaurus construction (for example, the method proposed by Tokunaga et al.) can provide the same advantage, besides which there is no human overhead.", "labels": [], "entities": [{"text": "statistics-based automatic thesaurus construction", "start_pos": 19, "end_pos": 68, "type": "TASK", "confidence": 0.5986969098448753}]}, {"text": "However, it has been empirically observed that the topology of the structure (especially at higher levels) is not necessarily reasonable when based solely on statistics.", "labels": [], "entities": []}, {"text": "To avoid this problem, we would like to introduce hand-crafted thesauri into our framework because the topology (such as MAMMAL is a hyper class of HUMAN) allows for higher levels of sophistication based on human knowledge.", "labels": [], "entities": []}, {"text": "Second, since each SBL reflects the statistics taken from co-occurrence data ~f the whole word set, statistics of each word can complement each other, and thus, the data sparseness problem tends to be minimized.", "labels": [], "entities": []}, {"text": "Let us take figure 1 again, and assume that the statistics for w4 are sparse or completely missing.", "labels": [], "entities": []}, {"text": "In previous statisticsbased approaches, the similarity between w4 and other words cannot be reasonably measured, or not measured at all.", "labels": [], "entities": []}, {"text": "However, in our method, similarity value such as vsm(wl, wa) can be reasonably measured because SBLs xl, x2 and x3 can be well-defined with sufficient statistics.", "labels": [], "entities": [{"text": "vsm", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9529808163642883}]}, {"text": "In section 2, we elaborate on the methodology of our word similarity measurement.", "labels": [], "entities": [{"text": "word similarity measurement", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6862639387448629}]}, {"text": "We then evaluate our method byway of an experiment in section 3 and applied this method to the task of word sense disambiguation in section 4. 2 Methodology", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.7422317862510681}]}], "datasetContent": [{"text": "We conducted experiments on noun entries in the Bunruigoihyo thesaurus.", "labels": [], "entities": [{"text": "Bunruigoihyo thesaurus", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.9025081396102905}]}, {"text": "Co-occurrence data was extracted from the RWC text base RWC-DB-TEXT-95-1 [Real World.", "labels": [], "entities": [{"text": "RWC text base RWC-DB-TEXT-95-1", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9142195731401443}, {"text": "Real World.", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9323717554410299}]}, {"text": "This text base consists of 4 years worth of Mainichi Shimbun newspaper articles, which were automatically annotated with morphological tags.", "labels": [], "entities": [{"text": "Mainichi Shimbun newspaper articles", "start_pos": 44, "end_pos": 79, "type": "DATASET", "confidence": 0.9452459663152695}]}, {"text": "The total number of morphemes is about 100 million.", "labels": [], "entities": []}, {"text": "Instead of conducting full parsing on the texts, several heuristics were used in order to obtain dependencies between nouns and verbs in the form of tuples (frequency, noun, postposition, verb What we evaluated here is the degree to which the simultaneous equation was successfully approximated through the use of the technique described in section 2.", "labels": [], "entities": []}, {"text": "In other words, to what extent the (original) statisticsbased word similarity can be realized by our framework.", "labels": [], "entities": []}, {"text": "We conducted this evaluation in the following way.", "labels": [], "entities": []}, {"text": "Let the statistics-based similarity between words a and b be vsm(a,b), and the similarity based on SBL be sbl(a, b).", "labels": [], "entities": []}, {"text": "Here, let us assume the inequality \"vsm(a, b) > vsm(c, d)\" for words a, b, c and d.", "labels": [], "entities": []}, {"text": "If this inequality can be maintained for our method, that is, \"sbl(a, b) > sbl(c, d)\", the similarity measurement is taken to be successful.", "labels": [], "entities": [{"text": "similarity measurement", "start_pos": 91, "end_pos": 113, "type": "METRIC", "confidence": 0.9404762387275696}]}, {"text": "The accuracy is then estimated by the ratio between the number of successful measurements and the total number of trials.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994990825653076}]}, {"text": "Since resolution of equations is time-consuming, we tentatively generalized 23,223 nouns into 303 semantic classes (represented by the first 4 digits of the semantic code given in the Bunruigoihyo thesaurus), reducing the total number of equations to 45,753.", "labels": [], "entities": []}, {"text": "shows the relation between the number of equations used and the accuracy: we divided the overall equation set into n equal subsets 3 (see section 2.3), and progressively increased the number of subsets used in the computation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9997401833534241}]}, {"text": "When the whole set of equations was provided, the accuracy became about 72%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9998286962509155}]}, {"text": "We also estimated the lower bound of this evaluation, that is, we also conducted the same trials using the Bunruigoihyo thesaurus.", "labels": [], "entities": [{"text": "Bunruigoihyo thesaurus", "start_pos": 107, "end_pos": 129, "type": "DATASET", "confidence": 0.919011652469635}]}, {"text": "In this case, if word a is more closely located to b than c is to d and \"vsm(a, b) > vsm(c,d)\", that trial measurement is taken to be successful.", "labels": [], "entities": []}, {"text": "We found that the lower bound was roughly 56%, and therefore, our framework outperformed this method.", "labels": [], "entities": []}, {"text": "3We arbitrarily set n = 15 so as to be able to resolve equations reasonably.", "labels": [], "entities": []}], "tableCaptions": []}