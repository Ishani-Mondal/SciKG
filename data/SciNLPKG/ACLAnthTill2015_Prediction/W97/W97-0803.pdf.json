{"title": [{"text": "Extending a thesaurus by classifying words Tokunaga Takenobu", "labels": [], "entities": [{"text": "Tokunaga", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8421440720558167}, {"text": "Takenobu", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.7005438208580017}]}], "abstractContent": [{"text": "This paper proposes a method for extending an existing thesaurus through classification of new words in terms of that thesaurus.", "labels": [], "entities": []}, {"text": "New words are classified on the basis of relative probabilities of.a word belonging to a given word class, with the probabilities calculated using noun-verb co-occurrence pairs.", "labels": [], "entities": []}, {"text": "Experiments using the Japanese Bunruigoihy5 thesaurus on about 420,000 co-occurrences showed that new words can be classified correctly with a maximum accuracy of more than 80%.", "labels": [], "entities": [{"text": "Japanese Bunruigoihy5 thesaurus", "start_pos": 22, "end_pos": 53, "type": "DATASET", "confidence": 0.6771706243356069}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9951633214950562}]}], "introductionContent": [{"text": "For most natural language processing (NLP) systems, thesauri comprise indispensable linguistic knowledge.", "labels": [], "entities": []}, {"text": "Roger's International Thesaurus and WordNet are typical English thesauri which have been widely used in past NLP research.", "labels": [], "entities": [{"text": "Roger's International Thesaurus", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.8091592192649841}, {"text": "WordNet", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8994763493537903}]}, {"text": "They are handcrafted, machine-readable and have fairly broad coverage.", "labels": [], "entities": []}, {"text": "However, since these thesauri were originally compiled for human use, they are not always suitable for computer-based natural language processing.", "labels": [], "entities": []}, {"text": "Limitations of handcrafted thesauri can be summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 limited vocabulary size \u2022 unclear classification criteria \u2022 building thesauri by hand requires considerable time and effort The vocabulary size of typical handcrafted thesauri ranges from 50,000 to 100,000 words, including general words in broad domains.", "labels": [], "entities": []}, {"text": "From the viewpoint of NLP systems dealing with a particular domain, however, these thesauri include many unnecessary (general) words and do not include necessary domain-specific words.", "labels": [], "entities": []}, {"text": "The second problem with handcrafted thesauri is that their classification is based on the intuition of lexicographers, with their classification criteria not always being clear.", "labels": [], "entities": []}, {"text": "For the purposes of NLP systems, their classification of words is sometimes too coarse and does not provide sufficient distinction between words, or is some times unnecessarily detailed.", "labels": [], "entities": []}, {"text": "Lastly, building thesauri by hand requires significant amounts of time and effort even for restricted domains.", "labels": [], "entities": []}, {"text": "Furthermore, this effort is repeated when a system is ported to another domain.", "labels": [], "entities": []}, {"text": "This criticism leads us to automatic approaches for building thesauri from large corpora.", "labels": [], "entities": []}, {"text": "Past attempts have basically taken the following steps.", "labels": [], "entities": []}, {"text": "(1) extract word co-occurrences (2) define similarities (distances) between words on the basis of co-occurrences (3) cluster words on the basis of similarities The most crucial part of this approach is gathering word co-occurrence data.", "labels": [], "entities": []}, {"text": "Co-occurrences are usually gathered on the basis of certain relations such as predicateargument, modifier-modified, adjacency, or mixture of these.", "labels": [], "entities": []}, {"text": "However, it is very difficult to gather sufficient co-occurrences to calculate similarities reliably.", "labels": [], "entities": []}, {"text": "It is sometimes impractical to build a large thesaurus from scratch based on only co-occurrence data.", "labels": [], "entities": []}, {"text": "Based on this observation, a third approach has been proposed, namely, combining linguistic knowledge and co-occurrence data.", "labels": [], "entities": []}, {"text": "This approach aims at compensating the sparseness of co~ occurrence data by using existing linguistic knowledge, such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9660279750823975}]}, {"text": "This paper follows this line of research and proposes a method to extend an existing thesaurus by classifying new words in terms of that thesaurus.", "labels": [], "entities": []}, {"text": "In other words, the proposed method identifies appropriate word classes of the thesaurus fora new word which is not included in the thesaurus.", "labels": [], "entities": []}, {"text": "This search process is facilitated based on the probability that a word belongs to a given word class.", "labels": [], "entities": []}, {"text": "The probability is calculated based on word co'occurrences.", "labels": [], "entities": []}, {"text": "As such, this method could also suffer from the data sparseness problem.", "labels": [], "entities": []}, {"text": "As Resnik pointed out, however, using the thesaurus structure (classes) can remedy this problem.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, the 23,223 nouns described in section 3 were classified in terms of the core thesaurus, BGH, using the three search strategies described in the previous section.", "labels": [], "entities": [{"text": "BGH", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9196466207504272}]}, {"text": "Classification was conducted for each strategy as follows.", "labels": [], "entities": []}, {"text": "k-nn Each noun is considered as a singleton cluster, and the probability that a target noun is classified into each of the non-target noun clusters is calculated.", "labels": [], "entities": []}, {"text": "category-based 10-fold cross validation was conducted for the category-based and cluster-based strategies, in that, 23,223 nouns were randomly divided into 10 groups, and one group of nouns was used for test data while the rest was used for training.", "labels": [], "entities": []}, {"text": "The test group was rotated 10 times, and therefore, all nouns were used as a test case.", "labels": [], "entities": []}, {"text": "The results were averaged over these 10 trials.", "labels": [], "entities": []}, {"text": "Each noun in the training data was categorized according to its BGH 5 digit class code, generating 544 category clusters (see).", "labels": [], "entities": [{"text": "BGH 5 digit class code", "start_pos": 64, "end_pos": 86, "type": "METRIC", "confidence": 0.8314342260360718}]}, {"text": "The probability of each noun in the test data being classified into each of these 544 cluster was calculated.", "labels": [], "entities": []}, {"text": "cluster-based In the case of the category-based approach, each noun in the training data was categorized into the leaf clusters of the BGH tree, that is, the 5 digit class categories 4.", "labels": [], "entities": [{"text": "BGH tree", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.9196979999542236}]}, {"text": "For the cluster-based approach, the nouns were also categorized into the intermediate class categories, that is, the 2 to 4 digit class categories.", "labels": [], "entities": []}, {"text": "Since we use the BGH hierarchy structure instead of constructing a duster hierarchy from scratch, in a strict sense, this does not coincide with the cluster-based approach as described in the previous section.", "labels": [], "entities": [{"text": "BGH hierarchy structure", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.84993843237559}]}, {"text": "However, searching through the BGH tree structure in atop down manner still enables us to save greatly on computational resources.", "labels": [], "entities": [{"text": "BGH tree structure", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9407305121421814}]}, {"text": "A simple top down search, in which the cluster with the highest probability is followed at each level, allows only one path leading to a single leaf (5 digit class code).", "labels": [], "entities": []}, {"text": "In order to take into account multiple word senses, we followed several paths at the same time.", "labels": [], "entities": []}, {"text": "More precisely, the difference between the probability of each cluster and the highest probability value for that level was calculated, and clusters for which the difference was within a certain threshold were left as candidate paths.", "labels": [], "entities": []}, {"text": "The threshold was set to 0.2 in this experiments.", "labels": [], "entities": []}, {"text": "The performance of each approach was evaluated on the basis of the number of correctly assigned class codes.", "labels": [], "entities": []}, {"text": "show the results of each approach.", "labels": [], "entities": []}, {"text": "Columns show the maximum number of class codes assigned to each target word.", "labels": [], "entities": []}, {"text": "For example, the column \"10\" means that a target word is assigned to up to 10 class codes.", "labels": [], "entities": []}, {"text": "If the correct class code is contained in these assigned codes, the test case is considered to be assigned the correct code.", "labels": [], "entities": []}, {"text": "Rows show the distribution word numbers on the basis of occurrence frequencies in the training data.", "labels": [], "entities": []}, {"text": "Each value in the table is the number of correct cases with its percentage in the parentheses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 Outline of Bunruigoihy3 (BGH)", "labels": [], "entities": [{"text": "Bunruigoihy3 (BGH)", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.8197223097085953}]}, {"text": " Table 2 Results for the k-nn approach", "labels": [], "entities": []}, {"text": " Table 3 Results for the category-based approach  ~eq\\k", "labels": [], "entities": []}]}