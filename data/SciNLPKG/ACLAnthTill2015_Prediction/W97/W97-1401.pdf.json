{"title": [{"text": "m Integration and Synchronization of Input Modes during Multimodal Human-Computer Interaction*", "labels": [], "entities": []}], "abstractContent": [{"text": "Ill BD ii [] i nH i mi i m I i gn ni ii I mR m m-i mi mi ABSTRACT Our ability to develop robust multimodal systems will depend on knowledge of the natural integration patterns that typify people's combined use of different input modes.", "labels": [], "entities": [{"text": "BD", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9428166151046753}, {"text": "ABSTRACT", "start_pos": 57, "end_pos": 65, "type": "TASK", "confidence": 0.6424392461776733}]}, {"text": "To provide a foundation for theory and design, the present research analyzed multimodal interaction while people spoke and wrote to a simulated dynamic map system.", "labels": [], "entities": []}, {"text": "Task analysis revealed that multimodal interaction occurred most frequently during spatial location commands, and with intermediate frequency during selection commands.", "labels": [], "entities": []}, {"text": "In addition, microanalysis of input signals identified sequential, simultaneous, point-and-speak, and compound integration patterns, as well as data on the temporal precedence of modes and on intermodal lags.", "labels": [], "entities": []}, {"text": "In synchronizing input streams, the temporal precedence of writing over speech was a major theme, with pen input conveying location information first in a sentence.", "labels": [], "entities": []}, {"text": "Linguistic analysis also revealed that the spoken and written modes consistently supplied complementary semantic information, rather than redundant.", "labels": [], "entities": []}, {"text": "One long-term goal of this research is the development of predictive models of natural modality integration to guide the design of emerging multimodal architectures.", "labels": [], "entities": []}, {"text": "Keywords multimodal interaction, integration and synchronization, speech and pen input, dynamic interactive maps, spatial location information, predictive modeling INTRODUC~ON As anew generation of multimodal/media systems begins to define itself, one theme that emerges frequently is the integration and synchronization requirements for combining different modes into a strategic whole system.", "labels": [], "entities": [{"text": "INTRODUC", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.7685599327087402}, {"text": "ON", "start_pos": 173, "end_pos": 175, "type": "METRIC", "confidence": 0.6151564717292786}]}, {"text": "From a linguistic perspective, the joint use of natural modes such as speech and manual gesturing has been described during human-human interaction, as has the role of gesture in beth discourse and in thought [6,7].", "labels": [], "entities": []}, {"text": "Gesture has been viewed as a cognitive aid in the realization of thinking, and also as a carrier of different semantic content than speech: \"Speech and gestures are different material carriers.., they are not redundant but are related, and so the necessary tension can exist between them to propel thought forward.., to make the gesture is to bring the new thought into being on a concrete plane.\"", "labels": [], "entities": [{"text": "realization of thinking", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7980622251828512}]}, {"text": "[7, p.18] The temporal synchrony between speech and gesture also has been analyzed for different languages [7,8].", "labels": [], "entities": []}, {"text": "Currently, little parallel work is available on modality integration during human-computer interaction, although such work", "labels": [], "entities": [{"text": "modality integration", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7340136766433716}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}