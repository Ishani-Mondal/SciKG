{"title": [{"text": "A. Generic Template to evaluate integrated components in spoken dialogue systems", "labels": [], "entities": []}], "abstractContent": [{"text": "enough to be employed in the commercial market We present a generic template for spoken dialogue systems integrating speech recognition and synthesis with 'higher-level' natural language dialogue modelling components.", "labels": [], "entities": [{"text": "speech recognition and synthesis", "start_pos": 117, "end_pos": 149, "type": "TASK", "confidence": 0.7421431466937065}, {"text": "natural language dialogue modelling", "start_pos": 170, "end_pos": 205, "type": "TASK", "confidence": 0.7313290387392044}]}, {"text": "The generic model is abstracted from a number of real application systems targetted at very different domains.", "labels": [], "entities": []}, {"text": "Our research aim in developing this generic template is to investigate anew approach to the evaluation of Dialogue Management Systems.", "labels": [], "entities": []}, {"text": "Rather than attempting to measure accuracy/speed of output, we propose principles for the evaluation of the underlying theoretical linguistic model of Dialogue Management in a given system, in terms of how well it fits our generic template for Dialogue Management Systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9984172582626343}, {"text": "Dialogue Management", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.8419908583164215}]}, {"text": "This is a measure of 'genericness' or 'application-independence' of a given system, which can be used to moderate accuracy/speed scores in comparisons of very unlike DMSs serving different domains.", "labels": [], "entities": [{"text": "accuracy/speed scores", "start_pos": 114, "end_pos": 135, "type": "METRIC", "confidence": 0.7771032005548477}]}, {"text": "This relates to (but is orthogonal to) Dialogue Management Systems evaluation in terms of naturalness and like measurable metrics (eg Dybkjaer et al 1995, Vilnat 1996, EAGLES 1994, Fraser 1995); it follows more closely emerging qualitative evaluation techniques for NL grammatical parsing schemes (Leech et al 1996, Atwell 1996).", "labels": [], "entities": [{"text": "NL grammatical parsing", "start_pos": 266, "end_pos": 288, "type": "TASK", "confidence": 0.6574694613615671}]}], "introductionContent": [], "datasetContent": [{"text": "There are two approaches to evaluating a dialogue management system: to use a qualitative or a quantitative measure.", "labels": [], "entities": []}, {"text": "A qualitative evaluation would rely on the user's opinion of the system.", "labels": [], "entities": []}, {"text": "Dybkjaer et al conducted interviews after each session and asked whether the dialogue seemed natural and pleasant.", "labels": [], "entities": []}, {"text": "Such a subjective evaluation is fraught with problems.", "labels": [], "entities": []}, {"text": "For example, the user may learn after the first attempt how to address the system and which words to use or avoid.", "labels": [], "entities": []}, {"text": "Subsequent evaluations of the same system may then vary even though the system has not changed.", "labels": [], "entities": []}, {"text": "Some users may find the system difficult to use whilst other will find it effortless.", "labels": [], "entities": []}, {"text": "\"Pleasantness\" differs from person to person, too.", "labels": [], "entities": [{"text": "Pleasantness", "start_pos": 1, "end_pos": 13, "type": "METRIC", "confidence": 0.8680707812309265}]}, {"text": "As Vilnat (1996) argues, there is no clear consensus of what comprises a good dialogue.", "labels": [], "entities": []}, {"text": "When asking the user, the designer has to make sure that the user is representative of the end user in terms of background and frequency of use.", "labels": [], "entities": []}, {"text": "Because of these problems, many researchers have tried to provide a means of objectively evaluating a system.", "labels": [], "entities": []}, {"text": "The two methodologies for quantitative evaluation, black and glass box, are concerned with input and output behaviour and the behaviour of each of the components in the system, respectively.", "labels": [], "entities": []}, {"text": "Glass box evaluation can rely on a comparison between the output of a component and a retrospective reference.", "labels": [], "entities": [{"text": "Glass box evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5616348286469778}]}, {"text": "By directly comparing the two it is possible to measure the accuracy of that component.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9995520710945129}]}, {"text": "The black box approach, on the other hand, cannot use this method to evaluate a dialogue since there is no \"correct\" dialogue to compare it with.", "labels": [], "entities": []}, {"text": "Despite this, objective evaluation of the dialogue is necessary in order to compare the performance of different systems.", "labels": [], "entities": []}, {"text": "Initial efforts have been made to standardise this (for example in EAGLES, see Fraser 1995a) but remain work in progress.", "labels": [], "entities": [{"text": "EAGLES", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.7725152373313904}]}], "tableCaptions": []}