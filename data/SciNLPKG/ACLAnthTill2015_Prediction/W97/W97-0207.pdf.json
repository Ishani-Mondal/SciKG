{"title": [], "abstractContent": [{"text": "mm m [] m mm mm mm mm m n m mm I mm mm m Abstract Semantic entropy is a measure of semantic -mbiguity and uninformative-hess.", "labels": [], "entities": []}, {"text": "It is a graded lexical feature which may play a role anywhere lexical semantics plays a role.", "labels": [], "entities": []}, {"text": "This paper presents a method for measuring semantic entropy using translational distributions of words in parallel text corpora.", "labels": [], "entities": []}, {"text": "The measurement method is well-defined for all words, including function words, and even for punctuation .", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic entropy is a measure of semantic ambignity and uninformativeness.", "labels": [], "entities": []}, {"text": "This paper presents a method for measuring semantic entropy using translational distributions of words in parallel text corpora.", "labels": [], "entities": []}, {"text": "The measurement method is well-defined for all words, including function words, and even for punctuation.", "labels": [], "entities": []}, {"text": "The hypothesis behind the measurement method is that semantically heavy words are more likely to have unique counterparts in other languages, so they tend to be translated more consistently than semantically lighter words.", "labels": [], "entities": []}, {"text": "The consistency with which words are translated can be calculated from the translational distributions of words in parallel texts in two languages (bitexts).", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9609789252281189}]}, {"text": "The translational distributions can be estimated using any reasonably good word translation model, such as those described in (BD+93; Che96) or in (Me196b).", "labels": [], "entities": [{"text": "word translation", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7047058939933777}, {"text": "BD+93; Che96)", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.7475108454624811}]}, {"text": "Semantic entropy is a graded lexical feature which may play a role anywhere lexical semantics plays a role.", "labels": [], "entities": []}, {"text": "For example, semantic entropy can be interpreted as semantic ambiguity.", "labels": [], "entities": []}, {"text": "On this interpretation, it can predict the difficulty of disambiguating the sense of a given word.", "labels": [], "entities": []}, {"text": "present a word-sense disambiguation algorithm involving minimization of semantic entropy weighted byword frequency.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6889058649539948}]}, {"text": "Yarowsky (Yar93) compares the semantic entropy of homographs conditioned on different contexts.", "labels": [], "entities": []}, {"text": "Another way to use semantic entropy for word-sense disambiguation is to allow disambiguation algorithms that favor precision over recall to ignore words with high semantic entropy.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7105621993541718}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9920821189880371}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.947786271572113}]}, {"text": "In the same vein, developers of interlinguas for machine translation can use semantic entropy to predict the required complexity of lexical elements of the representation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7154872864484787}]}, {"text": "Another interpretation of entropy is as the inverse of reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9709460735321045}]}, {"text": "Machine learning algorithms may benefit from discounting the importance of data that has high entropy.", "labels": [], "entities": []}, {"text": "For example, an algorithm learning selectional preferences may not want to generalize the statistical characteristics of \"take into account\" to other objects of \"take,\" if it knows that \"take\" has high semantic entropy.", "labels": [], "entities": []}, {"text": "I.e. the selectional preferences of \"take\" are hard to predict because it usually functions as a support verb.", "labels": [], "entities": []}, {"text": "Resnik has used semantic entropy to explore selectional preferences, although he measured it in a different way (Res93).", "labels": [], "entities": []}, {"text": "Semantic entropy can help researchers decide not only how to work with words, but also which words to work with.", "labels": [], "entities": []}, {"text": "Several applications in computational linguistics use stop-lists of unusual words.", "labels": [], "entities": []}, {"text": "The canonical example is information retrieval systems, which routinely remove function words from queries.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7831474840641022}]}, {"text": "Another example is algorithms for mapping bitext correspondence at the word level.", "labels": [], "entities": []}, {"text": "Such algorithms work better given a stop-list of words that are not likely to have cognates in other languages (Me196a).", "labels": [], "entities": [{"text": "Me196a", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.9703845977783203}]}, {"text": "For both of these applications, stop lists are typically constructed by rule of thumb and trial and error, uninformed by any theoretical underpinning.", "labels": [], "entities": []}, {"text": "A common first approximation is the set of closed-class words.", "labels": [], "entities": []}, {"text": "As will be illustrated in Section 3, semantic entropy maybe a better indicator of function-wordhood than syntactic class.", "labels": [], "entities": []}, {"text": "The function/content word distinction also has along history in psycholingnistics.", "labels": [], "entities": []}, {"text": "For example, early research in the cognitive neuroscience of language suggested that function words and content words elicit qualitatively different event-related brain potentials (K&H83).", "labels": [], "entities": []}, {"text": "Later work by the same researchers revealed that the differences were only quantitative and closely tied to word frequency.", "labels": [], "entities": []}, {"text": "Section 4 explores the relationship between frequency and semantic entropy.", "labels": [], "entities": []}, {"text": "It maybe as useful or more useful to control semantic entropy in psycholinguistic experiments, the way that word frequency is usually controlled.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Parts of speech sorted by mean semantic  entropy. Verbs include participles.", "labels": [], "entities": []}, {"text": " Table 2: Semantic entropy of punctuation has high  variance.  punctuation", "labels": [], "entities": []}, {"text": " Table 3: Adjectives  adjective  other  same  such  able  few", "labels": [], "entities": []}, {"text": " Table 4: Pronouns sorted by semantic entropy.  pronoun  'S", "labels": [], "entities": []}, {"text": " Table 5: Verbs with the highest semantic entropy.  verb  participle? frequency  E [  do  !  - 37113  8.44", "labels": [], "entities": [{"text": "verb  participle? frequency  E [  do  !  - 37113  8.44", "start_pos": 52, "end_pos": 106, "type": "METRIC", "confidence": 0.6850171820683912}]}]}