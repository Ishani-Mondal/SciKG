{"title": [{"text": "Word Triggers and the EM Algorithm", "labels": [], "entities": [{"text": "Word Triggers", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7026768624782562}]}], "abstractContent": [{"text": "In this paper, we study the use of so-called word trigger pairs to improve an existing language model, which is typically a tri-gram model in combination with a cache component.", "labels": [], "entities": []}, {"text": "A word trigger pair is defined as a long-distance word pair.", "labels": [], "entities": []}, {"text": "We present two methods to select the most significant single word trigger pairs.", "labels": [], "entities": []}, {"text": "The selected trigger pairs are used in a combined model where the interpolation parameters and trigger interaction parameters are trained by the EM algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we study the use of so-called word trigger pairs (for short: word triggers) ( to improve an existing language model, which is typically a trigram model in combination with a cache component.", "labels": [], "entities": []}, {"text": "We use a reference model p(wlh), i.e. the conditional probability of observing the word w fora given history h.", "labels": [], "entities": []}, {"text": "For a trigram model, this history h includes the two predecessor words of the word under consideration, but in general it can be the whole sequence of the last M predecessor words.", "labels": [], "entities": []}, {"text": "The criterion for measuring the quality of a language model p(Wlh ) is the so-called log-likelihood criterion, which fora corpus Wl, ..., wn, ...wN is defined by: According to this definition, the log-likelihood criterion measures for each position n how well the language model can predict the next word given the knowledge about the preceeding words and computes an average overall word positions n.", "labels": [], "entities": []}, {"text": "In the context of language modeling, the log-likelihood criterion F is converted to perplexity PP, defined by For applications where the topic-dependence of the language model is important, e.g. text dictation, the history h may reach back several sentences so that the history length M covers several hundred words, say, M = 400 as it is for the cache model.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.726376473903656}, {"text": "text dictation", "start_pos": 195, "end_pos": 209, "type": "TASK", "confidence": 0.769529402256012}]}, {"text": "Thus word trigger pairs can be viewed as longdistance word bigrams.", "labels": [], "entities": []}, {"text": "In this view, we are faced the problem of finding suitable word trigger pairs.", "labels": [], "entities": []}, {"text": "This will be achieved by analysing a large text corpus (i.e. several millions of running words) and learning those trigger pairs that are able to improve the baseline language model.", "labels": [], "entities": []}, {"text": "A related approach to capturing long-distance dependencies is based on stochastic variants of link grammars.", "labels": [], "entities": []}, {"text": "In several papers (, selection criteria for single word trigger pairs were studied.", "labels": [], "entities": []}, {"text": "In this paper, this work is extended as follows: \u2022 Single-Trigger Model: We consider the definition of a single word trigger pair.", "labels": [], "entities": []}, {"text": "Multi-Trigger Model: In practice, we have to take into account the interaction of many trigger pairs.", "labels": [], "entities": []}, {"text": "Here, we introduce a model for this purpose.", "labels": [], "entities": []}, {"text": "To really use the word triggers fora language model, they must be combined with an existing language model.", "labels": [], "entities": []}, {"text": "This is achieved by using linear interpolation between the existing language model and a model for the multitrigger effects.", "labels": [], "entities": []}, {"text": "The parameters of the resulting model, namely the trigger parameters and one interpolation parameter, are trained by the EM algorithm.", "labels": [], "entities": []}, {"text": "\u2022 We present experimental results on the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.986202672123909}]}, {"text": "Both the single-trigger approach and the multi-trigger approach are used to improve the perplexity of a baseline language model.", "labels": [], "entities": []}, {"text": "We give examples of selected trigger pairs with and without using the EM algorithm.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Effect of word trigger on test set perplexity (a) and interpolation parameter AM, AC, AT (b).", "labels": [], "entities": [{"text": "AM", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.5302953124046326}, {"text": "AT", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.966132402420044}]}, {"text": " Table 2: Triggered words w along with c~(w[v) for triggering word v.", "labels": [], "entities": []}, {"text": " Table 3: Triggered words w along with c~(w[v) for triggering word v.", "labels": [], "entities": []}]}