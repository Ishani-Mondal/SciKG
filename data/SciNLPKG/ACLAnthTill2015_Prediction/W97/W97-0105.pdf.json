{"title": [{"text": "Probabilistic Parsing of Unrestricted English Text, With a Highly-Detailed Grammar", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "As an additioaal means of improving the accuracy of our parser, we have been working towards effecting a dramatic increase in the size of our trai~ing treebank, via treebank conversion techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9989733695983887}]}, {"text": "We employ a statistical method for converting treebank from a less-detailed formatwand we have chosen the IBM/Lancaster Treebank ( as a first representative of such treeba~k~--to a more-detailed format, that of the ATR/Lancaster Treebank.", "labels": [], "entities": [{"text": "IBM/Lancaster Treebank", "start_pos": 106, "end_pos": 128, "type": "DATASET", "confidence": 0.859917938709259}, {"text": "ATR/Lancaster Treebank", "start_pos": 215, "end_pos": 237, "type": "DATASET", "confidence": 0.9300040155649185}]}, {"text": "There has been very little previous work on treebanl\u00a2 conversion.", "labels": [], "entities": [{"text": "treebanl\u00a2 conversion", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8279441793759664}]}, {"text": "() describe an effort to b~n_d-annotate text using the tagging schemes employed in various different treebanks, as a prelirnln~ry to attempting to learn, in away to be determined, how to convert a corpus automatically from one style of tagging markup to another.", "labels": [], "entities": []}, {"text": "( take on the problem of converting treeb~n~ conforming to their English grammar into a format conforming to a later version of the same grammar, and report a conversion accuracy of some 96% on a 141,000-word test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.7743531465530396}]}, {"text": "They employ a heuristic which scores source-treebank/target-treebank parse pairs based essentially on the percentage of identically-placed brackets in the two parses.", "labels": [], "entities": []}, {"text": "However, their target grammar 19 generates only 17 parses on average per sentence of test data.", "labels": [], "entities": []}, {"text": "Although they exhibit no parses with respect to their grammars, it can be assumed that they feature only rudimentary tag and non-terminal vocabularies.", "labels": [], "entities": []}, {"text": "The problem we face in learning to convert IBM/Lancaster Treebank parses into ATl~/Lancaster Treebank parses is rather more difficult than this.", "labels": [], "entities": [{"text": "IBM/Lancaster Treebank parses into ATl~/Lancaster Treebank parses", "start_pos": 43, "end_pos": 108, "type": "DATASET", "confidence": 0.7425543720071967}]}, {"text": "For instance, as noted in 3.1, the Parse Base of the ATR English Grammar, which generates the parses of the ATl~/Lancaster Treebank, is 1.76, which means that on average, the Grammar generates about 200 parses for 10-word sentence; 2000 parses fora IS-word sentence, and 70,000 parses fora 20-word sentence.", "labels": [], "entities": [{"text": "Parse Base", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9773784577846527}, {"text": "ATR English Grammar", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.9378337661425272}, {"text": "ATl~/Lancaster Treebank", "start_pos": 108, "end_pos": 131, "type": "DATASET", "confidence": 0.7657557874917984}]}, {"text": "Further, far from featuring a rudimentary set of lexicat tags and non-termlnal node labels, the ATl~/Lancaster Treebauk utilizes ~gaud presumably their source grammar as well   Treebank.", "labels": [], "entities": [{"text": "ATl~/Lancaster Treebauk", "start_pos": 96, "end_pos": 119, "type": "DATASET", "confidence": 0.7669997662305832}]}, {"text": "An impression of the di~cnlty of the treeb~nk conversion task undertaken here can be gained by closely contrasting the two parses of this 143,837 words included in the IBM/Lancaster Treeb~n~--35,575 words of Associated Press newswire and 108,262 words of Canadian Hansard le~slative proceedh~s--were treebanked with respect to the ATR English Grammar, in the exact same manner as the data in the ATl%/Lancaster Treeb~nk.", "labels": [], "entities": [{"text": "IBM/Lancaster Treeb", "start_pos": 168, "end_pos": 187, "type": "DATASET", "confidence": 0.5913739129900932}, {"text": "Associated Press newswire", "start_pos": 208, "end_pos": 233, "type": "DATASET", "confidence": 0.9072611331939697}, {"text": "ATR English Grammar", "start_pos": 331, "end_pos": 350, "type": "DATASET", "confidence": 0.9479817946751913}, {"text": "ATl%/Lancaster Treeb~nk", "start_pos": 396, "end_pos": 419, "type": "DATASET", "confidence": 0.9155015150705973}]}, {"text": "We will refer to the IBM/Lancaster Treeb~-k version of this data as the parallel corpus.", "labels": [], "entities": [{"text": "IBM/Lancaster Treeb~-k version of this data", "start_pos": 21, "end_pos": 64, "type": "DATASET", "confidence": 0.8772444725036621}]}, {"text": "As a preliminary step to treeb~k conversion, we aligned the parallel and ATI% corpora.", "labels": [], "entities": [{"text": "ATI", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.8956993818283081}]}, {"text": "87.3% of the parallel data--125,530 words--aligned essentially perfectly, and for the work reported here, we decided to operate only on this satisfactorily-aligned dat&", "labels": [], "entities": []}], "datasetContent": [{"text": "As discussed in 3.1, our first step in parsing is to tag each sentence.", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9621644020080566}]}, {"text": "The tagger currently produces an exact match 74% of the time for the 47,800-word test set, comparing against a single tag sequence for each sentence, le We present parsing results both for text which starts out correctly tagged 17 and for raw text.", "labels": [], "entities": []}, {"text": "R.esults for parsing from raw text are given for both the exact-match and exact-syntactic-match criteria described in 4.1.", "labels": [], "entities": [{"text": "exact-match", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9498144388198853}]}, {"text": "The performance of the parser on short sentences of correctly tagged data is extrememly good.", "labels": [], "entities": []}, {"text": "We feel this indicates that the models are performing well in scoring the parses.", "labels": [], "entities": []}, {"text": "The results deteriorate rapidly for longer sentences, but we believe the problem lies in the search procedure rather than the models.", "labels": [], "entities": []}, {"text": "A measure of the performance of a search is whether it ~2In a parallel experiment to determine consistency on tagging, we asked each of the three team members to choose the first correct tag from a raaked list of tags for each word of each sentence of test data.", "labels": [], "entities": [{"text": "consistency", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.948482096195221}]}, {"text": "These ranked lists were hand-constructed, and an effort was made to make them as difficult as possible to choose from.", "labels": [], "entities": []}, {"text": "About 4,800 words (152 sentences) of test data were utilized.", "labels": [], "entities": []}, {"text": "The result was a 3.1% expected rate of disagreement among the team members on the exact choice of tag.", "labels": [], "entities": []}, {"text": "1sOl these 248 sentence pairs, 85~ were exact matches in terms of the way they were tagged.", "labels": [], "entities": []}, {"text": "14Actually, the documents were selected from our \"main General-English Treebank\" of 800,000 words.", "labels": [], "entities": [{"text": "General-English Treebank\"", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.9137250979741415}]}, {"text": "l~i.e..the parse was wrong if even one tag was wrong; or, of couree, ira rule choice was wrong.", "labels": [], "entities": []}, {"text": "For the tags assigned r,o the roughly 5000 words in these 308 sentences, expected error rate was 2.9%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9939774572849274}]}, {"text": "Essentially none of these tagging e~rors had to do with the use of the syntactic portion of our tags; all of the errors were semantic; the same was true in the two tagging consistency experiments related above.", "labels": [], "entities": []}, {"text": "leas noted in 4.1 fn.", "labels": [], "entities": []}, {"text": "8, our experience indicates that we can expect a roughly 10~o improvement in this score when we compare performance against \"golden--standard\" test data in which all correct answers are indicated; this would bring our tagging accuracy into the 80-percent area.", "labels": [], "entities": [{"text": "tagging", "start_pos": 218, "end_pos": 225, "type": "TASK", "confidence": 0.9494824409484863}, {"text": "accuracy", "start_pos": 226, "end_pos": 234, "type": "METRIC", "confidence": 0.9053139090538025}]}, {"text": "lrFor the definition of the term ~crossing brackets\" used in  25.6% I percentage of parses which exactly match one of the humanproduced parses (\"exact match\") or which match bracket locations, role names, and syntactic part-of--speech tags only (\"syntactic exact match\").", "labels": [], "entities": []}, {"text": "suggests any candidates which are as likely as the correct un~wer.", "labels": [], "entities": []}, {"text": "If not, the parser has erred by \"omrn~sion\" rather than by \"commission': it has ommitted the correct parse from consideration, but not because it seemed ,mJ~lrely.", "labels": [], "entities": []}, {"text": "It is entirely possible that the correct parse is in fact among the highest-scoring parses.", "labels": [], "entities": []}, {"text": "These types of search error are non--existent for exhaustive search, but become important for sentences between 11 and 15 words in length, and dominate the results for longer sentences.", "labels": [], "entities": []}, {"text": "The results in reflect tagging accuracy as well as the pefformaace of the parser models per se.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.98829585313797}, {"text": "pefformaace", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9490930438041687}]}, {"text": "Note that tagging accuracy is quoted on a per-word basis, as is customary.", "labels": [], "entities": [{"text": "tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9559611678123474}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9260384440422058}]}, {"text": "From previous work, we estimate the accuracy of the tagger on the syntactic portion of tags to be about 94%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9997237324714661}]}, {"text": "Thus there is typically at least one error in semantic assignment in each sentence, and an error in syntactic assignment in one of every two sentences.", "labels": [], "entities": [{"text": "syntactic assignment", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7009209990501404}]}, {"text": "It is not surprising, .then, that the per-sentence parsing acclzracy suffers when parses are predicted from raw text.", "labels": [], "entities": [{"text": "per-sentence parsing", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.6980217397212982}]}, {"text": "We evaluate trsebank conversion to ATR-Treebank format in the same way as we evaluate the parser when it is trained in the normal ma-ner (cf. 4.1), except that test data consists of ATR-Treebank-format documents of which we also possess aligned source treebank (in this case: IBM/Lancaster-Treebank) versions.", "labels": [], "entities": []}, {"text": "In the performance results cited below, however, we show exact match only with the single correct parse of the test treebank, rather than with anyone of the correct parses indicated in the \"golden standard\" version of the test set.", "labels": [], "entities": [{"text": "exact match", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.8420665264129639}]}, {"text": "Experimental Results displays exact-match parsing results fora normal 6,556--word test set 22.", "labels": [], "entities": [{"text": "exact-match parsing", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7572725415229797}]}, {"text": "Crucially, the amount of tr~.~n;ng data here, 118,489 words, is only 17.5% as large as for the models of.", "labels": [], "entities": []}, {"text": "Considering the simplicity of the approach, we think these results constitute a proof of principle for the idea of treebank conversion.", "labels": [], "entities": [{"text": "treebank conversion", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7054114639759064}]}, {"text": "They indicate that we can build treebank conversion models of accuracy comparable to the current parser using much less data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9990906715393066}]}, {"text": "Of course, the results here do not include models used in tagging.", "labels": [], "entities": []}, {"text": "The treebank conversion models tag with an accuracy of 62.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9996042847633362}]}, {"text": "A detailed examination of those models shows that the syntactic models are better than the parser's, while the semi-tic models are worse.", "labels": [], "entities": []}, {"text": "A second direction which suggests itself is to pursue our scaled-down approach to treebank conversion, but with more tr~;u;ng data than we have used so far.", "labels": [], "entities": [{"text": "treebank conversion", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7205342352390289}]}, {"text": "Third, we may decide to implement the more laborious two-model approach desribed in 5.2. 23 Overall, we expect that conversion models which take full advantage of the existing database as well as of the parallel corpus as outlined above should produce data of high enough quality to use as training data for our parser.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparison of IBM Manu~!s and ATR/Lancaster General-English Treebanks", "labels": [], "entities": [{"text": "ATR/Lancaster General-English Treebanks", "start_pos": 40, "end_pos": 79, "type": "DATASET", "confidence": 0.9040323972702027}]}, {"text": " Table 4: Parsing results reported by Jelinek et. at. for IBM Manua!s task; see", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9107464551925659}, {"text": "IBM Manua!s task", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7559636235237122}]}, {"text": " Table 5: Parsing from text which starts out correctly tagged: percentage of parses which ex- actly match the single parse in the treeb~ulc, for a 6,556-word test set. \"Treeb~ulc-conversion\"  models are trained on 1].8,489 ~mning words of ATR/Lancaster Treeb~uk, together with aligned  IBM/Lancaster Treeb~. \"Parser\" models are trained on 676,401 r-nn;ngwords of ATR/Lancaster  Treebank alone.", "labels": [], "entities": [{"text": "ATR/Lancaster Treeb~uk", "start_pos": 239, "end_pos": 261, "type": "DATASET", "confidence": 0.8938808043797811}, {"text": "IBM/Lancaster Treeb", "start_pos": 286, "end_pos": 305, "type": "DATASET", "confidence": 0.6881063729524612}, {"text": "ATR/Lancaster  Treebank", "start_pos": 363, "end_pos": 386, "type": "DATASET", "confidence": 0.8960313498973846}]}]}