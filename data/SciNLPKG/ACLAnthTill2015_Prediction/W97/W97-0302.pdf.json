{"title": [{"text": "Global Thresholding and Multiple-Pass Parsing*", "labels": [], "entities": [{"text": "Multiple-Pass Parsing", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.6843527406454086}]}], "abstractContent": [{"text": "We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level.", "labels": [], "entities": [{"text": "beam thresholding", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8861880898475647}]}, {"text": "We also present anew thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement.", "labels": [], "entities": [{"text": "multiple pass parsing", "start_pos": 184, "end_pos": 205, "type": "TASK", "confidence": 0.6013784607251486}]}, {"text": "We use anew search algorithm to simultaneously optimize the thresholding parameters of the various algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we examine thresholding techniques for statistical parsers.", "labels": [], "entities": []}, {"text": "While there exist theoretically efficient (O (n 3)) algorithms for parsing Probabilistic Context-Free Grammars (PCFGs) and related formalisms, practical parsing algorithms usually make use of pruning techniques, such as beam thresholding, for increased speed.", "labels": [], "entities": [{"text": "parsing Probabilistic Context-Free Grammars (PCFGs)", "start_pos": 67, "end_pos": 118, "type": "TASK", "confidence": 0.8754900438444955}, {"text": "beam thresholding", "start_pos": 220, "end_pos": 237, "type": "TASK", "confidence": 0.7863295376300812}]}, {"text": "We introduce two novel thresholding techniques, global thresholding and multiple-pass parsing, and one significant variation on traditional beam thresholding.", "labels": [], "entities": [{"text": "multiple-pass parsing", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.6535113304853439}]}, {"text": "We examine the value of these techniques when used separately, and when combined.", "labels": [], "entities": []}, {"text": "In order to examine the combined techniques, we also introduce an algorithm for optimizing the settings *This material is based in part upon work supported by the National Science Foundation under Grant No. IRI-9350192 and a National Science Foundation Graduate Student Fellowship.", "labels": [], "entities": [{"text": "National Science Foundation Graduate Student Fellowship", "start_pos": 225, "end_pos": 280, "type": "DATASET", "confidence": 0.850910355647405}]}, {"text": "I would also like to thank Michael Collins, Rebecca Hwa, Lillian Lee, Wheeler Ruml, and Stuart Shieber for helpful discussions, and comments on earlier drafts, and the anonymous reviewers for their extensive comments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first goal was to show that entropy is a good surrogate for precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9969850182533264}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9937885403633118}]}, {"text": "We thus tried two experiments: one with a relatively large test set of 200 sentences, and one with a relatively small test set of 15 sentences.", "labels": [], "entities": []}, {"text": "Presumably, the 200 sentence test set should be much less noisy, and fairly indicative of performance.", "labels": [], "entities": []}, {"text": "We graphed both precision and recall, and entropy, versus time, as we swept the thresholding parameter over a sequence of values.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9994412064552307}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9978607296943665}, {"text": "entropy", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9523240327835083}]}, {"text": "As can be seen, entropy is significantly smoother than precision and recall for both size test corpora.", "labels": [], "entities": [{"text": "entropy", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.8760737776756287}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9996380805969238}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9995470643043518}]}, {"text": "Our second goal was to check thai the prior probability is indeed helpful.", "labels": [], "entities": []}, {"text": "We ran two experiments, one with the prior and one without.", "labels": [], "entities": []}, {"text": "Since the experiments without the prior were much worse than those with it, all other beam thresholding experiments included the prior.", "labels": [], "entities": [{"text": "beam thresholding", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7717790901660919}]}, {"text": "The results, shown in, indicate that the prior is a critical component.", "labels": [], "entities": []}, {"text": "This experiment was run on 200 sentences of test data.", "labels": [], "entities": []}, {"text": "Notice that as the time increases, the data tends to approach an asymptote, as shown in the left hand graph of.", "labels": [], "entities": []}, {"text": "In order to make these small asymptotic changes more clear,-we wished to expand the scale towards the asymptote.", "labels": [], "entities": []}, {"text": "The right hand graph was plotted with this expanded scale, based on log(entropy -asymptote), a slight variation on a normal log scale.", "labels": [], "entities": []}, {"text": "We use this scale in all the remaining entropy graphs.", "labels": [], "entities": []}, {"text": "A normal logarithmic scale is used for the time axis.", "labels": [], "entities": []}, {"text": "The fact that the time axis is logarithmic is especially useful for determining how much more efficient one algorithm is than another at a given performance level.", "labels": [], "entities": []}, {"text": "If one picks a performance level on the vertical axis, then the distance between the two curves at that level represents the ratio between their speeds.", "labels": [], "entities": []}, {"text": "There is roughly a factor of 8 to 10 difference between using the prior and not using it at all graphed performance levels, with a slow trend towards smaller differences as the thresholds are loosened.", "labels": [], "entities": []}, {"text": "We tried experiments comparing global thresholding to beam thresholding.", "labels": [], "entities": []}, {"text": "shows the results of this experiment, and later experiments.", "labels": [], "entities": []}, {"text": "In the best case, global thresholding works twice as well as beam thresholding, in the sense that to achieve the same level of performance requires only half as much time, although smaller improvements were more typical.", "labels": [], "entities": [{"text": "global thresholding", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.6469533443450928}, {"text": "beam thresholding", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8129249811172485}]}, {"text": "We have found that, in general, global thresholding works better on simpler grammars.", "labels": [], "entities": []}, {"text": "In some complicated grammars we explored in other work, there were systematic, strong correlations between nodes, which violated the independence approximation used in global thresholding.", "labels": [], "entities": []}, {"text": "This prevented us from using global thresholding with these grammars.", "labels": [], "entities": []}, {"text": "In the future, we may modify global thresholding to model some of these correlations.", "labels": [], "entities": []}, {"text": "get the advantages of both.", "labels": [], "entities": []}, {"text": "We ran a series of experiments using the thresholding optimization algorithm of Section 5.", "labels": [], "entities": [{"text": "thresholding optimization", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.8915983140468597}, {"text": "Section 5", "start_pos": 80, "end_pos": 89, "type": "DATASET", "confidence": 0.9078158140182495}]}, {"text": "The combination of beam and global thresholding together is clearly better than either alone, in some cases running 40% faster than global thresholding alone, while achieving the same performance level.", "labels": [], "entities": []}, {"text": "The combination generally runs twice as fast as beam thresholding alone, although up to a factor of three.", "labels": [], "entities": [{"text": "beam thresholding", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.9144201874732971}]}, {"text": "Multiple-pass parsing improves even further on our experiments combining beam and global thresholding.", "labels": [], "entities": [{"text": "Multiple-pass parsing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5718925297260284}]}, {"text": "Note that we used both beam and global thresholding for both the first and second pass in these experiments.", "labels": [], "entities": []}, {"text": "The first pass grammar was the very simple terminal-prime grammar, and the second pass grammar was the usual 6-gram grammar.", "labels": [], "entities": []}, {"text": "We evaluated multiple-pass parsing slightly differently from the other thresholding techniques.", "labels": [], "entities": [{"text": "multiple-pass parsing", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.753561794757843}]}, {"text": "In the experiments conducted here, our first and second pass grammars were very different from each other.", "labels": [], "entities": []}, {"text": "For a given parse to be returned, it must be in the intersection of both grammars, and reasonably likely according to both.", "labels": [], "entities": []}, {"text": "Since the first and second pass grammars capture different information, parses which are likely according to both are especially good.", "labels": [], "entities": []}, {"text": "The entropy of a sentence measures its likelihood according to the second pass, but ignores the fact that the returned parse must also be likely according to the first pass.", "labels": [], "entities": []}, {"text": "Thus, entropy, our measure in the previous experiments, which measures only likelihood according to the final pass, is not necessarily the right measure to use.", "labels": [], "entities": [{"text": "entropy", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.992106556892395}]}, {"text": "We therefore give precision and recall results in this section.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.999729573726654}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9996139407157898}]}, {"text": "We still optimized our thresholding parameters using the same 31 sentence held out corpus, and minimizing entropy versus number of productions, as before.", "labels": [], "entities": []}, {"text": "We should note that when we used a first pass grammar that captured a strict subset of the information in the second pass grammar, we have found that entropy is a very good measure of performance.", "labels": [], "entities": []}, {"text": "As in our earlier experiments, it tends to be well correlated with precision and recall but less subject to noise.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9996201992034912}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9989349246025085}]}, {"text": "It is only because of the grammar mismatch that we have changed the evaluation.", "labels": [], "entities": []}], "tableCaptions": []}