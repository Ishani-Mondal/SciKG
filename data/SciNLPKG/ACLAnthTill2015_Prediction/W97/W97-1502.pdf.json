{"title": [], "abstractContent": [], "introductionContent": [{"text": "Ina language understanding system where full, linguistically-motivated analyses of utterances are desired, the linguistic analyser needs to generate possible semantic representations and then choose the one most likely to be correct.", "labels": [], "entities": []}, {"text": "If the analyser is a component of a pipelined speech understanding system, the problem is magnified, as the speech recognizer will typically deliver not a word string but an N-best list or a lattice; the problem then becomes one of choosing between multiple analyses of several competing word sequences.", "labels": [], "entities": []}, {"text": "In practice, we can only come near to satisfactory disambiguation performance if the analyser is trained on a corpus of utterances from the same source (domain and task) as those it is intended to process.", "labels": [], "entities": []}, {"text": "Since this needs to be done afresh for each new source, and since a corpus of several thousand sentences will normally be needed, economic considerations mean it is highly desirable to do it as automatically as possible.", "labels": [], "entities": []}, {"text": "Furthermore, those aspects that cannot be automated should as far as possible not depend on the attention of experts in the system and in the representations it uses.", "labels": [], "entities": []}, {"text": "The Spoken Language Translator (SLT; Becket et al, forthcoming; Rayner) is a pipelined speech understanding system of the type assumed here.", "labels": [], "entities": [{"text": "Spoken Language Translator (SLT; Becket et al", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.7632215784655677}]}, {"text": "It is constructed from general-purpose speech recognition, language processing and speech synthesis components in order to allow relatively straightforward adaptation to new domains.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.712293416261673}, {"text": "speech synthesis", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7147103846073151}]}, {"text": "Linguistic processing in the SLT system is carried out by the Core Language Engine (CLE;.", "labels": [], "entities": [{"text": "Linguistic processing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7472294270992279}, {"text": "SLT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.963902473449707}]}, {"text": "Given an input string, N-best list or lattice, the CLE applies unification-based syntactic rules and their corresponding semantic rules to create zero or more quasi-logical form analyses of it; disambiguation is then a matter of selecting the correct (or at least, the best available) QLF.", "labels": [], "entities": []}, {"text": "This paper describes the TreeBanker, a program that facilitates supervised training by interacting with a non-expert user and that organizes the results of this training to provide the CLE with data in an appropriate format.", "labels": [], "entities": []}, {"text": "The CLE uses this data to analyse speech recognizer output efficiently and to choose accurately among the interpretations it creates.", "labels": [], "entities": [{"text": "speech recognizer output", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7303245961666107}]}, {"text": "I assume here that the coverage problem has been solved to the extent that the system's grammar and lexicon license the correct analyses of utterances often enough for practical usefulness.", "labels": [], "entities": []}, {"text": "The examples given in this paper are taken from the ATIS (Air Travel Inquiry System; Hemphill et al,) domain.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.7459641098976135}, {"text": "Air Travel Inquiry System; Hemphill et al,) domain", "start_pos": 58, "end_pos": 108, "type": "DATASET", "confidence": 0.8358381450176239}]}, {"text": "However, wider domains, such as that represented in the North American Business News (NAB) corpus, would present no particular problem to the TreeBanker as long as the (highly non-trivial) coverage problems for those domains were close enough to solution.", "labels": [], "entities": [{"text": "North American Business News (NAB) corpus", "start_pos": 56, "end_pos": 97, "type": "DATASET", "confidence": 0.6944749653339386}]}, {"text": "The examples given here are in fact all for Englis]h, but the TreeBanker has also successfully been used for Swedish and French customizations of the CLE ().", "labels": [], "entities": [{"text": "Englis]h", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9164913296699524}]}], "datasetContent": [{"text": "Using the TreeBanker, it is possible fora linguistically aware non-expert to judge around 40 sentences per hour after a few days practice.", "labels": [], "entities": []}, {"text": "When the user becomes still more practised, as will be the case if he judges a corpus of thousands of sentences, this figure rises to around 170 sentences per hour in the case of our most experienced user.", "labels": [], "entities": []}, {"text": "Thus it is reasonable to expect a corpus of 20,000 sentences to be judged in around three person weeks.", "labels": [], "entities": []}, {"text": "A much smaller amount of time needs to be spent by experts in making judgments he felt unable to make (perhaps for one percent of sentences once the user has got used to the system) and in checking the user's work (the TreeBanker includes a facility for picking out sentences where errors are mostly likely to have been made, by searching for discriminants with unusual values).", "labels": [], "entities": []}, {"text": "From these figures it would seem that the TreeBanker provides a much quicker and less skill-intensive way to arrive at a disambiguated set of analyses fora corpus than the manual annotation scheme involved in creating the Penn Treebank; however, the TreeBanker method depends on the prior existence of a grammar for the domain in question, which is of course a non-trivial requirement.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 222, "end_pos": 235, "type": "DATASET", "confidence": 0.994247168302536}]}, {"text": "present a scheme for selecting corpus sentences whose judging is likely to provide useful new information, rather than those that merely repeat old patterns.", "labels": [], "entities": []}, {"text": "The TreeBanker offers a related facility whereby judgments on one sentence maybe propagated to others having the same sequence of parts of speech.", "labels": [], "entities": []}, {"text": "This can be combined with the use of representative corpora in the CLE to allow only one representative of a particular pattern, out of perhaps dozens in the corpus as a whole, to be inspected.", "labels": [], "entities": []}, {"text": "This already significantly reduces the number of sentences needing to be judged, and hence the time required, and we expect further reductions as Engelson's and Dagan's ideas are applied at a finer level.", "labels": [], "entities": []}, {"text": "In the current implementation, the TreeBanker only makes use of context.independent properties: those derived from analyses of an utterance that are constructed without any reference to the context of use.", "labels": [], "entities": []}, {"text": "But utterance disambiguation in general requires the use of information from the context.", "labels": [], "entities": [{"text": "utterance disambiguation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9297374784946442}]}, {"text": "The context can influence choices of word sense, syntactic structure and, most obviously, anaphoric reference (see e.g. Carter, 1987, for an overview), so it might seem that a disambiguation component trained only on context-independent properties cannot give adequate performance.", "labels": [], "entities": []}, {"text": "However, for QLFs for the ATIS domain, and presumably for others of similar complexity, this is not in practice a problem.", "labels": [], "entities": [{"text": "ATIS domain", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9418313801288605}]}, {"text": "As explained earlier, anaphors are left unresolved at the stage of analysis and disambiguation we are discussing here; and contextual factors for sense and structural ambiguity resolution are virtually always \"frozen\" by the constraints imposed by the domain.", "labels": [], "entities": [{"text": "sense and structural ambiguity resolution", "start_pos": 146, "end_pos": 187, "type": "TASK", "confidence": 0.639791339635849}]}, {"text": "For example, although there are certainly contexts in which \"Tell me flights to Atlanta on Wednesday\" could mean \"Wait until Wednesday, and then tell me flights to Atlanta\", in the ATIS domain this reading is impossible and so \"on Wednesday\" must attach to \"flights\".", "labels": [], "entities": [{"text": "ATIS domain", "start_pos": 181, "end_pos": 192, "type": "DATASET", "confidence": 0.936322033405304}]}, {"text": "For a wider domain such as NAB, one could perhaps attack the context problem either by an initial phase of topic-spotting (using a different set of discriminant scores for each topic category), or by including some discriminants for features of the context itself among these to which training was applied.", "labels": [], "entities": [{"text": "NAB", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.9127134680747986}]}], "tableCaptions": []}