{"title": [{"text": "Learning New Compositions from Given Ones", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we study the problem of \"learning new compositions of words from given ones with a specific syntactic structure , e.g., AN or V-N structures.", "labels": [], "entities": []}, {"text": "We first cluster words according to the given compositions , then construct a cluster-based compositional frame for each word cluster, which contains both new and given compositions relevant with the words in the cluster.", "labels": [], "entities": []}, {"text": "In contrast to other methods, we don't pre-define the number of clusters, and formalize the problem of clustering words as a non-linear optimization one, in which we specify the environments of words based on word clusters to be determined, rather than their neighboring words.", "labels": [], "entities": []}, {"text": "To solve the problem , we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word compositions have long been a concern in lexicography, and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, e.g., parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etc.(e.g.,.", "labels": [], "entities": [{"text": "parsing, generation, lexicon building", "start_pos": 212, "end_pos": 249, "type": "TASK", "confidence": 0.6740967780351639}, {"text": "word sense disambiguation", "start_pos": 251, "end_pos": 276, "type": "TASK", "confidence": 0.6653837561607361}, {"text": "information retrieving", "start_pos": 282, "end_pos": 304, "type": "TASK", "confidence": 0.7570386528968811}]}, {"text": "But due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries.", "labels": [], "entities": []}, {"text": "So an urgent problem occurs: how to automatically acquire word compositions?", "labels": [], "entities": []}, {"text": "In general, word compositions fall into two categories: free compositions and bound compositions, i.e., collocations.", "labels": [], "entities": []}, {"text": "Free compositions refer to those in which words can be replaced by other similar ones, while inbound compositions, words cannot be replaced freely).", "labels": [], "entities": []}, {"text": "Free compositions are predictable, i.e., their reasonableness can be determined according to the syntactic and semantic properties of the words in them.", "labels": [], "entities": []}, {"text": "While bound compositions are not predictable, i.e., their reasonableness cannot be derived from the syntactic and semantic properties of the words in them.", "labels": [], "entities": []}, {"text": "Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studied(e.g.,.", "labels": [], "entities": []}, {"text": "The key of their methods is to make use of some statistical means, e.g., frequencies or mutual information, to quantify the compositional strength between words.", "labels": [], "entities": []}, {"text": "These methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones.", "labels": [], "entities": []}, {"text": "This is because in free compositions, words are related with each other in a more loose way, which may result in the invalidity of mutual information and other statistical means in distinguishing reasonable compositions from unreasonable ones.", "labels": [], "entities": []}, {"text": "In this paper, we start from a different point to explore the problem of automatic acquisition of free compositions.", "labels": [], "entities": []}, {"text": "Although we cannot list all free compositions, we can select some typical ones as those specified in some dictionaries(e.g.,.", "labels": [], "entities": []}, {"text": "According to the properties held by free compositions, we can reasonably suppose that selected compositions can provide strong clues for others.", "labels": [], "entities": []}, {"text": "Furthermore we suppose that words can be classified into clusters, with the members in each cluster similar in their compositional ability, which can be characterized as the set of the words able to combined with them to form meaningful phrases.", "labels": [], "entities": []}, {"text": "Thus any given composition, although specifying the relation between two words literally, suggests the relation between two clusters.", "labels": [], "entities": []}, {"text": "So for each word(or clus-ter), there exist some word clusters, the word (or the words in the cluster) can and only can combine with the words in the clusters to form meaningful phrases.", "labels": [], "entities": []}, {"text": "We call the set of these clusters compositional frame of the word (or the cluster).", "labels": [], "entities": []}, {"text": "A seemingly plausible method to determine compositional frames is to make use of pre-defined semantic classes in some thesauri(e.g.,).", "labels": [], "entities": []}, {"text": "The rationale behind the method is to take such an assumption that if one word can be combined with another one to form a meaningful phrase, the words similar to them in meaning can also be combined with each other.", "labels": [], "entities": []}, {"text": "But it has been shown that the similarity between words in meaning doesn't correspond to the similarity in compositional ability.", "labels": [], "entities": []}, {"text": "So adopting semantic classes to construct compositional frames will result in considerable redundancy.", "labels": [], "entities": []}, {"text": "An alternative to semantic class is word cluster based on distributional environment, which in general refers to the surrounding words distributed around certain word (e.g.,, or the classes of them(, or more complex statistical means (.", "labels": [], "entities": []}, {"text": "According to the properties of the clusters in compositional frames, the clusters should be based on the environment, which, however, is narrowed in the given compositions.", "labels": [], "entities": []}, {"text": "Because the given compositions are listed by hand, it is impossible to make use of statistical means to form the environment, the remaining choices are surrounding words or classes of them.", "labels": [], "entities": []}, {"text": "Pereira et a1.(1993) put forward a method to cluster nouns in V-N compositions, taking the verbs which can combine with a noun as its environment.", "labels": [], "entities": []}, {"text": "Although its goal is to deal with the problem of data sparseness, it suffers from the problem itself.", "labels": [], "entities": []}, {"text": "A strategy to alleviate the effects of the problem is to cluster nouns and verbs simultaneously.", "labels": [], "entities": []}, {"text": "But as a result, the problem of word clustering becomes a bootstrapping one, or a non-linear one: the environment is also to be determined.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7725316882133484}]}, {"text": "proposed a definite method to deal with the generalized version of the non-linear problem, but it suffers from the problem of local optimization.", "labels": [], "entities": []}, {"text": "In this paper, we focus on A-N compositions in Chinese, and explore the problem of learning new compositions from given ones.", "labels": [], "entities": []}, {"text": "In order to copy with the problem of sparseness, we take adjective clusters as nouns' environment, and take noun clusters as adjectives' environment.", "labels": [], "entities": []}, {"text": "In order to avoid local optimal solutions, we propose a cooperative evolutionary strategy.", "labels": [], "entities": []}, {"text": "The method uses no specific knowledge of A-N structure, and can be applied to other structures.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: in section 2, we give a formal description of the problem.", "labels": [], "entities": []}, {"text": "In section 3, we discuss a kind of cooperative evolution strategy to deal with the problem.", "labels": [], "entities": []}, {"text": "In section 4, we explore the problem of parameter estimation.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.6424978971481323}]}, {"text": "In section 5, we present our experiments and the results as well as their evaluation.", "labels": [], "entities": []}, {"text": "In section 6, we give some conclusions and discuss future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We randomly select 30 nouns and 43 adjectives, and retrieve 164 compositions(see Appendix I) be-    With the two parameters increasing, aF decreases slowly, while BF increases severely, which demonstrates the fact that the learning of new compositions from the given ones has reached the limit at the point: the other reasonable compositions will be learned at a cost of severely raising the redundancy rate.", "labels": [], "entities": [{"text": "BF", "start_pos": 163, "end_pos": 165, "type": "METRIC", "confidence": 0.9978508949279785}]}, {"text": "From, we can see that o~F generally increases as ~1 and t2 increase, this is because that to increase the thresholds of the distances between clusters means to raise the abstract degree of the model, then more reasonable compositions will be learned.", "labels": [], "entities": []}, {"text": "On the other hand, we can see from that when tl _> 0.4, t2 >_ 0.4, fiR roughly increases as ~1 and ~2 increase, but when tz < 0.4, or t2 < 0.4, fir changes in a more confused manner.", "labels": [], "entities": []}, {"text": "This is because that when tl < 0.4, or ~2 < 0.4, it maybe the case that much more reasonable compositions and much less unreasonable ones are learned, with tl and t2 increasing, which may result in fiR's reduction, otherwise fir will increase, but when tz >_ 0.4, t2 > 0.4, most reasonable compositions have been learned, so it tend to be the case that more unreasonable compositions will be learned as tl and t2 increase, thus fir increases in a rough way.", "labels": [], "entities": [{"text": "fiR's reduction", "start_pos": 198, "end_pos": 213, "type": "METRIC", "confidence": 0.7616891662279764}]}, {"text": "To explore the relation between % aF and fiE, we reduce or add the given compositions, then estimate Q and t2, and compute aRE and fiR.", "labels": [], "entities": []}, {"text": "Their correspondence is listed in.", "labels": [], "entities": []}, {"text": "From, we can see that as 7 increases, the estimated values for tl and t2 will decrease, and BE will also decrease.", "labels": [], "entities": [{"text": "BE", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9995824694633484}]}, {"text": "This demonstrates that if given less compositions, we should select bigger values for the two parameters in order to learn as many reasonJi, He and Huang able compositions as possible, however, which will lead to non-expectable increase in fly.", "labels": [], "entities": [{"text": "fly", "start_pos": 240, "end_pos": 243, "type": "METRIC", "confidence": 0.9989778995513916}]}, {"text": "If given more compositions, we only need to select smaller values for the two parameters to learn as many reasonable compositions as possible.", "labels": [], "entities": []}, {"text": "We select other 10 groups of adjectives and nouns, each group contains 20 adjectives and 20 nouns.", "labels": [], "entities": []}, {"text": "Among the 10 groups, 5 groups hold a sufficiency rate about 58.2%, the other 5 groups a sufficiency rate about 72.5%.", "labels": [], "entities": [{"text": "sufficiency rate", "start_pos": 37, "end_pos": 53, "type": "METRIC", "confidence": 0.9799037277698517}, {"text": "sufficiency rate", "start_pos": 88, "end_pos": 104, "type": "METRIC", "confidence": 0.9752151966094971}]}, {"text": "We let ~1 -~ 0.4 and t2 = 0.4 for the former 5 groups, and let tl = 0.3 and t2 = 0.3 for the latter 5 groups respectively to further consider the relation between 7, o~F and fiR, with the values for the two parameters fixed.", "labels": [], "entities": []}, {"text": "demonstrates that for any given compositions with fixed sufficiency rate, there exist close values for the parameters, which make c~F and fir maintain lower values, and if given enough compositions, the mean errors of O~FF and fie will be lower.", "labels": [], "entities": [{"text": "F", "start_pos": 132, "end_pos": 133, "type": "METRIC", "confidence": 0.9648834466934204}, {"text": "FF", "start_pos": 220, "end_pos": 222, "type": "METRIC", "confidence": 0.6095989346504211}]}, {"text": "So if given a large number of adjectives and nouns to be clustered, we can extract a small sample to estimate the appropriate values for the two parameters, and then apply them into the original tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The relation between 7,~1,t2, aF and fiR.", "labels": [], "entities": []}, {"text": " Table 2: The relation between 7, mean O~F and mean", "labels": [], "entities": [{"text": "mean O~F", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7991867810487747}]}]}