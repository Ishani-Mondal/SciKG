{"title": [{"text": "Aggregate and mixed-order Markov models for statistical language processing", "labels": [], "entities": [{"text": "statistical language processing", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.7785162329673767}]}], "abstractContent": [{"text": "We consider the use of language models whose size and accuracy are intermediate between different order n-gram models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.999091625213623}]}, {"text": "Two types of models are studied in particular.", "labels": [], "entities": []}, {"text": "Aggregate Markov models are class-based bigram models in which the mapping from words to classes is probabilis-tic.", "labels": [], "entities": []}, {"text": "Mixed-order Markov models combine bigram models whose predictions are conditioned on different words.", "labels": [], "entities": []}, {"text": "Both types of models are trained by Expectation-Maximization (EM) algorithms for maximum likelihood estimation.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.5619965195655823}]}, {"text": "We examine smoothing procedures in which these models are interposed between different order n-grams.", "labels": [], "entities": []}, {"text": "This is found to significantly reduce the perplexity of unseen word combinations .", "labels": [], "entities": []}], "introductionContent": [{"text": "The purpose of a statistical language model is to assign high probabilities to likely word sequences and low probabilities to unlikely ones.", "labels": [], "entities": []}, {"text": "The challenge here arises from the combinatorially large number of possibilities, only a fraction of which can ever be observed.", "labels": [], "entities": []}, {"text": "In general, language models must learn to recognize word sequences that are functionally similar but lexically distinct.", "labels": [], "entities": []}, {"text": "The learning problem, one of generalizing from sparse data, is particularly acute for large-sized vocabularies.", "labels": [], "entities": []}, {"text": "The simplest models of natural language are ngram Markov models.", "labels": [], "entities": []}, {"text": "In these models, the probability of each word depends on the n-1 words that precede it.", "labels": [], "entities": []}, {"text": "The problems in estimating robust models of this form are well-documented.", "labels": [], "entities": []}, {"text": "The number of parameters--or transition probabilities--scales as V n, where V is the vocabulary size.", "labels": [], "entities": []}, {"text": "For typical models (e.g., n = 3, V = 104), this number exceeds by many orders of magnitude the total number of words in any feasible training corpus.", "labels": [], "entities": []}, {"text": "The transition probabilities in n-gram models are estimated from the counts of word combinations in the training corpus.", "labels": [], "entities": []}, {"text": "Maximum likelihood (ML) estimation leads to zero-valued probabilities for unseen n-grams.", "labels": [], "entities": [{"text": "ML) estimation", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.6558971107006073}]}, {"text": "In practice, one adjusts or smoothes) the ML estimates so that the language model can generalize to new phrases.", "labels": [], "entities": []}, {"text": "Smoothing can be done in many ways--for example, by introducing artificial counts, backing off to lowerorder models, or combining models by interpolation.", "labels": [], "entities": [{"text": "Smoothing", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.978166937828064}]}, {"text": "Often a great deal of information:is lost in the smoothing procedure.", "labels": [], "entities": []}, {"text": "This is due to the great discrepancy between n-gram models of different order.", "labels": [], "entities": []}, {"text": "The goal of this paper is to investigate models that are intermediate, in both size and accuracy, between different order n-gram models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9973363280296326}]}, {"text": "We show that such models can \"intervene\" between different order ngrams in the smoothing procedure.", "labels": [], "entities": []}, {"text": "Experimentally, we find that this significantly reduces the perplexity of unseen word combinations.", "labels": [], "entities": []}, {"text": "The language models in this paper were evaluated on the ARPA North American Business News (NAB) corpus.", "labels": [], "entities": [{"text": "ARPA North American Business News (NAB) corpus", "start_pos": 56, "end_pos": 102, "type": "DATASET", "confidence": 0.9042874640888638}]}, {"text": "All our experiments used a vocabulary of sixty-thousand words, including tokens for punctuation, sentence boundaries, and an unknown word token standing for all out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "The training data consisted of approximately 78 million words (three million sentences); the test data, 13 million words (one-half million sentences).", "labels": [], "entities": []}, {"text": "All sentences were drawn randomly without replacement from the NAB corpus.", "labels": [], "entities": [{"text": "NAB corpus", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.987566739320755}]}, {"text": "All perplexity figures given in the paper are computed by combining sentence probabilities; the probability of sentence wow1 ...w~wn+l is given by yIn+lP(wilwo ..wi-1), where w0 and wn+l are i=1 the start-and end-of-sentence markers, respectively.", "labels": [], "entities": []}, {"text": "Though not reported below, we also confirmed that the results did not vary significantly for different randomly drawn test sets of the same size.", "labels": [], "entities": []}, {"text": "The organization of this paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we examine aggregate Markov models, or class-based bigram models) in which the mapping from words to classes is probabilistic.", "labels": [], "entities": []}, {"text": "We describe an iterative algorithm for discovering \"soft\" word classes, based on the Expectation-Maximization (EM) procedure for maximum likelihood estimation.", "labels": [], "entities": []}, {"text": "Several features make this algorithm attractive for large-vocabulary language modeling: it has no tuning parameters, converges monotonically in the log-likelihood, and handles probabilistic constraints in a natural way.", "labels": [], "entities": []}, {"text": "The number of classes, C, can be small or large depending on the constraints of the modeler.", "labels": [], "entities": []}, {"text": "Varying the number of classes leads to models that are intermediate between unigram (C = 1) and bigram (C = V) models.", "labels": [], "entities": []}, {"text": "In Section 3, we examine another sort of \"intermediate\" model, one that arises from combinations of non-adjacent words.", "labels": [], "entities": []}, {"text": "Language models using such combinations have been proposed by,, and Rosenfeld (1996), among others.", "labels": [], "entities": []}, {"text": "We consider specifically the skip-k transition matrices, M(wt_k, wt), whose predictions are conditioned on the kth previous word in the sentence.", "labels": [], "entities": []}, {"text": "(The value of k determines how many words one \"skips\" back to make the prediction.)", "labels": [], "entities": []}, {"text": "These predictions, conditioned on only a single previous word in the sentence, are inherently weaker than those conditioned on all k previous words.", "labels": [], "entities": []}, {"text": "Nevertheless, by combining several predictions of this form (for different values of k), we can create a model that is intermediate in size and accuracy between bigram and trigram models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9979994893074036}]}, {"text": "Mixed-order Markov models express the predictions P(wt[wt-1, wt-2,..., Wt-m) as a convex combination of skip-k transition matrices, M(wt-k, wt).", "labels": [], "entities": []}, {"text": "We derive an EM algorithm to learn the mixing coefficients, as well as the elements of the transition matrices.", "labels": [], "entities": []}, {"text": "The number of transition probabilities in these models scales as mV 2, as opposed to V m+l.", "labels": [], "entities": []}, {"text": "Mixed-order models are not as powerful as trigram models, but they can make much stronger predictions than bigram models.", "labels": [], "entities": []}, {"text": "The reason is that quite often the immediately preceding word has less predictive value than earlier words in the same sentence.", "labels": [], "entities": []}, {"text": "In Section 4, we use aggregate and mixed-order models to improve the probability estimates from n-grams.", "labels": [], "entities": []}, {"text": "This is done by interposing these models between different order n-grams in the smoothing procedure.", "labels": [], "entities": []}, {"text": "We compare our results to a baseline trigram model that backs off to bigram and unigram models.", "labels": [], "entities": []}, {"text": "The use of \"intermediate\" models is found to reduce the perplexity of unseen word combinations by over 50%.", "labels": [], "entities": []}, {"text": "In Section 5, we discuss some extensions to these models and some open problems for future research.", "labels": [], "entities": []}, {"text": "We conclude that aggregate and mixed-order models provide a compelling alternative to language models based exclusively on n-grams.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perplexities of aggregate Markov models on  the training and test sets; C is the number of classes.  The case C = 1 corresponds to a ML unigram model;  C = V, to a ML bigram model.", "labels": [], "entities": []}, {"text": " Table 2: Most probable assignments for the 300 most frequent words in an aggregate Markov model with  C = 32 classes. Class 14 is absent because it is not the most probable class for any of the selected words.)", "labels": [], "entities": []}, {"text": " Table 3: Results for ML mixed-order models; m de- notes the number of bigrams that were mixed into  each prediction. The first column shows the per- plexities on the training set. The s.ec0nd shows the  fraction of words in the test set that were assigned  zero probability. The case m = 1 corresponds to a  ML bigram model.", "labels": [], "entities": [{"text": "ML mixed-order", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8459790349006653}]}, {"text": " Table 5: Perplexities of bigram models smoothed by  aggregate Markov models with different numbers of  classes (C).", "labels": [], "entities": []}, {"text": " Table 6: Perplexities of smoothed mixed-order mod- els on the validation and test sets.", "labels": [], "entities": []}, {"text": " Table 6. This was  handled by a slight variant of the Katz procedure", "labels": [], "entities": []}, {"text": " Table 7: Perplexities of two smoothed trigram mod- els on the test set and the subset of unseen word  combinations. The baseline model backed off to bi- grams and unigrams; the other backed off to the  m = 2 model in Table 6.", "labels": [], "entities": []}, {"text": " Table 8: Effect of truncating trigrams that occur  less than t times. The table shows the baseline and  mixed-order perplexities on the test set, the num- ber of distinct trigrams with t or more counts, and  the fraction of trigrams in the test set that required  backing off.", "labels": [], "entities": []}]}