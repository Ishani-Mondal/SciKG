{"title": [{"text": "Resolving PP attachment Ambiguities with Memory-Based Learning", "labels": [], "entities": [{"text": "Resolving PP attachment", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9004153807957967}]}], "abstractContent": [{"text": "In this paper we describe the application of Memory-Based Learning to the problem of Prepositional Phrase attachment disam-biguation.", "labels": [], "entities": [{"text": "Prepositional Phrase attachment disam-biguation", "start_pos": 85, "end_pos": 132, "type": "TASK", "confidence": 0.7853359878063202}]}, {"text": "We compare Memory-Based Learning, which stores examples in memory and generalizes by using intelligent similarity metrics, with a number of recently proposed statistical methods that are well suited to large numbers of features.", "labels": [], "entities": []}, {"text": "We evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space.", "labels": [], "entities": []}], "introductionContent": [{"text": "A central issue in natural language analysis is structural ambiguity resolution.", "labels": [], "entities": [{"text": "natural language analysis", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.6522191762924194}, {"text": "structural ambiguity resolution", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6435377796490988}]}, {"text": "A sentence is structurally ambiguous when it can be assigned more than one syntactic structure.", "labels": [], "entities": []}, {"text": "The drosophila of structural ambiguity resolution is Prepositional Phrase (PP) attachment.", "labels": [], "entities": [{"text": "structural ambiguity resolution", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.7442691922187805}, {"text": "Prepositional Phrase (PP) attachment", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.5111121386289597}]}, {"text": "Several sources of information can be used to resolve PP attachment ambiguity.", "labels": [], "entities": [{"text": "PP attachment ambiguity", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.9060325622558594}]}, {"text": "Psycholinguistic theories have resulted in disambiguation strategies which use syntactic information only, i.e. structural properties of the parse tree are used to choose between different attachment sites.", "labels": [], "entities": []}, {"text": "Two principles based on syntactic information are Minimal Attachment (MA) and Late Closure (LC).", "labels": [], "entities": [{"text": "Minimal Attachment (MA)", "start_pos": 50, "end_pos": 73, "type": "METRIC", "confidence": 0.6878077030181885}, {"text": "Late Closure (LC)", "start_pos": 78, "end_pos": 95, "type": "METRIC", "confidence": 0.9503061175346375}]}, {"text": "MA tries to construct the parse tree that has the fewest nodes, whereas LC tries to attach new constituents as low in the parse tree as possible.", "labels": [], "entities": []}, {"text": "These strategies always choose the same attachment regardless of the lexical content of the sentence.", "labels": [], "entities": []}, {"text": "This results in a wrong attachment in one of the following sentences: 1 She cats pizza with a fork.", "labels": [], "entities": []}, {"text": "2 She cats pizza with anchovies.", "labels": [], "entities": []}, {"text": "In sentence 1, the PP \"with a fork\" is attached to the verb \"eats\" (high attachment).", "labels": [], "entities": []}, {"text": "Sentence 2 differs only minimally from the first sentence; here, the PP \"with anchovies\" does not attach to the verb but to the NP \"pizza\" (low attachment).", "labels": [], "entities": [{"text": "Sentence 2", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9144653677940369}]}, {"text": "In languages like English and Dutch, in which there is very little overt case marking, syntactic information alone does not suffice to explain the difference in attachment sites between such sentences.", "labels": [], "entities": []}, {"text": "The use of syntactic principles makes it necessary to re-analyse the sentence, using semantic or even pragmatic information, to reach the correct decision.", "labels": [], "entities": []}, {"text": "In the example sentences 1 and 2, the meaning of the head of the object of 'with' determines low or high attachment.", "labels": [], "entities": []}, {"text": "Several semantic criteria have been worked out to resolve structural ambiguities.", "labels": [], "entities": []}, {"text": "However, pinning down the semantic properties of all the words is laborious and expensive, and is only feasible in a very restricted domain.", "labels": [], "entities": []}, {"text": "The modeling of pragmatic inference seems to be even more difficult in a computational system.", "labels": [], "entities": [{"text": "modeling of pragmatic inference", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6270488202571869}]}, {"text": "Due to the difficulties with the modeling of semantic strategies for ambiguity resolution, an attractive alternative is to look at the statistics of word patterns in annotated corpora.", "labels": [], "entities": [{"text": "ambiguity resolution", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7605538368225098}]}, {"text": "In such a corpus, different kinds of information used to resolve attachment ambiguity are, implicitly, represented in co-occurrence regularities.", "labels": [], "entities": []}, {"text": "Several statistical techniques can use this information in learning attachment ambiguity resolution. were the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results.", "labels": [], "entities": [{"text": "learning attachment ambiguity resolution.", "start_pos": 59, "end_pos": 100, "type": "TASK", "confidence": 0.7776181548833847}, {"text": "PP attachment ambiguity resolution", "start_pos": 156, "end_pos": 190, "type": "TASK", "confidence": 0.7570083439350128}]}, {"text": "For sentences with a verb~noun attachment ambiguity, they measured the lexical association between the noun and the preposition, and the verb and the preposition in unambiguous sentences.", "labels": [], "entities": []}, {"text": "Their method bases attachment decisions on the ratio and More recently, a number of statistical methods better suited to larger numbers of features have been proposed for PP-attachment.", "labels": [], "entities": []}, {"text": "appl!ed Error-Driven TransformationBased Learning, Ratnaparkhi, applied a Maximum Entropy model, used a Loglinear model, and obtained good results using a BackOff model.", "labels": [], "entities": []}, {"text": "In this paper, we examine whether Memory-Based Learning (MBL), a family of statistical methods from the field of Machine Learning, can improve on the performance of previous approaches.", "labels": [], "entities": []}, {"text": "MemoryBased Learning is described in Section 2.", "labels": [], "entities": []}, {"text": "In order to make a fair comparison, we evaluated our methods on the common benchmark dataset first used in. in section 3, the experiments with our method on this data are described.", "labels": [], "entities": []}, {"text": "An important advantage of MBL is its use of similarity-based reasoning.", "labels": [], "entities": [{"text": "MBL", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.700444757938385}]}, {"text": "This makes it suited to the use of various unconventional representations of word patterns (Section 2).", "labels": [], "entities": []}, {"text": "In Section 3 a comparison is provided between two promising representational forms, Section 4 contains a comparison of our method to previous work, and we conclude with section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of the PP attachment data set.", "labels": [], "entities": [{"text": "PP attachment data set", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.878612756729126}]}, {"text": " Table 3: Scores on the Ratnaparkhi et al. PP-attachment test set (see text); the scores of Maximum Entropy  are taken from", "labels": [], "entities": [{"text": "Ratnaparkhi et al. PP-attachment test set", "start_pos": 24, "end_pos": 65, "type": "DATASET", "confidence": 0.5822712821619851}]}, {"text": " Table 4: Scores on the Ratnaparkhi et al. PP- attachment test set with Lexical Space representa- tions. The values of k, the voting function, and the  IG weights were determined on the training and val- idation sets.", "labels": [], "entities": [{"text": "Ratnaparkhi et al. PP- attachment test set", "start_pos": 24, "end_pos": 66, "type": "DATASET", "confidence": 0.5674964421325259}, {"text": "IG", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.7796877026557922}]}]}