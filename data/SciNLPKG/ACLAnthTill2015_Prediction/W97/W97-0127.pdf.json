{"title": [{"text": "Probabilistic Word Classification Based on Context- Sensitive Binary Tree Method", "labels": [], "entities": [{"text": "Word Classification", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.6390871852636337}]}], "abstractContent": [{"text": "Corpus-based statistical-oriented Chinese word classification can be regarded as a fundamental step for automatic or non-automatic, mono-lingual natural processing system.", "labels": [], "entities": [{"text": "Corpus-based statistical-oriented Chinese word classification", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.5070199191570282}]}, {"text": "Word classification can solve the problems of data sparseness and have far fewer parameters.", "labels": [], "entities": [{"text": "Word classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6806773245334625}]}, {"text": "So far, much r:,la~v~ work about word classification has been done.", "labels": [], "entities": [{"text": "word classification", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8429203927516937}]}, {"text": "All the work is based on some similarity metrics.", "labels": [], "entities": []}, {"text": "We use average mutual information as global similarity metric to do classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.9782309532165527}]}, {"text": "The clustering process is top-down splitting and the binary tree is growin~ with splitting.", "labels": [], "entities": []}, {"text": "In natural lan~lage, the effect of left neighbors and right neighbors of a word are asymmetric.", "labels": [], "entities": []}, {"text": "To utilize this directional information, we induce the left-right binary and right-left binary tree to represent this property.", "labels": [], "entities": []}, {"text": "The probability is also introduced in our algorithm to merge the resulting classes from left-right and right-left binary tree.", "labels": [], "entities": []}, {"text": "Also, we use the resulting classes to do experiments on word class-based language model.", "labels": [], "entities": []}, {"text": "Some classes results and perplexity of word class-based language model are presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word c'lassification play an important role in computational _i:~gu~s~.~.", "labels": [], "entities": []}, {"text": "Many casks in computational linguistics, whether they use statistical or symbolic methods, reduce the complexity of the probl~m by dealing with classes of words rather than individual words.", "labels": [], "entities": []}, {"text": "we know that some words share similar sorts of linguistic properties, thus they should belong to the same class.", "labels": [], "entities": []}, {"text": "Some words have several functions, thus they could belong to more than one class.", "labels": [], "entities": []}, {"text": "The questions are: What attributes distinguish one word from another?", "labels": [], "entities": []}, {"text": "How should we group similar words together so that the partition of word spaces is most likely, to reflect the linguistic properties of languaqe?", "labels": [], "entities": []}, {"text": "What meaningful label or name should be given to each word group?", "labels": [], "entities": []}, {"text": "These questions constitute the problem of finding a word classification.", "labels": [], "entities": [{"text": "word classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7052256613969803}]}, {"text": "At present, no method can find the optimal word classification.", "labels": [], "entities": [{"text": "word classification", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7222979813814163}]}, {"text": "However, researchers have been trying hard to find sub-optimal strategies which lead to useful classification.", "labels": [], "entities": []}, {"text": "From practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models.", "labels": [], "entities": [{"text": "word classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7870227992534637}]}, {"text": "Specially, it can be used as an alternative to grammatical part-ofspeech tagging) on statistical language modeling(Huang, Alleva, Hwang,, because Chinese language models u@ing part-of-speech information have had only a very limited success(e.g.).", "labels": [], "entities": [{"text": "grammatical part-ofspeech tagging", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.647115538517634}, {"text": "statistical language modeling", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.6758320927619934}]}, {"text": "The reason why there are so many of the difficulties in Chineuc part of-speech tagging are described by and.", "labels": [], "entities": [{"text": "Chineuc part of-speech tagging", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.7039063423871994}]}, {"text": "Much relative work on word classification has been done.", "labels": [], "entities": [{"text": "word classification", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8530318439006805}]}, {"text": "The work is based on some similarity metrics.", "labels": [], "entities": []}, {"text": "(; Wu, Wang,) and Pop (1996) present a transformation-based tagging.", "labels": [], "entities": []}, {"text": "Before a part-of-speech tagger can be built, the word classifications are performed to help us choose a set of part-of-speech.", "labels": [], "entities": []}, {"text": "They use the sum of two relative entropies obtained from neighboring words as the similarity metric to compare two words.", "labels": [], "entities": []}, {"text": "shows a long-distance left and right context of a word as left vector and right vector and the dimensions of each vector are 50.", "labels": [], "entities": []}, {"text": "MR ~ses Cosine as metric to measure the similarity between two words.", "labels": [], "entities": [{"text": "MR ~ses Cosine", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.7994250059127808}]}, {"text": "To solve the sparseness of the data, he applies a singular value decomposition.", "labels": [], "entities": []}, {"text": "Comparing with Brill,E.'s method, Schutze,H. takes 50 neighbors into account for each word.", "labels": [], "entities": []}, {"text": "proposed a simulated annealing method, the same as.", "labels": [], "entities": []}, {"text": "The pel-plexity, which is the inverse of the probability over the whole text, is measured will decide whether'a new classification ( obtained by moving only one word from its class to another, both word and class being randomly chosen) will replace a previous one.", "labels": [], "entities": []}, {"text": "Compared to the two methods described above, this method attempts to optimize the clustering using perplexity as a global measure.", "labels": [], "entities": []}, {"text": "Pereira, Tishby and Lee (1993) investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves.", "labels": [], "entities": []}, {"text": "More specifically, they model senses as probabilistic concepts or clusters C with correspondin~ cluster membership probabilities P(Clw ) for each word w.", "labels": [], "entities": []}, {"text": "That is, while most other class-based modeling techniques for natural language rely on \"hard\" Boolean classes, propose a method for \"soft\" class clustering.", "labels": [], "entities": []}, {"text": "He suggests a deterministic annealing procedure for clustering.", "labels": [], "entities": []}, {"text": "But as stated in their paper, they only considers the special case of classifying nouns according to the distribution as direct objects of verbs.", "labels": [], "entities": []}, {"text": "To addless the problems and utilize the advantages of the methods presented above, we put forward anew algorithm to automatically classify the words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Pentium 586/133MHz, 32M memory to calculate.", "labels": [], "entities": []}, {"text": "The OS is Windows NT 4.0.", "labels": [], "entities": []}, {"text": "And Visual C++ 4.0 is our programming language.", "labels": [], "entities": []}, {"text": "We use the electric news corpus named \"Chinese One hundred kinds of newspapers---1994\".", "labels": [], "entities": [{"text": "Chinese One hundred kinds of newspapers---1994", "start_pos": 39, "end_pos": 85, "type": "DATASET", "confidence": 0.7605510577559471}]}, {"text": "The total size of it is 780 million bytes.", "labels": [], "entities": []}, {"text": "It is not feasible to do classification experiments on this original corpus.", "labels": [], "entities": []}, {"text": "So, we extract apart of it which contain the news published in April from the original news texts.", "labels": [], "entities": []}, {"text": "\" and \" \" which denote the beginning and end of the sentence or word phrase respectively.", "labels": [], "entities": []}, {"text": "The texts are segmented by Variable-distance algorithm[ Gao, J. and Chen, X.X. (1996)] We select four subcorpora which contains 10323, 17451, 25130 and 44326 Chinese words.", "labels": [], "entities": []}, {"text": "The vocabulary contains 2103, 3577, 4606 and 6472 words correspondingly.", "labels": [], "entities": []}, {"text": "The results of the classification without introducing probabilities can be summarized in.", "labels": [], "entities": []}, {"text": "The computation of merging process is only equal to the splitting calculation in one level in the tree.", "labels": [], "entities": []}, {"text": "From table I, we can find surprisely that the computation time for right-left is much shorter than the time for left-right.", "labels": [], "entities": []}, {"text": "In the process of left-right, the left branch contains more words than the right branch.", "labels": [], "entities": []}, {"text": "To move each word from the left branch to the right branch, we need to match this word throughout the corpus.", "labels": [], "entities": []}, {"text": "But when we do the process of right-left, the left branch has less words than the right.", "labels": [], "entities": []}, {"text": "We only need to match the small number of words in the corpus.", "labels": [], "entities": []}, {"text": "From this, we can know that the preprocessed procedure costs much time.", "labels": [], "entities": []}, {"text": "The number of empty classes is increasing with the tree grows.", "labels": [], "entities": []}, {"text": "shows the number of empty classes in different levels in the leftright tree when we process the subcorpora containing 10323 words.", "labels": [], "entities": []}, {"text": "Although our method is to calculate distributional classification, it still demonstrates that it has powerful part-of-speech functions.", "labels": [], "entities": [{"text": "distributional classification", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7562639117240906}]}, {"text": "But some of classes present no obvious part-of-speech category.", "labels": [], "entities": []}, {"text": "Most of them conZain only' very small number of words.", "labels": [], "entities": []}, {"text": "This may caused by the predefined classification number.", "labels": [], "entities": []}, {"text": "Thus, excessive or insufficient classification maybe encountered.", "labels": [], "entities": []}, {"text": "And another shortcoming is that a small number of words in almost every resulting class doesn't belong to the part-of-speech categories which most of words in that class belong to.", "labels": [], "entities": []}], "tableCaptions": []}