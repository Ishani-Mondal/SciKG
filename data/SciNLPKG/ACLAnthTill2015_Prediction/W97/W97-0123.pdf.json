{"title": [{"text": "Maximum Entropy Model Learning of Subcategorization Preference* It", "labels": [], "entities": [{"text": "Subcategorization Preference", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.764360100030899}]}], "abstractContent": [{"text": "This paper proposes a novel method for learning probabilistic models of subcategorization preference of verbs.", "labels": [], "entities": []}, {"text": "Especially, we propose to consider the issues of case dependencie~ and noun class generalization in a uniform way.", "labels": [], "entities": [{"text": "noun class generalization", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.661749025185903}]}, {"text": "We adopt the maximum entropy model learn~,g method and apply it to the task of model learning of subcategorization preference.", "labels": [], "entities": []}, {"text": "Case dependencies and noun class generalization are represented as featura~ in the maximum entropy approach.", "labels": [], "entities": [{"text": "noun class generalization", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6135841210683187}]}, {"text": "The feature selection facility of the maximum entropy model learning makes it possible to find optimal case dependencies and optimal noun c!~ generalization levels.", "labels": [], "entities": []}, {"text": "We describe the results of the experiment on learning probabilistic models of subcategorization preference f~om the EDR Japanese bracketed corpus.", "labels": [], "entities": [{"text": "EDR Japanese bracketed corpus", "start_pos": 116, "end_pos": 145, "type": "DATASET", "confidence": 0.9295417666435242}]}, {"text": "We also evaluated the performance of the selected features and their estimated parameters in the subcategorization preference task.", "labels": [], "entities": []}], "introductionContent": [{"text": "In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years.", "labels": [], "entities": []}, {"text": "In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.723155066370964}]}, {"text": "For example, in the context of syntactic disambiguation, and proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7983853220939636}, {"text": "statistical parsing", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.619389533996582}]}, {"text": "As lexical/semantic information, used about 50 semantic categories, while used lexical forms of words.", "labels": [], "entities": []}, {"text": "proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.", "labels": [], "entities": []}, {"text": "In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7609997093677521}]}, {"text": "They put an assumption that syntactic and lexical/semantic features are dependent on each other.", "labels": [], "entities": []}, {"text": "In their models, syntactic and lexical/semantic features are combined together, and this causes each parameter to depend on both syntactic and lexical/semantic features.", "labels": [], "entities": []}, {"text": "On the other hand, in the context of automatic lexicon construction, the emphasis is mainly on the extraction of lexical/semantic collocational knowledge of specific words rather than its use in sentence parsing.", "labels": [], "entities": [{"text": "automatic lexicon construction", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6889646053314209}, {"text": "extraction of lexical/semantic collocational knowledge of specific words", "start_pos": 99, "end_pos": 171, "type": "TASK", "confidence": 0.7377954423427582}, {"text": "sentence parsing", "start_pos": 195, "end_pos": 211, "type": "TASK", "confidence": 0.745953768491745}]}, {"text": "For example, applied an information-theoretic data compression technique to corpus-based case frame learning, and proposed a method of finding case frames of verbs as compressed representation of verb-noun collocational data in corpus.", "labels": [], "entities": [{"text": "information-theoretic data compression", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.7198859850565592}]}, {"text": "The work concentrated on the extraction of declarative representation of case frames and did not consider their performance in sentence parsing.", "labels": [], "entities": [{"text": "extraction of declarative representation of case frames", "start_pos": 29, "end_pos": 84, "type": "TASK", "confidence": 0.8275576744760785}, {"text": "sentence parsing", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.7111575901508331}]}, {"text": "\"The authors would like to thank Dr. Kentaro Inui and Mr. Kiyoaki Shirai of Tokyo Institute of Technology for valuable information on implementing maximum entropy model learning.", "labels": [], "entities": []}, {"text": "This research was partially supported By the Ministry of Education, Science, Sports and Culture, Japan, Grant-in-Aid for Encouragement of As in the case of the models of,, and, this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 299, "end_pos": 317, "type": "TASK", "confidence": 0.7553718984127045}]}, {"text": "However, unlike the models of,, and, we put an assumption that syntactic and lexical/semantie features are independent.", "labels": [], "entities": []}, {"text": "Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.", "labels": [], "entities": [{"text": "extracting lexical/semantic collocational knowledge of verbs", "start_pos": 18, "end_pos": 78, "type": "TASK", "confidence": 0.7837969362735748}, {"text": "syntactic analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7894023656845093}]}, {"text": "More specifically, we propose a novel method for learning a probabilistic model of subcategorization preference of verbs.", "labels": [], "entities": []}, {"text": "In general, when learning lexical/semantic eollocational knowledge of verbs from corpus, it is necessary to consider the following two issues:", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of the selected features and their estimated parameters in the following subcategorization preference task.", "labels": [], "entities": []}, {"text": "Suppose that the following word sequence represents a verb-final Japanese sentence with a subordinate clause, where N=,..., N2k are nouns, Pz,...", "labels": [], "entities": []}, {"text": ",P2~ are casemarking post-positional particles, and vl, v2 are verbs, and the first verb vi is the head verb of the subordinate clause.", "labels": [], "entities": []}, {"text": "We consider the subcategorization ambiguity of the post-positional phrase Nf-p=: i.e, whether Nz-pz is subcategorized for by vl or v2.", "labels": [], "entities": []}, {"text": "We use held-out verb-noun collocations of the verbs vl and v2 which are not used in the training.", "labels": [], "entities": []}, {"text": "They are like those verb-noun collocations in the left side below.", "labels": [], "entities": []}, {"text": "Next, we generate erroneous verbnoun collocations of vl and v2 as those in the right side below, by choosing a case element Px: N= at random and moving it from vl to v2.", "labels": [], "entities": []}, {"text": "[Co~ pf:N= Then, we compare the products \u00a2(t) (in the equation) of the conditional probabilities of the constituent verb-noun collocations between the correct and the erroneous pairs, and calculate the rate of selecting the correct pair.", "labels": [], "entities": []}, {"text": "(a)-~(c) compares the precisions re and rh among the one-frame/independent-fr~me/partialframe/independent-case models.", "labels": [], "entities": [{"text": "precisions", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9898051023483276}]}, {"text": "We also compare the changes of the rate of the verb-noun collocations in the test set which satisfy the case covering relation ~_co with the set ,q of active features.", "labels": [], "entities": []}, {"text": "For the independent-frame model, we examined two different values of the independence parameter a, i.e., c~ -0.5 as a weak condition on independence judgment and ~ -0.9 as a strict condition on independence judgment.", "labels": [], "entities": []}, {"text": "shows the changes of the precisions r~, rh, and re as well as the case-coverage of the test data during the training for the independent-frame model (the independence parameter ~ -0.9).", "labels": [], "entities": [{"text": "precisions", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.991821825504303}]}, {"text": "Both of the precisions re and rh of the independent-frame model are higher than those of any other models.", "labels": [], "entities": [{"text": "precisions re", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9621305167675018}]}, {"text": "On the other hand, the case-coverage of the independent-frame model (as well as the that of one-frame model) is much lower than that of the partial-frame/independent-case models.", "labels": [], "entities": []}, {"text": "The decrease of the case-coverage in the independentframe/one-frame models is caused by the overfitting to the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of Selected Features for ukau(buy, incur)\" (Independent-Frame Model(a = 0.9))  FOrder [[", "labels": [], "entities": [{"text": "FOrder", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.5866883993148804}]}]}