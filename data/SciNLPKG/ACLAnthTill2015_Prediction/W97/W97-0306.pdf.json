{"title": [], "abstractContent": [{"text": "Learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words.", "labels": [], "entities": []}, {"text": "Three characteristic properties of this domain are (a) very high dimensionality, (b) both the learned concepts and the instances reside very sparsely in the feature space, and (c) a high variation in the number of active features in an instance.", "labels": [], "entities": []}, {"text": "In this work we study three mistake-driven learning algorithms fora typical task of this nature-text categorization.", "labels": [], "entities": []}, {"text": "We argue that these algorithms-which categorize documents bY learning a linear separator in the feature space-have a few properties that make them ideal for this domain.", "labels": [], "entities": []}, {"text": "We then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain.", "labels": [], "entities": []}, {"text": "In particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training.", "labels": [], "entities": []}, {"text": "Overall, we present an algorithm, a variation of Littlestone's Winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set.", "labels": [], "entities": [{"text": "Littlestone's Winnow", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.85738072792689}]}], "introductionContent": [{"text": "Learning problems in the natural language and text processing domains are often studied by mapping the text to a space whose dimensions are the measured features of the text, e.g., the words appearing in a document.", "labels": [], "entities": []}, {"text": "Three characteristic propertie s of this domain are (a) very high dimensionality, (b) both the learned concepts and the instances reside very sparsely in the feature space and, consequently, (c) there is a high variation in the number of active features in an instance.", "labels": [], "entities": []}, {"text": "Multiplicative weight-updating algorithms such as Winnow have been studied extensively in the theoretical learning literature.", "labels": [], "entities": []}, {"text": "Theoretical analysis has shown that they have exceptionally good behavior in domains with these characteristics, and in particular in the presence of irrelevant attributes, noise, and even a target function changing in time, but only recently have people started to use them in applications.", "labels": [], "entities": []}, {"text": "We address these claims empirically in an important application domain for machine learning -text categorization.", "labels": [], "entities": [{"text": "machine learning -text categorization", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7313139319419861}]}, {"text": "In particular, we study mistake-driven learning algorithms that are based on the Winnow family/, and investigate ways to apply them in domains with the above characteristics.", "labels": [], "entities": [{"text": "Winnow family", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9218753576278687}]}, {"text": "The learning algorithms studied here offer a large space of choices to be made and, correspondingly, may vary widely in performance when applied in specific domains.", "labels": [], "entities": []}, {"text": "We concentrate hereon the text processing domain, with the characteristics mentioned above, and explore this space of choices in it.", "labels": [], "entities": [{"text": "text processing domain", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.812146782875061}]}, {"text": "In particular, we investigate three variations of on-line prediction algorithms and evaluate them experimentally on large text categorization problems.", "labels": [], "entities": []}, {"text": "The algorithms we study are all learning algorithms for linear functions.", "labels": [], "entities": []}, {"text": "They are used to categorize documents by learning, for each category, a linear separator in the feature space.", "labels": [], "entities": []}, {"text": "The algorithms differ by whether they allow the use of negative or only positive weights and by the way they update their weights during the training phase.", "labels": [], "entities": []}, {"text": "We find that while a vanilla version of these algorithms performs rather well, a quantum leap in performance is achieved when we modify the algorithms to better address some of the specific characteristics we identify in textual domains.", "labels": [], "entities": []}, {"text": "In particular, we address problems such as wide variations in document sizes, word repetitions and the need to rank documents rather than just decide whether they belong to a category or not.", "labels": [], "entities": [{"text": "word repetitions", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.6675351709127426}]}, {"text": "In some cases we adopt solutions that are well known in the IR literature to the class of algorithms we use; in others we modify known algorithms to better suit the characteristics of the domain.", "labels": [], "entities": [{"text": "IR", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9495462775230408}]}, {"text": "We motivate the modifications to the basic algorithms and justify them experimentally by exhibiting their contribution to improvement in performance.", "labels": [], "entities": []}, {"text": "Overall, the best variation we investigate, performs significantly better than any known algorithm tested on this task, using a similar set of features.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: The next section describes the task of text categorization, how we model it as a classification task, and some related work.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8094316720962524}]}, {"text": "The family of algorithms we use is introduced in Section 3 and the extensions to the basic algorithms, along with their experimental evaluations, is presented in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we present our final experimental results and compare them to previous works in the literature.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Recall/precision break-even point (in percentages) for different versions of the algorithm. Each  figure is an average result for two pairs of training and testing sets, each containing 2000 training documents  and 1000 test documents.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9944851994514465}, {"text": "precision break-even point", "start_pos": 17, "end_pos": 43, "type": "METRIC", "confidence": 0.900724470615387}]}, {"text": " Table 2: Break-even points comparison. The data is split into training set and test set based on Lewis's  split -(Lewis, 1992), 14704 documents for training, 6746 for testing, and Apte's split -(Apte,", "labels": [], "entities": [{"text": "Apte's split -(Apte", "start_pos": 181, "end_pos": 200, "type": "METRIC", "confidence": 0.795240068435669}]}]}