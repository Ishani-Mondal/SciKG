{"title": [{"text": "A Self-Organlzing Japanese Word Segmenter using He-ristic Word Identification and Re-estimation", "labels": [], "entities": [{"text": "Self-Organlzing Japanese Word Segmenter", "start_pos": 2, "end_pos": 41, "type": "TASK", "confidence": 0.5367710366845131}]}], "abstractContent": [{"text": "We present a self-organized method to build a stochastic Japanese word segmenter from a small number of basic words and a large amount of unsegmented training text.", "labels": [], "entities": [{"text": "Japanese word segmenter", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6105741361776987}]}, {"text": "It consists of a word-based statistical language model, an initial estimation procedure, and a re-estimation procedure.", "labels": [], "entities": []}, {"text": "Initial word frequencies are estimated by counting all possible longest match strings between the training text and the word list.", "labels": [], "entities": []}, {"text": "The initial word list is au~nented by identifying words in the training text using a heuristic rule based on character type.", "labels": [], "entities": []}, {"text": "The word-based language model is then re-estimated to filter out inappropriate word hypotheses generated by the initial word identification.", "labels": [], "entities": []}, {"text": "When the word segmeuter is trained on 3.9M character texts and 1719 initial words, its word segmentation accuracy is 86.3% recall and 82.5% precision.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.6700253486633301}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9151993989944458}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9991243481636047}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9984404444694519}]}, {"text": "We find that the combination of heuristic word identi~cation and re-estimation is so effective that the initial word list need not be large.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word segmentation is an important problem for Japanese because word boundaries are not marked in its writing system.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7163898646831512}]}, {"text": "Other Asian languages such as Chinese and Thai have the same problem.", "labels": [], "entities": []}, {"text": "Any Japanese NLP application requ/res word segmentation as the first stage because there are phonological and semantic units whose pronunciation and meaning is not trivially derivable from that of the individual characters.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7869141399860382}]}, {"text": "Once word segmentation is done, all established techniques can be exploited to build practically important applications such as spelling correction and text retrieval Ina sense, Japanese word segmentation is a solved problem if (and only if) we have plenty of segmented training text.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7320061475038528}, {"text": "spelling correction", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.9291633367538452}, {"text": "text retrieval", "start_pos": 152, "end_pos": 166, "type": "TASK", "confidence": 0.77693310379982}, {"text": "Japanese word segmentation", "start_pos": 178, "end_pos": 204, "type": "TASK", "confidence": 0.5976870159308115}]}, {"text": "Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programi-g procedure.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7293674349784851}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9507995843887329}]}, {"text": "However, manually segmented corpora are not always available in a particular target domain and manual segmentation is very expensive.", "labels": [], "entities": []}, {"text": "The goal of our research is unsupervised learning of Japanese word segmentation.", "labels": [], "entities": [{"text": "Japanese word segmentation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.5351152022679647}]}, {"text": "That is, to build a Japanese word segmenter from a list of initial words and unsegmented training text.", "labels": [], "entities": [{"text": "Japanese word segmenter", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.5889574686686198}]}, {"text": "Today, it is easy to obtain a 10K-100K word list from either commercial or public domain on-line Japanese dictionaries.", "labels": [], "entities": []}, {"text": "Gigabytes of Japanese text are readily available from newspapers, patents, HTML documents, etc..", "labels": [], "entities": []}, {"text": "Few works have examined unsupervised word segmentation in Japanese.", "labels": [], "entities": [{"text": "unsupervised word segmentation", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.7061106363932291}]}, {"text": "Both and built a word-based language model from unsegmented text using a re-estimation procedure whose initial segmentation was obtained by a rule-based word segreenter.", "labels": [], "entities": []}, {"text": "The utility of this approach is limited because it presupposes the existence of a rule-based word segmenter like JUMAN.", "labels": [], "entities": []}, {"text": "It is impossible to build a word segmenter fora new domain without human intervention.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7304310500621796}]}, {"text": "For Chinese word segmentation, more self-organized approaches have been tried.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.5992707312107086}]}, {"text": "built a word unigram model using the Viterbi re-estimation whose initial estimates were derived from the frequencies in the corpus of the strings of each word in the lexicon.", "labels": [], "entities": []}, {"text": "combined a small seed segmented corpus and a large unsegmented corpus to build a word unigram model using the Viterbi re-estimation.", "labels": [], "entities": []}, {"text": "proposed a re-estimation procedure which alternates word segmentation and word frequency re-estimation on each half of the training text divided into halves.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6716629713773727}]}, {"text": "One of the major problems in unsupervised word segmentation is the treatment of unseen words.", "labels": [], "entities": [{"text": "unsupervised word segmentation", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.6905242304007212}]}, {"text": "wrote lexical rules for each productive morphological process, such as plural noun formation, Chinese personal names, and transliterations of foreign words.", "labels": [], "entities": [{"text": "plural noun formation", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.6552511552969614}]}, {"text": "used a statistical method called \"Two-Class Classifier\", which decided whether the string is actually a word based on the features derived from character N-gram.", "labels": [], "entities": []}, {"text": "In this paper, we present a self-organized method to build a Japanese word segmenter from a small number of basic words and a large amount of unsegmented training text using a novel re-estimation procedure.", "labels": [], "entities": [{"text": "Japanese word segmenter", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.5735805829366049}]}, {"text": "The major contribution of this paper is its treatment of unseen words.", "labels": [], "entities": [{"text": "treatment of unseen words", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.8676992058753967}]}, {"text": "We devised a statistical word formation model for unseen words which can be re-estimated.", "labels": [], "entities": [{"text": "word formation", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.673311710357666}]}, {"text": "We show that it is very effective to combine a heuristic initial word identification method with a reestimation procedure to filter out inappropriate word hypotheses.", "labels": [], "entities": [{"text": "heuristic initial word identification", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.6521619856357574}]}, {"text": "We also devised anew method to estimate initial word frequencies.", "labels": [], "entities": []}, {"text": "shows the configuration of our Japanese word segmenter.", "labels": [], "entities": [{"text": "Japanese word segmenter", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6449436744054159}]}, {"text": "In the following sections, we ffirst describe the statistical language model and the word segmentation algorithm.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7412902563810349}]}, {"text": "We then describe the initial word frequency estimation method and the initial word identification method.", "labels": [], "entities": [{"text": "word frequency estimation", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.5015391806761423}, {"text": "word identification", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.6788751184940338}]}, {"text": "Finally, we describe the experiment results of unsupervised word segmentation under various conditions.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7428039014339447}]}, {"text": "m 2 Language Model -rid Word Segmentation Algorithm 2.1 Word Segmentation Model k Let the input Japanese character sequence be C = ClC2 ... cm.", "labels": [], "entities": [{"text": "Word Segmentation Algorithm 2.1 Word Segmentation", "start_pos": 24, "end_pos": 73, "type": "TASK", "confidence": 0.6473346749941508}]}, {"text": "Our goal is to segment it into I word sequence W = wlw2.., w,.", "labels": [], "entities": []}, {"text": "The word segmentation task can be defined as finding a word segmentation l~ r that maximizes the joint probability of word sequence given character sequence Ill P(W[C).", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8261274695396423}]}, {"text": "Since the maximization is carried outwith fixed character sequence C, the word segmenter \u2022 only has to maximize the probability of the word sequence P(W).", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.6678865104913712}]}], "datasetContent": [{"text": "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses.", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.5840714424848557}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9218196868896484}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9991616010665894}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9992120265960693}, {"text": "bracketing of partial parses", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.8957178592681885}]}, {"text": "Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M.", "labels": [], "entities": []}, {"text": "Recall is defined as M/Std, and precision is defined as M/Sys.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9721692204475403}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9994263648986816}]}, {"text": "Since it is inconvenient to use both recall and precision all the we also use the F-measure to indicate the overall performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9992150068283081}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9978729486465454}, {"text": "F-measure", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.99837327003479}]}, {"text": "The F-measure was originally developed by the information STraining-I was used as plain texts that are taken from the same information sou.rce as training-O.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9959325194358826}]}, {"text": "Its word segmentation information was never used to ensure that tr~;=ing was unsupervised.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6564227044582367}]}, {"text": "It is calculated by F= f12 x P + R where P is precision, R is recall, and fl is the relative importance given to recall over precision.", "labels": [], "entities": [{"text": "F", "start_pos": 20, "end_pos": 21, "type": "METRIC", "confidence": 0.9935503005981445}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9992234706878662}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9988549947738647}, {"text": "fl", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9765633344650269}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9951812624931335}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9942599534988403}]}, {"text": "We set fl = 1.0 throughout this experiment.", "labels": [], "entities": [{"text": "fl", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9827103018760681}]}, {"text": "That is, we put equal importance on recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9992030262947083}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9987136125564575}]}], "tableCaptions": [{"text": " Table 1: The amount of tr~n~ng and test data", "labels": [], "entities": []}, {"text": " Table 2: Word Segmentation Accuracies", "labels": [], "entities": [{"text": "Word Segmentation Accuracies", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.5884377956390381}]}]}