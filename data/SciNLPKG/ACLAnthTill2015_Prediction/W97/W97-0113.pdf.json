{"title": [{"text": "Data Reliability and Its Effects on Automatic Abstracting", "labels": [], "entities": [{"text": "Data Reliability", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7594306468963623}]}], "abstractContent": [{"text": "We discuss a particular approach to automatic abstracting, where an abstract is created by extracting hnportant sentences from a text.", "labels": [], "entities": []}, {"text": "A primary purpose of the paper is to demonstrate that the reliability of human supplied annotations on corpora has crucial effects on how well an automatic abstracting system performs.", "labels": [], "entities": []}, {"text": "The corpus is developed through human judgements on possible s~,mmary sentences in a text.", "labels": [], "entities": []}, {"text": "The reliability of human judgements is evaluated by the kappa statistic, a reliability metric standardly used in behavioral sciences.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9626227021217346}]}, {"text": "The C4.5 decision tree method (Quinlan, 1993) is used to build a extraction model.", "labels": [], "entities": []}, {"text": "We demonstrate that there is a positive correlation of data reliability with a performance of automatic abstracting, and show results indicating that the reliability of human provided data is crucial for improving the performance of automatic abstracting.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We discarded data sets with K > 0.5 because they lacked a su~cient number of sentences for evaluation: the column-type data has only 19 sentences at 0.8 (Cf.).", "labels": [], "entities": []}, {"text": "This had left us with nine sets of data with associated threshold values, 0.1, 0.15, 0.2~ 0.25, 0.3, 0.35, 0.4, 0.45, and 0.5. 6 Texts contained in the evaluation data ranged in length from 314 to 535 sentences.", "labels": [], "entities": []}, {"text": "A part of a generated decision tree is given in.", "labels": [], "entities": []}, {"text": "See the caption for explanations.", "labels": [], "entities": []}, {"text": "The procedure for evaluation consists in the following steps: (1) choose at random 200 cases of category \"no\" and 40 of category \"yes\" from each of the data sets to form evaluation data; (2) divide the data so chosen into a training set and a test set; (3) build a decision tree from the training set, rnnning C4.5 with the default options; and (4) evaluate its performance on the test data.", "labels": [], "entities": []}, {"text": "Since the accuracy of evaluation can vary wildly depending on ways in which the data is divided into training and test sets, the re-sampling method of cross-validation is used here, which gives the average over possible partitions of the data into training and test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988394379615784}]}, {"text": "In particll]~r, we use a 10-fold cross-validation method where the data are divided into 10 blocks of cases, of which 9 blocks are used for the training and the remaining one for the eData with the threshold ---0.1, for instance, consists of coded representations of texts whose agreement rate is above or is equal to 0.1.   test.", "labels": [], "entities": []}, {"text": "Note that the method here gives arise to 10 possible divisions and an equal number of corresponding decision tree models.", "labels": [], "entities": []}, {"text": "The average performance of the generated models is then obtained and used as a summary estimate of the decision tree strategy fora particular set of evaluation data.", "labels": [], "entities": []}, {"text": "Further we use information retrieval metrics, recall and precision, to quantify the performance of the decision tree approach.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9993131160736084}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9976420998573303}]}, {"text": "Precision is the ratio of cases assigned correctly to the \"yes\" category to the total cases assigned to the \"yes\" category.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9895642995834351}]}, {"text": "Recall is the ratio of cases assigned correctly to the =yes\" category to the total \"yes\" cases.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9683408737182617}]}, {"text": "Furthermore, because different samplings of evaluation data from a source data set could produce wide variations in performance, we performed 50 runs of the evaluation procedure on each of the 9 data sets.", "labels": [], "entities": []}, {"text": "Each run used a separately (and randomly) sampled set of evaluation data.", "labels": [], "entities": []}, {"text": "Results of multiple runs of the procedure on a data set were then averaged to give a representative performance rating for that data set.", "labels": [], "entities": []}, {"text": "lists the average precision ratings for the nine data sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9961517453193665}]}, {"text": "Despite some fluctuations of the iigures, the results exhibit clear patterns (; the kappa coefllcient is strongly correlated with performance for texts of editorial type and of news-report type, but correlation for column-type texts is only marginal.", "labels": [], "entities": []}, {"text": "There are also marked differences in performance between text types; the decision tree method performs best on news reports and editorials, but worst on col-mug.", "labels": [], "entities": []}, {"text": "This means that the attributes used are effective only for texts of certain types.", "labels": [], "entities": []}, {"text": "The results suggest, further, that if attributes used are indeed a good predictor of s-mmary extracts, their strength as a predictor will be enhanced by the reliability or quality of human judgements.", "labels": [], "entities": []}, {"text": "Thus the method's poor performance on column-type texts, despite the fact that texts are becoming increasingly reliable, suggests a need to devise a set of attributes different from those for editorials and news reports.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on Corpus", "labels": [], "entities": [{"text": "Corpus", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.7192090749740601}]}, {"text": " Table 3: A hypothetical agreement table", "labels": [], "entities": []}, {"text": " Table 4: A matrix representation of a hypothetical example", "labels": [], "entities": []}, {"text": " Table 5: Kappa coefficients for judgements on sentence importance", "labels": [], "entities": [{"text": "Kappa", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9734688401222229}, {"text": "sentence importance", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6840270906686783}]}, {"text": " Table 9: Human reliability and precision of abstracting by extraction (averaged over 50 ruN).  Parenthetical figures denote recall rates.", "labels": [], "entities": [{"text": "reliability", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9787905812263489}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9990979433059692}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9991650581359863}]}]}