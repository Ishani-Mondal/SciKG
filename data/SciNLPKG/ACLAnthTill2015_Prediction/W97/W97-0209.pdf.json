{"title": [], "abstractContent": [{"text": "The absence of training data is areal problem for corpus-based approaches to sense disambiguation, one that is unlikely to be solved soon.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.8083178699016571}]}, {"text": "Selectional preference is traditionally connected with sense ambiguity ; this paper explores how a statistical model of selectional preference, requiring neither manual annotation of selection restrictions nor supervised training, can be used in sense disambiguation.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 246, "end_pos": 266, "type": "TASK", "confidence": 0.7127680778503418}]}], "introductionContent": [{"text": "It has long been observed that selectional constraints and word sense disambiguation are closely linked.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.7258307536443075}]}, {"text": "Indeed, the exemplar for sense disambiguation inmost computational settings (e.g., see Allen's (1995) discussion) is use of Boolean selection restrictions to constrain semantic interpretation.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7045855224132538}, {"text": "semantic interpretation", "start_pos": 168, "end_pos": 191, "type": "TASK", "confidence": 0.7725549340248108}]}, {"text": "For example, Mthough burgundy can be interpreted as either a color or a beverage, only the latter sense is available in the context of Mary drank burgundy, because the verb drink specifies the selection restriction +LIQUID for its direct objects.", "labels": [], "entities": [{"text": "Mthough", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.925632894039154}, {"text": "LIQUID", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9878396391868591}]}, {"text": "Problems with this approach arise, however, as soon as the domain of interest becomes too large or too rich to specify semantic features and selection restrictions accurately by hand.", "labels": [], "entities": []}, {"text": "This paper concerns the use of selectional constraints for automatic sense disambiguation in such broad-coverage settings.", "labels": [], "entities": [{"text": "automatic sense disambiguation", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6223261952400208}]}, {"text": "The approach combines statistical and knowledge-based methods, but unlike many recent corpus-based approaches to sense disambiguation, it takes as its starting point the assumption that senseannotated training text is not available.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.7743427157402039}]}, {"text": "Motivating this assumption is not only the limited availability of such text at present, but skepticism that the situation will change anytime soon.", "labels": [], "entities": []}], "datasetContent": [{"text": "Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn T~eebank project) and parts of which have been manually sense-tagged by the WordNet group (.", "labels": [], "entities": [{"text": "Brown corpus of American English", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.9145529389381408}, {"text": "Penn T~eebank project", "start_pos": 142, "end_pos": 163, "type": "DATASET", "confidence": 0.9121347188949585}]}, {"text": "A parsed, sense-tagged corpus was obtained by mergingthe WordNet sense-tagged corpus (approximately 200,000 words of source text from the Brown corpus, distributed across genres) with the corresponding Penn Treebank parses, a The rest of the Brown corpus (approximately 800,000 words of source text) remained as a parsed, but not sensetagged, training set.", "labels": [], "entities": [{"text": "WordNet sense-tagged corpus", "start_pos": 57, "end_pos": 84, "type": "DATASET", "confidence": 0.8935917615890503}, {"text": "Brown corpus", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.8450680375099182}, {"text": "Penn Treebank parses", "start_pos": 202, "end_pos": 222, "type": "DATASET", "confidence": 0.9776956836382548}, {"text": "Brown corpus", "start_pos": 242, "end_pos": 254, "type": "DATASET", "confidence": 0.855131983757019}]}, {"text": "3(1) Written message, (2) varsity letter, (3) alphabetic character.", "labels": [], "entities": []}, {"text": "3The merge was mostly automatic, requiring manual intervention for only 3 of 103 files.", "labels": [], "entities": [{"text": "3The merge", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8540407717227936}]}, {"text": "The test set for the verb-object relationship was constructed by first training a selectional preference model on the training corpus, using the T~eebank's tgrep utility to extract verb-object pairs from parse trees.", "labels": [], "entities": []}, {"text": "The 100 verbs that select most strongly for their objects were identified, excluding verbs appearing only once in the training corpus; test instances of the form (verb, object, correct sense) were then extracted from the merged test corpus, including all triples where verb was one of the 100 test verbs.", "labels": [], "entities": []}, {"text": "4 Evaluation materials were obtained in the same manner for several other surface syntactic reiationships, including verb-subject (John ~ admires), adjective-noun (tall =~ building), modifier-head (river =~ bank), and head-modifier (river ~= bank).", "labels": [], "entities": []}, {"text": "Following, disambiguation by random choice was used as a baseline: if a noun has one sense, use it; otherwise select at random among its senses.", "labels": [], "entities": []}, {"text": "Since both the algorithm and the baseline may involve random choices, evaluation involved multiple runs with different random seeds.", "labels": [], "entities": []}, {"text": "summarizes the results, taken over I0 runs, considering only ambiguous test cases.", "labels": [], "entities": []}, {"text": "All differences between the means for algorithm and baseline were statistically significant.", "labels": [], "entities": []}, {"text": "The results of the experiment show that disambignation using automatically acquired selectional constraints leads to performance significantly better than random choice.", "labels": [], "entities": []}, {"text": "Not surprisingly, though, the results are far from what one might expect to obtain with supervised training.", "labels": [], "entities": []}, {"text": "In that respect, the most direct point of comparison is the performance of frequency heuristic always choose the most frequent sense of a word as evaluated using the full sense-tagged corpus, including nouns, verbs, adjectives, and adverbs.", "labels": [], "entities": []}, {"text": "For ambiguous words, they report 58.2% correct, as compared to a random baseline of 26.8%.", "labels": [], "entities": [{"text": "correct", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9913559556007385}]}, {"text": "Crucially, however, the frequency heuristic requires sense-tagged training data (Miller et al. evaluated via cross-validation), and this paper starts from the assumption that such data are unavailable.", "labels": [], "entities": []}, {"text": "A fairer comparison, therefore, considers al-  ternative unsupervised algorithms-though unfortunately the literature contains more proposed algorithms than quantitative evaluations of those algorithms.", "labels": [], "entities": []}, {"text": "One experiment where results were reported was conducted by; their method involved using a stochastic search procedure to maximize the overlap in dictionary definitions (LDOCE) for alternative senses of words co-occurring in a sentence.", "labels": [], "entities": [{"text": "overlap in dictionary definitions (LDOCE)", "start_pos": 135, "end_pos": 176, "type": "METRIC", "confidence": 0.8029523151261466}]}, {"text": "They report an accuracy of 72% for disambiguation to the homograph level, and 47% for disambiguation to the sense level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9997019171714783}]}, {"text": "Since the task here involved WordNet sense distinctions, which are rather fine grained, the latter value is more appropriate for comparison.", "labels": [], "entities": []}, {"text": "Their experiment was more general in that they did not restrict themselves to nouns; on the other hand, their test set involved disambiguating words taken from full sentences, so the percentage correct may have been improved by the presence of unambiguous words. has also looked at unsupervised disambiguation of nouns using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 325, "end_pos": 332, "type": "DATASET", "confidence": 0.9759473204612732}]}, {"text": "Like Cowie et al., his algorithm optimizes a measure of semantic coherence over an entire sentence, in this case pairwise semantic distance between nouns in the sentence as measured using the noun taxonomy.", "labels": [], "entities": []}, {"text": "Comparison of results is somewhat difficult, however, for two reasons.", "labels": [], "entities": []}, {"text": "First, Sussna used an earlier version of WordNet (version 1.2) having a significantly smaller noun taxonomy (35K nodes vs. 49K nodes).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.971190333366394}]}, {"text": "Second, and more significant, in creating the test data, Sussna's human sense-taggers (tagging articles from the Time IR test collection) were permitted to tag a noun with as many senses as they felt were \"good,\" rather than making a forced choice; Sussna develops a scoring metric based on that fact rather than requiring exact matches to a single best sense.", "labels": [], "entities": [{"text": "Time IR test collection", "start_pos": 113, "end_pos": 136, "type": "DATASET", "confidence": 0.9473049491643906}]}, {"text": "This is quite a reasonable move (see discussion below), but unfortunately not an option in the present experiment.", "labels": [], "entities": []}, {"text": "Nonetheless, some comparison is possible, since he reports a \"% correct,\" apparently treating a sense assignment as correct if any of the \"good\" senses is chosen --his experiments have a lower bound (chance) of about 40% correct, with his algorithm performing at 53-55%, considering only ambiguous cases.", "labels": [], "entities": []}, {"text": "The best results reported for an unsupervised sense disambiguation method are those of, who uses evidence from a wider context (a window of 100 surrounding words) to buildup a co-occurrence model using classes from Roget's thesaurus.", "labels": [], "entities": []}, {"text": "He reports accuracy figures in the 72-99% range (mean 92%) in disambiguating test instances involving twelve \"interesting\" polysemons words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9993261098861694}]}, {"text": "As in the experiments by Cowie et al., the choice of coarser distinctions presumably accounts in part for the high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9984037280082703}]}, {"text": "By way of comparison, some words in Yarowsky's test set would require choosing among ten senses in WordNet, as compared to a maximum of six using the Roget's thesaurus categories; the mean level of polysemy for the tested words is a six-way distinction in WordNet as compared to a three-way distinction in Roget's thesaurus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9338173270225525}, {"text": "WordNet", "start_pos": 256, "end_pos": 263, "type": "DATASET", "confidence": 0.9534432291984558}]}, {"text": "As an aside, a rich taxonomy like WordNet permits a more continuous view of the sense vs. homograph distinction.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9630500674247742}]}, {"text": "For example, town has three senses in WordNet, corresponding to an administrative district, a geographical area, and a group of people.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9512732625007629}]}, {"text": "Given town as the object of leave, selectional preference will produce a tie between the first two senses, since both inherit their score from a common ancestor, (location).", "labels": [], "entities": []}, {"text": "In effect, the automatic selection of a class higher in the taxonomy as having the highest score provides the same coarse category that might be provided by a homograph/sense distinction in another setting.", "labels": [], "entities": []}, {"text": "The choice of coarser category varies dynamically with the context: as the argument in rural town, the same two senses still tie, but with (region) (a subclass of (location)) as the common ancestor that determines the score.", "labels": [], "entities": []}, {"text": "In other work, has shown that local collocational information, including selectional constraints, can be used to great effect in sense disambiguation, though his algorithm requires super-vised training.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.7837727963924408}]}, {"text": "The present work can be viewed as an attempt to take advantage of the same kind of information, but in an unsupervised setting.", "labels": [], "entities": []}], "tableCaptions": []}