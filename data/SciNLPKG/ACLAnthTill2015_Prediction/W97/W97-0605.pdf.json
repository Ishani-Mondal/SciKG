{"title": [{"text": "AUTOMATIC LEXICON ENHANCEMENT BY MEANS OF CORPUS TAGGING", "labels": [], "entities": [{"text": "AUTOMATIC LEXICON ENHANCEMENT BY MEANS OF CORPUS TAGGING", "start_pos": 0, "end_pos": 56, "type": "METRIC", "confidence": 0.6337593458592892}]}], "abstractContent": [{"text": "Using specialised text corpus to automatically enhance a general lexicon is the aim of this study.", "labels": [], "entities": []}, {"text": "Indeed, having lexicons which offer maximal cover on a specific topic is an important benefit in many applications of Automatic Speech and Natural Language Processing.", "labels": [], "entities": []}, {"text": "The enhancement of these lexicons can be made automatic as big corpora of specialised texts are available.", "labels": [], "entities": []}, {"text": "A syntactic tagging process, based on 3-class and 3-gram language models, allows us to automatically allocate possible syntactic categories to the Out-Of-Vocabulary (OOV) words which are found in the corpus processed.", "labels": [], "entities": [{"text": "syntactic tagging", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.6913204938173294}]}, {"text": "These OOV words generally occur several times in the corpus, and a number of these occurrences can be important.", "labels": [], "entities": []}, {"text": "By taking into account all the occurrences of an OOV word in a given text as a whole, we propose here a method for automatically extracting a specialised lexicon from a text corpus which is representative of a specific topic.", "labels": [], "entities": []}], "introductionContent": [{"text": "With both Automatic Speech Processing and Natural Language Processing it is necessary to use a lexicon which associates each item with a certain number of characteristics (syntactic, morphologic, frequency, phonetic, etc.).", "labels": [], "entities": [{"text": "Automatic Speech Processing", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6057634154955546}]}, {"text": "In Speech Recognition, these lexicons are necessary in the lexical access phases and the language modelisation as they allow the association between lexical items and recognised sounds while maintaining syntactic coherence within the sentence under analysis.", "labels": [], "entities": [{"text": "Speech Recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7242396175861359}]}, {"text": "In Speech Synthesis, the grapheme-to-phoneme transcription phase uses morphological and syntactical information to constjrain the phonetic transcription of the graphemes.", "labels": [], "entities": [{"text": "Speech Synthesis", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.6953362226486206}]}, {"text": "In both cases, using lexicons which have the maximum information about the subject is an important benefit.", "labels": [], "entities": []}, {"text": "The actual performance of Automatic Speech Treatment systems often limits their application to smaller subject-areas of language (medical texts, economic articles, etc.).", "labels": [], "entities": [{"text": "Automatic Speech Treatment", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7222730120023092}]}, {"text": "It is important to have specialised lexicons which cover these smaller subjectareas in order to optimise the synthesis or recognition applications.", "labels": [], "entities": []}, {"text": "But although general lexicons are readily available now, this is not the case for specialised lexicons which contain, for example, technical terms relevant to a subject, or family and brand names as can be found in journalistic texts.", "labels": [], "entities": []}, {"text": "When working with corpora we are faced by the evolutionary aspects of a given language.", "labels": [], "entities": []}, {"text": "The quicker the evolution of a specialised area, the more the dictionary will lack the ability to cover the subject, because a dictionary represents the state of a language at a given time.", "labels": [], "entities": []}, {"text": "The words missing from a lexicon (which we refer to here as Out-OfVocabulary words or OOV words) represent a significant problem.", "labels": [], "entities": []}, {"text": "In effect, whatever the size of the lexicon used, one can always find OOV words in texts.", "labels": [], "entities": []}, {"text": "If, fora given word, the lexical access fails, this failure can affect the processing of the word as well as the processing of the contextual words.", "labels": [], "entities": []}, {"text": "It would be useful to have dynamic lexicons which evolve in accordance with the corpora processed in order to limit, as much as possible, the OOV words.", "labels": [], "entities": []}, {"text": "Such an enhancement of lexicons could be automatic if big corpora of specialised texts were available : medical reports in an electronic form, newspaper available in CD-ROM, etc.", "labels": [], "entities": []}, {"text": "This interesting idea of automatically enhancing specialised lexicons from a general lexicon and a big corpus, is the aim of this paper.", "labels": [], "entities": []}, {"text": "By using statistical language models, we show how to automatically assign one or several categories to the OOV words which are found in our corpora.", "labels": [], "entities": []}, {"text": "Then, by taking into account all the occurences of each OOV word, we are able to automatically extract anew lexicon of OOV word with reliable labels associated to each word.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Correct Wrong  Common-words  95.6%  4.4%  Proper-names  92.4%  7.6%", "labels": [], "entities": []}]}