{"title": [{"text": "Exemplar-Based Word Sense Disambiguation: Some Recent Improvements", "labels": [], "entities": [{"text": "Exemplar-Based Word Sense Disambiguation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8179535418748856}]}], "abstractContent": [{"text": "In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy .", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7475558121999105}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9656363129615784}]}], "introductionContent": [{"text": "Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.841786801815033}]}, {"text": "Many different learning approaches have been used, including neural networks (, probabilistic algorithms (, decision lists, exemplar-based learning algorithms, etc.", "labels": [], "entities": []}, {"text": "In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\".", "labels": [], "entities": []}, {"text": "The seven algorithms that he evaluated are: a Naive-Bayes classifier, a perceptron, a decisiontree learner, a k nearest-neighbor classifier (exemplar-based learner), logic-based DNF and CNF learners, and a decision-list learner.", "labels": [], "entities": []}, {"text": "His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the \"line\" corpus tested.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9992651343345642}]}, {"text": "Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7538279592990875}]}, {"text": "This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which maybe unjustified in the domains tested.", "labels": [], "entities": []}, {"text": "Gale, Church and Yarowsky ( have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.7661206126213074}]}, {"text": "On the other hand, our past work on WSD) used an exemplar-based (or nearest neighbor) learning approach.", "labels": [], "entities": [{"text": "WSD)", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.9327061772346497}]}, {"text": "Our WSD program, LEXAS, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation from a sentence containing the word to be disambiguated.", "labels": [], "entities": [{"text": "LEXAS", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.807657778263092}]}, {"text": "These features from a sentence form an example.", "labels": [], "entities": []}, {"text": "LEXAS then uses the exemplar-based learning algorithm PEBLS to find the sense (class) of the word to be disambiguated.", "labels": [], "entities": [{"text": "PEBLS", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.7533642649650574}]}, {"text": "In this paper, we report recent improvements to the exemplar-based learning approach for WSD that have achieved higher disambiguation accuracy.", "labels": [], "entities": [{"text": "WSD", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9784625768661499}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9555168747901917}]}, {"text": "The exemplar-based learning algorithm PEBLS contains a number of parameters that must beset before running the algorithm.", "labels": [], "entities": [{"text": "PEBLS", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.7453593015670776}]}, {"text": "These parameters include the number of nearest neighbors to use for determining the class of a test example (i.e., kin a k nearest-neighbor classifier), exemplar weights, feature weights, etc.", "labels": [], "entities": []}, {"text": "We found that the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar-based classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9995075464248657}]}, {"text": "By using 10-fold cross validation on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9830326437950134}]}, {"text": "The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in to have the highest disambiguation accuracy among seven stateof-the-art machine learning algorithms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991186261177063}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9958184361457825}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief description of the exemplar-based algorithm PEBLS and the Naive-Bayes algorithm.", "labels": [], "entities": [{"text": "PEBLS", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.6666513085365295}]}, {"text": "Section 3 describes the 10-fold cross validation training procedure to determine the best k number of nearest neighbors to use.", "labels": [], "entities": [{"text": "cross validation training", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.752438505490621}]}, {"text": "Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9621497988700867}, {"text": "PEBLS", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.7080172300338745}]}, {"text": "Section 5 discusses the implications of the results.", "labels": [], "entities": []}, {"text": "Section 6 gives the conclusion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}