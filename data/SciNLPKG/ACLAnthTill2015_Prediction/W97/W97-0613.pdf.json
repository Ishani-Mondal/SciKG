{"title": [{"text": "The \"Casual Cashmere Diaper Bag\": Constraining Speech Recognition Using Examples", "labels": [], "entities": [{"text": "Casual Cashmere Diaper Bag", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.5951028764247894}, {"text": "Constraining Speech Recognition", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.641000767548879}]}], "abstractContent": [{"text": "We describe anew technology for using small collections of example sentences to automatically restrict a speech recognition grammar to allow only the more plausible subset of the sentences it would otherwise admit.", "labels": [], "entities": [{"text": "speech recognition grammar", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.7779961625734965}]}, {"text": "This technology is unusual because it bridges the gap between hand-built grammars (used with no training data) and statistical approaches (which require significant data).", "labels": [], "entities": []}, {"text": "1 Motivation Whenever the utterances to be recognized do not correspond directly to a large body of text, traditional statistical modeling cannot be used; there is not enough data to \"train\" a statistical model.", "labels": [], "entities": []}, {"text": "This new technique is relevant when the speech to be recognized contains patterns that can be straightforwardly abstracted from the semantics of the words appearing in them, but where writing simple rules to do so is not practical due to the size of the sentence set to be recognized.", "labels": [], "entities": []}, {"text": "Using this technology , a small representative set of utterances (on the order of hundreds of sentences or phrases) is combined with an overly-permissive general grammar to automatically create a much tighter grammar that is specific to the particular domain.", "labels": [], "entities": []}, {"text": "The grammar produced is a context free grammar in whatever BNF the speech recognizer of choice requires.", "labels": [], "entities": []}, {"text": "The technique works by using a grammar compiler that accepts grammars composed of rules written using both patterns and restrictions (which can be syntactic or semantic); the Unified Grammar compiler (Martin and Kehler, 1994) provides this foundation.", "labels": [], "entities": []}, {"text": "Our approach requires the developer to write grammars with rules which enforce semantic restrictions that test whether the class markings in the lexicon on a phrase head correspond to similar markings on the lexical entries for the possible modifiers of that head.", "labels": [], "entities": []}, {"text": "If the grammar writer took the trouble to mark the lexicon appropriately, then these tests, processed by the grammar compiler, would suffice to build the restricted grammar.", "labels": [], "entities": []}, {"text": "Instead, with the new technique, only the semantic markings necessary to understand the \"meaning\" of the resultant utterance need be marked by the grammar developer ; processing of a set of example sentences serves to automatically provide the additional lexical markings needed by the modifier words, and a subsequent grammar compilation reflects these restrictions in the pure BNF grammar used by the speech recog-nizer.", "labels": [], "entities": []}, {"text": "2 Background Currently, the best speaker-independent continuous speech recognition (SR) is orders of magnitude weaker than a human native speaker in recognizing arbitrary sequences of words.", "labels": [], "entities": [{"text": "speaker-independent continuous speech recognition (SR)", "start_pos": 33, "end_pos": 87, "type": "TASK", "confidence": 0.6868040093353817}]}, {"text": "That is, humans do pretty well on clearly spoken sequences of words chosen randomly from a pool of tens of thousands of words, while unconstrained SR systems only do as well when the vocabulary is much smaller, in the range of hundreds of words.", "labels": [], "entities": []}, {"text": "When the recognition is to be done over the telephone, the reduced signal-to-noise ratio of the speech data makes this weakness even more dramatic.", "labels": [], "entities": []}, {"text": "2.1 Language Models In order to achieve useful recognition rates, current SR systems impose constraints beyond just a limited vocabulary, either by specifying an exact grammar of the sequences which are allowed or by providing statistical likelihoods for word sequences (n-gram statistics).", "labels": [], "entities": []}, {"text": "The grammars are built by hand as context-free formalisms determining allowable word sequences.", "labels": [], "entities": []}, {"text": "The statistical models use tables of the \"raw\" probabilities of each word (unigram) usually augmented with additional tables of the likelihood of each word given each possible preceding word (bi-gram) or each possible two preceding words (tri-gram).", "labels": [], "entities": []}, {"text": "These statistical systems have been experimentally extended to include n-grams where n is exceeds three, but even for higher n they generally express only the probability of a word based on the adjacent preceding words.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}