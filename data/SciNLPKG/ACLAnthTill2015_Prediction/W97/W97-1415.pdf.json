{"title": [{"text": "The CARTOON project : Towards Integration of Multimodal and Linguistic Analysis for Cartographic Applications", "labels": [], "entities": []}], "abstractContent": [{"text": "Reference in multimodal input Human-Computer Interaction has already been studied in several experiments involving either simulated or implemented systems (Mignot and Carbonell 96, Huls and Bos 95) including cartographic application (Siroux et al. 95, Cheyer and Julia 95, Oviatt 96).", "labels": [], "entities": []}, {"text": "In this paper, we present our project named CARTOON (CARTography and cOOperatioN between modalities): we describe the current prototype but also how we plan to integrate tools providing linguistic analysis.", "labels": [], "entities": []}, {"text": "1. THE CURRENT PROTOTYPE The current prototype enables to combine speech recognition, mouse pointing and keyboard to interact with a cartographical database (figure 1).", "labels": [], "entities": [{"text": "THE CURRENT PROTOTYPE", "start_pos": 3, "end_pos": 24, "type": "METRIC", "confidence": 0.7149569392204285}, {"text": "speech recognition", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.730320617556572}]}, {"text": "Several functions are available such as requesting information on the name or location of a building, the shortest itinerary and the distance between two points, or zooming in and out.", "labels": [], "entities": []}, {"text": "Several combinations are possible such as : \u2022 What is the name of this <pointing> building ? \u2022 What is this <pointing> ? \u2022 Where is the police station ? \u2022 Show me the hospital \u2022 I want to go from here <pointing> to the hospital Figure 1: Events detected on the three modalities (speech, \u2022 I am in front of the police station.", "labels": [], "entities": []}, {"text": "How can mouse, keyboard) are displayed in the lower window as a I go here <pointing> ? function of time.", "labels": [], "entities": []}, {"text": "The recognized words were: '7 want to go\", \u2022 Show me how to go from here <pointing> \"here\", \"here\".", "labels": [], "entities": []}, {"text": "Two mouse clicks were also detected.", "labels": [], "entities": []}, {"text": "The to here <pointing>.", "labels": [], "entities": []}, {"text": "system displayed the corresponding itinerary.", "labels": [], "entities": []}, {"text": "Currently, there is no linguistic analysis.", "labels": [], "entities": []}, {"text": "Events produced by the speech recognition system (a Vecsys Datavox) are either words or sequences of words (\"I_want_to_go\").", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7029109001159668}, {"text": "Vecsys Datavox", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.8499476313591003}]}, {"text": "There are 38 such speech events which are characterized by: the recognized word, the time of utterance and the recognition score.", "labels": [], "entities": []}, {"text": "The pointing gestures events are characterized by an (x, y) position and the time of detection.", "labels": [], "entities": []}, {"text": "The overall architecture is described in figure 2 : events detected on the keyboard, mouse and speech modalities (left-hand side) are time-stamped coherently by a modality server and then integrated in the multimodal which merges them and activates the application.", "labels": [], "entities": []}, {"text": "Figure 2: current software and hardware architecture.", "labels": [], "entities": []}, {"text": "The multimodal interface is based on a theoretical framework of ~ types of cooperation between modalities ~ that we initially presented in (Martin and B&oule 93) and that has been used by other French researchers in (Catinis and Caelen 95, Coutaz and Nigay 94).", "labels": [], "entities": [{"text": "Martin and B&oule 93", "start_pos": 140, "end_pos": 160, "type": "DATASET", "confidence": 0.6346144129832586}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}