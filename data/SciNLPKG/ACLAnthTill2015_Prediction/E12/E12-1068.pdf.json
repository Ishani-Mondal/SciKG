{"title": [{"text": "Modeling Inflection and Word-Formation in SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9545652866363525}]}], "abstractContent": [{"text": "The current state-of-the-art in statistical machine translation (SMT) suffers from issues of sparsity and inadequate modeling power when translating into morphologically rich languages.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.8078216264645258}]}, {"text": "We model both inflection and word-formation for the task of translating into German.", "labels": [], "entities": [{"text": "translating into German", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.8901728987693787}]}, {"text": "We translate from English words to an underspecified German representation and then use linear-chain CRFs to predict the fully specified German representation.", "labels": [], "entities": []}, {"text": "We show that improved modeling of inflection and word-formation leads to improved SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9945623278617859}]}], "introductionContent": [{"text": "Phrase-based statistical machine translation (SMT) suffers from problems of data sparsity with respect to inflection and word-formation which are particularly strong when translating to a morphologically rich target language, such as German.", "labels": [], "entities": [{"text": "Phrase-based statistical machine translation (SMT)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8127774298191071}]}, {"text": "We address the problem of inflection by first translating to a stem-based representation, and then using a second process to inflect these stems.", "labels": [], "entities": []}, {"text": "We study several models for doing this, including: strongly lexicalized models, unlexicalized models using linguistic features, and models combining the strengths of both of these approaches.", "labels": [], "entities": []}, {"text": "We address the problem of word-formation for compounds in German, by translating from English into German word parts, and then determining whether to merge these parts to form compounds.", "labels": [], "entities": []}, {"text": "We make the following new contributions: (i) we introduce the first SMT system combining inflection prediction with synthesis of portmanteaus and compounds.", "labels": [], "entities": [{"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9957535266876221}, {"text": "inflection prediction", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7734382450580597}]}, {"text": "(ii) For inflection, we compare the mostly unlexicalized prediction of linguistic features (with a subsequent surface form generation step) versus the direct prediction of surface forms, and show that both approaches have complementary strengths.", "labels": [], "entities": []}, {"text": "(iii) We combine the advantages of the prediction of linguistic features with the prediction of surface forms.", "labels": [], "entities": []}, {"text": "We implement this in a CRF framework which improves on a standard phrase-based SMT baseline.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8654516339302063}]}, {"text": "(iv) We develop separate (but related) procedures for inflection prediction and dealing with word-formation (compounds and portmanteaus), in contrast with most previous work which usually either approaches both problems as inflectional problems, or approaches both problems as word-formation problems.", "labels": [], "entities": [{"text": "inflection prediction", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.713559478521347}]}, {"text": "We evaluate on the end-to-end SMT task of translating from English to German of the 2009 ACL workshop on SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9915781021118164}, {"text": "translating from English to German of the 2009 ACL workshop on SMT", "start_pos": 42, "end_pos": 108, "type": "TASK", "confidence": 0.6762043361862501}]}, {"text": "We achieve BLEU score increases on both the test set and the blind test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.976391077041626}]}], "datasetContent": [{"text": "To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package.", "labels": [], "entities": [{"text": "news translation", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7164446264505386}, {"text": "Moses SMT package", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.7846060196558634}]}, {"text": "We use the English/German data released for the 2009 ACL Workshop on Machine Translation shared task on translation.", "labels": [], "entities": [{"text": "English/German data released for the 2009 ACL Workshop on", "start_pos": 11, "end_pos": 68, "type": "DATASET", "confidence": 0.7710043002258647}, {"text": "Machine Translation shared task on translation", "start_pos": 69, "end_pos": 115, "type": "TASK", "confidence": 0.7354840338230133}]}, {"text": "There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en.", "labels": [], "entities": [{"text": "europarl-v4.de-en", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.9749064445495605}]}, {"text": "The monolingual data contains 9.8 M sentences.", "labels": [], "entities": []}, {"text": "To build the baseline, the data was tokenized using the Moses tokenizer and lowercased.", "labels": [], "entities": []}, {"text": "We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4.", "labels": [], "entities": []}, {"text": "We symmetrize using the \"grow-diag-final-and\" heuristic.", "labels": [], "entities": []}, {"text": "Our Moses systems use default settings.", "labels": [], "entities": []}, {"text": "The LM uses the monolingual data and is trained as a five-gram 9 using the SRILM-Toolkit).", "labels": [], "entities": [{"text": "SRILM-Toolkit", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.7067278027534485}]}, {"text": "We run MERT separately for each system.", "labels": [], "entities": [{"text": "MERT", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.46016332507133484}]}, {"text": "The recaser used is the same for all systems.", "labels": [], "entities": []}, {"text": "It is the standard recaser supplied with Moses, trained on all German training data.", "labels": [], "entities": [{"text": "German training data", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.6379884282747904}]}, {"text": "The dev set is wmt-2009-a and the test set is wmt-2009-b, and we report end-to-end case sensitive BLEU scores against the unmodified reference SGML file.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9104450941085815}, {"text": "SGML file", "start_pos": 143, "end_pos": 152, "type": "DATASET", "confidence": 0.7446840405464172}]}, {"text": "The blind test set used is wmt-2009-blind (all lines).", "labels": [], "entities": [{"text": "wmt-2009-blind", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8758066296577454}]}, {"text": "In developing our inflection prediction systems (and making such decisions as n-gram order used), we worked on the so-called \"clean data\" task, predicting the inflection on stemmed reference sentences (rather than MT output).", "labels": [], "entities": [{"text": "inflection prediction", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.734949380159378}]}, {"text": "We used the 2000 sentence dev-2006 corpus for this task.", "labels": [], "entities": []}, {"text": "Our contrastive systems consist of two steps, the first is a translation step using a similar Moses system (except that the German side is stemmed, with the markup indicated in Sec-7 http://www.statmt.org/wmt09/translation-task.html 8 However, we reduced the monolingual data (only) by retaining only one copy of each unique line, which resulted in 7.55 M sentences.", "labels": [], "entities": []}, {"text": "Add-1 smoothing for unigrams and Kneser-Ney smoothing for higher order n-grams, pruning defaults.", "labels": [], "entities": []}, {"text": "tion 2.3), and the second is inflection prediction as described previously in the paper.", "labels": [], "entities": [{"text": "inflection prediction", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7179723531007767}]}, {"text": "To derive the stem+markup representation we first parse the German training data and then produce the stemmed representation.", "labels": [], "entities": [{"text": "German training data", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.6962190866470337}]}, {"text": "We then build a system for translating from English words to German stems (the stem+markup representation), on the same data (so the German side of the parallel data, and the German language modeling uses the stem+markup representation).", "labels": [], "entities": [{"text": "translating from English words to German stems", "start_pos": 27, "end_pos": 73, "type": "TASK", "confidence": 0.818340173789433}]}, {"text": "Likewise, MERT is performed using references which are in the stem+markup representation.", "labels": [], "entities": [{"text": "MERT", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.8531031012535095}]}, {"text": "To train the inflection prediction systems, we use the monolingual data.", "labels": [], "entities": [{"text": "inflection prediction", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7192135751247406}]}, {"text": "The basic surface form model is trained on lowercased surface forms, the hybrid surface form model with features is trained on lowercased surface forms annotated with markup.", "labels": [], "entities": []}, {"text": "The linguistic feature prediction systems are trained on the monolingual data processed as described previously (see).", "labels": [], "entities": [{"text": "linguistic feature prediction", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6426871120929718}]}, {"text": "Our JSMs are trained using the SRILM Toolkit.", "labels": [], "entities": [{"text": "JSMs", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.828807532787323}, {"text": "SRILM Toolkit", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.8652134835720062}]}, {"text": "We use the SRILM disambig tool for predicting inflection, which takes a \"map\" that specifies the set of fully specified representations that each underspecified stem can map to.", "labels": [], "entities": [{"text": "predicting inflection", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.9178254306316376}]}, {"text": "For surface form models, it specifies the mapping from stems to lowercased surface forms (or surface forms with markup for the hybrid surface model).", "labels": [], "entities": []}, {"text": "We evaluated the end-to-end inflection system with the addition of compounds.", "labels": [], "entities": []}, {"text": "As in the inflection experiments described in Section 5, we use a 5-gram surface LM and a 7-gram POS LM, but for this experiment, they are trained on stemmed, split data.", "labels": [], "entities": []}, {"text": "The POS LM helps compound parts and heads appear incorrect order.", "labels": [], "entities": []}, {"text": "The BLEU score of the CRF on testis 14.04, which is low.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9996674060821533}, {"text": "CRF", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.6318860054016113}]}, {"text": "However the system produces 19 compound types which are in the reference but not in the parallel data, and therefore not accessible to other systems.", "labels": [], "entities": []}, {"text": "We also observe many more compounds in general.", "labels": [], "entities": []}, {"text": "The 100-best inflection rescoring technique previously discussed reached 14.07 on the test set.", "labels": [], "entities": []}, {"text": "Blind test results with CRF prediction are much better, 14.08, which is a statistically significant improvement over the baseline (13.68) and approaches the result we obtained without compounds.", "labels": [], "entities": [{"text": "CRF prediction", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.9385832250118256}]}, {"text": "Correctly generated compounds are single words which usually carry the same information as multiple words in English, and are hence likely underweighted by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.988146185874939}]}, {"text": "We again see many interesting generalizations.", "labels": [], "entities": []}, {"text": "For instance, take the case of translating English miniature cameras to the German compound Miniaturkameras.", "labels": [], "entities": []}, {"text": "miniature camera or miniature cameras does not occur in the training data, and so there is no appropriate phrase pair in any system (baseline, inflection, or inflection&compound-splitting).", "labels": [], "entities": []}, {"text": "However, our system with compound splitting has learned from split composita that English minia- 1 JSM morphological features 13.94 2 4 CRFs morphological features, lexical information 14.04: Results with Compounds on the test set ture can be translated as German Miniatur-and gets the correct output.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7049828916788101}]}], "tableCaptions": [{"text": " Table 4: BLEU scores (detokenized, case sensitive) on  the development test set wmt-2009-b", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987623691558838}, {"text": "development test set wmt-2009-b", "start_pos": 60, "end_pos": 91, "type": "DATASET", "confidence": 0.7919308841228485}]}, {"text": " Table 5: Comparing predicting surface forms directly  with predicting morphological features.", "labels": [], "entities": [{"text": "Comparing predicting surface forms", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.8130157142877579}]}, {"text": " Table 6: Accuracy for different training data sizes of  the single and the four separate joint sequence models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985393285751343}]}]}