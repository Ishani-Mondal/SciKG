{"title": [{"text": "Character-Based Pivot Translation for Under-Resourced Languages and Domains", "labels": [], "entities": [{"text": "Character-Based Pivot Translation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5884397526582082}]}], "abstractContent": [{"text": "In this paper we investigate the use of character-level translation models to support the translation from and to under-resourced languages and textual domains via closely related pivot languages.", "labels": [], "entities": []}, {"text": "Our experiments show that these low-level models can be successful even with tiny amounts of training data.", "labels": [], "entities": []}, {"text": "We test the approach on movie subtitles for three language pairs and legal texts for another language pair in a domain adaptation task.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7370865643024445}]}, {"text": "Our pivot translations outperform the baselines by a large margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-driven approaches have been extremely successful inmost areas of natural language processing (NLP) and can be considered the main paradigm in application-oriented research and development.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.8077523012955984}]}, {"text": "Research in machine translation is atypical example with the dominance of statistical models over the last decade.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8209259212017059}]}, {"text": "This is even enforced due to the availability of toolboxes such as Moses () which make it possible to build translation engines within days or even hours for any language pair provided that appropriate training data is available.", "labels": [], "entities": []}, {"text": "However, this reliance on training data is also the most severe limitation of statistical approaches.", "labels": [], "entities": []}, {"text": "Resources in large quantities are only available fora few languages and domains.", "labels": [], "entities": []}, {"text": "In the case of SMT, the dilemma is even more apparent as parallel corpora are rare and usually quite sparse.", "labels": [], "entities": [{"text": "SMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9925684928894043}]}, {"text": "Some languages can be considered lucky, for example, because of political situations that lead to the production of freely available translated material on a large scale.", "labels": [], "entities": []}, {"text": "A lot of research and development would not have been possible without the European Union and its language policies to give an example.", "labels": [], "entities": []}, {"text": "One of the main challenges of current NLP research is to port data-driven techniques to underresourced languages, which refers to the majority of the world's languages.", "labels": [], "entities": []}, {"text": "One obvious approach is to create appropriate data resources even for those languages in order to enable the use of similar techniques designed for high-density languages.", "labels": [], "entities": []}, {"text": "However, this is usually too expensive and often impossible with the quantities needed.", "labels": [], "entities": []}, {"text": "Another idea is to develop new models that can work with (much) less data but still make use of resources and techniques developed for other well-resourced languages.", "labels": [], "entities": []}, {"text": "In this paper, we explore pivot translation techniques for the translation from and to resourcepoor languages with the help of intermediate resource-rich languages.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6661849915981293}]}, {"text": "We explore the fact that many poorly resourced languages are closely related to well equipped languages, which enables low-level techniques such as characterbased translation.", "labels": [], "entities": [{"text": "characterbased translation", "start_pos": 148, "end_pos": 174, "type": "TASK", "confidence": 0.660679817199707}]}, {"text": "We can show that these techniques can boost the performance enormously, tested for several language pairs.", "labels": [], "entities": []}, {"text": "Furthermore, we show that pivoting can also be used to overcome data sparseness in specific domains.", "labels": [], "entities": []}, {"text": "Even high density languages are under-resourced inmost textual domains and pivoting via in-domain data of another language can help to adapt statistical models.", "labels": [], "entities": []}, {"text": "In our experiments, we observe that related languages have the largest impact in such a setup.", "labels": [], "entities": []}, {"text": "The remaining parts of the paper are organized as follows: First we describe the pivot translation approach used in this study.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.6901622116565704}]}, {"text": "Thereafter, we dis-cuss character-based translation models followed by a detailed presentation of our experimental results.", "labels": [], "entities": []}, {"text": "Finally, we briefly summarize related work and conclude the paper with discussions and prospects for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Character-level translations can be evaluated in the same way as other translation hypotheses, for example using automatic measures such as BLEU, NIST, METEOR etc.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.997925877571106}, {"text": "METEOR", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.8942202925682068}]}, {"text": "The same simple post-processing as mentioned in the previous section can be applied to turn the character translations into \"normal\" text.", "labels": [], "entities": []}, {"text": "However, it can be useful to look at some other measures as well that consider near matches on the character level instead of matching words and word N-grams only.", "labels": [], "entities": []}, {"text": "Character-level models have the ability to produce strings that maybe close to the reference and still do not match any of the words contained.", "labels": [], "entities": []}, {"text": "They may generate non-words that include mistakes which look like spelling-errors or minor grammatical mistakes.", "labels": [], "entities": []}, {"text": "Those words are usually close enough to the correct target words to be recognized by the user, which is often more acceptable than leaving foreign words untranslated.", "labels": [], "entities": []}, {"text": "This is especially true as many unknown words represent important content words that bear a lot of information.", "labels": [], "entities": []}, {"text": "The problem of unknown words is even more severe for morphologically rich language as many word forms are simply not part of (sparse) training data sets.", "labels": [], "entities": []}, {"text": "Untranslated words are especially annoying when translating languages that use different writing systems.", "labels": [], "entities": []}, {"text": "Consider, for example, the following subtitles in Macedonian (using Cyrillic letters) that have been translated from Bosnian (written in Latin characters): reference: \u0418 \u0447\u0430\u0448\u0430 \u0432\u0438\u043d\u043e, \u043a\u0430\u043a\u043e \u0438 \u0441\u0435\u043a\u043e\u0433\u0430\u0448.", "labels": [], "entities": []}, {"text": "word-based: \u0418 \u02c7 ca\u0161u vina, \u043a\u0430\u043a\u043e \u0441\u0435\u043a\u043e\u0433\u0430\u0448.", "labels": [], "entities": []}, {"text": "char-based: \u0418 \u0447\u0430\u0448\u0430 \u0432\u0438\u043d\u043e, \u043a\u0430\u043a\u043e \u0441\u0435\u043a\u043e\u0433\u0430\u0448.", "labels": [], "entities": []}, {"text": "reference: \u0412\u043e \u0441\u0442\u0430\u0440\u043e\u0442\u043e \u0441\u0432\u0435\u0442\u0438\u043b\u0438\u0448\u0442\u0435.", "labels": [], "entities": []}, {"text": "word-based: \u0412\u043e starom svetili\u0161tu.", "labels": [], "entities": []}, {"text": "char-based: \u0412\u043e \u0441\u0442\u0430\u0440 \u0441\u0432\u0435\u0442\u0438\u043b\u0438\u0448\u0442\u0435\u0442\u043e.", "labels": [], "entities": []}, {"text": "The underlined parts mark examples of characterlevel differences with respect to the reference translation.", "labels": [], "entities": []}, {"text": "For the pivot translation approach, it is important that the translations generated in the first step can be handled by the second one.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.6692417562007904}]}, {"text": "This means, that words generated by a character-based model should at least be valid input words for the second step, even though they might refer to erroneous inflections in that context.", "labels": [], "entities": []}, {"text": "Therefore, we add another measure to our experimental results presented below -the number of unknown words with respect to the input language of the second step.", "labels": [], "entities": []}, {"text": "This applies only to models that are used as the first step in pivot-based translations.", "labels": [], "entities": []}, {"text": "For other models, we include a string similarity measure based on the longest common subsequence ratio (LCSR)) in order to give an impression about the \"closeness\" of the system output to the reference translations.", "labels": [], "entities": [{"text": "longest common subsequence ratio (LCSR))", "start_pos": 70, "end_pos": 110, "type": "METRIC", "confidence": 0.7837364588465009}]}, {"text": "We conducted a series of experiments to test the ideas of (character-level) pivot translation for resource-poor languages.", "labels": [], "entities": [{"text": "character-level) pivot translation", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.6613430976867676}]}, {"text": "We chose to use data from a collection of translated subtitles compiled in the freely available OPUS corpus).", "labels": [], "entities": [{"text": "OPUS corpus", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.973886102437973}]}, {"text": "This collection includes a large variety of languages and contains mainly short sentences and sentence fragments, which suits character-level alignment very well.", "labels": [], "entities": [{"text": "character-level alignment", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.7480873465538025}]}, {"text": "The selected settings represent translation tasks between languages (and domains) for which only very limited training data is available or none at all.", "labels": [], "entities": [{"text": "translation tasks between languages", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8732392936944962}]}, {"text": "Below we present results from two general tasks: 4 (i) Translating between English and a resource-poor language (in both directions) via a pivot language that is close related to the resource-poor language.", "labels": [], "entities": []}, {"text": "(ii) Translating between two languages in a domain for which no indomain training data is available via a pivot language with in-domain data.", "labels": [], "entities": []}, {"text": "We will start with the presentation of the first task and the characterbased translation between closely related languages.", "labels": [], "entities": [{"text": "characterbased translation", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.5649412274360657}]}], "tableCaptions": [{"text": " Table 2: Translating from a related pivot language to the target language. Bosnian (bs) / Bulgarian (bg) - Macedonian (mk); Galician (gl) / Catalan (ca) -Spanish (es). Word-based refers to standard phrase-based SMT  models. All other models use phrases over character sequences. The WFST x:y models use weighted finite state  transducers for character alignment with units that are at most x and y characters long, respectively. Other  models use Viterbi alignments created by IBM model 4 using GIZA++ (Och and Ney, 2003) between characters  (IBM char ) or bigrams (IBM bigram ). LCSR refers to the averaged longest common subsequence ratio between  system translations and references. Results are significantly better (p < 0.01 ++ , p < 0.05 + ) or worse (p <  0.01 \u2212\u2212 , p < 0.05 \u2212 ) than the word-based baseline.", "labels": [], "entities": [{"text": "SMT", "start_pos": 212, "end_pos": 215, "type": "TASK", "confidence": 0.9124065041542053}, {"text": "character alignment", "start_pos": 343, "end_pos": 362, "type": "TASK", "confidence": 0.7239611446857452}]}, {"text": " Table 3: Translating from the source language to a related pivot language. UNK gives the proportion of unknown  words with respect to the translation model from the pivot language to English.", "labels": [], "entities": [{"text": "UNK", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.7081193327903748}]}, {"text": " Table 4: Translating between Galician/Catalan and En- glish via Spanish using a standard phrase-based SMT  baseline, Spanish-English SMT models to translate  from/to Catalan/Galician and pivot-based approaches  using word-level models or character-level models  (based on IBM bigram alignments) with either one-best  (1x1) or N-best lists (10x10 with \u03b1 = 0.85).", "labels": [], "entities": [{"text": "Translating between Galician/Catalan and En- glish", "start_pos": 10, "end_pos": 60, "type": "TASK", "confidence": 0.8238034910625882}, {"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.8847789764404297}, {"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.8727521896362305}]}, {"text": " Table 5: Translating between Macedonian (Maced)  and English via Bosnian (Bosn) / Bulgarian (Bulg).", "labels": [], "entities": [{"text": "Translating between Macedonian (Maced)  and English", "start_pos": 10, "end_pos": 61, "type": "TASK", "confidence": 0.7977717071771622}]}, {"text": " Table 7: Translating out-of-domain data via Dan- ish. Models using in-domain data are marked with  dgt and out-of-domain models are marked with subs.  subs+dgtLM refers to a model with an out-of-domain  translation model and an added in-domain language  model. The subscripts wo, ch and bi refer to word,  character and bigram models, respectively.", "labels": [], "entities": []}, {"text": " Table 8: Alternative word-based pivot translations be- tween Norwegian (no) and English (en).", "labels": [], "entities": []}]}