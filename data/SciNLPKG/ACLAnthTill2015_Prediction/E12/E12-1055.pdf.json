{"title": [{"text": "Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation", "labels": [], "entities": [{"text": "Perplexity Minimization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7498435378074646}, {"text": "Translation Model Domain Adaptation", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.9014669060707092}, {"text": "Statistical Machine Translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6811151504516602}]}], "abstractContent": [{"text": "We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7353665381669998}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.8192399640878042}]}, {"text": "While techniques for domain adaptation of monolin-gual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance , such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7286766767501831}, {"text": "language model domain adaptation", "start_pos": 159, "end_pos": 191, "type": "TASK", "confidence": 0.615850068628788}]}, {"text": "We also explore adapting multiple (4-10) data sets with no a priori distinction between in-domain and out-of-domain data except for an in-domain development set.", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing availability of parallel corpora from various sources, welcome as it maybe, leads to new challenges when building a statistical machine translation system fora specific domain.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.6506376465161642}]}, {"text": "The task of determining which parallel texts should be included for training, and which ones hurt translation performance, is tedious when performed through trial-and-error.", "labels": [], "entities": [{"text": "translation", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.9586830735206604}]}, {"text": "Alternatively, methods fora weighted combination exist, but there is conflicting evidence as to which approach works best, and the issue of determining weights is not adequately resolved.", "labels": [], "entities": []}, {"text": "The picture looks better in language modelling, where model interpolation through perplexity minimization has become a widespread method of domain adaptation.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7433371841907501}, {"text": "domain adaptation", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.7295265644788742}]}, {"text": "We investigate the applicability of this method for translation models, and discuss possible applications.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.982705295085907}]}, {"text": "We move the focus away from a binary combination of in-domain and out-of-domain data.", "labels": [], "entities": []}, {"text": "If we can scale up the number of models whose contributions we weight, this reduces the need fora priori knowledge about the fitness 1 of each potential training text, and opens new research opportunities, for instance experiments with clustered training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Apart from measuring the performance of the approaches introduced in section 2, we want to investigate the following open research questions.", "labels": [], "entities": []}, {"text": "1. Does an implementation of linear interpolation that is more closely tailored to translation modelling outperform a naive implementation?", "labels": [], "entities": [{"text": "translation modelling", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.9582188725471497}]}, {"text": "2. How do the approaches perform outside a binary setting, i.e. when we do notwork with one in-domain and one out-of-domain model, but with a higher number of models?", "labels": [], "entities": []}, {"text": "3. Can we apply perplexity minimization to other translation model features such as the lexical weights, and if yes, does a separate optimization of each translation model feature improve performance?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parallel data sets for German -French trans- lation task.", "labels": [], "entities": [{"text": "German -French trans- lation task", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6382348239421844}]}, {"text": " Table 2: Monolingual French data sets for German - French translation task.", "labels": [], "entities": [{"text": "German - French translation task", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6378649652004242}]}, {"text": " Table 3: Parallel data sets for Haiti Creole -English  translation task.", "labels": [], "entities": [{"text": "Haiti Creole -English  translation task", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.5886723200480143}]}, {"text": " Table 4: Monolingual English data sets for Haiti Cre- ole -English translation task.", "labels": [], "entities": [{"text": "Cre- ole -English translation task", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.7768749807562146}]}, {"text": " Table 5: Domain adaptation results DE-FR. Domain: Alpine texts. Full IN TM: Using the full in-domain parallel  corpus; small IN TM: using 10% of available in-domain parallel data.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7647709846496582}, {"text": "IN", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.7888525724411011}, {"text": "IN", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9269569516181946}]}, {"text": " Table 6: Domain adaptation results HT-EN. Domain: emergency SMS. Full IN TM: Using the full in-domain  parallel corpus; small IN TM: using 10% of available in-domain parallel data.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7535257637500763}, {"text": "IN", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.7624883055686951}]}]}