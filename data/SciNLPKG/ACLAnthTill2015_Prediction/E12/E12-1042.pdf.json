{"title": [{"text": "Spectral Learning for Non-Deterministic Dependency Parsing", "labels": [], "entities": [{"text": "Spectral Learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9090159833431244}, {"text": "Parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.7611028552055359}]}], "abstractContent": [{"text": "In this paper we study spectral learning methods for non-deterministic split head-automata grammars, a powerful hidden-state formalism for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.8376854360103607}]}, {"text": "We present a learning algorithm that, like other spectral methods, is efficient and non-susceptible to local minima.", "labels": [], "entities": []}, {"text": "We show how this algorithm can be formulated as a technique for inducing hidden structure from distributions computed by forward-backward recursions.", "labels": [], "entities": []}, {"text": "Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena.", "labels": [], "entities": []}, {"text": "Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations), and (2) using hidden states to pass information across factors (;.", "labels": [], "entities": []}, {"text": "Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization.", "labels": [], "entities": []}, {"text": "However, they have two main drawbacks.", "labels": [], "entities": []}, {"text": "(1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.", "labels": [], "entities": []}, {"text": "A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.", "labels": [], "entities": []}, {"text": "This can be achieved by means of feature engineering, but compressing such information into a state of bounded size will typically belabor intensive, and will not generalize across languages.", "labels": [], "entities": []}, {"text": "(2) Increasing the size of the factors generally results in polynomial increases in the parsing cost.", "labels": [], "entities": [{"text": "parsing", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.9800884127616882}]}, {"text": "In principle, hidden variable models could solve some of the problems of feature engineering in higher-order factorizations, since they could automatically induce the information in a derivation history that should be passed across factors.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7661221623420715}]}, {"text": "Potentially, they would require less feature engineering since they can learn from an annotated corpus an optimal way to compress derivations into hidden states.", "labels": [], "entities": []}, {"text": "For example, one line of work has added hidden annotations to the non-terminals of a phrase-structure grammar (, resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars.", "labels": [], "entities": []}, {"text": "A second line of work has modeled hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de).", "labels": [], "entities": []}, {"text": "Finally, a third line of work has induced hidden structure from the history of actions of a parser.", "labels": [], "entities": []}, {"text": "However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem.", "labels": [], "entities": [{"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.982417106628418}]}, {"text": "As a result, training a hidden-variable model is both expensive and prone to local minima issues.", "labels": [], "entities": []}, {"text": "In this paper we present a learning algorithm for hidden-state split head-automata grammars (SHAG).", "labels": [], "entities": [{"text": "hidden-state split head-automata grammars (SHAG)", "start_pos": 50, "end_pos": 98, "type": "TASK", "confidence": 0.6911408858639854}]}, {"text": "In this for-malism, head-modifier sequences are generated by a collection of finite-state automata.", "labels": [], "entities": []}, {"text": "In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation.", "labels": [], "entities": []}, {"text": "This representation allows the use of simple spectral algorithms for estimating the model parameters from data (.", "labels": [], "entities": []}, {"text": "In all previous work, the algorithms used to induce hidden structure require running repeated inference on training data-e.g. Expectation-Maximization, or split-merge algorithms.", "labels": [], "entities": []}, {"text": "In contrast, spectral methods are simple and very efficient -parameter estimation is reduced to computing some data statistics, performing SVD, and inverting matrices.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: \u2022 We present a spectral learning algorithm for inducing PNFA with applications to headautomata dependency grammars.", "labels": [], "entities": []}, {"text": "Our formulation is based on thinking about the distribution generated by a PNFA in terms of the forward-backward recursions.", "labels": [], "entities": []}, {"text": "\u2022 Spectral learning algorithms in previous work only use statistics of prefixes of sequences.", "labels": [], "entities": [{"text": "Spectral learning", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.8289158940315247}]}, {"text": "In contrast, our algorithm is able to learn from substring statistics.", "labels": [], "entities": []}, {"text": "\u2022 We derive an inside-outside algorithm for non-deterministic SHAG that runs in cubic time, keeping the costs of CFG parsing.", "labels": [], "entities": [{"text": "SHAG", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.9400975704193115}, {"text": "CFG parsing", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.8047708570957184}]}, {"text": "\u2022 In experiments we show that adding nondeterminism improves the accuracy of several baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9991995692253113}]}, {"text": "When we compare our algorithm to EM we observe a reduction of two orders of magnitude in training time.", "labels": [], "entities": [{"text": "EM", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.5261095762252808}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Next section describes the necessary background on SHAG and operator models.", "labels": [], "entities": [{"text": "SHAG", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.9768186807632446}]}, {"text": "Section 3 introduces Operator SHAG for parsing, and presents a spectral learning algorithm.", "labels": [], "entities": [{"text": "Operator SHAG", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.6889291703701019}, {"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9801203608512878}]}, {"text": "Section 4 presents a parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9774340987205505}]}, {"text": "Section 5 presents experiments and analysis of results, and section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiments is to show that incorporating hidden states in a SHAG using operator models can consistently improve parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9745346307754517}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.8565141558647156}]}, {"text": "A second goal is to compare the spectral learning algorithm to EM, a standard learning method that also induces hidden states.", "labels": [], "entities": []}, {"text": "The first set of experiments involve fully unlexicalized models, i.e. parsing part-of-speech tag sequences.", "labels": [], "entities": [{"text": "parsing part-of-speech tag sequences", "start_pos": 70, "end_pos": 106, "type": "TASK", "confidence": 0.8298097252845764}]}, {"text": "While this setting falls behind the stateof-the-art, it is nonetheless valid to analyze empirically the effect of incorporating hidden states via operator models, which results in large improvements.", "labels": [], "entities": []}, {"text": "Ina second set of experiments, we combine the unlexicalized hidden-state models with simple lexicalized models.", "labels": [], "entities": []}, {"text": "Finally, we present some analysis of the automaton learned by the spectral algorithm to seethe information that is captured in the hidden state space.", "labels": [], "entities": []}, {"text": "We now turn to combining lexicalized deterministic grammars with the unlexicalized grammars obtained in the previous experiment using the spectral algorithm.", "labels": [], "entities": []}, {"text": "The goal behind this experiment is to show that the information captured in hidden states is complimentary to head-modifier lexical preferences.", "labels": [], "entities": []}, {"text": "In this case X consists of lexical items, and we assume access to the PoS tag of each lexical item.", "labels": [], "entities": []}, {"text": "We will denote as ta and w a the PoS tag and word of a symbol a \u2208 \u00af X . We will estimate conditional distributions P(a | h, d, \u03c3), where a \u2208 X is a modifier, h \u2208 \u00af X is ahead, dis a direction, and \u03c3 is a deterministic state.", "labels": [], "entities": []}, {"text": "Following Collins (1999), we use three configurations of deterministic states: \u2022 LEX: a single state.", "labels": [], "entities": [{"text": "LEX", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9894423484802246}]}, {"text": "\u2022 LEX+F: two distinct states for first modifier and rest of modifiers.", "labels": [], "entities": [{"text": "LEX+F", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.876866360505422}]}, {"text": "\u2022 LEX+FCP: four distinct states, encoding: first modifier, previous modifier was a coordination, previous modifier was punctuation, and previous modifier was some other word.", "labels": [], "entities": [{"text": "LEX+FCP", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.7724300225575765}]}, {"text": "To estimate P we use a back-off strategy: To estimate PA we use two back-off levels, the fine level conditions on {w h , d, \u03c3} and the coarse level conditions on {t h , d, \u03c3}.", "labels": [], "entities": []}, {"text": "For P B we use three levels, which from fine to coarse are {t a , w h , d, \u03c3}, {t a , t h , d, \u03c3} and {t a }.", "labels": [], "entities": []}, {"text": "We follow Collins (1999) to estimate PA and P B from a treebank using a back-off strategy.", "labels": [], "entities": [{"text": "PA and P B", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.8275435268878937}]}, {"text": "We use a simple approach to combine lexical models with the unlexical hidden-state models we obtained in the previous experiment.", "labels": [], "entities": []}, {"text": "Namely, we use a log-linear model that computes scores for head-modifier sequences as where P sp and P det are respectively spectral and deterministic probabilistic models.", "labels": [], "entities": []}, {"text": "We tested combinations of each deterministic model with the spectral unlexicalized model using different number of states.", "labels": [], "entities": []}, {"text": "shows the accuracies of single deterministic models, together with combinations using different number of states.", "labels": [], "entities": []}, {"text": "In all cases, the combinations largely improve over the purely deterministic lexical counterparts, suggesting that the information encoded in hidden states is complementary to lexical preferences.", "labels": [], "entities": []}], "tableCaptions": []}