{"title": [{"text": "Learning for Microblogs with Distant Supervision: Political Forecasting with Twitter", "labels": [], "entities": []}], "abstractContent": [{"text": "Microblogging websites such as Twitter offer a wealth of insight into a popu-lation's current mood.", "labels": [], "entities": []}, {"text": "Automated approaches to identify general sentiment toward a particular topic often perform two steps: Topic Identification and Sentiment Analysis.", "labels": [], "entities": [{"text": "Topic Identification", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8829405903816223}, {"text": "Sentiment Analysis", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.9367374181747437}]}, {"text": "Topic Identification first identifies tweets that are relevant to a desired topic (e.g., a politician or event), and Sentiment Analysis extracts each tweet's attitude toward the topic.", "labels": [], "entities": [{"text": "Topic Identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7629054188728333}, {"text": "Sentiment Analysis", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.8717270195484161}]}, {"text": "Many techniques for Topic Identification simply involve selecting tweets using a keyword search.", "labels": [], "entities": [{"text": "Topic Identification", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.9195451438426971}]}, {"text": "Here, we present an approach that instead uses distant supervision to train a classifier on the tweets returned by the search.", "labels": [], "entities": []}, {"text": "We show that distant supervision leads to improved performance in the Topic Identification task as well in the downstream Sentiment Analysis stage.", "labels": [], "entities": [{"text": "Topic Identification task", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.915070096651713}, {"text": "Sentiment Analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.9514793455600739}]}, {"text": "We then use a system that incorporates distant supervision into both stages to analyze the sentiment toward President Obama expressed in a dataset of tweets.", "labels": [], "entities": []}, {"text": "Our results better correlate with Gallup's Presidential Job Approval polls than previous work.", "labels": [], "entities": [{"text": "Gallup's Presidential Job Approval polls", "start_pos": 34, "end_pos": 74, "type": "DATASET", "confidence": 0.8426503737767538}]}, {"text": "Finally, we discover a surprising baseline that outperforms previous work without a Topic Identification stage.", "labels": [], "entities": [{"text": "Topic Identification", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.8905622363090515}]}], "introductionContent": [{"text": "Social networks and blogs contain a wealth of data about how the general public views products, campaigns, events, and people.", "labels": [], "entities": []}, {"text": "Automated algorithms can use this data to provide instant feedback on what people are saying about a topic.", "labels": [], "entities": []}, {"text": "Two challenges in building such algorithms are (1) identifying topic-relevant posts, and (2) identifying the attitude of each post toward the topic.", "labels": [], "entities": []}, {"text": "This paper studies distant supervision () as a solution to both challenges.", "labels": [], "entities": []}, {"text": "We apply our approach to the problem of predicting Presidential Job Approval polls from Twitter data, and we present results that improve on previous work in this area.", "labels": [], "entities": [{"text": "predicting Presidential Job Approval polls", "start_pos": 40, "end_pos": 82, "type": "TASK", "confidence": 0.8267437100410462}]}, {"text": "We also present a novel baseline that performs remarkably well without using topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.7016836404800415}]}, {"text": "Topic identification is the task of identifying text that discusses a topic of interest.", "labels": [], "entities": [{"text": "Topic identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8298658132553101}]}, {"text": "Most previous work on microblogs uses simple keyword searches to find topic-relevant tweets on the assumption that short tweets do not need more sophisticated processing.", "labels": [], "entities": []}, {"text": "For instance, searches for the name \"Obama\" have been assumed to return a representative set of tweets about the U.S. President.", "labels": [], "entities": []}, {"text": "One of the main contributions of this paper is to show that keyword search can lead to noisy results, and that the same keywords can instead be used in a distantly supervised framework to yield improved performance.", "labels": [], "entities": [{"text": "keyword search", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.8580738604068756}]}, {"text": "Distant supervision uses noisy signals in text as positive labels to train classifiers.", "labels": [], "entities": []}, {"text": "For instance, the token \"Obama\" can be used to identify a series of tweets that discuss U.S. President Barack Obama.", "labels": [], "entities": []}, {"text": "Although searching for token matches can return false positives, using the resulting tweets as positive training examples provides supervision from a distance.", "labels": [], "entities": []}, {"text": "This paper experiments with several diverse sets of keywords to train distantly supervised classifiers for topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.7933940589427948}]}, {"text": "We evaluate each classifier on a hand-labeled dataset of political and apolitical tweets, and demonstrate an improvement in F1 score oversimple keyword search (.39 to .90 in the best case).", "labels": [], "entities": [{"text": "F1 score oversimple keyword search", "start_pos": 124, "end_pos": 158, "type": "METRIC", "confidence": 0.9244787335395813}]}, {"text": "We also make available the first labeled dataset for topic identification in politics to encourage future work.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7623714804649353}]}, {"text": "Sentiment analysis encompasses abroad field of research, but most microblog work focuses on two moods: positive and negative sentiment.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9498147964477539}]}, {"text": "Algorithms to identify these moods range from matching words in a sentiment lexicon to training classifiers with a hand-labeled corpus.", "labels": [], "entities": []}, {"text": "Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision).", "labels": [], "entities": []}, {"text": "This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7214889526367188}, {"text": "sentiment classifiers", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7831970751285553}, {"text": "political forecasting", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.7220989763736725}]}, {"text": "Interest in measuring the political mood of a country has recently grown).", "labels": [], "entities": []}, {"text": "Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings.", "labels": [], "entities": [{"text": "Approval", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9971038699150085}, {"text": "Disapproval", "start_pos": 184, "end_pos": 195, "type": "METRIC", "confidence": 0.91994309425354}]}, {"text": "In this paper we present a method for coupling two distantly supervised algorithms for topic identification and sentiment classification on Twitter.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.827839583158493}, {"text": "sentiment classification", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.9053764939308167}]}, {"text": "In Section 4, we describe our approach to topic identification and present anew annotated corpus of political tweets for future study.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.8303045332431793}]}, {"text": "In Section 5, we apply distant supervision to sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9651128351688385}]}, {"text": "Finally, Section 6 discusses our system's performance on modeling Presidential Job Approval ratings from Twitter data.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments in this paper use seven months of tweets from Twitter (www.twitter.com) collected between June 1, 2009 and December 31, 2009.", "labels": [], "entities": []}, {"text": "The corpus contains over 476 million tweets labeled with usernames and timestamps, collected through Twitter's 'spritzer' API without keyword filtering.", "labels": [], "entities": []}, {"text": "Tweets are aligned with polling data in Section 6 using their timestamps.", "labels": [], "entities": []}, {"text": "The full system is evaluated against the publicly available daily Presidential Job Approval polling data from Gallup 1 . Every day, Gallup asks 1,500 adults in the United States about whether they approve or disapprove of \"the job President Obama is doing as president.\"", "labels": [], "entities": [{"text": "Presidential Job Approval polling data from Gallup 1", "start_pos": 66, "end_pos": 118, "type": "DATASET", "confidence": 0.7105715349316597}]}, {"text": "The results are compiled into two trend lines for Approval and Disapproval ratings, as shown in.", "labels": [], "entities": [{"text": "Approval", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992141723632812}, {"text": "Disapproval", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9802772402763367}]}, {"text": "We compare our positive and negative sentiment scores against these two trends.", "labels": [], "entities": []}, {"text": "In order to evaluate distant supervision against keyword search, we created two new labeled datasets of political and apolitical tweets.", "labels": [], "entities": [{"text": "keyword search", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7698146402835846}]}, {"text": "The Political Dataset is an amalgamation of all four keyword extractions (PC-1 is a subset of PC-4) listed in.", "labels": [], "entities": [{"text": "Political Dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7184412479400635}]}, {"text": "It consists of 2,000 tweets randomly chosen from the keyword searches of PC-2, PC-3, PC-4, and PC-5 with 500 tweets from each.", "labels": [], "entities": []}, {"text": "This combined dataset enables an evaluation of how well each classifier can identify tweets from other classifiers.", "labels": [], "entities": []}, {"text": "The General Dataset contains 2,000 random tweets from the entire corpus.", "labels": [], "entities": [{"text": "General Dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9454648792743683}]}, {"text": "This dataset allows us to evaluate how well classifiers identify political tweets in the wild.", "labels": [], "entities": [{"text": "classifiers identify political tweets", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.8048519194126129}]}, {"text": "This paper's authors initially annotated the same 200 tweets in the General Dataset to compute inter-annotator agreement.", "labels": [], "entities": [{"text": "General Dataset", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9845653176307678}]}, {"text": "The Kappa was 0.66, which is typically considered good agreement.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9490527510643005}, {"text": "agreement", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9684329628944397}]}, {"text": "Most disagreements occurred over tweets about money and the economy.", "labels": [], "entities": []}, {"text": "We then split the remaining portions of the two datasets between the two annotators.", "labels": [], "entities": []}, {"text": "The Political Dataset contains 1,691 political and 309 apolitical tweets, and the General Dataset contains 28 political tweets and 1,978 apolitical tweets.", "labels": [], "entities": [{"text": "Political Dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.905810534954071}, {"text": "General Dataset", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.9046341776847839}]}, {"text": "These two datasets of 2000 tweets each are publicly available for future evaluation and comparison to this work 2 .  Our first experiment addresses the question of keyword variance.", "labels": [], "entities": []}, {"text": "We measure performance on the Political Dataset, a combination of all of our proposed political keywords.", "labels": [], "entities": [{"text": "Political Dataset", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.8306965231895447}]}, {"text": "Each keyword set contributed to 25% of the dataset, so the evaluation measures the extent to which a classifier identifies other keyword tweets.", "labels": [], "entities": []}, {"text": "We classified the 2000 tweets with the five distantly supervised classifiers and the one \"Obama\" keyword extractor from O'.", "labels": [], "entities": [{"text": "O'.", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.8530313372612}]}, {"text": "Results are shown on the left side of.", "labels": [], "entities": []}, {"text": "Precision and recall calculate correct identification of the political label.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9754117131233215}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9995567202568054}]}, {"text": "The five distantly supervised approaches perform similarly, and show remarkable robustness despite their different training sets.", "labels": [], "entities": []}, {"text": "In contrast, the keyword extractor only captures about a quarter of the political tweets.", "labels": [], "entities": []}, {"text": "PC-1 is the distantly supervised analog to the Obama keyword extractor, and we see that distant supervision increases its F1 score dramatically from 0.39 to 0.90.", "labels": [], "entities": [{"text": "PC-1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8922754526138306}, {"text": "Obama keyword extractor", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7194538911183676}, {"text": "F1 score", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9922528564929962}]}, {"text": "The second evaluation addresses the question of classifier performance on Twitter as a whole, not just on apolitical dataset.", "labels": [], "entities": []}, {"text": "We evaluate on the General Dataset just as on the Political Dataset.", "labels": [], "entities": [{"text": "General Dataset", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.8994056582450867}, {"text": "Political Dataset", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.7975429594516754}]}, {"text": "Results are shown on the right side of.", "labels": [], "entities": []}, {"text": "Most tweets posted to Twitter are not about politics, so the apolitical label dominates this more representative dataset.", "labels": [], "entities": []}, {"text": "Again, the five distant supervision classifiers have similar results.", "labels": [], "entities": []}, {"text": "The Obama keyword search has the highest precision, but drastically sacrifices recall.", "labels": [], "entities": [{"text": "Obama keyword search", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8230468432108561}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9995356798171997}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9990553259849548}]}, {"text": "Four of the five classifiers outperform keyword search in F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9794064164161682}]}, {"text": "The following experiments combine both topic identification and sentiment analysis.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8210909068584442}, {"text": "sentiment analysis", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9523604214191437}]}, {"text": "The previous sections described six topic identification approaches, and two sentiment analysis approaches.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.8533422946929932}, {"text": "sentiment analysis", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.9472446143627167}]}, {"text": "We evaluate all combinations of these systems, and compare their final sentiment scores for each day in the nearly seven-month period over which our dataset spans.", "labels": [], "entities": []}, {"text": "Gallup's Daily Job Approval reports two numbers: Approval and Disapproval.", "labels": [], "entities": [{"text": "Gallup's Daily Job Approval", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.9440391182899475}, {"text": "Approval", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9993605017662048}, {"text": "Disapproval", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9970847964286804}]}, {"text": "We calculate individual sentiment scores S pos and S neg for each day, and compare the two sets of trends using Pearson's correlation coefficient.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 112, "end_pos": 145, "type": "METRIC", "confidence": 0.8251816779375076}]}, {"text": "O'Connor et al. do not explicitly evaluate these two, but instead use the ratio S ratio . We also calculate this daily ratio from Gallup for comparison purposes by dividing the Approval by the Disapproval.", "labels": [], "entities": [{"text": "Gallup", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.9541365504264832}, {"text": "Approval", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9996083378791809}, {"text": "Disapproval", "start_pos": 193, "end_pos": 204, "type": "METRIC", "confidence": 0.9987700581550598}]}], "tableCaptions": [{"text": " Table 2: Correlation between Gallup polling data and  the extracted sentiment with a lexicon (trends shown  in", "labels": [], "entities": [{"text": "Gallup polling data", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9125699996948242}]}, {"text": " Table 3: Correlation between Gallup Approval / Dis- approval ratio and extracted sentiment ratio scores.", "labels": [], "entities": [{"text": "Gallup Approval / Dis- approval ratio", "start_pos": 30, "end_pos": 67, "type": "METRIC", "confidence": 0.7836030466215951}, {"text": "extracted sentiment ratio scores", "start_pos": 72, "end_pos": 104, "type": "METRIC", "confidence": 0.8722998946905136}]}, {"text": " Table 4: Pearson's correlation coefficient of Sentiment  Analysis without Topic Identification.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.7688120454549789}, {"text": "Sentiment  Analysis without Topic Identification", "start_pos": 47, "end_pos": 95, "type": "TASK", "confidence": 0.8910419106483459}]}]}