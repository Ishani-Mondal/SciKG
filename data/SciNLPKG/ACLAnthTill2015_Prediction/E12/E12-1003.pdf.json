{"title": [{"text": "A Bayesian Approach to Unsupervised Semantic Role Induction", "labels": [], "entities": [{"text": "Semantic Role Induction", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.6477925082047781}]}], "abstractContent": [{"text": "We introduce two Bayesian models for un-supervised semantic role labeling (SRL) task.", "labels": [], "entities": [{"text": "semantic role labeling (SRL) task", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.8279735616275242}]}, {"text": "The models treat SRL as clustering of syntactic signatures of arguments with clusters corresponding to semantic roles.", "labels": [], "entities": [{"text": "SRL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9849297404289246}]}, {"text": "The first model induces these clusterings independently for each predicate, exploiting the Chinese Restaurant Process (CRP) as a prior.", "labels": [], "entities": [{"text": "Chinese Restaurant Process (CRP)", "start_pos": 91, "end_pos": 123, "type": "DATASET", "confidence": 0.8105343083540598}]}, {"text": "Ina more refined hierarchical model, we inject the intuition that the clus-terings are similar across different predicates , even though they are not necessarily identical.", "labels": [], "entities": []}, {"text": "This intuition is encoded as a distance-dependent CRP with a distance between two syntactic signatures indicating how likely they are to correspond to a single semantic role.", "labels": [], "entities": []}, {"text": "These distances are automatically induced within the model and shared across predicates.", "labels": [], "entities": []}, {"text": "Both models achieve state-of-the-art results when evaluated on PropBank, with the coupled model consistently outperforming the factored counterpart in all experimental setups .", "labels": [], "entities": [{"text": "PropBank", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9722833633422852}]}], "introductionContent": [{"text": "Semantic role labeling (SRL)), a shallow semantic parsing task, has recently attracted a lot of attention in the computational linguistic community).", "labels": [], "entities": [{"text": "Semantic role labeling (SRL))", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8147318164507548}, {"text": "semantic parsing task", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8059169252713522}]}, {"text": "The task involves prediction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role.", "labels": [], "entities": [{"text": "prediction of predicate argument structure", "start_pos": 18, "end_pos": 60, "type": "TASK", "confidence": 0.7793388605117798}]}, {"text": "For example, in the following sentences:   Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1).", "labels": [], "entities": [{"text": "A1", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9300074577331543}]}, {"text": "SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering, textual entailment (, machine translation (, and dialogue systems (), among others.", "labels": [], "entities": [{"text": "SRL representations", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9029828608036041}, {"text": "natural language processing", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.6593651274840037}, {"text": "question answering", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.8910419642925262}, {"text": "textual entailment", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.7392008900642395}, {"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.7659487724304199}]}, {"text": "Though syntactic representations are often predictive of semantic roles, the interface between syntactic and semantic representations is far from trivial.", "labels": [], "entities": []}, {"text": "The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.", "labels": [], "entities": []}, {"text": "Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.", "labels": [], "entities": [{"text": "predicting shallow semantic representations", "start_pos": 64, "end_pos": 107, "type": "TASK", "confidence": 0.8177907019853592}]}, {"text": "These resources are scarce and expensive to create, and even the largest of them have low coverage.", "labels": [], "entities": []}, {"text": "Moreover, these models are domain-specific, and their performance drops substantially when they are used in anew domain.", "labels": [], "entities": []}, {"text": "Such domain specificity is arguably unavoidable fora semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses).", "labels": [], "entities": []}, {"text": "The necessity fora large amounts of human-annotated data for every language and domain is one of the major obstacles to the wide-spread adoption of semantic role representations.", "labels": [], "entities": []}, {"text": "These challenges motivate the need for unsupervised methods which, instead of relying on labeled data, can exploit large amounts of unlabeled texts.", "labels": [], "entities": []}, {"text": "In this paper, we propose simple and effi-cient hierarchical Bayesian models for this task.", "labels": [], "entities": []}, {"text": "It is natural to split the SRL task into two stages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage).", "labels": [], "entities": [{"text": "SRL task", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.9321260750293732}]}, {"text": "In this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage.", "labels": [], "entities": [{"text": "labeling stage", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.8953105509281158}]}, {"text": "Identification, though an important problem, can be tackled with heuristics) or, potentially, by using a supervised classifier trained on a small amount of data.", "labels": [], "entities": [{"text": "Identification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9703513383865356}]}, {"text": "We follow, and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate.", "labels": [], "entities": []}, {"text": "In our first model, as inmost of the previous work on unsupervised SRL, we define an independent model for each predicate.", "labels": [], "entities": [{"text": "SRL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9141708612442017}]}, {"text": "We use the Chinese Restaurant Process (CRP) as a prior for the clustering of syntactic signatures.", "labels": [], "entities": [{"text": "Chinese Restaurant Process (CRP)", "start_pos": 11, "end_pos": 43, "type": "DATASET", "confidence": 0.7793280382951101}, {"text": "clustering of syntactic signatures", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.7559681385755539}]}, {"text": "The resulting model achieves state-of-the-art results, substantially outperforming previous methods evaluated in the same setting.", "labels": [], "entities": []}, {"text": "In the first model, for each predicate we independently induce a linking between syntax and semantics, encoded as a clustering of syntactic signatures.", "labels": [], "entities": []}, {"text": "The clustering implicitly defines the set of permissible alternations, or changes in the syntactic realization of the argument structure of the verb.", "labels": [], "entities": []}, {"text": "Though different verbs admit different alternations, some alternations are shared across multiple verbs and are very frequent (e.g., passivization, example sentences (a) vs. (d), or dativization: John gave a book to Mary vs. John gave Mary a book).", "labels": [], "entities": []}, {"text": "Therefore, it is natural to assume that the clusterings should be similar, though not identical, across verbs.", "labels": [], "entities": []}, {"text": "Our second model encodes this intuition by replacing the CRP prior for each predicate with a distance-dependent CRP (dd-CRP) prior (Blei and Frazier, 2011) shared across predicates.", "labels": [], "entities": []}, {"text": "The distance between two syntactic signatures encodes how likely they are to correspond to a single semantic role.", "labels": [], "entities": []}, {"text": "Unlike most of the previous work exploiting distance-dependent CRPs (), we do not encode prior or external knowledge in the distance function but rather induce it automatically within our Bayesian model.", "labels": [], "entities": []}, {"text": "The coupled dd-CRP model consistently outperforms the factored CRP counterpart across all the experimental settings (with gold and predicted syntactic parses, and with gold and automatically identified arguments).", "labels": [], "entities": []}, {"text": "Both models admit efficient inference: the estimation time on the Penn Treebank WSJ corpus does not exceed 30 minutes on a single processor and the inference algorithm is highly parallelizable, reducing inference time down to several minutes on multiple processors.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 66, "end_pos": 90, "type": "DATASET", "confidence": 0.9858498722314835}]}, {"text": "This suggests that the models scale to much larger corpora, which is an important property fora successful unsupervised learning method, as unlabeled data is abundant.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 begins with a definition of the semantic role labeling task and discuss some specifics of the unsupervised setting.", "labels": [], "entities": [{"text": "semantic role labeling task", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7124142721295357}]}, {"text": "In Section 3, we describe CRPs and dd-CRPs, the key components of our models.", "labels": [], "entities": []}, {"text": "In Sections 4 -6, we describe our factored and coupled models and the inference method.", "labels": [], "entities": []}, {"text": "Section 7 provides both evaluation and analysis.", "labels": [], "entities": []}, {"text": "Finally, additional related work is presented in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We keep the general setup of, to evaluate our models and compare them to the current state of the art.", "labels": [], "entities": []}, {"text": "We run all of our experiments on the standard CoNLL 2008 shared task () version of Penn Treebank WSJ and PropBank.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.8825693279504776}, {"text": "Penn Treebank WSJ", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9564664562543234}, {"text": "PropBank", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.7403090596199036}]}, {"text": "In addition to gold dependency analyses and gold PropBank annotations, it has dependency structures generated automatically by the MaltParser (.", "labels": [], "entities": []}, {"text": "We vary our experimental setup as follows: \u2022 We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in four experimental regimes.", "labels": [], "entities": []}, {"text": "\u2022 In order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by a clustering algorithm as a preprocessing step.", "labels": [], "entities": [{"text": "predicate argument fillers", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.6503551006317139}]}, {"text": "In particular, we use Brown (Br) clustering induced over RCV1 corpus (.", "labels": [], "entities": [{"text": "RCV1 corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9408678114414215}]}, {"text": "Although the clustering is hierarchical, we only use a cluster at the lowest level of the hierarchy for each word.", "labels": [], "entities": []}, {"text": "We use the purity (PU) and collocation (CO) metrics as well as their harmonic mean (F1) to measure the quality of the resulting clusters.", "labels": [], "entities": [{"text": "purity (PU) and collocation (CO)", "start_pos": 11, "end_pos": 43, "type": "METRIC", "confidence": 0.7674674457973905}, {"text": "harmonic mean (F1)", "start_pos": 69, "end_pos": 87, "type": "METRIC", "confidence": 0.8782059192657471}]}, {"text": "Purity measures the degree to which each cluster contains arguments sharing the same gold role: where if Ci is the set of arguments in the i-th induced cluster, G j is the set of arguments in the jth gold cluster, and N is the total number of arguments.", "labels": [], "entities": []}, {"text": "Collocation evaluates the degree to which arguments with the same gold roles are assigned to a single cluster.", "labels": [], "entities": []}, {"text": "It is computed as follows: We compute the aggregate PU, CO, and F1 scores overall predicates in the same way as) by weighting the scores of each predicate by the number of its argument occurrences.", "labels": [], "entities": [{"text": "CO", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.8954662680625916}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.991454005241394}]}, {"text": "Note that since our goal is to evaluate the clustering algorithms, we do not include incorrectly identified arguments (i.e. mistakes made by the heuristic defined in Section 2) when computing these metrics.", "labels": [], "entities": []}, {"text": "We evaluate both factored and coupled models proposed in this work with and without Brown word clustering of argument fillers.", "labels": [], "entities": []}, {"text": "Our models are robust to parameter settings, they were tuned (to an order of magnitude) on the development set and were the same for all model variants: \u03b1 = 1.e-3, \u03b2 = 1.e-3, \u03b7 0 = 1.e-3, \u03b7 1 = 1.e-10, T = 5.", "labels": [], "entities": [{"text": "T", "start_pos": 202, "end_pos": 203, "type": "METRIC", "confidence": 0.9616690278053284}]}, {"text": "Although they can be induced within the model, we set them by hand to indicate granularity preferences.", "labels": [], "entities": []}, {"text": "We compare our results with the following alternative approaches.", "labels": [], "entities": []}, {"text": "The syntactic function baseline (SyntF) simply clusters predicate arguments according to the dependency relation to their head.", "labels": [], "entities": []}, {"text": "Following, we allocate a cluster for each of 20 most frequent relations in the CoNLL dataset and one cluster for all other relations.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9612307548522949}]}, {"text": "We also compare our performance with the Latent Logistic classification (, Split-Merge clustering (, and Graph Partitioning (Lang and Lapata, 2011b) approaches (labeled LLogistic, SplitMerge, and GraphPart, respectively) which achieve the current best unsupervised SRL results in this setting.", "labels": [], "entities": [{"text": "Latent Logistic classification", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.6612184842427572}, {"text": "Graph", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.950672447681427}, {"text": "SRL", "start_pos": 265, "end_pos": 268, "type": "TASK", "confidence": 0.8855365514755249}]}], "tableCaptions": [{"text": " Table 1: Argument clustering performance with gold  argument identification. Bold-face is used to highlight  the best F1 scores.", "labels": [], "entities": [{"text": "Argument clustering", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7704882919788361}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9976975321769714}]}, {"text": " Table 2: Argument clustering performance with auto- matic argument identification.", "labels": [], "entities": [{"text": "Argument clustering", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7914077937602997}, {"text": "auto- matic argument identification", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.7259195446968079}]}]}