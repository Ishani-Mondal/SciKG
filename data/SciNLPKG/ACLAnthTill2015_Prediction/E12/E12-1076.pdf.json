{"title": [{"text": "Midge: Generating Image Descriptions From Computer Vision Detections", "labels": [], "entities": [{"text": "Midge: Generating Image Descriptions From Computer Vision Detections", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.6565131213929918}]}], "abstractContent": [{"text": "This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections.", "labels": [], "entities": []}, {"text": "By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees.", "labels": [], "entities": []}, {"text": "Results show that the generation system outper-forms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is becoming areal possibility for intelligent systems to talk about the visual world.", "labels": [], "entities": []}, {"text": "New ways of mapping computer vision to generated language have emerged in the past few years, with a focus on pairing detections in an image to words).", "labels": [], "entities": [{"text": "pairing detections in an image to words", "start_pos": 110, "end_pos": 149, "type": "TASK", "confidence": 0.8636875663484845}]}, {"text": "The goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic , summaries that add content where the computer vision system does not, and captions copied directly from other images that are globally) and locally similar (.", "labels": [], "entities": []}, {"text": "A commonality between all of these approaches is that they aim to produce naturalsounding descriptions from computer vision detections.", "labels": [], "entities": []}, {"text": "This commonality is our starting point: We aim to design a system capable of producing natural-sounding descriptions from computer vision detections that are flexible enough to become more descriptive and poetic, or include likely inThe busby the road with a clear blue sky formation from a language model, or to be short and simple, but as true to the image as possible.", "labels": [], "entities": []}, {"text": "Rather than using a fixed template capable of generating one kind of utterance, our approach therefore lies in generating syntactic trees.", "labels": [], "entities": []}, {"text": "We use a tree-generating process (Section 4.3) similar to a Tree Substitution Grammar, but preserving some of the idiosyncrasies of the Penn Treebank syntax) on which most statistical parsers are developed.", "labels": [], "entities": [{"text": "Penn Treebank syntax", "start_pos": 136, "end_pos": 156, "type": "DATASET", "confidence": 0.9776128133138021}]}, {"text": "This allows us to automatically parse and train on an unlimited amount of text, creating data-driven models that flesh out descriptions around detected objects in a principled way, based on what is both likely and syntactically well-formed.", "labels": [], "entities": []}, {"text": "An example generated description is given in, and example vision output/natural language generation (NLG) input is given in Figure 2.", "labels": [], "entities": []}, {"text": "The system (\"Midge\") generates descriptions in present-tense, declarative phrases, as a na\u00a8\u0131vena\u00a8\u0131ve viewer without prior knowledge of the photograph's content.", "labels": [], "entities": []}, {"text": "Midge is built using the following approach: An image processed by computer vision algorithms can be characterized as a triple <A i , Bi , Ci >, where:  \u2022 A i is the set of object/stuff detections with bounding boxes and associated \"attribute\" detections within those bounding boxes.", "labels": [], "entities": []}, {"text": "\u2022 Bi is the set of action or pose detections associated to each a i \u2208 A i . \u2022 Ci is the set of spatial relationships that hold between the bounding boxes of each pair Similarly, a description of an image can be characterized as a triple \u2022 A dis the set of nouns in the description with associated modifiers.", "labels": [], "entities": []}, {"text": "\u2022 B dis the set of verbs associated to each ad \u2208 A d . \u2022 Cd is the set of prepositions that hold between each pair of ad , a e \u2208 A d . With this representation, mapping The problem then becomes: (1) How to filter out detections that are wrong; (2) how to order the objects so that they are mentioned in a natural way; (3) how to connect these ordered objects within a syntactically/semantically well-formed tree; and (4) how to add further descriptive information from language modeling alone, if required.", "labels": [], "entities": []}, {"text": "Our solution lies in using A i and A d as description anchors.", "labels": [], "entities": []}, {"text": "In computer vision, object detections form the basis of action/pose, attribute, and spatial relationship detections; therefore, in our approach to language generation, nouns for the object detections are used as the basis for the description.", "labels": [], "entities": [{"text": "object detections", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.736374020576477}, {"text": "spatial relationship detections", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.793610135714213}, {"text": "language generation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7671433389186859}]}, {"text": "Likelihood estimates of syntactic structure and word co-occurrence are conditioned on object nouns, and this enables each noun head in a description to select for the kinds of structures it tends to appear in (syntactic constraints) and the other words it tends to occur with (semantic constraints).", "labels": [], "entities": []}, {"text": "This is a data-driven way to generate likely adjectives, prepositions, determiners, etc., taking the intersection of what the vision system predicts and how the object noun tends to be described.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each set of sentences is generated with \u03b1 (likelihood cutoff) set to .01 and \u03b3 (observation count cutoff) set to 3.", "labels": [], "entities": [{"text": "likelihood cutoff) set", "start_pos": 43, "end_pos": 65, "type": "METRIC", "confidence": 0.891003355383873}, {"text": "observation count cutoff)", "start_pos": 80, "end_pos": 105, "type": "METRIC", "confidence": 0.8733007162809372}]}, {"text": "We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al.", "labels": [], "entities": []}, {"text": "Human judgments were collected using Amazon's Mechanical Turk (Amazon, 2011).", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (Amazon, 2011)", "start_pos": 37, "end_pos": 76, "type": "DATASET", "confidence": 0.9167583386103312}]}, {"text": "We follow recommended practices for evaluating an NLG system) and for running a study on Mechanical Turk, using a balanced design with each subject rating 3 descriptions from each system.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.5744649022817612}]}, {"text": "Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test.", "labels": [], "entities": []}, {"text": "Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at least 1 of the 3 human-written descriptions were removed and replaced.", "labels": [], "entities": []}, {"text": "It is important to note that this evaluation compares full generation systems; many factors are at play in each system that may also influence participants' perception, e.g., sentence length (Napoles et al., 2011) and punctuation decisions.", "labels": [], "entities": []}, {"text": "The systems are evaluated on a set of 840 images evaluated in the original  system.", "labels": [], "entities": []}, {"text": "Participants were asked to judge the statements given in   We report the scores for the systems in.", "labels": [], "entities": []}, {"text": "Results are analyzed using the non-parametric Wilcoxon Signed-Rank test, which uses median values to compare the different systems.", "labels": [], "entities": []}, {"text": "Midge outperforms all recent automatic approaches on CORRECTNESS and ORDER, and Yang et al. additionally on HUMANLIKENESS and MAIN AS-PECTS.", "labels": [], "entities": [{"text": "ORDER", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.942136287689209}]}, {"text": "Differences between Midge and Kulkarni et al. are significant at p < .01; Midge and Yang et al. at p < .001.", "labels": [], "entities": []}, {"text": "For all metrics, human-written descriptions still outperform automatic approaches (p < .001).", "labels": [], "entities": []}, {"text": "These findings are striking, particularly because Midge uses the same input as the Kulkarni et al. system.", "labels": [], "entities": [{"text": "Midge", "start_pos": 50, "end_pos": 55, "type": "TASK", "confidence": 0.5558041334152222}]}, {"text": "Using syntactically informed word co-occurrence statistics from a large corpus of descriptive text improves over state-of-the-art, allowing syntactic trees to be generated that capture the variation of natural language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Median scores for systems, mean and standard deviation in parentheses. Distance between points on the  rating scale cannot be assumed to be equidistant, and so we analyze results using a non-parametric test.", "labels": [], "entities": []}]}