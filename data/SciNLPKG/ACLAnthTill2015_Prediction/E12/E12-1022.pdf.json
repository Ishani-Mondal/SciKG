{"title": [{"text": "DualSum: a Topic-Model based approach for update summarization", "labels": [], "entities": [{"text": "update summarization", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.8170665800571442}]}], "abstractContent": [{"text": "Update summarization is anew challenge in multi-document summarization focusing on summarizing a set of recent documents relatively to another set of earlier documents.", "labels": [], "entities": [{"text": "Update summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8732726275920868}, {"text": "multi-document summarization", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6695267558097839}, {"text": "summarizing a set of recent documents", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.8321029345194498}]}, {"text": "We present an unsupervised proba-bilistic approach to model novelty in a document collection and apply it to the generation of update summaries.", "labels": [], "entities": []}, {"text": "The new model, called DUALSUM, results in the second or third position in terms of the ROUGE met-rics when tuned for previous TAC competitions and tested on TAC-2011, being statistically indistinguishable from the winning system.", "labels": [], "entities": [{"text": "DUALSUM", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9308039546012878}, {"text": "ROUGE", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9966093301773071}, {"text": "TAC", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.7824512124061584}, {"text": "TAC-2011", "start_pos": 157, "end_pos": 165, "type": "DATASET", "confidence": 0.9458675384521484}]}, {"text": "A manual evaluation of the generated summaries shows state-of-the art results for DUALSUM with respect to focus, coherence and overall responsiveness.", "labels": [], "entities": [{"text": "DUALSUM", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.5626744031906128}]}], "introductionContent": [{"text": "Update summarization is the problem of extracting and synthesizing novel information in a collection of documents with respect to a set of documents assumed to be known by the reader.", "labels": [], "entities": [{"text": "Update summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9249404668807983}]}, {"text": "This problem has received much attention in recent years, as can be observed in the number of participants to the special track on update summarization organized by DUC and TAC since 2007.", "labels": [], "entities": [{"text": "DUC", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.9564436078071594}, {"text": "TAC", "start_pos": 173, "end_pos": 176, "type": "DATASET", "confidence": 0.52948397397995}]}, {"text": "The problem is usually formalized as follows: Given two collections A and B, where the documents in A chronologically precede the documents in B, generate a summary of B under the assumption that the user of the summary has already read the documents in A.", "labels": [], "entities": []}, {"text": "Extractive techniques are the most common approaches in multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6903714239597321}]}, {"text": "Summaries generated by such techniques consist of sentences extracted from the document collection.", "labels": [], "entities": []}, {"text": "Extracts can have coherence and cohesion problems, but they generally offer a good tradeoff between linguistic quality and informativeness.", "labels": [], "entities": []}, {"text": "While numerous extractive summarization techniques have been proposed for multidocument summarization (), few techniques have been specifically designed for update summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.5690474212169647}, {"text": "multidocument summarization", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7305935025215149}, {"text": "update summarization", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.6290286183357239}]}, {"text": "Most existing approaches handle it as a redundancy removal problem, with the goal of producing a summary of collection B that is as dissimilar as possible from either collection A or from a summary of collection A.", "labels": [], "entities": []}, {"text": "A problem with this approach is that it can easily classify as redundant sentences in which novel information is mixed with existing information (from collection A).", "labels": [], "entities": []}, {"text": "Furthermore, while this approach can identify sentences that contain novel information, it cannot model explicitly what the novel information is.", "labels": [], "entities": []}, {"text": "Recently, Bayesian models have successfully been applied to multi-document summarization showing state-of-the-art results in summarization competitions ().", "labels": [], "entities": [{"text": "summarization competitions", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.9020451009273529}]}, {"text": "These approaches offer clear and rigorous probabilistic interpretations that many other techniques lack.", "labels": [], "entities": []}, {"text": "Furthermore, they have the advantage of operating in unsupervised settings, which can be used in real-world scenarios, across domains and languages.", "labels": [], "entities": []}, {"text": "To our best knowledge, previous work has not used this approach for update summarization.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7955533564090729}]}, {"text": "In this article, we propose a novel nonparametric Bayesian approach for update summarization.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.8902097046375275}]}, {"text": "Our approach, which is a variation of Latent Dirichlet Allocation (LDA) (, aims to learn to distinguish between common information and novel information.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 38, "end_pos": 70, "type": "METRIC", "confidence": 0.8316926479339599}]}, {"text": "We have evaluated this approach on the ROUGE scores and demonstrate that it produces comparable results to the top system in TAC-2011.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9749624133110046}, {"text": "TAC-2011", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.9145750999450684}]}, {"text": "Furthermore, our approach improves over that system when evaluated manually in terms of linguistic quality and overall responsiveness.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Bayesian graphical model described in the previous section can be run over a set of news collections to learn the background distribution, a joint distribution for each collection, an update distribution for each collection and the documentspecific distributions.", "labels": [], "entities": []}, {"text": "Once this is done, one of the learned collections can be used to generate the summary that best approximates this collection, using the greedy algorithm described by.", "labels": [], "entities": []}, {"text": "Still, there are some parameters that can be defined and which affects the results obtained: \u2022 DUALSUM's choice of hyper-parameters affects how the topics are learned.", "labels": [], "entities": []}, {"text": "\u2022 The documents can be represented with ngrams of different lengths.", "labels": [], "entities": []}, {"text": "\u2022 It is possible to generate a summary that approximates the joint distribution, the updateonly distribution, or a combination of both.", "labels": [], "entities": []}, {"text": "This section describes how these parameters have been tuned.", "labels": [], "entities": []}, {"text": "DUALSUM and the three baselines have been automatically evaluated using the TAC-2011 dataset.", "labels": [], "entities": [{"text": "DUALSUM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8829638957977295}, {"text": "TAC-2011 dataset", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9707452356815338}]}, {"text": "shows the ROUGE results obtained.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9907721281051636}]}, {"text": "Because of the non-deterministic nature of Gibbs sampling, the results reported here are the average of five runs for all the baselines and for DUALSUM.", "labels": [], "entities": [{"text": "DUALSUM", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.7258555889129639}]}, {"text": "DUALSUM outperforms two of the baselines in all three ROUGE metrics, and it also outperforms TOPICSUM B on two of the three metrics.", "labels": [], "entities": [{"text": "DUALSUM", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9437246322631836}, {"text": "ROUGE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9331008195877075}, {"text": "TOPICSUM B", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.7773050367832184}]}, {"text": "The top three systems in TAC-2011 have been included for comparison.", "labels": [], "entities": [{"text": "TAC-2011", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.879080593585968}]}, {"text": "The results between these three systems, and between them and DU-ALSUM, are all indistinguishable at 95% confidence.", "labels": [], "entities": [{"text": "DU-ALSUM", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.8711850047111511}]}, {"text": "Note that the best baseline, TOPICSUM B , is quite competitive, with results that are indistinguishable to the top participants in this year's evaluation.", "labels": [], "entities": [{"text": "TOPICSUM B", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.8350163996219635}]}, {"text": "Note as well that, because we have five different runs for our algorithms, whereas we just have one output for the TAC participants, the confidence intervals in the second case were slightly bigger when checking for statistical significance, so it is slightly harder for these systems to assert that they outperform the baselines with 95% confidence.", "labels": [], "entities": []}, {"text": "These results would have made DUALSUM the second best system for ROUGE-1 and ROUGE-SU4, and the third best system in terms of ROUGE-2.", "labels": [], "entities": [{"text": "DUALSUM", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.7265638709068298}]}, {"text": "The supplementary materials contain a detailed example of the the topic model obtained for the background in the TAC-2011 dataset, and the base and update models for collection D1110.", "labels": [], "entities": [{"text": "TAC-2011 dataset", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9640193581581116}]}, {"text": "As expected, the top unigrams and bigrams are all closed-class words and auxiliary verbs.", "labels": [], "entities": []}, {"text": "Because trigrams are longer, background trigrams actually include some content words (e.g. university or director).", "labels": [], "entities": []}, {"text": "Regarding the models for \u03c6 A and \u03c6 B , the base distribution contains words related to the original event of an earthquake in Sichuan province (China), and the update distribution focuses more on the official (updated) death toll numbers.", "labels": [], "entities": []}, {"text": "It can be noted here that the tokenizer we used is very simple (splitting tokens separated with white-spaces or punctuation) so that numbers such as 7.9 (the magnitude of the earthquake) and 12,000 or 14,000 are divided into two tokens.", "labels": [], "entities": []}, {"text": "We thought this might be a for the bigram-based system to produce better results, but we ran the summarizers with a numbers-aware tokenizer and the statistical differences between versions still hold.", "labels": [], "entities": []}, {"text": "While the ROUGE metrics provides an arguable estimate of the informativeness of a generated summary, it does not account for other important aspects such as the readability or the overall responsiveness.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.7512948513031006}]}, {"text": "To evaluate such aspects, a manual evaluation is required.", "labels": [], "entities": []}, {"text": "A fairly standard approach for manual evaluation is through pairwise comparison ().", "labels": [], "entities": []}, {"text": "In this approach, raters are presented with pairs of summaries generated by two systems and they are asked to say which one is best with respect to some aspects.", "labels": [], "entities": []}, {"text": "We followed a similar approach to compare DualSum with Peer 43 -the best system with respect to ROUGE-2, on the TAC 2011 dataset.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.5544218420982361}, {"text": "TAC 2011 dataset", "start_pos": 112, "end_pos": 128, "type": "DATASET", "confidence": 0.971525231997172}]}, {"text": "For each collection, raters were presented with three summaries: a reference summary randomly chosen from the model summaries, and the summaries generated by Peer 43 and DualSum.", "labels": [], "entities": [{"text": "DualSum", "start_pos": 170, "end_pos": 177, "type": "DATASET", "confidence": 0.9002618193626404}]}, {"text": "They were asked to read the summaries and say which one of the two generated summaries is best with respect to: 1) Overall responsiveness: which summary is best overall (both in terms of content and fluency), 2) Focus: which summary contains less irrelevant details, 3) Coherence: which summary is more coherent and 4) Non-redundancy: which summary repeats less the same information.", "labels": [], "entities": []}, {"text": "For each aspect, the rater could also reply that both summary were of the same quality.", "labels": [], "entities": []}, {"text": "For each of the 44 collections in TAC-2011, 3 ratings were collected from raters . Results are reported in.", "labels": [], "entities": [{"text": "TAC-2011", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8783723711967468}]}, {"text": "DualSum outperforms Peer 43 in three aspects, including Overall Responsiveness, which aggregates all the other scores and can be considered the most important one.", "labels": [], "entities": [{"text": "Peer 43", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.5208709090948105}, {"text": "Overall Responsiveness", "start_pos": 56, "end_pos": 78, "type": "METRIC", "confidence": 0.8131609857082367}]}, {"text": "In total 132 raters participated to the task via our own crowdsourcing platform, not mentioned yet for blind review.", "labels": [], "entities": []}, {"text": "garding Non-redundancy, DualSum and Peer 43 obtain similar results but the majority of raters found no difference between the two systems.", "labels": [], "entities": [{"text": "DualSum", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.7624574303627014}]}, {"text": "Fleiss \u03ba has been used to measure the inter-rater agreement.", "labels": [], "entities": [{"text": "Fleiss \u03ba", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9036836326122284}]}, {"text": "For each aspect, we observe \u03ba \u223c 0.2 which corresponds to a slight agreement; but if we focus on tasks where the 3 ratings reflect a preference for either of the two systems, then \u03ba \u223c 0.5, which indicates moderate agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9536080956459045}]}], "tableCaptions": [{"text": " Table 1: Results on the TAC-2011 dataset.  \u2021 ,  \u2020 and  *  indicate that a result is significantly better than TOPICSUM B ,  TOPICSUM A\u222aB and TOPICSUM A +TOPICSUM B , respectively (p < 0.05).", "labels": [], "entities": [{"text": "TAC-2011 dataset", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.953532874584198}, {"text": "TOPICSUM B", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.7528454661369324}]}, {"text": " Table 2: Results of the side-by-side manual evaluation.", "labels": [], "entities": []}]}