{"title": [{"text": "Elliphant: Improved Automatic Detection of Zero Subjects and Impersonal Constructions in Spanish", "labels": [], "entities": [{"text": "Improved Automatic Detection of Zero Subjects", "start_pos": 11, "end_pos": 56, "type": "TASK", "confidence": 0.7155632724364599}]}], "abstractContent": [{"text": "In pro-drop languages, the detection of explicit subjects, zero subjects and non-referential impersonal constructions is crucial for anaphora and co-reference resolution.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.7861301004886627}]}, {"text": "While the identification of explicit and zero subjects has attracted the attention of researchers in the past, the automatic identification of impersonal constructions in Spanish has not been addressed yet and this work is the first such study.", "labels": [], "entities": [{"text": "automatic identification of impersonal constructions in Spanish", "start_pos": 115, "end_pos": 178, "type": "TASK", "confidence": 0.7411608397960663}]}, {"text": "In this paper we present a corpus to underpin research on the automatic detection of these linguistic phenomena in Spanish and a novel machine learning-based methodology for their computational treatment.", "labels": [], "entities": []}, {"text": "This study also provides an analysis of the features , discusses performance across two different genres and offers error analysis.", "labels": [], "entities": [{"text": "error", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.9435345530509949}]}, {"text": "The evaluation results show that our system performs better in detecting explicit subjects than alternative systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Subject ellipsis is the omission of the subject in a sentence.", "labels": [], "entities": []}, {"text": "We consider not only missing referential subject (zero subject) as manifestation of ellipsis, but also non-referential impersonal constructions.", "labels": [], "entities": []}, {"text": "Various natural language processing (NLP) tasks benefit from the identification of elliptical subjects, primarily anaphora resolution) and co-reference resolution).", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.6888157576322556}]}, {"text": "The difficulty in detecting missing subjects and non-referential pronouns has been acknowledged since the first studies on the computational treatment of anaphora.", "labels": [], "entities": []}, {"text": "However, this task is of crucial importance when processing pro-drop languages since subject ellipsis is a pervasive phenomenon in these languages.", "labels": [], "entities": []}, {"text": "For instance, in our Spanish corpus, 29% of the subjects are elided.", "labels": [], "entities": [{"text": "Spanish corpus", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.8078016042709351}]}, {"text": "Our method is based on classification of all expressions in subject position, including the recognition of Spanish non-referential impersonal constructions which, to the best of our knowledge, has not yet been addressed.", "labels": [], "entities": []}, {"text": "The necessity of identifying such kind of elliptical constructions has been specifically highlighted in work about Spanish zero pronouns) and co-reference resolution.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.7433296740055084}]}, {"text": "The main contributions of this study are: \u2022 A public annotated corpus in Spanish to compare different strategies for detecting explicit subjects, zero subjects and impersonal constructions.", "labels": [], "entities": []}, {"text": "\u2022 The first ML based approach to this problem in Spanish and a thorough analysis regarding features, learnability, genre and errors.", "labels": [], "entities": [{"text": "ML", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9767574667930603}]}, {"text": "\u2022 The best performing algorithms to automatically detect explicit subjects and impersonal constructions in Spanish.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the classes of Spanish subjects, while Section 3 provides a literature review.", "labels": [], "entities": []}, {"text": "Section 4 describes the creation and the annotation of the corpus and in Section 5 the machine learning (ML) method is presented.", "labels": [], "entities": []}, {"text": "The analysis of the features, the learning curves, the genre impact and the error analysis are all detailed in Section 6.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 76, "end_pos": 90, "type": "METRIC", "confidence": 0.9512966871261597}]}, {"text": "Finally, in Section 7, conclusions are drawn and plans for future work are discussed.", "labels": [], "entities": []}, {"text": "This work is an extension of the first author master's thesis and a preliminary version of the algorithm was presented in .", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine the most accurate algorithm for our classification task, two comparisons of learning algorithms implemented in WEKA) were carried out.", "labels": [], "entities": [{"text": "classification task", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.902106761932373}, {"text": "WEKA", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.8858842253684998}]}, {"text": "Firstly, the classification was performed using 20% of the training instances.", "labels": [], "entities": [{"text": "classification", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9579405784606934}]}, {"text": "Secondly, the seven highest performing classifiers were compared using 100% of the  training data and ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "The corpus was partitioned into training and tested using ten-fold cross-validation for randomly ordered instances in both cases.", "labels": [], "entities": []}, {"text": "The lazy learning classifier K* (, using a blending parameter of 40%, was the best performing one, with an accuracy of 87.6% for ten-fold cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9980529546737671}]}, {"text": "K* differs from other instance-based learners in that it computes the distance between two instances using a method motivated by information theory, where a maximum entropy-based distance function is used.", "labels": [], "entities": []}, {"text": "shows the results for each class using ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "In contrast to previous work, the K* algorithm) was found to provide the most accurate classification in the current study.", "labels": [], "entities": []}, {"text": "Other approaches have employed various classification algorithms, including JRip in WEKA), with precision of 74% and recall of 60%, and K-nearest neighbors in TiMBL: both in) with precision of 73% and recall of 69%, and in () with precision of 82% and recall of 71%.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8534876108169556}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9989321827888489}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9989977478981018}, {"text": "TiMBL", "start_pos": 159, "end_pos": 164, "type": "DATASET", "confidence": 0.845381498336792}, {"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.993514895439148}, {"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9982611536979675}, {"text": "precision", "start_pos": 231, "end_pos": 240, "type": "METRIC", "confidence": 0.9979801774024963}, {"text": "recall", "start_pos": 252, "end_pos": 258, "type": "METRIC", "confidence": 0.998908281326294}]}, {"text": "Since there is no previous ML approach for this task in Spanish, our baselines for the explicit subjects and the zero subjects are the parser output and the previous rule-based work with the highest performance).", "labels": [], "entities": [{"text": "ML", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9679747819900513}]}, {"text": "For the impersonal constructions the baseline is a simple greedy algorithm that classifies as an impersonal construction every verb whose lemma is categorized as a verb with impersonal use according to the RAE dictionary).", "labels": [], "entities": [{"text": "RAE dictionary", "start_pos": 206, "end_pos": 220, "type": "DATASET", "confidence": 0.8080966472625732}]}, {"text": "Our method outperforms the Connexor parser which identifies the explicit subjects but makes no distinction between zero subjects and impersonal constructions.", "labels": [], "entities": []}, {"text": "Connexor yields 74.9% overall accuracy and 80.2% and 65.6% F-measure for explicit and elliptic subjects, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9799675941467285}, {"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9986473917961121}]}, {"text": "To compare with Ferr\u00e1ndez and Peral   it without impersonal constructions.", "labels": [], "entities": []}, {"text": "We achieve a precision of 87% for explicit subjects compared to 80%, and a precision of 87% for zero subjects compared to their 98%.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9995622038841248}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9991106390953064}]}, {"text": "The overall accuracy is the same for both techniques, 87.5%, but our results are more balanced.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997580647468567}]}, {"text": "Nevertheless, the approaches and corpora used in both studies are different, and hence it is not possible to do a fair comparison.", "labels": [], "entities": []}, {"text": "For example, their corpus has 46% of zero subjects while ours has only 26%.", "labels": [], "entities": []}, {"text": "For impersonal constructions our method outperforms the RAE baseline (precision 6.5%, recall 77.7%, F-measure 12.0% and accuracy 70.4%).", "labels": [], "entities": [{"text": "RAE", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.7880417704582214}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9965835809707642}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9992948770523071}, {"text": "F-measure", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.98847496509552}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9989044666290283}]}, {"text": "The low performance of the RAE baseline is due to the fact that verbs with impersonal use are often ambiguous.", "labels": [], "entities": [{"text": "RAE", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8246670961380005}]}, {"text": "For these cases, we first tagged them as ambiguous and then, we defined additional criteria after analyzing then manually.", "labels": [], "entities": []}, {"text": "The resulting annotated criteria are stated in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Instances per class in ESZIC Corpus.", "labels": [], "entities": [{"text": "Instances", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9667009115219116}, {"text": "ESZIC Corpus", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9499508440494537}]}, {"text": " Table 4: Features, definitions and values.", "labels": [], "entities": []}, {"text": " Table 5: K* performance (87.6% accuracy for ten-fold  cross validation).", "labels": [], "entities": [{"text": "K* performance", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8875913619995117}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9945547580718994}]}, {"text": " Table 6: Summary of accuracy comparison with previ- ous work.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8169644474983215}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9967495203018188}]}, {"text": " Table 7: Accuracy of cross-genre training and testing  evaluation (ten-fold evaluation).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9862546920776367}]}]}