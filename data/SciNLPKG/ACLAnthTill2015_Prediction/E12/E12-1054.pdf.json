{"title": [{"text": "Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History", "labels": [], "entities": [{"text": "Wikipedia Revision History", "start_pos": 69, "end_pos": 95, "type": "DATASET", "confidence": 0.9718376994132996}]}], "abstractContent": [{"text": "We evaluate measures of contextual fitness on the task of detecting real-word spelling errors.", "labels": [], "entities": [{"text": "detecting real-word spelling errors", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.800787165760994}]}, {"text": "For that purpose, we extract naturally occurring errors and their contexts from the Wikipedia revision history.", "labels": [], "entities": [{"text": "Wikipedia revision history", "start_pos": 84, "end_pos": 110, "type": "DATASET", "confidence": 0.9176074465115865}]}, {"text": "We show that such natural errors are better suited for evaluation than the previously used artificially created errors.", "labels": [], "entities": []}, {"text": "In particular , the precision of statistical methods has been largely overestimated , while the precision of knowledge-based approaches has been underestimated.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9978058934211731}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9982081651687622}]}, {"text": "Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations.", "labels": [], "entities": []}, {"text": "Finally, we show that statistical and knowledge-based methods can be combined for increased performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition), optical character recognition (, co-reference resolution (), or malapropism detection.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7914430201053619}, {"text": "optical character recognition", "start_pos": 133, "end_pos": 162, "type": "TASK", "confidence": 0.7461257974306742}, {"text": "malapropism detection", "start_pos": 197, "end_pos": 218, "type": "TASK", "confidence": 0.6861779540777206}]}, {"text": "The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar.", "labels": [], "entities": []}, {"text": "We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (.", "labels": [], "entities": [{"text": "malapropism detection", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.711232379078865}, {"text": "optical character recognition", "start_pos": 242, "end_pos": 271, "type": "TASK", "confidence": 0.6403728425502777}]}, {"text": "A malapropism or real-word spelling error occurs when a word is replaced with another correctly spelled word which does not suit the context, e.g. \"People with lots of honey usually live in big houses.\", where 'money' was replaced with 'honey'.", "labels": [], "entities": []}, {"text": "Besides typing mistakes, a major source of such errors is the failed attempt of automatic spelling correctors to correct a misspelled word.", "labels": [], "entities": [{"text": "automatic spelling correctors", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6582111418247223}]}, {"text": "A real-word spelling error is hard to detect, as the erroneous word is not misspelled and fits syntactically into the sentence.", "labels": [], "entities": []}, {"text": "Thus, measures of contextual fitness are required to detect words that do not fit their contexts.", "labels": [], "entities": []}, {"text": "Existing measures of contextual fitness can be categorized into knowledge-based) and statistical methods.", "labels": [], "entities": []}, {"text": "Both test the lexical cohesion of a word with its context.", "labels": [], "entities": []}, {"text": "For that purpose, knowledge-based approaches employ the structural knowledge encoded in lexical-semantic networks like WordNet, while statistical approaches rely on co-occurrence counts collected from large corpora, e.g. the Google Web1T corpus).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9521340131759644}, {"text": "Google Web1T corpus", "start_pos": 225, "end_pos": 244, "type": "DATASET", "confidence": 0.6606884698073069}]}, {"text": "So far, evaluation of contextual fitness measures relied on artificial datasets () which are created by taking a sentence that is known to be correct, and replacing a word with a similar word from the vocabulary.", "labels": [], "entities": []}, {"text": "This has a couple of disadvantages: (i) the replacement might be a synonym of the original word and perfectly valid in the given context, (ii) the generated error might be very unlikely to be made by a human, and (iii) inserting artificial errors often leads to unnatural sentences that are quite easy to correct, e.g. if the word class has changed.", "labels": [], "entities": []}, {"text": "However, even if the word class is unchanged, the original word and its replacement might still be variants of the same lemma, e.g. a noun in singular and plural, or a verb in present and past form.", "labels": [], "entities": []}, {"text": "This usually leads to a sentence where the error can be easily detected using syntactical or statistical methods, but is almost impossible to detect for knowledge-based measures of contextual fitness, as the meaning of the word stays more or less unchanged.", "labels": [], "entities": []}, {"text": "To estimate the impact of this issue, we randomly sampled 1,000 artificially created real-word spelling errors 1 and found 387 singular/plural pairs and 57 pairs which were in another direct relation (e.g. adjective/adverb).", "labels": [], "entities": []}, {"text": "This means that almost half of the artificially created errors are not suited for an evaluation targeted at finding optimal measures of contextual fitness, as they over-estimate the performance of statistical measures while underestimating the potential of semantic measures.", "labels": [], "entities": []}, {"text": "In order to investigate this issue, we present a framework for mining naturally occurring errors and their contexts from the Wikipedia revision history.", "labels": [], "entities": [{"text": "Wikipedia revision history", "start_pos": 125, "end_pos": 151, "type": "DATASET", "confidence": 0.8597137133280436}]}, {"text": "We use the resulting English and German datasets to evaluate statistical and knowledge-based measures.", "labels": [], "entities": []}, {"text": "We make the full experimental framework publicly available 2 which will allow reproducing our experiments as well as conducting follow-up experiments.", "labels": [], "entities": []}, {"text": "The framework contains (i) methods to extract natural errors from Wikipedia, (ii) reference implementations of the knowledge-based and the statistical methods, and (iii) the evaluation datasets described in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using our framework for mining real-word spelling errors in context, we extracted an English dataset , and a German dataset . Although the output generally was of high quality, manual post-processing was necessary 7 , as (i) for some pairs the available context did not provide enough information to decide which form was correct, and (ii) a problem that might be specific to Wikipedia -vandalism.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.846710741519928}, {"text": "German dataset", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.8329262137413025}]}, {"text": "The revisions are full of cases where words are replaced with similar sounding but greasy alternatives.", "labels": [], "entities": []}, {"text": "A relatively mild example is \"In romantic comedies, there is a love story about a man and a woman who fall in love, along with silly or funny comedy farts.\", where 'parts' was replaced with 'farts' only to be changed back shortly afterwards by a Wikipedia vandalism hunter.", "labels": [], "entities": []}, {"text": "We removed all cases that resulted from obvious vandalism.", "labels": [], "entities": []}, {"text": "For further experiments, a small list of offensive terms could be added to the stopword list to facilitate this process.", "labels": [], "entities": []}, {"text": "A connected problem is correct words that get falsely corrected by Wikipedia editors (without the malicious intend from the previous examples, but with similar consequences).", "labels": [], "entities": []}, {"text": "For example, the initially correct sentence \"Dung beetles roll it into a ball, sometimes being up to 50 times their own weight.\" was 'corrected' by exchanging weight with wait.", "labels": [], "entities": []}, {"text": "We manually removed such obvious mistakes, but are still left with some borderline cases.", "labels": [], "entities": []}, {"text": "In the sentence \"By the 1780s the goals of England were so full that convicts were often chained up in rotting old ships.\" the obvious error 'goal' was changed by some Wikipedia editor to 'jail'.", "labels": [], "entities": [{"text": "England", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9328983426094055}]}, {"text": "However, actually it should have been the old English form for jail 'gaol' which can be deduced when looking at the full context and later versions of the article.", "labels": [], "entities": []}, {"text": "We decided to not remove these rare cases, because 'jail' is a valid correction in this context.", "labels": [], "entities": []}, {"text": "After manual inspection, we are left with 466 English and 200 German errors.", "labels": [], "entities": []}, {"text": "Given that we restricted our experiment to 5 million English and German revisions, much larger datasets can be extracted if the whole revision history is taken into account.", "labels": [], "entities": []}, {"text": "Our snapshot of the English Wikipedia contains 305\u00b710 6 revisions.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.880322128534317}]}, {"text": "Even if not all of them correspond to article revisions, it is safe to assume that more than 10,000 real-word spelling errors can be extracted from this version of Wikipedia.", "labels": [], "entities": []}, {"text": "Using the same amount of source revisions, we found significantly more English than German errors.", "labels": [], "entities": []}, {"text": "This might be due to (i) English having more short nouns or verbs than German that are more likely to be confused with each other, and (ii) the English Wikipedia being known to attract a larger amount of non-native editors which might lead to higher rates of real-word spelling errors.", "labels": [], "entities": []}, {"text": "However, this issue needs to be further investigated e.g. based on comparable corpora build on the basis of different language editions of Wikipedia.", "labels": [], "entities": []}, {"text": "Further refining the identification of real-word errors in Wikipedia would allow evaluating how frequent such errors actually occur, and how long it takes the Wikipedia editors to detect them.", "labels": [], "entities": [{"text": "identification of real-word errors in Wikipedia", "start_pos": 21, "end_pos": 68, "type": "TASK", "confidence": 0.7701143523057302}]}, {"text": "If errors persist over along time, using measures of contextual fitness for detection would be even more important.", "labels": [], "entities": []}, {"text": "Another interesting observation is that the average edit distance is around 1.4 for both datasets.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9243265986442566}]}, {"text": "This means that a substantial proportion of errors involve more than one edit operation.", "labels": [], "entities": []}, {"text": "Given that many measures of contextual fitness allow at most one edit, many naturally occurring errors will not be detected.", "labels": [], "entities": []}, {"text": "However, allowing a larger edit distance enormously increases the search space resulting in increased run-time and possibly decreased detection precision due to more false positives.", "labels": [], "entities": [{"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9243150949478149}]}, {"text": "In contrast to the quite challenging process of mining naturally occurring errors, creating artificial errors is relatively straightforward.", "labels": [], "entities": []}, {"text": "From a corpus that is known to be free of spelling errors, sentences are randomly sampled.", "labels": [], "entities": []}, {"text": "For each sentence, a random word is selected and all strings with edit distance smaller than a given threshold (2 in our case) are generated.", "labels": [], "entities": []}, {"text": "If one of those generated strings is a known word from the vocabulary, it is picked as the artificial error.", "labels": [], "entities": []}, {"text": "Previous work on evaluating real-word spelling correction) used a dataset sampled from the Wall Street Journal corpus which is not freely available.", "labels": [], "entities": [{"text": "real-word spelling correction", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.7679222027460734}, {"text": "Wall Street Journal corpus", "start_pos": 91, "end_pos": 117, "type": "DATASET", "confidence": 0.9791967868804932}]}, {"text": "Thus, we created a comparable English dataset of 1,000 artificial errors based on the easily available Brown corpus (Francis W..", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.8798831701278687}]}, {"text": "Additionally, we created a German dataset with 1,000 artificial errors based on the TIGER corpus.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8781021535396576}, {"text": "TIGER corpus", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9447113573551178}]}], "tableCaptions": [{"text": " Table 1: Performance of the statistical approach using  a trigram model based on Google Web1T.", "labels": [], "entities": [{"text": "Google Web1T", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.8179470598697662}]}, {"text": " Table 2: Influence of the n-gram model on the perfor- mance of the statistical approach.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the knowledge-based ap- proach using the JiangConrath semantic relatedness  measure.", "labels": [], "entities": [{"text": "JiangConrath semantic relatedness", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.7122728625933329}]}, {"text": " Table 4: Performance of knowledge-based approach  using different relatedness measures.", "labels": [], "entities": []}, {"text": " Table 5: Results obtained by a combination of the best  statistical and knowledge-based configuration. 'Best- Single' is the best precision or recall obtained by a sin- gle measure. 'Union' merges the detections of both  approaches. 'Intersection' only detects an error if both  methods agree on a detection.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9981162548065186}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.8866641521453857}]}]}