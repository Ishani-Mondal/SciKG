{"title": [], "abstractContent": [{"text": "Low interannotator agreement (IAA) is a well-known issue in manual semantic tagging (sense tagging).", "labels": [], "entities": [{"text": "interannotator agreement (IAA)", "start_pos": 4, "end_pos": 34, "type": "METRIC", "confidence": 0.7597519159317017}, {"text": "manual semantic tagging (sense tagging)", "start_pos": 60, "end_pos": 99, "type": "TASK", "confidence": 0.6924827737467629}]}, {"text": "IAA correlates with the granularity of word senses and they both correlate with the amount of information they give as well as with its reliability.", "labels": [], "entities": [{"text": "IAA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7344021201133728}, {"text": "reliability", "start_pos": 136, "end_pos": 147, "type": "METRIC", "confidence": 0.9893507361412048}]}, {"text": "We compare different approaches to semantic tagging in WordNet, FrameNet, Prop-Bank and OntoNotes with a small tagged data sample based on the Corpus Pattern Analysis to present the reliable information gain (RG), a measure used to optimize the semantic granularity of a sense inventory with respect to its reliability indicated by the IAA in the given data set.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7163323909044266}, {"text": "WordNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9563170075416565}, {"text": "reliable information gain (RG)", "start_pos": 182, "end_pos": 212, "type": "METRIC", "confidence": 0.8482037633657455}, {"text": "IAA in the given data set", "start_pos": 336, "end_pos": 361, "type": "DATASET", "confidence": 0.6797612657149633}]}, {"text": "RG can also be used as feedback for lexicographers, and as a supporting component of automatic semantic classifiers, especially when dealing with a very fine-grained set of semantic categories .", "labels": [], "entities": []}], "introductionContent": [{"text": "The term semantic tagging is used in two divergent areas: 1) recognizing objects of semantic importance, such as entities, events and polarity, often tailored to a restricted domain, or 2) relating occurrences of words in a corpus to a lexicon and selecting the most appropriate semantic categories (such as synsets, semantic frames, wordsenses, semantic patterns or framesets).", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7375177443027496}, {"text": "relating occurrences of words in a corpus to a lexicon and selecting the most appropriate semantic categories (such as synsets, semantic frames, wordsenses, semantic patterns or framesets)", "start_pos": 189, "end_pos": 377, "type": "Description", "confidence": 0.6955417152494192}]}, {"text": "We are concerned with the second case, which seeks to make lexical semantics tractable for computers.", "labels": [], "entities": []}, {"text": "Lexical semantics, as opposed to propositional semantics, focuses the meaning of lexical items.", "labels": [], "entities": [{"text": "Lexical semantics", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8494679629802704}]}, {"text": "The disciplines that focus lexical semantics are lexicology and lexicography rather than logic.", "labels": [], "entities": []}, {"text": "By semantic tagging we mean a process of assigning semantic categories to target words in given contexts.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7180977761745453}]}, {"text": "This process can be either manual or automatic.", "labels": [], "entities": []}, {"text": "Traditionally, semantic tagging relies on the tacit assumption that various uses of polysemous words can be sorted into discrete senses; understanding or using an unfamiliar word be then like looking it up in a dictionary.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8255811631679535}]}, {"text": "When building a dictionary entry fora given word, the lexicographer sorts a number of its occurrences into discrete senses present (or emerging) in his/her mental lexicon, which is supposed to be shared by all speakers of the same language.", "labels": [], "entities": []}, {"text": "The assumed common mental representation of a words meaning should make it easy for other humans to assign random occurrences of the word to one of the pre-defined senses (.", "labels": [], "entities": []}, {"text": "This assumption seems to be falsified by the interannotator agreement (IAA, sometimes ITA) constantly reported much lower in semantic than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which IAA measure indicates a reliable annotation.", "labels": [], "entities": [{"text": "interannotator agreement (IAA, sometimes ITA)", "start_pos": 45, "end_pos": 90, "type": "METRIC", "confidence": 0.6260954849421978}]}, {"text": "In some projects (e.g. OntoNotes ()), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see).", "labels": [], "entities": []}, {"text": "Consequently, using different measures for IAA makes the reported IAA values incomparable across different projects.", "labels": [], "entities": [{"text": "IAA", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8664335608482361}, {"text": "IAA", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.6922991871833801}]}, {"text": "Even skilled lexicographers have trouble selecting one discrete sense fora concordance, and, more to say, when the tagging performance of lexicographers and ordinary annotators (students) was compared, the experiment showed that the mental representations of a word's semantics differ for each group (, and cf.).", "labels": [], "entities": []}, {"text": "Lexicographers are trained in considering subtle differences among various uses of a word, which ordinary language users do not reflect.", "labels": [], "entities": []}, {"text": "Identifying a semantic difference between uses of a word and deciding whether a difference is important enough to constitute a separate sense means presenting a word with a certain degree of semantic granularity.", "labels": [], "entities": []}, {"text": "Intuitively, the finer the granularity of a word entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected.", "labels": [], "entities": [{"text": "IAA", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9537439346313477}]}, {"text": "Also, the annotators are less confident in their decisions, when they have many options to choose from  reported a drop in subjective annotators confidence in words with 8+ senses).", "labels": [], "entities": []}, {"text": "Despite all the known issues in semantic tagging, the major lexical resources),,) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus ().", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7020741552114487}]}, {"text": "More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses anymore, e.g. in distributional semantics and recognizing textual entailment (e.g. ( and).", "labels": [], "entities": [{"text": "WSD", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9655010104179382}, {"text": "semantic labeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6471426635980606}]}, {"text": "It is a remarkable fact that, to the best of our knowledge, there is no measure that would relate granularity, reliability of the annotation (derived from IAA) and the resulting information gain.", "labels": [], "entities": [{"text": "reliability", "start_pos": 111, "end_pos": 122, "type": "METRIC", "confidence": 0.9973002076148987}]}, {"text": "Therefore it is impossible to say where the optimum for granularity and IAA lies.", "labels": [], "entities": [{"text": "IAA", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9968113303184509}]}, {"text": "2 Approaches to semantic tagging 2.1 Semantic tagging vs. morphological or syntactic analysis Manual semantic tagging is in many respects similar to morphological tagging and syntactic analysis: human annotators are trained to sort certain elements occurring in a running text according to a reference source.", "labels": [], "entities": [{"text": "semantic tagging 2.1 Semantic tagging", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.7586838364601135}, {"text": "syntactic analysis Manual semantic tagging", "start_pos": 75, "end_pos": 117, "type": "TASK", "confidence": 0.6574495494365692}, {"text": "syntactic analysis", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.7145546227693558}]}, {"text": "There is, nevertheless, a substantial difference: whereas morphologically or syntactically annotated data exist separately from the reference (tagset, annotation guide, annotation scheme), a semantically tagged resource can be regarded both as a corpus of texts disambiguated according to an attached inventory of semantic categories and as a lexicon with links to example concordances for each semantic category.", "labels": [], "entities": []}, {"text": "So, in semantically tagged resources, the data and the reference are intertwined.", "labels": [], "entities": []}, {"text": "Such double-faced semantic resources have also been called semantic concordances ().", "labels": [], "entities": []}, {"text": "For instance, one of the earlier versions of WordNet, the largest lexical resource for English, was used in the semantic concordance).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9345762729644775}]}, {"text": "More recent lexical resources have been built as semantic concordances from the very beginning), OntoNotes word senses).", "labels": [], "entities": []}, {"text": "In morphological or syntactic annotation, the tagset or inventory of constituents are given beforehand and are supposed to hold for all tokens/sentences contained in the corpus.", "labels": [], "entities": []}, {"text": "Problematic and theory-dependent issues are few and mostly well-known in advance.", "labels": [], "entities": []}, {"text": "Therefore they can be reflected by a few additional conventions in the annotation manual (e.g. whereto draw the line between particles and prepositions or between adjectives and verbs in past participles or whereto attach a prepositional phrase following a noun phrase and how to treat specific \"financialspeak\" structures ().", "labels": [], "entities": []}, {"text": "Even in difficult cases, there are hardly more than two options of interpretation.", "labels": [], "entities": []}, {"text": "Data manually annotated for morphology or surface syntax are reliable enough to train syntactic parsers with an accuracy above 80 % (e.g. ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9990058541297913}]}, {"text": "On the other hand, semantic tagging actually employs a different tagset for each word lemma.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7956370115280151}]}, {"text": "Even within the same part of speech, individual words require individual descriptions.", "labels": [], "entities": []}, {"text": "Possible similarities among them come into relief ex post rather than that they could be imposed on the lexicographers from the beginning.", "labels": [], "entities": []}, {"text": "When assigning senses to concordances, the annotator often has to select among more than two relevant options.", "labels": [], "entities": []}, {"text": "These two aspects make achieving good IAA much harder than in morphology and syn-tax tasks.", "labels": [], "entities": [{"text": "IAA", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.956259548664093}]}, {"text": "In addition, while a linguistically educated annotator can have roughly the same idea of parts of speech as the author of the tagset, there is no chance that two humans (not even two professional lexicographers) would create identical entries for e.g. a polysemous verb.", "labels": [], "entities": []}, {"text": "Any human evaluation of complete entries would be subjective.", "labels": [], "entities": []}, {"text": "The maximum to be achieved is that the entry reflects the corpus data in a reasonable granular way on which annotators still can reach reasonable IAA.", "labels": [], "entities": [{"text": "IAA", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.8456436991691589}]}], "datasetContent": [{"text": "An automatic classifier is considered to be a function c that-the same way as annotators-assigns tags to instances s \u2208 S, so that c(s) = {t}, t \u2208 T . The traditional way to evaluate the accuracy of an automatic classifier means to compare its output with the correct semantic tags on a Gold Standard (GS) dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9953832030296326}, {"text": "Gold Standard (GS) dataset", "start_pos": 286, "end_pos": 312, "type": "DATASET", "confidence": 0.7099179079135259}]}, {"text": "Within our formal framework, we can imagine that we have a \"gold\" annotator Ag , so that the GS dataset is represented by Ag (s 1 ), . .", "labels": [], "entities": [{"text": "GS dataset", "start_pos": 93, "end_pos": 103, "type": "DATASET", "confidence": 0.8975785970687866}]}, {"text": ", Ag (s r ).", "labels": [], "entities": [{"text": "Ag", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9963104128837585}]}, {"text": "Then the classic accuracy score can be computed as 1 However, that approach does not take into consideration the fact that some semantic tags are quite confusing even for human annotators.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9595861434936523}]}, {"text": "In our opinion, automatic classifier should not be penalized for mistakes that would be made even by humans.", "labels": [], "entities": []}, {"text": "So we propose a more complex evaluation score using the knowledge of the expected tagging confusion stored in CPM.", "labels": [], "entities": [{"text": "CPM", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.8220853805541992}]}, {"text": "Definition: Classifier evaluation Score with respect to tagging confusion is defined as the proportion Score(c) = S(c)/S max , where gives an illustration of the fact that using different \u03b1 values one can get different re-sults when comparing tagging accuracy for different words (a classifier based on bag-of-words approach was used).", "labels": [], "entities": [{"text": "proportion Score(c) = S(c)/S max", "start_pos": 92, "end_pos": 124, "type": "METRIC", "confidence": 0.7269331514835358}]}, {"text": "The same holds true for comparison of different classifiers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. The  corresponding confusion matrices are shown in", "labels": [], "entities": []}, {"text": " Table 2: Aggregated Confusion Matrix.", "labels": [], "entities": []}, {"text": " Table 3: Example of all confusion matrices for the target word submit and three annotators.", "labels": [], "entities": []}, {"text": " Table 4: Example of Confusion Probability Matrix.", "labels": [], "entities": []}, {"text": " Table 5: Frequency and Reliable Gain of tags.", "labels": [], "entities": [{"text": "Reliable Gain", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.8390572965145111}]}, {"text": " Table 6: Aggregated Confusion Matrix after merging.", "labels": [], "entities": []}, {"text": " Table 7: Confusion Probability Matrix after merging.", "labels": [], "entities": []}, {"text": " Table 8: Evaluation with different \u03b1 values.", "labels": [], "entities": []}]}