{"title": [{"text": "MaltOptimizer: An Optimization Tool for MaltParser", "labels": [], "entities": []}], "abstractContent": [{"text": "Data-driven systems for natural language processing have the advantage that they can easily be ported to any language or domain for which appropriate training data can be found.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6679966251055399}]}, {"text": "However, many data-driven systems require careful tuning in order to achieve optimal performance, which may require specialized knowledge of the system.", "labels": [], "entities": []}, {"text": "We present MaltOptimizer, a tool developed to facilitate optimization of parsers developed using MaltParser, a data-driven dependency parser generator.", "labels": [], "entities": []}, {"text": "MaltOptimizer performs an analysis of the training data and guides the user through a three-phase optimization process, but it can also be used to perform completely automatic optimization.", "labels": [], "entities": []}, {"text": "Experiments show that MaltOptimizer can improve parsing accuracy by up to 9 percent absolute (labeled attachment score) compared to default settings.", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9654751420021057}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.934526801109314}, {"text": "labeled attachment score)", "start_pos": 94, "end_pos": 119, "type": "METRIC", "confidence": 0.8188390880823135}]}, {"text": "During the demo session, we will run MaltOptimizer on different data sets (user-supplied if possible) and show how the user can interact with the system and track the improvement in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 182, "end_pos": 189, "type": "TASK", "confidence": 0.9680091142654419}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.8498981595039368}]}], "introductionContent": [{"text": "In building NLP applications for new languages and domains, we often want to reuse components for tasks like part-of-speech tagging, syntactic parsing, word sense disambiguation and semantic role labeling.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7287830412387848}, {"text": "syntactic parsing", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.727682814002037}, {"text": "word sense disambiguation", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.6827364762624105}, {"text": "semantic role labeling", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.6574609279632568}]}, {"text": "From this perspective, components that rely on machine learning have an advantage, since they can be quickly adapted to new settings provided that we can find suitable training data.", "labels": [], "entities": []}, {"text": "However, such components may require careful feature selection and parameter tuning in order to give optimal performance, a task that can be difficult for application developers without specialized knowledge of each component.", "labels": [], "entities": []}, {"text": "A typical example is MaltParser (), a widely used transition-based dependency parser with state-of-the-art performance for many languages, as demonstrated in the CoNLL shared tasks on multilingual dependency parsing.", "labels": [], "entities": [{"text": "transition-based dependency parser", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.616214781999588}, {"text": "multilingual dependency parsing", "start_pos": 184, "end_pos": 215, "type": "TASK", "confidence": 0.5515200098355612}]}, {"text": "MaltParser is an open-source system that offers a wide range of parameters for optimization.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9222447276115417}]}, {"text": "It implements nine different transition-based parsing algorithms, each with its own specific parameters, and it has an expressive specification language that allows the user to define arbitrarily complex feature models.", "labels": [], "entities": []}, {"text": "Finally, any combination of parsing algorithm and feature model can be combined with a number of different machine learning algorithms available in LIBSVM) and LIBLINEAR.", "labels": [], "entities": [{"text": "LIBSVM", "start_pos": 148, "end_pos": 154, "type": "DATASET", "confidence": 0.8189703822135925}]}, {"text": "Just running the system with default settings when training anew parser is therefore very likely to result in suboptimal performance.", "labels": [], "entities": []}, {"text": "However, selecting the best combination of parameters is a complicated task that requires knowledge of the system as well as knowledge of the characteristics of the training data.", "labels": [], "entities": []}, {"text": "This is why we present MaltOptimizer, a tool for optimizing MaltParser fora new language or domain, based on an analysis of the training data.", "labels": [], "entities": []}, {"text": "The optimization is performed in three phases: data analysis, parsing algorithm selection, and feature selection.", "labels": [], "entities": [{"text": "data analysis", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7312119603157043}, {"text": "parsing algorithm selection", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.9157924254735311}, {"text": "feature selection", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.6953866481781006}]}, {"text": "The tool can be run in \"batch mode\" to perform completely automatic optimization, but it is also possible for the user to manually tune parameters after each of the three phases.", "labels": [], "entities": []}, {"text": "In this way, we hope to cater for users without specific knowledge of MaltParser, who can use the tool for black box optimization, as well as expert users, who can use it interactively to speedup optimization.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9339907169342041}, {"text": "black box optimization", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.657697339852651}]}, {"text": "Experiments on a number of data sets show that using MaltOptimizer for completely automatic optimization gives consistent and often substantial improvements over the default settings for MaltParser.", "labels": [], "entities": []}, {"text": "The importance of feature selection and parameter optimization has been demonstrated for many NLP tasks (, and there are general optimization tools for machine learning, such as Paramsearch ().", "labels": [], "entities": [{"text": "feature selection", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7017078548669815}, {"text": "parameter optimization", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7941109538078308}]}, {"text": "In addition, has explored automatic feature selection specifically for MaltParser, but MaltOptimizer is the first system that implements a complete customized optimization process for this system.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we describe the optimization process implemented in MaltOptimizer (Section 2), report experiments (Section 3), outline the demonstration (Section 4), and conclude (Section 5).", "labels": [], "entities": []}, {"text": "A more detailed description of MaltOptimizer with additional experimental results can be found in Ballesteros and Nivre (2012).", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to assess the usefulness and validity of the optimization procedure, we have run all three phases of the optimization on all the 13 data sets from the CoNLL-X shared task on multilingual dependency parsing ().", "labels": [], "entities": [{"text": "validity", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9768494963645935}, {"text": "multilingual dependency parsing", "start_pos": 183, "end_pos": 214, "type": "TASK", "confidence": 0.5550637443860372}]}, {"text": "shows the labeled attachment scores with default settings and after each of the three optimization phases, as well as the difference between the final configuration and the default.", "labels": [], "entities": []}, {"text": "The first thing to note is that the optimization improves parsing accuracy for all languages without exception, although the amount of improvement varies considerably from about 1 percentage point for Chinese, Japanese and Swedish to 8-9 points for Dutch, Czech and Turkish.", "labels": [], "entities": [{"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.974763810634613}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9620294570922852}]}, {"text": "For most languages, the greatest improvement comes from feature selection in phase 3, but we also see sig-nificant improvement from phase 2 for languages with a substantial amount of non-projective dependencies, such as Czech, Dutch and Slovene, where the selection of parsing algorithm can be very important.", "labels": [], "entities": [{"text": "parsing", "start_pos": 269, "end_pos": 276, "type": "TASK", "confidence": 0.937453031539917}]}, {"text": "The time needed to run the optimization varies from about half an hour for the smaller data sets to about one day for very large data sets like the one for Czech.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Labeled attachment score per phase and with  comparison to default settings for the 13 training sets  from the CoNLL-X shared task (Buchholz and Marsi,  2006).", "labels": [], "entities": [{"text": "Labeled attachment score", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.664847751458486}, {"text": "CoNLL-X shared task", "start_pos": 121, "end_pos": 140, "type": "DATASET", "confidence": 0.7217372059822083}]}]}