{"title": [{"text": "Compensating for Annotation Errors in Training a Relation Extractor", "labels": [], "entities": []}], "abstractContent": [{"text": "The well-studied supervised Relation Extraction algorithms require training data that is accurate and has good coverage.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9342519640922546}]}, {"text": "To obtain such a gold standard, the common practice is to do independent double annotation followed by adjudication.", "labels": [], "entities": []}, {"text": "This takes significantly more human effort than annotation done by a single annotator.", "labels": [], "entities": []}, {"text": "We do a detailed analysis on a snapshot of the ACE 2005 annotation files to understand the differences between single-pass annotation and the more expensive nearly three-pass process, and then propose an algorithm that learns from the much cheaper single-pass annotation and achieves a performance on a par with the extractor trained on multi-pass annotated data.", "labels": [], "entities": [{"text": "ACE 2005 annotation files", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.9747958779335022}]}, {"text": "Furthermore, we show that given the same amount of human labor, the better way to do relation annotation is not to annotate with high-cost quality assurance, but to annotate more.", "labels": [], "entities": [{"text": "relation annotation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8540350496768951}]}], "introductionContent": [{"text": "Relation Extraction aims at detecting and categorizing semantic relations between pairs of entities in text.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.936408519744873}]}, {"text": "It is an important NLP task that has many practical applications such as answering factoid questions, building knowledge bases and improving web search.", "labels": [], "entities": [{"text": "answering factoid questions", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.8806727528572083}]}, {"text": "Supervised methods for relation extraction have been studied extensively since rich annotated linguistic resources, e.g. the Automatic Content Extraction 1 (ACE) training corpus, were released.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.9748912155628204}]}, {"text": "We will give a summary of related methods in section 2.", "labels": [], "entities": []}, {"text": "Those methods rely on accurate and complete annotation.", "labels": [], "entities": []}, {"text": "To obtain high quality annotation, the common wisdom is to let two annotators independently annotate a corpus, and then asking a senior annotator to adjudicate the disagreements 2 . This annotation procedure roughly requires 3 passes 3 over the same corpus.", "labels": [], "entities": []}, {"text": "Therefore it is very expensive.", "labels": [], "entities": []}, {"text": "The ACE 2005 annotation on relations is conducted in this way.", "labels": [], "entities": [{"text": "ACE 2005 annotation", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9580628673235575}]}, {"text": "In this paper, we analyzed a snapshot of ACE training data and found that each annotator missed a significant fraction of relation mentions and annotated some spurious ones.", "labels": [], "entities": [{"text": "ACE training data", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.6562687357266744}]}, {"text": "We found that it is possible to separate most missing examples from the vast majority of true-negative unlabeled examples, and in contrast, most of the relation mentions that are adjudicated as incorrect contain useful expressions for learning a relation extractor.", "labels": [], "entities": []}, {"text": "Based on this observation, we propose an algorithm that purifies negative examples and applies transductive inference to utilize missing examples during the training process on the single-pass annotation.", "labels": [], "entities": []}, {"text": "Results show that the extractor trained on single-pass annotation with the proposed algorithm has a performance that is close to an extractor trained on the 3-pass annotation.", "labels": [], "entities": []}, {"text": "We further show that the proposed algorithm trained on a single-pass annotation on the complete set of documents has a higher performance than an extractor trained on 3-pass annotation on 90% of the documents in the same corpus, although the effort of doing a single-pass annotation over the entire set costs less than half that of doing 3 passes over 90% of the documents.", "labels": [], "entities": []}, {"text": "From the perspective of learning a high-performance relation extractor, it suggests that a better way to do relation annotation is not to annotate with a high-cost quality assurance, but to annotate more.", "labels": [], "entities": [{"text": "relation annotation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7454655766487122}]}], "datasetContent": [{"text": "Experiments were conducted over the same set of documents on which we did analysis: the 511 documents which have completed annotation in all of the fp1, fp2 and adj from the ACE 2005 Multilingual Training Data V3.0.", "labels": [], "entities": [{"text": "ACE 2005 Multilingual Training Data V3.0", "start_pos": 174, "end_pos": 214, "type": "DATASET", "confidence": 0.9210065404574076}]}, {"text": "To reemphasize, we apply the hierarchical learning scheme and we focus on improving relation detection while keeping relation classification unchanged (results show that its performance is improved because of the improved detection).", "labels": [], "entities": [{"text": "relation detection", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8335545659065247}, {"text": "relation classification", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.752221405506134}]}, {"text": "We use SVM as our learning algorithm with the full feature set from.", "labels": [], "entities": []}, {"text": "Baseline algorithm: The relation detector is unchanged.", "labels": [], "entities": []}, {"text": "We follow the common practice, which is to use annotated examples as positive ones and all possible untagged relation mentions as negative ones.", "labels": [], "entities": []}, {"text": "We sub-sampled the negative data by \u00bd since that shows better performance.", "labels": [], "entities": []}, {"text": "+purify: This algorithm adds an additional purification preprocessing step (section 4.2) before the hierarchical learning RDC algorithm.", "labels": [], "entities": []}, {"text": "After purification, the RDC algorithm is trained on the positive examples and purified negative examples.", "labels": [], "entities": []}, {"text": "We set N=2000 12 in all experiments.", "labels": [], "entities": []}, {"text": "We included this large random sample so that the balance of positive to negative examples in the unlabeled set would be similar to that of the labeled data.", "labels": [], "entities": []}, {"text": "The test data is not included in the unlabeled set.", "labels": [], "entities": []}, {"text": "We choose 2000 because it is close to the number of relations missed from each single-pass annotation.", "labels": [], "entities": []}, {"text": "In practice, it contains more than 70% of the false negatives, and it is less than 10% of the unannotated examples.", "labels": [], "entities": []}, {"text": "To estimate how many examples are missing (section 3.4), one +tSVM: First, the same purification process of +purify is applied.", "labels": [], "entities": []}, {"text": "Then we follow the steps described in section 4.3 to construct the set of unlabeled examples, and set all the rest of purified negative examples to be negative.", "labels": [], "entities": []}, {"text": "Finally, we train TSVM on both labeled and unlabeled data and replace the relation detection in the RDC algorithm.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.785819798707962}]}, {"text": "The relation classification is unchanged.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8957895636558533}]}, {"text": "All experiments are done with 5-fold cross validation 13 using testing data from adj.", "labels": [], "entities": []}, {"text": "The first three rows show experiments trained on fp1, and the last row (ADJ) shows the unmodified RDC algorithm trained on adj for comparison.", "labels": [], "entities": []}, {"text": "The purification of negative examples shows significant performance gain, 3.7% F1 on relation detection and 3.4% on relation classification.", "labels": [], "entities": [{"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9998466968536377}, {"text": "relation detection", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8697001338005066}, {"text": "relation classification", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.8716135919094086}]}, {"text": "The precision decreases but recall increases substantially since the missing examples are not treated as negatives.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996585845947266}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9997414946556091}]}, {"text": "Experiment shows that the purification process removes more than 60% of the false negatives.", "labels": [], "entities": []}, {"text": "Transductive SVM further improved performance by a relatively small margin.", "labels": [], "entities": []}, {"text": "This shows that the latent positive examples can help refine the model.", "labels": [], "entities": []}, {"text": "Results also show that transductive inference can find around 17% of missing relation mentions.", "labels": [], "entities": []}, {"text": "We notice that the performance of relation classification is improved since by improving relation detection, some examples that do not express a relation are removed.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8204587399959564}, {"text": "relation detection", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8443843126296997}]}, {"text": "The classification performance on single-pass annotation is close to the one trained on adj due to the help from a better relation detector trained with our algorithm.", "labels": [], "entities": []}, {"text": "We also did 5-fold cross validation with a model trained on a fraction of the 4/5 (4 folds) of adj data (each experiment shown in table 4 uses 4 folds of adj documents for training since one fold is left for cross validation).", "labels": [], "entities": [{"text": "cross validation", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.6611471176147461}, {"text": "cross validation", "start_pos": 208, "end_pos": 224, "type": "TASK", "confidence": 0.7167105078697205}]}, {"text": "The documents are sampled randomly.", "labels": [], "entities": []}, {"text": "shows results for varying training data size.", "labels": [], "entities": []}, {"text": "Compared to the results shown in the \"+tSVM\" row of table 3, we can see that our best model trained on single-pass annotation outperforms SVM trained on 90% of the dual-pass, adjudicated data in both relation detection and classification, although it costs less than half the 3-pass annotation.", "labels": [], "entities": [{"text": "relation detection and classification", "start_pos": 200, "end_pos": 237, "type": "TASK", "confidence": 0.7340480387210846}]}, {"text": "This suggests that given the same amount of human effort for should perform multiple passes of independent annotation on a small dataset and measure inter-annotator agreements.", "labels": [], "entities": []}, {"text": "relation annotation, annotating more documents with single-pass offers advantages over annotating less data with high quality assurance (dual passes and adjudication).", "labels": [], "entities": [{"text": "relation annotation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7758914828300476}]}, {"text": "studied WSD annotation from a cost-effectiveness viewpoint.", "labels": [], "entities": [{"text": "WSD annotation", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.9421395063400269}]}, {"text": "They showed empirically that, with same amount of annotation dollars spent, single-annotation is better than dual-annotation and adjudication.", "labels": [], "entities": []}, {"text": "The common practice for quality control of WSD annotation is similar to Relation annotation.", "labels": [], "entities": [{"text": "WSD annotation", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.900048553943634}, {"text": "Relation annotation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.5538244247436523}]}, {"text": "However, the task of WSD annotation is very different from relation annotation.", "labels": [], "entities": [{"text": "WSD annotation", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9431976675987244}, {"text": "relation annotation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8162823021411896}]}, {"text": "WSD requires that every example must be assigned some tag, whereas that is not required for relation tagging.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7494396567344666}, {"text": "relation tagging", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.811077356338501}]}, {"text": "Moreover, relation tagging requires identifying two arguments and correctly categorizing their types.", "labels": [], "entities": [{"text": "relation tagging", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9319570660591125}]}], "tableCaptions": [{"text": " Table 2. Performance of RDC trained on fp1/fp2/adj, and tested on adj.", "labels": [], "entities": [{"text": "RDC", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.7873413562774658}]}, {"text": " Table 3. 5-fold cross-validation results. All are trained on fp1 (except the last row showing the unchanged algorithm trained  on adj for comparison), and tested on adj. McNemar's test show that the improvement from +purify to +tSVM, and from  +tSVM to ADJ are statistically significant (with p<0.05).", "labels": [], "entities": []}, {"text": " Table 4. Performance with SVM trained on a fraction of adj. It shows 5 fold cross validation results.", "labels": [], "entities": []}]}