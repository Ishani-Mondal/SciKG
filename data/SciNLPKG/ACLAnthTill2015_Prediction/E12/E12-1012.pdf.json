{"title": [{"text": "Adaptation of Statistical Machine Translation Model for Cross-Lingual Information Retrieval in a Service Context", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.7050907015800476}, {"text": "Cross-Lingual Information Retrieval in a Service Context", "start_pos": 56, "end_pos": 112, "type": "TASK", "confidence": 0.7227701076439449}]}], "abstractContent": [{"text": "This work proposes to adapt an existing general SMT model for the task of translating queries that are subsequently going to be used to retrieve information from a target language collection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9903871417045593}]}, {"text": "In the scenario that we focus on access to the document collection itself is not available and changes to the IR model are not possible.", "labels": [], "entities": [{"text": "IR", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.8924010396003723}]}, {"text": "We propose two ways to achieve the adaptation effect and both of them are aimed at tuning parameter weights on a set of parallel queries.", "labels": [], "entities": []}, {"text": "The first approach is via a standard tuning procedure optimizing for BLEU score and the second one is via a reranking approach optimizing for MAP score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9829275608062744}, {"text": "MAP score", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.6202532649040222}]}, {"text": "We also extend the second approach by using syntax-based features.", "labels": [], "entities": []}, {"text": "Our experiments show improvements of 1-2.5 in terms of MAP score over the retrieval with the non-adapted translation.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.947454571723938}]}, {"text": "We show that these improvements are due both to the integration of the adaptation and syntax-features for the query translation task.", "labels": [], "entities": [{"text": "query translation task", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7965073784192404}]}], "introductionContent": [{"text": "Cross Lingual Information Retrieval (CLIR) is an important feature for any digital content provider in today's multilingual environment.", "labels": [], "entities": [{"text": "Cross Lingual Information Retrieval (CLIR)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7359786544527326}]}, {"text": "However, many of the content providers are not willing to change existing well-established document indexing and search tools, nor to provide access to their document collection by a third-party external service.", "labels": [], "entities": []}, {"text": "The work presented in this paper assumes such a context of use, where a query translation service allows translating queries posed to the search engine of a content provider into several target languages, without requiring changes to the undelying IR system used and without accessing, at translation time, the content provider's document set.", "labels": [], "entities": [{"text": "query translation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7763040959835052}]}, {"text": "Keeping in mind these constraints, we present two approaches on query translation optimisation.", "labels": [], "entities": [{"text": "query translation optimisation", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.839499851067861}]}, {"text": "One of the important observations done during the CLEF 2009 campaign) related to CLIR was that the usage of Statistical Machine Translation (SMT) systems (eg. Google Translate) for query translation led to important improvements in the cross-lingual retrieval performance (the best CLIR performance increased from\u02dc55%from\u02dc55% of the monolingual baseline in 2008 to more than 90% in 2009 for French and German target languages).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.768806204199791}, {"text": "query translation", "start_pos": 181, "end_pos": 198, "type": "TASK", "confidence": 0.7313668876886368}]}, {"text": "However, generalpurpose SMT systems are not necessarily adapted for query translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9487076997756958}, {"text": "query translation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7941337823867798}]}, {"text": "That is because SMT systems trained on a corpus of standard parallel phrases take into account the phrase structure implicitly.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9918550252914429}]}, {"text": "The structure of queries is very different from the standard phrase structure: queries are very short and the word order might be different than the typical full phrase one.", "labels": [], "entities": []}, {"text": "This problem can be seen as a problem of genre adaptation for SMT, where the genre is \"query\".", "labels": [], "entities": [{"text": "genre adaptation", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7901883125305176}, {"text": "SMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9772895574569702}]}, {"text": "To our knowledge, no suitable corpora of parallel queries is available to train an adapted SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9934102892875671}]}, {"text": "Small corpora of parallel queries however can be obtained (eg. CLEF tracks) or manually created.", "labels": [], "entities": []}, {"text": "We suggest to use such corpora in order to adapt the SMT model parameters for query translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9801903367042542}, {"text": "query translation", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.8119471073150635}]}, {"text": "In our approach the parameters of the SMT models are optimized on the basis of the parallel queries set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9905362725257874}]}, {"text": "This is achieved either directly in the SMT system using the MERT (Minimum Error Rate Training) algorithm and optimiz-ing according to the BLEU 2 () score, or via reranking the Nbest translation candidates generated by a baseline system based on new parameters (and possibly new features) that aim to optimize a retrieval metric.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9805946946144104}, {"text": "MERT (Minimum Error Rate Training) algorithm", "start_pos": 61, "end_pos": 105, "type": "METRIC", "confidence": 0.8507085591554642}, {"text": "BLEU 2 () score", "start_pos": 139, "end_pos": 154, "type": "METRIC", "confidence": 0.944931298494339}]}, {"text": "It is important to note that both of the proposed approaches allow keeping the MT system independent of the document collection and indexing, and thus suitable fora query translation service.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9538313150405884}, {"text": "document collection and indexing", "start_pos": 108, "end_pos": 140, "type": "TASK", "confidence": 0.6255218014121056}, {"text": "query translation", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7041693925857544}]}, {"text": "These two approaches can also be combined by using the model produced with the first approach as a baseline that produces the Nbest list of translations that is then given to the reranking approach.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first present related work addressing the problem of query translation.", "labels": [], "entities": [{"text": "query translation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.8081298172473907}]}, {"text": "We then describe two approaches towards adapting an SMT system to the query-genre: tuning the SMT system on a parallel set of queries (Section 3.1) and adapting machine translation via the reranking framework (Section 3.2).", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.988084614276886}, {"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9751309752464294}, {"text": "machine translation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.692582294344902}]}, {"text": "We then present our experimental settings and results (Section 4) and conclude in section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Top: total number of parallel queries gathered  from all the CLEF tasks (size of the tuning set). Bot- tom: number of queries extracted from the tasks for  which the human relevance judgements were availble  (size of the reranking training set).", "labels": [], "entities": [{"text": "Bot- tom", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9452452063560486}]}, {"text": " Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task.", "labels": [], "entities": [{"text": "MAP", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.5107094049453735}, {"text": "CLEF AdHoc TEL 2009 task", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.8681857347488403}]}, {"text": " Table 3: BLEU and MAP performance on CLEF AdHoc TEL 2009 task for the genre-tuned model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992376565933228}, {"text": "MAP", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9941034913063049}, {"text": "CLEF AdHoc TEL 2009 task", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.8738221287727356}]}, {"text": " Table 4: Feature weights for the query-genre tuned model. Abbreviations: DW -distortion weight, LM -language  model weight, PP -phrase penalty, WP -word penalty, \u03c6-phrase translation probability, lex-lexical weighting.", "labels": [], "entities": [{"text": "DW -distortion weight", "start_pos": 74, "end_pos": 95, "type": "METRIC", "confidence": 0.9630163908004761}, {"text": "\u03c6-phrase translation", "start_pos": 163, "end_pos": 183, "type": "TASK", "confidence": 0.6698320955038071}]}, {"text": " Table 5: Some examples of queries translations (T1:  baseline, T2: after reranking with lab-lex), MAP and  1-gramm BLEU scores for German-English.", "labels": [], "entities": [{"text": "MAP", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9970274567604065}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.969632089138031}]}]}