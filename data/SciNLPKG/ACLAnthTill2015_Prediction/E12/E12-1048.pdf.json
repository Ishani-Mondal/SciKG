{"title": [{"text": "Evaluating language understanding accuracy with respect to objective outcomes in a dialogue system", "labels": [], "entities": [{"text": "Evaluating language understanding", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8266333937644958}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9076920747756958}]}], "abstractContent": [{"text": "It is not always clear how the differences in intrinsic evaluation metrics fora parser or classifier will affect the performance of the system that uses it.", "labels": [], "entities": []}, {"text": "We investigate the relationship between the intrinsic evaluation scores of an interpretation component in a tutorial dialogue system and the learning outcomes in an experiment with human users.", "labels": [], "entities": []}, {"text": "Following the PARADISE methodology , we use multiple linear regression to build predictive models of learning gain, an important objective outcome metric in tutorial dialogue.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8056867122650146}]}, {"text": "We show that standard intrinsic metrics such as F-score alone do not predict the outcomes well.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9948662519454956}]}, {"text": "However, we can build predictive performance functions that account for up to 50% of the variance in learning gain by combining features based on standard evaluation scores and on the confusion matrix entries.", "labels": [], "entities": []}, {"text": "We argue that building such predictive models can help us better evaluate performance of NLP components that cannot be distinguished based on F-score alone, and illustrate our approach by comparing the current interpretation component in the system to anew classifier trained on the evaluation data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.9741528034210205}]}], "introductionContent": [{"text": "Much of the work in natural language processing relies on intrinsic evaluation: computing standard evaluation metrics such as precision, recall and Fscore on the same data set to compare the performance of different approaches to the same NLP problem.", "labels": [], "entities": [{"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9991143345832825}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9878520965576172}, {"text": "Fscore", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9907032251358032}]}, {"text": "However, once a component, such as a parser, is included in a larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved overall system performance.", "labels": [], "entities": []}, {"text": "Therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations.", "labels": [], "entities": []}, {"text": "For example, NLP components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of a textual entailment (RTE) system; parser performance could be evaluated by how well it contributes to an information retrieval task (.", "labels": [], "entities": [{"text": "information retrieval task", "start_pos": 266, "end_pos": 292, "type": "TASK", "confidence": 0.778716524442037}]}, {"text": "However, task-based evaluation can be difficult and expensive for interactive applications.", "labels": [], "entities": []}, {"text": "Specifically, task-based evaluation for dialogue systems typically involves collecting data from a number of people interacting with the system, which is time-consuming and labor-intensive.", "labels": [], "entities": []}, {"text": "Thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human participants.", "labels": [], "entities": []}, {"text": "This problem can be addressed via the use of the PARADISE evaluation methodology for spoken dialogue systems ().", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.7590579986572266}]}, {"text": "Ina PARADISE study, after an initial data collection with users, a performance function is created to predict an outcome metric (e.g., user satisfaction) which can normally only be measured through user surveys.", "labels": [], "entities": []}, {"text": "Typically, a multiple linear regression is used to fit a predictive model of the desired metric based on the values of interaction parameters that can be derived from system logs without additional user studies (e.g., dialogue length, word error rate, number of misunderstandings).", "labels": [], "entities": []}, {"text": "PARADISE models have been used extensively in task-oriented spoken dialogue systems to establish which components of the system most need improvement, with user satisfaction as the outcome metric.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.6594202518463135}]}, {"text": "In tutorial dialogue, PARADISE studies investigated which manually annotated features predict learning outcomes, to justify new features needed in the system (Forbes-).", "labels": [], "entities": []}, {"text": "We adapt the PARADISE methodology to evaluating individual NLP components, linking commonly used intrinsic evaluation scores with extrinsic outcome metrics.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9371166229248047}]}, {"text": "We describe an evaluation of an interpretation component of a tutorial dialogue system, with student learning gain as the target outcome measure.", "labels": [], "entities": []}, {"text": "We first describe the evaluation setup, which uses standard classification accuracy metrics for system evaluation (Section 2).", "labels": [], "entities": []}, {"text": "We discuss the results of the intrinsic system evaluation in Section 3.", "labels": [], "entities": []}, {"text": "We then show that standard evaluation metrics do not serve as good predictors of system performance for the system we evaluated.", "labels": [], "entities": []}, {"text": "However, adding confusion matrix features improves the predictive model (Section 4).", "labels": [], "entities": []}, {"text": "We argue that in practical applications such predictive metrics should be used alongside standard metrics for component evaluations, to better predict how different components will perform in the context of a specific task.", "labels": [], "entities": []}, {"text": "We demonstrate how this technique can help differentiate the output quality between a majority class baseline, the system's output, and the output of anew classifier we trained on our data (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we discuss some limitations and possible extensions to this approach (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "The interpretation component of BEETLE II was developed based on the transcripts of 8 sessions  Although the intrinsic evaluation shows that the BEETLE II interpreter performs better than the baseline on the F score, ultimately system developers are not interested in improving interpretation for its own sake: they want to know whether the time spent on improvements, and the complications in system design which may accompany them, are worth the effort.", "labels": [], "entities": [{"text": "F score", "start_pos": 208, "end_pos": 215, "type": "METRIC", "confidence": 0.9696211814880371}]}, {"text": "Specifically, do such changes translate into improvement in overall system performance?", "labels": [], "entities": []}, {"text": "To answer this question without running expensive user studies we can build a model which predicts likely outcomes based on the data observed so far, and then use the model's predictions as an additional evaluation metric.", "labels": [], "entities": []}, {"text": "We chose a multiple linear regression model for this task, linking the classification scores with learning gain as measured during the data collection.", "labels": [], "entities": []}, {"text": "This approach follows the general PARADISE approach (), but while PARADISE is typically used to determine which system components need  Recall from Section 2.1 that each participant in our data collection was given a pre-test and a post-test, measuring their knowledge of course material.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.8576431274414062}]}, {"text": "The test score was equal to the proportion of correctly answered questions.", "labels": [], "entities": []}, {"text": "The normalized learning gain, post\u2212pre 1\u2212pre is a metric typically used to assess system quality in intelligent tutoring, and this is the metric we are trying to model.", "labels": [], "entities": []}, {"text": "Thus, the training data for our model consists of 35 instances, each corresponding to a single dialogue and the learning gain associated with it.", "labels": [], "entities": []}, {"text": "We can compute intrinsic evaluation scores for each dialogue, in order to build a model that predicts that student's learning gain based on these scores.", "labels": [], "entities": []}, {"text": "If the model's predictions are sufficiently reliable, we can also use them for predicting the learning gain that a student could achieve when interacting with anew version of the interpretation component for the system, not yet tested with users.", "labels": [], "entities": []}, {"text": "We can then use the predicted score to compare different implementations and choose the one with the highest predicted learning gain.", "labels": [], "entities": []}, {"text": "lists the feature sets we used.", "labels": [], "entities": []}, {"text": "We tried two basic types of features.", "labels": [], "entities": []}, {"text": "First, we used the evaluation scores reported in the previous section as features.", "labels": [], "entities": []}, {"text": "Second, we hypothesized that some errors that the system makes are likely to be worse than others from a tutoring perspective.", "labels": [], "entities": []}, {"text": "For example, if the student gives a contradictory answer, accepting it as correct may lead to student misconceptions; on the other hand, calling an irrelevant answer \"partially correct but incomplete\" maybe less of a problem.", "labels": [], "entities": []}, {"text": "Therefore, we computed separate confusion matrices for each student.", "labels": [], "entities": []}, {"text": "We normalized each confusion matrix cell by the total number of incorrect classifications for that student.", "labels": [], "entities": []}, {"text": "We then added features based on confusion frequencies to our feature set.", "labels": [], "entities": []}, {"text": "Ideally, we should add 20 different features to our model, corresponding to every possible confusion.", "labels": [], "entities": []}, {"text": "However, we are facing a sparse data problem, illustrated by the overall confusion matrix for the corpus in.", "labels": [], "entities": []}, {"text": "For example, we only observed 25 instances where a contradictory utterance was miscategorized as correct (compared to 200 \"contradictory-pc incomplete\" confusions), and so for many students this misclassification was never observed, and predictions based on this feature are not likely to be reliable.", "labels": [], "entities": []}, {"text": "Therefore, we limited our features to those misclassifications that occurred at least twice for each student (i.e., at least 70 times in the entire corpus).", "labels": [], "entities": []}, {"text": "The list of resulting features is shown in the \"conf\" row of.", "labels": [], "entities": []}, {"text": "Since only a small number of features was included, this limits the applicability of the model we derived from this data set to the systems which make similar types of confusions.", "labels": [], "entities": []}, {"text": "However, it is still interesting to investigate whether confusion probabilities provide additional information compared to standard evaluation metrics.", "labels": [], "entities": []}, {"text": "We discuss how better coverage could be obtained in Section 6.", "labels": [], "entities": [{"text": "coverage", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9253011345863342}, {"text": "Section 6", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.8859641849994659}]}, {"text": "selection implemented in the R stepwise regression library.", "labels": [], "entities": []}, {"text": "As measures of model quality, we report R 2 , the percentage of variance accounted for by the models (a typical measure of fit in regression modeling), and mean squared error (MSE).", "labels": [], "entities": [{"text": "R 2", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9868834912776947}, {"text": "mean squared error (MSE)", "start_pos": 156, "end_pos": 180, "type": "METRIC", "confidence": 0.9578420619169871}]}, {"text": "These were estimated using leave-one-out crossvalidation, since our data set is small.", "labels": [], "entities": []}, {"text": "We used feature ablation to evaluate the contribution of different features.", "labels": [], "entities": []}, {"text": "First, we investigated models using precision, recall or F-score alone.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9994971752166748}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9988705515861511}, {"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9976959824562073}]}, {"text": "As can be seen from the table, precision is not predictive of learning gain, while F-score and recall perform similarly to one another, with R 2 = 0.12.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9995494484901428}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9959558248519897}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9992125034332275}]}, {"text": "The models from can be used to compare different possible implementations of the interpretation component, under the assumption that the component with a higher predicted learning gain score is more appropriate to use in an ITS.", "labels": [], "entities": []}, {"text": "To show how our predictive models can be used in making implementation decisions, we compare three possible choices for an interpretation component: the original BEETLE II interpreter, the baseline classifier described earlier, and anew decision tree classifier trained on our data.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.8337615132331848}]}, {"text": "We built a decision tree classifier using the Weka implementation of C4.5 pruned decision trees, with default parameters.", "labels": [], "entities": [{"text": "Weka", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9389711618423462}]}, {"text": "As features, we used lexical similarity scores computed by the Text::Similarity package 4 . We computed 8 features: the similarity between student answer and either the expected answer text or the question text, using 4 different scores: raw number of overlapping words, F1 score, lesk score and cosine score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 271, "end_pos": 279, "type": "METRIC", "confidence": 0.9900997579097748}]}, {"text": "Its intrinsic evaluation scores are shown in, estimated using 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "We can compare BEETLE II and baseline classifier using the \"scores.all\" model.", "labels": [], "entities": [{"text": "BEETLE II", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9463329017162323}]}, {"text": "Crossvalidation MSE  score for BEETLE II is 0.66.", "labels": [], "entities": [{"text": "MSE  score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9190455675125122}, {"text": "BEETLE", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9266231060028076}]}, {"text": "The predicted score for the baseline is 0.28.", "labels": [], "entities": []}, {"text": "We cannot use the models based on confusion scores (\"conf\", \"conf+scores.f\" or \"full\") for evaluating the baseline, because the confusions it makes are always to predict that the answer is correct when the actual label is \"incomplete\" or \"contradictory\".", "labels": [], "entities": []}, {"text": "Such situations were too rare in our training data, and therefore were not included in the models (as discussed in Section 4.1).", "labels": [], "entities": []}, {"text": "Additional data will need to be collected before this model can reasonably predict baseline behavior.", "labels": [], "entities": []}, {"text": "Compared to our new classifier, BEETLE II has lower overall accuracy (0.43 vs. 0.53), but performs micro-and macro-averaged scores.", "labels": [], "entities": [{"text": "BEETLE II", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9433565735816956}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9992508292198181}]}, {"text": "BEE-TLE II precision is higher than that of the classifier.", "labels": [], "entities": [{"text": "BEE-TLE II", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9494017064571381}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.6533371210098267}]}, {"text": "This is not unexpected given how the system was designed: since misunderstandings caused dialogue breakdown in pilot tests, the interpreter was built to prefer rejecting utterances as uninterpretable rather than assigning them to an incorrect class, leading to high precision but lower recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 266, "end_pos": 275, "type": "METRIC", "confidence": 0.9976656436920166}, {"text": "recall", "start_pos": 286, "end_pos": 292, "type": "METRIC", "confidence": 0.9977474808692932}]}, {"text": "However, we can use all our predictive models to evaluate the classifier.", "labels": [], "entities": []}, {"text": "We checked the the confusion matrix (not shown here due to space limitations), and saw that the classifier made some of the same types of confusions that BEETLE II interpreter made.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.5780972242355347}]}, {"text": "On the \"scores.all\" model, the predicted learning gain score for the classifier is 0.63, also very close to BEETLE II.", "labels": [], "entities": [{"text": "learning gain score", "start_pos": 41, "end_pos": 60, "type": "METRIC", "confidence": 0.8743343154589335}, {"text": "BEETLE II", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9833236932754517}]}, {"text": "But with the \"conf+scores.all\" model, the predicted score is 0.89, compared to 0.59 for BEETLE II, indicating that we should prefer the newly built classifier.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9652583003044128}]}, {"text": "Looking at individual class performance, the classifier performs better than the BEETLE II interpreter on identifying \"correct\" and \"contradictory\" answers, but does not do as well for partially correct but incomplete, and for irrelevant answers.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9175514578819275}]}, {"text": "Using our predictive performance metric highlights the differences between the classifiers and effectively helps determine which confusion types are the most important.", "labels": [], "entities": []}, {"text": "One limitation of this prediction, however, is that the original system's output is considerably more complex: the BEETLE II interpreter explicitly identifies correct, incorrect and missing parts of the student answer which are then used by the system to formulate adaptive feedback.", "labels": [], "entities": [{"text": "BEETLE II interpreter", "start_pos": 115, "end_pos": 136, "type": "METRIC", "confidence": 0.9153085947036743}]}, {"text": "This is an important feature of the system because it allows for implementation of strategies such as acknowledging and restating correct parts of the an-  swer.", "labels": [], "entities": []}, {"text": "However, we could still use a classifier to \"double-check\" the interpreter's output.", "labels": [], "entities": []}, {"text": "If the predictions made by the original interpreter and the classifier differ, and in particular when the classifier assigns the \"contradictory\" label to an answer, BEETLE II may choose to use a generic strategy for contradictory utterances, e.g. telling the student that their answer is incorrect without specifying the exact problem, or asking them to re-read portions of the material.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.7851653099060059}]}], "tableCaptions": [{"text": " Table 1: Distribution of annotated labels in the evalu- ation corpus", "labels": [], "entities": []}, {"text": " Table 2: Intrinsic Evaluation Results for the BEETLE II and a majority class baseline", "labels": [], "entities": [{"text": "Intrinsic Evaluation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.909402459859848}, {"text": "BEETLE II", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.940099447965622}]}, {"text": " Table 3: Confusion matrix for BEETLE II. System predicted values are in rows; actual values in columns.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.8292015790939331}]}, {"text": " Table 6: Intrinsic evaluation scores for our newly built  classifier.", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9318684339523315}]}]}