{"title": [{"text": "Validation of sub-sentential paraphrases acquired from parallel monolingual corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "The task of paraphrase acquisition from related sentences can be tackled by a variety of techniques making use of various types of knowledge.", "labels": [], "entities": [{"text": "paraphrase acquisition from related sentences", "start_pos": 12, "end_pos": 57, "type": "TASK", "confidence": 0.9336926937103271}]}, {"text": "In this work, we make the hypothesis that their performance can be increased if candidate paraphrases can be validated using information that characterizes paraphrases independently of the set of techniques that proposed them.", "labels": [], "entities": []}, {"text": "We implement this as a bi-class classification problem (i.e. paraphrase vs. not paraphrase), allowing any paraphrase acquisition technique to be easily integrated into the combination system.", "labels": [], "entities": []}, {"text": "We report experiments on two languages, English and French, with 5 individual techniques on parallel mono-lingual parallel corpora obtained via multiple translation, and a large set of classification features including surface to contex-tual similarity measures.", "labels": [], "entities": []}, {"text": "Relative improvements in F-measure close to 18% are obtained on both languages over the best performing techniques.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9830014705657959}]}], "introductionContent": [{"text": "The fact that natural language allows messages to be conveyed in a great variety of ways constitutes an important difficulty for NLP, with applications in both text analysis and generation.", "labels": [], "entities": [{"text": "text analysis and generation", "start_pos": 160, "end_pos": 188, "type": "TASK", "confidence": 0.6986118108034134}]}, {"text": "The term paraphrase is now commonly used in the NLP litterature to refer to textual units of equivalent meaning at the phrasal level (including single words).", "labels": [], "entities": [{"text": "NLP litterature", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8667525947093964}]}, {"text": "For instance, the phrases six months and half a year form a paraphrase pair applicable in many different contexts, as they would appropriately denote the same concept.", "labels": [], "entities": []}, {"text": "Although one can envisage to manually build high-coverage lists of synonyms, enumerating meaning equivalences at the level of phrases is too daunting a task for humans.", "labels": [], "entities": []}, {"text": "Because this type of knowledge can however greatly benefit many NLP applications, automatic acquisition of such paraphrases has attracted a lot of attention, and significant research efforts have been devoted to this objective.", "labels": [], "entities": []}, {"text": "Central to acquiring paraphrases is the need of assessing the quality of the candidate paraphrases produced by a given technique.", "labels": [], "entities": []}, {"text": "Most works to date have resorted to human evaluation of paraphrases on the levels of grammaticality and meaning equivalence.", "labels": [], "entities": []}, {"text": "Human evaluation is however often criticized as being both costly and non reproducible, and the situation is even more complicated by the inherent complexity of the task that can produce low inter-judge agreement.", "labels": [], "entities": []}, {"text": "Taskbased evaluation involving the use of paraphrasing into some application thus seem an acceptable solution, provided the evaluation methodologies for the given task are deemed acceptable.", "labels": [], "entities": []}, {"text": "This, in turn, puts the emphasis on observing the impact of paraphrasing on the targeted application and is rarely accompanied by a study of the intrinsic limitations of the paraphrase acquisition technique used.", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.829843670129776}]}, {"text": "The present work is concerned with the task of sub-sentential paraphrase acquisition from pairs of related sentences.", "labels": [], "entities": [{"text": "sub-sentential paraphrase acquisition from pairs of related sentences", "start_pos": 47, "end_pos": 116, "type": "TASK", "confidence": 0.8223853893578053}]}, {"text": "A large variety of techniques have been proposed that can be applied to this task.", "labels": [], "entities": []}, {"text": "They typically make use of different kinds of automatically or manually acquired knowledge.", "labels": [], "entities": []}, {"text": "We make the hypothesis that their performance can be increased if candidate para-phrases can be validated using information that characterize paraphrases in complement to the set of techniques that proposed them.", "labels": [], "entities": []}, {"text": "We propose to implement this as a bi-class classification problem (i.e. paraphrase vs. not paraphrase), allowing any paraphrase acquisition technique to be easily integrated into the combination system.", "labels": [], "entities": []}, {"text": "In this article, we report experiments on two languages, English and French, with 5 individual techniques based on a) statistical word alignment models, b) translational equivalence, c) handcoded rules of term variation, d) syntactic similarity, and e) edit distance on word sequences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.6827598214149475}]}, {"text": "We used parallel monolingual parallel corpora obtained via multiple translation from a single language as our sources of related sentences, and a large set of features including surface to contextual similarity measures.", "labels": [], "entities": []}, {"text": "Relative improvements in F-measure close to 18% are obtained on both languages over the best performing techniques.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9830015897750854}]}, {"text": "The remainder of this article is organized as follows.", "labels": [], "entities": []}, {"text": "We first briefly review previous work on sub-sentential paraphrase acquisition in section 2.", "labels": [], "entities": [{"text": "sub-sentential paraphrase acquisition", "start_pos": 41, "end_pos": 78, "type": "TASK", "confidence": 0.6386395891507467}]}, {"text": "We then describe our experimental setting in section 3 and the individual techniques that we have studied in section 4.", "labels": [], "entities": []}, {"text": "Section 5 is devoted to our approach for validating paraphrases proposed by individual techniques.", "labels": [], "entities": []}, {"text": "Finally, section 6 concludes the article and presents some of our future work in the area of paraphrase acquisition.", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9594940543174744}]}], "datasetContent": [{"text": "We used the main aspects of the methodology described by  for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition.", "labels": [], "entities": [{"text": "sub-sentential paraphrase acquisition", "start_pos": 153, "end_pos": 190, "type": "TASK", "confidence": 0.6497837702433268}]}, {"text": "Pairs of related sentences are hand-aligned to define a set of reference atomic paraphrase pairs at the level of words or phrases, denoted as R atom 1 . We conducted a small-scale study to assess different types of corpora of related sentences: 1.", "labels": [], "entities": []}, {"text": "single language translation Corpora obtained by several independent human translation of the same sentences (e.g. ().", "labels": [], "entities": []}, {"text": "2. multiple language translation Same as above, but where a sentence is translated from 4 different languages into the same language ().", "labels": [], "entities": [{"text": "multiple language translation", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.704775353272756}]}, {"text": "Results for the 5 individual techniques are given on the left part of  English to French, compared with from Chinese to English), which should be consequently easier to word-align.", "labels": [], "entities": []}, {"text": "This is for example clearly shown by the results of the statistical aligner GIZA, which obtains a 7.68 advantage on recall for French over English.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8025874495506287}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9992363452911377}]}, {"text": "The two linguistically-aware techniques, FASTR and SYNT, have a very strong precision on the more parallel French corpus, but fail to achieve an acceptable recall on their own.", "labels": [], "entities": [{"text": "FASTR", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9354858994483948}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9979762434959412}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9981397390365601}]}, {"text": "This is not surprising : FASTR metarules are focussed on term variant extraction, and SYNT requires two syntactic trees to be highly comparable to extract sub-sentential paraphrases.", "labels": [], "entities": [{"text": "term variant extraction", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6278696556886038}]}, {"text": "When these constrained conditions are met, these two techniques appear to perform quite well in terms of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9987599849700928}]}, {"text": "GIZA and TER p perform roughly in the same range on French, with acceptable precision and recall, TER p performing overall better, with e.g. a 1.14 advantage on F-measure on French and 4.19 on English.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9618121385574341}, {"text": "TER", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9941756725311279}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.999527096748352}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9993126392364502}, {"text": "TER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9799504280090332}, {"text": "F-measure", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9969519376754761}]}, {"text": "The fact that TER p performs comparatively better on English than on French 8 , with a 1.76 advantage on F-measure, is not contradictory: the implemented edit distance makes it possible to align reasonably distant words and phrases independently from syntax, and to find alignments for close remaining words, so the differences of performance between the two languages are not necessarily expected to be comparable with the results of a statistical alignment technique.", "labels": [], "entities": [{"text": "TER p", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9756605923175812}, {"text": "F-measure", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9904888868331909}]}, {"text": "English being a poorly-inflected language, alignment clues between two sentential paraphrases are expected to be more numerous Recall that all specific linguistic modules for English only from TERp had been disabled, so the better performance on English cannot be explained by a difference in terms of resources used. than for highly-inflected French.", "labels": [], "entities": [{"text": "TERp", "start_pos": 193, "end_pos": 197, "type": "DATASET", "confidence": 0.7474120259284973}]}, {"text": "PIVOT is on par with GIZA as regards precision, but obtains a comparatively much lower recall (differences of 19.32 and 19.80 on recall on French and English respectively).", "labels": [], "entities": [{"text": "PIVOT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6928438544273376}, {"text": "GIZA", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.49526697397232056}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.999470055103302}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9995310306549072}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9978633522987366}]}, {"text": "This may first be due in part to the paraphrasing score threshold used for PIVOT, but most certainly to the use of a bilingual corpus from the domain of parliamentary debates to extract paraphrases when our test sets are from the news domain: we maybe observing differences inherent to the domain, and possibly facing the issue of numerous \"out-ofvocabulary\" phrases, in particular for named entities which frequently occur in the news domain.", "labels": [], "entities": [{"text": "paraphrasing score threshold", "start_pos": 37, "end_pos": 65, "type": "METRIC", "confidence": 0.8551682829856873}, {"text": "PIVOT", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.5793474316596985}]}, {"text": "Importantly, we can note that we obtain at best a recall of 45.98 on French (GIZA) and of 45.37 on English (TER p ).", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9995070695877075}, {"text": "French (GIZA)", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.6778919994831085}, {"text": "TER p )", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9698107441266378}]}, {"text": "This may come as a disappointment but, given the broad set of techniques evaluated, this should rather underline the inherent complexity of the task.", "labels": [], "entities": []}, {"text": "Also, recall that the metrics used do not consider identity paraphrases (e.g. at the same time \u2194 at the same time), as well as the fact that gold standard alignment is a very difficult process as shown by interjudge agreement values and our example from section 3.", "labels": [], "entities": [{"text": "gold standard alignment", "start_pos": 141, "end_pos": 164, "type": "TASK", "confidence": 0.4941541651884715}]}, {"text": "This, again, confirms that the task that is addressed is indeed a difficult one, and provides further justification for initially focussing on parallel monolingual corpora, albeit scarce, for conducting fine-grained studies on sub-sentential paraphrasing.", "labels": [], "entities": []}, {"text": "Lastly, we can also note that precision is not very high, with (at best, using TER p\u2192P ) average values for all techniques of 40.97 and 40.46 on French and English, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9996237754821777}, {"text": "TER p\u2192P ) average", "start_pos": 79, "end_pos": 96, "type": "METRIC", "confidence": 0.9587419927120209}]}, {"text": "Several facts may provide explanations for this observation.", "labels": [], "entities": []}, {"text": "First, it should be noted that none of those techniques, except SYNT, was originally developed for the task of sub-sentential paraphrase acquisition from monolingual parallel corpora.", "labels": [], "entities": [{"text": "sub-sentential paraphrase acquisition from monolingual parallel corpora", "start_pos": 111, "end_pos": 182, "type": "TASK", "confidence": 0.7812823738370623}]}, {"text": "This results in definitions that are at best closely related to this task.", "labels": [], "entities": []}, {"text": "Designing new techniques was not one of the objectives of our study, so we have reused existing techniques, originally developed with different aims (bilingual parallel corpora word alignment (GIZA), term variant recognition (FASTR), Machine Translation evaluation (TER p )).", "labels": [], "entities": [{"text": "bilingual parallel corpora word alignment (GIZA)", "start_pos": 150, "end_pos": 198, "type": "TASK", "confidence": 0.6474537216126919}, {"text": "term variant recognition (FASTR)", "start_pos": 200, "end_pos": 232, "type": "TASK", "confidence": 0.7406187752882639}, {"text": "Machine Translation evaluation (TER", "start_pos": 234, "end_pos": 269, "type": "TASK", "confidence": 0.7688945055007934}]}, {"text": "Also, techniques such as GIZA and TER p attempt to align as many words as possible in a sentence pair, when gold standard alignments sometimes contain gaps.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9448253512382507}, {"text": "TER p", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9750761091709137}]}, {"text": "Finally, the metrics used will count as false small variations of gold standard paraphrases (e.g. missing function word): the acceptability or not of such candidates could be either evaluated in a scenario where such \"acceptable\" variants would betaken into account, and could be considered in the context of some actual use of the acquired paraphrases in some application.", "labels": [], "entities": []}, {"text": "Nonetheless, on average the techniques in our study produce more candidates that are not in the gold standard: this will bean important fact to keep in mind when tackling the task of combining their outputs.", "labels": [], "entities": []}, {"text": "In particular, we will investigate the use of features indicating the combination of techniques that predicted a given paraphrase pair, aiming to capture consensus information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Various indicators of sentence pair comparability for different corpus types. Statistics are reported for  French on sets of 100 sentence pairs.", "labels": [], "entities": []}, {"text": " Table 2: Results on the test set on English and French for the 5 individual paraphrase acquisition techniques (left  part) and for the 2 combination techniques (right part).", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7134937644004822}]}, {"text": " Table 3. A number of pairs of tech- niques have strong complementarity values, the  strongest one being for GIZA and TER p for both  languages. According to these figures, PIVOT  identify paraphrases which are slightly more sim- ilar to those of TER p than those of GIZA. Inter- estingly, FASTR and SYNT exhibit a strong com- plementarity, where in French, for instance, they  only have a very small proportion of paraphrases  in common. Considering the set of all other tech- niques, GIZA provides the more new paraphrases  on French and TER p on English.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.820217490196228}, {"text": "TER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9907784461975098}, {"text": "FASTR", "start_pos": 290, "end_pos": 295, "type": "METRIC", "confidence": 0.8740344047546387}, {"text": "TER", "start_pos": 540, "end_pos": 543, "type": "METRIC", "confidence": 0.9483466744422913}]}, {"text": " Table 3: Values of complementarity on the test set for  both languages, where the following formula was used  for the set of technique outputs T = {t 1 , t 2 , ..., t n } :", "labels": [], "entities": []}]}