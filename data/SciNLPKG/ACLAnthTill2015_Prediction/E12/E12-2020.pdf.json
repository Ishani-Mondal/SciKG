{"title": [{"text": "HadoopPerceptron: a Toolkit for Distributed Perceptron Training and Prediction with MapReduce", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a set of open-source software modules to perform structured Perceptron Training, Prediction and Evaluation within the Hadoop framework.", "labels": [], "entities": [{"text": "Perceptron Training", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7072395980358124}, {"text": "Prediction and Evaluation", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.7368181546529134}]}, {"text": "Apache Hadoop is a freely available environment for running distributed applications on a computer cluster.", "labels": [], "entities": []}, {"text": "The software is designed within the Map-Reduce paradigm.", "labels": [], "entities": []}, {"text": "Thanks to distributed computing, the proposed software reduces substantially execution times while handling huge data-sets.", "labels": [], "entities": []}, {"text": "The distributed Perceptron training algorithm preserves convergence properties, thus guar-anties same accuracy performances as the serial Perceptron.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9988834261894226}]}, {"text": "The presented modules can be executed as stand-alone software or easily extended or integrated in complex systems.", "labels": [], "entities": []}, {"text": "The execution of the modules applied to specific NLP tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Perceptron training algorithm) is widely applied in the Natural Language Processing community for learning complex structured models.", "labels": [], "entities": []}, {"text": "The non-probabilistic nature of the perceptron parameters makes it possible to incorporate arbitrary features without the need to calculate a partition function, which is required for its discriminative probabilistic counterparts such as CRFs ().", "labels": [], "entities": []}, {"text": "Additionally, the Perceptron is robust to approximate inference in large search spaces.", "labels": [], "entities": [{"text": "Perceptron", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.8704900741577148}]}, {"text": "Nevertheless, Perceptron training is proportional to inference which is frequently non-linear in the input sequence size.", "labels": [], "entities": []}, {"text": "Therefore, training can be time-consuming for complex model structures.", "labels": [], "entities": []}, {"text": "Furthermore, for an increasing number of tasks is fundamental to leverage on huge sources of data as the World Wide Web.", "labels": [], "entities": [{"text": "World Wide Web", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.8148422439893087}]}, {"text": "Such difficulties render the scalability of the Perceptron a challenge.", "labels": [], "entities": []}, {"text": "In order to improve scalability, propose a distributed training strategy called iterative parameter mixing, and show that it has similar convergence properties to the standard perceptron algorithm; it finds a separating hyperplane if the training set is separable; it produces models with comparable accuracies to those trained serially on all the data; and reduces training times significantly by exploiting computing clusters.", "labels": [], "entities": []}, {"text": "With this paper we present the HadoopPerceptron package.", "labels": [], "entities": []}, {"text": "It provides a freely available open-source implementation of the iterative parameter mixing algorithm for training the structured perceptron on a generic sequence labeling tasks.", "labels": [], "entities": []}, {"text": "Furthermore, the package provides two additional modules for prediction and evaluation.", "labels": [], "entities": [{"text": "prediction", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.9690527319908142}]}, {"text": "The three software modules are designed within the MapReduce programming model) and implemented using the Apache Hadoop distributed programming Framework.", "labels": [], "entities": []}, {"text": "The presented HadoopPerceptron package reduces execution time significantly compared to its serial counterpart while maintaining comparable performance.", "labels": [], "entities": []}, {"text": "Figure 1: Distributed perceptron with iterative parameter mixing strategy.", "labels": [], "entities": []}, {"text": "Each w (i,n) is computed in parallel.", "labels": [], "entities": []}, {"text": "\u00b5 n = {\u00b5 1,n , . .", "labels": [], "entities": []}, {"text": ", \u00b5 S,n }, \u2200\u00b5 i,n \u2208 \u00b5 n : \u00b5 i,n \u2265 0 and \u2200n :", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate HadoopPerceptron training time and prediction accuracy on a part-of-speech (POS) task using the PennTreeBank corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.925374448299408}, {"text": "PennTreeBank corpus", "start_pos": 111, "end_pos": 130, "type": "DATASET", "confidence": 0.9894030392169952}]}, {"text": "We use sections 0-18 of the Wall Street Journal for training, and sections 22-24 for testing.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9365304112434387}]}, {"text": "We compare the regular percepton trained serially on all the training data with the distributed perceptron trained with iterative parameter mixing with variable number of splits S \u2208 {10, 20}.", "labels": [], "entities": []}, {"text": "For each system, we report the prediction accuracy measure on the final test set to determine if any loss is observed as a consequence of distributed training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9174103736877441}]}, {"text": "For each system, plots accuracy results computed at the end of every training epoch against consumed wall-clock time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995121955871582}]}, {"text": "We observe that iterative mixing parameter achieves comparable performance to its serial counterpart while converging orders of magnitude faster.", "labels": [], "entities": []}, {"text": "Furthermore, we note that the distributed algorithm achieves a slightly higher final accuracy  than serial training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.8954244256019592}]}, {"text": "suggest that this is due to the bagging effect that the distributed training has, and due to parameter mixing that is similar to the averaged perceptron.", "labels": [], "entities": []}, {"text": "We note also that increasing the number of splits increases the number of epoch required to attain convergence, while reducing the time required per epoch.", "labels": [], "entities": []}, {"text": "This implies a trade-off between slower convergence and quicker epochs when selecting a larger number of splits.", "labels": [], "entities": []}], "tableCaptions": []}