{"title": [{"text": "A Comparative Study of Reinforcement Learning Techniques on Dialogue Management", "labels": [], "entities": [{"text": "Dialogue Management", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7379605323076248}]}], "abstractContent": [{"text": "Adaptive Dialogue Systems are rapidly becoming part of our everyday lives.", "labels": [], "entities": [{"text": "Adaptive Dialogue", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9095039665699005}]}, {"text": "As they progress and adopt new technologies they become more intelligent and able to adapt better and faster to their environment.", "labels": [], "entities": []}, {"text": "Research in this field is currently focused on how to achieve adaptation, and particularly on applying Reinforcement Learning (RL) techniques, so a comparative study of the related methods, such as this, is necessary.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6116290628910065}]}, {"text": "In this work we compare several standard and state of the art online RL algorithms that are used to train the dialogue manager in a dynamic environment, aiming to aid researchers / developers choose the appropriate RL algorithm for their system.", "labels": [], "entities": []}, {"text": "This is the first work, to the best of our knowledge, to evaluate online RL algorithms on the dialogue problem and in a dynamic environment .", "labels": [], "entities": [{"text": "RL", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9483827948570251}]}], "introductionContent": [{"text": "Dialogue Systems (DS) are systems that are able to make natural conversation with their users.", "labels": [], "entities": []}, {"text": "There are many types of DS that serve various aims, from hotel and flight booking to providing information or keeping company and forming long term relationships with the users.", "labels": [], "entities": [{"text": "flight booking", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.6500735282897949}]}, {"text": "Other interesting types of DS are tutorial systems, whose goal is to teach something new, persuasive systems whose goal is to affect the user's attitude towards something through casual conversation and rehabilitation systems that aim at engaging patients to various activities that help their rehabilitation process.", "labels": [], "entities": []}, {"text": "DS that incorporate adaptation to their environment are called Adaptive Dialogue Systems (ADS).", "labels": [], "entities": []}, {"text": "Over the past few years ADS have seen a lot of progress and have attracted the research community's and industry's interest.", "labels": [], "entities": [{"text": "ADS", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8959057927131653}]}, {"text": "There is a number of available ADS, applying state of the art techniques for adaptation and learning, such as the one presented by , where the authors propose an ADS that provides tourist information in a fictitious town.", "labels": [], "entities": []}, {"text": "Their system is trained using RL and some clever state compression techniques to make it scalable, it is robust to noise and able to recover from errors (misunderstandings).", "labels": [], "entities": []}, {"text": "propose a travel planning ADS, that is able to learn dialogue policies using RL, building on top of existing handcrafted policies.", "labels": [], "entities": [{"text": "travel planning ADS", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7134962677955627}]}, {"text": "This enables the designers of the system to provide prior knowledge and the system can then learn the details.", "labels": [], "entities": []}, {"text": "proposes an affective ADS which serves as a museum guide.", "labels": [], "entities": []}, {"text": "It is able to adapt to each user's personality by assessing his / her emotional state and current mood and also adapt its output to the user's expertise level.", "labels": [], "entities": []}, {"text": "The system itself has an emotional state that is affected by the user and affects its output.", "labels": [], "entities": []}, {"text": "An example ADS architecture is depicted in, where we can see several components trying to understand the user's utterance and several others trying to express the system's response.", "labels": [], "entities": []}, {"text": "The system first attempts to convert spoken input to text using the Automatic Speech Recognition (ASR) component and then tries to infer the meaning using the Natural Language Understanding (NLU) component.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.7483671605587006}]}, {"text": "At the core lies the Dialogue Manager (DM), a component responsible for understanding what the user's utterance means and deciding which action to take that will lead to achieving his / her goals.", "labels": [], "entities": []}, {"text": "The DM may also take into account contextual information or historical data before making a decision.", "labels": [], "entities": [{"text": "DM", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9184768795967102}]}, {"text": "After the system has decided what to say, it uses the Referring Expression Generation (REG) component to create appropriate referring expressions, the Natural Language Generation (NLG) component to create the textual form of the output and last, the Text To Speech (TTS) component to convert the text to spoken output.", "labels": [], "entities": []}, {"text": "Trying to make ADS as human-like as possible researchers have focused on techniques that achieve adaptation, i.e. adjust to the current user's personality, behaviour, mood, needs and to the environment in general.", "labels": [], "entities": []}, {"text": "Examples include adaptive or trainable NLG (, where the authors formulate their problem as a statistical planning problem and use RL to find a policy according to which the system will decide how to present information.", "labels": [], "entities": []}, {"text": "Another example is adaptive REG, where the authors again use RL to choose one of three strategies (jargon, tutorial, descriptive) according to the user's expertise level.", "labels": [], "entities": []}, {"text": "An example of adaptive TTS is the work of, where the authors propose a model that sorts paraphrases with respect to predictions of which sounds more natural.", "labels": [], "entities": [{"text": "TTS", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8611409068107605}]}, {"text": "propose a RL algorithm to optimize ADS parameters in general.", "labels": [], "entities": []}, {"text": "Last, many researchers have used RL to achieve adaptive Dialogue Management ().", "labels": [], "entities": [{"text": "adaptive Dialogue Management", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6460007230440775}]}, {"text": "As the reader may have noticed, the current trend in training these components is the application of RL techniques.", "labels": [], "entities": [{"text": "RL", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.955000638961792}]}, {"text": "RL is a well established field of artificial intelligence and provides us with robust frameworks that are able to deal with uncertainty and can scale to real world problems.", "labels": [], "entities": [{"text": "RL", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7905563116073608}]}, {"text": "One subcategory of RL is Online RL where the system can be trained on the fly, as it interacts with its environment.", "labels": [], "entities": [{"text": "RL", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9223101139068604}]}, {"text": "These techniques have recently begun to be applied to Dialogue Management and in this paper we perform an extensive evaluation of several standard and state of the art Online RL techniques on a generic dialogue problem.", "labels": [], "entities": [{"text": "Dialogue Management", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.9116603136062622}]}, {"text": "Our experiments were conducted with user simulations, with or without noise and using a model that is able to alter the user's needs at any given point.", "labels": [], "entities": []}, {"text": "We were thus able to see how well each algorithm adapted to minor (noise / uncertainty) or major (change in user needs) changes in the environment.", "labels": [], "entities": []}, {"text": "In general, RL algorithms fall in two categories, planning and learning algorithms.", "labels": [], "entities": [{"text": "RL algorithms", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9129206240177155}]}, {"text": "Planning or model-based algorithms use training examples from previous interactions with the environment as well as a model of the environment that simulates interactions.", "labels": [], "entities": []}, {"text": "Learning or modelfree algorithms only use training examples from previous interactions with the environment and that is the main difference of these two categories, according to.", "labels": [], "entities": []}, {"text": "The goal of an RL algorithm is to learn a good policy (or strategy) that dictates how the system should interact with the environment.", "labels": [], "entities": [{"text": "RL algorithm", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.8974446356296539}]}, {"text": "An algorithm then can follow a specific policy (i.e. interact with the environment in a specific, maybe predefined, way) while searching fora good policy.", "labels": [], "entities": []}, {"text": "This way of learning is called \"off policy\" learning.", "labels": [], "entities": []}, {"text": "The opposite is \"on policy\" learning, when the algorithm follows the policy that it is trying to learn.", "labels": [], "entities": []}, {"text": "This will become clear in section 2.2 where we provide the basics of RL.", "labels": [], "entities": [{"text": "RL", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.8322879076004028}]}, {"text": "Last, these algorithms can be categorized as policy iteration or value iteration algorithms, according to the way they evaluate and train a policy.", "labels": [], "entities": []}, {"text": "shows the algorithms we evaluated along with some of their characteristics.", "labels": [], "entities": []}, {"text": "We selected representative algorithms for each category and used the Dyna architecture) to implement model based algorithms.", "labels": [], "entities": []}, {"text": "SARSA(\u03bb)), Q Learning, Q(\u03bb) and AC-QV (Wiering and Van Hasselt, 2009) are well established RL algorithms, proven to work and simple to implement.", "labels": [], "entities": []}, {"text": "A serious disadvantage though is the fact that they do not scale well (assuming we have enough memory), as also supported by our results in section 5.", "labels": [], "entities": []}, {"text": "Least Squares SARSA(\u03bb)) is a variation of SARSA(\u03bb) that uses the least squares method to find the optimal policy.", "labels": [], "entities": []}, {"text": "Incremental Actor Critic (IAC) ( and Natural Actor Critic (NAC)) are actor -critic algorithms that follow the expected rewards gradient and the natural or Fisher Information gradient respectively.", "labels": [], "entities": []}, {"text": "An important attribute of many learning algorithms is function approximation which allows them to scale to real world problems.", "labels": [], "entities": [{"text": "function approximation", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7712443470954895}]}, {"text": "Function approximation attempts to approximate a target function by selecting from a class of functions that closely resembles the target.", "labels": [], "entities": [{"text": "Function approximation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8924575746059418}]}, {"text": "Care must betaken however, when applying this method, because many RL algorithms are not guaranteed to converge when using function approximation.", "labels": [], "entities": [{"text": "function approximation", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7261985540390015}]}, {"text": "On the other hand, policy gradient algorithms (algorithms that perform gradient ascend/descend on a performance surface), such as NAC or Natural Actor Belief Critic ) have good guarantees for convergence, even if we use function approximation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our main goal was to evaluate how each algorithm behaves in the following situations: \u2022 The system needs to adapt to a noise free environment.", "labels": [], "entities": []}, {"text": "\u2022 The system needs to adapt to a noisy environment.", "labels": [], "entities": []}, {"text": "\u2022 There is a change in the environment and the system needs to adapt.", "labels": [], "entities": []}, {"text": "To ensure each algorithm performed to the best of its capabilities we tuned each one's parameters in an exhaustive manner.", "labels": [], "entities": []}, {"text": "shows the parameter values selected for each algorithm.", "labels": [], "entities": []}, {"text": "The parameter in -greedy strategies was set to 0.01 and model-based algorithms trained their model for 15 iterations after each interaction with the environment.", "labels": [], "entities": []}, {"text": "Learning rates \u03b1 and \u03b2 and exploration parameter decayed as the episodes progressed to allow better stability.", "labels": [], "entities": []}, {"text": "At each episode the algorithms need enough iterations to explore the state space.", "labels": [], "entities": []}, {"text": "At the initial stages of learning, though, it is possible that some algorithms fall into loops and require a very large number of iterations before reaching a terminal state.", "labels": [], "entities": []}, {"text": "It would not hurt then if we bound the number of iterations to a reasonable limit, provided it allows enough \"negative\" rewards to be accumulated when following a \"bad\" direction.", "labels": [], "entities": []}, {"text": "In our evaluation the algorithms were allowed 2|D| iterations, ensuring enough steps for exploration but not allowing \"bad\" directions to be followed for too long.", "labels": [], "entities": []}, {"text": "To assess each algorithm's performance and convergence speed, we run each algorithm 100 times on a slot filling problem with 6 slots, 6 actions and 300 episodes.", "labels": [], "entities": [{"text": "slot filling problem", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7633146444956461}]}, {"text": "The average reward over a high number of episodes indicates how stable each algorithm is after convergence.", "labels": [], "entities": []}, {"text": "User query q was set to be {s 1 , ..., s 5 } and there was no noise in the environment, meaning that the action of querying a slot deterministically gets the system into a state where that slot is filled.", "labels": [], "entities": []}, {"text": "This can be formulated as: Pt (d j |d i , am ) = 1, P c (s j ) = 1\u2200j, \u03bd = 0 and\u02dcAand\u02dc and\u02dcA i,j = 1, \u2200i, j.", "labels": [], "entities": [{"text": "Aand", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9560031294822693}]}, {"text": "To evaluate the algorithms' performance in the presence of uncertainty we run each for 100 times, on the same slot filling problem but with , with varying and available action density values.", "labels": [], "entities": [{"text": "slot filling problem", "start_pos": 110, "end_pos": 130, "type": "TASK", "confidence": 0.8107095758120219}]}, {"text": "At each run, each algorithm was evaluated using the same transition probabilities and available actions.", "labels": [], "entities": []}, {"text": "To assess how the algorithms respond to environmental changes we conducted a similar but noise free experiment, where after a certain number of episodes the query q was changed.", "labels": [], "entities": []}, {"text": "Remember that q models the required information for the system to be able to answer with some degree of certainty, so changing q corresponds to requiring different slots to be filled by the user.", "labels": [], "entities": []}, {"text": "For this experiment we randomly generated two queries of approximately 65% of the number of slots.", "labels": [], "entities": []}, {"text": "The algorithms then needed to learn a policy for the first query and then adapt to the second, when the change occurs.", "labels": [], "entities": []}, {"text": "This could, for example, model scenarios where hotel booking becomes unavailable or some airports are closed, in a travel planning ADS.", "labels": [], "entities": []}, {"text": "Last, we evaluated each algorithm's scalability, by running each for 100 times on various slot filling problems, beginning with a problem with 4 slots and 4 actions up to a problem with 8 slots and 8 actions.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.7216389775276184}]}, {"text": "We measured the return averaged over the 100 runs each algorithm achieved.", "labels": [], "entities": []}, {"text": "Despite many notable efforts, a standardized evaluation framework for ADS or DS is still considered an open question by the research community.", "labels": [], "entities": [{"text": "ADS or DS", "start_pos": 70, "end_pos": 79, "type": "TASK", "confidence": 0.6992765665054321}]}, {"text": "The work in (Pietquin and Hastie, 2011) provides a very good survey of current techniques that evaluate several aspects of Dialogue Systems.", "labels": [], "entities": []}, {"text": "When RL is applied, researchers typically use the reward function as a metric of performance.", "labels": [], "entities": [{"text": "RL", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9278603196144104}]}, {"text": "This will be our evaluation metric as well, since it is common across all algorithms.", "labels": [], "entities": []}, {"text": "As defined in section 2.3, it penalizes attempts to answer the user's query with incomplete information as well as lengthy dialogues.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Optimized parameter values.", "labels": [], "entities": []}, {"text": " Table 5: Average number of episodes required  for convergence after the change.", "labels": [], "entities": [{"text": "convergence", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.977691650390625}]}, {"text": " Table 6: Average number of episodes required  for convergence on various problem dimensions.", "labels": [], "entities": [{"text": "convergence", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9687572121620178}]}]}