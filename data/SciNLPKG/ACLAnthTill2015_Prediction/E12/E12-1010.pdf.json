{"title": [{"text": "Answer Sentence Retrieval by Matching Dependency Paths Acquired from Question/Answer Sentence Pairs", "labels": [], "entities": [{"text": "Answer Sentence Retrieval", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8485783735911051}, {"text": "Acquired", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.7508562207221985}, {"text": "Question/Answer Sentence Pairs", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.5860314786434173}]}], "abstractContent": [{"text": "In Information Retrieval (IR) in general and Question Answering (QA) in particular , queries and relevant textual content often significantly differ in their properties and are therefore difficult to relate with traditional IR methods, e.g. keyword matching.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.8821409821510315}, {"text": "Question Answering (QA)", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.8488516628742218}, {"text": "keyword matching", "start_pos": 241, "end_pos": 257, "type": "TASK", "confidence": 0.7794284224510193}]}, {"text": "In this paper we describe an algorithm that addresses this problem, but rather than looking at it on a term matching/term re-formulation level, we focus on the syntactic differences between questions and relevant text passages.", "labels": [], "entities": []}, {"text": "To this end we propose a novel algorithm that analyzes dependency structures of queries and known relevant text passages and acquires transformational patterns that can be used to retrieve relevant textual content.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm in a QA setting, and show that it out-performs a baseline that uses only dependency information contained in the questions by 300% and that it also improves performance of a state of the art QA system significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is a well known problem in Information Retrieval (IR) and Question Answering (QA) that queries and relevant textual content often significantly differ in their properties, and are therefore difficult to match with traditional IR methods.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.8735520839691162}, {"text": "Question Answering (QA)", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.8321352779865265}]}, {"text": "A common example is a user entering words to describe their information need that do not match the words used in the most relevant indexed documents.", "labels": [], "entities": []}, {"text": "This work addresses this problem, but shifts focus from words to syntactic structures of questions and relevant pieces of text.", "labels": [], "entities": []}, {"text": "To this end, we present a novel algorithm that analyses the dependency structures of known valid answer sentence and from these acquires patterns that can be used to more precisely retrieve relevant text passages from the underlying document collection.", "labels": [], "entities": []}, {"text": "To achieve this, the position of key phrases in the answer sentence relative to the answer itself is analyzed and linked to a certain syntactic question type.", "labels": [], "entities": []}, {"text": "Unlike most previous work that uses dependency paths for QA (see Section 2), our approach does not require a candidate sentence to be similar to the question in any respect.", "labels": [], "entities": []}, {"text": "We learn valid dependency structures from the known answer sentences alone, and therefore are able to link a much wider spectrum of answer sentences to the question.", "labels": [], "entities": []}, {"text": "The work in this paper is presented and evaluated in a classical factoid Question Answering (QA) setting.", "labels": [], "entities": [{"text": "factoid Question Answering (QA)", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.7313375075658163}]}, {"text": "The main reason for this is that in QA suitable training and test data is available in the public domain, e.g. via the Text REtrieval Conference (TREC), see for example.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC)", "start_pos": 119, "end_pos": 151, "type": "TASK", "confidence": 0.5891502251227697}]}, {"text": "The methods described in this paper however can also be applied to other IR scenarios, e.g. web search.", "labels": [], "entities": []}, {"text": "The necessary condition for our approach to work is that the user query is somewhat grammatically well formed; this kind of queries are commonly referred to as Natural Language Queries or NLQs.", "labels": [], "entities": []}, {"text": "Table 1 provides evidence that users indeed search the web with NLQs.", "labels": [], "entities": []}, {"text": "The data is based on two query sets sampled from three months of user logs from a popular search engine, using two different sampling techniques.", "labels": [], "entities": []}, {"text": "The \"head\" set samples queries taking query frequency into account, so that more common queries have a proportionally higher chance of being selected.", "labels": [], "entities": []}, {"text": "The \"tail\" query set samples only queries that have been is-", "labels": [], "entities": []}], "datasetContent": [{"text": "For each created pattern, at least one matching example must exists: the sentence that was used to create it in the first place.", "labels": [], "entities": []}, {"text": "However, we do not know how precise each pattern is.", "labels": [], "entities": []}, {"text": "To this end, an additional processing step between pattern creation and application is needed: pattern evaluation.", "labels": [], "entities": [{"text": "pattern evaluation", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7887672185897827}]}, {"text": "Similar approaches to ours have been described in the relevant literature, many of them concerned with bootstrapping, starting with ().", "labels": [], "entities": []}, {"text": "The general purpose of this step is to use the available data about questions and their correct answers to evaluate how often each created pattern returns a corrector an incorrect result.", "labels": [], "entities": []}, {"text": "This data is stored with each pattern and the result of the equation, often called pattern precision, can be used during retrieval stage.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.8491745591163635}]}, {"text": "Pattern precision in our case is defined as: We use Lucene to retrieve the top 100 paragraphs from the AQUAINT corpus by issuing a query that consists of the query's key words and all non-stop words in the answer.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.7615474462509155}, {"text": "Lucene", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9463027715682983}, {"text": "AQUAINT corpus", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.921098917722702}]}, {"text": "Then, all patterns are loaded whose antecedent matches the query that is currently being processed.", "labels": [], "entities": []}, {"text": "After that, constituents from all sentences in the retrieved 100 paragraphs are aligned to the query's constituents in the same way as for the sentences during pattern creation, see Section 5.", "labels": [], "entities": []}, {"text": "Now, the paths specified in these patterns are searched for in the paragraphs' parse trees.", "labels": [], "entities": []}, {"text": "If they are all found, it is checked whether they all point to the same node and whether this node's surface structure is in some morphological form present in the answer strings associated with the question in our training data.", "labels": [], "entities": []}, {"text": "If this is the case a variable in the pattern named correct is increased by 1, otherwise the variable incorrect is increased by 1.", "labels": [], "entities": []}, {"text": "After the evaluation process is finished the final version of the pattern given as an example in  The variables correct and incorrect are used during retrieval, where the score of an answer candidate ac is the sum of all scores of all matching patterns p: where The highest scoring candidate is selected.", "labels": [], "entities": []}, {"text": "We would like to explicitly call out one property of our algorithm: It effectively returns two entities: a) a sentence that constitutes a valid response to the query, b) the head node of a phrase in that sentence that constitutes the answer.", "labels": [], "entities": []}, {"text": "Therefore the algorithm can be used for sentence retrieval or for answer retrieval.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7890175879001617}, {"text": "answer retrieval", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.8804495334625244}]}, {"text": "It depends on the application which of the two behaviors is desired.", "labels": [], "entities": []}, {"text": "In the next section, we evaluate its answer retrieval performance.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.861300528049469}]}, {"text": "This section provides an evaluation of the algorithm described in this paper.", "labels": [], "entities": []}, {"text": "The key questions we seek to answer are the following: 1.", "labels": [], "entities": []}, {"text": "How does our method perform when compared to a baseline that extracts dependency paths from the question?", "labels": [], "entities": []}, {"text": "2. How much does the described algorithm improve performance of a state-of-the-art QA system?", "labels": [], "entities": []}, {"text": "3. What is the effect of training data size on performance?", "labels": [], "entities": []}, {"text": "Can we expect that more training data would further improve the algorithm's performance?", "labels": [], "entities": []}, {"text": "We use all factoid questions in TREC's QA test sets from 2002 to 2006 for evaluation for which a known answer exists in the AQUAINT corpus.", "labels": [], "entities": [{"text": "TREC's QA test sets from 2002", "start_pos": 32, "end_pos": 61, "type": "DATASET", "confidence": 0.9058310815266201}, {"text": "AQUAINT corpus", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.9372361600399017}]}, {"text": "Additionally, the data in () is used.", "labels": [], "entities": []}, {"text": "In this paper the authors attempt to identify a much more complete set of relevant documents fora subset of TREC 2002 questions than TREC itself.", "labels": [], "entities": [{"text": "TREC 2002 questions", "start_pos": 108, "end_pos": 127, "type": "DATASET", "confidence": 0.8181900580724081}, {"text": "TREC", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.8683319687843323}]}, {"text": "We adopt across validation approach for our evaluation.", "labels": [], "entities": []}, {"text": "shows how the data is split into five folds.", "labels": [], "entities": []}, {"text": "In order to evaluate the algorithm's patterns we need a set of sentences to which they can be applied.", "labels": [], "entities": []}, {"text": "Ina traditional QA system architecture,    see e.g., the document or passage retrieval step performs this function.", "labels": [], "entities": []}, {"text": "This step is crucial to a QA system's performance, because it is impossible to locate answers in the subsequent answer extraction step if the passages returned during passage retrieval do not contain the answer in the first place.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.719794824719429}]}, {"text": "This also holds true in our case: the patterns cannot be expected to identify a correct answer if none of the sentences used as input contains the correct answer.", "labels": [], "entities": []}, {"text": "We therefore use two different evaluation sets to evaluate our algorithm: 1.", "labels": [], "entities": []}, {"text": "The first set contains for each question all sentences in the top 100 paragraphs returned by Lucene when using simple queries made up from the question's key words.", "labels": [], "entities": []}, {"text": "It cannot be guaranteed that answers to every question are present in this test set.", "labels": [], "entities": []}, {"text": "2. For the second set, the query additionally list all known correct answers to the question as parts of one OR operator.", "labels": [], "entities": []}, {"text": "This increases the chance that the evaluation set actually contains valid answer sentences significantly.", "labels": [], "entities": []}, {"text": "In order to provide a quantitative characterization of the two evaluation sets we estimated the number of correct answer sentences they contain.", "labels": [], "entities": []}, {"text": "For each paragraph it was determined whether it contained one of the known answer strings and at least of one of the question key words.", "labels": [], "entities": []}, {"text": "Tables 2 and 3 show for each evaluation set how many answers on average it contains per question.", "labels": [], "entities": []}, {"text": "The column \"= 0\" for example shows the fraction of questions for which no valid answer sentence is contained in the evaluation set, while column \">= 90\" gives the fraction of questions with 90 or more valid answer sentences.", "labels": [], "entities": []}, {"text": "The last two columns show mean and median values.", "labels": [], "entities": [{"text": "mean", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9805906414985657}]}], "tableCaptions": [{"text": " Table 2: Fraction of sentences that contain correct answers in Evaluation Set 1 (approximation).", "labels": [], "entities": []}, {"text": " Table 3: Fraction of sentences that contain correct answers in Evaluation Set 2 (approximation).", "labels": [], "entities": []}, {"text": " Table 4: Splits into training and tests sets of the data  used for evaluation. T02 stands for TREC 2002 data  etc. Lin02 is based on (Lin and Katz, 2005). The #  rows show how many question/answer sentence pairs  are used for training and for testing.", "labels": [], "entities": [{"text": "T02", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9434204697608948}, {"text": "TREC 2002 data", "start_pos": 95, "end_pos": 109, "type": "DATASET", "confidence": 0.7322302858034769}]}, {"text": " Table 5: Performance based on evaluation set 1.", "labels": [], "entities": []}, {"text": " Table 6: Performance based on evaluation set 2.", "labels": [], "entities": []}, {"text": " Table 7: Baseline performance based on evaluation set  1.", "labels": [], "entities": []}, {"text": " Table 8: Baseline performance based on evaluation set  2.", "labels": [], "entities": []}, {"text": " Table 9: Top-1 accuracy of the QuALiM system on its  own and when combined with the algorithm described  in this paper. All increases are statistically significant  using a sign test (p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9794014692306519}]}, {"text": " Table 10: Top-1 accuracy of the QuALiM system on  its own and when combined with the algorithm de- scribed in this paper, when only considering questions  for which a pattern could be acquired from the training  data. All increases are statistically significant using a  sign test (p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9861425757408142}]}]}