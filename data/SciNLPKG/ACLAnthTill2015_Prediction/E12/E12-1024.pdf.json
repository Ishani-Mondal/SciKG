{"title": [{"text": "A Probabilistic Model of Syntactic and Semantic Acquisition from Child-Directed Utterances and their Meanings", "labels": [], "entities": [{"text": "Syntactic and Semantic Acquisition", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.6122464090585709}]}], "abstractContent": [{"text": "This paper presents an incremental prob-abilistic learner that models the acquis-tion of syntax and semantics from a corpus of child-directed utterances paired with possible representations of their meanings.", "labels": [], "entities": []}, {"text": "These meaning representations approximate the contextual input available to the child; they do not specify the meanings of individual words or syntactic derivations.", "labels": [], "entities": []}, {"text": "The learner then has to infer the meanings and syntactic properties of the words in the input along with a parsing model.", "labels": [], "entities": []}, {"text": "We use the CCG grammatical framework and train a non-parametric Bayesian model of parse structure with online variational Bayesian expectation maximization.", "labels": [], "entities": [{"text": "parse structure", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.9143880307674408}]}, {"text": "When tested on utterances from the CHILDES corpus, our learner outperforms a state-of-the-art semantic parser.", "labels": [], "entities": [{"text": "CHILDES corpus", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.8805660903453827}]}, {"text": "In addition, it models such aspects of child acquisition as \"fast mapping ,\" while also countering previous criticisms of statistical syntactic learners.", "labels": [], "entities": []}], "introductionContent": [{"text": "Children learn language by mapping the utterances they hear onto what they believe those utterances mean.", "labels": [], "entities": []}, {"text": "The precise nature of the child's prelinguistic representation of meaning is not known.", "labels": [], "entities": [{"text": "prelinguistic representation of meaning", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.7259239703416824}]}, {"text": "We assume for present purposes that it can be approximated by compositional logical representations such as (1), where the meaning is a logical expression that describes a relationship have between the person you refers to and the object another(x, cookie(x)): Utterance : you have another cookie (1) Meaning : have(you, another(x, cookie(x))) Most situations will support a number of plausible meanings, so the child has to learn in the face of propositional uncertainty 1 , from a set of contextually afforded meaning candidates, as here: The task is then to learn, from a sequence of such (utterance, meaning-candidates) pairs, the correct lexicon and parsing model.", "labels": [], "entities": []}, {"text": "Here we present a probabilistic account of this task with an emphasis on cognitive plausibility.", "labels": [], "entities": []}, {"text": "Our criteria for plausibility are that the learner must not require any language-specific information prior to learning and that the learning algorithm must be strictly incremental: it sees each training instance sequentially and exactly once.", "labels": [], "entities": []}, {"text": "We define a Bayesian model of parse structure with Dirichlet process priors and train this on a set of (utterance, meaning-candidates) pairs derived from the CHILDES corpus) using online variational Bayesian EM.", "labels": [], "entities": [{"text": "parse structure", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.9130448698997498}, {"text": "CHILDES corpus", "start_pos": 158, "end_pos": 172, "type": "DATASET", "confidence": 0.8930635154247284}]}, {"text": "We evaluate the learnt grammar in three ways.", "labels": [], "entities": []}, {"text": "First, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9990460276603699}, {"text": "parsing unseen utterances", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.886382520198822}]}, {"text": "We show that it outperforms a state-of-the-art semantic parser () when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by.", "labels": [], "entities": []}, {"text": "We then examine the learning curves of some individual words, showing that the model can learn word meanings on the basis of a single exposure, similar to the fast mapping phenomenon observed in children.", "labels": [], "entities": []}, {"text": "Finally, we show that our learner captures the step-like learning curves for word order regularities that claim children show.", "labels": [], "entities": []}, {"text": "This result counters Thornton and Tesan's criticism of statistical grammar learners-that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}