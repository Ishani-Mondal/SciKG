{"title": [{"text": "Recall-Oriented Learning of Named Entities in Arabic Wikipedia", "labels": [], "entities": [{"text": "Recall-Oriented Learning of Named Entities", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6347206592559814}]}], "abstractContent": [{"text": "We consider the problem of NER in Arabic Wikipedia, a semisupervised domain adaptation setting for which we have no labeled training data in the target domain.", "labels": [], "entities": [{"text": "NER", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9647642970085144}]}, {"text": "To facilitate evaluation, we obtain annotations for articles in four topical groups, allowing annotators to identify domain-specific entity types in addition to standard categories.", "labels": [], "entities": []}, {"text": "Standard supervised learning on newswire text leads to poor target-domain recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9952067732810974}]}, {"text": "We train a sequence model and show that a simple modification to the online learner-a loss function encouraging it to \"arrogantly\" favor recall over precision-substantially improves recall and F 1.", "labels": [], "entities": [{"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9928039908409119}, {"text": "precision-substantially", "start_pos": 149, "end_pos": 172, "type": "METRIC", "confidence": 0.9762969613075256}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9996626377105713}, {"text": "F 1", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9741136431694031}]}, {"text": "We then adapt our model with self-training on unlabeled target-domain data; enforcing the same recall-oriented bias in the self-training stage yields marginal gains.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 95, "end_pos": 110, "type": "METRIC", "confidence": 0.985491931438446}]}], "introductionContent": [{"text": "This paper considers named entity recognition (NER) in text that is different from most past research on NER.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.7875915368398031}]}, {"text": "Specifically, we consider Arabic Wikipedia articles with diverse topics beyond the commonly-used news domain.", "labels": [], "entities": []}, {"text": "These data challenge past approaches in two ways: First, Arabic is a morphologically rich language.", "labels": [], "entities": []}, {"text": "Named entities are referenced using complex syntactic constructions (cf. English NEs, which are primarily sequences of proper nouns).", "labels": [], "entities": []}, {"text": "The Arabic script suppresses most vowels, increasing lexical ambiguity, and lacks capitalization, a key clue for English NER.", "labels": [], "entities": [{"text": "English NER", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.44095782935619354}]}, {"text": "Second, much research has focused on the use of news text for system building and evaluation.", "labels": [], "entities": [{"text": "system building", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.8387236297130585}]}, {"text": "Wikipedia articles are not news, belonging instead to a wide range of domains that are not clearly delineated.", "labels": [], "entities": []}, {"text": "One hallmark of this divergence between Wikipedia and the news domain is a difference in the distributions of named entities.", "labels": [], "entities": []}, {"text": "Indeed, the classic named entity types (person, organization, location) may not be the most apt for articles in other domains (e.g., scientific or social topics).", "labels": [], "entities": []}, {"text": "On the other hand, Wikipedia is a large dataset, inviting semisupervised approaches.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.9214339852333069}]}, {"text": "In this paper, we describe advances on the problem of NER in Arabic Wikipedia.", "labels": [], "entities": [{"text": "NER", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8878562450408936}]}, {"text": "The techniques are general and make use of well-understood building blocks.", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 A small corpus of articles annotated in anew scheme that provides more freedom for annotators to adapt NE analysis to new domains; \u2022 An \"arrogant\" learning approach designed to boost recall in supervised training as well as self-training; and \u2022 An empirical evaluation of this technique as applied to a well-established discriminative NER model and feature set.", "labels": [], "entities": [{"text": "NE analysis", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.9490249156951904}, {"text": "recall", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.9951344132423401}]}, {"text": "Experiments show consistent gains on the challenging problem of identifying named entities in Arabic Wikipedia text.", "labels": [], "entities": [{"text": "identifying named entities in Arabic Wikipedia text", "start_pos": 64, "end_pos": 115, "type": "TASK", "confidence": 0.7804257018225533}]}], "datasetContent": [{"text": "During annotation, two articles (Prussia and Amman) were reserved for training annotators on the task.", "labels": [], "entities": [{"text": "Prussia", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9669870734214783}, {"text": "Amman", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.7396935224533081}]}, {"text": "Once they were accustomed to annotation, both independently annotated a third article.", "labels": [], "entities": []}, {"text": "We used this 4,750-word article ) to measure inter-annotator agreement.", "labels": [], "entities": []}, {"text": "provides scores for tokenlevel agreement measures and entity-level F 1 between the two annotated versions of the article.", "labels": [], "entities": [{"text": "entity-level F 1", "start_pos": 54, "end_pos": 70, "type": "METRIC", "confidence": 0.7252575059731802}]}, {"text": "These measures indicate strong agreement for locating and categorizing NEs both at the token and chunk levels.", "labels": [], "entities": []}, {"text": "Closer examination of agreement scores shows that PER and MIS classes have the lowest rates of agreement.", "labels": [], "entities": [{"text": "PER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7273208498954773}, {"text": "MIS", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.45761290192604065}]}, {"text": "That the miscellaneous class, used for infrequent or articlespecific NEs, receives poor agreement is unsurprising.", "labels": [], "entities": []}, {"text": "The low agreement on the PER class seems to be due to the use of titles and descriptive terms in personal names.", "labels": [], "entities": [{"text": "PER class", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.5879645943641663}]}, {"text": "Despite explicit guidelines to exclude the titles, annotators disagreed on the inclusion of descriptors that disambiguate the NE (e.g., the father in : George Bush, the father).", "labels": [], "entities": []}, {"text": "The position and boundary measures ignore the distinctions between the POLM classes.", "labels": [], "entities": []}, {"text": "To avoid artificial inflation of the token and token position agreement rates, we exclude the 81% of tokens tagged by both annotators as not belonging to an entity.: Custom NE categories suggested by one or both annotators for 10 articles.", "labels": [], "entities": []}, {"text": "Article titles are translated from Arabic.", "labels": [], "entities": []}, {"text": "We investigate two questions in the context of NER for Arabic Wikipedia: \u2022 Loss function: Does integrating a cost function into our learning algorithm, as we have done in the recall-oriented perceptron ( \u00a74.1), improve recall and overall performance on Wikipedia data?", "labels": [], "entities": [{"text": "NER", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9607065320014954}, {"text": "recall", "start_pos": 219, "end_pos": 225, "type": "METRIC", "confidence": 0.9985321760177612}]}, {"text": "\u2022 Semisupervised learning for domain adaptation: Can our models benefit from large amounts of unlabeled Wikipedia data, in addition to the (out-of-domain) labeled data?", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7419626712799072}]}, {"text": "We experiment with a self-training phase following the fully supervised learning phase.", "labels": [], "entities": []}, {"text": "We report experiments for the possible combinations of the above ideas.", "labels": [], "entities": []}, {"text": "These are summarized in table 5.", "labels": [], "entities": []}, {"text": "Note that the recall-oriented perceptron can be used for the supervised learning phase, for the self-training phase, or both.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.9278427958488464}]}, {"text": "This leaves us with the following combinations: \u2022 reg/none (baseline): regular supervised learner.", "labels": [], "entities": []}, {"text": "\u2022 ROP/none: recall-oriented supervised learner.: Tuning the recall-oriented cost parameter for different learning settings.", "labels": [], "entities": [{"text": "ROP", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9863449335098267}]}, {"text": "We optimized for development set F 1 , choosing penalty \u03b2 = 200 for recall-oriented supervised learning (in the plot, ROP/*-this is regardless of whether a stage of self-training will follow); \u03b2 = 100 for recalloriented self-training following recall-oriented supervised learning (ROP/ROP); and \u03b2 = 3200 for recall-oriented self-training following regular supervised learning (reg/ROP).", "labels": [], "entities": [{"text": "ROP", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9677492380142212}]}, {"text": "\u2022 reg/reg: standard self-training setup.", "labels": [], "entities": []}, {"text": "\u2022 ROP/reg: recall-oriented supervised learner, followed by standard self-training.", "labels": [], "entities": [{"text": "ROP", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9787662029266357}, {"text": "recall-oriented", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.9586775898933411}]}, {"text": "\u2022 reg/ROP: regular supervised model as the initial labeler for recall-oriented self-training.", "labels": [], "entities": [{"text": "ROP", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.929607093334198}]}, {"text": "\u2022 ROP/ROP (the \"double ROP\" condition): recalloriented supervised model as the initial labeler for recall-oriented self-training.", "labels": [], "entities": [{"text": "ROP/ROP", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.815193255742391}]}, {"text": "Note that the two ROPs can use different cost parameters.", "labels": [], "entities": []}, {"text": "For evaluating our models we consider the named entity detection task, i.e., recognizing which spans of words constitute entities.", "labels": [], "entities": [{"text": "named entity detection task", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7432762756943703}]}, {"text": "This is measured by per-entity precision, recall, and F 1 . To measure statistical significance of differences between models we use implementation of the paired bootstrap resampler of), taking 10,000 samples for each comparison.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9658981561660767}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9997524619102478}, {"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9946241080760956}]}], "tableCaptions": [{"text": " Table 2: Inter-annotator agreement measurements.", "labels": [], "entities": []}, {"text": " Table 4: Number of words (entity mentions) in data sets.", "labels": [], "entities": []}, {"text": " Table 5: Entity detection precision, recall, and F 1 for each learning setting, microaveraged across the 24 articles  in our Wikipedia test set. Rows differ in the supervised learning condition on the ACE+ANER data (regular  vs. recall-oriented perceptron). Columns indicate whether this supervised learning phase was followed by self- training on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training.", "labels": [], "entities": [{"text": "Entity detection precision", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7291538119316101}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9992930889129639}, {"text": "F 1", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9966093897819519}, {"text": "Wikipedia test set", "start_pos": 126, "end_pos": 144, "type": "DATASET", "confidence": 0.925369918346405}, {"text": "ACE+ANER data", "start_pos": 202, "end_pos": 215, "type": "DATASET", "confidence": 0.8112080097198486}]}]}