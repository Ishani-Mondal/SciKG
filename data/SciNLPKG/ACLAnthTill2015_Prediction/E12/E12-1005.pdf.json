{"title": [{"text": "Evaluating Distributional Models of Semantics for Syntactically Invariant Inference", "labels": [], "entities": [{"text": "Syntactically Invariant Inference", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7495148380597433}]}], "abstractContent": [{"text": "A major focus of current work in distri-butional models of semantics is to construct phrase representations composition-ally from word representations.", "labels": [], "entities": []}, {"text": "However, the syntactic contexts which are modelled are usually severely limited, a fact which is reflected in the lexical-level WSD-like evaluation methods used.", "labels": [], "entities": []}, {"text": "In this paper, we broaden the scope of these models to build sentence-level representations, and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition.", "labels": [], "entities": []}, {"text": "We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8812522888183594}]}, {"text": "We find that the models out-perform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "A number of unsupervised semantic models (, for example) have recently been proposed which are inspired at least in part by the distributional hypothesis-that a word's meaning can be characterized by the contexts in which it appears.", "labels": [], "entities": []}, {"text": "Such models represent word meaning as one or more high-dimensional vectors which capture the lexical and syntactic contexts of the word's occurrences in a training corpus.", "labels": [], "entities": []}, {"text": "Much of the recent work in this area has, following, focused on the notion of compositionality as the litmus test of a truly semantic model.", "labels": [], "entities": []}, {"text": "Compositionality is a natural way to construct representations of linguistic units larger than a word, and it has along history in Montagovian semantics for dealing with argument structure and assembling rich semantical expressions of the kind found in predicate logic.", "labels": [], "entities": []}, {"text": "While compositionality may thus provide a convenient recipe for producing representations of propositionally typed phrases, it is not a necessary condition fora semantic representation.", "labels": [], "entities": []}, {"text": "Rather, that distinction still belongs to the crucial ability to support inference.", "labels": [], "entities": []}, {"text": "It is not the intention of this paper to argue for or against compositionality in semantic representations.", "labels": [], "entities": []}, {"text": "Rather, our interest is in evaluating semantic models in order to determine their suitability for inference tasks.", "labels": [], "entities": []}, {"text": "In particular, we contend that it is desirable and arguably necessary fora compositional semantic representation to support inference invariantly, in the sense that the particular syntactic construction that guided the composition should not matter relative to the representations of syntactically different phrases with the same meanings.", "labels": [], "entities": []}, {"text": "For example, we can assert that John threw the ball and The ball was thrown by John have the same meaning for the purposes of inference, even though they differ syntactically.", "labels": [], "entities": []}, {"text": "An analogy can be drawn to research in image processing, in which it is widely regarded as important for the representations of images to be invariant to rotation and scaling.", "labels": [], "entities": [{"text": "image processing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8079864382743835}]}, {"text": "What we should want is a representation of sentence meaning that is invariant to diathesis, other regular syntactic alternations in the assignment of argument structure, and, ideally, even invariant to other meaningpreserving or near-preserving paraphrases.", "labels": [], "entities": []}, {"text": "Existing evaluations of distributional semantic models fall short of measuring this.", "labels": [], "entities": []}, {"text": "One evaluation approach consists of lexical-level word substitution tasks which primarily evaluate a system's ability to disambiguate word senses within a controlled syntactic environment, for example).", "labels": [], "entities": [{"text": "word substitution", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7268203347921371}]}, {"text": "Another approach is to evaluate parsing accuracy, for example), which is really a formalism-specific approximation to argument structure analysis.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.8557873964309692}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.8329168558120728}, {"text": "argument structure analysis", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.7691454589366913}]}, {"text": "These evaluations may certainly be relevant to specific components of, for example, machine translation or natural language generation systems, but they tell us little about a semantic model's ability to support inference.", "labels": [], "entities": [{"text": "machine translation or natural language generation", "start_pos": 84, "end_pos": 134, "type": "TASK", "confidence": 0.7276873290538788}]}, {"text": "In this paper, we propose a general framework for evaluating distributional semantic models that build sentence representations, and suggest two evaluation methods that test the notion of structurally invariant inference directly.", "labels": [], "entities": []}, {"text": "Both rely on determining whether sentences express the same semantic relation between entities, a crucial step in solving a wide variety of inference tasks like recognizing textual entailment, information retrieval, question answering, and summarization.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 193, "end_pos": 214, "type": "TASK", "confidence": 0.7795625627040863}, {"text": "question answering", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.8746395111083984}, {"text": "summarization", "start_pos": 240, "end_pos": 253, "type": "TASK", "confidence": 0.9917855858802795}]}, {"text": "The first evaluation is a relation classification task, where a semantic model is tested on its ability to recognize whether a pair of sentences both contain a particular semantic relation, such as Company X acquires Company Y. The second task is a question answering task, the goal of which is to locate the sentence in a document that contains the answer.", "labels": [], "entities": [{"text": "relation classification task", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8272601763407389}, {"text": "question answering task", "start_pos": 249, "end_pos": 272, "type": "TASK", "confidence": 0.7815872132778168}]}, {"text": "Here, the semantic model must match the question, which expresses a proposition with a missing argument, to the answer-bearing sentence which contains the full proposition.", "labels": [], "entities": []}, {"text": "We apply these new evaluation protocols to several recent distributional models, extending several of them to build sentence representations.", "labels": [], "entities": []}, {"text": "We find that the models outperform a simple lemma overlap model only slightly, but that combining these models with the lemma overlap model can improve performance.", "labels": [], "entities": []}, {"text": "This result is likely due to weaknesses in current models' ability to deal with issues such as named entities, coreference, and negation, which are not emphasized by existing evaluation methods, but it does suggest that distributional models of semantics can play a more central role in systems that require deep, precise inference.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe a simple, general framework for evaluating semantic models.", "labels": [], "entities": []}, {"text": "Our framework consists of the following components: a semantic model to be evaluated, pairs of sentences that are considered to have high similarity, and pairs of sentences that are considered to have low similarity.", "labels": [], "entities": []}, {"text": "In particular, the semantic model is a binary function, s = M(x, x \u2032 ), which returns a realvalued similarity score, s, given a pair of arbitrary linguistic units (that is, words, phrases, sentences, etc.), x and x \u2032 . Note that this formulation of the semantic model is agnostic to whether the models use compositionality to build a phrase represen-tation from constituent representations, and even to the actual representation used.", "labels": [], "entities": []}, {"text": "The model is tested by applying it to each element in the following two sets: with high similarity} L = {(l, l \u2032 )|l and l \u2032 are linguistic units with low similarity} The resulting sets of similarity scores are: The semantic model is evaluated according to its ability to separate S H and S L . We will define specific measures of separation for the tasks that we propose shortly.", "labels": [], "entities": []}, {"text": "While the particular definitions of \"high similarity\" and \"low similarity\" depend on the task, at the crux of both our evaluations is that two sentences are similar if they express the same semantic relation between a given entity pair, and dissimilar otherwise.", "labels": [], "entities": []}, {"text": "This threshold for similarity is closely tied to the argument structure of the sentence, and allows considerable flexibility in the other semantic content that maybe contained in the sentence, unlike the bidirectional paraphrase detection task.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 218, "end_pos": 243, "type": "TASK", "confidence": 0.7673947910467783}]}, {"text": "Yet it ensures that a consistent and useful distinction for inference is being detected, unlike unconstrained similarity judgments.", "labels": [], "entities": []}, {"text": "Also, compared to word similarity assessments or paraphrase elicitation, determining whether a sentence expresses a semantic relation is a much easier task cognitively for human judges.", "labels": [], "entities": []}, {"text": "This binary judgment does not involve interpreting a numerical scale or coming up with an open-ended set of alternative paraphrases.", "labels": [], "entities": []}, {"text": "It is thus easier to get reliable annotated data.", "labels": [], "entities": []}, {"text": "Below, we present two tasks that instantiate this evaluation framework and choice of similarity threshold.", "labels": [], "entities": []}, {"text": "They differ in that the first is targeted towards recognizing declarative sentences or phrases, while the second is targeted towards a question answering scenario, where one argument in the semantic relation is queried.", "labels": [], "entities": [{"text": "question answering", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7234994322061539}]}, {"text": "We drew a number of recent distributional semantic models to compare in this paper.", "labels": [], "entities": []}, {"text": "We first describe the models and our reimplementation of them, before describing the tasks and the datasets used in detail and the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task 1 results in AUC scores. The values in bold indicate the best performing model for a particular  training corpus. The expected random baseline performance is 0.5.", "labels": [], "entities": [{"text": "AUC", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.6707077026367188}]}, {"text": " Table 2: Task 1 dataset characteristics. N is the total  number of sentences. + is the number of sentences  that express the relation.", "labels": [], "entities": []}, {"text": " Table 3: Task 2 results, in normalized rank scores.  Subset is the cases where lemma overlap does not  achieve a perfect score. The two columns on the right  indicate performance using the sum of the scores from  the lemma overlap and the semantic model. The ex- pected random baseline performance is 0.5.", "labels": [], "entities": []}]}