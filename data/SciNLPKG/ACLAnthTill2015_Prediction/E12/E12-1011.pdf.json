{"title": [{"text": "Can Click Patterns across User's Query Logs Predict Answers to Definition Questions?", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we examined click patterns produced by users of Yahoo!", "labels": [], "entities": []}, {"text": "search engine when prompting definition questions.", "labels": [], "entities": []}, {"text": "Regularities across these click patterns are then utilized for constructing a large and heterogeneous training corpus for answer ranking.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.9397142231464386}]}, {"text": "Ina nutshell, answers are extracted from clicked web-snippets originating from any class of web-site, including Knowledge Bases (KBs).", "labels": [], "entities": []}, {"text": "On the other hand, non-answers are acquired from redundant pieces of text across web-snippets.", "labels": [], "entities": []}, {"text": "The effectiveness of this corpus was assessed via training two state-of-the-art models, wherewith answers to unseen queries were distinguished.", "labels": [], "entities": []}, {"text": "These testing queries were also submitted by search engine users, and their answer candidates were taken from their respective returned web-snippets.", "labels": [], "entities": []}, {"text": "This corpus helped both techniques to finish with an accuracy higher than 70%, and to predict over 85% of the answers clicked by users.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9993133544921875}]}, {"text": "In particular, our results underline the importance of non-KB training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is a well-known fact that definition queries are very popular across users of commercial search engines ().", "labels": [], "entities": [{"text": "definition queries", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8865785002708435}]}, {"text": "The essential characteristic of definition questions is their aim for discovering as much as possible descriptive information about the concept being defined (a.k.a. definiendum, pl. definienda).", "labels": [], "entities": []}, {"text": "Some examples of this kind of query include \"Who is Benjamin Millepied?\" and \"Tell me about Bank of America\".", "labels": [], "entities": [{"text": "Benjamin Millepied?\"", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.8825675447781881}, {"text": "about Bank of America\"", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.659196138381958}]}, {"text": "It is a standard practice of definition question answering (QA) systems to mine KBs (e.g., online encyclopedias and dictionaries) for reliable descriptive information on the definiendum (.", "labels": [], "entities": [{"text": "definition question answering (QA)", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.8508611520131429}]}, {"text": "Normally, these pieces of information (i.e., nuggets) explain different facets of the definiendum (e.g., \"ballet choreographer\" and \"born in Bordeaux\"), and the main idea consists in projecting the acquired nuggets into the set of answer candidates afterwards.", "labels": [], "entities": []}, {"text": "However, the performance of this category of method falls into sharp decline whenever few or no coverage is found across KBs ().", "labels": [], "entities": []}, {"text": "Put differently, this technique usually succeeds in discovering the most relevant facts about the most promiment sense of the definiendum.", "labels": [], "entities": []}, {"text": "But it often misses many pertinent nuggets, especially those that can be paraphrased in several ways; and/or those regarding ancillary senses of the definiendum, which are hardly found in KBs.", "labels": [], "entities": []}, {"text": "As a means of dealing with this, current strategies try to construct general definition models inferred from a collection of definitions coming from the Internet or KBs (Androutsopoulos and).", "labels": [], "entities": []}, {"text": "To a great extent, models exploiting non-KB sources demand considerable annotation efforts, or when the data is obtained automatically, they benefit from empirical thresholds that ensure a certain degree of similarity to an array of KB articles.", "labels": [], "entities": []}, {"text": "These thesholds attempt to trade-off the cleanness of the training material against its coverage.", "labels": [], "entities": []}, {"text": "Moreover, gathering negative samples is also hard as it is not easy to find wide-coverage authoritative sources of non-descriptive information about a particular definiendum.", "labels": [], "entities": []}, {"text": "Our approach has different innovative aspects compared to other research in the area of definition extraction.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.96001797914505}]}, {"text": "It is at the crossroads of query log analysis and QA systems.", "labels": [], "entities": [{"text": "query log analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.712478518486023}]}, {"text": "We study the click behavior of search engines' users with regard to definition questions.", "labels": [], "entities": []}, {"text": "Based on this study, we propose a novel way of acquiring large-scale and heterogeneous training material for this task, which consists of: \u2022 automatically obtaining positive samples in accordance with click patterns of search engine users.", "labels": [], "entities": []}, {"text": "This aids in harvesting a host of descriptions from non-KB sources in conjunction with descriptive information from KBs.", "labels": [], "entities": []}, {"text": "\u2022 automatically acquiring negative data in consonance with redundancy patterns across snippets displayed within search engine results when processing definition queries.", "labels": [], "entities": []}, {"text": "In brief, our experiments reveal that these patterns can be effectively exploited for devising efficient models.", "labels": [], "entities": []}, {"text": "Given the huge amount of amassed data, we additionally contrast the performance of systems built on top of samples originated solely from KB, non-KB, and both combined.", "labels": [], "entities": []}, {"text": "Our comparison corroborates that KBs yield massive trustworthy descriptive knowledge, but they do not bear enough diversity to discriminate all answering nuggets within any kind of text.", "labels": [], "entities": []}, {"text": "Essentially, our experiments unveil that non-KB data is richer and therefore it is useful for discovering more descriptive nuggets than KB material.", "labels": [], "entities": []}, {"text": "But its usage relies on its cleanness and on a negative set.", "labels": [], "entities": []}, {"text": "Many people had these intuitions before, but to the best of our knowledge, we provide the first empirical confirmation and quantification.", "labels": [], "entities": []}, {"text": "The road-map of this paper is as follows: section 2 touches on related works; section 3 digs deeper into click patterns for definition questions, subsequently section 4 explains our corpus construction strategy; section 5 describes our experiments, and section 6 draws final conclusions.", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.743742823600769}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Comparison of performance, the total amount  and origin of training data, and the number of recog- nized descriptions.", "labels": [], "entities": []}]}