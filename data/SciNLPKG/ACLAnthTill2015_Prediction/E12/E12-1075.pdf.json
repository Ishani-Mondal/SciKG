{"title": [{"text": "Syntax-Based Word Ordering Incorporating a Large-Scale Language Model", "labels": [], "entities": [{"text": "Syntax-Based Word Ordering Incorporating", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7950781732797623}]}], "abstractContent": [{"text": "A fundamental problem in text generation is word ordering.", "labels": [], "entities": [{"text": "text generation", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7440447211265564}, {"text": "word ordering", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7780193388462067}]}, {"text": "Word ordering is a com-putationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation.", "labels": [], "entities": [{"text": "Word ordering", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7578980624675751}, {"text": "statistical machine translation", "start_pos": 170, "end_pos": 201, "type": "TASK", "confidence": 0.6837852497895559}]}, {"text": "There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011).", "labels": [], "entities": []}, {"text": "By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task.", "labels": [], "entities": []}, {"text": "One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency.", "labels": [], "entities": []}, {"text": "We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying on-line large-margin training.", "labels": [], "entities": []}, {"text": "Our system significantly improved on the baseline by 3.7 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9995083808898926}]}], "introductionContent": [{"text": "One fundamental problem in text generation is word ordering, which can be abstractly formulated as finding a grammatical order fora multiset of words.", "labels": [], "entities": [{"text": "text generation", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7347408533096313}, {"text": "word ordering", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.756333976984024}]}, {"text": "The word ordering problem can also include word choice, where only a subset of the input words are used to produce the output.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7927852272987366}, {"text": "word choice", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.7509265244007111}]}, {"text": "Word ordering is a difficult problem.", "labels": [], "entities": [{"text": "Word ordering", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7804909944534302}]}, {"text": "Finding the best permutation fora set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem.", "labels": [], "entities": []}, {"text": "In practice, exploring the whole search space of permutations is often prevented by adding constraints.", "labels": [], "entities": []}, {"text": "In phrase-based machine translation (, a distortion limit is used to constrain the position of output phrases.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.6218743224938711}]}, {"text": "In syntax-based machine translation systems such as and, synchronous grammars limit the search space so that polynomial time inference is feasible.", "labels": [], "entities": [{"text": "syntax-based machine translation", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.713200032711029}]}, {"text": "In fluency improvement (, parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local.", "labels": [], "entities": [{"text": "fluency improvement", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8608916997909546}, {"text": "word ordering", "start_pos": 125, "end_pos": 138, "type": "TASK", "confidence": 0.7089154273271561}]}, {"text": "Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search.", "labels": [], "entities": [{"text": "word ordering task", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7958494027455648}]}, {"text": "uses a dependency grammar to solve word ordering, and uses CCG) for word ordering and word choice.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.7240741699934006}, {"text": "word ordering", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.800905853509903}, {"text": "word choice", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.7861953675746918}]}, {"text": "The use of syntax models makes their search problems harder than word permutation using an N -gram language model only.", "labels": [], "entities": []}, {"text": "Both methods apply heuristic search.", "labels": [], "entities": []}, {"text": "Zhang and Clark developed a bottom-up best-first algorithm to build output syntax trees from input words, where search is guided by learning for both efficiency and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9944177865982056}]}, {"text": "The framework is flexible in allowing a large range of constraints to be added for particular tasks.", "labels": [], "entities": []}, {"text": "We extend the work of Zhang and Clark (2011) (Z&C) in two ways.", "labels": [], "entities": []}, {"text": "First, we apply online largemargin training to guide search.", "labels": [], "entities": [{"text": "guide search", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.7638156414031982}]}, {"text": "Compared to the perceptron algorithm on \"constituent level features\" by Z&C, our training algorithm is theoretically more elegant (see Section 3) and converges more smoothly empirically (see Section 5).", "labels": [], "entities": []}, {"text": "Using online large-margin training not only improves the output quality, but also allows the incorporation of an N -gram language-model into the system.", "labels": [], "entities": []}, {"text": "N -gram models have been used as a standard component in statistical machine translation, but have not been applied to the syntactic model of Z&C.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6761984328428904}]}, {"text": "Intuitively, an N -gram model can improve local fluency when added to a syntax model.", "labels": [], "entities": []}, {"text": "Our experiments show that a four-gram model trained using the English GigaWord corpus gave improvements when added to the syntaxbased baseline system.", "labels": [], "entities": [{"text": "English GigaWord corpus", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.767531136671702}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, we improve on the performance of the Z&C system for the challenging task of the general word ordering problem.", "labels": [], "entities": [{"text": "general word ordering problem", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.7254811823368073}]}, {"text": "Second, we develop a novel method for incorporating a large-scale language model into a syntax-based generation system.", "labels": [], "entities": []}, {"text": "Finally, we analyse large-margin training in the context of learning-guided best-first search, offering a novel solution to this computationally hard problem.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use sections 2-21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test.", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9789549112319946}]}, {"text": "Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words.", "labels": [], "entities": []}, {"text": "Following Z&C, we treat base noun phrases (i.e. NP s that do not recursively contain other NPs) as atomic units for the input.", "labels": [], "entities": []}, {"text": "Output sequences are compared with the original sentences to evaluate their quality.", "labels": [], "entities": []}, {"text": "We follow previous work and use the BLEU metric () to compare outputs with references.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9778690338134766}]}, {"text": "Z&C use two methods to construct leaf edges.", "labels": [], "entities": []}, {"text": "The first is to assign lexical categories according to a dictionary.", "labels": [], "entities": []}, {"text": "There are 26.8 lexical categories for each word on average using this method, corresponding to 26.8 leaf edges.", "labels": [], "entities": []}, {"text": "The other method is to use a pre-processing step -a CCG supertagger  standard sequence, assuming that for some problems the ambiguities can be reduced (e.g. when the input is already partly correctly ordered).", "labels": [], "entities": []}, {"text": "Z&C use different probability cutoff levels (the \u03b2 parameter in the supertagger) to control the pruning.", "labels": [], "entities": []}, {"text": "Here we focus mainly on the dictionary method, which leaves lexical category disambiguation entirely to the generation system.", "labels": [], "entities": []}, {"text": "For comparison, we also perform experiments with lexical category pruning.", "labels": [], "entities": []}, {"text": "We chose \u03b2 = 0.0001, which leaves 5.4 leaf edges per word on average.", "labels": [], "entities": []}, {"text": "We used the SRILM Toolkit) to build a true-case 4-gram language model estimated over the CCGBank training and development data and a large additional collection of fluent sentences in the Agence France-Presse (AFP) and Xinhua News Agency (XIN) subsets of the English GigaWord Fourth Edition (), a total of over 1 billion tokens.", "labels": [], "entities": [{"text": "SRILM Toolkit", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.7812255620956421}, {"text": "CCGBank training and development data", "start_pos": 89, "end_pos": 126, "type": "DATASET", "confidence": 0.828623378276825}, {"text": "English GigaWord Fourth Edition", "start_pos": 259, "end_pos": 290, "type": "DATASET", "confidence": 0.7361005693674088}]}, {"text": "The GigaWord data was first pre-processed to replicate the CCGBank tokenization.", "labels": [], "entities": [{"text": "GigaWord data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8675045371055603}, {"text": "CCGBank", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9210094213485718}]}, {"text": "The total number of sentences and tokens in each LM component is shown in.", "labels": [], "entities": []}, {"text": "The language model vocabulary consists of the 46,574 words that occur in the concatenation of the CCGBank training, development, and test sets.", "labels": [], "entities": []}, {"text": "The LM probabilities are estimated using modified Kneser-Ney smoothing) with interpolation of lower n-gram orders.", "labels": [], "entities": []}, {"text": "A set of development test results without lexical category pruning (i.e. using the full dictionary) is shown in.", "labels": [], "entities": []}, {"text": "We train the baseline system and our systems under various settings for 10 iterations, and measure the output BLEU scores after each iteration.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9966310858726501}]}, {"text": "The timeout value for each sentence is set to 5 seconds.", "labels": [], "entities": []}, {"text": "The highest score (max BLEU) and averaged score (avg. BLEU) of each system over the 10 training iterations are shown in the table.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9494908452033997}, {"text": "averaged score (avg. BLEU)", "start_pos": 33, "end_pos": 59, "type": "METRIC", "confidence": 0.8088806470235189}]}, {"text": "The first three rows represent the baseline system, our largin-margin training system (margin), and our system with the N -gram model incorporated using g-precomputed interpolation.", "labels": [], "entities": [{"text": "margin", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.940708577632904}]}, {"text": "For interpolation we manually chose \u03b1 = 0.8, \u03b2 = 0.16 and \u03b3 = 0.04, respectively.", "labels": [], "entities": []}, {"text": "These values could be optimized by development experiments with alternative configurations, which may lead to further improvements.", "labels": [], "entities": []}, {"text": "Our system with large-margin training gives higher BLEU scores than the baseline system consistently overall iterations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9994146823883057}]}, {"text": "The N -gram model led to further improvements.", "labels": [], "entities": []}, {"text": "The last four rows in the table show results of our system with the N -gram model added using test-time interpolation.", "labels": [], "entities": []}, {"text": "The syntax model is trained with the optimal number of iterations, and different \u03b1, \u03b2, and \u03b3 values are used to integrate the language model.", "labels": [], "entities": []}, {"text": "Compared with the system using no N -gram model (margin), test-time interpolation did not improve the accuracies.", "labels": [], "entities": [{"text": "margin", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9623902440071106}, {"text": "accuracies", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9926014542579651}]}, {"text": "The row with \u03b1, \u03b2, \u03b3 = 0 represents our system with the N -gram model loaded, and the scores g four , g tri and g bi computed for each N -gram during decoding, but the scores of edges are computed without using N -gram probabilities.", "labels": [], "entities": []}, {"text": "The scoring model is the same as the syntax model (margin), but the results are lower than the row \"margin\", because computing N -gram probabilities made the system slower, exploring less hypotheses under the same timeout setting.", "labels": [], "entities": [{"text": "margin", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9880281686782837}, {"text": "margin", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.7474283576011658}]}, {"text": "The comparison between g-precomputed interpolation and test-time interpolation shows that the system gives better scores when the syntax model takes into consideration the N -gram model during training.", "labels": [], "entities": []}, {"text": "One question that arises is whether gfree interpolation will outperform g-precomputed interpolation.", "labels": [], "entities": []}, {"text": "g-free interpolation offers the freedom of \u03b1, \u03b2 and \u03b3 during training, and can potentially reach a better combination of the parameter values.", "labels": [], "entities": []}, {"text": "However, the training algorithm failed to converge with g-free interpolation.", "labels": [], "entities": []}, {"text": "One possible explanation is that real-valued features from the language model made our large-margin training harder.", "labels": [], "entities": []}, {"text": "Another possible reason is that our training process with heavy pruning does not accommodate this complex model.", "labels": [], "entities": []}, {"text": "shows a set of development experiments with lexical category pruning (with the supertagger parameter \u03b2 = 0.0001).", "labels": [], "entities": []}, {"text": "The scores of the three different systems are calculated by varying the number of training iterations.", "labels": [], "entities": []}, {"text": "The large-margin training system (margin) gave consistently better scores than the baseline system, and adding a language model (margin +LM) improves the scores further.", "labels": [], "entities": [{"text": "margin", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9507705569267273}]}, {"text": "shows some manually chosen examples for which our system gave significant improvements over the baseline.", "labels": [], "entities": []}, {"text": "For most other sentences the improvements are not as obvious.", "labels": [], "entities": []}, {"text": "For each baseline margin margin +LM as a nonexecutive director Pierre Vinken , 61 years old , will join the board . 29 Nov. 61 years old , the board will join as a nonexecutive director Nov. 29 , Pierre Vinken . as a nonexecutive director Pierre Vinken , 61 years old , will join the board Nov. 29 . Lorillard nor smokers were aware of the Kent cigarettes of any research on the workers who studied the researchers of any research who studied Neither the workers were aware of smokers on the Kent cigarettes nor the researchers Neither Lorillard nor any research on the workers who studied the Kent cigarettes were aware of smokers of the researchers . you But 35 years ago have to recognize that these events took place . recognize But you took place that these events have to 35 years ago . But you have to recognize that these events took place 35 years ago . investors to pour cash into money funds continue in Despite yields recent declines Despite investors , yields continue to pour into money funds recent declines in cash . Despite investors , recent declines in yields continue to pour cash into money funds . yielding The top money funds are currently well over 9 % . The top money funds currently are yielding well over 9 % . The top money funds are yielding well over 9 % currently . where A buffet breakfast , held in the museum was food and drinks to . everyday visitors banned everyday visitors are banned to where A buffet breakfast was held , food and drinks in the museum . A buffet breakfast , everyday visitors are banned to where food and drinks was held in the museum . A Commonwealth Edison spokesman said an administrative nightmare would be tracking down the past 3 12 years that the two million customers have . whose changed tracking A Commonwealth Edison spokesman said that the two million customers whose addresses have changed down during the past 3 12 years would bean administrative nightmare . an administrative nightmare whose addresses would be tracking down A Commonwealth Edison spokesman said that the two million customers have changed during the past 3 12 years . The $ 2.5 billion Byron 1 plant , Ill.", "labels": [], "entities": []}, {"text": ", was completed . near Rockford in 1985 The $ 2.5 billion Byron 1 plant was near completed in Rockford , Ill.", "labels": [], "entities": []}, {"text": ", 1985 . The $ 2.5 billion Byron 1 plant near Rockford , Ill.", "labels": [], "entities": [{"text": "Byron 1 plant near Rockford", "start_pos": 27, "end_pos": 54, "type": "DATASET", "confidence": 0.9159806966781616}]}, {"text": ", was completed in 1985 . will ( During its centennial year , The Wall Street Journal report events of the past century that stand as milestones of American business history . ) as The Wall Street Journal ( During its centennial year , milestones stand of American business history that will report events of the past century . ) During its centennial year events will report , The Wall Street Journal that stand as milestones of American business history ( of the past century ) .: Some chosen examples with significant improvements (supertagger parameter \u03b2 = 0.0001).", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.933897336324056}, {"text": "Wall Street Journal", "start_pos": 185, "end_pos": 204, "type": "DATASET", "confidence": 0.9417678912480673}, {"text": "Wall Street Journal", "start_pos": 382, "end_pos": 401, "type": "DATASET", "confidence": 0.9217437903086344}, {"text": "supertagger parameter \u03b2", "start_pos": 535, "end_pos": 558, "type": "METRIC", "confidence": 0.8389061689376831}]}, {"text": "method, the examples are chosen from the development output with lexical category pruning, after the optimal number of training iterations, with the timeout set to 5s.", "labels": [], "entities": []}, {"text": "We also tried manually selecting examples without lexical category pruning, but the improvements were not as obvious, partly because the overall fluency was lower for all the three systems.", "labels": [], "entities": []}, {"text": "shows a set of examples chosen randomly from the development test outputs of our system with the N -gram model.", "labels": [], "entities": []}, {"text": "The optimal number of training iterations is used, and a timeout of 1 minute is used in addition to the 5s timeout for comparison.", "labels": [], "entities": []}, {"text": "With more time to decode each input, the system gave a BLEU score of 44.61, higher than 41.50 with the 5s timout.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9822856485843658}]}, {"text": "While some of the outputs we examined are reasonably fluent, most are to some extent fragmentary.", "labels": [], "entities": []}, {"text": "In general, the system outputs are still far below human fluency.", "labels": [], "entities": []}, {"text": "Some samples are 2 Part of the reason for some fragmentary outputs is the default output mechanism: partial derivations from the chart are greedily put together when timeout occurs before a goal hypothesis is found.", "labels": [], "entities": []}, {"text": "syntactically grammatical, but are semantically anomalous.", "labels": [], "entities": []}, {"text": "For example, person names are often confused with company names, verbs often take unrelated subjects and objects.", "labels": [], "entities": []}, {"text": "The problem is much more severe for long sentences, which have more ambiguities.", "labels": [], "entities": []}, {"text": "For specific tasks, extra information (such as the source text for machine translation) can be available to reduce ambiguities.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7051719576120377}]}], "tableCaptions": [{"text": " Table 1: Number of sentences and tokens by language  model source.", "labels": [], "entities": []}, {"text": " Table 2: Development experiments without lexical category pruning.", "labels": [], "entities": []}, {"text": " Table 4: Some examples chosen at random from development test outputs without lexical category pruning.", "labels": [], "entities": []}, {"text": " Table 6: Test results with lexical category pruning (su- pertagger parameter \u03b2 = 0.0001).", "labels": [], "entities": [{"text": "su- pertagger parameter \u03b2", "start_pos": 54, "end_pos": 79, "type": "METRIC", "confidence": 0.8051828265190124}]}]}