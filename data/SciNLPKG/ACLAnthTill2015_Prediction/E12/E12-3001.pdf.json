{"title": [{"text": "Improving Pronoun Translation for Statistical Machine Translation", "labels": [], "entities": [{"text": "Improving Pronoun Translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8660105069478353}, {"text": "Statistical Machine Translation", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.8282116254170736}]}], "abstractContent": [{"text": "Machine Translation is a well-established field, yet the majority of current systems translate sentences in isolation, losing valuable contextual information from previously translated sentences in the discourse.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8039329051971436}]}, {"text": "One important type of contextual information concerns who or what a coreferring pronoun corefers to (i.e., its antecedent).", "labels": [], "entities": [{"text": "coreferring pronoun corefers", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.6598921616872152}]}, {"text": "Languages differ significantly in how they achieve coreference, and awareness of antecedents is important in choosing the correct pronoun.", "labels": [], "entities": []}, {"text": "Disregarding a pronoun's antecedent in translation can lead to inappropriate coreferring forms in the target text, seriously degrading a reader's ability to understand it.", "labels": [], "entities": []}, {"text": "This work assesses the extent to which source-language annotation of coreferring pronouns can improve English-Czech Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 116, "end_pos": 153, "type": "TASK", "confidence": 0.7711066951354345}]}, {"text": "As with previous attempts that use this method, the results show little improvement.", "labels": [], "entities": []}, {"text": "This paper attempts to explain why and to provide insight into the factors affecting performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "It is well-known that in many natural languages, a pronoun that corefers must bear similar features to its antecedent.", "labels": [], "entities": []}, {"text": "These can include similar number, gender (morphological or referential), and/or animacy.", "labels": [], "entities": []}, {"text": "If a pronoun and its antecedent occur in the same unit of translation (N-gram or syntactic tree), these agreement features can influence the translation.", "labels": [], "entities": []}, {"text": "But this locality cannot be guaranteed in either phrase-based or syntax-based Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 78, "end_pos": 115, "type": "TASK", "confidence": 0.7902460545301437}]}, {"text": "If it is not within the same unit, a coreferring pronoun will be translated without knowledge of its antecedent, meaning that its translation will simply reflect local frequency.", "labels": [], "entities": []}, {"text": "Incorrectly translating a pronoun can result in readers/listeners identifying the wrong antecedent, which can mislead or confuse them.", "labels": [], "entities": []}, {"text": "There have been two recent attempts to solve this problem within the framework of phrasebased SMT.", "labels": [], "entities": [{"text": "phrasebased SMT", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.48473452031612396}]}, {"text": "Both involve annotation projection, which in this context means annotating coreferential pronouns in the sourcelanguage with features derived from the translation of their aligned antecedents, and then building a translation model of the annotated forms.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7329996526241302}]}, {"text": "When translating a coreferring pronoun in anew source-language text, the antecedent is identified and its translation used (differently in the two attempts cited above) to annotate the pronoun prior to translation.", "labels": [], "entities": []}, {"text": "The aim of this work was to better understand why neither of the previous attempts achieved more than a small improvement in translation quality associated with coreferring pronouns.", "labels": [], "entities": []}, {"text": "Only by understanding this will it be possible to ascertain whether the method of annotation projection is intrinsically flawed or the unexpectedly small improvement is due to other factors.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7031655013561249}]}, {"text": "Errors can arise when: 1. Deciding whether or not a third person pronoun corefers; 2.", "labels": [], "entities": [{"text": "Deciding whether or not a third person pronoun corefers", "start_pos": 26, "end_pos": 81, "type": "TASK", "confidence": 0.5664105084207323}]}, {"text": "Identifying the pronoun antecedent; 3.", "labels": [], "entities": [{"text": "Identifying the pronoun antecedent", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8531443327665329}]}, {"text": "Identifying the head of the antecedent, which serves as the source of its features; 4.", "labels": [], "entities": []}, {"text": "Aligning the source and target texts at the phrase and word levels.", "labels": [], "entities": []}, {"text": "Factoring out the first two decisions would show whether the lack of significant improvement was simply due to imperfect coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.9047386646270752}]}, {"text": "In order to control for these errors several different manually annotated versions of the Penn Wall Street Journal corpus were used, each providing different annotations over the same text.", "labels": [], "entities": [{"text": "Penn Wall Street Journal corpus", "start_pos": 90, "end_pos": 121, "type": "DATASET", "confidence": 0.9738404750823975}]}, {"text": "The BBN Pronoun Coreference and Entity Type corpus) was used to provide coreference information in the sourcelanguage and exclude non-referential pronouns.", "labels": [], "entities": [{"text": "BBN Pronoun Coreference and Entity Type corpus", "start_pos": 4, "end_pos": 50, "type": "DATASET", "confidence": 0.8515262688909259}]}, {"text": "It also formed the source-language side of the parallel training corpus.", "labels": [], "entities": []}, {"text": "The PCEDT 2.0 corpus), which contains a close Czech translation of the Penn Wall Street Journal corpus, provided reference translations for testing and the target-language side of the parallel corpus for training.", "labels": [], "entities": [{"text": "PCEDT 2.0 corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8846143086751302}, {"text": "Penn Wall Street Journal corpus", "start_pos": 71, "end_pos": 102, "type": "DATASET", "confidence": 0.9666313052177429}]}, {"text": "To minimise (although not completely eliminate) errors associated with antecedent head identification (item 3 above), the parse trees in the Penn Treebank 3.0 corpus) were used.", "labels": [], "entities": [{"text": "Penn Treebank 3.0 corpus", "start_pos": 141, "end_pos": 165, "type": "DATASET", "confidence": 0.9888988882303238}]}, {"text": "The gold standard annotation provided by these corpora allowed me to assume perfect identification of coreferring pronouns and coreference resolution and near-perfect antecedent head noun identification.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.8349823355674744}, {"text": "head noun identification", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.7076918085416158}]}, {"text": "These assumptions could not be made if state-ofthe-art methods had been used as they cannot yet achieve sufficiently high levels of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9959631562232971}]}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "The use of pronominal coreference in English and Czech and the problem of anaphora resolution are described in Section 2.", "labels": [], "entities": [{"text": "pronominal coreference", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.6134149134159088}, {"text": "anaphora resolution", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7398141324520111}]}, {"text": "The works of and are discussed in Section 3, and the source-language annotation projection method is described in Section 4.", "labels": [], "entities": [{"text": "source-language annotation projection", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.5983696480592092}]}, {"text": "The results are presented and discussed in Section 5 and future work is outlined in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "No standard method yet exists for evaluating pronoun translation in SMT.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7791076600551605}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9138767123222351}]}, {"text": "Early work focussed on the development of techniques for anaphora resolution and their integration within Machine Translation, with little mention of evaluation.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7939939498901367}, {"text": "Machine Translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.8196258246898651}]}, {"text": "In recent work, evaluation has become much more important.", "labels": [], "entities": [{"text": "evaluation", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.961139440536499}]}, {"text": "Both Le Nagard & and consider and reject BLEU () as ill-suited for evaluating pronoun translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9976400136947632}, {"text": "evaluating pronoun translation", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6388315558433533}]}, {"text": "While Hardmeier & Federico propose and use a strict recall and precision based metric for English-German translation, I found it unsuitable for English-Czech translation, given the highly inflective nature of Czech.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.996337890625}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9796849489212036}, {"text": "English-German translation", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.5102066397666931}]}, {"text": "Given the importance of evaluation to the goal of assessing the effectiveness of annotation projection for improving the translation of coreferring pronouns, I carried out two separate types of evaluation -an automated evaluation which could be applied to the entire test set, and an indepth manual assessment that might provide more information, but could only be performed on a subset of the test set.", "labels": [], "entities": []}, {"text": "The automated evaluation is based on the fact that a Czech pronoun must agree in number and gender with its antecedent.", "labels": [], "entities": []}, {"text": "Thus one can count the number of pronouns in the translation output for which this agreement holds, rather than simply score the output against a single reference translation.", "labels": [], "entities": []}, {"text": "To obtain these figures, the automated evaluation process counted: 1.", "labels": [], "entities": []}, {"text": "Total pronouns in the input English test file.", "labels": [], "entities": [{"text": "English test file", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.7168630063533783}]}, {"text": "The representation of valid Czech translations of English pronouns takes the form of a list provided by an expert in Czech NLP, which ignores case and focusses solely on number and gender.", "labels": [], "entities": []}, {"text": "In contrast, the manual evaluation carried out by that same expert, who is also a native speaker of Czech, was used to determine whether deviations from the single reference translation provided in the PCEDT 2.0 corpus were valid alternatives or simply poor translations.", "labels": [], "entities": [{"text": "PCEDT 2.0 corpus", "start_pos": 202, "end_pos": 218, "type": "DATASET", "confidence": 0.9040673772493998}]}, {"text": "The following judgements were provided: 1.", "labels": [], "entities": []}, {"text": "Whether the pronoun had been translated correctly, or in the case of a dropped pronoun, whether pro-drop was appropriate; 2.", "labels": [], "entities": []}, {"text": "If the pronoun translation was incorrect, whether a native Czech speaker would still be able to derive the meaning; 3.", "labels": [], "entities": []}, {"text": "For input to the Annotated system, whether the pronoun had been correctly annotated with respect to the Czech translation of its identified antecedent; 4.", "labels": [], "entities": [{"text": "Annotated system", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.8587119579315186}]}, {"text": "Where an English pronoun was translated differently by the Baseline and Annotated systems, which was better.", "labels": [], "entities": []}, {"text": "If both translated an English pronoun to a valid Czech translation, equal correctness was assumed.", "labels": [], "entities": [{"text": "correctness", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9829509854316711}]}, {"text": "In order to ensure that the manual assessor was directed to the Czech translations aligned to the English pronouns, additional markup was automatically inserted into the English and Czech texts: (1) coreferential pronouns in both English and Czech texts were marked with the head noun of their antecedent (denoted by *), and (2) coreferential pronouns in the English source texts were marked with the Czech translation of the antecedent head, and those in the Czech target texts were marked with the original English pronoun that they were aligned to: English text input to the Baseline system: the u.s.", "labels": [], "entities": []}, {"text": ", claiming some success in its trade diplomacy , ...", "labels": [], "entities": [{"text": "trade diplomacy", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.8838496804237366}]}, {"text": "Czech translation output by the Baseline system: usa , tvrd\u00ed n\u011bkte\u0159\u00ed jej\u00ed(its) obchodn\u00edobchodn\u00ed\u00b4obchodn\u00ed\u00fasp\u011bch v diplomacii , ...", "labels": [], "entities": []}, {"text": "English text input to the Annotated system: the u.s.* , claiming some success in its(u.s.,usa).mascin.pl trade diplomacy , ...", "labels": [], "entities": [{"text": "Annotated system", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.9168392419815063}, {"text": "trade diplomacy", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.6420849114656448}]}, {"text": "Czech translation output by the Annotated system: usa ,* tvrd\u00ed n\u011bkte\u0159\u00edn\u011bkte\u0159\u00ed\u00b4n\u011bkte\u0159\u00ed\u00fasp\u011bchu ve sv\u00e9(its.mascin.pl) obchodn\u00ed diplomacii , ...", "labels": [], "entities": []}, {"text": "Automated evaluation of both \"Development\" and \"Final\" test sets (see) shows that even factoring out the problems of accurate identification of coreferring pronouns, coreference resolution and antecedent head-finding, does not improve performance of the Annotated system much above that of the Baseline.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.9291618764400482}]}, {"text": "Taking the accuracy of pronoun translation to be the proportion of coreferential English pronouns having a valid Czech translation that agrees in both number and gender with their antecedent, yields the following on the two test sets: Baseline system: Development -44/141 (31.21%) Final -142/331 (42.90%) Annotated system: Development -46/141 (32.62%) Final -146/331 (44.10%) There are, however, several reasons for not taking this evaluation as definitive.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9989604949951172}, {"text": "pronoun translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.716245025396347}, {"text": "Annotated", "start_pos": 305, "end_pos": 314, "type": "METRIC", "confidence": 0.9674030542373657}]}, {"text": "Firstly, it relies on the accuracy of the word alignments output by the decoder to identify the Czech translations of the English pronoun and its antecedent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9989672899246216}]}, {"text": "Secondly, these results fail to capture variation between the translations produced by the Baseline and Annotated systems.", "labels": [], "entities": []}, {"text": "Whilst there is a fairly high degree of overlap, for approximately 1/3 of the \"Development\" set pronouns and 1/6 of the \"Final\" set pronouns, the Czech translation is different.", "labels": [], "entities": [{"text": "overlap", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9754887223243713}]}, {"text": "Since the goal of this work was to understand what is needed in order to improve the translation of coreferential pronouns, manual evaluation was critical for understanding the potential capabilities of source-side annotation.", "labels": [], "entities": [{"text": "translation of coreferential pronouns", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.829375222325325}]}, {"text": "The sample files provided for manual evaluation contained 31 pronouns for which the translations provided by the two systems differed (differences) and 72 for which the translation provided by the systems was the same (matches).", "labels": [], "entities": []}, {"text": "Thus, the sample comprised 103 of the 472 coreferential pronouns (about 22%) from across both test sets.", "labels": [], "entities": []}, {"text": "Of this sample, it is the differences that indicate the relative performance of the two systems.", "labels": [], "entities": []}, {"text": "Of the 31 pronouns in this set, 16 were 3 rd -person pronouns, 2 were reflexive personal pronouns and 13 were possessive pronouns.", "labels": [], "entities": []}, {"text": "The results corresponding to evaluation criterion 4 in Section 4.4 provide a comparison of the overall quality of pronoun translation for both systems.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.751856118440628}]}, {"text": "These results for the \"Development\" and \"Final\" test sets (see) suggest that the performance of the Annotated system is comparable with, and even marginally better than, that of the Baseline system, especially when the pronoun annotation is correct.", "labels": [], "entities": [{"text": "Annotated system", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.7199544310569763}]}, {"text": "An example of where the Annotated system produces a better translation than the Baseline system is: Annotated English: he said mexico could be one of the next countries to be removed from the priority list because of its.neut.sg efforts to craft anew patent law . Baseline translation: \u02c7 rekl , \u02c7 ze mexiko by mohl b\u00b4ytb\u00b4yt jeden z dal\u0161\u00edch zem\u00ed , aby byl odvol\u00e1n z prioritou seznam , proto\u017ee jej\u00ed snahy podpo\u0159it nov\u00e9 patentov\u00b4ypatentov\u00b4y z\u00e1kon . Annotated translation: \u02c7 rekl , \u02c7 ze mexiko by mohl b\u00b4ytb\u00b4yt jeden z dal\u0161\u00edch zem\u00ed , aby byl odvol\u00e1n z prioritou seznam , proto\u017ee jeho snahy podpo\u0159it nov\u00e9 patentov\u00b4ypatentov\u00b4y z\u00e1kon . In this example, the English pronoun \"its\", which refers to \"mexico\" is annotated as neuter and singular (as extracted from the Baseline translation).", "labels": [], "entities": []}, {"text": "Both systems translate \"mexico\" as \"mexiko\" (neuter, singular) but differ in their translation of the pronoun.", "labels": [], "entities": []}, {"text": "The Baseline system translates \"its\" incorrectly as \"jej\u00ed\" (feminine, singular), whereas the Annotated system produces the more correct translation: \"jeho\" (neuter, singular), which agrees with the antecedent in both number and gender.", "labels": [], "entities": [{"text": "Annotated", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.8247672319412231}]}, {"text": "An analysis of the judgements on the remaining three evaluation criteria (outlined in Section 4.4) for the 31 differences provides further information.", "labels": [], "entities": []}, {"text": "The Baseline system appears to be more accurate, with 19 pronouns either correctly translated (in terms of number and gender) or appropriately dropped, compared with 17 for the Annotated system.", "labels": [], "entities": []}, {"text": "Of those pronouns, the meaning could still be understood for 7/12 for the Baseline system compared with 8/14 for the Annotated system.", "labels": [], "entities": [{"text": "Annotated system", "start_pos": 117, "end_pos": 133, "type": "DATASET", "confidence": 0.9080013930797577}]}, {"text": "On the surface this may seem strange but it appears to be due to a small number of cases in which the translations produced by both systems were incorrect but those produced by the Annotated system were deemed to be marginally better.", "labels": [], "entities": [{"text": "Annotated system", "start_pos": 181, "end_pos": 197, "type": "DATASET", "confidence": 0.9312461614608765}]}, {"text": "Due to the small sample size it is difficult to form a complete picture of where one system may perform consistently better than the other.", "labels": [], "entities": []}, {"text": "The annotation of both number and gender was accurate for 18 pronouns.", "labels": [], "entities": []}, {"text": "Whilst this accuracy is not particularly high, the results (see) suggest that translation is more accurate for those pronouns that are correctly annotated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993839263916016}, {"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9524829983711243}]}, {"text": "Whilst pro-drop in Czech was not explicitly handled in the annotation process, manual evaluation revealed that both systems were able to successfully 'learn' a few (local) scenarios in which pro-drop is appropriate.", "labels": [], "entities": []}, {"text": "This was unexpected but found to be due to instances in which there are short distances between the pronoun and verb in English.", "labels": [], "entities": []}, {"text": "For example, many of the occurrences of \"she\" in English appear in the context of \"she said...\" and are translated correctly with the verb form \"...\u02c7 rekla...\".", "labels": [], "entities": []}, {"text": "An example of where the Annotated system correctly drops a pronoun is: rect identification of the English antecedent head noun, (2) incorrect identification of the Czech translation of the antecedent head noun in the Baseline output due to errors in the word alignments, and (3) errors in the PCEDT 2.0 alignment file (affecting training only).", "labels": [], "entities": [{"text": "rect identification of the English antecedent head noun", "start_pos": 71, "end_pos": 126, "type": "TASK", "confidence": 0.8956066593527794}, {"text": "PCEDT 2.0 alignment file", "start_pos": 293, "end_pos": 317, "type": "DATASET", "confidence": 0.878199577331543}]}, {"text": "While \"perfect\" annotation of the BBN Pronoun Coreference and Entity Type, the PCEDT 2.0 and the Penn Treebank 3.0 corpora has been assumed, errors in these corpora cannot be completely ruled out.", "labels": [], "entities": [{"text": "BBN Pronoun Coreference", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.9025335311889648}, {"text": "PCEDT 2.0", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.903412789106369}, {"text": "Penn Treebank 3.0 corpora", "start_pos": 97, "end_pos": 122, "type": "DATASET", "confidence": 0.9838399142026901}]}], "tableCaptions": [{"text": " Table 1: Sizes of the training and testing datasets", "labels": [], "entities": [{"text": "Sizes", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.9862715005874634}]}, {"text": " Table 2: Automated Evaluation Results for both test sets", "labels": [], "entities": []}]}