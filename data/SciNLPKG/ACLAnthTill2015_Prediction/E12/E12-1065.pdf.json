{"title": [{"text": "Character-based Kernels for Novelistic Plot Structure", "labels": [], "entities": []}], "abstractContent": [{"text": "Better representations of plot structure could greatly improve computational methods for summarizing and generating stories.", "labels": [], "entities": [{"text": "summarizing and generating stories", "start_pos": 89, "end_pos": 123, "type": "TASK", "confidence": 0.8549131155014038}]}, {"text": "Current representations lack abstraction , focusing too closely on events.", "labels": [], "entities": []}, {"text": "We present a kernel for comparing novelistic plots at a higher level, in terms of the cast of characters they depict and the social relationships between them.", "labels": [], "entities": []}, {"text": "Our kernel compares the characters of different novels to one another by measuring their frequency of occurrence overtime and the descriptive and emotional language associated with them.", "labels": [], "entities": []}, {"text": "Given a corpus of 19th-century novels as training data, our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Every culture has stories, and storytelling is one of the key functions of human language.", "labels": [], "entities": []}, {"text": "Yet while we have robust, flexible models for the structure of informative documents (for instance; Abu Jbara and Radev, 2011)), current approaches have difficulty representing the narrative structure of fictional stories.", "labels": [], "entities": []}, {"text": "This causes problems for any task requiring us to model fiction, including summarization and generation of stories; show that state-of-the-art summarizers perform extremely poorly on short fictional texts . A major problem with applying models for informative text to fiction is that the most important structure underlying the narrative-its plot-occurs at a high level of abstraction, while the actual narration is of a series of lower-level events.", "labels": [], "entities": [{"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9895720481872559}]}, {"text": "A short synopsis of Jane Austen's novel Pride and Prejudice, for example, is that Elizabeth Bennet first thinks Mr. Darcy is arrogant, but later grows to love him.", "labels": [], "entities": []}, {"text": "But this is not stated straightforwardly in the text; the reader must infer it from the behavior of the characters as they participate in various everyday scenes.", "labels": [], "entities": []}, {"text": "In this paper, we present the plot kernel, a coarse-grained, but robust representation of novelistic plot structure.", "labels": [], "entities": []}, {"text": "The kernel evaluates the similarity between two novels in terms of the characters and their relationships, constructing functional analogies between them.", "labels": [], "entities": []}, {"text": "These are intended to correspond to the labelings produced by human literary critics when they write, for example, that Elizabeth Bennet and Emma Woodhouse are protagonists of their respective novels.", "labels": [], "entities": []}, {"text": "By focusing on which characters and relationships are important, rather than specifically how they interact, our system can abstract away from events and focus on more easily-captured notions of what makes a good story.", "labels": [], "entities": []}, {"text": "The ability to find correspondences between characters is key to eventually summarizing or even generating interesting stories.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9772579073905945}]}, {"text": "Once we can effectively model the kinds of people a romance or an adventure story is usually about, and what kind of relationships should exist between them, we can begin trying to analyze new texts by comparison with familiar ones.", "labels": [], "entities": []}, {"text": "In this work, we evaluate our system on the comparatively easy task projects/autosummarize.", "labels": [], "entities": []}, {"text": "Although this cannot be treated as a scientific experiment, the results are unusably bad; they consist mostly of short exclamations containing the names of major characters. of recognizing acceptable novels (section 6), but recognition is usually a good first step toward generation-a recognition model can always be used as part of a generate-and-rank pipeline, and potentially its underlying representation can be used in more sophisticated ways.", "labels": [], "entities": []}, {"text": "We show a detailed analysis of the character correspondences discovered by our system, and discuss their potential relevance to summarization, in section 9.", "labels": [], "entities": [{"text": "summarization", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.9845743775367737}]}], "datasetContent": [{"text": "We focus on the 19th century novel, partly following  and partly because these texts are freely available via Project Gutenberg.", "labels": [], "entities": []}, {"text": "Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair).", "labels": [], "entities": []}, {"text": "We select 41 texts, taking 11 as a development set and the remaining 30 as a test set; a complete list is given in Appendix A. We focus on the novels used in , but in some cases add additional romances by an already-included author.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.6986000537872314}]}, {"text": "We also selected 10 of the least romantic works as an outof-domain set; experiments on these are in section 8.", "labels": [], "entities": []}, {"text": "We evaluate our kernels on their ability to distinguish between real novels from our dataset and artificial surrogate novels of three types.", "labels": [], "entities": []}, {"text": "First, we alter the order of areal novel by permuting its chapters before computing features.", "labels": [], "entities": []}, {"text": "We construct one uniformally-random permutation for each test novel.", "labels": [], "entities": []}, {"text": "Second, we change the identities of the characters by reassigning the temporal features for the different characters uniformally at random while leaving the unigram features unaltered.", "labels": [], "entities": []}, {"text": "(For example, we might assign the frequency, emotion and relationship curves for \"Mr. Collins\" to \"Miss Elizabeth Bennet\" instead.)", "labels": [], "entities": [{"text": "frequency", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9929732084274292}]}, {"text": "Again, we produce one test instance of this type for each test novel.", "labels": [], "entities": []}, {"text": "Third, we experiment with a more difficult ordering task by taking the chapters in reverse.", "labels": [], "entities": []}, {"text": "In each case, we use our kernel to perform a ranking task, deciding whether k(x, y) > k(x, y perm ).", "labels": [], "entities": []}, {"text": "Since this is a binary forced-choice classification, a random baseline would score 50%.", "labels": [], "entities": []}, {"text": "We evaluate performance in the case where we are given only a single training document x, and fora whole training set X, in which case we combine the decisions using a weighted nearest neighbor (WNN) strategy: In each case, we perform the experiment in a leave-one-out fashion; we include the 11 development documents in X, but not in the test set.", "labels": [], "entities": []}, {"text": "Thus there are 1200 single-document comparisons and 30 with WNN.", "labels": [], "entities": [{"text": "WNN", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9383562207221985}]}, {"text": "The results of our three systems (the baseline, the first-order kernel k 1 and the second-order kernel k 2 ) are shown in.", "labels": [], "entities": []}, {"text": "(The sentiment-only baseline has no characterspecific features, and so cannot perform the character task.)", "labels": [], "entities": []}, {"text": "Using the full dataset and second-order kernel k 2 , our system's performance on these tasks is quite good; we are correct 90% of the time for order and character examples, and 67% for the  more difficult reverse cases.", "labels": [], "entities": []}, {"text": "Results of this quality rely heavily on the WNN strategy, which trusts close neighbors more than distant ones.", "labels": [], "entities": []}, {"text": "In the single training point setup, the system is much less accurate.", "labels": [], "entities": []}, {"text": "In this setting, the system is forced to make decisions for all pairs of texts independently, including pairs it considers very dissimilar because it has failed to find any useful correspondences.", "labels": [], "entities": []}, {"text": "Performance for these pairs is close to chance, dragging down overall scores (52% for reverse) even if the system performs well on pairs where it finds good correspondences, enabling a higher WNN score (67%).", "labels": [], "entities": []}, {"text": "The reverse case is significantly harder than order.", "labels": [], "entities": []}, {"text": "This is because randomly permuting a novel actually breaks up the temporal continuity of the text-for instance, a minor character who appeared in three adjacent chapters might now appear in three separate places.", "labels": [], "entities": []}, {"text": "Reversing the text does not cause this kind of disruption, so correctly detecting a reversal requires the system to represent patterns with a distinct temporal orientation, for instance an intensification in the main character's emotions, or in the number of paragraphs focusing on pairwise relationships, toward the end of the text.", "labels": [], "entities": [{"text": "detecting a reversal", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7056156694889069}]}, {"text": "The baseline system is ineffective at detecting either ordering or reversals . The first-order kernel k 1 is as good ask 2 in detecting character permutations, but less effective on reorderings and reversals.", "labels": [], "entities": []}, {"text": "As we will show in section 9, k 1 places more emphasis on correspondences between minor characters and between places, while k 2 is more sensitive to protagonists and their relationships, which carry the richest temporal informa-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top five stemmed unigram dependency fea- tures for \"Miss Elizabeth Bennet\", protagonist of  Pride and Prejudice, and their frequencies.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of kernels ranking 30 real novels  against artificial surrogates (chance accuracy 50%).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9900688529014587}, {"text": "chance accuracy", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.7681923806667328}]}, {"text": " Table 3: Accuracy of kernels ranking 10 non-romance  novels against artificial surrogates, with 41 romances  used for comparison.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9853571057319641}]}]}