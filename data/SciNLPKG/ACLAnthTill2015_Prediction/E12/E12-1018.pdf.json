{"title": [{"text": "Tree Representations in Probabilistic Models for Extended Named Entities Detection", "labels": [], "entities": [{"text": "Extended Named Entities Detection", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.6499770283699036}]}], "abstractContent": [{"text": "In this paper we deal with Named Entity Recognition (NER) on transcriptions of French broadcast data.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.80106254418691}, {"text": "French broadcast data", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.6047568917274475}]}, {"text": "Two aspects make the task more difficult with respect to previous NER tasks: i) named entities annotated used in this work have a tree structure, thus the task cannot be tackled as a sequence labelling task; ii) the data used are more noisy than data used for previous NER tasks.", "labels": [], "entities": []}, {"text": "We approach the task in two steps, involving Conditional Random Fields and Probabilis-tic Context-Free Grammars, integrated in a single parsing algorithm.", "labels": [], "entities": []}, {"text": "We analyse the effect of using several tree representations.", "labels": [], "entities": []}, {"text": "Our system outperforms the best system of the evaluation campaign by a significant margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity Recognition is a traditinal task of the Natural Language Processing domain.", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7322733700275421}]}, {"text": "The task aims at mapping words in a text into semantic classes, suchlike persons, organizations or localizations.", "labels": [], "entities": []}, {"text": "While at first the NER task was quite simple, involving a limited number of classes, along the years the task complexity increased as more complex class taxonomies were defined).", "labels": [], "entities": [{"text": "NER task", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.914494663476944}]}, {"text": "The interest in the task is related to its use in complex frameworks for (semantic) content extraction, suchlike Relation Extraction applications ().", "labels": [], "entities": [{"text": "semantic) content extraction", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.7362044304609299}, {"text": "Relation Extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.8240546882152557}]}, {"text": "This work presents research on a Named Entity Recognition task defined with anew set of named entities.", "labels": [], "entities": [{"text": "Named Entity Recognition task", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7325976714491844}]}, {"text": "The characteristic of such set is in that named entities have a tree structure.", "labels": [], "entities": []}, {"text": "As concequence the task cannot be tackled as a sequence labelling approach.", "labels": [], "entities": []}, {"text": "Additionally, the use of noisy data like transcriptions of French broadcast data, makes the task very challenging for traditional NLP solutions.", "labels": [], "entities": []}, {"text": "To deal with such problems, we adopt a two-steps approach, the first being realized with Conditional Random Fields (CRF)), the second with a Probabilistic Context-Free Grammar (PCFG)).", "labels": [], "entities": []}, {"text": "The motivations behind that are: \u2022 Since the named entities have a tree structure, it is reasonable to use a solution coming from syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7391988337039948}]}, {"text": "However preliminary experiments using such approaches gave poor results.", "labels": [], "entities": []}, {"text": "\u2022 Despite the tree-structure of the entities, trees are not as complex as syntactic trees, thus, before designing an ad-hoc solution for the task, which require a remarkable effort and yet it doesn't guarantee better performances, we designed a solution providing good results and which required a limited development effort.", "labels": [], "entities": []}, {"text": "\u2022 Conditional Random Fields are models robust to noisy data, like automatic transcriptions of ASR systems (, thus it is the best choice to deal with transcriptions of broadcast data.", "labels": [], "entities": []}, {"text": "Once words have been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 171, "end_pos": 175, "type": "DATASET", "confidence": 0.9571620225906372}]}, {"text": "The two models are integrated in a single parsing algorithm.", "labels": [], "entities": []}, {"text": "We analyze the effect of the use of  several tree representations, which result in different parsing models with different performances.", "labels": [], "entities": []}, {"text": "We provide a detailed evaluation of our models.", "labels": [], "entities": []}, {"text": "Results can be compared with those obtained in the evaluation campaign where the same data were used.", "labels": [], "entities": []}, {"text": "Our system outperforms the best system of the evaluation campaign by a significant margin.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: in the next section we introduce the extended named entities used in this work, in section 3 we describe our two-steps algorithm for parsing entity trees, in section 4 we detail the second step of our approach based on syntactic parsing approaches, in particular we describe the different tree representations used in this work to encode entity trees in parsing models.", "labels": [], "entities": [{"text": "parsing entity trees", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.9048210183779398}]}, {"text": "In section 6 we describe and comment experiments, and finally, in section 7, we draw some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe experiments performed to evaluate our models.", "labels": [], "entities": []}, {"text": "We first describe the settings used for the two models involved in the entity tree parsing, and then describe and comment the results obtained on the test corpus.", "labels": [], "entities": [{"text": "entity tree parsing", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.6848714749018351}]}, {"text": "All results are expressed in terms of Slot Error Rate (SER) () which has a similar definition of word error rate for ASR systems, with the difference that substitution errors are split in three types: i) correct entity type with wrong segmentation; ii) wrong entity type with correct segmentation; iii) wrong entity type with wrong segmentation; here, i) and ii) are given half points, while iii), as well as insertion and deletion errors, are given full points.", "labels": [], "entities": [{"text": "Slot Error Rate (SER)", "start_pos": 38, "end_pos": 59, "type": "METRIC", "confidence": 0.7748191008965174}, {"text": "word error rate", "start_pos": 97, "end_pos": 112, "type": "METRIC", "confidence": 0.6814305384953817}]}, {"text": "Moreover, results are given using the well known F 1 measure, defined as a function of precision and recall.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9656877915064493}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9993947744369507}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9976379871368408}]}, {"text": "In this section we evaluate the models in terms of the evaluation metrics described in previous section, Slot Error Rate (SER) and F1 measure.", "labels": [], "entities": [{"text": "Slot Error Rate (SER)", "start_pos": 105, "end_pos": 126, "type": "METRIC", "confidence": 0.8506491382916769}, {"text": "F1 measure", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9919270575046539}]}, {"text": "In order to evaluate PCFG models alone, we performed entity tree parsing using as input reference transcriptions, i.e. manual transcriptions and reference component annotations taken from development and test sets.", "labels": [], "entities": [{"text": "entity tree parsing", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.6435071527957916}]}, {"text": "This can be considered a kind of oracle evaluations and provides us an upper bound of the performance of the PCFG models.", "labels": [], "entities": []}, {"text": "Results for this evaluation are reported in.", "labels": [], "entities": []}, {"text": "As it can be intuitively expected, adding more contextualization in the trees results in more accurate models, the simplest model, baseline, has the worst oracle performance, filler-parent and parent-context models, adding similar contextualization information, have very similar oracle performances.", "labels": [], "entities": []}, {"text": "Same line of reasoning applies to models parent-node and parent-node-filler, which also add similar contextualization and have very similar oracle predictions.", "labels": [], "entities": []}, {"text": "These last two models have also the best absolute oracle performances.", "labels": [], "entities": []}, {"text": "However, adding more contextualization in the trees results also in more rigid models, the fact that models are robust on reference transcriptions and based on reference component annotations, doesn't imply a proportional robustness on component sequences generated by CRF models.", "labels": [], "entities": []}, {"text": "This intuition is confirmed from results reported in table 5, where areal evaluation of our models is reported, using this time CRF output components as input to PCFG models, to parse entity trees.", "labels": [], "entities": []}, {"text": "The results reported in table 5 show in particular that models using baseline, filler-parent and parent-context tree representations have similar performances, especially on test set.", "labels": [], "entities": []}, {"text": "Models characterized by parent-node and parent-node-filler tree representations have indeed the best performances, although the gain with respect to the other models is not as much as it could be expected given the difference in the oracle performances discussed above.", "labels": [], "entities": []}, {"text": "In particular the best absolute performance is obtained with the model parent-node-filler.", "labels": [], "entities": []}, {"text": "As we mentioned in subsection 4.1, this model represents the best trade-off between rigidity and accuracy using the same label for all entity fillers, but still distinguishing between fillers found in entity structures and other fillers found in words not instantiating any entity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9990359544754028}]}], "tableCaptions": [{"text": " Table 1: Statistics on the training and development sets of the", "labels": [], "entities": []}, {"text": " Table 2: Statistics on the test set of the Quaero corpus, divided in", "labels": [], "entities": [{"text": "Quaero corpus", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8760806918144226}]}, {"text": " Table 3: Statistics showing the characteristics of the different", "labels": [], "entities": []}, {"text": " Table 4: Results computed from oracle predictions obtained with", "labels": [], "entities": []}, {"text": " Table 5: Results obtained with our combined algorithm based on", "labels": [], "entities": []}, {"text": " Table 6: Results obtained with our combined algorithm based on", "labels": [], "entities": []}]}