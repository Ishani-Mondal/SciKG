{"title": [{"text": "Generalization Methods for In-Domain and Cross-Domain Opinion Holder Extraction", "labels": [], "entities": [{"text": "Cross-Domain Opinion Holder Extraction", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.6985174939036369}]}], "abstractContent": [{"text": "In this paper, we compare three different generalization methods for in-domain and cross-domain opinion holder extraction being simple unsupervised word clustering, an induction method inspired by distant supervision and the usage of lexical resources.", "labels": [], "entities": [{"text": "cross-domain opinion holder extraction", "start_pos": 83, "end_pos": 121, "type": "TASK", "confidence": 0.6479637622833252}, {"text": "word clustering", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.7207590043544769}]}, {"text": "The generalization methods are incorporated into diverse classifiers.", "labels": [], "entities": []}, {"text": "We show that generalization causes significant improvements and that the impact of improvement depends on the type of classifier and on how much training and test data differ from each other.", "labels": [], "entities": [{"text": "generalization", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9734067320823669}]}, {"text": "We also address the less common case of opinion holders being realized in patient position and suggest approaches including a novel (linguistically-informed) extraction method how to detect those opinion holders without labeled training data as standard datasets contain too few instances of this type.", "labels": [], "entities": []}], "introductionContent": [{"text": "Opinion holder extraction is one of the most important subtasks in sentiment analysis.", "labels": [], "entities": [{"text": "Opinion holder extraction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8349917729695638}, {"text": "sentiment analysis", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.9545617401599884}]}, {"text": "The extraction of sources of opinions is an essential component for complex real-life applications, such as opinion question answering systems or opinion summarization systems (.", "labels": [], "entities": [{"text": "opinion question answering", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.7365688681602478}, {"text": "opinion summarization", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.7335413992404938}]}, {"text": "Common approaches designed to extract opinion holders are based on data-driven methods, in particular supervised learning.", "labels": [], "entities": [{"text": "extract opinion holders", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8758280475934347}]}, {"text": "In this paper, we examine the role of generalization for opinion holder extraction in both indomain and cross-domain classification.", "labels": [], "entities": [{"text": "opinion holder extraction", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.6628192563851675}, {"text": "cross-domain classification", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.7445061504840851}]}, {"text": "Generalization may not only help to compensate the availability of labeled training data but also conciliate domain mismatches.", "labels": [], "entities": []}, {"text": "In order to illustrate this, compare for instance (1) and (2).", "labels": [], "entities": []}, {"text": "(1) Malaysia did not agree to such treatment of Al-Qaeda soldiers as they were prisoners-of-war and should be accorded treatment as provided for under the Geneva Convention.", "labels": [], "entities": []}, {"text": "(2) Japan wishes to build a $21 billion per year aerospace industry centered on commercial satellite development.", "labels": [], "entities": []}, {"text": "Though both sentences contain an opinion holder, the lexical items vary considerably.", "labels": [], "entities": []}, {"text": "However, if the two sentences are compared on the basis of some higher level patterns, some similarities become obvious.", "labels": [], "entities": []}, {"text": "In both cases the opinion holder is an entity denoting a person and this entity is an agent 1 of some predictive predicate (i.e. agree in (1) and wishes in (2)), more specifically, an expression that indicates that the agent utters a subjective statement.", "labels": [], "entities": []}, {"text": "Generalization methods ideally capture these patterns, for instance, they may provide a domain-independent lexicon for those predicates.", "labels": [], "entities": []}, {"text": "In some cases, even higher order features, such ascertain syntactic constructions may vary throughout the different domains.", "labels": [], "entities": []}, {"text": "In (1) and (2), the opinion holders are agents of a predictive predicate, whereas the opinion holder her daughters in (3) is a patient 2 of embarrasses.", "labels": [], "entities": []}, {"text": "If only sentences, such as (1) and (2), occur in the training data, a classifier will not correctly extract the opinion holder in (3), unless it obtains additional knowledge as to which predicates take opinion holders as patients.", "labels": [], "entities": []}, {"text": "In this work, we will consider three different generalization methods being simple unsupervised word clustering, an induction method and the usage of lexical resources.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.7354996800422668}]}, {"text": "We show that generalization causes significant improvements and that the impact of improvement depends on how much training and test data differ from each other.", "labels": [], "entities": [{"text": "generalization", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9851306080818176}]}, {"text": "We also address the issue of opinion holders in patient position and present methods including a novel extraction method to detect these opinion holders without any labeled training data as standard datasets contain too few instances of them.", "labels": [], "entities": []}, {"text": "In the context of generalization it is also important to consider different classification methods as the incorporation of generalization may have a varying impact depending on how robust the classifier is by itself, i.e. how well it generalizes even with a standard feature set.", "labels": [], "entities": []}, {"text": "We compare two stateof-the-art learning methods, conditional random fields and convolution kernels, and a rule-based method.", "labels": [], "entities": []}], "datasetContent": [{"text": "CK and RB have an instance space that is different from the one of CRF.", "labels": [], "entities": []}, {"text": "While CRF produces a prediction for every word token in a sentence, CK and RB only produce a prediction for every noun phrase.", "labels": [], "entities": [{"text": "RB", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9850600361824036}]}, {"text": "For evaluation, we project the predictions from RB and CK to word token level in order to ensure comparability.", "labels": [], "entities": [{"text": "RB", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9882136583328247}]}, {"text": "We evaluate the sequential output with precision, recall and F-score as defined in).", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9997624754905701}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9997069239616394}, {"text": "F-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.998997151851654}]}, {"text": "only be measured on the FICTION-domain since this is the only domain with a significant proportion of those opinion holders.", "labels": [], "entities": [{"text": "FICTION-domain", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.7929350137710571}]}, {"text": "Learning-based Methods shows the performance of the learningbased methods CRF and CK on an in-domain evaluation (ETHICS-domain) using different amounts of labeled training data.", "labels": [], "entities": [{"text": "ETHICS-domain", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.8699051737785339}]}, {"text": "We carryout a 5-fold cross-validation and use n% of the training data in the training folds.", "labels": [], "entities": []}, {"text": "The table shows that CK is more robust than CRF.", "labels": [], "entities": []}, {"text": "The fewer training data are used the more important generalization becomes.", "labels": [], "entities": []}, {"text": "CRF benefits much more from generalization than CK.", "labels": [], "entities": [{"text": "CRF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5725215673446655}, {"text": "generalization", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.964200496673584}]}, {"text": "Interestingly, the CRF configuration with the best generalization is usually as good as plain CK.", "labels": [], "entities": []}, {"text": "This proves the effectiveness of CK.", "labels": [], "entities": []}, {"text": "In principle, Lex is the strongest generalization method while Clus is by far the weakest.", "labels": [], "entities": [{"text": "Lex", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.6348134875297546}]}, {"text": "For Clus, systematic improvements towards no generalization (even though they are minor) can only be observed with CRF.", "labels": [], "entities": []}, {"text": "As far as combinations are concerned, either Lex+Induc or All performs best.", "labels": [], "entities": []}, {"text": "This in-domain evaluation proves that opinion holder extraction is different from namedentity recognition.", "labels": [], "entities": [{"text": "opinion holder extraction", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6649150749047598}, {"text": "namedentity recognition", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6894194632768631}]}, {"text": "Simple unsupervised generalization, such as word clustering, is not effective and popular sequential classifiers are less robust than margin-based tree-kernels.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7831673920154572}]}, {"text": "complements in that it compares the learning-based methods with the best rule-based classifier and also displays precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9996050000190735}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9984211921691895}]}, {"text": "RB achieves a high recall, whereas the learning-based methods always excel RB in precision.", "labels": [], "entities": [{"text": "RB", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9261744022369385}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.999624490737915}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9985982775688171}]}, {"text": "Applying generalization to the learningbased methods results in an improvement of both recall and precision if few training data are used.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9996116757392883}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9991760849952698}]}, {"text": "The impact on precision decreases, however, the more training data are added.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9994592070579529}]}, {"text": "There is always a significant increase in recall but learning-based methods may not reach the level of RB even though they use the same resources.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9990329742431641}, {"text": "RB", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9958425164222717}]}, {"text": "This is a side-effect of preserving a much higher precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9965294003486633}]}, {"text": "It also explains why learning-based methods with generalization may have a lower F-score than RB.", "labels": [], "entities": [{"text": "F-score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9987534284591675}]}, {"text": "Learning-based Methods presents the results of out-of-domain classifiers.", "labels": [], "entities": []}, {"text": "The complete ETHICS-dataset is used for training.", "labels": [], "entities": [{"text": "ETHICS-dataset", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.938532292842865}]}, {"text": "Some properties are similar to the previous experiments: CK always outperforms CRF.", "labels": [], "entities": []}, {"text": "RB provides a high recall whereas the learningbased methods maintain a higher precision.", "labels": [], "entities": [{"text": "RB", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9662721753120422}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9994822144508362}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9970223307609558}]}, {"text": "Similar to the in-domain setting using few labeled training data, the incorporation of generalization increases both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9995008707046509}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9987412095069885}]}, {"text": "Moreover, a combination of generalization methods is better than just using one method on average, although Lex is again a fairly robust individual generalization method.", "labels": [], "entities": []}, {"text": "Generalization is more effective in this setting than on the in-domain evaluation using all training data, in particular for CK, since the training and test data are much more different from each other and suitable generalization methods partly close that gap.", "labels": [], "entities": []}, {"text": "There is a notable difference in precision between the SPACE-and FICTION-domain (and also the source domain ETHICS).", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9992285966873169}, {"text": "SPACE-and", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.34251049160957336}, {"text": "FICTION-domain", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.5910472273826599}, {"text": "ETHICS", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.8819559216499329}]}, {"text": "We strongly assume that this is due to the distribution of opinion holders in those datasets.", "labels": [], "entities": []}, {"text": "The FICTION-domain contains much more opinion holders, therefore the chance that a predicted opinion holder is correct is much higher.", "labels": [], "entities": [{"text": "FICTION-domain", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.6592699289321899}]}, {"text": "With regard to recall, a similar level of performance as in the ETHICS-domain can only be achieved in the SPACE-domain, i.e. CK achieves a recall of 60%.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9959748387336731}, {"text": "ETHICS-domain", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.972991943359375}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9991342425346375}]}, {"text": "In the FICTION-domain, however, the recall is much lower (best recall of CK is below 47%).", "labels": [], "entities": [{"text": "FICTION-domain", "start_pos": 7, "end_pos": 21, "type": "METRIC", "confidence": 0.4757925570011139}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9998096823692322}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9991686344146729}]}, {"text": "This is no surprise as the SPACEdomain is more similar to the source domain than the FICTION-domain since ETHICS and SPACE are news texts.", "labels": [], "entities": [{"text": "FICTION-domain", "start_pos": 85, "end_pos": 99, "type": "METRIC", "confidence": 0.5661391019821167}, {"text": "ETHICS", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.9598760008811951}]}, {"text": "FICTION contains more out-ofdomain language.", "labels": [], "entities": [{"text": "FICTION", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8716975450515747}]}, {"text": "Therefore, RB (which exclusively uses domain-independent knowledge) outperforms both learning-based methods including the ones incorporating generalization.", "labels": [], "entities": [{"text": "RB", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9420689940452576}]}, {"text": "Similar results have been observed for rule-based classifiers from other tasks in cross-domain sentiment analysis, such as subjectivity detection and polarity classification.", "labels": [], "entities": [{"text": "cross-domain sentiment analysis", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.7350294689337412}, {"text": "subjectivity detection", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7074602246284485}, {"text": "polarity classification", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.7421539425849915}]}, {"text": "High-level information as it is encoded in a rule-based classifier generalizes better than learning-based methods.", "labels": [], "entities": []}, {"text": "We setup another experiment exclusively for the FICTION-domain in which we combine the output of our best learning-based method, i.e. CK, with the prediction of a rule-based classifier.", "labels": [], "entities": [{"text": "FICTION-domain", "start_pos": 48, "end_pos": 62, "type": "METRIC", "confidence": 0.6948604583740234}]}, {"text": "The combined classifier will predict an opinion holder, if either classifier predicts one.", "labels": [], "entities": []}, {"text": "The motivation for this is the following: The FICTION-domain is the only domain to have a significant proportion of opinion holders appearing as patients.", "labels": [], "entities": [{"text": "FICTION-domain", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.471147358417511}]}, {"text": "We want to know how much of them can be recognized with the best out-of-domain classifier using training data with only very few instances of this type and what benefit the addition of using various RBs which have a clearer notion of these constructions brings about.", "labels": [], "entities": []}, {"text": "Moreover, we already observed that the learning-based methods have a bias towards preserving a high precision and this may have as a consequence that the generalization features incorporated into CK will not receive sufficiently large weights.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.992345929145813}]}, {"text": "Unlike the SPACE-domain where a sufficiently high recall is already achieved with CK (presumably due to its stronger similarity towards the source domain) the FICTION-domain maybe more severely affected by this bias and evidence from RB may compensate for this.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9986868500709534}, {"text": "FICTION-domain", "start_pos": 159, "end_pos": 173, "type": "METRIC", "confidence": 0.9802886247634888}, {"text": "RB", "start_pos": 234, "end_pos": 236, "type": "METRIC", "confidence": 0.9335829615592957}]}, {"text": "shows the performance of those combined classifiers.", "labels": [], "entities": []}, {"text": "For all generalization types considered, there is, indeed, an improvement by adding information from RB resulting in a large boost in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9989767074584961}]}, {"text": "Already the application of our induction approach Induc results in an increase of more than 8% points compared to plain CK.", "labels": [], "entities": []}, {"text": "The table also shows that there is always some improvement if RB considers opinion holders as patients (AG+PT).", "labels": [], "entities": [{"text": "AG+PT)", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.8999274522066116}]}, {"text": "This can be considered as some evidence that (given the available data we use) opinion holders in patient position can only be effectively extracted with the help of RBs.", "labels": [], "entities": []}, {"text": "It is also   further evidence that our novel approach to extract those predicates ( \u00a73.3.2) is effective.", "labels": [], "entities": []}, {"text": "The combined approach in not only outperforms CK (discussed above) but also RB).", "labels": [], "entities": [{"text": "RB", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9943552017211914}]}, {"text": "We manually inspected the output of the classifiers to find also cases in which CK detect opinion holders that RB misses.", "labels": [], "entities": [{"text": "RB", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9411427974700928}]}, {"text": "CK has the advantage that it is not only bound to the relationship between candidate holder and predicate.", "labels": [], "entities": []}, {"text": "It learns further heuristics, e.g. that sentence-initial mentions of persons are likely opinion holders.", "labels": [], "entities": []}, {"text": "In (12), for example, this heuristics fires while RB overlooks this instance as to give someone a share of advice is not part of the lexicon.", "labels": [], "entities": [{"text": "RB", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9335318803787231}]}, {"text": "(12) She later gives Charlotte her share of advice on running a household.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the different domain corpora.", "labels": [], "entities": []}, {"text": " Table 2: Some automatically induced clusters.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of opinion holders as patients.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9713815450668335}]}, {"text": " Table 6: F-score of the different rule-based classifiers.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9989068508148193}]}, {"text": " Table 7: F-score of in-domain (ETHICS) learning- based classifiers.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9970924854278564}]}, {"text": " Table 8: Comparison of best RB with learning-based  approaches on in-domain classification.", "labels": [], "entities": []}, {"text": " Table 10: Combination of out-of-domain CK and rule- based classifiers on FICTION (i.e. distant domain).", "labels": [], "entities": [{"text": "FICTION", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9481886625289917}]}, {"text": " Table 9: Comparison of best RB with learning-based approaches on out-of-domain classification.", "labels": [], "entities": []}]}