{"title": [{"text": "What's in a Name? Entity Type Variation across Two Biomedical Subdomains", "labels": [], "entities": [{"text": "What's in a Name? Entity Type Variation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6147272421254052}]}], "abstractContent": [{"text": "There are lexical, syntactic, semantic and discourse variations amongst the languages used in various biomedical subdomains.", "labels": [], "entities": []}, {"text": "It is important to recognise such differences and understand that biomedical tools that work well on some subdomains may notwork as well on others.", "labels": [], "entities": []}, {"text": "We report hereon the semantic variations that occur in the sublanguages of two biomedical subdo-mains, i.e. cell biology and pharmacology, at the level of named entity information.", "labels": [], "entities": []}, {"text": "By building a classifier using ratios of named entities as features, we show that named entity information can discriminate between documents from each subdomain.", "labels": [], "entities": []}, {"text": "More specifically, our classifier can distinguish between documents belonging to each sub-domain with an accuracy of 91.1% F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9994485974311829}, {"text": "F-score", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9972647428512573}]}], "introductionContent": [{"text": "Biomedical information extraction efforts in the past decade have focussed on fundamental tasks needed to create intelligent systems capable of improving search engine results and easing the work of biologists.", "labels": [], "entities": [{"text": "Biomedical information extraction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6803386708100637}]}, {"text": "More specifically, researchers have concentrated mainly on named entity recognition, mapping them to concepts in curated databases ( and extracting simple binary relations between entities.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6216552356878916}]}, {"text": "Recently, an increasing number of resources that facilitate the training of systems to extract more detailed information have become available, e.g.,), GENE-TAG (), BioInfer (), GENIA ( ), GREC () and Metaknowledge GE-NIA ().", "labels": [], "entities": [{"text": "GENE-TAG", "start_pos": 152, "end_pos": 160, "type": "DATASET", "confidence": 0.8279266357421875}, {"text": "GENIA", "start_pos": 178, "end_pos": 183, "type": "DATASET", "confidence": 0.8663146495819092}, {"text": "GREC", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.5083811283111572}, {"text": "Metaknowledge GE-NIA", "start_pos": 201, "end_pos": 221, "type": "DATASET", "confidence": 0.665423184633255}]}, {"text": "Moreover, several other annotated corpora have been developed for shared task purposes, such as BioCreative I, II, III) and BioNLP Shared).", "labels": [], "entities": []}, {"text": "Many of the tools currently used for biomedical language processing were trained and evaluated on such popular corpora, most of which consist of documents from the molecular biology subdomain.", "labels": [], "entities": [{"text": "biomedical language processing", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6763551036516825}]}, {"text": "However, previous studies (discussed in Section 2) have established that different biomedical sublanguages exhibit linguistic variations.", "labels": [], "entities": []}, {"text": "It follows that tools which were developed and evaluated on corpora derived from one subdomain might not always perform as well on corpora from other subdomains.", "labels": [], "entities": []}, {"text": "Understanding these linguistic variations is essential to the process of adaptating natural language processing tools to new domains.", "labels": [], "entities": []}, {"text": "In this paper, we highlight the variations between biomedical sublanguages by focussing on the different types of named entities (NEs) that are relevant to them.", "labels": [], "entities": []}, {"text": "We show that the frequencies of different named entity types vary enough to allow a classifier for scientific subdomains to be built based upon them.", "labels": [], "entities": []}, {"text": "The study is performed on open access journal articles present in the UK PubMed Central 1 (UKPMC) (, an article database that extends the functionality of the original PubMed Central (PMC) repository 2 . This database was chosen as our source, since most of the documents within it are already tagged with named entity information.", "labels": [], "entities": [{"text": "UK PubMed Central 1 (UKPMC)", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.9411013041223798}, {"text": "PubMed Central (PMC) repository", "start_pos": 168, "end_pos": 199, "type": "DATASET", "confidence": 0.7974831859270731}]}, {"text": "We report hereon the results obtained for two biomedical subdomains, i.e. cell biology and pharmacology.", "labels": [], "entities": []}, {"text": "Our focus on these two particular subdomains is motivated by an increasing interest expressed by the biomedical research community, according to recent findings that have shown their relevance to discovering possible causes and treatments for incurable diseases, such as cancer or Alzheimer's Disease.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the corpus described previously, we created a training set for supervised machine learning algorithms.", "labels": [], "entities": []}, {"text": "Every document in the corpus was transformed into a vector consisting of 20 features.", "labels": [], "entities": []}, {"text": "Each of these features corresponds to an entity type in, having a numeric value ranging from 0 to 1.", "labels": [], "entities": []}, {"text": "This number represents the ratio of the specific entity type to the total number of named entities recognised in that document, as shown in Equation 2.", "labels": [], "entities": []}, {"text": "where n type represents the number of NEs of a certain type in a document and N represents the total number of NEs in that document.", "labels": [], "entities": []}, {"text": "Furthermore, each vector was labelled with the subdomain to which the respective document belongs (i.e., cell biology or pharmacology).", "labels": [], "entities": []}, {"text": "Weka) was employed as the machine learning framework, due to its large variety of classification algorithms.", "labels": [], "entities": [{"text": "Weka", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9408429861068726}]}, {"text": "We experimented with a large number of classifiers, ranging from Bayesian nets to functions, decision trees, decision rules and meta-classifiers.", "labels": [], "entities": []}, {"text": "The best performing classifiers are shown in.", "labels": [], "entities": []}, {"text": "BayesNet is an implementation of Bayesian Networks, SMO is an implementation of Support Vector Machines, J48 is an implementation of decision trees, whilst Jrip is an implementation of decision rules.", "labels": [], "entities": [{"text": "BayesNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9265555739402771}]}, {"text": "Random Forest is an ensemble classifier that consists of many decision trees (in this study, J48 was used), outputting the class that occurs most frequently in the output of individual trees.", "labels": [], "entities": []}, {"text": "The baseline that has been used is ZeroR, a simple algorithm that classifies all instances as pertaining to the majority class.", "labels": [], "entities": []}, {"text": "Since our classes have equal numbers of instances, the F-score of ZeroR is 50%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9993828535079956}, {"text": "ZeroR", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.5974833369255066}]}, {"text": "As can be seen from  We also employed AdaBoost in conjunction with the previously mentioned four classifiers, and the results are given in.", "labels": [], "entities": []}, {"text": "AdaBoost is a meta-algorithm that adapts itself during the course of several iterations in the sense that in each iteration, classifiers built are tweaked to correct those instances misclassified by prior classifiers.", "labels": [], "entities": [{"text": "AdaBoost", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9180687665939331}]}, {"text": "In this study, AdaBoost was run over 20 iterations, and it significantly improved the result of J48, by almost 4%, to 90.3%.", "labels": [], "entities": [{"text": "AdaBoost", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.8956567049026489}, {"text": "J48", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.857650876045227}]}, {"text": "However, AdaBoost decreased the F-score of Random Forest by 1% and that of BayesNet by 0.3%.: Classification results for AdaBoost in conjunction with the best-performing algorithms.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9982861876487732}]}], "tableCaptions": [{"text": " Table 1: Named entity types and their source.", "labels": [], "entities": []}, {"text": " Table 3: Ratios of NE types to the total number of NEs  in the two subdomains.", "labels": [], "entities": []}, {"text": " Table 4: Classification results for the best-performing  algorithms.", "labels": [], "entities": []}, {"text": " Table 5: Classification results for AdaBoost in con- junction with the best-performing algorithms.", "labels": [], "entities": []}, {"text": " Table 6: Attribute selection output from two attribute  evaluators.", "labels": [], "entities": []}, {"text": " Table 7: Confusion matrix for the Random Forest clas- sifier.", "labels": [], "entities": [{"text": "Random Forest clas- sifier", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.9622862935066223}]}]}