{"title": [{"text": "Instance-Driven Attachment of Semantic Annotations over Conceptual Hierarchies", "labels": [], "entities": []}], "abstractContent": [{"text": "Whether automatically extracted or human generated, open-domain factual knowledge is often available in the form of semantic annotations (e.g., composed-by) that take one or more specific instances (e.g., rhapsody in blue, george gershwin) as their arguments.", "labels": [], "entities": []}, {"text": "This paper introduces a method for converting flat sets of instance-level annotations into hierarchically organized, concept-level annotations, which capture not only the broad semantics of the desired arguments (e.g., 'People' rather than 'Loca-tions'), but also the correct level of generality (e.g., 'Composers' rather than 'People', or 'Jazz Composers').", "labels": [], "entities": []}, {"text": "The method refrains from encoding features specific to a particular domain or annotation, to ensure immediate applicability to new, previously unseen annotations.", "labels": [], "entities": []}, {"text": "Over a gold standard of semantic annotations and concepts that best capture their arguments, the method substantially outperforms three baselines, on average, computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Background: Knowledge about the world can bethought of as semantic assertions or annotations, at two levels of granularity: instance level (e.g., rhapsody in blue, tristan und isolde, george gershwin, richard wagner) and concept level (e.g., 'Musical Compositions', 'Works of Art', 'Composers').", "labels": [], "entities": []}, {"text": "Instance-level annotations correspond to factual knowledge that can be found in repositories extracted automatically from text () * Contributions made during an internship at or manually created within encyclopedic resources.", "labels": [], "entities": []}, {"text": "Such facts could state, for instance, that rhapsody in blue was composedby george gershwin, or that tristan und isolde was composed-by richard wagner.", "labels": [], "entities": []}, {"text": "In comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., 'Musical Compositions' are composed-by 'Composers'.", "labels": [], "entities": []}, {"text": "The frequent occurrence of instances, relative to more abstract concepts, in Web documents and popular Web search queries (, is both an asset and a liability from the point of view of knowledge acquisition.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 184, "end_pos": 205, "type": "TASK", "confidence": 0.7078627049922943}]}, {"text": "On one hand, it makes instance-level annotations relatively easy to find, either from manually created resources, or extracted automatically from text ().", "labels": [], "entities": []}, {"text": "On the other hand, it makes conceptlevel annotations more difficult to acquire directly.", "labels": [], "entities": []}, {"text": "While \"Rhapsody in Blue was composed by George Gershwin [..]\" may occur in some form within Web documents, the more abstract \"Musical compositions are composed by musicians\" is unlikely to occur.", "labels": [], "entities": []}, {"text": "A more practical approach to collecting concept-level annotations is to indirectly derive them from already plentiful instance-level annotations, effectively distilling factual knowledge into more abstract, concise and generalizable knowledge.", "labels": [], "entities": []}, {"text": "Contributions: This paper introduces a method for converting flat sets of specific, instancelevel annotations into hierarchically organized, concept-level annotations.", "labels": [], "entities": []}, {"text": "As illustrated in, the resulting annotations must capture not just the broad semantics of the desired arguments (e.g., 'People' rather than 'Locations' or 'Prod- The attachment of semantic annotations (e.g., composedby) into a conceptual hierarchy, a portion of which is shown in the diagram, requires the identification of the correct concept at the correct level of generality (e.g., 'Composers' rather than 'Jazz Composers' or 'People', for the right argument of composed-by).", "labels": [], "entities": []}, {"text": "ucts', as the right argument of the annotation composed-by), but actually identify the concepts at the correct level of generality/specificity (e.g., 'Composers' rather than 'Artists' or 'Jazz Composers') in the underlying conceptual hierarchy.", "labels": [], "entities": []}, {"text": "To ensure portability to new, previously unseen annotations, the proposed method avoids encoding features specific to a particular domain or annotation.", "labels": [], "entities": []}, {"text": "In particular, the use of annotations' labels (composed-by) as lexical features might be tempting, but would anchor the annotation model to that particular annotation.", "labels": [], "entities": []}, {"text": "Instead, the method relies only on features that generalize across annotations.", "labels": [], "entities": []}, {"text": "Over a gold standard of semantic annotations and concepts that best capture their arguments, the method substantially outperforms three baseline methods.", "labels": [], "entities": []}, {"text": "On average, the method computes concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts of the various annotations.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental runs exploit ranking features described in the previous section, employing: \u2022 one of three learning algorithms: naive Bayes (NAIVEBAYES), maximum entropy (MAXENT), or perceptron (PERCEPTRON) (Mitchell, 1997), chosen for their scalability to larger datasets via distributed implementations.", "labels": [], "entities": []}, {"text": "\u2022 one of three ways of combining the values of features collected for individual candidate concepts into values of features for pairs of candidate concepts: the raw ratio of the values of the respective features of the two concepts (0 when the denominator is 0); the ratio scaled to the interval; or a binary value indicating which of the values is larger.", "labels": [], "entities": []}, {"text": "For completeness, the experiments include three additional, baseline runs.", "labels": [], "entities": []}, {"text": "Each baseline computes scores for all candidate concepts based on the respective metric; then candidate concepts are ranked in decreasing order of their scores.", "labels": [], "entities": []}, {"text": "The baselines metrics are: \u2022 INSTPERCENT ranks candidate concepts by the percentage of matched instances that are descendants of the concept.", "labels": [], "entities": [{"text": "INSTPERCENT", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9039363265037537}]}, {"text": "It emphasizes concepts which are \"proven\" to belong to the annotation; \u2022 ENTROPY ranks candidate concepts by the entropy of the proportion of matched descendant instances of the concept; \u2022 AVGDEPTH ranks candidate concepts by their distances to half of the maximum hierarchy height, emphasizing a balance of generality and specificity.", "labels": [], "entities": [{"text": "ENTROPY", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.97844398021698}, {"text": "AVGDEPTH", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.7477210164070129}]}, {"text": "Gold Standard of Concept-Level Annotations: A random, weighted sample of 200 annotation labels (e.g., corresponding to composed-by, playinstrument) is selected, out of the set of labels of all instance-level annotations collected from Freebase.", "labels": [], "entities": []}, {"text": "During sampling, the weights are the counts of distinct instance-level annotations (e.g., <rhapsody in blue, george gershwin>) available for the label.", "labels": [], "entities": []}, {"text": "The arguments of the annotation labels are then manually annotated with a gold concept, which is the category from the Wikipedia hierarchy that best captures their semantics.", "labels": [], "entities": []}, {"text": "The manual annotation is carried out independently by two human judges, who then verify each other's work and discard inconsistencies.", "labels": [], "entities": []}, {"text": "For example, the gold concept of the left argument of composed-by is annotated to be the Wikipedia category 'Musical Compositions'.", "labels": [], "entities": []}, {"text": "In the process, some annotation labels are discarded, when (a) it is not clear what concept captures an argument (e.g., for the right argument of functionof-building), or (b) more than 5000 candidate concepts are available via propagation for one of the arguments, which would cause too many training or testing examples to be generated via concept pairs, and slowdown the experiments.", "labels": [], "entities": []}, {"text": "The retained 139 annotation labels, whose arguments have been labeled with their respective gold concepts, form the gold standard for the experiments.", "labels": [], "entities": []}, {"text": "More precisely, an entry in the resulting gold standard consists of an annotation label, one of its arguments being considered (left or right), and a gold concept that best captures that argument.", "labels": [], "entities": []}, {"text": "The set of annotation labels from the gold standard is quite diverse and covers many domains of potential interest, e.g., has-company('Industries', 'Companies'), written-by('Films', 'Screenwriters'), member-of ('Politicians','Political Parties'), or part-of-movement('Artists', 'Art Movements').", "labels": [], "entities": []}, {"text": "Evaluation Metric: Following previous work on selectional preferences (, each entry in the gold standard, (i.e., each argument fora given annotation) is evaluated separately.", "labels": [], "entities": []}, {"text": "Experimental runs compute a ranked list of candidate concepts for each entry in the gold standard.", "labels": [], "entities": []}, {"text": "In theory, a computed candidate concept is better if it is closer semantically to the gold concept.", "labels": [], "entities": []}, {"text": "In practice, the accuracy of a ranked list of candidate concepts, relative to the gold concept of the annotation label, is measured by two scoring metrics that correspond to the mean reciprocal rank score (MRR)) and a modification of it (DRR): N is the number of annotations and rank i is the rank of the gold concept in the returned list for MRR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9990580677986145}, {"text": "reciprocal rank score (MRR))", "start_pos": 183, "end_pos": 211, "type": "METRIC", "confidence": 0.8343561987082163}, {"text": "DRR)", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.9707746505737305}, {"text": "MRR", "start_pos": 343, "end_pos": 346, "type": "TASK", "confidence": 0.766996443271637}]}, {"text": "An annotation a i receives no credit for MRR if the gold concept does not appear in the corresponding ranked list.", "labels": [], "entities": [{"text": "MRR", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.6961908340454102}]}, {"text": "For DRR, rank i is the rank of a candidate concept in the returned list and Len is the length of  the minimum path in the hierarchy between the concept and the gold concept.", "labels": [], "entities": [{"text": "Len", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9672657251358032}]}, {"text": "Len is minimum (0) if the candidate concept is the same as the gold standard concept.", "labels": [], "entities": [{"text": "Len", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9402075409889221}]}, {"text": "A given annotation a i receives no credit for DRR if no path is found between the returned concepts and the gold concept.", "labels": [], "entities": [{"text": "DRR", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.8942598104476929}]}, {"text": "As an illustration, fora single annotation, the right argument of composed-by, the ranked list of concepts returned by an experimental maybe ['Symphonies by Anton Bruckner', 'Symphonies by Joseph Haydn', 'Symphonies by Gustav Mahler', 'Musical Compositions', ..], with the gold concept being 'Musical Compositions'.", "labels": [], "entities": []}, {"text": "The length of the path between 'Symphonies by Anton Bruckner' etc. and 'Musical Compositions' is 2 (via 'Symphonies').", "labels": [], "entities": []}, {"text": "Therefore, the MRR score would be 0.25 (given by the fourth element of the ranked list), whereas the DRR score would be 0.33 (given by the first element of the ranked list).", "labels": [], "entities": [{"text": "MRR score", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9641037881374359}, {"text": "DRR score", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9814159274101257}]}, {"text": "MRR and DRR are computed in five-fold cross validation.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.44271260499954224}, {"text": "DRR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9853684306144714}]}, {"text": "Concretely, the gold standard is split into five folds such that the sets of annotation labels in each fold are disjoint.", "labels": [], "entities": []}, {"text": "Thus, none of the annotation labels in testing appears in training.", "labels": [], "entities": []}, {"text": "This restriction makes the evaluation more rigurous and conservative as it actually assesses the extent the models learned are applicable to new, previously unseen annotation labels.", "labels": [], "entities": []}, {"text": "If this restriction were relaxed, the baselines would preform equivalently as they do not depend on the training data, but the learned methods would likely do better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training/Testing Examples: The top table shows examples of raw statistics gathered for three candidate  concepts for the left argument of the annotation acted-in. The second table shows the training/testing examples  generated from these concepts and statistics. Each example represents a pair of concepts which is labeled positive  if the first concept is closer to the correct concept than the second concept. Features shown here are the ratio  between a statistic for the first concept and a statistic for the second (e.g. DEPTH for Actors-English Actors is 2  as 'Actors' has depth of 6 and 'English Actors' has depth of 3). Some features omitted due to space constraints.", "labels": [], "entities": [{"text": "DEPTH", "start_pos": 536, "end_pos": 541, "type": "METRIC", "confidence": 0.6823375821113586}]}, {"text": " Table 3: Precision Results: Accuracy of ranked lists  of concepts (Wikipedia categories) computed by var- ious runs, as an average over the gold standard of  concept-level annotations, considering the top N can- didate concepts computed for each gold standard entry.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9952940344810486}]}]}