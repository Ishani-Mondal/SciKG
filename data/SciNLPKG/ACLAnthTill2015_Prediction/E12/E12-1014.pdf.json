{"title": [{"text": "Toward Statistical Machine Translation without Parallel Corpora", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.7338678439458212}]}], "abstractContent": [{"text": "We estimate the parameters of a phrase-based statistical machine translation system from monolingual corpora instead of a bilingual parallel corpus.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.5795248746871948}]}, {"text": "We extend existing research on bilingual lexicon induction to estimate both lexical and phrasal translation probabilities for MT-scale phrase-tables.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.675366481145223}, {"text": "MT-scale phrase-tables", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.8624749481678009}]}, {"text": "We propose a novel algorithm to estimate reordering probabilities from mono-lingual data.", "labels": [], "entities": []}, {"text": "We report translation results for an end-to-end translation system using these monolingual features alone.", "labels": [], "entities": []}, {"text": "Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights.", "labels": [], "entities": []}, {"text": "In this paper, we examine an idealization where a phrase-table is given.", "labels": [], "entities": []}, {"text": "We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone.", "labels": [], "entities": []}, {"text": "We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9993058443069458}]}], "introductionContent": [{"text": "The parameters of statistical models of translation are typically estimated from large bilingual parallel corpora ().", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.938633918762207}]}, {"text": "However, these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building a good translation system.", "labels": [], "entities": []}, {"text": "We attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.6278220017751058}]}, {"text": "In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables.", "labels": [], "entities": [{"text": "MT-scale phrase-tables", "start_pos": 245, "end_pos": 267, "type": "TASK", "confidence": 0.8540659546852112}]}, {"text": "We then introduce a novel algorithm to estimate reordering features from monolingual data alone, and we report the performance of a phrase-based statistical model () estimated using these monolingual features.", "labels": [], "entities": []}, {"text": "Most of the prior work on lexicon induction is motivated by the idea that it could be applied to machine translation but stops short of actually doing so.", "labels": [], "entities": [{"text": "lexicon induction", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8555066585540771}, {"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7350292950868607}]}, {"text": "Lexicon induction holds the potential to create machine translation systems for languages which do not have extensive parallel corpora.", "labels": [], "entities": [{"text": "Lexicon induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8708496987819672}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7247822880744934}]}, {"text": "Training would only require two large monolingual corpora and a small bilingual dictionary, if one is available.", "labels": [], "entities": []}, {"text": "The idea is that intrinsic properties of monolingual data (possibly along with a handful of bilingual pairs to act as example mappings) can provide independent but informative cues to learn translations because words (and phrases) behave similarly across languages.", "labels": [], "entities": []}, {"text": "This work is the first attempt to extend and apply these ideas to an end-to-end machine translation pipeline.", "labels": [], "entities": [{"text": "machine translation pipeline", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.7687183320522308}]}, {"text": "While we make an explicit assumption that a table of phrasal translations is given a priori, we induce every other parameter of a full phrasebased translation system from monolingual data alone.", "labels": [], "entities": []}, {"text": "The contributions of this work are: \u2022 In Section 2.2 we analyze the challenges of using bilingual lexicon induction for statistical MT (performance on low frequency items, and moving from words to phrases).", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6548560659090678}, {"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.8117912411689758}]}, {"text": "\u2022 In Sections 3.1 and 3.2 we use multiple cues present in monolingual data to estimate lexical and phrasal translation scores.", "labels": [], "entities": []}, {"text": "\u2022 In Section 3.3 we propose a novel algorithm for estimating phrase reordering features from monolingual texts.", "labels": [], "entities": [{"text": "estimating phrase reordering features from monolingual texts", "start_pos": 50, "end_pos": 110, "type": "TASK", "confidence": 0.8204638361930847}]}, {"text": "\u2022 Finally, in Section 5 we systematically drop feature functions from a phrase table and then replace them with monolingually estimated equivalents, reporting end-to-end translation quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Spanish-English language pair to test our method for estimating the parameters of an SMT system from monolingual corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9917029738426208}]}, {"text": "This allows us to compare our method against the normal bilingual training procedure.", "labels": [], "entities": []}, {"text": "We expect bilingual training to result in higher translation quality because it is a more direct method for learning translation probabilities.", "labels": [], "entities": []}, {"text": "We systematically remove different parameters from the standard phrase-based model, and then replace them with our monolingual equivalents.", "labels": [], "entities": []}, {"text": "Our goal is to recover as much of the loss as possible for each of the deleted bilingual components.", "labels": [], "entities": []}, {"text": "The standard phrase-based model that we use as our top-line is the Moses system () trained over the full Europarl v5 parallel corpus.", "labels": [], "entities": [{"text": "Europarl v5 parallel corpus", "start_pos": 105, "end_pos": 132, "type": "DATASET", "confidence": 0.9432590156793594}]}, {"text": "With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters.", "labels": [], "entities": []}, {"text": "All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9334492087364197}]}, {"text": "To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles.", "labels": [], "entities": []}, {"text": "These development and test datasets were distributed in the WMT shared task.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.5067331194877625}]}, {"text": "MERT was re-run for every experiment.", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5005699396133423}]}, {"text": "We estimate the parameters of our model from two sets of monolingual data, detailed in \u2022 First, we treat the two sides of the Europarl parallel corpus as independent, monolingual corpora.", "labels": [], "entities": [{"text": "Europarl parallel corpus", "start_pos": 126, "end_pos": 150, "type": "DATASET", "confidence": 0.9642636775970459}]}, {"text": "also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical.", "labels": [], "entities": []}, {"text": "\u2022 Next, we estimate the features from truly monolingual corpora.", "labels": [], "entities": []}, {"text": "To estimate the contextual and temporal similarity features, we use the Spanish and English Gigaword corpora.", "labels": [], "entities": []}, {"text": "These corpora are substantially larger than the Europarl corpora, providing 27x as much Spanish and 67x as much English for contextual similarity, and 6x as many paired dates for temporal similarity.", "labels": [], "entities": [{"text": "Europarl corpora", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.9790630340576172}]}, {"text": "Topical similarity is estimated using Spanish and English Wikipedia articles that are paired with interlanguage links.", "labels": [], "entities": []}, {"text": "To project context vectors from Spanish to English, we use a bilingual dictionary containing entries for 49,795 Spanish words.", "labels": [], "entities": []}, {"text": "Note that end-toend translation quality is robust to substantially reducing dictionary size, but we omit these experiments due to space constraints.", "labels": [], "entities": []}, {"text": "The context vectors for words and phrases incorporate cooccurrence counts using a two-word window on either side.", "labels": [], "entities": []}, {"text": "The title of our paper uses the word towards because we assume that an inventory of phrase pairs is given.", "labels": [], "entities": []}, {"text": "Future work will explore inducing the Figure 7: Much of the loss in BLEU score when bilingually estimated features are removed from a SpanishEnglish translation system (experiments 1-4) can be recovered when they are replaced with monolingual equivalents estimated from monolingual Europarl data (experiments 5-10).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9824087619781494}, {"text": "Europarl data", "start_pos": 282, "end_pos": 295, "type": "DATASET", "confidence": 0.9734342694282532}]}, {"text": "The labels indicate how the different types of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.", "labels": [], "entities": []}, {"text": "Figure 8: Performance of monolingual features derived from truly monolingual corpora.", "labels": [], "entities": []}, {"text": "Over 82% of the BLEU score loss can be recovered.", "labels": [], "entities": [{"text": "BLEU score loss", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.9630930423736572}]}, {"text": "phrase table itself from monolingual texts.", "labels": [], "entities": []}, {"text": "Across all of our experiments, we use the phrase table that the bilingual model learned from the Europarl parallel corpus.", "labels": [], "entities": [{"text": "Europarl parallel corpus", "start_pos": 97, "end_pos": 121, "type": "DATASET", "confidence": 0.95309845606486}]}, {"text": "We keep its phrase pairs, but we drop all of its scores.", "labels": [], "entities": []}, {"text": "gives details of the phrase pairs.", "labels": [], "entities": []}, {"text": "In our experiments, we estimated similarity and reordering scores for more than 3 million phrase pairs.", "labels": [], "entities": [{"text": "similarity and reordering scores", "start_pos": 33, "end_pos": 65, "type": "METRIC", "confidence": 0.7703070342540741}]}, {"text": "For each source phrase, the set of possible translations was constrained and likely to contain good translations.", "labels": [], "entities": []}, {"text": "However, the average number of possible translations was high (ranging from nearly 100 translations for each unigram to 14 for each trigram).", "labels": [], "entities": []}, {"text": "These contain a lot of noise and result in low end-to-end translation quality without good estimates of translation quality, as the experiments in Section 5.1 show.", "labels": [], "entities": []}, {"text": "Because many details of our estimation procedures must be omitted for space, we distribute our full set of code along with scripts for running our experiments and output translations.", "labels": [], "entities": []}, {"text": "These maybe downed from http://www.cs. jhu.edu/ \u02dc anni/papers/lowresmt/  Figures 7 and 8 give experimental results.", "labels": [], "entities": []}, {"text": "shows the performance of the standard phrasebased model when each of the bilingually estimated features are removed.", "labels": [], "entities": []}, {"text": "It shows how much of the performance loss can be recovered using our monolingual features when they are estimated from the Europarl training corpus but treating each side as an independent, monolingual corpus.", "labels": [], "entities": [{"text": "Europarl training corpus", "start_pos": 123, "end_pos": 147, "type": "DATASET", "confidence": 0.9657616217931112}]}, {"text": "shows the recovery when using truly monolingual corpora to estimate the parameters.", "labels": [], "entities": []}, {"text": "Experiments 1-4 remove bilingually estimated parameters from the standard model.", "labels": [], "entities": []}, {"text": "For SpanishEnglish, the relative contribution of the phrasetable features (which include the phrase translation probabilities \u03c6 and the lexical weights w) is greater than the reordering probabilities.", "labels": [], "entities": [{"text": "phrase translation probabilities \u03c6", "start_pos": 93, "end_pos": 127, "type": "TASK", "confidence": 0.6790784075856209}]}, {"text": "When the reordering probability p o (orientation|f, e) is eliminated and replaced with a simple distancebased distortion feature that does not require a bitext to estimate, the score dips only marginally since word order in English and Spanish is similar.", "labels": [], "entities": [{"text": "distancebased distortion", "start_pos": 96, "end_pos": 120, "type": "METRIC", "confidence": 0.8494557738304138}]}, {"text": "However, when both the reordering and the phrase table features are dropped, leaving only the LM feature and the phrase penalty, the resulting translation quality is abysmal, with the score dropping a total of over 17 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.998843789100647}]}], "tableCaptions": [{"text": " Table 1: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.", "labels": [], "entities": []}]}