{"title": [{"text": "User Edits Classification Using Document Revision Histories", "labels": [], "entities": [{"text": "User Edits Classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.754120926062266}, {"text": "Document Revision Histories", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.7748427192370096}]}], "abstractContent": [{"text": "Document revision histories area useful and abundant source of data for natural language processing, but selecting relevant data for the task at hand is not trivial.", "labels": [], "entities": [{"text": "Document revision histories", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8238312800725301}, {"text": "natural language processing", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.6435616612434387}]}, {"text": "In this paper we introduce a scalable approach for automatically distinguishing between factual and fluency edits in document revision histories.", "labels": [], "entities": [{"text": "automatically distinguishing between factual and fluency edits in document revision histories", "start_pos": 51, "end_pos": 144, "type": "TASK", "confidence": 0.5775213675065474}]}, {"text": "The approach is based on supervised machine learning using language model probabilities, string similarity measured over different representations of user edits, comparison of part-of-speech tags and named entities, and a set of adap-tive features extracted from large amounts of unlabeled user edits.", "labels": [], "entities": []}, {"text": "Applied to contiguous edit segments, our method achieves statistically significant improvements over a simple yet effective edit-distance base-line.", "labels": [], "entities": []}, {"text": "It reaches high classification accuracy (88%) and is shown to generalize to additional sets of unseen data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.941065788269043}]}], "introductionContent": [{"text": "Many online collaborative editing projects such as Wikipedia 1 keep track of complete revision histories.", "labels": [], "entities": [{"text": "Wikipedia 1", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9196597933769226}]}, {"text": "These contain valuable information about the evolution of documents in terms of content as well as language, style and form.", "labels": [], "entities": []}, {"text": "Such data is publicly available in large volumes and constantly growing.", "labels": [], "entities": []}, {"text": "According to Wikipedia statistics, in August 2011 the English Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.8798677027225494}]}, {"text": "The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.", "labels": [], "entities": [{"text": "revision edits", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.9367718696594238}]}, {"text": "Exploiting document revision histories has proven useful fora variety of natural language processing (NLP) tasks, including sentence compression ) and simplification), information retrieval), textual entailment recognition (, and paraphrase extraction).", "labels": [], "entities": [{"text": "Exploiting document revision histories", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7128618061542511}, {"text": "sentence compression", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7084170132875443}, {"text": "information retrieval", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.7954703569412231}, {"text": "textual entailment recognition", "start_pos": 192, "end_pos": 222, "type": "TASK", "confidence": 0.8185201485951742}, {"text": "paraphrase extraction", "start_pos": 230, "end_pos": 251, "type": "TASK", "confidence": 0.8491898775100708}]}, {"text": "The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories.", "labels": [], "entities": []}, {"text": "The need for an automated classification method has been identified, but to the best of our knowledge has not been directly addressed.", "labels": [], "entities": []}, {"text": "Previous approaches have either applied simple heuristics () or manual annotations) to restrict the data to the type of edits relevant to the NLP task at hand.", "labels": [], "entities": []}, {"text": "The work described in this paper shows that it is possible to automatically distinguish between factual and fluency edits.", "labels": [], "entities": [{"text": "factual and fluency edits", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.6060053110122681}]}, {"text": "This is very desirable as it does not rely on heuristics, which often generalize poorly, and does not require manual annotation beyond a small collection of training data, thereby allowing for much larger data sets of revision histories to be used for NLP research.", "labels": [], "entities": []}, {"text": "In this paper, we make the following novel contributions: We address the problem of automated classification of user edits as factual or fluency edits users, anonymous users, software bots and reverts.", "labels": [], "entities": [{"text": "automated classification of user edits as factual or fluency edits users", "start_pos": 84, "end_pos": 156, "type": "TASK", "confidence": 0.8122210610996593}]}, {"text": "Source: http://stats.wikimedia.org. by defining the scope of user edits, extracting a large collection of such user edits from the English Wikipedia, constructing a manually labeled dataset, and setting up a classification baseline.", "labels": [], "entities": []}, {"text": "A set of features is designed and integrated into a supervised machine learning framework.", "labels": [], "entities": []}, {"text": "It is composed of language model probabilities and string similarity measured over different representations, including part-of-speech tags and named entities.", "labels": [], "entities": []}, {"text": "Despite their relative simplicity, the features achieve high classification accuracy when applied to contiguous edit segments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.965302586555481}]}, {"text": "We go beyond labeled data and exploit large amounts of unlabeled data.", "labels": [], "entities": []}, {"text": "First, we demonstrate that the trained classifier generalizes to thousands of examples identified by user comments as specific types of fluency edits.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce anew method for extracting features from an evolving set of unlabeled user edits.", "labels": [], "entities": []}, {"text": "This method is successfully evaluated as an alternative or supplement to the initial supervised approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we extract a large amount of user edits from revision histories of the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.8714076280593872}]}, {"text": "The extraction process scans pairs of subsequent revisions of article pages and ignores any revision that was reverted due to vandalism.", "labels": [], "entities": []}, {"text": "It parses the Wikitext and filters out markup, hyperlinks, tables and templates.", "labels": [], "entities": []}, {"text": "The process analyzes the clean text of the two revisions 4 and computes the difference between them.", "labels": [], "entities": []}, {"text": "The process identifies the overlap between edit segments and sentence boundaries and extracts user edits.", "labels": [], "entities": []}, {"text": "Features are calculated and user edits are stored and indexed.", "labels": [], "entities": []}, {"text": "LM features are calculated against a large English 4-gram lan-guage model built by SRILM) with modified interpolated Kneser-Ney smoothing using the AFP and Xinhua portions of the English Gigaword corpus (LDC2003T05).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9308297634124756}, {"text": "English Gigaword corpus", "start_pos": 179, "end_pos": 202, "type": "DATASET", "confidence": 0.831831693649292}]}, {"text": "We extract a total of 4.3 million user edits of which 2.52 million (almost 60%) are insertions and deletions of complete sentences.", "labels": [], "entities": []}, {"text": "Although these may include fluency edits such as sentence reordering or rewriting from scratch, we assume that the large majority is factual.", "labels": [], "entities": [{"text": "sentence reordering", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.6916832476854324}]}, {"text": "Of the remaining 1.78 million edits, the majority (64.5%) contains single deleted, inserted or replaced segments.", "labels": [], "entities": []}, {"text": "We decide to focus on this subset because sentences with multiple non-contiguous edit segments are more likely to contain mixed cases of unrelated factual and fluency edits, as illustrated by example (2) in.", "labels": [], "entities": []}, {"text": "Learning to classify contiguous edit segments seems to be a reasonable way of breaking down the problem into smaller parts.", "labels": [], "entities": []}, {"text": "We filter out user edits with edit distance longer than 100 characters or 10 words that we assume to be factual.", "labels": [], "entities": []}, {"text": "The resulting dataset contains 923,820 user edits: 58% replaced segments, 25.5% inserted segments and 16.5% deleted segments.", "labels": [], "entities": []}, {"text": "Manual labeling of user edits is carried out by a group of annotators with near native or native level of English.", "labels": [], "entities": []}, {"text": "All annotators receive the same written guidelines.", "labels": [], "entities": []}, {"text": "In short, fluency labels are assigned to edits of letter case, spelling, grammar, synonyms, paraphrases, co-referents, language and style.", "labels": [], "entities": []}, {"text": "Factual labels are assigned to edits of dates, numbers and figures, named entities, semantic change or disambiguation, addition or removal of content.", "labels": [], "entities": []}, {"text": "A random set of 2,676 instances is labeled: 2,008 instances with a majority agreement of at least two annotators are selected as training set, 270 instances are held out as development set, 164 trivial fluency corrections of a single letter's case and 234 instances with no clear agreement among annotators are excluded.", "labels": [], "entities": []}, {"text": "The last group (8.7%) emphasizes that the task is, to a limited extent, subjective.", "labels": [], "entities": []}, {"text": "It suggests that automated classification of certain user edits would be difficult.", "labels": [], "entities": [{"text": "automated classification of certain user edits", "start_pos": 17, "end_pos": 63, "type": "TASK", "confidence": 0.6982855846484503}]}, {"text": "Nevertheless, inter-rater agreement between annotators is high to very high.", "labels": [], "entities": [{"text": "agreement", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.8505672216415405}]}, {"text": "Kappa values between 0.74 to 0.84 are measured between six pairs of annotators, each pair annotated a common subset of at least 100 instances.", "labels": [], "entities": []}, {"text": "describes the resulting dataset, which we also make available to the research community.", "labels": [], "entities": []}, {"text": "Character-level Edit Distance Fluency (725) Factual (821) Factual Fluency (283): A decision tree that uses character-level edit distance as a sole feature.", "labels": [], "entities": []}, {"text": "The tree correctly classifies 76% of the labeled user edits.: Classification accuracy using the baseline, each feature set added to the baseline, and all features combined.", "labels": [], "entities": [{"text": "Classification", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.9249544739723206}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9808305501937866}]}, {"text": "Statistical significance at p < 0.05 is indicated by \u2020 w.r.t the baseline (using the same classifier), and by \u2227 w.r.t to another classifier marked by \u2228 (using the same features).", "labels": [], "entities": []}, {"text": "Highest accuracy per classifier is marked in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9952861666679382}]}], "tableCaptions": [{"text": " Table 3: Dataset of nearly 1 million user edits  with single deleted, inserted or replaced segments,  of which 2K are labeled. The labels are almost  equally distributed. The distribution over edit seg- ment types and edit distance intervals is detailed.", "labels": [], "entities": []}, {"text": " Table 4: Classification accuracy using the base- line, each feature set added to the baseline, and  all features combined. Statistical significance at  p < 0.05 is indicated by  \u2020 w.r.t the baseline (us- ing the same classifier), and by \u2227 w.r.t to another  classifier marked by \u2228 (using the same features).  Highest accuracy per classifier is marked in bold.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9378640651702881}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9581377506256104}, {"text": "Statistical significance", "start_pos": 124, "end_pos": 148, "type": "METRIC", "confidence": 0.8122080564498901}, {"text": "accuracy", "start_pos": 317, "end_pos": 325, "type": "METRIC", "confidence": 0.9183668494224548}]}, {"text": " Table 5: Fraction of correctly classified edits per  type: fluency edits (left) and factual edits (right),  using the baseline, each feature set added to the  baseline, and all features combined.", "labels": [], "entities": []}, {"text": " Table 6: Error types based on manual examina- tion of 50 fluency edit misclassifications and 50  factual edit misclassifications.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9397542476654053}]}, {"text": " Table 8: Classifying unlabeled data selected by  user comments that suggest a fluency edit. The  SVM classifier is trained using the labeled data.  User comments are not used as features.", "labels": [], "entities": []}, {"text": " Table 11: Classification accuracy using features  from unlabeled data. The first two rows are identi- cal to Table 4. Statistical significance at p < 0.05  is indicated by:  \u2020 w.r.t the baseline;  \u2021 w.r.t all fea- tures excluding features from unlabeled data; and  \u2227 w.r.t to another classifier marked by \u2228 (using the  same features). The best result is marked in bold.", "labels": [], "entities": [{"text": "Classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.964995801448822}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9218311309814453}, {"text": "Statistical significance", "start_pos": 119, "end_pos": 143, "type": "METRIC", "confidence": 0.878229558467865}]}]}