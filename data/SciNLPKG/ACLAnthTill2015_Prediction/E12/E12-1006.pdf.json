{"title": [], "abstractContent": [{"text": "A serious bottleneck of comparative parser evaluation is the fact that different parsers subscribe to different formal frameworks and theoretical assumptions.", "labels": [], "entities": [{"text": "comparative parser evaluation", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.7854029933611552}]}, {"text": "Converting outputs from one framework to another is less than optimal as it easily introduces noise into the process.", "labels": [], "entities": []}, {"text": "Here we present a principled protocol for evaluating parsing results across frameworks based on function trees, tree generalization and edit distance metrics.", "labels": [], "entities": []}, {"text": "This extends a previously proposed framework for cross-theory evaluation and allows us to compare a wider class of parsers.", "labels": [], "entities": [{"text": "cross-theory evaluation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.809137374162674}]}, {"text": "We demonstrate the usefulness and language independence of our procedure by evaluating constituency and dependency parsers on English and Swedish.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of statistical parsers is to recover a formal representation of the grammatical relations that constitute the argument structure of natural language sentences.", "labels": [], "entities": []}, {"text": "The argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc., which are useful for further (e.g., semantic) processing.", "labels": [], "entities": []}, {"text": "The parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data.", "labels": [], "entities": []}, {"text": "For example, grammatical relations maybe encoded on top of dependency arcs in a dependency tree, they may decorate nodes in a phrase-structure tree), or they maybe read off of positions in a phrase-structure tree using hard-coded conversion procedures).", "labels": [], "entities": []}, {"text": "This diversity poses a challenge to cross-experimental parser evaluation, namely: How can we evaluate the performance of these different parsers relative to one another?", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7980003654956818}]}, {"text": "Current evaluation practices assume a set of correctly annotated test data (or gold standard) for evaluation.", "labels": [], "entities": []}, {"text": "Typically, every parser is evaluated with respect to its own formal representation type and the underlying theory which it was trained to recover.", "labels": [], "entities": []}, {"text": "Therefore, numerical scores of parses across experiments are incomparable.", "labels": [], "entities": []}, {"text": "When comparing parses that belong to different formal frameworks, the notion of a single gold standard becomes problematic, and there are two different questions we have to answer.", "labels": [], "entities": []}, {"text": "First, what is an appropriate gold standard for cross-parser evaluation?", "labels": [], "entities": []}, {"text": "And secondly, how can we alleviate the differences between formal representation types and theoretical assumptions in order to make our comparison sound -that is, to make sure that we are not comparing apples and oranges?", "labels": [], "entities": []}, {"text": "A popular way to address this has been to pick one of the frameworks and convert all parser outputs to its formal type.", "labels": [], "entities": []}, {"text": "When comparing constituency-based and dependency-based parsers, for instance, the output of constituency parsers has often been converted to dependency structures prior to evaluation.", "labels": [], "entities": []}, {"text": "This solution has various drawbacks.", "labels": [], "entities": []}, {"text": "First, it demands a conversion script that maps one representation type to another when some theoretical assumptions in one framework maybe incompatible with the other one.", "labels": [], "entities": []}, {"text": "In the constituency-to-dependency case, some constituency-based structures (e.g., coordination and ellipsis) do not comply with the single head assumption of dependency treebanks.", "labels": [], "entities": []}, {"text": "Secondly, these scripts maybe labor intensive to create, and are available mostly for English.", "labels": [], "entities": []}, {"text": "So the evaluation protocol becomes language-dependent.", "labels": [], "entities": []}, {"text": "In we proposed a general protocol for handling annotation discrepancies when comparing parses across different dependency theories.", "labels": [], "entities": []}, {"text": "The protocol consists of three phases: converting all structures into function trees, for each sentence, generalizing the different gold standard function trees to get their common denominator, and employing an evaluation measure based on tree edit distance (TED) which discards edit operations that recover theory-specific structures.", "labels": [], "entities": [{"text": "tree edit distance (TED)", "start_pos": 239, "end_pos": 263, "type": "METRIC", "confidence": 0.6986883580684662}]}, {"text": "Although the protocol is potentially applicable to a wide class of syntactic representation types, formal restrictions in the procedures effectively limit its applicability only to representations that are isomorphic to dependency trees.", "labels": [], "entities": []}, {"text": "The present paper breaks new ground in the ability to soundly compare the accuracy of different parsers relative to one another given that they employ different formal representation types and obey different theoretical assumptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9977399110794067}]}, {"text": "Our solution generally confines with the protocol proposed in but is re-formalized to allow for arbitrary linearly ordered labeled trees, thus encompassing constituency-based as well as dependency-based representations.", "labels": [], "entities": []}, {"text": "The framework in assumes structures that are isomorphic to dependency trees, bypassing the problem of arbitrary branching.", "labels": [], "entities": []}, {"text": "Here we lift this restriction, and define a protocol which is based on generalization and TED measures to soundly compare the output of different parsers.", "labels": [], "entities": [{"text": "TED", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9967209696769714}]}, {"text": "We demonstrate the utility of this protocol by comparing the performance of different parsers for English and Swedish.", "labels": [], "entities": []}, {"text": "For English, our parser evaluation across representation types allows us to analyze and precisely quantify previously encountered performance tendencies.", "labels": [], "entities": []}, {"text": "For Swedish we show the first ever evaluation between dependency-based and constituency-based parsing models, all trained on the Swedish treebank data.", "labels": [], "entities": [{"text": "constituency-based parsing", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.5093089938163757}, {"text": "Swedish treebank data", "start_pos": 129, "end_pos": 150, "type": "DATASET", "confidence": 0.8876136342684428}]}, {"text": "All in all we show that our extended protocol, which can handle linearlyordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in a representation-independent and language-independent fashion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types.", "labels": [], "entities": []}, {"text": "Dependency trees are evaluated using attachment scores), phrase-structure trees are evaluated using, LFG-based parsers postulate an evaluation procedure based on fstructures (, and soon.", "labels": [], "entities": []}, {"text": "From a downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized.", "labels": [], "entities": []}, {"text": "The bottom line is simply which parsing framework most accurately recovers a useful representation that helps to unravel the human-perceived interpretation.", "labels": [], "entities": []}, {"text": "Relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provide an interface to semantic interpretation.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 170, "end_pos": 193, "type": "TASK", "confidence": 0.7360962182283401}]}, {"text": "They are more intuitively understood than, say, phrase-structure trees, and thus they are also more useful for practical applications.", "labels": [], "entities": []}, {"text": "For these reasons, relational schemes have been repeatedly singled out as an appropriate level of representation for the evaluation of statistical parsers.", "labels": [], "entities": []}, {"text": "The annotated data which statistical parsers are trained on encode these grammatical relationships in different ways.", "labels": [], "entities": []}, {"text": "Dependency treebanks provide a ready-made representation of grammatical relations on top of arcs connecting the words in the sentence ().", "labels": [], "entities": []}, {"text": "The Penn Treebank and phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9943698644638062}]}, {"text": "Treebanks like Tiger for German () and Talbanken for Swedish) explicitly map phrase structures onto grammatical relations using dedicated edge labels.", "labels": [], "entities": []}, {"text": "The RelationalRealizational structures of encode relational networks (sets of relations) projected and realized by syntactic categories on top of ordinary phrase-structure nodes.", "labels": [], "entities": []}, {"text": "Ina relational-realizational structure like (c) we can remove the projection nodes (sets) and realization nodes (phrase labels), which leaves the function nodes intact.", "labels": [], "entities": []}, {"text": "tion of the dominated span.", "labels": [], "entities": []}, {"text": "Function trees benefit from the same advantages as other relational schemes, namely that they are intuitive to understand, they provide the interface for semantic interpretation, and thus maybe useful for downstream applications.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.7223288416862488}]}, {"text": "Yet they do not suffer from formal restrictions inherent in dependency structures, for instance, the single head assumption.", "labels": [], "entities": []}, {"text": "For many formal representation types there exists a fully deterministic, heuristics-free, procedure mapping them to function trees.", "labels": [], "entities": []}, {"text": "In we illustrate some such procedures fora simple transitive sentence.", "labels": [], "entities": []}, {"text": "Now, while all the structures at the right hand side of are of the same formal type (function trees), they have different tree structures due to different theoretical assumptions underlying the original formal frameworks.", "labels": [], "entities": []}, {"text": "Once we have converted framework-specific representations into function trees, the problem of cross-framework evaluation can potentially be reduced to a cross-theory evaluation following.", "labels": [], "entities": []}, {"text": "The main idea is that once all structures have been converted into function trees, one can perform a formal operation called generalization in order to harmonize the differences between theories, and measure accurately the distance of parse hypotheses from the generalized gold.", "labels": [], "entities": []}, {"text": "The generalization operation defined in, however, cannot handle trees that may contain unary chains, and therefore cannot be used for arbitrary function trees.", "labels": [], "entities": []}, {"text": "Consider for instance (t1) and (t2) in.", "labels": [], "entities": []}, {"text": "According to the definition of subsumption in, (t1) is subsumed by (t2) and vice versa, so the two trees should be identical -but they are not.", "labels": [], "entities": []}, {"text": "The interpretation we wish to give to a function tree such as (t1) is that the word w has both the grammatical function f1 and the grammatical function f2.", "labels": [], "entities": []}, {"text": "This can be graphically represented as a set of labels dominating w, as in (t3).", "labels": [], "entities": []}, {"text": "We call structures such as (t3) multifunction trees.", "labels": [], "entities": []}, {"text": "In the next section we formally define multi-function trees, and then use them to develop our protocol for cross-framework and crosstheory evaluation.", "labels": [], "entities": []}, {"text": "We validate our cross-framework evaluation procedure on two languages, English and Swedish.", "labels": [], "entities": []}, {"text": "For English, we compare the performance of two dependency parsers, MaltParser () and MSTParser (), and two constituency-based parsers, the Berkeley parser () and the Brown parser.", "labels": [], "entities": []}, {"text": "All experiments use Penn Treebank (PTB) data.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) data", "start_pos": 20, "end_pos": 44, "type": "DATASET", "confidence": 0.9735721449057261}]}, {"text": "For Swedish, we compare MaltParser and MSTParser with two variants of the Berkeley parser, one trained on phrase structure trees, and one trained on a variant of the Relational-Realizational representation of.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.9186530709266663}]}, {"text": "All experiments use the Talbanken Swedish Treebank (STB) data.", "labels": [], "entities": [{"text": "Talbanken Swedish Treebank (STB) data", "start_pos": 24, "end_pos": 61, "type": "DATASET", "confidence": 0.897898656981332}]}, {"text": "We use sections 02-21 of the WSJ Penn Treebank for training and section 00 for evaluation and analysis.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.912418007850647}]}, {"text": "We use two different native gold standards subscribing to different theories of encoding grammatical relations in tree structures: \u2022 THE DEPENDENCY-BASED THEORY is the theory encoded in the basic Stanford Dependencies (SD) scheme.", "labels": [], "entities": [{"text": "THE", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9443529844284058}, {"text": "DEPENDENCY-BASED", "start_pos": 137, "end_pos": 153, "type": "METRIC", "confidence": 0.7099696397781372}, {"text": "THEORY", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.5478591322898865}, {"text": "Stanford Dependencies (SD) scheme", "start_pos": 196, "end_pos": 229, "type": "DATASET", "confidence": 0.836569090684255}]}, {"text": "We obtain the set of basic stanford dependency trees using the software of de Marneffe et al. and train the dependency parsers directly on it.", "labels": [], "entities": []}, {"text": "\u2022 THE CONSTITUENCY-BASED THEORY is the theory reflected in the phrase-structure representation of the PTB () enriched with function labels compatible with the Stanford Dependencies (SD) scheme.", "labels": [], "entities": [{"text": "THE CONSTITUENCY-BASED THEORY", "start_pos": 2, "end_pos": 31, "type": "METRIC", "confidence": 0.6652616659800211}, {"text": "PTB", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.8505661487579346}, {"text": "Stanford Dependencies (SD) scheme", "start_pos": 159, "end_pos": 192, "type": "DATASET", "confidence": 0.751530776421229}]}, {"text": "We obtain trees that reflect this theory by TL-Unification of the PTB multifunction trees with the SD multi-function trees (PTB tl SD) as illustrated in.", "labels": [], "entities": [{"text": "TL-Unification", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.7057013511657715}]}, {"text": "The theory encoded in the multi-function trees corresponding to SD is different from the one obtained by our TL-Unification, as maybe seen from the difference between the flat SD multifunction tree and the result of the PTB tl SD in.", "labels": [], "entities": [{"text": "PTB tl SD", "start_pos": 220, "end_pos": 229, "type": "DATASET", "confidence": 0.8595280051231384}]}, {"text": "Another difference concerns coordination structures, encoded as binary branching trees in SD and as flat productions in the PTB tl SD.", "labels": [], "entities": [{"text": "PTB tl SD", "start_pos": 124, "end_pos": 133, "type": "DATASET", "confidence": 0.9366493821144104}]}, {"text": "Such differences are not only observable but also quantifiable, and using our redefined TED metric the cross-theory overlap is 0.8571.", "labels": [], "entities": [{"text": "overlap", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.5630578994750977}]}, {"text": "The two dependency parsers were trained using the same settings as in, using SVMTool () to predict part-of-speech tags at parsing time.", "labels": [], "entities": []}, {"text": "The two constituency parsers were used with default settings and were allowed to predict their own partof-speech tags.", "labels": [], "entities": []}, {"text": "We report three different evaluation metrics for the different experiments: Note that some PTB nodes remain without an SD label.", "labels": [], "entities": []}, {"text": "\u2022 LAS/UAS ( \u2022 PARSEVAL ( \u2022 TEDEVAL as defined in Section 3 We use LAS/UAS for dependency parsers that were trained on the same dependency theory.", "labels": [], "entities": []}, {"text": "We use ParseEval to evaluate phrase-structure parsers that were trained on PTB trees in which dashfeatures and empty traces are removed.", "labels": [], "entities": [{"text": "PTB trees", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.9053930640220642}]}, {"text": "We use our implementation of TEDEVAL to evaluate parsing results across all frameworks under two different scenarios: 3 TEDEVAL SINGLE evaluates against the native gold multi-function trees.", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 120, "end_pos": 127, "type": "METRIC", "confidence": 0.8855034708976746}]}, {"text": "TEDEVAL MULTIPLE evaluates against the generalized (cross-theory) multi-function trees.", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6277574300765991}, {"text": "MULTIPLE", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.49777689576148987}]}, {"text": "Unlabeled TEDEVAL scores are obtained by simply removing all labels from the multi-function nodes, and using unlabeled edit operations.", "labels": [], "entities": []}, {"text": "We calculate pairwise statistical significance using a shuffling test with 10K iterations.", "labels": [], "entities": []}, {"text": "present the results of our crossframework evaluation for English Parsing.", "labels": [], "entities": [{"text": "English Parsing", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.6392292380332947}]}, {"text": "In the left column of we report ParsEval scores for constituency-based parsers.", "labels": [], "entities": [{"text": "ParsEval", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.876950740814209}]}, {"text": "As expected, FScores for the Brown parser are higher than the F-Scores of the Berkeley parser.", "labels": [], "entities": [{"text": "FScores", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9972357153892517}, {"text": "F-Scores", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.990113377571106}]}, {"text": "F-Scores are however not applicable across frameworks.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9300380945205688}]}, {"text": "In the rightmost column of we report the LAS/UAS results for all parsers.", "labels": [], "entities": [{"text": "LAS/UAS", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.6845125754674276}]}, {"text": "If a parser yields a constituency tree, it is converted to and evaluated on SD.", "labels": [], "entities": []}, {"text": "Here we see that MST outperforms Malt, though the differences for labeled dependencies are insignificant.", "labels": [], "entities": [{"text": "MST", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.6140187978744507}, {"text": "Malt", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8687441945075989}]}, {"text": "We also observe here a familiar pattern from and others, where the constituency parsers significantly outperform the dependency parsers after conversion of their output into dependencies.", "labels": [], "entities": []}, {"text": "The conversion to SD allows one to compare results across formal frameworks, but not without a cost.", "labels": [], "entities": []}, {"text": "The conversion introduces a set of annotation specific decisions which may introduce a bias into the evaluation.", "labels": [], "entities": []}, {"text": "In the middle column of we report the TEDEVAL metrics measured against the generalized gold standard for all parsing frameworks.", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9879642724990845}]}, {"text": "We can now confirm that the constituency-based parsers significantly outperform the dependency parsers, and that this is not due to specific theoretical decisions which are seen to affect LAS/UAS metrics ().", "labels": [], "entities": [{"text": "LAS/UAS", "start_pos": 188, "end_pos": 195, "type": "TASK", "confidence": 0.6011656125386556}]}, {"text": "For the dependency parsers we now see that Malt outperforms MST on labeled dependencies slightly, but the difference is insignificant.", "labels": [], "entities": []}, {"text": "The fact that the discrepancy in theoretical assumptions between different frameworks indeed affects the conversion-based evaluation procedure is reflected in the results we report in.", "labels": [], "entities": []}, {"text": "Here the leftmost and rightmost columns report TEDEVAL scores against the own native gold (SINGLE) and the middle column against the generalized gold (MULTIPLE).", "labels": [], "entities": [{"text": "TEDEVAL", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.986810564994812}, {"text": "SINGLE", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9031955003738403}, {"text": "MULTIPLE", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.8870388269424438}]}, {"text": "Had the theories for SD and PTB tl SD been identical, TEDEVAL SINGLE and TEDEVAL MULTIPLE would have been equal in each line.", "labels": [], "entities": [{"text": "PTB tl SD", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.9132869442303976}, {"text": "TEDEVAL SINGLE", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.8593105971813202}, {"text": "TEDEVAL MULTIPLE", "start_pos": 73, "end_pos": 89, "type": "METRIC", "confidence": 0.6773430705070496}]}, {"text": "Because of theoretical discrepancies, we see small gaps in parser performance between these cases.", "labels": [], "entities": []}, {"text": "Our protocol ensures that such discrepancies do not bias the results.", "labels": [], "entities": []}], "tableCaptions": []}