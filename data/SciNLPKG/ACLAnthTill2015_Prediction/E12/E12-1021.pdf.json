{"title": [{"text": "Incorporating Lexical Priors into Topic Models", "labels": [], "entities": [{"text": "Incorporating Lexical Priors", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8042787710825602}]}], "abstractContent": [{"text": "Topic models have great potential for helping users understand document corpora.", "labels": [], "entities": []}, {"text": "This potential is stymied by their purely un-supervised nature, which often leads to topics that are neither entirely meaningful nor effective in extrinsic tasks (Chang et al., 2009).", "labels": [], "entities": []}, {"text": "We propose a simple and effective way to guide topic models to learn topics of specific interest to a user.", "labels": [], "entities": []}, {"text": "We achieve this by providing sets of seed words that a user believes are representative of the underlying topics in a corpus.", "labels": [], "entities": []}, {"text": "Our model uses these seeds to improve both topic-word distributions (by biasing topics to produce appropriate seed words) and to improve document-topic distributions (by biasing documents to select topics related to the seed words they contain).", "labels": [], "entities": []}, {"text": "Extrinsic evaluation on a document clustering task reveals a significant improvement when using seed information, even over other models that use seed information na\u00a8\u0131velyna\u00a8\u0131vely.", "labels": [], "entities": [{"text": "document clustering task", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7690508464972178}]}], "introductionContent": [{"text": "Topic models such as Latent Dirichlet Allocation (LDA) () have emerged as a powerful tool to analyze document collections in an unsupervised fashion.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.6642342309157053}]}, {"text": "When fit to a document collection, topic models implicitly use document level co-occurrence information to group semantically related words into a single topic.", "labels": [], "entities": []}, {"text": "Since the objective of these models is to maximize the probability of the observed data, they have a tendency to explain only the most obvious and superficial aspects of a corpus.", "labels": [], "entities": []}, {"text": "They effectively sacrifice performance on rare topics to do a better job in modeling frequently occurring words.", "labels": [], "entities": []}, {"text": "The user is then left with a skewed impression of the corpus, and perhaps one that does not perform well in extrinsic tasks.", "labels": [], "entities": []}, {"text": "To illustrate this problem, we ran LDA on the most frequent five categories of the Reuters-21578 () text corpus.", "labels": [], "entities": [{"text": "LDA", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.7515273094177246}, {"text": "Reuters-21578 () text corpus", "start_pos": 83, "end_pos": 111, "type": "DATASET", "confidence": 0.9334547221660614}]}, {"text": "This document distribution is very skewed: more than half of the collection belongs to the most frequent category (\"Earn\").", "labels": [], "entities": [{"text": "Earn", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.7870832085609436}]}, {"text": "The five topics identified by the LDA are shown in.", "labels": [], "entities": []}, {"text": "A brief observation of the topics reveals that LDA has roughly allocated topics 1 & 2 for the most frequent class (\"Earn\") and one topic for the subsequent two frequent classes (\"Acquisition\" and \"Forex\") and merged the least two frequent classes (\"Crude\" and \"Grain\") into a single topic.", "labels": [], "entities": [{"text": "LDA", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8199244737625122}]}, {"text": "The red colored words in topic 5 correspond to the \"Crude\" class and blue words are from the \"Grain\" class.", "labels": [], "entities": [{"text": "Grain\" class", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.7406531969706217}]}, {"text": "This leads to the situation where the topics identified by LDA are not in accordance with the underlying topical structure of the corpus.", "labels": [], "entities": []}, {"text": "This is a problem not just with LDA: it is potentially a problem with any extension thereof that have focused on improving the semantic coherence of the words in each topic (, the document topic distributions () or other aspects.", "labels": [], "entities": []}, {"text": "We address this problem by providing some additional information to the model.", "labels": [], "entities": []}, {"text": "Initially, along with the document collection, a user may provide higher level view of the document collection.", "labels": [], "entities": []}, {"text": "For instance, as discussed in Section 4.4, when run on historical NIPS papers, LDA fails to find topics related to Brain Imaging, Cognitive Science or Hardware, even though we know from the call for mln, dlrs, billion, year, pct, company, share, april, record, cts, quarter, march, earnings, stg, first, pay mln, NUM, cts, loss, net, dlrs, shr, profit, revs, year, note, oper, avg, shrs, sales, includes lt, company, shares, corp, dlrs, stock, offer, group, share, common, board, acquisition, shareholders bank, market, dollar, pct, exchange, foreign, trade, rate, banks, japan, yen, government, rates, today oil, tonnes, prices, mln, wheat, production, pct, gas, year, grain, crude, price, corn, dlrs, bpd, opec: Topics identified by LDA on the frequent-5 categories of the Reuters corpus.", "labels": [], "entities": [{"text": "lt, company, shares, corp, dlrs, stock, offer, group, share, common, board, acquisition, shareholders bank, market, dollar, pct, exchange, foreign, trade, rate, banks, japan, yen, government, rates, today oil, tonnes, prices, mln, wheat, production, pct, gas, year, grain, crude, price, corn", "start_pos": 404, "end_pos": 695, "type": "Description", "confidence": 0.8413645803154289}, {"text": "Reuters corpus", "start_pos": 775, "end_pos": 789, "type": "DATASET", "confidence": 0.8998775780200958}]}, {"text": "The categories are Earn, Acquisition, Forex, Grain and Crude (in the order document frequency).", "labels": [], "entities": [{"text": "Acquisition", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.8659001588821411}, {"text": "Grain", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9880669116973877}]}, {"text": "1 company, billion, quarter, shrs, earnings 2 acquisition, procurement, merge 3 exchange, currency, trading, rate, euro 4 grain, wheat, corn, oilseed, oil 5 natural, gas, oil, fuel, products, petrol papers that such topics should exist in the corpus.", "labels": [], "entities": []}, {"text": "By allowing the user to provide some seed words related to these underrepresented topics, we encourage the model to find evidence of these topics in the data.", "labels": [], "entities": []}, {"text": "Importantly, we only encourage the model to follow the seed sets and do not force it.", "labels": [], "entities": []}, {"text": "So if it has compelling evidence in the data to overcome the seed information then it still has the freedom to do so.", "labels": [], "entities": []}, {"text": "Our seeding approach in combination with the interactive topic modeling () will allow a user to both explore a corpus, and also guide the exploration towards the distinctions that he/she finds more interesting.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate different aspects of the model separately.", "labels": [], "entities": []}, {"text": "Our experimental setup proceeds as follows: a) Using an existing model, we evaluate the effectiveness of automatically derived constraints indicating the potential benefits of adding seed words into the topic models.", "labels": [], "entities": []}, {"text": "b) We evaluate each of our proposed models in different settings and compare with multiple baseline systems.", "labels": [], "entities": []}, {"text": "Since our aim is to overcome the dominance of majority topics by encouraging the topicality structure identified by the topic models to align with that of the document corpus, we choose extrinsic evaluation as the primary evaluation method.", "labels": [], "entities": []}, {"text": "We use document clustering task and use frequent-5 categories of Reuters-21578 corpus () and four classes from the 20 Newsgroups data set (i.e.'rec.autos', 'sci.electronics', 'comp.hardware' and 'alt.atheism').", "labels": [], "entities": [{"text": "Reuters-21578 corpus", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.9782220125198364}, {"text": "Newsgroups data set", "start_pos": 118, "end_pos": 137, "type": "DATASET", "confidence": 0.8402844270070394}]}, {"text": "For both the corpora we do the standard preprocessing of removing stopwords and infrequent words (.", "labels": [], "entities": []}, {"text": "For all the models, we use a Collapsed Gibbs sampler () for the inference process.", "labels": [], "entities": []}, {"text": "We use the standard hyperparameters values \u03b1 = 1.0, \u03b2 = 0.01 and \u03c4 = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters  We run all the models with the same number of topics as the number of clusters.", "labels": [], "entities": []}, {"text": "Then, for each document, we find the topic that has maximum probability in the posterior document-topic distribution and assign it to that cluster.", "labels": [], "entities": []}, {"text": "The accuracy of the document clustering is measured in terms of F-measure and Variation of Information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994611144065857}, {"text": "F-measure", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9977515339851379}, {"text": "Variation", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9491687417030334}]}, {"text": "Fmeasure is calculated based on the pairs of documents, i.e. if two documents belong to a cluster in both ground truth and the clustering proposed by the system then it is counted as correct, otherwise it is counted as wrong.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9146752953529358}]}, {"text": "Variational Information (VI) of two clusterings X and Y is given as (Meil\u02d8 a, 2007): where H(X) denotes the entropy of the clustering X and I(X, Y ) denotes the mutual information between the two clusterings.", "labels": [], "entities": []}, {"text": "For VI, a lower value indicates a better clustering.", "labels": [], "entities": [{"text": "VI", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.6291484236717224}]}, {"text": "All the accuracies are averaged over 25 different random initializations and all the significance results are measured using the t-test at p = 0.01.", "labels": [], "entities": []}, {"text": "We ran LDA and SeededLDA models on the NIPS papers from 2001 to 2010.", "labels": [], "entities": [{"text": "NIPS papers from 2001", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.9722092598676682}]}, {"text": "For this corpus, the seed words are chosen from the call for proposal., barrels, energy, first, petroleum 0, mln, cts, net, loss, 2, dlrs, shr, 3, profit, 4, 5, 6, revs, 7, 9, 8, year, note, 1986, 10, 0, sales tonnes, wheat, mln, grain, week, corn, department, year, export, program, agriculture, 0, soviet, prices bank, market, pct, dollar, exchange, billion, stg, today, foreign, rate, banks, japan, yen, rates, trade  There are 10 major areas with sub areas under each of them.", "labels": [], "entities": []}, {"text": "We ran both the models with 10 topics.", "labels": [], "entities": []}, {"text": "For SeededLDA, the words in each of the areas are selected as seed words and we filter out the ambiguous seed words.", "labels": [], "entities": []}, {"text": "Upon a qualitative observation of the output topics, we found that LDA has identified seven major topics and left out \"Brain Imaging\", \"Cognitive Science and Artificial Intelligence\" and \"Hardware Technologies\" areas.", "labels": [], "entities": [{"text": "LDA", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.7925803661346436}, {"text": "Brain Imaging", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.6947295963764191}]}, {"text": "Not surprisingly, but reassuringly, these areas are underrepresented among the NIPS papers.", "labels": [], "entities": [{"text": "NIPS papers", "start_pos": 79, "end_pos": 90, "type": "DATASET", "confidence": 0.9697680175304413}]}, {"text": "On the other hand, SeededLDA successfully identifies all of the major topics.", "labels": [], "entities": [{"text": "SeededLDA", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.8986298441886902}]}, {"text": "The topics identified by LDA and SeededLDA are shown in the supplementary material.", "labels": [], "entities": [{"text": "LDA", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8267919421195984}]}], "tableCaptions": [{"text": " Table 4: Accuracies on document clustering task with different models.  *  indicates significant improvement  compared to the z-labels approach, as measured by the t-test with p = 0.01. The relative performance gains are  with respect to the LDA model and are provided for comparison with Dirichlet Forest method (in", "labels": [], "entities": [{"text": "document clustering", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7582037448883057}]}, {"text": " Table 5: Topics identified by SeededLDA on the frequent-5 categories of Reuters corpus", "labels": [], "entities": [{"text": "frequent-5 categories of Reuters corpus", "start_pos": 48, "end_pos": 87, "type": "DATASET", "confidence": 0.8699232935905457}]}, {"text": " Table 6: Effect of ambiguous seed words on Seed- edLDA.", "labels": [], "entities": [{"text": "Seed- edLDA", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.617606540520986}]}]}