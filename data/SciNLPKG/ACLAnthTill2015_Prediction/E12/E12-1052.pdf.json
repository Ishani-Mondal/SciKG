{"title": [{"text": "Joint Satisfaction of Syntactic and Pragmatic Constraints Improves Incremental Spoken Language Understanding", "labels": [], "entities": [{"text": "Satisfaction of Syntactic and Pragmatic Constraints Improves Incremental Spoken Language Understanding", "start_pos": 6, "end_pos": 108, "type": "TASK", "confidence": 0.7006830844012174}]}], "abstractContent": [{"text": "We present a model of semantic processing of spoken language that (a) is robust against ill-formed input, such as can be expected from automatic speech recognisers, (b) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, (c) uses a principled, expressive semantic representation formalism (RMRS) with a well-defined model theory , and (d) works continuously (produc-ing meaning representations on a word-byword basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far).", "labels": [], "entities": []}, {"text": "We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline).", "labels": [], "entities": []}], "introductionContent": [{"text": "Incremental processing for spoken dialogue systems (i. e., the processing of user input even while it still maybe extended) has received renewed attention recently).", "labels": [], "entities": [{"text": "Incremental processing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8674038946628571}]}, {"text": "Most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g. ( )), but has otherwise followed atypical pipeline architecture where processing results are passed only in one direction towards the next module.", "labels": [], "entities": []}, {"text": "In this paper, we investigate whether the other potential advantage of incremental processingproviding \"higher-level\"-feedback to lower-level modules, in order to improve subsequent processing of the lower-level module-can be realised as well.", "labels": [], "entities": []}, {"text": "Specifically, we experimented with giving a syntactic parser feedback about whether semantic readings of nominal phrases it is in the process of constructing have a denotation in the given context or not.", "labels": [], "entities": []}, {"text": "Based on the assumption that speakers do plan their referring expressions so that they can successfully refer, we use this information to re-rank derivations; this in turn has an influence on how the derivations are expanded, given continued input.", "labels": [], "entities": []}, {"text": "As we show in our experiments, fora corpus of realistic dialogue utterances collected in a Wizard-of-Oz setting, this strategy led to an absolute improvement in computing the intended denotation of around 10 % over a baseline (even more using a more permissive metric), both for manually transcribed test data as well as for the output of automatic speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 349, "end_pos": 367, "type": "TASK", "confidence": 0.6985608488321304}]}, {"text": "The remainder of this paper is structured as follows: We discuss related work in the next section, and then describe in general terms our model and its components.", "labels": [], "entities": []}, {"text": "In Section 4 we then describe the data resources we used for the experiments and the actual implementation of the model, the baselines for comparison, and the results of our experiments.", "labels": [], "entities": []}, {"text": "We close with a discussion and an outlook on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our parser, semantic construction and reference resolution modules are implemented within the InproTK toolkit for incremental spoken dialogue systems development ( . In this toolkit, incremental hypotheses are modified as more information becomes available overtime.", "labels": [], "entities": [{"text": "semantic construction", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7153109461069107}, {"text": "reference resolution", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7262052744626999}]}, {"text": "Our modules support all such modifications (i. e. also allow to revert their states and output if word input is revoked).", "labels": [], "entities": []}, {"text": "As explained in Section 4.1, we used offline recognition results in our evaluation.", "labels": [], "entities": []}, {"text": "However, the results would be identical if we were to use the incremental speech recognition output of InproTK directly.", "labels": [], "entities": [{"text": "InproTK", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9203684329986572}]}, {"text": "The system performs several times faster than real-time on a standard workstation computer.", "labels": [], "entities": []}, {"text": "We thus consider it ready to improve practical end-toend incremental systems which perform withinturn actions such as those outlined in . The parser was run with a base-beam factor of 0.01; this parameter may need to be adjusted if a larger grammar was used.", "labels": [], "entities": []}, {"text": "shows an overview of the experiment results.", "labels": [], "entities": []}, {"text": "The table lists, separately for the manual transcriptions and the ASR transcripts, first the number of times that the final reading did not resolve at all, or to a wrong entitiy; did not uniquely resolve, but included the correct entity in its denotiation; or did uniquely resolve to the correct entity (-1, 0, and 1, respectively).", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.8106006383895874}]}, {"text": "The next lines show \"strict accuracy\" (proportion of \"1\" among all results) at the end of utterance, and \"relaxed accuracy\" (which allows ambiguity, i.e., is the set {0, 1}).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8716402053833008}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.6853566765785217}]}, {"text": "incr.scr is the incremental score as described above, which includes in the evaluation the development of references and not just the final state.", "labels": [], "entities": []}, {"text": "(And in that sense, is the most appropriate metric here, as it captures the incremental behaviour.)", "labels": [], "entities": []}, {"text": "This score is shown both as absolute  number as well as averaged for each utterance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of logical forms (flattened into first-order base-language formulae) and reference resolution  results for incrementally parsing and resolving 'nimm den winkel in der dritten reihe'", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7317951321601868}]}, {"text": " Table 2: Results of the Experiments. See text for explanation of metrics.", "labels": [], "entities": []}]}