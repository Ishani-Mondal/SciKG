{"title": [{"text": "Mining Co-Occurrence Matrices for SO-PMI Paradigm Word Candidates", "labels": [], "entities": [{"text": "SO-PMI Paradigm Word Candidates", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.7333503887057304}]}], "abstractContent": [{"text": "This paper is focused on one aspect of SO-PMI, an unsupervised approach to sentiment vocabulary acquisition proposed by Turney (Turney and Littman, 2003).", "labels": [], "entities": [{"text": "SO-PMI", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.9659482836723328}, {"text": "sentiment vocabulary acquisition", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.8570640881856283}]}, {"text": "The method, originally applied and evaluated for English, is often used in bootstrap-ping sentiment lexicons for European languages where no such resources typically exist.", "labels": [], "entities": []}, {"text": "In general, SO-PMI values are computed from word co-occurrence frequencies in the neighbourhoods of two small sets of paradigm words.", "labels": [], "entities": []}, {"text": "The goal of this work is to investigate how lexeme selection affects the quality of obtained sentiment estimations.", "labels": [], "entities": [{"text": "sentiment estimations", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7081554532051086}]}, {"text": "This has been achieved by comparing ad hoc random lexeme selection with two alternative heuristics, based on clustering and SVD decomposition of a word co-occurrence matrix, demonstrating superiority of the latter methods.", "labels": [], "entities": []}, {"text": "The work can be also interpreted as sensitivity analysis on SO-PMI with regard to paradigm word selection.", "labels": [], "entities": [{"text": "paradigm word selection", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.5824845234553019}]}, {"text": "The experiments were carried out for Polish.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper seeks to improve one of the main methods of unsupervised lexeme sentiment polarity assignment.", "labels": [], "entities": [{"text": "lexeme sentiment polarity assignment", "start_pos": 68, "end_pos": 104, "type": "TASK", "confidence": 0.7004519402980804}]}, {"text": "The method, introduced by, is described in more detail in Section 2.", "labels": [], "entities": []}, {"text": "It relies on two sets of paradigm words, positive and negative, which determine the polarity of unseen words.", "labels": [], "entities": []}, {"text": "The method is resource lean and therefore often used in languages other than English.", "labels": [], "entities": []}, {"text": "Recent examples include Japanese ( and German ().", "labels": [], "entities": []}, {"text": "Unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc manner.", "labels": [], "entities": []}, {"text": "One notable example of manual paradigm word selection method was presented in.", "labels": [], "entities": [{"text": "manual paradigm word selection", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.5484530925750732}]}, {"text": "In this context, an interesting variation of the semantic orientation-pointwise mutual information (SO-PMI) algorithm for Japanese was suggested by (.", "labels": [], "entities": []}, {"text": "Authors, motivated by excessive leaning toward positive opinions, proposed to modify the algorithm by introducing balancing factor and detecting neutral expressions.", "labels": [], "entities": []}, {"text": "As will be demonstrated, this problem can be addressed by proper selection of paradigm pairs.", "labels": [], "entities": []}, {"text": "One not entirely realistic, but nevertheless interesting theoretical possibility is to pick pairs of opposing adjectives with the highest loadings identified in Osgood's experiments on semantic differential ().", "labels": [], "entities": []}, {"text": "In the experiments, respondents were presented with a noun and asked to choose its appropriate position on a scale between two bipolar adjectives (for example: adequate-inadequate, valuable-worthless, hot-cold).", "labels": [], "entities": []}, {"text": "Factor analysis of the results revealed three distinctive factors, called Osgood dimensions.", "labels": [], "entities": []}, {"text": "The first of the dimensions, often considered synonymous with the notion of sentiment, was called Evaluative because its foundational adjective pair (one with the highest loading) is goodbad.", "labels": [], "entities": []}, {"text": "The first problem with using adjective pairs as exemplary for word co-occurrence distributions on the basis of their loadings, is the fact that factor loadings as measured by Osgood et al. are not necessarily reflected in word frequency phenomena.", "labels": [], "entities": []}, {"text": "The second problem is translation: an adjective pair, central in English, may not be as strongly associated with a dimension (here: Evaluative) in other languages and cultures.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9849955439567566}]}, {"text": "The approach we suggest in this paper assumes a latent structure behind word co-occurrence frequencies.", "labels": [], "entities": []}, {"text": "The structure maybe seen as a mixture of latent variables of unknown distributions that drives word selection.", "labels": [], "entities": [{"text": "word selection", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.852052241563797}]}, {"text": "Some of the variables are more likely to produce certain types of highly evaluative words (words with high sentiment scores).", "labels": [], "entities": []}, {"text": "We do not attempt to model the structure in a generative way as in for example probabilistic latent semantic analysis (PLSA) or latent Dirichlet allocation (LDA).", "labels": [], "entities": [{"text": "probabilistic latent semantic analysis (PLSA)", "start_pos": 79, "end_pos": 124, "type": "TASK", "confidence": 0.7317618174212319}]}, {"text": "A generative approximation is not feasible when using corpora such as the balanced, 300-million version of the National Corpus of Polish (NKJP) () 1 applied in the experiments described in the next sections, which does not enable creating a word-document matrix and organizing word occurrences by documents or narrowly specified topics.", "labels": [], "entities": [{"text": "National Corpus of Polish (NKJP)", "start_pos": 111, "end_pos": 143, "type": "DATASET", "confidence": 0.939179071358272}]}, {"text": "Therefore, we propose different techniques.", "labels": [], "entities": []}, {"text": "We begin with asymmetric matrix of word cooccurences and attempt to discover as much of its structure as possible using two well established techniques: Singular Value Decomposition and clustering.", "labels": [], "entities": []}, {"text": "The discovered structures are then used to optimize the selection of words for paradigm sets used in SO-PMI.", "labels": [], "entities": [{"text": "SO-PMI", "start_pos": 101, "end_pos": 107, "type": "TASK", "confidence": 0.9014818668365479}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we define the SO-PMI measure and briefly formulate the problem.", "labels": [], "entities": [{"text": "SO-PMI measure", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.7363599538803101}]}, {"text": "Section 3 describes obtaining the set of sentiment word candidates, which are then used to generate asymmetric co-occurence matrix as outlined in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 delineates the details of human word scoring, which serves as a basis for evaluations in 9.", "labels": [], "entities": [{"text": "human word scoring", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6204143563906351}]}, {"text": "Sections 6, 7 and 8 describe three distinct approaches to paradigm sets generation.", "labels": [], "entities": [{"text": "paradigm sets generation", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6630046765009562}]}], "datasetContent": [{"text": "Using continous SO-PMI and multi point scales for human scoring facilitates formulating the problem as a regression one, where goodness of fit of the estimations can be computed using different measures than in the case of classification.", "labels": [], "entities": []}, {"text": "This, however, demands a mapping such that ranges of the continuous SO-PMI scale correspond to discrete human scores.", "labels": [], "entities": []}, {"text": "We propose to base such a mapping on dividing the SO-PMI range into 10 segments {s 0 , ..., s 10 } of various length, each of which corresponds to one discrete human value.", "labels": [], "entities": []}, {"text": "The choice of values (locations) of specific points is a subject of minimization where the error function E over a set of words Wis as follows: For each word w, the distance function dist returns the number of segments between the correct segment s c and the estimated segment s e using the SO-PMI.", "labels": [], "entities": [{"text": "error function E", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.8725847005844116}]}, {"text": "We minimize E and find optimum locations for points separating each segment using Powell's conjugate direction method, determined the most effective for this task.", "labels": [], "entities": []}, {"text": "Powell's algorithm is a non-gradient numerical optimization technique, applicable to areal valued function which does not need not be differentiable.", "labels": [], "entities": []}, {"text": "presents E errors and extreme (min and max) SO-PMI values computed over two independent samples of 500 lexemes.", "labels": [], "entities": [{"text": "E errors", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9662152230739594}, {"text": "SO-PMI", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8754197955131531}]}, {"text": "Error columns indicated as E denote errors computed either on nonoptimized default (def ) or optimized segments (min).", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9514057040214539}]}, {"text": "Each combination of paradigm words and each sample required re-computing optimum values of points dividing the SO-PMI scale into segments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frobenius norm of the spectrum for 10, 20  and 100 first eigenvalues.", "labels": [], "entities": [{"text": "Frobenius norm", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8505945801734924}]}, {"text": " Table 2: SO-PMI ranges and error (E) values on two  independent random samples of N=500. 3 randomized  selections (r 1 \u2212 r 3 ), Affinity Propagation (af f ) and  SVD (svd).", "labels": [], "entities": [{"text": "error (E)", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8823778927326202}, {"text": "Affinity", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.991214394569397}, {"text": "SVD", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.7740074992179871}]}]}