{"title": [{"text": "Detecting Highly Confident Word Translations from Comparable Corpora without Any Prior Knowledge", "labels": [], "entities": [{"text": "Detecting Highly Confident Word Translations from Comparable Corpora without Any Prior Knowledge", "start_pos": 0, "end_pos": 96, "type": "TASK", "confidence": 0.8559599816799164}]}], "abstractContent": [{"text": "In this paper, we extend the work on using latent cross-language topic models for identifying word translations across comparable corpora.", "labels": [], "entities": [{"text": "identifying word translations", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.7904549638430277}]}, {"text": "We present a novel precision-oriented algorithm that relies on per-topic word distributions obtained by the bilingual LDA (BiLDA) latent topic model.", "labels": [], "entities": []}, {"text": "The algorithm aims at harvesting only the most probable word translations across languages in a greedy fashion, without any prior knowledge about the language pair, relying on a symmetrization process and the one-to-one constraint.", "labels": [], "entities": []}, {"text": "We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin.", "labels": [], "entities": []}, {"text": "In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicons serve as an invaluable resource of knowledge in various natural language processing tasks, such as dictionary-based crosslanguage information retrieval () and statistical machine translation (SMT).", "labels": [], "entities": [{"text": "crosslanguage information retrieval", "start_pos": 135, "end_pos": 170, "type": "TASK", "confidence": 0.5850590268770853}, {"text": "statistical machine translation (SMT)", "start_pos": 178, "end_pos": 215, "type": "TASK", "confidence": 0.8217198650042216}]}, {"text": "In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand.", "labels": [], "entities": []}, {"text": "Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing.", "labels": [], "entities": []}, {"text": "Therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.", "labels": [], "entities": []}, {"text": "Thus, it is much easier to build a high-volume comparable corpus.", "labels": [], "entities": []}, {"text": "A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics).", "labels": [], "entities": []}, {"text": "Over the years, several approaches for mining translations from non-parallel corpora have emerged, all sharing the same Firthian assumption, often called the distributionial hypothesis, which states that words with a similar meaning are likely to appear in similar contexts across languages.", "labels": [], "entities": [{"text": "mining translations from non-parallel corpora", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.7803664326667785}]}, {"text": "All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need fora seed lexicon of translations to efficiently bridge the gap between languages.", "labels": [], "entities": []}, {"text": "That seed lexicon is usually crawled from the Web or obtained from parallel corpora.", "labels": [], "entities": []}, {"text": "Recently, have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, based on improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.998859167098999}, {"text": "bilingual lexicon extraction", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.6876828571160635}]}, {"text": "Other methods such as () try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.", "labels": [], "entities": []}, {"text": "However, the quality of their initial seed lexicon is disputable, since the construction of their lexicon is languagepair biased and cannot be completely employed on distant languages.", "labels": [], "entities": []}, {"text": "It solely relies on unsatisfactory language-pair independent cross-language clues such as words shared across languages.", "labels": [], "entities": []}, {"text": "Recent work from Vuli\u00b4cVuli\u00b4c et al.(2011) utilized the distributional hypothesis in a different direction.", "labels": [], "entities": []}, {"text": "It attempts to abrogate the need of a seed lexicon as a prerequisite for bilingual lexicon extraction.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6753460268179575}]}, {"text": "They train a cross-language topic model on document-aligned comparable corpora and introduce different methods for identifying word translations across languages, underpinned by pertopic word distributions from the trained topic model.", "labels": [], "entities": [{"text": "identifying word translations across languages", "start_pos": 115, "end_pos": 161, "type": "TASK", "confidence": 0.7847759604454041}]}, {"text": "Due to the fact that they deal with comparable Wikipedia data, their translation model contains a lot of noise, and some words are poorly translated simply because there are not enough occurrences in the corpus.", "labels": [], "entities": []}, {"text": "The goal of this work is to design an algorithm which will learn to harvest only the most probable translations from the perword topic distributions.", "labels": [], "entities": []}, {"text": "The translations learned by the algorithm then might serve as a highly accurate, precision-based initial seed lexicon, which can then be used as a tool for translating source word vectors into the target language.", "labels": [], "entities": [{"text": "precision-based", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.9862776398658752}]}, {"text": "The key advantage of such a lexicon lies in the fact that there is no language-pair dependent prior knowledge involved in its construction (e.g., orthographic features).", "labels": [], "entities": []}, {"text": "Hence, it is completely applicable to any language pair for which there exist sufficient comparable data for training of the topic model.", "labels": [], "entities": []}, {"text": "Since comparable corpora often construct a very noisy environment, it is of the utmost importance fora precision-oriented algorithm to learn when to stop the process of matching words, and which candidate pairs are surely not translations of each other.", "labels": [], "entities": [{"text": "precision-oriented", "start_pos": 103, "end_pos": 121, "type": "METRIC", "confidence": 0.9567950963973999}]}, {"text": "The method described in this paper follows this intuition: while extracting a bilingual lexicon, we try to rematch words, keeping only the most confident candidate pairs and disregarding all the others.", "labels": [], "entities": []}, {"text": "After that step, the most confident candidate pairs might be used with some of the existing context-based techniques to find translations for the words discarded in the previous step.", "labels": [], "entities": []}, {"text": "The algorithm is based on: (1) the assumption of symmetry, and (2) the one-to-one constraint.", "labels": [], "entities": [{"text": "symmetry", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.900808572769165}]}, {"text": "The idea of symmetrization has been borrowed from the symmetrization heuristics introduced for word alignments in SMT, where the intersection heuristics is employed fora precision-oriented algorithm.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7365182489156723}, {"text": "SMT", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.7137649655342102}]}, {"text": "In our setting, it basically means that we keep a translation pair (w Si , w T j ) if and only if, after the symmetrization process, the top translation candidate for the source word w Si is the target word w Ti and vice versa.", "labels": [], "entities": []}, {"text": "The one-to-one constraint aims at matching the most confident candidates during the early stages of the algorithm, and then excluding them from further search.", "labels": [], "entities": []}, {"text": "The utility of the constraint for parallel corpora has already been evaluated by.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief overview of the methods, relying on per-topic word distributions, which serve as the tool for computing crosslanguage similarity between words.", "labels": [], "entities": []}, {"text": "In Section 3, we motivate the main assumptions of the algorithm and describe the full algorithm.", "labels": [], "entities": []}, {"text": "Section 4 justifies the underlying assumptions of the algorithm by providing comparisons with a current-state-of-the-art system for Italian-English and Dutch-English language pairs.", "labels": [], "entities": []}, {"text": "It also contains another set of experiments which investigates the potential of the algorithm in building a language-pair unbiased seed lexicon, and compares the lexicon with other seed lexicons.", "labels": [], "entities": []}, {"text": "Finally, Section 5 lists conclusion and possible paths of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "All our experiments rely on BiLDA training with comparable data.", "labels": [], "entities": [{"text": "BiLDA", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.7994787096977234}]}, {"text": "Corpora and software for BiLDA training are obtained from Vuli\u00b4c.", "labels": [], "entities": [{"text": "BiLDA training", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7895627915859222}, {"text": "Vuli\u00b4c", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.9790383577346802}]}, {"text": "We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper.", "labels": [], "entities": []}, {"text": "The linear interpolation parameter for the combined TI+Cue method is set to \u03bb = 0.1.", "labels": [], "entities": []}, {"text": "The parameters of the algorithm, adjusted on a set of 500 randomly sampled Italian words, are set to the following values in all experiments, except where noted different: P 0 = 0.20, P f = 0.00, dec p = 0.01, N 0 = 3, and N f = 10.", "labels": [], "entities": []}, {"text": "The initial ground truth for our source vocabularies has been constructed by the freely available Google Translate tool.", "labels": [], "entities": []}, {"text": "The final ground truth for our test sets has been established after we have manually revised the list of pairs obtained by Google Translate, deleting incorrect entries and adding additional correct entries.", "labels": [], "entities": []}, {"text": "All translation candidates are evaluated against this benchmark lexicon.", "labels": [], "entities": []}, {"text": "With this set of experiments, we wanted to test whether both the assumption of symmetry and the one-to-one assumption are useful in improving precision of the initial TI+Cue lexicon extraction method.", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9985424280166626}, {"text": "TI+Cue lexicon extraction", "start_pos": 167, "end_pos": 192, "type": "TASK", "confidence": 0.5229352593421936}]}, {"text": "We compare three different lexicon extraction algorithms: (1) the basic TI+Cue extraction algorithm (LALG-BASIC) which serves as the baseline algorithm 5 , (2) the algorithm from Section 3, but without the one-to-one assumption (LALG-SYM), meaning that if we find a translation pair, we still keep words from the translation pair in their respective vocabularies, and (3) the complete algorithm from Section 3 (LALG-ALL).", "labels": [], "entities": [{"text": "TI+Cue extraction", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.5607329607009888}]}, {"text": "In order to evaluate these lexicon extraction algorithms for both Italian-English and Dutch-English, we have constructed a test set of 650 Italian nouns, and a test set of 1000 Dutch nouns of high and medium frequency.", "labels": [], "entities": []}, {"text": "Precision scores for both language pairs and for all lexicon extraction algorithms are provided in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9875859022140503}]}, {"text": "Based on these results, it is clearly visible that both assumptions our algorithm makes are valid  and contribute to better overall scores.", "labels": [], "entities": []}, {"text": "Therefore in all further experiments we will use the LALG-ALL extraction algorithm.", "labels": [], "entities": [{"text": "LALG-ALL extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8390627205371857}]}, {"text": "The next set of experiments aims at exploring how precision scores change while we gradually decrease threshold values.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 50, "end_pos": 66, "type": "METRIC", "confidence": 0.9686107039451599}]}, {"text": "The main goal of these experiments is to detect when to stop with the extraction of translation candidates in order to preserve a lexicon of only highly accurate translations.", "labels": [], "entities": []}, {"text": "We have fixed the maximum search space depth N 0 = N f = 3.", "labels": [], "entities": []}, {"text": "We used the same test sets from Experiment I. displays the change of precision in relation to different threshold values, where we start harvesting translations from the threshold P 0 = 0.2 down to P f = 0.0.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9967246651649475}]}, {"text": "Since our goal is to extract as many correct translation pairs as possible, but without decreasing the precision scores, we have also examined what impact this gradual decrease of threshold also has on the number of extracted translations.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9991945624351501}]}, {"text": "We have opted for the F \u03b2 measure (van: Since our task is precision-oriented, we have set \u03b2 = 0.5.", "labels": [], "entities": [{"text": "F \u03b2 measure", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.973030686378479}, {"text": "precision-oriented", "start_pos": 58, "end_pos": 76, "type": "METRIC", "confidence": 0.9958893656730652}]}, {"text": "F 0.5 measure values precision as twice as important as recall.", "labels": [], "entities": [{"text": "F 0.5 measure", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9464689095815023}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9985989928245544}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9987240433692932}]}, {"text": "The F 0.5 scores are also provided in.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9815006852149963}]}, {"text": "Finally, we wanted to test how many accurate translation pairs our best scoring LALG-ALL algorithm is able to acquire from the entire source vocabulary, with very high precision still remaining paramount.", "labels": [], "entities": [{"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9981639981269836}]}, {"text": "The obtained highly-precise seed lexicon then might be employed for an additional bootstrapping procedure similar to () or simply for translating context vectors as in ().", "labels": [], "entities": []}, {"text": "If we do not know anything about a given language pair, we can only use words shared across languages as lexical clues for the construction of a seed lexicon.", "labels": [], "entities": []}, {"text": "It often leads to a low precision lexicon, since many false friends are detected.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9861931204795837}]}, {"text": "For Italian-English, we have found 431 nouns shared between the two languages, of which 350 were correct translations, leading to a precision of 0.8121.", "labels": [], "entities": [{"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9981855750083923}]}, {"text": "As an illustration, if we take the first 431 translation pairs retrieved by LALG-ALL, there are 427 correct translation pairs, leading to a precision of 0.9907.", "labels": [], "entities": [{"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9968763589859009}]}, {"text": "Some pairs do not share any orthographic similarities: (uccello, bird), (tastiera, keyboard), (salute, health), (terremoto, earthquake) etc.", "labels": [], "entities": []}, {"text": "Following, we have also employed simple transformation rules for the adoption of words from one language to another.", "labels": [], "entities": []}, {"text": "The rules specific to the Italian-English translation process that have been employed are: (R1) if an Italian noun ends in \u2212ione, but not in \u2212zione, strip the finale to obtain the corresponding English noun.", "labels": [], "entities": [{"text": "R1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.976983368396759}]}, {"text": "Otherwise, strip the suffix \u2212zione, and append \u2212tion; (R2) if a noun ends in \u2212ia, but not in \u2212zia or \u2212f ia, replace the suffix \u2212ia with \u2212y.", "labels": [], "entities": []}, {"text": "If a noun ends in \u2212zia, replace the suffix with \u2212cy and if a noun ends in \u2212f ia, replace  \u2022 A lexicon containing only words shared across languages (LEX-1).", "labels": [], "entities": [{"text": "LEX-1", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.883505642414093}]}, {"text": "\u2022 A lexicon containing shared words and translation pairs found by applying the languagespecific transformation rules (LEX-2).", "labels": [], "entities": [{"text": "LEX-2", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.8975456357002258}]}, {"text": "\u2022 A lexicon containing only translation pairs obtained by the LALG-ALL algorithm that score above a certain threshold P (LEX-LALG).", "labels": [], "entities": []}, {"text": "\u2022 A combination of the lexicons LEX-1 and LEX-LALG (LEX-1+LEX-LALG).", "labels": [], "entities": []}, {"text": "Nonmatching duplicates are resolved by taking the translation pair from LEX-LALG as the correct one.", "labels": [], "entities": []}, {"text": "Note that this lexicon is completely language-pair independent.", "labels": [], "entities": []}, {"text": "\u2022 A lexicon combining only translation pairs found by applying the language-specific transformation rules and LEX-LALG (LEX-R+LEX-LALG).", "labels": [], "entities": []}, {"text": "\u2022 A combination of the lexicons LEX-2 and LEX-LALG, where non-matching duplicates are resolved by taking the translation pair from LEX-LALG if it is present in LEX-1, and from LEX-2 otherwise (LEX-2+LEX-LALG).", "labels": [], "entities": []}, {"text": "According to the results from, we can conclude that adding translation pairs extracted by our LALG-ALL algorithm has a major positive impact on both precision and coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9992801547050476}, {"text": "coverage", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9481605887413025}]}, {"text": "Obtaining results for two different language pairs proves that the approach is generic and applicable to any other language pairs.", "labels": [], "entities": []}, {"text": "The previous approach relying on work from has been outperformed in terms of precision and coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9993973970413208}, {"text": "coverage", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9678005576133728}]}, {"text": "Additionally, we have shown that adding simple translation rules for languages sharing same roots might lead to even better scores (LEX-2+LEX-LALG).", "labels": [], "entities": [{"text": "LEX-2", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9527149796485901}, {"text": "LEX-LALG", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.7144864797592163}]}, {"text": "However, it is not always possible to rely on such knowledge, and the usefulness of the designed LALG-ALL algorithm really comes to the fore when the algorithm is applied on distant language pairs which do not share many words and cognates, and word translation rules cannot be easily established.", "labels": [], "entities": [{"text": "word translation", "start_pos": 245, "end_pos": 261, "type": "TASK", "confidence": 0.7244736105203629}]}, {"text": "In such cases, without any prior knowledge about the languages involved in a translation process, one is left with the linguistically unbiased LEX-1+LEX-LALG lexicon, which also displays a promising performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision scores on our test sets for the 3 dif- ferent lexicon extraction algorithms.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9782307147979736}]}, {"text": " Table 2: A comparison of different lexicons. For lexicons employing our LALG-ALL algorithm, only translation  candidates that scored above the threshold P = 0.11 have been kept.", "labels": [], "entities": []}]}