{"title": [{"text": "Skip N-grams and Ranking Functions for Predicting Script Events", "labels": [], "entities": [{"text": "Predicting Script Events", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8856617410977682}]}], "abstractContent": [{"text": "In this paper, we extend current state-of-the-art research on unsupervised acquisition of scripts, that is, stereotypical and frequently observed sequences of events.", "labels": [], "entities": []}, {"text": "We design, evaluate and compare different methods for constructing models for script event prediction: given a partial chain of events in a script, predict other events that are likely to belong to the script.", "labels": [], "entities": [{"text": "script event prediction", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7030584812164307}]}, {"text": "Our work aims to answer key questions about how best to (1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events.", "labels": [], "entities": [{"text": "predicting new script events", "start_pos": 199, "end_pos": 227, "type": "TASK", "confidence": 0.8535147458314896}]}, {"text": "We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models.", "labels": [], "entities": [{"text": "ranking event predictions", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.7464278737703959}]}], "introductionContent": [{"text": "There has been recent interest in automatically acquiring world knowledge in the form of scripts (, that is, frequently recurring situations that have a stereotypical sequence of events, such as a visit to a restaurant.", "labels": [], "entities": [{"text": "automatically acquiring world knowledge in the form of scripts (, that is, frequently recurring situations that have a stereotypical sequence of events, such as a visit to a restaurant", "start_pos": 34, "end_pos": 218, "type": "Description", "confidence": 0.7626057378947735}]}, {"text": "All of the techniques so far proposed for this task share a common sub-task: given an event or partial chain of events, predict other events that belong to the same script ().", "labels": [], "entities": []}, {"text": "Such a model can then serve as input to a system that identifies the order of the events within that script or that generates a story using the selected events.", "labels": [], "entities": []}, {"text": "In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script.", "labels": [], "entities": []}, {"text": "In particular, we consider the following questions: \u2022 How should representative chains of events be selected from the source text?", "labels": [], "entities": []}, {"text": "\u2022 Given an event chain, how should statistics be gathered from it?", "labels": [], "entities": []}, {"text": "\u2022 Given event n-gram statistics, which ranking function best predicts the events fora script?", "labels": [], "entities": []}, {"text": "In the process of answering these questions, this article makes several contributions to the field of script and narrative event chain understanding: \u2022 We explore for the first time the use of skipgrams for collecting narrative event statistics, and show that this approach performs better than classic n-gram statistics.", "labels": [], "entities": [{"text": "script and narrative event chain understanding", "start_pos": 102, "end_pos": 148, "type": "TASK", "confidence": 0.6529115289449692}]}, {"text": "\u2022 We propose anew method for ranking events given a partial script, and show that it performs substantially better than ranking methods from prior work.", "labels": [], "entities": []}, {"text": "\u2022 We propose anew evaluation procedure (using Recall@N) for the cloze test, and advocate its usage instead of average rank used previously in the literature.", "labels": [], "entities": []}, {"text": "\u2022 We provide a systematic analysis of the interactions between the choices made when constructing an event prediction model.", "labels": [], "entities": [{"text": "event prediction", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7179320156574249}]}, {"text": "Section 2 gives an overview of the prior work related to this task.", "labels": [], "entities": []}, {"text": "Section 3 lists and briefly describes different approaches that try to provide answers to the three questions posed in this introduction, while Section 4 presents the results of our experiments and reports on our findings.", "labels": [], "entities": []}, {"text": "Finally, Section 5 provides a conclusive discussion along with ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments aimed to answer three questions: Which event chains are worth keeping?", "labels": [], "entities": []}, {"text": "How should event bigram counts be collected?", "labels": [], "entities": []}, {"text": "And which ranking method is best for predicting script events?", "labels": [], "entities": [{"text": "predicting script events", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.9125120242436727}]}, {"text": "To answer these questions we use two corpora, the Reuters Corpus and the Andrew Lang Fairy Tale Corpus, to evaluate our three different chain selection methods, {all chains, long chains, the longest chain}, our three different bigram counting methods, {regular bigrams, 1-skip bigrams, 2-skip bigrams}, and our three different ranking methods, {Chambers & Jurafsky PMI, ordered PMI, bigram probabilities}.", "labels": [], "entities": [{"text": "Reuters Corpus", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9795297980308533}, {"text": "Andrew Lang Fairy Tale Corpus", "start_pos": 73, "end_pos": 102, "type": "DATASET", "confidence": 0.7965590000152588}]}, {"text": "We follow the approach of, evaluating our models for predicting script events in a narrative cloze task.", "labels": [], "entities": []}, {"text": "The narrative cloze task is inspired by the classic psychological cloze task in which subjects are given a sentence with a word missing and asked to fill in the blank.", "labels": [], "entities": []}, {"text": "Similarly, in the narrative cloze task, the system is given a sequence of events from a script where one event is missing, and asked to predict the missing event.", "labels": [], "entities": []}, {"text": "The difficulty of a cloze task depends a lot on the context around the missing item -in some cases it maybe quite predictable, but in many cases there is no single correct answer, though some answers are more probable than others.", "labels": [], "entities": []}, {"text": "Thus, performing well on a cloze task is more about ranking the missing event highly, and not about proposing a single \"correct\" event.", "labels": [], "entities": []}, {"text": "In this way, narrative cloze is like perplexity in a language model.", "labels": [], "entities": []}, {"text": "However, where perplexity measures how good the model is at predicting a script event given the previous events in the script, narrative cloze measures how good the model is at predicting what is missing between events in the script.", "labels": [], "entities": []}, {"text": "Thus narrative cloze is somewhat more appropriate to our task, and at the same time simplifies comparisons to prior work.", "labels": [], "entities": []}, {"text": "Rather than manually constructing a set of scripts on which to run the cloze test, we follow Chambers and Jurafsky in reserving a section of our parsed corpora for testing, and then using the event chains from that section as the scripts for which the system must predict events.", "labels": [], "entities": []}, {"text": "Given an event chain of length n, we run n cloze tests, with a different one of then events removed each time to create a partial script from the remaining n \u2212 1 events (see.", "labels": [], "entities": []}, {"text": "Given a partial script as input, an accurate event prediction model should rank the missing event highly in the guess list that it generates as output.", "labels": [], "entities": [{"text": "event prediction", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7273722290992737}]}, {"text": "We consider two approaches to evaluating the guess lists produced in response to narrative cloze tests.", "labels": [], "entities": []}, {"text": "Both are defined in terms of a test collection C, consisting of |C| partial scripts, where for each partial script c with missing event e, rank sys (c) is the rank of e in the system's guess list for c.", "labels": [], "entities": []}, {"text": "The average rank of the missing event across all of the partial scripts: This is the evaluation metric used by Chambers and Jurafsky (2008).", "labels": [], "entities": []}, {"text": "\u2022 Recall@N. The fraction of partial scripts where the missing event is ranked N or less in the guess list.", "labels": [], "entities": [{"text": "Recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9861866235733032}]}, {"text": "In our experiments we use N = 50, but results are roughly similar for lower and higher values of N . Recall@N has not been used before for evaluating models that predict script events, however we suggest that it is a more reliable metric than Average rank.", "labels": [], "entities": [{"text": "Recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.8970065712928772}, {"text": "Average rank", "start_pos": 243, "end_pos": 255, "type": "METRIC", "confidence": 0.9177004098892212}]}, {"text": "When calculating the average rank, the length of the guess lists will have a significant influence on results.", "labels": [], "entities": []}, {"text": "For instance, if a small model is trained with only a small vocabulary of events, its guess lists will usually be shorter than a larger model, but if both models predict the missing event at the bottom of the list, the larger model will get penalized more.", "labels": [], "entities": []}, {"text": "Recall@N does not have this issue -it is not influenced by length of the guess lists.", "labels": [], "entities": []}, {"text": "An alternative evaluation metric would have been mean average precision (MAP), a metric commonly used to evaluate information retrieval.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 49, "end_pos": 77, "type": "METRIC", "confidence": 0.9607723653316498}, {"text": "information retrieval", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.7939837276935577}]}, {"text": "Mean average precision reduces to mean reciprocal rank (MRR) when there's only a single answer as in the case of narrative cloze, and would have scored the ranked lists as: Note that mean reciprocal rank has the same issues with guess list length that average rank does.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.5174346566200256}, {"text": "mean reciprocal rank (MRR)", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.8842542469501495}]}, {"text": "Thus, since it does not aid us in comparing to prior work, and it has the same deficiencies as average rank, we do not report MRR in this article.", "labels": [], "entities": [{"text": "MRR", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.8045841455459595}]}], "tableCaptions": [{"text": " Table 1: Chain selection methods for the Reuters corpus  -comparison of average ranks and Recall@50.", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8783826231956482}, {"text": "Recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.6787135004997253}]}, {"text": " Table 2: Chain selection methods for the Fairy Tale  corpus -comparison of average ranks and Recall@50.", "labels": [], "entities": [{"text": "Fairy Tale  corpus", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.8401438395182291}, {"text": "Recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.992179274559021}]}, {"text": " Table 3: Event bigram selection methods for the  Reuters corpus -comparison of average ranks and Re- call@50.", "labels": [], "entities": [{"text": "Event bigram selection", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6234129865964254}, {"text": "Reuters corpus", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9072394967079163}, {"text": "Re- call@50", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.8551122307777405}]}, {"text": " Table 4: Event bigram selection methods for the Fairy  Tales corpus -comparison of average ranks and Re- call@50.", "labels": [], "entities": [{"text": "Event bigram selection", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6184493800004324}, {"text": "Fairy  Tales corpus", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.8105832934379578}, {"text": "Re- call", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9596624771753947}]}, {"text": " Table 5: Ranking methods for the Reuters corpus - comparison of average ranks and Recall@50.", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.880550891160965}, {"text": "Recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.6263751983642578}]}, {"text": " Table 6: Ranking methods for the Fairy Tale corpus - comparison of average ranks and Recall@50.", "labels": [], "entities": [{"text": "Fairy Tale corpus", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.7924854358037313}, {"text": "Recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9913205504417419}]}]}