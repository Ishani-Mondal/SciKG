{"title": [{"text": "Structural and Topical Dimensions in Multi-Task Patent Translation", "labels": [], "entities": [{"text": "Structural and Topical Dimensions", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7192874774336815}, {"text": "Multi-Task Patent Translation", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6381699641545614}]}], "abstractContent": [{"text": "Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents.", "labels": [], "entities": [{"text": "Patent translation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8433688282966614}]}, {"text": "In this paper we analyze patents along the orthogonal dimensions of topic and textual structure.", "labels": [], "entities": []}, {"text": "We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.7793052196502686}]}, {"text": "We study multi-task learning techniques that exploit com-monalities between tasks by mixtures of translation models or by multi-task meta-parameter tuning.", "labels": [], "entities": []}, {"text": "We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters.", "labels": [], "entities": []}, {"text": "A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Patents are an important tool for the protection of intellectual property and also play a significant role in business strategies in modern economies.", "labels": [], "entities": []}, {"text": "Patent translation is an enabling technique for patent prior art search which aims to detect a patent's novelty and thus needs to be cross-lingual fora multitude of languages.", "labels": [], "entities": [{"text": "Patent translation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7354721873998642}, {"text": "patent prior art search", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6191052794456482}]}, {"text": "Patent translation is complicated by a highly specialized vocabulary, consisting of technical terms specific to the field of invention the patent relates to.", "labels": [], "entities": [{"text": "Patent translation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8321642875671387}]}, {"text": "Patents are written in a sophisticated legal jargon (\"patentese\") that is not found in everyday language and exhibits a complex textual structure.", "labels": [], "entities": []}, {"text": "Also, patents are often intentionally ambiguous or vague in order to maximize the coverage of the claims.", "labels": [], "entities": []}, {"text": "In this paper, we analyze patents with respect to the orthogonal dimensions of topic -the technical field covered by the patent -and structure -a patent's text sections -, with respect to their influence on machine translation performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.809542715549469}]}, {"text": "The topical dimension of patents is characterized by the International Patent Classification (IPC) 1 which categorizes patents hierarchically into 8 sections, 120 classes, 600 subclasses, down to 70,000 subgroups at the leaf level.", "labels": [], "entities": [{"text": "International Patent Classification (IPC)", "start_pos": 57, "end_pos": 98, "type": "DATASET", "confidence": 0.8486428360144297}]}, {"text": "Orthogonal to the patent classification, patent documents can be sub-categorized along the dimension of textual structure.", "labels": [], "entities": []}, {"text": "Article 78.1 of the European Patent Convention (EPC) lists all sections required in a patent document 2 : \"A European patent application shall contain: (a) a request for the grant of a European patent; (b) a description of the invention; (c) one or more claims; (d) any drawings referred to in the description or the claims; (e) an abstract, and satisfy the requirements laid down in the Implementing Regulations.\"", "labels": [], "entities": [{"text": "European Patent Convention (EPC)", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.4975350896517436}]}, {"text": "The request for grant contains the patent title; thus a patent document comprises the textual elements of title, description, claim, and abstract.", "labels": [], "entities": []}, {"text": "We investigate whether it is worthwhile to treat different values along the structural and topical dimensions as different tasks that are not completely independent of each other but share some commonalities, yet differ enough to counter a simple pooling of data.", "labels": [], "entities": []}, {"text": "For example, we consider different tasks such as patents from different IPC classes, or along an orthogonal dimension, patent documents of all IPC classes but consisting only of titles or only of claims.", "labels": [], "entities": []}, {"text": "We ask whether such tasks should be addressed as separate translation tasks, or whether translation performance can be improved by learning several tasks simultaneously through shared models that are more sophisticated than simple data pooling.", "labels": [], "entities": []}, {"text": "Our goal is to learn a patent translation system that performs well across several different tasks, thus benefits from shared information, but is yet able to address the specifics of each task.", "labels": [], "entities": [{"text": "patent translation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.740182101726532}]}, {"text": "One contribution of this paper is a thorough analysis of the differences and similarities of multilingual patent data along the dimensions of textual structure and topic.", "labels": [], "entities": []}, {"text": "The second contribution is the experimental investigation of the influence of various such tasks on patent translation performance.", "labels": [], "entities": [{"text": "patent translation", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7937705814838409}]}, {"text": "Starting from baseline models that are trained on individual tasks or on data pooled from all tasks, we apply mixtures of translation models and multi-task minimum error rate training to multiple patent translation tasks.", "labels": [], "entities": []}, {"text": "A by-product of our research is a parallel patent corpus of over 23 million sentence pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we used the phrase-based, open-source SMT toolkit Moses 6 ( . For language modeling, we computed 5-gram models using IRSTLM) and queried the model with KenLM shows a first comparison of results of Moses models trained on 500,000 parallel sentences from patent text sections balanced over IPC classes, against Moses trained on 1.7 Million sentences of parliament proceedings from Europarl 8 ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9417111873626709}, {"text": "language modeling", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7142909169197083}, {"text": "KenLM", "start_pos": 172, "end_pos": 177, "type": "METRIC", "confidence": 0.5374210476875305}, {"text": "Europarl 8", "start_pos": 399, "end_pos": 409, "type": "DATASET", "confidence": 0.9593015611171722}]}, {"text": "The best result on each section is indicated in boldface.", "labels": [], "entities": []}, {"text": "The Europarl model performs very poorly on all three sections in compar-ison to the task-specific MAREC model, although the former has been learned on more than three times the amount of data.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9519034028053284}, {"text": "MAREC", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.6207137107849121}]}, {"text": "An analysis of the output of both system shows that the Europarl model suffers from two problems: Firstly, there is an obvious out of vocabulary (OOV) problem of the Europarl model compared to the MAREC model.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9151699542999268}]}, {"text": "Secondly, the Europarl model suffers from incorrect word sense disambiguation, as illustrated by the samples in source steuerbar leitet Europarl taxable is in charge of MAREC controllable guiding reference controllable guides: Output of Europarl model on MAREC data.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.607820600271225}, {"text": "Europarl taxable", "start_pos": 136, "end_pos": 152, "type": "DATASET", "confidence": 0.7809889018535614}, {"text": "MAREC data", "start_pos": 255, "end_pos": 265, "type": "DATASET", "confidence": 0.8780316114425659}]}, {"text": "shows the results of the evaluation across text sections; we measured the performance of separately trained and tuned individual models on every section.", "labels": [], "entities": []}, {"text": "The results allow some conclusions about the textual characteristics of the sections and indicate similarities.", "labels": [], "entities": []}, {"text": "Naturally, every task is best translated with a model trained on the respective section, as the BLEU scores on the diagonal are the highest in every column.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9990449547767639}]}, {"text": "Accordingly, we are interested in the runner-up on each section, which is indicated in bold font.", "labels": [], "entities": []}, {"text": "The results on abstracts suggest that this section bears the strongest resemblance to claims, since the model trained on claims achieves a respectable score.", "labels": [], "entities": []}, {"text": "The abstract model seems to be the most robust and varied model, yielding the runner-up score on all other sections.", "labels": [], "entities": []}, {"text": "Claims are easiest to translate, yielding the highest overall BLEU score of 0.4879.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9843403398990631}]}, {"text": "In contrast to that, all models score considerably lower on titles.", "labels": [], "entities": []}, {"text": "The cross-section evaluation on the IPC classes shows similar patterns.", "labels": [], "entities": [{"text": "IPC classes", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8844022750854492}]}, {"text": "Each section is best translated with a model trained on data from the same section.", "labels": [], "entities": []}, {"text": "Note that best section scores vary considerably, ranging from 0.5719 on C to 0.4714 on H, indicating that higher-scoring classes, such as C and A, are more homogeneous and therefore easier to translate.", "labels": [], "entities": []}, {"text": "C, the Chemistry section, presumably benefits from the fact that the data contain chemical formulae, which are language-independent and do not have to be translated.", "labels": [], "entities": []}, {"text": "Again, for determining the relationship between the classes, we examine the best runner-up on each section, considering the BLEU score, although asymmetrical, as a kind of measure of similarity between classes.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.982548862695694}]}, {"text": "We can establish symmetric relationships between sections A and C, B and F as well as G and H, which means that the models are mutual runner-up on the other's test section.", "labels": [], "entities": []}, {"text": "The similarities of translation tasks established in the previous section can be confirmed by information-theoretic similarity measures that perform a pairwise comparison of the vocabulary probability distribution of each task-specific corpus.", "labels": [], "entities": []}, {"text": "This distribution is calculated on the basis of the 500 most frequent words in the union of two corpora, normalized by vocabulary size.", "labels": [], "entities": []}, {"text": "As metric we use the A-distance measure of.", "labels": [], "entities": [{"text": "A-distance measure", "start_pos": 21, "end_pos": 39, "type": "METRIC", "confidence": 0.983497679233551}]}, {"text": "If A is the set of events on which the word distributions of two corpora are defined, then the A-distance is the supremum of the difference of probabilities assigned to the same event.", "labels": [], "entities": []}, {"text": "Low distance means higher similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9970964193344116}]}, {"text": "shows the A-distance of corpora specific to IPC classes.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9931788444519043}]}, {"text": "The most similar section or sections -apart from the section itself on the diagonal -is indicated in boldface.", "labels": [], "entities": []}, {"text": "The pairwise similarity of A and C, B and F, G and H obtained by BLEU score is confirmed.", "labels": [], "entities": [{"text": "similarity", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.8269832730293274}, {"text": "BLEU score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9689313769340515}]}, {"text": "Furthermore, a close similarity between E and F is indicated.", "labels": [], "entities": []}, {"text": "G and H (electricity and physics, respectively) are very similar to each other but not close to any other section apart from B.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of parallel sentences in output with  input/output ratio of sentence aligner.", "labels": [], "entities": []}, {"text": " Table 3: Average number of tokens and average type  frequencies in text sections.", "labels": [], "entities": []}, {"text": " Table 5: BLEU scores and OOV rate for Europarl base- line and MAREC model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993531107902527}, {"text": "OOV rate", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9857906997203827}, {"text": "Europarl base- line", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.8774340599775314}, {"text": "MAREC", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9726219773292542}]}, {"text": " Table 7: BLEU scores for 500k individual text section  models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978042244911194}]}, {"text": " Table 8: BLEU scores for 300k individual IPC section models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985111355781555}]}, {"text": " Table 9: Pairwise A-distance for 300k IPC training sets.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.937691330909729}]}, {"text": " Table 10: Mixture and pooling on text sections.", "labels": [], "entities": [{"text": "Mixture", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9835011959075928}]}, {"text": " Table 11: Mixture and pooling on IPC sections.", "labels": [], "entities": [{"text": "Mixture", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9906401634216309}, {"text": "pooling", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9218306541442871}, {"text": "IPC sections", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.7541790306568146}]}, {"text": " Table 12: Multi-task tuning on text sections.", "labels": [], "entities": []}, {"text": " Table 13: Multi-task tuning on IPC sections.", "labels": [], "entities": []}, {"text": " Table 15: Multi-task tuning on 16,000 sentences  pooled from IPC sections. \"<\" denotes a statistically  significant difference to the best result.", "labels": [], "entities": [{"text": "Multi-task tuning", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7802569568157196}]}]}