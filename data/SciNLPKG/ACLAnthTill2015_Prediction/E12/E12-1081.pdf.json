{"title": [{"text": "Learning the Fine-Grained Information Status of Discourse Entities", "labels": [], "entities": []}], "abstractContent": [{"text": "While information status (IS) plays a crucial role in discourse processing, there have only been a handful of attempts to automatically determine the IS of discourse entities.", "labels": [], "entities": [{"text": "information status (IS)", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.6598891615867615}, {"text": "discourse processing", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7185704112052917}, {"text": "determine the IS of discourse entities", "start_pos": 136, "end_pos": 174, "type": "TASK", "confidence": 0.6985369175672531}]}, {"text": "We examine a related but more challenging task, fine-grained IS determination, which involves classifying a discourse entity as one of 16 IS subtypes.", "labels": [], "entities": [{"text": "IS determination", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.8927369117736816}]}, {"text": "We investigate the use of rich knowledge sources for this task in combination with a rule-based approach and a learning-based approach.", "labels": [], "entities": []}, {"text": "In experiments with a set of Switchboard dialogues, the learning-based approach achieves an accuracy of 78.7%, outperforming the rule-based approach by 21.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9994376301765442}]}], "introductionContent": [{"text": "A linguistic notion central to discourse processing is information status (IS).", "labels": [], "entities": [{"text": "discourse processing", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7167419791221619}, {"text": "information status (IS)", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.6620251119136811}]}, {"text": "It describes the extent to which a discourse entity, which is typically referred to by noun phrases (NPs) in a dialogue, is available to the hearer.", "labels": [], "entities": []}, {"text": "Different definitions of IS have been proposed over the years.", "labels": [], "entities": []}, {"text": "In this paper, we adopt proposal, since it is primarily built upon and well-known definitions, and is empirically shown by Nissim et al. to yield an annotation scheme for IS in dialogue that has good reproducibility.", "labels": [], "entities": [{"text": "IS in dialogue", "start_pos": 171, "end_pos": 185, "type": "TASK", "confidence": 0.8600448369979858}]}, {"text": "Specifically, adopt a threeway classification scheme for IS, defining a discourse entity as (1) old to the hearer if it is known to the hearer and has previously been referred to in the dialogue; (2) new if it is unknown to her and has not been previously referred to; and (3) mediated (henceforth med) if it is newly mentioned in the dialogue but she can infer its identity from a previously-mentioned entity.", "labels": [], "entities": [{"text": "IS", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9575544595718384}]}, {"text": "To capture finergrained distinctions for IS, Nissim et al. allow an old or med entity to have a subtype, which subcategorizes an old or med entity.", "labels": [], "entities": [{"text": "IS", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9622038006782532}]}, {"text": "For instance, a med entity has the subtype set if the NP that refers to it is in a set-subset relation with its antecedent.", "labels": [], "entities": []}, {"text": "IS plays a crucial role in discourse processing: it provides an indication of how a discourse model should be updated as a dialogue is processed incrementally.", "labels": [], "entities": [{"text": "discourse processing", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7134543806314468}]}, {"text": "Its importance can be reflected in part in the amount of attention it has received in theoretical linguistics over the years (e.g.,,,,,), and in part in the benefits it can potentially bring to NLP applications.", "labels": [], "entities": []}, {"text": "One task that could benefit from knowledge of IS is identity coreference: since new entities by definition have not been previously referred to, an NP marked as new does not need to be resolved, thereby improving the precision of a coreference resolver.", "labels": [], "entities": [{"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9985874891281128}, {"text": "coreference resolver", "start_pos": 232, "end_pos": 252, "type": "TASK", "confidence": 0.8008995652198792}]}, {"text": "Knowledge of fine-grained or subcategorized IS is valuable for other NLP tasks.", "labels": [], "entities": []}, {"text": "For instance, an NP marked asset signifies that it is in a set-subset relation with its antecedent, thereby providing important clues for bridging anaphora resolution (e.g.,).", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7377519011497498}]}, {"text": "Despite the potential usefulness of IS in NLP tasks, there has been little work on learning the IS of discourse entities.", "labels": [], "entities": []}, {"text": "To investigate the plausibility of learning IS, annotate a set of Switchboard dialogues with such information 2 , and subsequently present a rule-based approach and a learning-based approach to acquiring such knowledge).", "labels": [], "entities": []}, {"text": "More recently, we have improved Nissim's learning-based approach by augmenting her feature set, which comprises seven string-matching and grammatical features, with lexical and syntactic features.", "labels": [], "entities": []}, {"text": "Despite the improvements, the performance on new entities remains poor: an Fscore of 46.5% was achieved.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9966086149215698}]}, {"text": "Our goal in this paper is to investigate finegrained IS determination, the task of classifying a discourse entity as one of the 16 IS subtypes defined by.", "labels": [], "entities": [{"text": "finegrained IS determination", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.5878381431102753}]}, {"text": "Owing in part to the increase in the number of categories, finegrained IS determination is arguably a more challenging task than the 3-class IS determination task that Nissim and R&N investigated.", "labels": [], "entities": [{"text": "finegrained IS determination", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6784037748972574}, {"text": "IS determination task", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.8330467740694681}]}, {"text": "To our knowledge, this is the first empirical investigation of automated fine-grained IS determination.", "labels": [], "entities": [{"text": "IS determination", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.8896675705909729}]}, {"text": "We propose a knowledge-rich approach to finegrained IS determination.", "labels": [], "entities": [{"text": "IS determination", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.8489706516265869}]}, {"text": "Our proposal is motivated in part by Nissim's and R&N's poor performance on new entities, which we hypothesize can be attributed to their sole reliance on shallow knowledge sources.", "labels": [], "entities": []}, {"text": "In light of this hypothesis, our approach employs semantic and world knowledge extracted from manually and automatically constructed knowledge bases, as well as coreference information.", "labels": [], "entities": []}, {"text": "The relevance of coreference to IS determination can be seen from the definition of IS: anew entity is not coreferential with any previously-mentioned entity, whereas an old entity may.", "labels": [], "entities": [{"text": "IS determination", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.96806401014328}]}, {"text": "While our use of coreference information for IS determination and our earlier claim that IS annotation would be useful for coreference resolution may seem to have created a chicken-andegg problem, they do not: since coreference resolution and IS determination can benefit from each other, it maybe possible to formulate an approach where the two tasks can mutually bootstrap.", "labels": [], "entities": [{"text": "IS determination", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.9686387479305267}, {"text": "coreference resolution", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.9448263049125671}, {"text": "coreference resolution", "start_pos": 216, "end_pos": 238, "type": "TASK", "confidence": 0.910438060760498}, {"text": "IS determination", "start_pos": 243, "end_pos": 259, "type": "TASK", "confidence": 0.782959371805191}]}, {"text": "We investigate rule-based and learning-based approaches to fine-grained IS determination.", "labels": [], "entities": [{"text": "IS determination", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.9279561042785645}]}, {"text": "In the rule-based approach, we manually compose rules to combine the aforementioned knowledge sources.", "labels": [], "entities": []}, {"text": "While we could employ the same knowledge sources in the learning-based approach, we chose to encode, among other knowledge sources, the hand-written rules and their predictions directly as features for the learner.", "labels": [], "entities": []}, {"text": "In an evaluation on 147 Switchboard dialogues, our learningbased approach to fine-grained IS determination achieves an accuracy of 78.7%, substantially outperforming the rule-based approach by 21.3%.", "labels": [], "entities": [{"text": "IS determination", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.9450507760047913}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9993894100189209}]}, {"text": "Equally importantly, when employing these linguistically rich features to learn Nissim's 3-class IS determination task, the resulting classifier achieves an accuracy of 91.7%, surpassing the classifier trained on R&N's state-of-the-art feature set by 8.8% in absolute accuracy.", "labels": [], "entities": [{"text": "Nissim's 3-class IS determination task", "start_pos": 80, "end_pos": 118, "type": "TASK", "confidence": 0.6291092236836752}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.999300479888916}, {"text": "R&N's state-of-the-art feature set", "start_pos": 213, "end_pos": 247, "type": "DATASET", "confidence": 0.6957980607237134}, {"text": "accuracy", "start_pos": 268, "end_pos": 276, "type": "METRIC", "confidence": 0.9319743514060974}]}, {"text": "Improvements on the new class are particularly substantial: its F-score rises from 46.7% to 87.2%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9993143081665039}]}], "datasetContent": [{"text": "We employ dataset, which comprises 147 Switchboard dialogues.", "labels": [], "entities": []}, {"text": "We parti-tion them into a training set (117 dialogues) and a test set (30 dialogues).", "labels": [], "entities": []}, {"text": "A total of 58,835 NPs are annotated with IS types and subtypes.", "labels": [], "entities": []}, {"text": "The distributions of NPs over the IS subtypes in the training set and the test set are shown in  Next, we evaluate the rule-based approach and the learning-based approach to determining the IS subtype of each hand-annotated NP in the test set.", "labels": [], "entities": []}, {"text": "shows the results of the two approaches.", "labels": [], "entities": []}, {"text": "Specifically, row 1 shows their accuracy, which is defined as the percentage of correctly classified instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9992771744728088}]}, {"text": "For each approach, we present results that are generated based on gold coreference chains as well as automatic chains computed by the Stanford resolver.", "labels": [], "entities": []}, {"text": "As we can see, the rule-based approach achieves accuracies of 66.0% (gold coreference) and 57.4% (Stanford coreference), whereas the learning-based approach achieves accuracies of 86.4% (gold) and 78.7% (Stanford).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9837117791175842}]}, {"text": "In other words, the gold coreference results are better than the Stanford coreference results, and the learningbased results are better than the rule-based results.", "labels": [], "entities": []}, {"text": "While perhaps neither of these results are surprising, we are pleasantly surprised by the extent to which the learned classifier outperforms the handcrafted rules: accuracies increase by 20.4% and 21.3% when gold coreference and Stanford coreference are used, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 164, "end_pos": 174, "type": "METRIC", "confidence": 0.9981852173805237}]}, {"text": "In other words, machine learning has \"transformed\" a ruleset that achieves mediocre performance into a system that achieves relatively high performance.", "labels": [], "entities": []}, {"text": "These results also suggest that coreference plays a crucial role in IS subtype determination: accuracies could increase by up to 7.7-8.6% if we solely improved coreference resolution performance.", "labels": [], "entities": [{"text": "IS subtype determination", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.9365948041280111}, {"text": "accuracies", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9972148537635803}, {"text": "coreference resolution", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.7898288369178772}]}, {"text": "This is perhaps not surprising: IS and coreference can mutually benefit from each other.", "labels": [], "entities": [{"text": "coreference", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8776970505714417}]}, {"text": "To gain additional insight into the task, we also show in rows 2-17 of the performance on each of the 16 subtypes, expressed in terms of recall (R), precision (P), and F-score (F).", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9649853408336639}, {"text": "precision (P)", "start_pos": 149, "end_pos": 162, "type": "METRIC", "confidence": 0.9546888917684555}, {"text": "F-score (F)", "start_pos": 168, "end_pos": 179, "type": "METRIC", "confidence": 0.961417019367218}]}, {"text": "A few points deserve mention.", "labels": [], "entities": []}, {"text": "First, in comparison to the rule-based approach, the learning-based approach achieves considerably better performance on almost all classes.", "labels": [], "entities": []}, {"text": "One that is of particular interest is the new class.", "labels": [], "entities": []}, {"text": "As we can see in row 17, its F-score rises by about 30 points.", "labels": [], "entities": [{"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9986091256141663}]}, {"text": "These gains are accompanied by a simultaneous rise in recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9995198249816895}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9986211061477661}]}, {"text": "In particular, recall increases by about 40 points.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9997850060462952}]}, {"text": "Now, recall from the introduc-  tion that previous attempts on 3-class IS determination by Nissim and R&N have achieved poor performance on the new class.", "labels": [], "entities": [{"text": "3-class IS determination", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.6190550923347473}]}, {"text": "We hypothesize that the use of shallow features in their approaches were responsible for the poor performance they observed, and that using our knowledge-rich feature set could improve its performance.", "labels": [], "entities": []}, {"text": "We will test this hypothesis at the end of this section.", "labels": [], "entities": []}, {"text": "Other subtypes that are worth discussing are med/aggregation, med/func value, and med/poss.", "labels": [], "entities": []}, {"text": "Recall that the rules we designed for these classes were only crude approximations, or, perhaps more precisely, simplified versions of the definitions of the corresponding subtypes.", "labels": [], "entities": []}, {"text": "For instance, to determine whether an NP belongs to med/aggregation, we simply look for occurrences of \"and\" and \"or\" (Rule 9), whereas its definition requires that not all of the NPs in the coordinated phrase are new.", "labels": [], "entities": []}, {"text": "Despite the over-simplicity of these rules, machine learning has enabled the available features to be combined in such away that high performance is achieved for these classes (see rows 14-16).", "labels": [], "entities": []}, {"text": "Also worth examining are those classes for which the hand-crafted rules rely on sophisticated knowledge sources.", "labels": [], "entities": []}, {"text": "They include med/part, which relies on ReVerb; med/situation, which relies on FrameNet; and med/event, which relies on WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9526175260543823}]}, {"text": "As we can see from the rule-based results (rows 10-12), these knowledge sources have yielded rules that achieved perfect precision but low recall: 19.5% for part, 28.7% for situation, and 10.5 for event.", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9963181018829346}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9990695118904114}]}, {"text": "Nevertheless, the learning algorithm has again discovered a profitable way to combine the available features, enabling the Fscores of these classes to increase by 35.1-50.6%.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9877653121948242}]}, {"text": "While most classes are improved by machine learning, the same is not true for old/event and med/bound, whose F-scores are 4.5% (row 3) and 5.1% (row 9), respectively, when Stanford coreference is employed.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9883242845535278}]}, {"text": "This is perhaps not surprising.", "labels": [], "entities": []}, {"text": "Recall that the multi-class SVM classifier was trained to maximize classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9215948581695557}]}, {"text": "Hence, if it encounters a class that is both difficult to learn and is under-represented, it may as well aim to achieve good performance on the easierto-learn, well-represented classes at the expense of these hard-to-learn, under-represented classes.", "labels": [], "entities": []}, {"text": "In an attempt to gain additional insight into the performance contribution of each of the five types of features used in the learning-based approach, we conduct feature ablation experiments.", "labels": [], "entities": []}, {"text": "Results are shown in, where each row shows the accuracy of the classifier trained on all types of features except for the one shown in that row.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9992687106132507}]}, {"text": "For easy reference, the accuracy of the classifier trained on all types of features is shown in row 1 of the table.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996104836463928}]}, {"text": "According to the paired t-test (p < 0.05), performance drops significantly whichever feature type is removed.", "labels": [], "entities": []}, {"text": "This suggests that all five feature types are contributing positively to overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9971267580986023}]}, {"text": "Also, the markables features are the least important in the presence of other feature groups, whereas mark-   able predictions and unigrams are the two most important feature groups.", "labels": [], "entities": []}, {"text": "To get a better idea of the utility of each feature type, we conduct another experiment in which we train five classifiers, each of which employs exactly one type of features.", "labels": [], "entities": []}, {"text": "The accuracies of these classifiers are shown in.", "labels": [], "entities": []}, {"text": "As we can see, the markables features have the smallest contribution, whereas unigrams have the largest contribution.", "labels": [], "entities": []}, {"text": "Somewhat interesting are the results of the classifiers trained on the rule conditions: the rules are far more effective when gold coreference is used.", "labels": [], "entities": []}, {"text": "This can be attributed to the fact that the design of the rules was based in part on the definitions of the subtypes, which assume the availability of perfect coreference information.", "labels": [], "entities": []}, {"text": "To gain some insight into the extent to which a knowledge source or a rule contributes to the overall performance of the rule-based approach, we conduct ablation experiments: in each experiment, we measure the performance of the ruleset after removing a particular rule or knowledge source from it.", "labels": [], "entities": []}, {"text": "Specifically, rows 2-4 of show the accuracies of the ruleset after removing the memorization rule (Rule 17), the rule that uses ReVerb's output (Rule 12), and the cue words used in Rules 4 and 10, respectively.", "labels": [], "entities": []}, {"text": "For easy reference, the accuracy of the original ruleset is shown in row 1 of the table.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996311664581299}]}, {"text": "According to the paired t-test (p < 0.05), performance drops significantly in all three ablation experiments.", "labels": [], "entities": []}, {"text": "This suggests that the memorization rule, ReVerb, and the cue words all contribute positively to the accuracy of the ruleset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9991426467895508}]}, {"text": "could be attributed to their sole reliance on lexico-syntactic features.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we (1) train a 3-class classifier using the five types of features we employed in our learning-based approach, computing the features based on the Stanford coreference chains; and (2) compare its results against those obtained via the lexico-syntactic approach in R&N on our test set.", "labels": [], "entities": []}, {"text": "Results of these experiments, which are shown in, substantiate our hypothesis: when we replace R&N's features with ours, accuracy rises from 82.9% to 91.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9994478821754456}]}, {"text": "These gains can be attributed to large improvements in identifying new and med entities, for which F-scores increase by about 40 points and 10 points, respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.991432785987854}]}], "tableCaptions": [{"text": " Table 3: IS subtype accuracies and F-scores. In each row, the strongest result, as well as those that are statistically  indistinguishable from it according to the paired t-test (p < 0.05), are boldfaced.", "labels": [], "entities": [{"text": "IS subtype accuracies", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.5544968148072561}, {"text": "F-scores", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9914344549179077}]}, {"text": " Table 4: Accuracies of feature ablation experiments.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9807393550872803}]}, {"text": " Table 5: Accuracies of classifiers for each feature type.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9594129323959351}]}, {"text": " Table 6: Accuracies of the simplified ruleset.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9891666769981384}]}, {"text": " Table 7: Accuracies on IS types.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9886960983276367}]}]}