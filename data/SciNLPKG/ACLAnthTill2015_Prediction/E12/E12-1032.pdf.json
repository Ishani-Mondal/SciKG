{"title": [{"text": "Extending the Entity-based Coherence Model with Multiple Ranks", "labels": [], "entities": []}], "abstractContent": [{"text": "We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data.", "labels": [], "entities": []}, {"text": "We associate multiple ranks with the set of permutations originating from the same source document , as opposed to the original pairwise rankings.", "labels": [], "entities": []}, {"text": "We also study the effect of the permutations used in training, and the effect of the coreference component used in entity extraction.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.848139762878418}]}, {"text": "With no additional manual annotations required, our extended model is able to outperform the original model on two tasks: sentence ordering and summary coherence rating.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7587721049785614}]}], "introductionContent": [{"text": "Coherence is important in a well-written document; it helps make the text semantically meaningful and interpretable.", "labels": [], "entities": []}, {"text": "Automatic evaluation of coherence is an essential component of various natural language applications.", "labels": [], "entities": []}, {"text": "Therefore, the study of coherence models has recently become an active research area.", "labels": [], "entities": []}, {"text": "A particularly popular coherence model is the entity-based local coherence model of Barzilay and Lapata (B&L).", "labels": [], "entities": [{"text": "Barzilay and Lapata (B&L)", "start_pos": 84, "end_pos": 109, "type": "DATASET", "confidence": 0.7791094332933426}]}, {"text": "This model represents local coherence by transitions, from one sentence to the next, in the grammatical role of references to entities.", "labels": [], "entities": []}, {"text": "It learns a pairwise ranking preference between alternative renderings of a document based on the probability distribution of those transitions.", "labels": [], "entities": []}, {"text": "In particular, B&L associated a lower rank with automatically created permutations of a source document, and learned a model to discriminate an original text from its permutations (see Section 3.1 below).", "labels": [], "entities": [{"text": "B&L", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.7590561310450236}]}, {"text": "However, coherence is matter of degree rather than a binary distinction, so a model based only on such pairwise rankings is insufficiently fine-grained and cannot capture the subtle differences in coherence between the permuted documents.", "labels": [], "entities": []}, {"text": "Since the first appearance of B&L's model, several extensions have been proposed (see Section 2.3 below), primarily focusing on modifying or enriching the original feature set by incorporating other document information.", "labels": [], "entities": [{"text": "B&L's model", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8856356263160705}]}, {"text": "By contrast, we wish to refine the learning procedure in away such that the resulting model will be able to evaluate coherence on a more fine-grained level.", "labels": [], "entities": []}, {"text": "Specifically, we propose a concise extension to the standard entity-based coherence model by learning not only from the original document and its corresponding permutations but also from ranking preferences among the permutations themselves.", "labels": [], "entities": []}, {"text": "We show that this can be done by assigning a suitable objective score for each permutation indicating its dissimilarity from the original one.", "labels": [], "entities": []}, {"text": "We call this a multiple-rank model since we train our model on a multiple-rank basis, rather than taking the original pairwise ranking approach.", "labels": [], "entities": []}, {"text": "This extension can also be easily combined with other extensions by incorporating their enriched feature sets.", "labels": [], "entities": []}, {"text": "We show that our multiple-rank model outperforms B&L's basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in.", "labels": [], "entities": [{"text": "B&L", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.7734206120173136}, {"text": "sentence ordering", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7460959553718567}]}, {"text": "In sentence ordering, we experiment with different approaches to assigning dissimilarity scores and ranks (Section 5.1.1).", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7811591327190399}]}, {"text": "We also experiment with different entity extraction approaches Manila Miles Island Quake Baco: A fragment of an entity grid for five entities across three sentences.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7551863789558411}, {"text": "Miles Island Quake Baco", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.8214711844921112}]}, {"text": "(Section 5.1.2) and different distributions of permutations used in training (Section 5.1.3).", "labels": [], "entities": []}, {"text": "We show that these two aspects are crucial, depending on the characteristics of the dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two evaluation tasks for's entity-based model are sentence ordering and summary coherence rating.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7736706435680389}]}, {"text": "In sentence ordering, a set of random permutations is created for each source document, and the learning procedure is conducted on this synthetic mixture of coherent and incoherent documents.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7481392025947571}]}, {"text": "experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents).", "labels": [], "entities": []}, {"text": "A training data instance is constructed as a pair consisting of a source document and one of its random permutations, and the permuted document is always considered to be less coherent than the source document.", "labels": [], "entities": []}, {"text": "The entity transition features are then used to train a support vector machine ranker) to rank the source documents higher than the permutations.", "labels": [], "entities": []}, {"text": "The model is tested on a different set of source documents and their permutations, and the performance is evaluated as the fraction of correct pairwise rankings in the test set.", "labels": [], "entities": []}, {"text": "In summary coherence rating, a similar experimental framework is adopted.", "labels": [], "entities": []}, {"text": "However, in this task, rather than training and evaluating on a set of synthetic data, system-generated summaries and human-composed reference summaries from the Document Understanding Conference were used.", "labels": [], "entities": [{"text": "Document Understanding Conference", "start_pos": 162, "end_pos": 195, "type": "TASK", "confidence": 0.6295740107695261}]}, {"text": "Human annotators were asked to give a coherence score on a seven-point scale for each item.", "labels": [], "entities": []}, {"text": "The pairwise ranking preferences between summaries generated from the same input document cluster (excluding the pairs consisting of two human-written summaries) are used by a support vector machine ranker to learn a discriminant function to rank each pair according to their coherence scores.", "labels": [], "entities": []}, {"text": "Following, we wish to train a discriminative model to give the correct ranking preference between two documents in terms of their degree of coherence.", "labels": [], "entities": []}, {"text": "We experiment on the same two tasks as in their work: sentence ordering and summary coherence rating.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7696965336799622}]}], "tableCaptions": [{"text": " Table 2: Accuracies (%) of extending the stan- dard entity-based coherence model with multiple-rank  learning in sentence ordering using Coreference+ op- tion. Accuracies which are significantly better than the  baseline (p < .05) are indicated by *.", "labels": [], "entities": []}, {"text": " Table 3: Accuracies (%) of extending the stan- dard entity-based coherence model with multiple-rank  learning in sentence ordering using Coreference\u00b1 op- tion. Accuracies which are significantly better than the  baseline (p < .05) are indicated by *.", "labels": [], "entities": []}, {"text": " Table 4: Accuracies (%) of extending the stan- dard entity-based coherence model with multiple-rank  learning in sentence ordering using Coreference\u2212 op- tion. Accuracies which are significantly better than the  baseline are indicated by * (p < .05) and ** (p < .01).", "labels": [], "entities": []}, {"text": " Table 4.  Even with such a coarse approximation of  coreference resolution, our model is able to  achieve around 85% accuracy in most test cases,  except for dataset Earthquakes, training on PS BL  gives poorer performance than the standard model  by a small margin. But such inferior perfor- mance should be expected, because as explained  above, coreference resolution is crucial to this  dataset, since entities tend to be realized through  pronouns; simple string matching introduces too  much noise into training, especially when our  model wants to train a more fine-grained discrim- inative system than B&L's. However, we can see  from the result of training on PS M , if the per- mutations used in training do not involve swap- ping sentences which are too far away, the result- ing noise is reduced, and our model outperforms  theirs. And for dataset Accidents, our model  consistently outperforms the baseline model by a  large margin (with significance test at p < .01).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.8279077112674713}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9988250136375427}, {"text": "coreference resolution", "start_pos": 349, "end_pos": 371, "type": "TASK", "confidence": 0.81266188621521}]}, {"text": " Table 5: Accuracies (%) of extending the stan- dard entity-based coherence model with multiple-rank  learning in summary rating. Baselines are results of  standard entity-based coherence model. Accuracies  which are significantly better than the corresponding  baseline are indicated by * (p < .05) and ** (p < .01).", "labels": [], "entities": []}]}