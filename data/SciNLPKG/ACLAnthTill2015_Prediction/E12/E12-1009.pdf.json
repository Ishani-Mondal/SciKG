{"title": [{"text": "The Best of Both Worlds -A Graph-based Completion Model for Transition-based Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available.", "labels": [], "entities": [{"text": "Transition-based dependency parsers", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6577726105848948}]}, {"text": "In this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of abeam, combining the advantages of transition-based and graph-based approaches.", "labels": [], "entities": []}, {"text": "We also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase inaccuracy.", "labels": [], "entities": []}, {"text": "We apply the new transition-based parser on ty-pologically different languages such as En-glish, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based vs. the graph-based approach).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7730205655097961}]}, {"text": "The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank).", "labels": [], "entities": []}, {"text": "The transition-based approach is based on the conceptually (and cognitively) compelling idea More references will be provided in sec. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental left-to-right parsing process.", "labels": [], "entities": []}, {"text": "As anew word is processed, the parser has to decide on one out of a small number of possible transitions (adding a dependency arc pointing to the left or right and/or pushing or popping a word on/from a stack representation).", "labels": [], "entities": []}, {"text": "Obviously, the learning can be based on the feature information available at a particular snapshot in incremental processing, i.e., only surface information for the unparsed material to the right, but full structural information for the parts of the string already processed.", "labels": [], "entities": []}, {"text": "For the completely processed parts, there are no principled limitations as regards the types of structural configurations that can be checked in feature functions.", "labels": [], "entities": []}, {"text": "The graph-based approach in contrast emphasizes the objective of exhaustive search overall possible trees spanning the input words.", "labels": [], "entities": []}, {"text": "Commonly, dynamic programming techniques are used to decide on the optimal tree for each particular word span, considering all candidate splits into subspans, successively building longer spans in a bottom-up fashion (similar to chart-based constituent parsing).", "labels": [], "entities": [{"text": "chart-based constituent parsing", "start_pos": 229, "end_pos": 260, "type": "TASK", "confidence": 0.7288334369659424}]}, {"text": "Machine learning drives the process of deciding among alternative candidate splits, i.e., feature information can draw on full structural information for the entire material in the span under consideration.", "labels": [], "entities": []}, {"text": "However, due to the dynamic programming approach, the features cannot use arbitrarily complex structural configurations: otherwise the dynamic programming chart would have to be split into exponentially many special states.", "labels": [], "entities": []}, {"text": "The typical feature models are based on combinations of edges (so-called second-order factors) that closely follow the bottom-up combination of subspans in the parsing algorithm, i.e., the feature functions depend on the presence of two specific dependency edges.", "labels": [], "entities": []}, {"text": "Configurations not directly supported by the bottom-up building of larger spans are more cumbersome to integrate into the model (since the combination algorithm has to be adjusted), in particular for third-order factors or higher.", "labels": [], "entities": []}, {"text": "Empirically, i.e., when applied in supervised machine learning experiments based on existing treebanks for various languages, both strategies (and further refinements of them not mentioned here) turnout roughly equal in their capability of picking upmost of the relevant patterns well; some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results: in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser.", "labels": [], "entities": []}, {"text": "Another successful technique to combine parsers is voting as carried out by.", "labels": [], "entities": []}, {"text": "The present paper addresses the question if and how a more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results.", "labels": [], "entities": []}, {"text": "The main issue and solution strategy.", "labels": [], "entities": []}, {"text": "In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has to be transition-based at the top level.", "labels": [], "entities": []}, {"text": "The advantages of the graph-based approach -a more globally informed basis for the decision among different attachment options -have to be included as part of the scoring procedure.", "labels": [], "entities": []}, {"text": "As a prerequisite, our algorithm will require a memory for storing alternative analyses among which to choose.", "labels": [], "entities": []}, {"text": "This has been previously introduced in transitionbased approaches in the form of a beam): rather than representing only the best-scoring history of transitions, the k best-scoring alternative histories are kept around.", "labels": [], "entities": []}, {"text": "As we will indicate in the following, the mere addition of beam search does not help overcome a representational key issue of transition-based parsing: in many situations, a transition-based parser is forced to make an attachment decision fora given input word at a point where no or only partial information about the word's own dependents (and further decendents) is available.", "labels": [], "entities": []}, {"text": "illustrates such a case.: The left set of brackets indicates material that has been processed or is under consideration; on the right is the input, still to be processed.", "labels": [], "entities": []}, {"text": "Access to information that is yet unavailable would help the parser to decide on the correct transition.", "labels": [], "entities": []}, {"text": "Here, the parser has to decide whether to create an edge between house and with or between bought and with (which is technically achieved by first popping house from the stack and then adding the edge).", "labels": [], "entities": []}, {"text": "At this time, no information about the object of with is available; with fails to provide what we calla complete factor for the calculation of the scores of the alternative transitions under consideration.", "labels": [], "entities": []}, {"text": "In other words, the model cannot make use of any evidence to distinguish between the two examples in, and it is bound to get one of the two cases wrong.", "labels": [], "entities": []}, {"text": "illustrates the same case from the perspective of a graph-based parser.: A second order model as used in graph-based parsers has access to the crucial information to build the correct tree.", "labels": [], "entities": []}, {"text": "In this case, the parser condsiders the word friend (as opposed to garden, for instance) as it introduces the bold-face edge.", "labels": [], "entities": []}, {"text": "Here, the combination of subspans is performed at a point when their internal structure has been finalized, i.e., the attachment of with (to bought or house) is not decided until it is clear that friend is the object of with; hence, the semantically important lexicalization of with's object informs the higher-level attachment decision through a socalled second order factor in the feature model.", "labels": [], "entities": []}, {"text": "Given a suitable amount of training data, the model can thus learn to make the correct decision.", "labels": [], "entities": []}, {"text": "The dynamic-programming based graphbased parser is designed in such away that any score calculation is based on complete factors for the subspans that are combined at this point.", "labels": [], "entities": []}, {"text": "Note that the problem for the transition-based parser cannot be remedied by beam search alone.", "labels": [], "entities": []}, {"text": "If we were to keep the two options for attaching with around in abeam (say, with a slightly higher score for attachment to house, but with bought following narrowly behind), there would be no point in the further processing of the sentence at which the choice could be corrected: the transition-based parser still needs to make the decision that friend is attached to with, but this will not lead the parser to reconsider the decision made earlier on.", "labels": [], "entities": []}, {"text": "The strategy we describe in this paper applies in this very type of situation: whenever information is added in the transition-based parsing process, the scores of all the histories stored in the beam are recalculated based on a scoring model inspired by the graph-based parsing approach, i.e., taking complete factors into account as they become incrementally available.", "labels": [], "entities": []}, {"text": "As a consequence the beam is reordered, and hence, the incorrect preference of an attachment of with to house (based on incomplete factors) can later be corrected as friend is processed and the complete second-order factor becomes available.", "labels": [], "entities": []}, {"text": "The integrated transition-based parsing strategy has a number of advantages: (1) We can integrate and investigate a number of third order factors, without the need to implement a more complex parsing model each time anew to explore the properties of such distinct model.", "labels": [], "entities": []}, {"text": "(2) The parser with completion model maintains the favorable complexity of transition-based parsers.", "labels": [], "entities": []}, {"text": "(3) The completion model compensates for the lower accuracy of cases when only incomplete information is available.", "labels": [], "entities": [{"text": "completion", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.5450267791748047}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9983832836151123}]}, {"text": "(4) The parser combines the two leading parsing paradigms in a single efficient parser without stacking the two approaches.", "labels": [], "entities": []}, {"text": "Therefore the parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder.", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss related work.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models.", "labels": [], "entities": []}, {"text": "In Section 5, we describe experiments and provide evaluation results on selected data sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of different parsing systems are often hard to compare due to differences in phrase structure to dependency conversions, corpus version, and experimental settings.", "labels": [], "entities": []}, {"text": "For better comparison, we provide results on English for two commonly used data sets, based on two different conversions of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.996199905872345}]}, {"text": "The first uses the Penn2Malt conversion based on the head- finding rules of. gives an overview of the properties of the corpus.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.9499560594558716}, {"text": "head- finding", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.7906486789385477}]}, {"text": "The annotation of the corpus does not contain non-projective links.", "labels": [], "entities": []}, {"text": "The training data was 10-fold jackknifed with our own tagger.", "labels": [], "entities": []}, {"text": "Table 1 shows the tagging accuracy.", "labels": [], "entities": [{"text": "tagging", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9422445297241211}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9809114933013916}]}, {"text": "lists the accuracy of our transitionbased parser with completion model together with results from related work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996892213821411}]}, {"text": "All results use predicted PoS tags.", "labels": [], "entities": []}, {"text": "As a baseline, we present in addition results without the completion model and a graph-based parser with second order features (G 2a ).", "labels": [], "entities": []}, {"text": "For the Graph-based parser, we used 10 training iterations.", "labels": [], "entities": []}, {"text": "The following rows denoted with Ta , T 2a , T 2ab , T 2ab3a , T 2ab3b , T 2ab3bc , and T 2a3abc present the result for the parser with completion model.", "labels": [], "entities": []}, {"text": "The subscript letters denote the used factors of the completion model as shown in to 7.", "labels": [], "entities": []}, {"text": "The parsers with subscribed plus (e.g. G 2a+ ) in addition use feature templates that contain one word left or right of the head, dependent, siblings, and grandchildren.", "labels": [], "entities": []}, {"text": "We left those feature in our previous models out as they may interfere with the second and third order factors.", "labels": [], "entities": []}, {"text": "As in previous work, we exclude punctuation marks for the English data converted with Penn2Malt in the evaluation, cf. (. We optimized the feature model of our parser on section 24 and used section 23 for evaluation.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.9158205986022949}]}, {"text": "We use abeam size of 80 for our transition-based parser and 25 training iterations.", "labels": [], "entities": []}, {"text": "The second English data set was obtained by using the LTH conversion schema as used in the CoNLL Shared.", "labels": [], "entities": [{"text": "English data set", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.7662631968657175}, {"text": "CoNLL Shared", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9450485110282898}]}, {"text": "This corpus preserves the non-projectivity of the phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS Parser UAS LAS ( 90.9) 91.5 92.1 ( 92.9 ( 93.04 93  tags.", "labels": [], "entities": [{"text": "automatic assigned PoS Parser UAS LAS", "start_pos": 122, "end_pos": 159, "type": "METRIC", "confidence": 0.5725201815366745}]}, {"text": "From the same data set, we selected the corpora for Czech and German.", "labels": [], "entities": []}, {"text": "In all cases, we used the provided training, development, and test data split, cf. (. In contrast to the evaluation of the Penn2Malt conversion, we include punctuation marks for these corpora and follow in that the evaluation schema of the CoNLL Shared Task 2009.", "labels": [], "entities": [{"text": "Penn2Malt conversion", "start_pos": 123, "end_pos": 143, "type": "DATASET", "confidence": 0.963785856962204}, {"text": "CoNLL Shared Task 2009", "start_pos": 240, "end_pos": 262, "type": "DATASET", "confidence": 0.8340518474578857}]}, {"text": "presents the results as obtained for these data set.", "labels": [], "entities": []}, {"text": "The transition-based parser obtains higher accuracy scores for Czech but still lower scores for English and German.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9992056488990784}]}, {"text": "For Czech, the result of T is 1.59 percentage points higher than the top labeled score in the CoNLL shared task 2009.", "labels": [], "entities": [{"text": "T", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9972405433654785}, {"text": "CoNLL shared task 2009", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.8006239384412766}]}, {"text": "The reason is that T includes already third order features that are needed to determine some edge labels.", "labels": [], "entities": []}, {"text": "The transition-based parser with completion model: Chinese Attachment Scores for the conversion of CTB 5 with head rules of.", "labels": [], "entities": [{"text": "Chinese Attachment Scores", "start_pos": 51, "end_pos": 76, "type": "METRIC", "confidence": 0.6051634550094604}]}, {"text": "We take the standard split of CTB 5 and use inline with previous work gold segmentation, POStags and exclude punctuation marks for the evaluation. of the score.", "labels": [], "entities": [{"text": "CTB 5", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.685858815908432}, {"text": "POStags", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.827669620513916}]}, {"text": "Small and statistically significant improvements provides the additional second order factor (2b).", "labels": [], "entities": [{"text": "second order factor", "start_pos": 73, "end_pos": 92, "type": "METRIC", "confidence": 0.9273999333381653}]}, {"text": "We tried to determine the best third order factors or set of factors but we cannot denote such a factor which is the best for all languages.", "labels": [], "entities": []}, {"text": "For German, we obtained a significant improvement with the factor (3b).", "labels": [], "entities": []}, {"text": "We believe that this is due to the flat annotation of PPs in the German corpus.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8985338807106018}]}, {"text": "If we combine all third order factors we obtain for the Penn2Malt conversion a small improvement of 0.2 percentage points over the results of (2ab).", "labels": [], "entities": [{"text": "Penn2Malt conversion", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.9634646773338318}]}, {"text": "We think that a more deep feature selection for third order factors may help to improve the actuary further.", "labels": [], "entities": []}, {"text": "In, we present results on the Chinese Treebank.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.9887000918388367}]}, {"text": "To our knowledge, we obtain the best published results so far.", "labels": [], "entities": []}, {"text": "The results of the baseline T compared to T 2ab3abc are statistically significant (p < 0.01).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the training, development and  test data split converted to dependency graphs with  head-finding rules of (Yamada and Matsumoto, 2003).  The last column shows the accuracy of Part-of-Speech  tags.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9993904829025269}]}, {"text": " Table 2: English Attachment Scores for the  Penn2Malt conversion of the Penn Treebank for the  test set. Punctuation is excluded from the evaluation.  The results marked with  \u2020 are not directly comparable  to our work as they depend on additional sources of  information (Brown Clusters).", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.9640235304832458}, {"text": "Penn Treebank", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9867901504039764}, {"text": "Punctuation", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9452993273735046}]}, {"text": " Table 3: Labeled Attachment Scores of parsers that  use the data sets of the CoNLL shared task 2009. In  line with previous work, punctuation is included. The  parsers marked with  \u2020 used a joint model for syntactic  parsing and semantic role labelling. We provide more  parsing results for the languages of CoNLL-X Shared  Task at http://code.google.com/p/mate-tools/.", "labels": [], "entities": [{"text": "CoNLL shared task 2009", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.8402605950832367}, {"text": "syntactic  parsing", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.7022157609462738}]}, {"text": " Table 4: Chinese Attachment Scores for the conver- sion of CTB 5 with head rules of", "labels": [], "entities": [{"text": "CTB 5", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.9212694466114044}]}]}