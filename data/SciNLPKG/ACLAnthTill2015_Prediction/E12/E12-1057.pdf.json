{"title": [{"text": "The effect of domain and text type on text prediction quality", "labels": [], "entities": [{"text": "text prediction", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7671354711055756}]}], "abstractContent": [{"text": "Text prediction is the task of suggesting text while the user is typing.", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7910139858722687}]}, {"text": "Its main aim is to reduce the number of keystrokes that are needed to type a text.", "labels": [], "entities": []}, {"text": "In this paper, we address the influence of text type and domain differences on text prediction quality.", "labels": [], "entities": [{"text": "text prediction quality", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.7668035626411438}]}, {"text": "By training and testing our text prediction algorithm on four different text types (Wikipedia, Twitter, transcriptions of conversational speech and FAQ) with equal corpus sizes, we found that there is a clear effect of text type on text prediction quality: training and testing on the same text type gave percentages of saved keystrokes between 27 and 34%; training on a different text type caused the scores to drop to percentages between 16 and 28%.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7201284021139145}, {"text": "FAQ", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.8581792712211609}, {"text": "text prediction", "start_pos": 232, "end_pos": 247, "type": "TASK", "confidence": 0.7700682580471039}]}, {"text": "In our case study, we compared a number of training corpora fora specific data set for which training data is sparse: questions about neurological issues.", "labels": [], "entities": []}, {"text": "We found that both text type and topic domain play a role in text prediction quality.", "labels": [], "entities": [{"text": "text prediction quality", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7778986493746439}]}, {"text": "The best performing training corpus was a set of medical pages from Wikipedia.", "labels": [], "entities": []}, {"text": "The second-best result was obtained by leave-one-out experiments on the test questions, even though this training corpus was much smaller (2,672 words) than the other corpora (1.5 Million words).", "labels": [], "entities": []}], "introductionContent": [{"text": "Text prediction is the task of suggesting text while the user is typing.", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7910139858722687}]}, {"text": "Its main aim is to reduce the number of keystrokes that are needed to type a text, thereby saving time.", "labels": [], "entities": []}, {"text": "Text prediction algorithms have been implemented for mobile devices, office software (Open Office Writer), search engines (Google query completion), and in specialneeds software for writers who have difficulties typing).", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.767587274312973}]}, {"text": "In most applications, the scope of the prediction is the completion of the current word; hence the oftenused term 'word completion'.", "labels": [], "entities": []}, {"text": "The most basic method for word completion is checking after each typed character whether the prefix typed since the last whitespace is unique according to a lexicon.", "labels": [], "entities": [{"text": "word completion", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.879858672618866}]}, {"text": "If it is, the algorithm suggests to complete the prefix with the lexicon entry.", "labels": [], "entities": []}, {"text": "The algorithm may also suggest to complete a prefix even before the word's uniqueness point is reached, using statistical information on the previous context.", "labels": [], "entities": []}, {"text": "Moreover, it has been shown that significantly better prediction results can be obtained if not only the prefix of the current word is included as previous context, but also previous words or characters (Van den Bosch and).", "labels": [], "entities": []}, {"text": "In the current paper, we followup on this work by addressing the influence of text type and domain differences on text prediction quality.", "labels": [], "entities": [{"text": "text prediction quality", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.7749633391698202}]}, {"text": "Brief messages on mobile devices (such as text messages, Twitter and Facebook updates) are of a different style and lexicon than documents typed in office software.", "labels": [], "entities": []}, {"text": "In addition, the topic domain of the text also influences its content.", "labels": [], "entities": []}, {"text": "These differences may cause an algorithm trained on one text type or domain to perform poorly on another.", "labels": [], "entities": []}, {"text": "The questions that we aim to answer in this paper are (1) \"What is the effect of text type differences on the quality of a text prediction algorithm?\" and (2) \"What is the best choice of training data if domain-and text type-specific data is sparse?\".", "labels": [], "entities": [{"text": "text prediction algorithm", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.7917611797650655}]}, {"text": "To answer these questions, we perform three experiments: 1.", "labels": [], "entities": []}, {"text": "A series of within-text type experiments on four different types of Dutch text: Wikipedia articles, Twitter data, transcriptions of con-versational speech and web pages of Frequently Asked Questions (FAQ).", "labels": [], "entities": []}, {"text": "2. A series of across-text type experiments in which we train and test on different text types; 3.", "labels": [], "entities": []}, {"text": "A case study using texts from a specific domain and text type: questions about neurological issues.", "labels": [], "entities": []}, {"text": "Training data for this combination of language (Dutch), text type (FAQ) and domain (medical/neurological) is sparse.", "labels": [], "entities": []}, {"text": "Therefore, we search for the type of training data that gives the best prediction results for this corpus.", "labels": [], "entities": []}, {"text": "We compare the following training corpora: \u2022 The corpora that we compared in the text type experiments: Wikipedia, Twitter, Speech and FAQ, 1.5 Million words per corpus.", "labels": [], "entities": []}, {"text": "\u2022 A 1.5 Million words training corpus that is of the same domain as the target data: medical pages from Wikipedia; \u2022 The 359 questions from the neuro-QA data themselves, evaluated in a leaveone-out setting (359 times training on 358 questions and evaluating on the remaining questions).", "labels": [], "entities": []}, {"text": "The prospective application of the third series of experiments is the development of a text prediction algorithm in an online care platform: an online community for patients seeking information about their illness.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.689633846282959}]}, {"text": "In this specific case the target group is patients with language disabilities due to neurological disorders.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: In Section 2 we give a brief overview of text prediction methods discussed in the literature.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.8112674653530121}]}, {"text": "In Section 3 we present our approach to text prediction.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.8682248592376709}]}, {"text": "Sections 4 and 5 describe the experiments that we carried out and the results we obtained.", "labels": [], "entities": []}, {"text": "We phrase our conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our algorithms on corpus data.", "labels": [], "entities": []}, {"text": "This means that we have to make assumptions about user behaviour.", "labels": [], "entities": []}, {"text": "We assume that the user confirms a suggested word as soon as it is suggested correctly, not typing any additional characters before confirming.", "labels": [], "entities": []}, {"text": "We evaluate our text prediction algorithms in terms of the percentage of keystrokes saved K: in which n is the number of words in the test set, W i is the number of keystrokes that have been typed before the word i is correctly suggested and F i is the number of keystrokes that would be needed to type the complete word i.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7818154990673065}]}, {"text": "For example, our algorithm correctly predicts the word niveau after the context in gt o t e en n iv in the test set.", "labels": [], "entities": []}, {"text": "Assuming that the user confirms the word niveau at this point, three keystrokes were needed for the prefix niv.", "labels": [], "entities": []}, {"text": "So, W i = 3 and F i = 6.", "labels": [], "entities": [{"text": "F", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9774054884910583}]}, {"text": "The number of keystrokes needed for whitespace and punctuation are unchanged: these have to be typed anyway, independently of the support by a text prediction algorithm.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.6930604726076126}]}, {"text": "In this section, we describe the first and second series of experiments.", "labels": [], "entities": []}, {"text": "The case study on questions from the neurological domain is described in Section 5.", "labels": [], "entities": []}, {"text": "For each of the four text types, we compare the buffer types 'Prefix' and 'Buffer15'.", "labels": [], "entities": [{"text": "Prefix", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.8861348628997803}, {"text": "Buffer15", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9281533360481262}]}, {"text": "In each experiment, we use 1.5 Million words from the corpus to train the algorithm and 100,000 words to test it.", "labels": [], "entities": []}, {"text": "We investigate the importance of text type differences for text prediction with a series of experiments in which we train and test our algorithm on texts of different text types.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.8127297163009644}]}, {"text": "We keep the size of the train and test sets the same: 1.5 Million words and 100,000 words respectively.", "labels": [], "entities": []}, {"text": "The results are in. shows that for all text types, the buffer of 15 characters that crosses word borders gives better results than the prefix of the current word only.", "labels": [], "entities": []}, {"text": "We get a relative improvement of 35% (for FAQ) to 62% (for Speech) of Buffer15 compared to Prefix-only.", "labels": [], "entities": [{"text": "FAQ", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.7117822170257568}, {"text": "Buffer15", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9231994152069092}, {"text": "Prefix-only", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9002022743225098}]}, {"text": "shows that text type differences have an influence on text prediction quality: all acrosstext type experiments lead to lower results than the within-text type experiments.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.8086912333965302}]}, {"text": "From the results in  We aim to find the training set that gives the best text prediction result for the neuro-QA questions.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.6457929313182831}]}, {"text": "We compare the following training corpora: \u2022 The corpora that we compared in the text type experiments: Wikipedia, Twitter, Speech and FAQ, 1.5 Million words per corpus.", "labels": [], "entities": []}, {"text": "\u2022 A 1.5 Million words training corpus that is of the same topic domain as the target data: Wikipedia articles from the medical domain; \u2022 The 359 questions from the neuro-QA data themselves, evaluated in a leave-one-out setting (359 times training on 358 questions and evaluating on the remaining questions).", "labels": [], "entities": []}, {"text": "In order to create the 'medical Wikipedia' corpus, we consulted the category structure of the Wikipedia corpus.", "labels": [], "entities": [{"text": "medical Wikipedia' corpus", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.7093687951564789}, {"text": "Wikipedia corpus", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9199410378932953}]}, {"text": "The Wikipedia category 'Geneeskunde' (Medicine) contains 69,898 pages and in the deeper nodes of the hierarchy we see many non-medical pages, such as trappist beers (ordered under beer, booze, alcohol, Psychoactive drug, drug, and then medicine).", "labels": [], "entities": []}, {"text": "If we remove all pages that are more than five levels under the 'Geneeskunde' category root, 21,071 pages are left, which contain fairly over the 1.5 Million words that we need.", "labels": [], "entities": [{"text": "Geneeskunde' category root", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.875372976064682}]}, {"text": "We used the first 1.5 Million words of the corpus in our experiments.", "labels": [], "entities": []}, {"text": "The text prediction results for the different corpora are in.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6334916055202484}]}, {"text": "For each corpus, the out-ofvocabulary rate is given: the percentage of words in the Neuro-QA questions that do not occur in the corpus.", "labels": [], "entities": [{"text": "out-ofvocabulary rate", "start_pos": 21, "end_pos": 42, "type": "METRIC", "confidence": 0.9664583206176758}]}], "tableCaptions": [{"text": " Table 1: Results from the within-text type experiments in terms of percentages of saved keystrokes.  Prefix means: 'use the previous characters of the current word as features'. Buffer 15 means 'use a buffer  of the previous 15 characters as features'.  Prefix Buffer15  Wikipedia 22.2%  30.5%  Twitter  21.3%  29.2%  Speech  20.7%  33.4%  FAQ  20.2%  27.2%", "labels": [], "entities": [{"text": "Prefix", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9571577310562134}, {"text": "Buffer", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9812954664230347}, {"text": "FAQ", "start_pos": 341, "end_pos": 344, "type": "DATASET", "confidence": 0.7184544205665588}]}, {"text": " Table 2: Results from the across-text type experiments in terms of percentages of saved keystrokes, using  the best-scoring configuration from the within-text type experiments: a buffer of 15 characters  Trained on Tested on Wikipedia Tested on Twitter Tested on Speech Tested on FAQ  Wikipedia  30.5%  16.5%  22.3%  24.9%  Twitter  17.9%  29.2%  27.9%  20.7%  Speech  19.7%  22.5%  33.4%  21.0%  FAQ  22.6%  18.2%  22.9%  27.2%", "labels": [], "entities": [{"text": "FAQ  Wikipedia", "start_pos": 281, "end_pos": 295, "type": "DATASET", "confidence": 0.9440511167049408}, {"text": "FAQ", "start_pos": 398, "end_pos": 401, "type": "DATASET", "confidence": 0.9647988080978394}]}, {"text": " Table 4: Results for the neuro-QA questions only in terms of percentages of saved keystrokes, using  different training sets. The text prediction configuration used in all settings is Buffer15. The test samples  are 359 questions with an average length of 7.5 words. The percentages of saved keystrokes are means  over the 359 questions.  Training corpus  # words  Mean % of saved keystrokes in  neuro-QA questions (stdev)", "labels": [], "entities": [{"text": "Buffer15", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9055072069168091}]}]}