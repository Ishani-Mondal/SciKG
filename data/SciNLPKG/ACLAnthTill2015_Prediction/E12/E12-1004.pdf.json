{"title": [{"text": "Entailment above the word level in distributional semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce two ways to detect entail-ment using distributional semantic representations of phrases.", "labels": [], "entities": []}, {"text": "Our first experiment shows that the entailment relation between adjective-noun constructions and their head nouns (big cat |= cat), once represented as semantic vector pairs, generalizes to lexical entailment among nouns (dog |= animal).", "labels": [], "entities": []}, {"text": "Our second experiment shows that a classi-fier fed semantic vector pairs can similarly generalize the entailment relation among quantifier phrases (many dogs|=some dogs) to entailment involving unseen quantifiers (all cats|=several cats).", "labels": [], "entities": []}, {"text": "Moreover, nominal and quantifier phrase entailment appears to be cued by different distributional correlates , as predicted by the type-based view of entailment informal semantics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantics (DS) approximates linguistic meaning with vectors summarizing the contexts where expressions occur.", "labels": [], "entities": [{"text": "Distributional semantics (DS)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7566760182380676}]}, {"text": "The success of DS in lexical semantics has validated the hypothesis that semantically similar expressions occur in similar contexts.", "labels": [], "entities": []}, {"text": "Formal semantics (FS) represents linguistic meanings as symbolic formulas and assemble them via composition rules.", "labels": [], "entities": [{"text": "Formal semantics (FS)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7654014527797699}]}, {"text": "FS has successfully modeled quantification and captured inferential relations between phrases and between sentences.", "labels": [], "entities": []}, {"text": "The strengths of DS and FS have been complementary to date: On one hand, DS has induced large-scale semantic representations from corpora, but it has been largely limited to the lexical domain.", "labels": [], "entities": [{"text": "FS", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.6195765733718872}]}, {"text": "On the other hand, FS has provided sophisticated models of sentence meaning, but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data.", "labels": [], "entities": [{"text": "FS", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9596362113952637}, {"text": "sentence meaning", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7166840583086014}]}, {"text": "Given these complementary strengths, we naturally ask if DS and FS can address each other's limitations.", "labels": [], "entities": [{"text": "DS", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.5842089653015137}, {"text": "FS", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9090795516967773}]}, {"text": "Two recent strands of research are bringing DS closer to meeting core FS challenges.", "labels": [], "entities": [{"text": "DS", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9879978895187378}, {"text": "FS", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.929444432258606}]}, {"text": "One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (.", "labels": [], "entities": []}, {"text": "The other strand attempts to reformulate FS's notion of logical inference in terms that DS can capture.", "labels": [], "entities": [{"text": "FS", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.6239652633666992}]}, {"text": "In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal).", "labels": [], "entities": []}, {"text": "This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expressions bear on inference.", "labels": [], "entities": []}, {"text": "Here we focus on phrasal vectors harvested directly from the corpus rather than obtained compositionally.", "labels": [], "entities": []}, {"text": "Ina first experiment, we exploit the entailment properties of a class of composite expressions, namely adjective-noun constructions (ANs), to harvest training data for an entailment recognizer.", "labels": [], "entities": []}, {"text": "The recognizer is then successfully applied to detect lexical entailment.", "labels": [], "entities": []}, {"text": "In short, since almost all ANs entail the noun they contain (red car entails car), the distributional vectors of AN-N pairs can train a classifier to detect noun pairs that stand in the same relation (dog entails animal).", "labels": [], "entities": []}, {"text": "With almost no manual effort, we achieve performance nearly identical with the state-of-the-art balAPinc measure that crafted, which detects feature inclusion between the two nouns' occurrence contexts.", "labels": [], "entities": []}, {"text": "Our second experiment goes beyond lexical inference.", "labels": [], "entities": []}, {"text": "We look at phrases built from a quantifying determiner and a noun (QNs) and use their distributional vectors to recognize entailment relations of the form many dogs |= some dogs, between two QNs sharing the same noun.", "labels": [], "entities": []}, {"text": "It turns out that a classifier trained on a set of Q 1 N |= Q 2 N pairs can recognize entailment in pairs with anew quantifier configuration.", "labels": [], "entities": []}, {"text": "For example, we can train on many dogs |= some dogs then correctly predict all cats|=several cats.", "labels": [], "entities": []}, {"text": "Interestingly, on the QN entailment task, neither our classifier trained on AN-N pairs nor the balAPinc method beat baseline methods.", "labels": [], "entities": [{"text": "QN entailment task", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8009951909383138}]}, {"text": "This suggests that our successful QN classifiers tap into vector properties beyond such relations as feature inclusion that those methods for nominal entailment rely upon.", "labels": [], "entities": [{"text": "nominal entailment", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.8016192018985748}]}, {"text": "Together, our experiments show that corpusharvested DS representations of composite expressions such as ANs and QNs contain sufficient information to capture and generalize their inference patterns.", "labels": [], "entities": []}, {"text": "This result brings DS closer to the central concerns of FS.", "labels": [], "entities": [{"text": "DS", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9218550324440002}, {"text": "FS", "start_pos": 56, "end_pos": 58, "type": "DATASET", "confidence": 0.5487773418426514}]}, {"text": "In particular, the QN study is the first to our knowledge to show that DS vectors capture semantic properties not only of content words, but of an important class of function words (quantifying determiners) deeply studied in FS but of little interest until now in DS.", "labels": [], "entities": [{"text": "FS", "start_pos": 225, "end_pos": 227, "type": "DATASET", "confidence": 0.5844718813896179}]}, {"text": "Besides these theoretical implications, our results are of practical import.", "labels": [], "entities": []}, {"text": "First, our AN study presents a novel, practical method for detecting lexical entailment that reaches state-of-theart performance with little or no manual intervention.", "labels": [], "entities": [{"text": "AN", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.8588389158248901}, {"text": "detecting lexical entailment", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.8737380504608154}]}, {"text": "Lexical entailment is in turn fundamental for constructing ontologies and other lexical resources (.", "labels": [], "entities": []}, {"text": "Second, our QN study demonstrates that phrasal entailment can be automatically detected and thus paves the way to apply DS to advanced NLP tasks such as recognizing textual entailment ().", "labels": [], "entities": [{"text": "QN study", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.8926447331905365}, {"text": "recognizing textual entailment", "start_pos": 153, "end_pos": 183, "type": "TASK", "confidence": 0.7905877828598022}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. For each of these 30 quantifier pairs  (Q 1 , Q 2 ), we enumerate those WordNet nouns N  such that semantic vectors are available for both  Q 1 N and Q 2 N (that is, both sequences occur in  at least 100 times). Each such noun then gives", "labels": [], "entities": []}, {"text": " Table 2: Detecting lexical entailment. Results ranked  by accuracy and expressed as percentages. 95% con- fidence intervals around accuracy calculated by bino- mial exact tests.", "labels": [], "entities": [{"text": "Detecting lexical entailment", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.916467030843099}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9991340041160583}, {"text": "con- fidence intervals", "start_pos": 102, "end_pos": 124, "type": "METRIC", "confidence": 0.8635697215795517}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9889847040176392}]}, {"text": " Table 3: Detecting quantifier entailment. Results  ranked by accuracy and expressed as percentages.  95% confidence intervals around accuracy calculated  by binomial exact tests.", "labels": [], "entities": [{"text": "Detecting quantifier entailment", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8868523041407267}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9992467164993286}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9978047013282776}]}, {"text": " Table 4: Breakdown of results with leaving-one- quantifier-out (SVM quantifier-out ) training regime.", "labels": [], "entities": []}]}