{"title": [{"text": "Experimenting with Distant Supervision for Emotion Classification", "labels": [], "entities": [{"text": "Distant Supervision", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.9341311156749725}, {"text": "Emotion Classification", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.90547314286232}]}], "abstractContent": [{"text": "We describe a set of experiments using automatically labelled data to train supervised classifiers for multi-class emotion detection in Twitter messages with no manual intervention.", "labels": [], "entities": [{"text": "multi-class emotion detection in Twitter messages", "start_pos": 103, "end_pos": 152, "type": "TASK", "confidence": 0.7627970774968466}]}, {"text": "By cross-validating between models trained on different labellings for the same six basic emotion classes, and testing on manually labelled data, we conclude that the method is suitable for some emotions (happiness, sadness and anger) but less able to distinguish others; and that different labelling conventions are more suitable for some emotions than others.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a set of experiments into classifying Twitter messages into the six basic emotion classes of.", "labels": [], "entities": []}, {"text": "The motivation behind this work is twofold: firstly, to investigate the possibility of detecting emotions of multiple classes (rather than purely positive or negative sentiment) in such short texts; and secondly, to investigate the use of distant supervision to quickly bootstrap large datasets and classifiers without the need for manual annotation.", "labels": [], "entities": []}, {"text": "Text classification according to emotion and sentiment is a well-established research area.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8462594449520111}]}, {"text": "In this and other areas of text analysis and classification, recent years have seen arise in use of data from online sources and social media, as these provide very large, often freely available datasets (see e.g. () amongst many others).", "labels": [], "entities": [{"text": "text analysis and classification", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7559667229652405}]}, {"text": "However, one of the challenges this poses is that of data annotation: given very large amounts of data, often consisting of very short texts, written in unconventional style and without accompanying metadata, audio/video signals or access to the author for disambiguation, how can we easily produce a gold-standard labelling for training and/or for evaluation and test?", "labels": [], "entities": [{"text": "data annotation", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7581837475299835}]}, {"text": "One possible solution that is becoming popular is crowd-sourcing the labelling task, as the easy access to very large numbers of annotators provided by tools such as Amazon's Mechanical Turk can help with the problem of dataset size; however, this has its own attendant problems of annotator reliability (see e.g. (), and cannot directly help with the inherent problem of ambiguity -using many annotators does not guarantee that they can understand or correctly assign the author's intended interpretation or emotional state.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a different approach via distant supervision (see e.g. ().", "labels": [], "entities": []}, {"text": "By using conventional markers of emotional content within the texts themselves as a surrogate for explicit labels, we can quickly retrieve large subsets of (noisily) labelled data.", "labels": [], "entities": []}, {"text": "This approach has the advantage of giving us direct access to the authors' own intended interpretation or emotional state, without relying on thirdparty annotators.", "labels": [], "entities": []}, {"text": "Of course, the labels themselves maybe noisy: ambiguous, vague or not having a direct correspondence with the desired classification.", "labels": [], "entities": []}, {"text": "We therefore experiment with multiple such conventions with apparently similar meanings -here, emoticons (following) and Twitter hashtags -allowing us to examine the similarity of classifiers trained on independent labels but intended to detect the same underlying class.", "labels": [], "entities": []}, {"text": "We also investigate the precision and correspondence of particular labels with the desired emotion classes by testing on a small set of man-ually labelled data.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9979185461997986}]}, {"text": "We show that the success of this approach depends on both the conventional markers chosen and the emotion classes themselves.", "labels": [], "entities": []}, {"text": "Some emotions are both reliably marked by different conventions and distinguishable from other emotions; this seems particularly true for happiness, sadness and anger, indicating that this approach can provide not only the basic distinction required for sentiment analysis but some more finer-grained information.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 254, "end_pos": 272, "type": "TASK", "confidence": 0.8156223595142365}]}, {"text": "Others are either less distinguishable from short text messages, or less reliably marked.", "labels": [], "entities": []}], "datasetContent": [{"text": "Throughout, the markers (emoticons and/or hashtags) used as labels in any experiment were removed before feature extraction in that experiment -labels were not used as features.", "labels": [], "entities": []}, {"text": "To simulate the task of detecting emotion classes from a general stream of messages, we first built for each convention type C and each emotion class E a dataset DC E of size N containing (a) as positive instances, N/2 messages containing markers of the emotion class E and no other markers of type C, and (b) as negative instances, N/2 messages containing markers of type C of any other emotion class.", "labels": [], "entities": []}, {"text": "For example, the positive instance set for emoticon-marked anger was based on those tweets which contained :-@ or :@, but none of the emoticons from the happy, sad, surprise, disgust or fear classes; any hashtags were allowed, including those associated with emotion classes.", "labels": [], "entities": []}, {"text": "The negative instance set contained a representative sample of the same number of instances, with each having at least one of the happy, sad, surprise, disgust or fear emoticons but not containing :-@ or :@.", "labels": [], "entities": []}, {"text": "This of course excludes messages with no emotional markers; for this to act as an approximation of the general task therefore requires a assumption that unmarked messages reflect the same distribution over emotion classes as marked messages.", "labels": [], "entities": []}, {"text": "For emotion-carrying but unmarked messages, this does seem intuitively likely, but requires investigation.", "labels": [], "entities": []}, {"text": "For neutral objective messages it is clearly false, but as stated above we assume a preliminary stage of subjectivity detection in any practical application.", "labels": [], "entities": [{"text": "subjectivity detection", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7434337735176086}]}, {"text": "Performance was evaluated using 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "Results are shown as the bold figures in; despite the small dataset sizes in some cases, a \u03c7 2 test shows all to be significantly different from chance.", "labels": [], "entities": []}, {"text": "The bestperforming classes show accuracies very similar to those achieved by for their binary positive/negative classification, as might be expected; for emoticon markers, the best classes are happy, sad and anger; interestingly the best classes for hashtag markers are not the same -happy performs best, but disgust and fear outperform sad and anger, and surprise performs particularly badly.", "labels": [], "entities": []}, {"text": "For sad, one reason maybe a dual meaning of the tag #sad (one emotional and one expressing ridicule); for anger one possibility is the popularity on Twitter of the game \"Angry Birds\"; for surprise, the data seems split between two rather distinct usages, ones expressing the author's emotion, but one expressing an intended effect on the audience (see (5)).", "labels": [], "entities": []}, {"text": "However, deeper analysis is needed to establish the exact causes.", "labels": [], "entities": []}, {"text": "(5) a. broke 100 followers.", "labels": [], "entities": []}, {"text": "#surprised im glad that the HOFF is one of them. b. Who's excited for the Big Game?", "labels": [], "entities": [{"text": "HOFF", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.5863924026489258}]}, {"text": "We know we are AND we have a #surprise for you!", "labels": [], "entities": []}, {"text": "To investigate whether the different convention types actually convey similar properties (and hence are used to mark similar messages) we then compared these accuracies to those obtained by training classifiers on the dataset fora different convention: in other words, for each emotion class E, train a classifier on dataset D C1 E and test on D C2 E . As the training and testing sets are different, we now test on the entire dataset rather than using cross-validation.", "labels": [], "entities": []}, {"text": "Results are shown as the italic figures in; a \u03c7 2 test shows all to be significantly different from the bold sameconvention results.", "labels": [], "entities": []}, {"text": "Accuracies are lower overall, but the highest figures (between 63% and 68%) are achieved for happy, sad and anger; here perhaps we can have some confidence that not only are the markers acting as predictable labels themselves, but also seem to be labelling the same thing (and therefore perhaps are actually labelling the emotion we are hoping to label).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9932904243469238}]}, {"text": "To investigate whether these independent classifiers can be used in multi-class classification (distinguishing emotion classes from each other rather than just distinguishing one class from a general \"other\" set), we next cross-tested the classifiers between emotion classes: training models on one emotion and testing on the others -for each convention type C and each emotion class E1, train a classifier on dataset DC E1 and test on etc.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.7197070121765137}]}, {"text": "The datasets in Experiment 1 had an uneven balance of emotion classes (including a high proportion of happy instances) which could bias results; for this experiment, therefore, we created datasets with an even balance of emotions among the negative instances.", "labels": [], "entities": []}, {"text": "For each convention type C and each emotion class E1, we built a dataset DC E1 of size N containing (a) as positive instances, N/2 messages containing markers of the emotion class E1 and no other markers of type C, and (b) as negative instances, N/2 messages consisting of N/10 messages containing only markers of class E2, N/10 messages containing only markers of class E3 etc.", "labels": [], "entities": []}, {"text": "Results were then generated as in Experiment 1.", "labels": [], "entities": []}, {"text": "Within-class results are shown in and are similar to those obtained in Experiment 1; again, differences between bold/italic results are statistically significant.", "labels": [], "entities": []}, {"text": "Cross-class results are shown in.", "labels": [], "entities": []}, {"text": "The happy class was well distinguished from other emotion classes for both convention types (i.e. cross-class classification accuracy is low compared to the within-class figures in italics and parentheses).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9320091605186462}]}, {"text": "The sad class also seems well distinguished when using hashtags as labels, although less so when using emoticons.", "labels": [], "entities": []}, {"text": "However, other emotion classes show a surprisingly high cross-class performance in many cases -in other words, they are producing disappointingly similar classifiers.", "labels": [], "entities": []}, {"text": "This poor discrimination for negative emotion classes maybe due to ambiguity or vagueness in the label, similarity of the verbal content associ-  ated with the emotions, or of genuine frequent copresence of the emotions.", "labels": [], "entities": []}, {"text": "Given the close lexical specification of emotions in hashtag labels, the latter reasons seem more likely; however, with emoticon labels, we suspect that the emoticons themselves are often used in ambiguous or vague ways.", "labels": [], "entities": []}, {"text": "As one way of investigating this directly, we tested classifiers across labelling conventions as well as across emotion classes, to determine whether the (lack of) cross-class discrimination holds across convention marker types.", "labels": [], "entities": []}, {"text": "In the case of ambiguity or vagueness of emoticons, we would expect emoticon-trained models to fail to discriminate hashtag-labelled test sets, but hashtag-trained models to discriminate emoticonlabelled test sets well; if on the other hand the cause lies in the overlap of verbal content or the emotions themselves, the effect should be similar in either direction.", "labels": [], "entities": []}, {"text": "This experiment also helps determine in more detail whether the labels used label similar underlying properties.", "labels": [], "entities": []}, {"text": "For the three classes happy, sad and perhaps anger, models trained using emoticon labels do a reasonable job of distinguishing classes in hashtag-labelled data, and vice versa.", "labels": [], "entities": []}, {"text": "However, for the other classes, discrimination is worse.", "labels": [], "entities": []}, {"text": "Emoticon-trained models appear to give (undesirably) higher performance across emotion classes in hashtag-labelled data (for the problematic non-happy classes).", "labels": [], "entities": []}, {"text": "Hashtag-trained models perform around the random 50% level on emoticon-labelled data for those classes, even when tested on nominally the same emotion as they are trained on.", "labels": [], "entities": []}, {"text": "For both label types, then, the lower within-class and higher cross-class performance with these negative classes (fear, surprise, disgust) suggests that these emotion classes are genuinely hard to tell apart (they are all negative emotions, and may use similar words), or are simply often expressed simultaneously.", "labels": [], "entities": []}, {"text": "The higher performance of emoticon-trained classifiers compared to hashtag-trained classifiers, though, also suggests vagueness or ambiguity in emoticons: data labelled with emoticons nominally thought to be associated with surprise produces classifiers which perform well on data labelled with many other hashtag classes, suggesting that those emotions were present in the training data.", "labels": [], "entities": []}, {"text": "Conversely, the more specific hashtag labels produce classifiers which perform poorly on data labelled with emoticons and which thus contains a range of actual emotions.", "labels": [], "entities": []}, {"text": "To confirm whether either (or both) set of automatic (distant) labels do in fact label the underlying emotion class intended, we used human annotators via Amazon's Mechanical Turk to label a set of 1,000 instances.", "labels": [], "entities": []}, {"text": "These instances were all labelled with emoticons (we did not use hashtaglabelled data: as hashtags are so lexically close to the names of the emotion classes being labelled, their presence may influence labellers unduly) and were evenly distributed across the 6 classes, in so far as indicated by the emoticons.", "labels": [], "entities": []}, {"text": "Labellers were asked to choose the primary emotion class (from the fixed set of six) associated with the message; they were also allowed to specify if any other classes were also present.", "labels": [], "entities": []}, {"text": "Each data instance was labelled by three different annotators.", "labels": [], "entities": []}, {"text": "Agreement between labellers was poor overall.", "labels": [], "entities": []}, {"text": "The three annotators unanimously agreed in only 47% of cases overall; although two of three agreed in 83% of cases.", "labels": [], "entities": []}, {"text": "Agreement was worst for the three classes already seen to be problematic: surprise, fear and disgust.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9780598282814026}, {"text": "surprise", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.763619601726532}]}, {"text": "To create our dataset for this experiment, we therefore took only instances which were given the same primary label by all labellers -i.e. only those examples which we could take as reliably and unambiguously labelled.", "labels": [], "entities": []}, {"text": "This gave an unbalanced dataset, with numbers varying from 266 instances for happy to only 12 instances for each of surprise and fear.", "labels": [], "entities": []}, {"text": "Classifiers were trained using the datasets from Experiment 2.", "labels": [], "entities": []}, {"text": "Performance is shown in; given the imbalance between class numbers in the test dataset, evaluation is given as recall, precision and F-score for the class in question rather than a simple accuracy figure (which is biased by the high proportion of happy examples).", "labels": [], "entities": [{"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9996060729026794}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9989400506019592}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9981889128684998}, {"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9967934489250183}]}, {"text": "Again, results for happy are good, and correspond fairly closely to the levels of accuracy reported by and others for the binary positive/negative sentiment detection task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.997931957244873}, {"text": "binary positive/negative sentiment detection task", "start_pos": 122, "end_pos": 171, "type": "TASK", "confidence": 0.7435634987694877}]}, {"text": "Emoticons give significantly better performance than hashtags here.", "labels": [], "entities": [{"text": "Emoticons", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.85954350233078}]}, {"text": "Results for sad and anger are reasonable, and provide a baseline for further experiments with more advanced features and classification methods once more manually annotated data is available for these classes.", "labels": [], "entities": []}, {"text": "In contrast, hashtags give much better performance with these classes than the (perhaps vague or ambiguous) emoticons.", "labels": [], "entities": []}, {"text": "The remaining emotion classes, however, show poor performance for both labelling conventions.", "labels": [], "entities": []}, {"text": "The observed low precision and high recall can be adjusted using classifier parameters, but F-scores are not improved.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9880263805389404}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9992555975914001}, {"text": "F-scores", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.996624231338501}]}, {"text": "Note that Experiment 1 shows that both emoticon and hashtag labels are to some extent predictable, even for these classes; however, Experiment 2 shows that they may not be reliably different to each other, and Experiment 3 tells us that they do not appear to coincide well with human annotator judgements of emotions.", "labels": [], "entities": []}, {"text": "More reliable labels may therefore be required; although we do note that the low reliability of the human annotations for these classes, and the correspondingly small amount of annotated data used in this evaluation, means we hesitate to draw strong conclusions about fear, surprise and disgust.", "labels": [], "entities": [{"text": "reliability", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9694405794143677}]}, {"text": "An approach which considers multiple classes to be associated with individual messages may also be beneficial: using majoritydecision labels rather than unanimous labels improves F-scores for surprise to 23-35% by including many examples also labelled as happy (although this gives no improvements for other classes).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9958993792533875}]}], "tableCaptions": [{"text": " Table 2: Experiment 1: Within-class results. Same- convention (bold) figures are accuracies over 10-fold  cross-validation; cross-convention (italic) figures are  accuracies over full sets.", "labels": [], "entities": []}, {"text": " Table 4: Experiment 2: Cross-class results. Same-class figures from 10-fold cross-validation are shown in  (italics) for comparison; all other figures are accuracies over full sets.", "labels": [], "entities": []}, {"text": " Table 3: Experiment 2: Within-class results. Same- convention (bold) figures are accuracies over 10-fold  cross-validation; cross-convention (italic) figures are  accuracies over full sets.", "labels": [], "entities": []}, {"text": " Table 5: Experiment 2: Cross-class, cross-convention results (train on hashtags, test on emoticons and vice  versa). All figures are accuracies over full sets. Accuracies over 60% are shown in bold.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9936664700508118}]}, {"text": " Table 6: Experiment 3: Results on manual labels.", "labels": [], "entities": []}]}