{"title": [{"text": "Hierarchical Bayesian Language Modelling for the Linguistically Informed", "labels": [], "entities": [{"text": "Hierarchical Bayesian Language Modelling", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5346979051828384}]}], "abstractContent": [{"text": "In this work I address the challenge of augmenting n-gram language models according to prior linguistic intuitions.", "labels": [], "entities": []}, {"text": "I argue that the family of hierarchical Pitman-Yor language models is an attractive vehicle through which to address the problem, and demonstrate the approach by proposing a model for German compounds.", "labels": [], "entities": []}, {"text": "In an empirical evaluation, the model outperforms the Kneser-Ney model in terms of perplex-ity, and achieves preliminary improvements in English-German translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The importance of effective language models in machine translation (MT) and automatic speech recognition (ASR) is widely recognised.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8403842210769653}, {"text": "automatic speech recognition (ASR)", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.7909526427586874}]}, {"text": "n-gram models, in particular ones using Kneser-Ney (KN) smoothing, have become the standard workhorse for these tasks.", "labels": [], "entities": []}, {"text": "These models are not ideal for languages that have relatively free word order and/or complex morphology.", "labels": [], "entities": []}, {"text": "The ability to encode additional linguistic intuitions into models that already have certain attractive properties is an important piece of the puzzle of improving machine translation quality for those languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7650949358940125}]}, {"text": "But despite their widespread use, KN n-gram models are not easily extensible with additional model components that target particular linguistic phenomena.", "labels": [], "entities": []}, {"text": "I argue in this paper that the family of hierarchical Pitman-Yor language models (HPYLM)) are suitable for investigations into more linguistically-informed n-gram language models.", "labels": [], "entities": []}, {"text": "Firstly, the flexibility to specify arbitrary back-off distributions makes it easy to incorporate multiple models into a larger n-gram model.", "labels": [], "entities": []}, {"text": "Secondly, the Pitman-Yor process prior) generates distributions that are well-suited to a variety of powerlaw behaviours, as is often observed in language.", "labels": [], "entities": []}, {"text": "Catering fora variety of those is important since the frequency distributions of, say, suffixes, could be quite different from that of words.", "labels": [], "entities": []}, {"text": "KN smoothing is less flexibility in this regard.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8194721937179565}]}, {"text": "And thirdly, the basic inference algorithms have been parallelised, which should in principle allow the approach to still scale to large data sizes.", "labels": [], "entities": []}, {"text": "As a test bed, I consider compounding in German, a common phenomenon that creates challenges for machine translation into German.", "labels": [], "entities": [{"text": "compounding in German", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8767446279525757}, {"text": "machine translation into German", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.8110087364912033}]}], "datasetContent": [{"text": "The aim of the experiments reported here is to test whether the richer account of compounds in the proposed language models has positive effects on the predictability of unseen text and the generation of better translations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Monolingual evaluation results. The second  column shows perplexity measured all WMT11 Ger- man development data (7065 sentences). At the word  level, all are trigram models, while F are bigram mod- els using the specified segmentation scheme. The third  column has test cross-entropies measured only on the  6099 compounds in the test set (given their contexts ).", "labels": [], "entities": [{"text": "WMT11 Ger- man development data", "start_pos": 91, "end_pos": 122, "type": "DATASET", "confidence": 0.7970612843831381}]}, {"text": " Table 3: Translation results, BLEU (1-ref), 3003 test  sentences. Trigram language models, no count prun- ing, no \"unknown word\" token.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9987196922302246}]}, {"text": " Table 5: Compound n-grams in the test set for which  the absolute difference \u2206 = P HPYLM+c \u2212P mKN is great- est. C is n-gram count in the training data. Asterisks  denote words that are not compounds, linguistically  speaking. Abbrevs: r. = reduktionen, s.= senkungen", "labels": [], "entities": []}]}