{"title": [{"text": "Inferring Selectional Preferences from Part-Of-Speech N-grams", "labels": [], "entities": [{"text": "Selectional Preferences", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7828374207019806}]}], "abstractContent": [{"text": "We present the PONG method to compute selectional preferences using part-of-speech (POS) N-grams.", "labels": [], "entities": []}, {"text": "From a corpus labeled with grammatical dependencies, PONG learns the distribution of word relations for each POS N-gram.", "labels": [], "entities": []}, {"text": "From the much larger but unlabeled Google N-grams corpus, PONG learns the distribution of POS N-grams fora given pair of words.", "labels": [], "entities": []}, {"text": "We derive the probability that one word has a given grammatical relation to the other.", "labels": [], "entities": []}, {"text": "PONG estimates this probability by combining both distributions, whether or not either word occurs in the labeled corpus.", "labels": [], "entities": []}, {"text": "PONG achieves higher average precision on 16 relations than a state-of-the-art baseline in a pseudo-disambiguation task, but lower coverage and recall.", "labels": [], "entities": [{"text": "PONG", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7492272853851318}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9869323372840881}, {"text": "coverage", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9962590932846069}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9987251162528992}]}], "introductionContent": [{"text": "Selectional preferences specify plausible fillers for the arguments of a predicate, e.g., celebrate.", "labels": [], "entities": []}, {"text": "Can you celebrate a birthday?", "labels": [], "entities": []}, {"text": "Can you celebrate a pencil?", "labels": [], "entities": []}, {"text": "Arguably yes: Today the Acme Pencil Factory celebrated its one-billionth pencil.", "labels": [], "entities": [{"text": "Acme Pencil Factory", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.8623586098353068}]}, {"text": "However, such a contrived example is unnatural because unlike birthday, pencil lacks a strong association with celebrate.", "labels": [], "entities": []}, {"text": "How can we compute the degree to which birthday or pencil is a plausible and typical object of celebrate?", "labels": [], "entities": []}, {"text": "Formally, we are interested in computing the probability Pr(r | t, R), where (as specifies), t is a target word such as celebrate, r is a word possibly related to it, such as birthday or pencil, and R is a possible relation between them, whether a semantic role such as the agent of an action, or a grammatical dependency such as the object of a verb.", "labels": [], "entities": []}, {"text": "We call t the \"target\" because originally it referred to a vocabulary word targeted for instruction, and r its \"relative.\"", "labels": [], "entities": []}, {"text": "Notation Description Ra relation between words ta target word r, r possible relatives oft g a word N-gram g i and g j i th and j th words of g p the POS N-gram of g: Notation used throughout this paper Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation, dependency parsing (), and semantic role labeling ().", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 311, "end_pos": 336, "type": "TASK", "confidence": 0.6702813307444254}, {"text": "dependency parsing", "start_pos": 338, "end_pos": 356, "type": "TASK", "confidence": 0.776470959186554}, {"text": "semantic role labeling", "start_pos": 365, "end_pos": 387, "type": "TASK", "confidence": 0.7005935708681742}]}, {"text": "However, selectional preferences can also apply to natural language generation tasks such as sentence generation and question generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7347820202509562}, {"text": "sentence generation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7469137012958527}, {"text": "question generation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7882220447063446}]}, {"text": "For generation tasks, choosing the right word to express a specified argument of a relation requires knowing its connotations -that is, its selectional preferences.", "labels": [], "entities": []}, {"text": "Therefore, it is useful to know selectional preferences for many different relations.", "labels": [], "entities": []}, {"text": "Such knowledge could have many uses.", "labels": [], "entities": []}, {"text": "In education, they could help teach word connotations.", "labels": [], "entities": [{"text": "teach word connotations", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.6177923580010732}]}, {"text": "In machine learning they could help computers learn languages.", "labels": [], "entities": []}, {"text": "In machine translation, they could help generate more natural wording.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7455791532993317}]}, {"text": "This paper introduces a method named PONG (for Part-Of-Speech N-Grams) to compute selectional preferences for many different relations by combining part-of-speech information and Google N-grams.", "labels": [], "entities": [{"text": "PONG", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9762667417526245}]}, {"text": "PONG achieves higher precision on a pseudo-disambiguation task than the best previous model), but lower coverage.", "labels": [], "entities": [{"text": "PONG", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7439286708831787}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9990692734718323}, {"text": "coverage", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9856671690940857}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the relations for which we compute selectional preferences.", "labels": [], "entities": []}, {"text": "Section 5 relates PONG to prior work.", "labels": [], "entities": [{"text": "PONG", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.7401726841926575}]}], "datasetContent": [{"text": "To evaluate PONG, we use a standard pseudodisambiguation task, detailed in Section 4.1.", "labels": [], "entities": [{"text": "PONG", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.5341973900794983}]}, {"text": "Section 4.2 describes our test set.", "labels": [], "entities": []}, {"text": "Section 4.3 lists the metrics we evaluate on this test set.", "labels": [], "entities": []}, {"text": "Section 4.4 describes the baselines we compare PONG against on these metrics, and Section 4.5 describes the relations we compare them on.", "labels": [], "entities": []}, {"text": "Section 4.6 reports our results.", "labels": [], "entities": []}, {"text": "Section 4.7 analyzes sources of error.", "labels": [], "entities": []}, {"text": "The pseudo-disambiguation task () is as follows: given a target word t, a relation R, a relative r, and a random distracter r', prefer either r or r', whichever is likelier to have relation R to word t.", "labels": [], "entities": []}, {"text": "This evaluation does not use a threshold: just prefer whichever word is likelier according to the model being evaluated.", "labels": [], "entities": []}, {"text": "If the model assigns only one of the words a probability, prefer it, based on the assumption that the unknown probability of the other word is lower.", "labels": [], "entities": []}, {"text": "If the model assigns the same probability to both words, or no probability to either word, do not prefer either word.", "labels": [], "entities": []}, {"text": "PONG's precision was significantly better than EPP (p<0.001) but worse than DEP (p<0.0001).", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9977422952651978}, {"text": "DEP", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.8479505181312561}]}, {"text": "Still, PONG's high precision validates its underlying assumption that POS Ngrams strongly predict grammatical dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9976257681846619}]}, {"text": "On coverage and recall, EPP beat PONG, which beat DEP (p<0.0001).", "labels": [], "entities": [{"text": "coverage", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9508914947509766}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9985741376876831}, {"text": "EPP", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.601993203163147}, {"text": "PONG", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.5419754981994629}, {"text": "DEP", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.5171758532524109}]}, {"text": "PONG's F-score was higher, but not significantly, than EPP's (p>0.5) or DEP's (p>0.02).", "labels": [], "entities": [{"text": "PONG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.738450825214386}, {"text": "F-score", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.9935262203216553}, {"text": "EPP", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.7904348969459534}, {"text": "DEP", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.7002108097076416}]}], "tableCaptions": [{"text": " Table 3: Test set size for each relation", "labels": [], "entities": []}, {"text": " Table 4: Relations tested in the pseudo-disambiguation experiment.  Relation names and descriptions are from de Marneffe and Manning (2008) except for prep_of.  Target and relative POS are the most frequent POS pairs for the relations in our labeled WSJ corpus.", "labels": [], "entities": [{"text": "Relation", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9573578834533691}, {"text": "WSJ corpus", "start_pos": 251, "end_pos": 261, "type": "DATASET", "confidence": 0.9545430541038513}]}, {"text": " Table 5: Coverage, Precision, Recall, and F-score for various relations; R T is the inverse of relation R.  PONG uses POS N-grams, EPP uses distributional similarity, and DEP uses dependency parses.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9310574531555176}, {"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9977625608444214}, {"text": "Recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9939804077148438}, {"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9975786805152893}]}]}