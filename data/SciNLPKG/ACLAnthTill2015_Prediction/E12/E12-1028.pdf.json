{"title": [{"text": "Automatic generation of short informative sentiment summaries", "labels": [], "entities": [{"text": "generation of short informative sentiment summaries", "start_pos": 10, "end_pos": 61, "type": "TASK", "confidence": 0.6752698620160421}]}], "abstractContent": [{"text": "In this paper, we define anew type of summary for sentiment analysis: a single-sentence summary that consists of a supporting sentence that conveys the overall sentiment of a review as well as a convincing reason for this sentiment.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9599685072898865}]}, {"text": "We present a system for extracting supporting sentences from online product reviews, based on a simple and unsupervised method.", "labels": [], "entities": [{"text": "extracting supporting sentences from online product reviews", "start_pos": 24, "end_pos": 83, "type": "TASK", "confidence": 0.8502996223313468}]}, {"text": "We design a novel comparative evaluation method for summarization, using a crowdsourcing service.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9920921921730042}]}, {"text": "The evaluation shows that our sentence extraction method performs better than a baseline of taking the sentence with the strongest sentiment.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7286820858716965}]}], "introductionContent": [{"text": "Given the success of work on sentiment analysis in NLP, increasing attention is being focused on how to present the results of sentiment analysis to the user.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9448179006576538}, {"text": "sentiment analysis", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8835951387882233}]}, {"text": "In this paper, we address an important use case that has so far been neglected: quick scanning of short summaries of a body of reviews with the purpose of finding a subset of reviews that can be studied in more detail.", "labels": [], "entities": [{"text": "quick scanning of short summaries of a body of reviews", "start_pos": 80, "end_pos": 134, "type": "TASK", "confidence": 0.6711105048656464}]}, {"text": "This use case occurs in companies that want to quickly assess, perhaps on a daily basis, what consumers think about a particular product.", "labels": [], "entities": []}, {"text": "One-sentence summaries can be quickly scanned -similar to the summaries that search engines give for search results -and the reviews that contain interesting and new information can then be easily identified.", "labels": [], "entities": []}, {"text": "Consumers who want to quickly scan review summaries to pick out a few reviews that are helpful fora purchasing decision area similar use case.", "labels": [], "entities": []}, {"text": "For a one-sentence summary to be useful in this context, it must satisfy two different \"information needs\": it must convey the sentiment of the review, but it must also provide a specific reason for that sentiment, so that the user can make an informed decision as to whether reading the entire review is likely to be worth the user's timeagain similar to the purpose of the summary of a web page in search engine results.", "labels": [], "entities": []}, {"text": "We calla sentence that satisfies these two criteria a supporting sentence.", "labels": [], "entities": []}, {"text": "A supporting sentence contains information on the sentiment as well as a specific reason for why the author arrived at this sentiment.", "labels": [], "entities": []}, {"text": "Examples for supporting sentences are \"The picture quality is very good\" or \"The battery life is 2 hours\".", "labels": [], "entities": []}, {"text": "Non-supporting sentences contain opinions without such reasons such as \"I like the camera\" or \"This camera is not worth the money\".", "labels": [], "entities": []}, {"text": "To address use cases of sentiment analysis that involve quick scanning and selective reading of large numbers of reviews, we present a simple unsupervised system in this paper that extracts one supporting sentence per document and show that it is superior to a baseline of selecting the sentence with the strongest sentiment.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9272844791412354}]}, {"text": "One problem we faced in our experiments was that standard evaluations of summarization would have been expensive to conduct for this study.", "labels": [], "entities": [{"text": "summarization", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9784860610961914}]}, {"text": "We therefore used crowdsourcing to perform anew type of comparative evaluation method that is different from training set and gold standard creation, the dominant way crowdsourcing has been used in NLP so far.", "labels": [], "entities": []}, {"text": "In summary, our contributions in this paper are as follows.", "labels": [], "entities": []}, {"text": "We define supporting sentences, anew type of sentiment summary that is appropriate in situations where both the sentiment of a review and a good reason for that sentiment need to be conveyed succinctly.", "labels": [], "entities": []}, {"text": "We present a simple unsupervised method for extracting supporting sentences and show that it is superior to a baseline in a novel crowdsourcing-based evaluation.", "labels": [], "entities": []}, {"text": "In the next section, we describe related work that is relevant to our new approach.", "labels": [], "entities": []}, {"text": "In Section 3 we present the approach we use to identify supporting sentences.", "labels": [], "entities": []}, {"text": "Section 4 describes the feature representation of sentences and the classification method.", "labels": [], "entities": []}, {"text": "In Section 5 we give an overview of the crowdsourcing evaluation.", "labels": [], "entities": []}, {"text": "Section 6 discusses our experimental results.", "labels": [], "entities": []}, {"text": "In Sections 7 and 8, we present our conclusions and plans for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "One standard way to evaluate summarization systems is to create hand-edited summaries and to compute some measure of similarity (e.g., word or n-gram overlap) between automatic and human summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.9792531728744507}]}, {"text": "An alternative for extractive summaries is to classify all sentences in the document with respect to their appropriateness as summary sentences.", "labels": [], "entities": [{"text": "extractive summaries", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.6852391958236694}]}, {"text": "An automatic summary can then be scored based on its ability to correctly identify good summary sentences.", "labels": [], "entities": []}, {"text": "Both of these methods require a large annotation effort and are most likely too complex to be outsourced to a crowdsourcing service because the creation of manual summaries requires skilled writers.", "labels": [], "entities": []}, {"text": "For the second type of evaluation, ranking sentences according to a criterion is a lot more time consuming than making a binary decision -so ranking the 13 or 14 sentences that a review contains on average for the entire test set would be a significant annotation effort.", "labels": [], "entities": []}, {"text": "It would also be difficult to obtain consistent and repeatable annotation in crowdsourcing on this task due to its subtlety.", "labels": [], "entities": []}, {"text": "We therefore designed a novel evaluation methodology in this paper that has a much smaller startup cost.", "labels": [], "entities": []}, {"text": "It is well known that relative judgments are easier to make on difficult tasks than absolute judgments.", "labels": [], "entities": []}, {"text": "For example, much recent work on relevance ranking in information retrieval relies on relative relevance judgments (one document is more relevant than another) rather than absolute relevance judgments.", "labels": [], "entities": [{"text": "relevance ranking in information retrieval", "start_pos": 33, "end_pos": 75, "type": "TASK", "confidence": 0.6617141306400299}]}, {"text": "We adopt this general idea and only request such relative judgments on supporting sentences from annotators.", "labels": [], "entities": []}, {"text": "Unlike a complete ranking of the sentences (which would require m(m \u2212 1)/2 judgments where m is the length of the review), we choose a setup where we need to only elicit a single relative judgment per review, one relative judgment on a sentence pair (consisting of the baseline sentence and the system sentence) for each of the 1380 reviews selected in the previous section.", "labels": [], "entities": []}, {"text": "This is a manageable annotation task that can be run on a crowdsourcing service in a short time and at little cost.", "labels": [], "entities": []}, {"text": "We use Amazon Mechanical Turk (AMT) for this annotation task.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.8718754053115845}]}, {"text": "The main advantage of AMT is that cost per annotation task is very low, so that we can obtain large annotated datasets for an af-", "labels": [], "entities": [{"text": "AMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9558548331260681}]}], "tableCaptions": [{"text": " Table 1: Key statistics of our dataset", "labels": [], "entities": []}, {"text": " Table 3: AMT evaluation results. Numbers are percentages or counts. BL = baseline, SY = system, N-D = no  decision, B=S = same sentence selected by baseline and system", "labels": [], "entities": [{"text": "AMT evaluation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8454276621341705}, {"text": "BL", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9799847602844238}]}]}