{"title": [{"text": "Lexical surprisal as a general predictor of reading time", "labels": [], "entities": [{"text": "Lexical surprisal", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8123363256454468}]}], "abstractContent": [{"text": "Probabilistic accounts of language processing can be psychologically tested by comparing word-reading times (RT) to the conditional word probabilities estimated by language models.", "labels": [], "entities": [{"text": "word-reading times (RT)", "start_pos": 89, "end_pos": 112, "type": "METRIC", "confidence": 0.7099486887454987}]}, {"text": "Using surprisal as a linking function, a significant correlation between unlexicalized surprisal and RT has been reported (e.g., Demberg and Keller, 2008), but success using lexicalized models has been limited.", "labels": [], "entities": [{"text": "RT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.56279057264328}]}, {"text": "In this study, phrase structure grammars and recurrent neural networks estimated both lexicalized and unlex-icalized surprisal for words of independent sentences from narrative sources.", "labels": [], "entities": []}, {"text": "These same sentences were used as stimuli in a self-paced reading experiment to obtain RTs.", "labels": [], "entities": [{"text": "RTs", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.868425190448761}]}, {"text": "The results show that lexicalized sur-prisal according to both models is a significant predictor of RT, outperforming its un-lexicalized counterparts.", "labels": [], "entities": [{"text": "RT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9382957816123962}]}], "introductionContent": [{"text": "Context-sensitive, prediction-based processing has been proposed as a fundamental mechanism of cognition: Faced with the problem of responding in real-time to complex stimuli, the human brain would use basic information from the environment, in conjunction with previous experience, in order to extract meaning and anticipate the immediate future.", "labels": [], "entities": [{"text": "prediction-based processing", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.8286143839359283}]}, {"text": "Such a cognitive style is a well-established finding in low level sensory processing (e.g.,), but has also been proposed as a relevant mechanism in higher order processes, such as language.", "labels": [], "entities": []}, {"text": "Indeed, there is ample evidence to show that human language comprehension is both incremental and predictive.", "labels": [], "entities": []}, {"text": "For example, on-line detection of semantic or syntactic anomalies can be observed in the brain's EEG signal) and eye gaze is directed in anticipation at depictions of plausible sentence completions (.", "labels": [], "entities": [{"text": "on-line detection of semantic or syntactic anomalies", "start_pos": 13, "end_pos": 65, "type": "TASK", "confidence": 0.770621793610709}]}, {"text": "Moreover, probabilistic accounts of language processing have identified unpredictability as a major cause of processing difficulty in language comprehension.", "labels": [], "entities": [{"text": "language processing", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6999204009771347}]}, {"text": "In such incremental processing, parsing would entail a pre-allocation of resources to expected interpretations, so that effort would be related to the suitability of such an allocation to the actually encountered stimulus.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9667595028877258}]}, {"text": "Possible sentence interpretations can be constrained by both linguistic and extra-linguistic context, but while the latter is difficult to evaluate, the former can be easily modeled: The predictability of a word for the human parser can be expressed as the conditional probability of a word given the sentence so far, which can in turn be estimated by language models trained on text corpora.", "labels": [], "entities": []}, {"text": "These probabilistic accounts of language processing difficulty can then be validated against empirical data, by taking reading time (RT) on a word as a measure of the effort involved in its processing.", "labels": [], "entities": [{"text": "reading time (RT)", "start_pos": 119, "end_pos": 136, "type": "METRIC", "confidence": 0.8572514593601227}]}, {"text": "Recently, several studies have followed this approach, using \"surprisal\" (see Section 1.1) as the linking function between effort and predictability.", "labels": [], "entities": []}, {"text": "These can be computed for each word in a text, or alternatively for the words' parts of speech (POS).", "labels": [], "entities": []}, {"text": "In the latter case, the obtained estimates can give an indication of the importance of syntactic structure in developing upcoming-word expectations, but ignore the rich lexical information that is doubtlessly employed by the human parser to constrain predictions.", "labels": [], "entities": []}, {"text": "However, whereas such an unlexicalized (i.e., POS-based) surprisal has been shown to significantly predict RTs, success with lexical (i.e., word-based) surprisal has been limited.", "labels": [], "entities": [{"text": "RTs", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9790721535682678}]}, {"text": "This can be attributed to data sparsity (larger training corpora might be needed to provide accurate lexical surprisal than for the unlexicalized counterpart), or to the noise introduced by participant's world knowledge, inaccessible to the models.", "labels": [], "entities": []}, {"text": "The present study thus sets out to find such a lexical surprisal effect, trying to overcome possible limitations of previous research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three hundred and sixty-one sentences, all comprehensible out of context and containing only words included in the subset of the BNC used to train the models, were randomly selected from three freely accessible on-line novels 2 (for additional details, see Frank, 2012).", "labels": [], "entities": [{"text": "BNC", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.9332612752914429}]}, {"text": "The fictional narrative provides a good contrast to the pre-viously examined newspaper editorials from the Dundee corpus, since participants did not need prior knowledge regarding the details of the stories, and a less specialised language and style were employed.", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9647533893585205}]}, {"text": "In addition, the randomly selected sentences did not makeup coherent texts (in contrast,, employed short stories), so that they were independent from each other, both for the models and the readers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model comparison between best performing  word-based PSG and RNN.", "labels": [], "entities": []}, {"text": " Table 2: Word-vs. POS-based models: comparisons  between best models overall, and best models within  each category.", "labels": [], "entities": []}]}