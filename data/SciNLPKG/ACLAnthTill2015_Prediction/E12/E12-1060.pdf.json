{"title": [{"text": "Word Sense Induction for Novel Sense Detection", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5920215944449106}, {"text": "Novel Sense Detection", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.759442667166392}]}], "abstractContent": [{"text": "We apply topic modelling to automatically induce word senses of a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses.", "labels": [], "entities": []}, {"text": "We start by exploring the utility of standard topic models for word sense induction (WSI), with a predetermined number of topics (=senses).", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7995993147293726}]}, {"text": "We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the WSI task.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 139, "end_pos": 147, "type": "TASK", "confidence": 0.8826444149017334}]}, {"text": "We goon to establish state-of-the-art results over two WSI datasets, and apply the proposed model to a novel sense detection task.", "labels": [], "entities": [{"text": "WSI datasets", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8503832817077637}, {"text": "sense detection task", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.8284412423769633}]}], "introductionContent": [{"text": "Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8084533413251241}]}, {"text": "It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.7865526427825292}]}, {"text": "While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by and in an information retrieval context.", "labels": [], "entities": []}, {"text": "A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses overtime.", "labels": [], "entities": [{"text": "WSI", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8581478595733643}, {"text": "novel sense detection", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.6405241290728251}]}, {"text": "One of the key challenges in WSI is learning the appropriate sense granularity fora given word, i.e. the number of senses that best captures the token occurrences of that word.", "labels": [], "entities": [{"text": "WSI", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9347856044769287}]}, {"text": "Building on the work of and others, we approach WSI via topic modelling -using Latent Dirichlet Allocation (LDA:) and derivative approaches -and use the topic model to determine the appropriate sense granularity.", "labels": [], "entities": [{"text": "WSI", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9724547863006592}, {"text": "Latent Dirichlet Allocation (LDA:)", "start_pos": 79, "end_pos": 113, "type": "METRIC", "confidence": 0.909490684668223}]}, {"text": "Topic modelling is an unsupervised approach to jointly learn topics -in the form of multinomial probability distributions over words -and per-document topic assignments -in the form of multinomial probability distributions over topics.", "labels": [], "entities": [{"text": "Topic modelling", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8208653032779694}]}, {"text": "LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words.", "labels": [], "entities": []}, {"text": "LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP:,).", "labels": [], "entities": [{"text": "sense granularity determination", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6529509524504343}]}, {"text": "Our contributions in this paper are as follows.", "labels": [], "entities": []}, {"text": "We first establish the effectiveness of HDP for WSI over both the, and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity fora given word.", "labels": [], "entities": [{"text": "WSI", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.936037003993988}]}, {"text": "We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets.", "labels": [], "entities": [{"text": "WSI", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9052364230155945}, {"text": "SemEval datasets", "start_pos": 178, "end_pos": 194, "type": "DATASET", "confidence": 0.7351559400558472}]}, {"text": "Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results.", "labels": [], "entities": [{"text": "sense detection task", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.8844003081321716}]}], "datasetContent": [{"text": "To facilitate comparison of our proposed method for WSI with previous approaches, we use the dataset from the SemEval-2007 and SemEval-2010 word sense induction tasks.", "labels": [], "entities": [{"text": "WSI", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9797800183296204}, {"text": "SemEval-2010 word sense induction tasks", "start_pos": 127, "end_pos": 166, "type": "TASK", "confidence": 0.6660161733627319}]}, {"text": "We first experiment with the SemEval-2010 dataset, as it includes explicit training and test data for each target word and utilises a more robust evaluation methodology.", "labels": [], "entities": [{"text": "SemEval-2010 dataset", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.7644213736057281}]}, {"text": "We then return to experiment with the SemEval-2007 dataset, for comparison purposes with other published results for topic modelling approaches to WSI.", "labels": [], "entities": [{"text": "SemEval-2007 dataset", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8933184742927551}, {"text": "WSI", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.5610892176628113}]}, {"text": "Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task ().", "labels": [], "entities": [{"text": "WSI", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9594720005989075}, {"text": "SemEval-2010 WSI shared task", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.6188964247703552}]}, {"text": "The dataset contains 100 target words: 50 nouns and 50 verbs.", "labels": [], "entities": []}, {"text": "For each target word, a fixed set of training and test instances are supplied, typically 1 to 3 sentences in length, each containing the target word.", "labels": [], "entities": []}, {"text": "The default approach to evaluation for the SemEval-2010 WSI task is in the form of WSD over the test data, based on the senses that have been automatically induced from the training data.", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7913375894228617}]}, {"text": "Because the induced senses will likely vary in number and nature between systems, the WSD evaluation has to incorporate a sense alignment step, which it performs by splitting the test instances into two sets: a mapping set and an evaluation set.", "labels": [], "entities": [{"text": "WSD evaluation", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.8877561390399933}]}, {"text": "The optimal mapping from induced senses to gold-standard senses is learned from the mapping set, and the resulting sense alignment is used to map the predictions of the WSI system to pre-defined senses for the evaluation set.", "labels": [], "entities": []}, {"text": "The particular split we use to calculate WSD effectiveness in this paper is 80%/20% (mapping/test), averaged across 5 random splits.", "labels": [], "entities": [{"text": "WSD", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9886220097541809}]}, {"text": "The SemEval-2010 training data consists of approximately 163K training instances for the 100 target words, all taken from the web.", "labels": [], "entities": [{"text": "SemEval-2010 training data", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.6223839322725931}]}, {"text": "The test data is approximately 9K instances taken from a variety of news sources.", "labels": [], "entities": []}, {"text": "Following the standard approach used by the participating systems in the SemEval-2010 task, we induce senses only from the training instances, and use the learned model to assign senses to the test instances.", "labels": [], "entities": []}, {"text": "In our original experiments with LDA, we set the number of topics (T ) for each target word to the number of senses represented in the test data for that word (varying T for each target word).", "labels": [], "entities": []}, {"text": "This is based on the unreasonable assumption that we will have access to gold-standard information on sense granularity for each target word, and is done to establish an upper bound score for LDA.", "labels": [], "entities": []}, {"text": "We then relax the assumption, and use a fixed T setting for each of sets of nouns (T = 7) and verbs (T = 3), based on the average number of senses from the test data in each case.", "labels": [], "entities": []}, {"text": "Finally, we introduce positional context features for LDA, once again using the fixed T values for nouns and verbs.", "labels": [], "entities": []}, {"text": "We next apply HDP to the WSI task, using positional features, but learning the number of senses automatically for each target word via the model.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.9139152467250824}]}, {"text": "Finally, we experiment with adding dependency features to the model.", "labels": [], "entities": []}, {"text": "To summarise, we provide results for the following models: We compare our models with two baselines from the SemEval-2010 task: (1) Baseline Random -randomly assign each test instance to one of four senses; (2) Baseline MFS -most frequent sense baseline, assigning all test instances to one sense; and also a benchmark system (UoY), in the form of the University of York system (, which achieved the best overall WSD results in the original SemEval-2010 task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WSD F-score over the SemEval-2010 dataset", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7096242904663086}, {"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.5236423015594482}, {"text": "SemEval-2010 dataset", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.8383090198040009}]}, {"text": " Table 2. These senses are learnt  using HDP with both positional word features  (e.g. husband #-1, indicating the lemma husband  to the immediate left of the target word) and de- pendency features (e.g. cheat#prep on#wife). The  first observation to make is that senses 7, 8 and  9 are \"junk\" senses, in that the top-10 terms do not convey a coherent sense. These topics are an  artifact of HDP: they are learnt at a much later  stage of the iterative process of Gibbs sampling  and are often smaller than other topics (i.e. have  more zero-probability terms). We notice that they  are assigned as topics to instances very rarely (al- though they are certainly used to assign topics to  non-target words in the instances), and as such,  they do not present a real issue when assigning  the sense to an instance, as they are likely to be  overshadowed by the dominant senses. 7 This con- clusion is born out when we experimented with  manually filtering out these topics when assign- ing instance to senses: there was no perceptible  change in the results, reinforcing our suggestion  that these topics do not impact on target word  sense assignment.", "labels": [], "entities": [{"text": "target word  sense assignment", "start_pos": 1120, "end_pos": 1149, "type": "TASK", "confidence": 0.6748785078525543}]}, {"text": " Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional  word and dependency features)", "labels": [], "entities": []}, {"text": " Table 4: Novelty score (\"Nov\"), ratio of frequency in  the ukWaC sample and BNC, and frequency of the  novel sense in the manually-annotated 100 instances  from the ukWaC sample (where applicable), for all  lemmas in our dataset. Lemmas shown in boldface  have a novel sense in the ukWaC sample compared to  the BNC.", "labels": [], "entities": [{"text": "ukWaC sample", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9701563119888306}, {"text": "BNC", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.6129387021064758}, {"text": "ukWaC sample", "start_pos": 166, "end_pos": 178, "type": "DATASET", "confidence": 0.9751739501953125}, {"text": "ukWaC sample", "start_pos": 283, "end_pos": 295, "type": "DATASET", "confidence": 0.9760858118534088}, {"text": "BNC", "start_pos": 313, "end_pos": 316, "type": "DATASET", "confidence": 0.9259123802185059}]}, {"text": " Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologies  of: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.", "labels": [], "entities": []}]}