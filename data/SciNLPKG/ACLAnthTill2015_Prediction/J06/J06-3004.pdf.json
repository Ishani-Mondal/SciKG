{"title": [{"text": "Characterizing and Predicting Corrections in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Characterizing and Predicting Corrections in Spoken Dialogue", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7224974632263184}]}], "abstractContent": [{"text": "This article focuses on the analysis and prediction of corrections, defined as turns where a user tries to correct a prior error made by a spoken dialogue system.", "labels": [], "entities": [{"text": "prediction of corrections", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.8286086718241373}]}, {"text": "We describe our labeling procedure of various corrections types and statistical analyses of their features in a corpus collected from a train information spoken dialogue system.", "labels": [], "entities": []}, {"text": "We then present results of machine-learning experiments designed to identify user corrections of speech recognition errors.", "labels": [], "entities": [{"text": "user corrections of speech recognition errors", "start_pos": 77, "end_pos": 122, "type": "TASK", "confidence": 0.6827589025100073}]}, {"text": "We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history.", "labels": [], "entities": [{"text": "speech recognition process", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.8202181657155355}]}, {"text": "Our best-performing features reduce classification error from baselines of 25.70-28.99% to 15.72%.", "labels": [], "entities": [{"text": "classification error", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.5321666598320007}]}], "introductionContent": [{"text": "Compared to many other systems, spoken dialogue systems (SDS) tend to have more difficulties in correctly interpreting user input.", "labels": [], "entities": [{"text": "spoken dialogue systems (SDS)", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.7125477194786072}]}, {"text": "Whereas a car normally goes left if the driver turns the steering wheel in that direction or a vacuum cleaner starts working if one pushes the on button, interactions between a user and a spoken dialogue system are often hampered by mismatches between the action intended by the user and the action executed by the system.", "labels": [], "entities": []}, {"text": "Such mismatches are mainly due to errors in the Automatic Speech Recognition (ASR) and/or the Natural Language Understanding (NLU) component of these systems; they can also be due to wrong default assumptions of the system or the fact that a user asks out-of-topic questions for which the system was not designed.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.7580335885286331}]}, {"text": "To solve these mismatches, users often have to put considerable effort into trying to make it clear to the system that there was a problem, and trying to correct it by reentering misrecognized or misinterpreted information.", "labels": [], "entities": []}, {"text": "Previous research has already brought to light that it is not always easy for users to determine whether their intended actions were carried out correctly or not, in particular when the dialogue system does not give appropriate feedback about its internal representation at the right moment.", "labels": [], "entities": []}, {"text": "In addition, users' corrections may miss their goal because corrections themselves are more difficult for the system to recognize and interpret correctly, which may lead to so-called cyclic (or spiral) errors.", "labels": [], "entities": []}, {"text": "Given that current spoken dialogue systems are not sufficiently robust, there is need for sophisticated error-handling strategies to gracefully solve communication problems between the system and the user.", "labels": [], "entities": []}, {"text": "Ideally, apart from strategies to prevent errors, error handling would consist of steps to immediately detect an error when it occurs and to interact with the user to correct the error in subsequent exchanges.", "labels": [], "entities": [{"text": "error handling", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.726063683629036}]}, {"text": "To date, attempts to improve system performance have largely focused on improving ASR accuracy or simplifying the task, either by further constraining the domain and functionality of the system or by further restricting the vocabulary the system must recognize.", "labels": [], "entities": [{"text": "ASR", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9906649589538574}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9225114583969116}]}, {"text": "Such studies include work on improved acoustic and semantic confidence scores), on new system architectures for error handling (), on new interfaces that are more user-friendly for error recovery (, and on the use of error-recovery strategies that are based on analyses of human-human dialogues, including the use of facial expressions.", "labels": [], "entities": [{"text": "error handling", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.6997634619474411}, {"text": "error recovery", "start_pos": 181, "end_pos": 195, "type": "TASK", "confidence": 0.6839342415332794}]}, {"text": "However, as ASR accuracy improves, dialogue systems will be called upon to handle evermore complex tasks and ever less restricted vocabularies.", "labels": [], "entities": [{"text": "ASR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.988585889339447}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9648499488830566}]}, {"text": "So, it seems likely that spoken dialogue systems will, for the foreseeable future, always require effective error detection and repair strategies.", "labels": [], "entities": [{"text": "error detection and repair", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.7143774554133415}]}, {"text": "In previous research), we identified new procedures to detect recognition errors, which perform well when tested on two different corpora, the TOOT and W99 corpora (train information and conference registration dialogues) collected using two different ASR systems ().", "labels": [], "entities": [{"text": "TOOT", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.8875083923339844}, {"text": "W99 corpora (train information and conference registration dialogues) collected", "start_pos": 152, "end_pos": 231, "type": "DATASET", "confidence": 0.8484339930794456}]}, {"text": "We found that prosodic features, in combination with information already available to the recognizer, such as acoustic confidence scores, grammar, and recognized string, can distinguish speaker turns that are misrecognized far better than traditional methods for ASR rejection (the system decision that its hypothesis is so weak that it should reprompt for fresh input), which use acoustic confidence scores alone.", "labels": [], "entities": [{"text": "ASR rejection", "start_pos": 263, "end_pos": 276, "type": "TASK", "confidence": 0.991993248462677}]}, {"text": "Related work has been done by and.", "labels": [], "entities": []}, {"text": "In the current study, we turn to the question of how people try to correct ASR errors in their interactions with machines and the role that prosody may play in identifying user corrections and in helping to analyze them.", "labels": [], "entities": [{"text": "ASR", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9481752514839172}]}, {"text": "Understanding how users attempt to correct system failures and why their attempts succeed or fail is important to improve the design of future spoken dialogue systems.", "labels": [], "entities": []}, {"text": "For example, knowing whether they are more likely to repeat or rephrase their utterances, add new information or shorten their input, and how system behavior influences these choices can suggest appropriate on-line modifications to the system's interaction strategy or to the recognition procedure it employs.", "labels": [], "entities": []}, {"text": "Determining which speaker behaviors are more successful in correcting system errors can also lead to improvements in the help information such systems provide.", "labels": [], "entities": [{"text": "correcting system errors", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8651678959528605}]}, {"text": "There is growing evidence that there is much variance in the way people react to system errors and that the variance can be explained on the basis of particular properties of the dialogue system or the dialogue context.", "labels": [], "entities": []}, {"text": "In particular, dialogue confirmation strategies may hinder users' ability to correct system error.", "labels": [], "entities": [{"text": "dialogue confirmation", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8261120915412903}]}, {"text": "For instance, if a system wrongly presents information as being correct, as when it verifies information implicitly, users become confused about how to respond.", "labels": [], "entities": []}, {"text": "Other studies have shown that speakers tend to switch to a prosodically \"marked\" speaking style after communication errors, comparing repetition corrections with the speech being repeated.", "labels": [], "entities": [{"text": "repetition corrections", "start_pos": 134, "end_pos": 156, "type": "METRIC", "confidence": 0.9541534185409546}]}, {"text": "Although this speaking style maybe effective in problematic human-human communicative settings, there is evidence that suggests it leads to further errors in human-machine interactions.", "labels": [], "entities": []}, {"text": "That corrections are difficult for ASR systems is generally explained by the fact that they tend to be hyperarticulated-higher, louder, longer-than other turns, where ASR models are not well adapted to handle this special speaking style, although recent studies suggest that ASR systems are becoming less vulnerable to hyperarticulation ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9893854260444641}]}, {"text": "So, repair strategies in human-machine interactions can be more or less effective.", "labels": [], "entities": [{"text": "repair", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9582821130752563}]}, {"text": "Therefore, increased knowledge about the efficiency of different correction strategies can lead to a number of possible courses of action.", "labels": [], "entities": []}, {"text": "System strategy might be chosen to favor the type(s) of correction the system can most easily process.", "labels": [], "entities": []}, {"text": "Or, having chosen a particular interaction strategy, the system repair strategy might be tuned to handle the correction types that that strategy is likely to produce.", "labels": [], "entities": [{"text": "system repair", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.7057657241821289}]}, {"text": "Alternatively, the system's dialogue manager might use the detection of corrections as a signal that it should modify its interaction strategy, either locally, by beginning a subdialogue for faster error recovery, or globally, by changing its initiative or confirmation strategies, or even directing the user to a human operator.", "labels": [], "entities": [{"text": "error recovery", "start_pos": 198, "end_pos": 212, "type": "TASK", "confidence": 0.7243980765342712}]}, {"text": "Or, since corrections are often hyperarticulated, detection of a correction could serve as a signal to the ASR engine to run a recognizer trained on hyperarticulated speech in parallel with its normal processor, to better transcribe the speech.", "labels": [], "entities": [{"text": "ASR", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9246236085891724}]}, {"text": "All of these possibilities, however, assume that user corrections can be detected by the system reliably during the dialogue.", "labels": [], "entities": []}, {"text": "In this article, we describe an analysis of user corrections of system error collected in the TOOT spoken dialogue system.", "labels": [], "entities": [{"text": "TOOT spoken dialogue system", "start_pos": 94, "end_pos": 121, "type": "DATASET", "confidence": 0.7469047904014587}]}, {"text": "In the next section, we describe the corpus itself and how it was collected and labeled.", "labels": [], "entities": []}, {"text": "The corpus is suitable to gain insight into the different correction strategies that speakers exploit in different dialogue contexts and interaction styles.", "labels": [], "entities": []}, {"text": "Then, we characterize the nature of corrections in this corpus in terms of when they occur, how well they are handled by the system, what distinguishes their prosody from other utterances, their relationship to the utterances they correct, and how they differ according to dialogue strategy.", "labels": [], "entities": []}, {"text": "Then we present results of some machine-learning experiments designed to automatically distinguish corrections from other user input, using features that we derived as potentially useful from our descriptive analyses.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our descriptive study showed that differences in dialogue strategy affect the type and success of user corrections.", "labels": [], "entities": []}, {"text": "For example, TOOT users more frequently repeat their misrecognized turns and produce the fewest corrections per task when TOOT has the initiative and explicitly confirms all user input.", "labels": [], "entities": [{"text": "corrections", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.9088248610496521}, {"text": "TOOT", "start_pos": 122, "end_pos": 126, "type": "DATASET", "confidence": 0.9041157960891724}]}, {"text": "So, we hypothesized that system conditions might prove important in our learning experiments.", "labels": [], "entities": []}, {"text": "We thus include features representing the system's current initiative and confirmation strategies (inittype, conftype), whether users could adapt the system's dialogue strategies (adapt), and the combined initiative and confirmation setting (realstrat).", "labels": [], "entities": []}, {"text": "In this section we investigate whether the features described in Section 4.1 (or interesting subsets of them) can in fact be used to accurately predict whether a turn will be a correction or not.", "labels": [], "entities": []}, {"text": "We describe experiments using the machine-learning program RIPPER) to automatically induce such prediction models.", "labels": [], "entities": []}, {"text": "RIPPER takes as input the classes to be learned, the names and possible values of a set of features, and training data specifying the class and feature values for each training example.", "labels": [], "entities": []}, {"text": "For our experiments, the features presented in comprise the independent variables for our learning experiments.", "labels": [], "entities": []}, {"text": "The dependent variable to be learned, correction (T) versus non-correction (F), corresponds to the hand-labeled observations described above.", "labels": [], "entities": [{"text": "correction (T)", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.9467530697584152}]}, {"text": "Given a vector of values for the independent and dependent variables for each speaker turn, RIPPER outputs a classification model for classifying future examples.", "labels": [], "entities": []}, {"text": "The model is learned using greedy search guided by an information gain metric and is expressed as an ordered set of if-then rules.", "labels": [], "entities": []}, {"text": "When multiple rules are applicable, RIPPER uses the first rule it finds.", "labels": [], "entities": []}, {"text": "When no rules are applicable, RIPPER classifies the turn as a non-correction (F) by default.", "labels": [], "entities": [{"text": "RIPPER", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.6501792669296265}, {"text": "non-correction (F)", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.691044270992279}]}, {"text": "shows the performance of the learned classification models for some of the feature sets we examined; all performance figures are estimated using 25-fold cross-validation on the 2,328 turns in our corpus.", "labels": [], "entities": []}, {"text": "The Features column identifies the set of features (as defined in used to learn the model.", "labels": [], "entities": []}, {"text": "The second column, DIA, indicates which type of dialogue history features (PreTurn, PrepreTurn, Prior, and/or PMean) were also included in the feature set; these features represent the same types of information (e.g., f0max) that the Features column denotes, but for one or more previous turns in the dialogue.", "labels": [], "entities": [{"text": "PrepreTurn", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.8336846232414246}]}, {"text": "The third column shows the mean error and standard error (SE) predicted for the model specified by the first two columns.", "labels": [], "entities": [{"text": "mean error", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9619439244270325}, {"text": "standard error (SE)", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.9020863771438599}]}, {"text": "When error estimates in different rows differ by plus or minus twice the standard error, they are significantly different.", "labels": [], "entities": []}, {"text": "The remaining columns show the mean recall, precision, and F \u03b2 = 1 for corrections (focus class = T) and non-corrections (focus class = F), respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9746786952018738}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9994263648986816}, {"text": "F \u03b2", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9855541586875916}, {"text": "corrections", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.8983446955680847}]}, {"text": "6 For comparison purposes, we compare our predictions to two potential baselines.", "labels": [], "entities": []}, {"text": "The Majority baseline predicts that all turns are non-corrections (the majority class of F), and has a classification error of 28.99%.", "labels": [], "entities": [{"text": "F", "start_pos": 89, "end_pos": 90, "type": "METRIC", "confidence": 0.947052538394928}, {"text": "classification error", "start_pos": 103, "end_pos": 123, "type": "METRIC", "confidence": 0.9649145007133484}]}, {"text": "The Prerejbool baseline predicts that all turns following rejected turns (prerejbool = T) are correctionssince after rejections, TOOT asks users to repeat their turn 7 -and all others are noncorrections; this baseline gives a classification error of 25.70%.", "labels": [], "entities": [{"text": "Prerejbool baseline", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7232678234577179}, {"text": "prerejbool = T)", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.7764873802661896}, {"text": "correctionssince", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.9642311930656433}]}, {"text": "The first question addressed in our experiments is whether or not corrections can be predicted significantly better than our baselines.", "labels": [], "entities": []}, {"text": "shows that in fact they can.", "labels": [], "entities": []}, {"text": "Our best-performing feature set (Raw+ASR+SYS+POS, DIA = PreTurn) cuts the majority baseline error almost in half, from 28.99% to 15.72%, and predicts significantly better than the rejection-based baseline as well.", "labels": [], "entities": [{"text": "PreTurn", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.8705020546913147}, {"text": "majority baseline error", "start_pos": 74, "end_pos": 97, "type": "METRIC", "confidence": 0.5630162954330444}]}, {"text": "This feature set includes raw versions of all our prosodic features and all of the non-prosodic features, for both the turn being classified 6 Recall is the percentage of actual members of a class that are identified, whereas precision is the percentage of predicted class members that are in fact members.", "labels": [], "entities": [{"text": "Recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9829742908477783}, {"text": "precision", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.9995481371879578}]}, {"text": "The definition of F \u03b2 is (\u03b2 2 +1)PrecisionRecall \u03b2 2 Precicison+Recall ; \u03b2 = 1 equally weights precision and recall.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9771188497543335}, {"text": "PrecisionRecall \u03b2 2 Precicison+", "start_pos": 33, "end_pos": 64, "type": "METRIC", "confidence": 0.8610923528671265}, {"text": "Recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.5265448093414307}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9996023774147034}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9979230761528015}]}, {"text": "These values are computed using our own cross-validation program, while error is computed using RIPPER's cross-validation option.", "labels": [], "entities": [{"text": "error", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9560836553573608}]}, {"text": "7 Although users are asked to repeat their turn, 29% of the turns after rejections are not in fact corrections (e.g., the user instead asks for help or asks the system to repeat the prompt). and the immediately prior turn.", "labels": [], "entities": []}, {"text": "Note that even if all of the available features are used for learning (i.e., the normalized versions of prosodic features and all of the various history features (PROS+ASR+SYS+POS, DIA = all, error = 16.38%)), performance is statistically comparable to this model.", "labels": [], "entities": [{"text": "DIA", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9395500421524048}, {"text": "error", "start_pos": 192, "end_pos": 197, "type": "METRIC", "confidence": 0.9577463269233704}]}, {"text": "8 In addition, the recall, precision and F \u03b2 = 1 values in show that corrections are generally predicted with better precision than recall whereas the reverse holds for non-corrections, and that non-corrections (the majority class) are easier to accurately predict than corrections.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9996641874313354}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9995818734169006}, {"text": "F \u03b2 = 1", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9711584895849228}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9890086054801941}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9874304533004761}]}, {"text": "We next turn to an examination of the contribution of the different types of features we used for prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 98, "end_pos": 108, "type": "TASK", "confidence": 0.9694955945014954}]}, {"text": "First, we consider the utility of our non-prosodic features.", "labels": [], "entities": []}, {"text": "shows that, using only non-prosodic features (ASR, SYS, POS), corrections can still be predicted with an accuracy statistically equivalent to our best results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9990411400794983}]}, {"text": "That is, using all feature types (PROS+ASR+SYS+POS, DIA = all, error = 16.38%) is equivalent to using only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%).", "labels": [], "entities": []}, {"text": "Similarly, restricting our feature set to the ASR-derived subset of our non-prosodic features (ASR, DIA = all, error = 16.41%) or removing all dialogue history (ASR+SYS+POS, DIA = none, error = 18.60%) yields results equivalent to our best-performing classifier.", "labels": [], "entities": []}, {"text": "However, when only those ASR features derived from the acoustic confidence score (i.e., conf, preconf, ppreconf, pmnconf, rejbool, prerejbool, pprerejbool, priorrejboolnum, priorrejboolpct) are used for prediction, then performance does significantly degrade (conf+rejbool, DIA = all, error = 21.23%).", "labels": [], "entities": [{"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9139764308929443}, {"text": "prerejbool", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9379032850265503}, {"text": "DIA", "start_pos": 274, "end_pos": 277, "type": "METRIC", "confidence": 0.992664098739624}, {"text": "error", "start_pos": 285, "end_pos": 290, "type": "METRIC", "confidence": 0.9656422734260559}]}, {"text": "So, it appears that there are numerous ways to classify corrections successfully, using various combinations of feature types.", "labels": [], "entities": [{"text": "classify corrections", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.866364598274231}]}, {"text": "This finding is an important one because it suggests that systems that have access to restricted kinds of information can still hope to identify user corrections with some confidence.", "labels": [], "entities": []}, {"text": "In particular, simply using information available to current ASR systems, such as acoustic confidence score, recognized string, grammar, and features derived from these, produces classification results equivalent to our best-performing classifier.", "labels": [], "entities": [{"text": "ASR", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9703612327575684}, {"text": "acoustic confidence score", "start_pos": 82, "end_pos": 107, "type": "METRIC", "confidence": 0.6405369341373444}]}, {"text": "A caveat here is that some of the features in this ASR feature set (e.g., grammar and recognized string) are less likely to generalize from task to task.", "labels": [], "entities": [{"text": "ASR", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9762824177742004}]}, {"text": "Turning now to the role of prosodic features in classifying corrections, shows that use of only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%) 9 slightly (but not quite significantly) outperforms use of only raw prosodic features (Raw, DIA = all, error = 19.68%).", "labels": [], "entities": [{"text": "classifying corrections", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.9169177412986755}, {"text": "ASR+SYS+POS", "start_pos": 119, "end_pos": 130, "type": "METRIC", "confidence": 0.40326804518699644}]}, {"text": "However, using raw prosodic features alone (error = 19.68%) is comparable to using only ASR features alone (ASR, DIA = all, error = 16.41%).", "labels": [], "entities": []}, {"text": "And both significantly outperform the majority class and rejection-based baselines.", "labels": [], "entities": []}, {"text": "Note also that prediction from raw prosodic features alone (19.68%) is not improved by the inclusion of their normalized versions (PROS, DIA = all, error = 20.33%).", "labels": [], "entities": [{"text": "prediction", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.7756524682044983}, {"text": "PROS", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.8336346745491028}, {"text": "DIA", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9401872158050537}]}, {"text": "Thus, ASRderived features and prosodic features seem to provide equally successful classifications of user corrections.", "labels": [], "entities": []}, {"text": "Since ASR-derived features, in particular, acoustic confidence score, are currently used by spoken dialogue systems to determine when to reject a turn, our results suggest that such features can also be useful for identifying corrections.", "labels": [], "entities": [{"text": "acoustic confidence score", "start_pos": 43, "end_pos": 68, "type": "METRIC", "confidence": 0.838775118192037}, {"text": "identifying corrections", "start_pos": 214, "end_pos": 237, "type": "TASK", "confidence": 0.8820532560348511}]}, {"text": "Although prosodic features are rarely made use of in spoken dialogue systems, they would, in fact, seem more likely to generalize across tasks and recognizers than the ASR features.", "labels": [], "entities": []}, {"text": "Now we turn to the issue of how useful features of the dialogue history are in classifying corrections.", "labels": [], "entities": [{"text": "classifying corrections", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.8845434784889221}]}, {"text": "Recall that our best-performing ruleset used only a limited dia- logue history-features from the preceding turn (Raw+ASR+SYS+POS, DIA = PreTurn, error = 15.72%).", "labels": [], "entities": [{"text": "PreTurn", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.7945259809494019}, {"text": "error", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.951484739780426}]}, {"text": "While adding features of the turn two turns back (PrepreTurn ) and of the dialogue as a whole (Prior and PMean ) does not significantly change the error (Raw+ASR+SYS+POS, DIA = all, error = 16.16%), removing the features of the immediately previous turn from the dialogue history does in fact cause a significant increase in error rate (Raw+ASR+SYS+POS, DIA = none, error = 18.68%).", "labels": [], "entities": [{"text": "PrepreTurn", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.7673830389976501}, {"text": "error rate", "start_pos": 325, "end_pos": 335, "type": "METRIC", "confidence": 0.9813801646232605}]}, {"text": "However, as discussed above, when only non-prosodic features are considered (ASR+SYS+POS), there is no significant difference between DIA = all and DIA = none.", "labels": [], "entities": [{"text": "DIA", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9942019581794739}, {"text": "DIA", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9764382243156433}]}, {"text": "So, it seems that features of the immediate local context can improve our ability to classify corrections accurately when prosodic features are included, but adding a larger local context window and a global context does not improve over these results.", "labels": [], "entities": []}, {"text": "Contextual features seem particularly important to performance when only raw prosodic features are considered (Raw, DIA = all, error = 19.68%).", "labels": [], "entities": [{"text": "DIA", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9730185270309448}, {"text": "error", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9422862529754639}]}, {"text": "When the raw prosodic features of the dialogue history are removed, the error rate dramatically increases (Raw, DIA = none, error = 25.35%).", "labels": [], "entities": [{"text": "error rate", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9935562610626221}, {"text": "Raw", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9261665344238281}, {"text": "DIA", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9821558594703674}, {"text": "error", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9908466935157776}]}, {"text": "However, if the normalized prosodic features (which themselves encode much of the historical information) are also included, then removing the DIA versions of these features does not significantly degrade performance (PROS, DIA = all, error = 20.33% vs. PROS, DIA = none, error = 20.53%).", "labels": [], "entities": [{"text": "PROS", "start_pos": 254, "end_pos": 258, "type": "DATASET", "confidence": 0.7781829237937927}, {"text": "DIA", "start_pos": 260, "end_pos": 263, "type": "METRIC", "confidence": 0.9328257441520691}]}, {"text": "We might explain the larger role that prosodic context plays in classification by returning to the differences we found between prosodic features of corrections and non-corrections, described in Section 3.", "labels": [], "entities": []}, {"text": "In our descriptive analyses we found that prosodic features such as pitch, duration, and loudness reliably distinguish corrections based on relative differences between the two types of turns, not absolute differences.", "labels": [], "entities": [{"text": "pitch", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9879412055015564}, {"text": "duration", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9611864686012268}]}, {"text": "In prediction also, it seems that some form of normalization by context improves the performance of prosodic features.", "labels": [], "entities": [{"text": "prediction", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.9640161395072937}]}, {"text": "When we examine which class of features performs best in the absence of contextual information, we see that the prosodic features (PROS, DIA = none, error = 20.53%) significantly outperform the ASR-derived features (ASR, DIA = none, error = 24.19%), which in turn significantly outperform either of the remaining feature types (POS and SYS).", "labels": [], "entities": []}, {"text": "also shows the cases in which the addition of new sources of knowledge improves prediction performance.", "labels": [], "entities": []}, {"text": "For DIA = none, the statistically significant improvements involve adding the feature diadist (distance of the current turn from the beginning of the dialogue): For example, ASR+POS (error = 20.4%) outperforms both ASR (error = 24.19%) and POS (error = 29%), and ASR+SYS+POS (error = 18.6%) outperforms ASR+SYS (error = 23.46%).", "labels": [], "entities": []}, {"text": "Again, these are features that are easily made available to current spoken dialogue systems.", "labels": [], "entities": []}, {"text": "The classification model learned from the best-performing feature set in is shown in.", "labels": [], "entities": []}, {"text": "Rules are presented in order of importance in classifying data.", "labels": [], "entities": []}, {"text": "The first rule RIPPER finds with this feature set specifies that if the duration of the current turn is \u2265 3.89046 seconds, and if the acoustic confidence score of the prior turn is \u2264 \u22120.645234, and if the percentage of silence in the current turn is \u2264 53.9474%, then predict that the turn is a correction; this rule correctly predicts 153 corrections and incorrectly predicts that 10 non-corrections are corrections.", "labels": [], "entities": [{"text": "acoustic confidence score", "start_pos": 134, "end_pos": 159, "type": "METRIC", "confidence": 0.8258065780003866}]}, {"text": "So, this rule applies when the previous turn has a low confidence score and the current turn exhibits some marked prosodic features.", "labels": [], "entities": []}, {"text": "The fourth rule predicts a correction after a previous rejection, but only when the rejected turn was relatively short with a low confidence score.", "labels": [], "entities": []}, {"text": "The fifth rule predicts a correction when TOOT uses a particular confirmation strategy for turns that are relatively long and far from the beginning of the dialogue.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6420432925224304}]}, {"text": "The sixth rule predicts a correction when the previous turn is spoken soon after the prompt, and contains the problem indicator help.", "labels": [], "entities": []}, {"text": "Note that this use of the domain-independent help is the only reference to a lexical item in this ruleset.", "labels": [], "entities": []}, {"text": "This ruleset includes features from all of the feature subsets in our inventory (PROS, ASR, SYS, POS, DIA).", "labels": [], "entities": []}, {"text": "For the current turn, the feature types that appear in the rules are PROS (dur, zeros), ASR (conf, gram, syls), SYS (conftype), and POS (diadist).", "labels": [], "entities": [{"text": "ASR", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9694794416427612}, {"text": "POS", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9267025589942932}]}, {"text": "Of the previous turn's features, only two feature sets emerge as important: PROS (pref0mn, predur, preppau, pretempo) and ASR.", "labels": [], "entities": [{"text": "PROS", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.5749976634979248}, {"text": "ASR", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.7736901044845581}]}, {"text": "Furthermore, within a feature set such as PROS, the useful features of the current and previous turns differ somewhat (e.g., zeros is useful for the current turn, whereas tempo is useful for the prior turn), suggesting important differences in the prosodic characteristics of corrections versus the turns they follow.", "labels": [], "entities": [{"text": "PROS", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.793032169342041}]}, {"text": "When we look at a ruleset produced using only features commonly available to current dialogue systems, such as ASR+SYS+POS (DIA = all), we see that creative use of these features could in fact support correction classification ().", "labels": [], "entities": [{"text": "POS", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.4695385694503784}, {"text": "correction classification", "start_pos": 201, "end_pos": 226, "type": "TASK", "confidence": 0.8954420983791351}]}, {"text": "For example, the fourth rule predicts that the current turn is a correction when it is not too short, and when the pre turn indicates awareness (evidenced by the presence of no) of a problem in the ppre turn (which was recognized with low confidence).", "labels": [], "entities": [{"text": "pre", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9667737483978271}]}, {"text": "This ruleset uses both ASR (gram, nofeat, syls) and SYS (conftype) features of the current turn; although only one rule in fact makes use of SYS features.", "labels": [], "entities": [{"text": "ASR", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9691324830055237}, {"text": "SYS", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.8935481905937195}]}, {"text": "For the contextual DIA features, only the ASR features occur in the rule-set:, PrepreTurn (ppreconf, ppreynstr), and Prior and PMean (pmnconf, priorynstrpct, pmnwordsstr, priorrejnum).", "labels": [], "entities": [{"text": "PrepreTurn", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9586739540100098}]}, {"text": "Comparing this ruleset to the previous one, we see that where timing features (dur, predur, zeros, pretempo, preppau) appear often when prosodic features are available, related features such as syls and wordsstr (from which, e.g., tempo is estimated) maybe compensating in this ruleset.", "labels": [], "entities": []}, {"text": "And of course the rejection feature (prerejbool) itself is a function of the confidence score of the prior turn.", "labels": [], "entities": [{"text": "rejection feature (prerejbool)", "start_pos": 18, "end_pos": 48, "type": "METRIC", "confidence": 0.7963386297225952}]}, {"text": "Note also that lexical features of the recognized string (nofeat, prenofeat, ppreynstr, prestr, priorynstrpct) emerge as quite useful in this ruleset-especially as contextual features.", "labels": [], "entities": []}, {"text": "So, what the system has recognized in prior turns is a good predictor of whether the current turn is a correction.", "labels": [], "entities": []}, {"text": "Also note that the overall verbosity of the previous dialogue (pmnwordsstr) appears in two of the rules.", "labels": [], "entities": []}, {"text": "An example of a ruleset learned from only prosodic features (Raw, DIA = all, from) is shown in.", "labels": [], "entities": []}, {"text": "This ruleset is notably terser than those shown in Figures 4 and 5 and includes primarily timing-based features (current turn features dur, zeros, and tempo; local contextual feature pretempo; and dialogue-level features pmndur and pmnppau).", "labels": [], "entities": [{"text": "tempo", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.9550199508666992}]}, {"text": "However, all prosodic feature types but f0 appear at least once in the ruleset, and features specific to the current turn differ from those relevant to different types of dialogue history.", "labels": [], "entities": []}, {"text": "As with our previous descriptive findings discussed in Section 3, this ruleset shows that corrections are longer, louder, follow longer pauses, and contain less internal silence than non-corrections, and that these features can be used successfully to identify them.", "labels": [], "entities": []}, {"text": "The machine-learning experiments described in Section 4.2 were motivated by our long-term goal to incorporate a correction predictor into future versions of our spoken dialogue system.", "labels": [], "entities": []}, {"text": "As such, the experiments were limited to a binary prediction task (predicting whether a turn was a correction or a non-correction) and only considered features readily available to our dialogue system.", "labels": [], "entities": []}, {"text": "In this section we present additional experiments removing some of these restrictions, with the goal of further investigating some of the descriptive findings discussed in Section 3.", "labels": [], "entities": []}, {"text": "Recall from Section 3.2 that there were some differences in the prosodic features of corrections produced by native versus non-native speakers when such features were normalized by the first turn in the dialogue.", "labels": [], "entities": []}, {"text": "We thus investigated whether adding a native speaker feature (currently manually labeled) to the prosodic feature set Norm1 (DIA = all) would improve prediction accuracy.", "labels": [], "entities": [{"text": "Norm1", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.5343531966209412}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.8535022139549255}]}, {"text": "Although the error was reduced from 24.32% to 22.68%, this difference was not statistically significant.", "labels": [], "entities": [{"text": "error", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9885486364364624}]}, {"text": "Furthermore, when we added the native speaker feature to both the best-performing ruleset in (Raw+ASR+SYS+POS, DIA = PreTurn) and the best-performing prosodic feature set (Raw, DIA = all), the error rates actually increased; again, however, the differences were not statistically significant.", "labels": [], "entities": [{"text": "PreTurn", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9460682272911072}]}, {"text": "Also, in Sections 3.1 and 3.4, we identified differences between different types of corrections, which suggests that our features might be more effectively used to predict each correction type differently.", "labels": [], "entities": []}, {"text": "In other words, what would happen if instead of predicting whether a turn was a correction (T) or not (F) (the binary classification task investigated above), we predicted whether a turn was ADD, ADD/OMIT, OMIT, PAR, REP, or F (i.e., not a correction).", "labels": [], "entities": [{"text": "PAR", "start_pos": 212, "end_pos": 215, "type": "METRIC", "confidence": 0.9538133144378662}, {"text": "REP", "start_pos": 217, "end_pos": 220, "type": "METRIC", "confidence": 0.9485260844230652}, {"text": "F", "start_pos": 225, "end_pos": 226, "type": "METRIC", "confidence": 0.9888743162155151}]}, {"text": "Because, as shows, we only have limited amounts of data for several of our classes (e.g., only 2% of our corrections are ADD/OMIT); we performed a simpler version of this experiment, combining our three lowest frequency classes (ADD, ADD/OMIT, and PAR) into the single class MISC.", "labels": [], "entities": [{"text": "PAR", "start_pos": 248, "end_pos": 251, "type": "METRIC", "confidence": 0.9875374436378479}]}, {"text": "Using the best feature set from (Raw+ASR+SYS+POS, DIA = PreTurn), shows our results using 25-fold cross-validation.", "labels": [], "entities": [{"text": "PreTurn", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9300146102905273}]}, {"text": "First, note that our overall estimated error is now 24.13% \u00b1 0.89%.", "labels": [], "entities": [{"text": "error", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.8771195411682129}]}, {"text": "Although this is a huge increase compared to the 15.72% error rate of our original binary classifier, it should be noted that considering correction types separately makes our class distribution quite skewed, with the data for our three correction classes much smaller than the majority class.", "labels": [], "entities": []}, {"text": "Nevertheless, our classifier yields a slight but significant decrease compared to the majority baseline error, and a nonsignificant decrease compared to the Prerejbool baseline error (both baselines remain the same as in).", "labels": [], "entities": [{"text": "Prerejbool baseline error", "start_pos": 157, "end_pos": 182, "type": "DATASET", "confidence": 0.623872697353363}]}, {"text": "With respect to precision and recall, although the absolute numbers for corrections are much lower than in, we again see that predicting corrections yields higher precision than recall, whereas predicting noncorrections yields higher recall than precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9993215799331665}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9984342455863953}, {"text": "predicting corrections", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.8987997174263}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9962726831436157}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9923074245452881}, {"text": "recall", "start_pos": 234, "end_pos": 240, "type": "METRIC", "confidence": 0.995171844959259}, {"text": "precision", "start_pos": 246, "end_pos": 255, "type": "METRIC", "confidence": 0.9889954328536987}]}, {"text": "Finally, an examination of the learned ruleset (which contains four rules for predicting MISC, two rules for predicting OMIT, and seven rules for predicting REP) does show that features are used differently across correction types.", "labels": [], "entities": [{"text": "predicting MISC", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.8324103653430939}, {"text": "predicting OMIT", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.7745638787746429}, {"text": "predicting REP", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.7817232012748718}]}, {"text": "For example, the feature prestr is only used to predict repetition corrections (in particular, after a turn containing help).", "labels": [], "entities": [{"text": "prestr", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.7257336378097534}, {"text": "predict repetition corrections", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6670644680658976}]}, {"text": "Our rules also show some overlap with our earlier descriptive findings.", "labels": [], "entities": []}, {"text": "For example, we noted that corrections of rejections were more likely to be repetitions, and find the feature prerejbool in two of the rules for predicting repetitions.", "labels": [], "entities": []}, {"text": "These findings suggest that if more data were available, predicting corrections by type might prove a useful strategy.", "labels": [], "entities": [{"text": "predicting corrections", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.9247076511383057}]}], "tableCaptions": [{"text": " Table 2  Distribution of correction types.", "labels": [], "entities": []}, {"text": " Table 3  Corrections versus non-corrections by prosodic feature.", "labels": [], "entities": []}, {"text": " Table 4  Mean concept accuracy by correction position in Chain.", "labels": [], "entities": [{"text": "Mean concept", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.88288214802742}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.5216832160949707}]}, {"text": " Table 5  Corrections by system strategy.", "labels": [], "entities": []}, {"text": " Table 6  Means and standard deviations for prosodic features over all turns.", "labels": [], "entities": [{"text": "Means", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9909201860427856}]}, {"text": " Table 7  Estimated error, recall, precision, and F \u03b2 = 1 for predicting corrections.", "labels": [], "entities": [{"text": "error", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.7896207571029663}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9995622038841248}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9996726512908936}, {"text": "F \u03b2", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9835904836654663}, {"text": "predicting corrections", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8722415864467621}]}, {"text": " Table 8  Predicting correction types (error \u00b1 SE = 24.13 \u00b1 0.89)", "labels": [], "entities": [{"text": "Predicting correction", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7484252154827118}, {"text": "error \u00b1 SE", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.8974258701006571}]}]}