{"title": [{"text": "Orthographic Errors in Web Pages: Toward Cleaner Web Corpora", "labels": [], "entities": [{"text": "Orthographic Errors", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7399664223194122}]}], "abstractContent": [{"text": "Since the Web by far represents the largest public repository of natural language texts, recent experiments, methods, and tools in the area of corpus linguistics often use the Web as a corpus.", "labels": [], "entities": []}, {"text": "For applications where high accuracy is crucial, the problem has to be faced that a non-negligible number of orthographic and grammatical errors occur in Web documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.992668867111206}]}, {"text": "In this article we investigate the distribution of orthographic errors of various types in Web pages.", "labels": [], "entities": []}, {"text": "As a by-product, methods are developed for efficiently detecting erroneous pages and for marking orthographic errors in acceptable Web documents, reducing thus the number of errors in corpora and linguistic knowledge bases automatically retrieved from the Web.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automated analysis of large corpora has many useful applications.", "labels": [], "entities": []}, {"text": "Suitable language repositories can be used for deriving models of a given natural language, as needed for speech recognition, language generation (Oh and Rudickny 2000), and text correction).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7484075427055359}, {"text": "language generation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7336795926094055}, {"text": "text correction", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.8137374818325043}]}, {"text": "Other corpus-based methods determine associations between words), which yields a basis for computing thesauri, or dictionaries of terminological expressions and multiword lexemes.", "labels": [], "entities": []}, {"text": "From multilingual texts, translation lexica can be generated ().", "labels": [], "entities": []}, {"text": "The analysis of technical texts is used to automatically build dictionaries of acronyms fora given field, and related methods help to compute dictionaries that cover the special vocabulary of a given thematic area ().", "labels": [], "entities": []}, {"text": "In computer-assisted language learning (CALL), mining techniques for corpora are used to create individualized and user-centric exercises for grammar and text understanding.", "labels": [], "entities": [{"text": "computer-assisted language learning (CALL)", "start_pos": 3, "end_pos": 45, "type": "TASK", "confidence": 0.7738413214683533}, {"text": "text understanding", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7147564738988876}]}, {"text": "By Zipf's law, most words, phrases, and specific grammatical constructions have a very low frequency.", "labels": [], "entities": []}, {"text": "Furthermore, the number of text genres and special thematic areas that come with their own picture of language is large.", "labels": [], "entities": []}, {"text": "This explains that most of the aforementioned applications can only work when built on top of huge heterogeneous corpora.", "labels": [], "entities": []}, {"text": "Since the Web represents by far the largest public repository for natural language texts, and since Web search engines such as Google offer simple access to pages where language material of a given orthographic, grammatical, or thematic kind is found, many recent experiments and technologies use the Web as a corpus.", "labels": [], "entities": []}, {"text": "One potential problem for Web-based corpus linguistics is caused by the fact that words and phrases occurring in Web pages are sometimes erroneous.", "labels": [], "entities": [{"text": "Web-based corpus linguistics", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6344181696573893}]}, {"text": "Typing errors represent one widespread phenomenon.", "labels": [], "entities": [{"text": "Typing errors", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.5370829105377197}]}, {"text": "Many Web pages, say, in English, are written by non-native speakers, or by persons with very modest language competence.", "labels": [], "entities": []}, {"text": "As a consequence, spelling errors and grammatical bugs result.", "labels": [], "entities": []}, {"text": "The character sets that are used for writing Web pages are often not fully adequate for the alphabet of a given language, which represents another systematic source for inaccuracies.", "labels": [], "entities": []}, {"text": "Furthermore, a small number of texts found in the Web is obtained via optical character recognition (OCR), which may again lead to garbled words.", "labels": [], "entities": [{"text": "optical character recognition", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.6801650126775106}]}, {"text": "As a consequence of these and other error sources, the Web contains a considerable number of \"bad\" pages with language material that is inappropriate for corpus construction.", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.7188450396060944}]}, {"text": "In one way or the other, all the aforementioned applications are affected by these inadequacies.", "labels": [], "entities": []}, {"text": "While the problem is probably not too serious for approaches that merely collect statistical information about given language items, the construction of dictionaries and related linguistic knowledge bases-which are, after all, meant to be used in different scenarios of automated language processing-becomes problematic if too many erroneous entries are retrieved from Web pages.", "labels": [], "entities": []}, {"text": "Obviously, in computer-assisted language learning it is a principal concern that words and phrases from the Web that are presented to the user are error free.", "labels": [], "entities": []}, {"text": "In discussions we found that problems resulting from erroneous language material in Web pages for distinct applications are broadly acknowledged (see also Section 4.4 of Kilgarriff and Grefenstette).", "labels": [], "entities": [{"text": "Kilgarriff and Grefenstette", "start_pos": 170, "end_pos": 197, "type": "DATASET", "confidence": 0.8639454444249471}]}, {"text": "Still, to the best of our knowledge, a serious analysis of the frequency and distribution of orthographic errors in the Web is missing, and no general methods have been developed that help to detect and exclude pages with too many erroneous words.", "labels": [], "entities": []}, {"text": "In this article we first report on a series of experiments that try to answer the following questions: 1.", "labels": [], "entities": []}, {"text": "What are important types of orthographic errors found in Web pages?", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Number of Web pages, number of normal tokens (tokens composed of standard letters only),  and sizes in megabytes of the \"general\" corpora. Numbers (1) and (2) refer to primary and  secondary corpora, respectively.", "labels": [], "entities": []}, {"text": " Table 2  Selected topics and statistics of English (E) and German (G) corpora for specific thematic areas.  Numbers (1) and (2) refer to corpora crawled with the simple and the refined strategy,  respectively.", "labels": [], "entities": []}, {"text": " Table 9  Some members of the top 1,000 most frequent English words transformed by typical OCR error  transformations and the number of Google hits of a garbled version.", "labels": [], "entities": []}, {"text": " Table 11  Most frequent German words with vowels\u00e4vowels\u00a8vowels\u00e4, \u00a8  o, \u00a8  u; frequencies of correct spelling and frequency  after applying e-transformation. Frequencies are counted in arbitrary Web pages (left part of the  table) and in PDF documents in the Web.", "labels": [], "entities": [{"text": "frequency", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.8419390320777893}]}, {"text": " Table 12  Size of error dictionaries.", "labels": [], "entities": []}, {"text": " Table 13  Underproduction of the maximal error dictionary in the primary English general HTML corpus.", "labels": [], "entities": [{"text": "maximal error dictionary", "start_pos": 34, "end_pos": 58, "type": "METRIC", "confidence": 0.9086140394210815}]}, {"text": " Table 15. The large  number of standard words among the hits in the class Best is caused by an incomplete- ness of our English dictionary, which does not always contain both the British and the  American spelling variants.", "labels": [], "entities": []}, {"text": " Table 15  Overproduction of the maximal error dictionary in the English general HTML corpus.", "labels": [], "entities": [{"text": "maximal error dictionary", "start_pos": 33, "end_pos": 57, "type": "METRIC", "confidence": 0.8967553973197937}, {"text": "English general HTML corpus", "start_pos": 65, "end_pos": 92, "type": "DATASET", "confidence": 0.5996039733290672}]}, {"text": " Table 16  Overproduction of the maximal error dictionary in the German general HTML corpus.", "labels": [], "entities": [{"text": "maximal error dictionary", "start_pos": 33, "end_pos": 57, "type": "METRIC", "confidence": 0.8879549900690714}, {"text": "German general HTML corpus", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.780459001660347}]}, {"text": " Table 17  Naive estimates of the ratio between the real number of errors and the number of hits of the  error dictionaries for distinct quality classes.", "labels": [], "entities": []}, {"text": " Table 18  Mean error rate for arbitrary orthographic errors in various document classes; results for the  general English HTML corpus.", "labels": [], "entities": [{"text": "Mean error rate", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.934649666150411}]}, {"text": " Table 19  Mean error rate for arbitrary orthographic errors in various document classes; results for the  general German HTML corpus.", "labels": [], "entities": [{"text": "Mean error rate", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.9336588382720947}, {"text": "German HTML corpus", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.8302709062894186}]}, {"text": " Table 20  Mean of error rates for all error types in primary and secondary general HTML corpora.", "labels": [], "entities": [{"text": "Mean of error rates", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.9435399919748306}]}, {"text": " Table 26  Composition of corpora retrieved with the simple (1) and the refined (2) crawling strategies. The  refined strategy (2) helps to avoid documents of type Chat and Junk, attracting documents of  type Prof at the same time.", "labels": [], "entities": []}, {"text": " Table 27  Mean error rates (estimates) for distinct document genres in seven corpora.", "labels": [], "entities": [{"text": "Mean error rates", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.9292323788007101}]}, {"text": " Table 28  Evaluation of filters F k , 1 \u2264 k \u2264 5, for English general HTML corpus, user-defined threshold  \u00b5 = 10 (top), \u00b5 = 5 (middle), and \u00b5 = 1 (bottom).", "labels": [], "entities": []}, {"text": " Table 30  Measuring the quality of distinct dictionaries for text correction. D crawl is produced by an  unfiltered crawl, D +F  crawl by a filtered crawl. For D +F+ED", "labels": [], "entities": [{"text": "text correction", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7733378708362579}]}, {"text": " Table 31  English (E) and German (G) documents of the Europarl corpora, sizes, error rates w.r.t. maximal  English and German error dictionaries, numbers of hits of the error dictionaries, and numbers of  real errors among hits.", "labels": [], "entities": [{"text": "Europarl corpora", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9822708070278168}]}]}