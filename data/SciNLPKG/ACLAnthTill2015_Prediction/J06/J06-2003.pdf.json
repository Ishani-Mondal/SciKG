{"title": [{"text": "Building and Using a Lexical Knowledge Base of Near-Synonym Differences", "labels": [], "entities": [{"text": "Lexical Knowledge Base of Near-Synonym Differences", "start_pos": 21, "end_pos": 71, "type": "TASK", "confidence": 0.5436826596657435}]}], "abstractContent": [{"text": "Choosing the wrong word in a machine translation or natural language generation system can convey unwanted connotations, implications, or attitudes.", "labels": [], "entities": [{"text": "Choosing the wrong word in a machine translation or natural language generation", "start_pos": 0, "end_pos": 79, "type": "TASK", "confidence": 0.7345146884520849}]}, {"text": "The choice between near-synonyms such as error, mistake, slip, and blunder-words that share the same core meaning, but differ in their nuances-can be made only if knowledge about their differences is available.", "labels": [], "entities": [{"text": "slip", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.5186200141906738}]}, {"text": "We present a method to automatically acquire anew type of lexical resource: a knowledge base of near-synonym differences.", "labels": [], "entities": []}, {"text": "We develop an unsupervised decision-list algorithm that learns extraction patterns from a special dictionary of synonym differences.", "labels": [], "entities": []}, {"text": "The patterns are then used to extract knowledge from the text of the dictionary.", "labels": [], "entities": []}, {"text": "The initial knowledge base is later enriched with information from other machine-readable dictionaries.", "labels": [], "entities": []}, {"text": "Information about the collocational behavior of the near-synonyms is acquired from free text.", "labels": [], "entities": []}, {"text": "The knowledge base is used by Xenon, a natural language generation system that shows how the new lexical resource can be used to choose the best near-synonym in specific situations.", "labels": [], "entities": []}, {"text": "1. Near-Synonyms Near-synonyms are words that are almost synonyms, but not quite.", "labels": [], "entities": []}, {"text": "They are not fully intersubstitutable, but vary in their shades of denotation or connotation, or in the components of meaning they emphasize; they may also vary in grammatical or collocational constraints.", "labels": [], "entities": []}, {"text": "For example, the word foe emphasizes active warfare more than enemy does (Gove 1984); the distinction between forest and woods is a complex combination of size, proximity to civilization, and wildness (as determined by the type of animals and plants therein) (Room 1981); among the differences between task and job is their collocational behavior with the word daunting: daunting task is a better collocation than daunting job.", "labels": [], "entities": []}, {"text": "More examples are given in Table 1 (Hirst 1995).", "labels": [], "entities": []}, {"text": "There are very few absolute synonyms, if they exist at all.", "labels": [], "entities": []}, {"text": "So-called dictionaries of synonyms actually contain near-synonyms.", "labels": [], "entities": []}, {"text": "This is made clear by dictionaries such as Webster's New Dictionary of Synonyms (Gove 1984) and Choose the Right Word (here-after CTRW) (Hayakawa 1994), which list clusters of similar words and explicate the differences between the words in each cluster.", "labels": [], "entities": [{"text": "Webster's New Dictionary of Synonyms (Gove 1984)", "start_pos": 43, "end_pos": 91, "type": "DATASET", "confidence": 0.8182248055934906}]}, {"text": "An excerpt from CTRW is presented in Figure 1.", "labels": [], "entities": [{"text": "CTRW", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.909935474395752}]}, {"text": "These dictionaries are in effect dictionaries of near-synonym discrimination.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our program was able to extract 12,365 distinctions from 7,450 of the 14,138 sentences of CTRW.", "labels": [], "entities": [{"text": "CTRW", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.9473676681518555}]}, {"text": "(The rest of the sentences usually do not contain directly expressed distinctions; for example: A terror-stricken person who is drowning may in panic resist the efforts of someone who is trying to save him.)", "labels": [], "entities": []}, {"text": "In order to evaluate the final results, we randomly selected 25 clusters as a development set, and another 25 clusters as a test set.", "labels": [], "entities": []}, {"text": "The development set was used to tune the program by adding new patterns if they helped improve the results.", "labels": [], "entities": []}, {"text": "The test set was used exclusively for testing.", "labels": [], "entities": []}, {"text": "We built by hand a standard solution for each set.", "labels": [], "entities": []}, {"text": "The baseline algorithm is to choose the default values whenever possible.", "labels": [], "entities": []}, {"text": "There are no defaults for the near-synonyms the sentence is about or for peripheral concepts; therefore, for these, the baseline algorithm assigns the sentence subject and object, respectively, using only tuples extracted by the chunker.", "labels": [], "entities": []}, {"text": "The measures that we used for evaluating each piece of information extracted from a sentence fragment were precision and recall.", "labels": [], "entities": [{"text": "evaluating each piece of information extracted from a sentence fragment", "start_pos": 30, "end_pos": 101, "type": "TASK", "confidence": 0.6718985497951507}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9996597766876221}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9991264939308167}]}, {"text": "The results to be evaluated have four components for ATTITUDE-STYLE DISTINCTIONS and five components for DENOTATIONAL DISTINCTIONS.", "labels": [], "entities": [{"text": "ATTITUDE-STYLE", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.9529982805252075}]}, {"text": "There could be missing components (except strength and frequency, which take default values).", "labels": [], "entities": []}, {"text": "Precision is the total number of correct components found (summed overall the sentences in the test set) divided by the total number of components found.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9939159750938416}]}, {"text": "Recall is the total number of correct components found divided by the number of components in the standard solution.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.988029420375824}]}, {"text": "For example, for the sentence Sometimes, however, profit can refer to gains outside the context of moneymaking, the program obtains profit, usually, medium, Denotation, gains outside the context of moneymaking, whereas the solution is profit, sometimes, medium, Denotation, gains outside the context of money-making.", "labels": [], "entities": []}, {"text": "The precision is .80 (four correct out of five found), and the recall is also .80 (four correct out of five in the standard solution).", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994670748710632}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9996204376220703}]}, {"text": "presents the results of the evaluation.", "labels": [], "entities": []}, {"text": "The first row of the table presents the results as a whole (all the components of the extracted lexical knowledge base).", "labels": [], "entities": []}, {"text": "Our system increases precision by 36 percentage points and recall by 46 percentage points over baseline on the development set.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9996888637542725}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.999727189540863}]}, {"text": "3 Recall and precision are both slightly higher still on the test set; this shows that the patterns added during the development stage were general.", "labels": [], "entities": [{"text": "Recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9974950551986694}, {"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9997654557228088}]}, {"text": "The second row of the table gives the evaluation results for extracting only the class of the distinction expressed, ignoring the strengths, frequencies, and peripheral concepts.", "labels": [], "entities": []}, {"text": "This allows fora more direct evaluation of the acquired extraction patterns.", "labels": [], "entities": []}, {"text": "The baseline algorithm attained higher precision than in the case when all the components are considered because the default class Denotation is the most frequent in CTRW.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9990792274475098}, {"text": "CTRW", "start_pos": 166, "end_pos": 170, "type": "DATASET", "confidence": 0.9404040575027466}]}, {"text": "Our algorithm attained slightly higher precision and recall on the development set than it did in the complete evaluation, probably due to a few cases in which the frequency and strength were incorrectly extracted, and slightly lower on the test set, probably due to some cases in which the frequency and strength were easy to extract correctly.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994367957115173}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9992668032646179}]}, {"text": "The components of Xenon to be evaluated here are the near-synonym choice module and the near-synonym collocation module.", "labels": [], "entities": []}, {"text": "We evaluate each module in interaction with the sentence-realization module HALogen, 19 first individually and then both working together.", "labels": [], "entities": []}, {"text": "An evaluation of HALogen itself was presented by Langkilde-Geary (2002a) using a section of the Penn Treebank as test set.", "labels": [], "entities": [{"text": "HALogen", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.7799123525619507}, {"text": "Penn Treebank", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9939915537834167}]}, {"text": "HALogen was able to produce output for 80% of a set of 2,400 inputs (automatically derived from the test sentences by an input construction tool).", "labels": [], "entities": [{"text": "HALogen", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.822337806224823}]}, {"text": "The output was 94% correct when the input representation was fully specified, and between 94% and 55% for various other experimental settings.", "labels": [], "entities": []}, {"text": "The accuracy was measured using the BLEU score () and the string edit distance by comparing the generated sentences with the original sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995529055595398}, {"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9707634449005127}, {"text": "string edit distance", "start_pos": 58, "end_pos": 78, "type": "METRIC", "confidence": 0.5996469358603159}]}, {"text": "This evaluation method can be considered as English-to-English translation via meaning representation.", "labels": [], "entities": []}, {"text": "7.5.1 Evaluation of the Near-Synonym Choice Module.", "labels": [], "entities": []}, {"text": "For the evaluation of the nearsynonym choice module, we conducted two experiments.", "labels": [], "entities": []}, {"text": "(The collocation module was disabled for these experiments.)", "labels": [], "entities": []}, {"text": "Experiment 1 involved simple monolingual generation.", "labels": [], "entities": []}, {"text": "Xenon was given a suite of inputs: Each was an interlingual representation of a sentence and the set of nuances that correspond to a near-synonym in the sentence (see).", "labels": [], "entities": []}, {"text": "The sentence generated by Xenon was considered correct if the expected near-synonym, whose nuances were used as input preferences, is chosen.", "labels": [], "entities": []}, {"text": "The sentences used in this first experiment were very simple; therefore, the interlingual representations were easily built by hand.", "labels": [], "entities": []}, {"text": "In the interlingual representation, the near-synonym was replaced with the corresponding metaconcept.", "labels": [], "entities": []}, {"text": "There was only one near-synonym in each sentence.", "labels": [], "entities": []}, {"text": "Two data sets were used in Experiment 1: a development set of 32 near-synonyms of the five clusters presented in in order to set the exponent k of the scaling function in equation, and a test set of 43 near-synonyms selected from six clusters, namely, the set of English near-synonyms shown in.", "labels": [], "entities": []}, {"text": "For the evaluation of the near-synonym collocation module, we collected sentences from the BNC that contain preferred collocations from the knowledge base of near-synonym collocational behavior.", "labels": [], "entities": [{"text": "BNC", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9006465077400208}]}, {"text": "The BNC was preferred over Hansard for these evaluation experiments because it is a balanced corpus and contains the collocations of interest, whereas Hansard does not contain some of the collocations and near-synonyms of interest.", "labels": [], "entities": [{"text": "BNC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7092512845993042}]}, {"text": "The sentences were collected from the first half of the BNC (50 million words).", "labels": [], "entities": [{"text": "BNC", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9407913088798523}]}, {"text": "Sentence data sets 3 and 4 contain collocations for the development set of near-synonyms in; sentence data sets 5 and 6 contain collocations for the English near-synonyms in.", "labels": [], "entities": []}, {"text": "Sets 3 and 5 include at most two sentences per collocation (the first two sentences from the corpus, except in cases when the input construction tool failed to produce valid interlingual representations); sets 4 and 6 include all the sentences with collocations as they occurred in the fragment of the corpus (except the sentences for which the input construction tool failed).", "labels": [], "entities": []}, {"text": "For example, for set 4 there were initially 527 sentences, and the input construction tool succeeded on 297 of them.", "labels": [], "entities": []}, {"text": "Set 3 was used for development-to choose the discount weights (see below)-and the others only for testing.", "labels": [], "entities": []}, {"text": "The architecture of this experiment is the same as that of the English-to-English experiments), except that in this case it was the near-synonym choice module that was disabled.", "labels": [], "entities": []}, {"text": "We observe that the sentence data sets may contain collocations for the wrong senses of some near-synonyms because, as explained in Section 3.4, the near-synonym collocations knowledge base may contain, fora cluster, collocations fora different sense.", "labels": [], "entities": []}, {"text": "For example, the collocation trains run appears in the cluster flow, gush, pour, run, spout, spurt, squirt, stream, when it should appear only in another cluster.", "labels": [], "entities": []}, {"text": "In this case the nearsynonym run should not be replaced with the metaconcept generic flow v because it corresponds to a different metaconcept.", "labels": [], "entities": []}, {"text": "These sentences should be eliminated from the data sets, but this would involve disambiguation or manual elimination.", "labels": [], "entities": []}, {"text": "However, they do not affect the evaluation results because they are unlikely to produce anticollocations.", "labels": [], "entities": []}, {"text": "This is because trains run is a frequent bigram, whereas trains flow is not; Xenon will make the correct choice by default.", "labels": [], "entities": []}, {"text": "Sentence data set 3 was used to choose the best values of the discount weights W anti colloc and W less pref colloc . In fact, the latter could be approximated by the former, treating less preferred collocations as anti-collocations, because the number of less preferred collocations is very small in the knowledge base.", "labels": [], "entities": [{"text": "Sentence data set", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.7600911557674408}]}, {"text": "As the value of the discount weight W anti colloc increased (from 0.0 and 1.0), the number of anti-collocations generated decreased; there were no anti-collocations left for W anti colloc = 0.995.", "labels": [], "entities": []}, {"text": "presents the results of the evaluation experiments.", "labels": [], "entities": []}, {"text": "These results refer to the evaluation of Xenon with the near-synonym collocations module enabled and the near-synonym choice module disabled (lexical nuances are ignored in this experiment).", "labels": [], "entities": []}, {"text": "The baseline used for comparison is obtained by running HALogen only, without any extension modules (no knowledge of collocations).", "labels": [], "entities": []}, {"text": "For each test, the first four columns contain the number of test cases, the number of near-synonyms correctly chosen by the baseline system, the number of preferred collocations, and the number of anticollocations produced by the baseline system.", "labels": [], "entities": []}, {"text": "The remainder of the columns present results obtained by running Xenon with only the near-synonym collocations module enabled (i.e., HALogen and the collocations module): the number of near-synonyms correctly chosen, the number of preferred collocations produced, and the number of anti-collocations produced.", "labels": [], "entities": [{"text": "HALogen", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.756686270236969}]}, {"text": "The number of anti-collocations was successfully reduced to zero, except for sentence sets 4 and 6 where 1% of the anti-collocations remained.", "labels": [], "entities": []}, {"text": "The sixth column (correct choices or accuracy) differs from the seventh column (preferred collocations) in the following way: The correct choice is the near-synonym used in the original BNC sentence; sometimes the generated sentence can choose a different nearsynonym that is not the expected one but which participates in a preferred collocation (this happens when more than one near-synonym from the same cluster collocates well with the collocate word).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9986103773117065}]}, {"text": "For example, both serious mistake and serious blunder are preferred collocations, while only one of mistake and blunder is the correct choice in any particular context.", "labels": [], "entities": []}, {"text": "The number of correct choices is relevant in this experiment only to show that the collocations module does not have a negative effect on correctness; it even increases the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9980150461196899}]}, {"text": "The interaction between the nearsynonym choice module and the collocations module increases Xenon's performance.", "labels": [], "entities": []}, {"text": "To prove this, we repeated the experiments of the previous section, but this time with input preferences (the nuances of the near-synonym from the original sentence).", "labels": [], "entities": []}, {"text": "The architecture of this testis the same as that of the English-to-English experiments in Section 7.5.1, depicted in. shows the number of correct near-synonym choices (and the percent accuracy) for the baseline case (no nuances, no collocation module; i.e., HALogen by itself), for the collocations module alone (i.e., HALogen and the collocations module only; this column is also part of), for the near-synonym choice module alone (i.e., HALogen and the nuances module only), and for Xenon with both modules enabled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.997174859046936}]}, {"text": "When both modules are enabled there is a slight increase inaccuracy on sentence data sets 4, 5, and 6; the accuracy onset 3 is the same as using the near-synonyms module only.", "labels": [], "entities": [{"text": "accuracy onset 3", "start_pos": 107, "end_pos": 123, "type": "METRIC", "confidence": 0.979323128859202}]}], "tableCaptions": [{"text": " Table 2  Precision and recall of the baseline and of our algorithm (for all the components and for the  distinction class only; boldface indicates best results).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9759904742240906}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9991449117660522}]}, {"text": " Table 3  Example of counts, mutual information scores, and t-test scores for the collocate daunting with  near-synonyms of task. The second column shows the number of hits for the collocation daunting  x, where x is the near-synonym in the first column. The third column shows PMI prox (scaled by  10 5 for readability), the fourth column, the t values between the collocation with maximum  frequency (daunting task) and daunting x, and the last column, the t-test between daunting x and  the collocations with minimum frequency (daunting stint and daunting hitch).", "labels": [], "entities": []}, {"text": " Table 5  Example of collocations extracted for the near-synonym task. The first collocation was selected  (ranked in the set of first T collocations) by four measures; the second collocation was selected  by two measures.", "labels": [], "entities": []}, {"text": " Table 6  Results of Experiment 1 (boldface indicates best results).", "labels": [], "entities": []}, {"text": " Table 7  Results of Experiment 2 (boldface indicates best results).", "labels": [], "entities": []}, {"text": " Table 8  The results of the evaluation of Xenon's collocations module (boldface indicates best results).", "labels": [], "entities": []}, {"text": " Table 9  Correct near-synonym choices for the baseline system (HALogen only), for HALogen with each  module of Xenon separately, and for HALogen with both modules of Xenon (boldface indicates  best results).", "labels": [], "entities": []}]}