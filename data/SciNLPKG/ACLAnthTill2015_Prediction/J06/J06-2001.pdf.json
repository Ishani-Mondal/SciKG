{"title": [{"text": "Experiments on the Automatic Induction of German Semantic Verb Classes", "labels": [], "entities": [{"text": "Automatic Induction of German Semantic Verb Classes", "start_pos": 19, "end_pos": 70, "type": "TASK", "confidence": 0.8398738418306623}]}], "abstractContent": [{"text": "This article presents clustering experiments on German verbs: A statistical grammar model for German serves as the source fora distributional verb description at the lexical syntax-semantics interface, and the unsupervised clustering algorithm k-means uses the empirical verb properties to perform an automatic induction of verb classes.", "labels": [], "entities": []}, {"text": "Various evaluation measures are applied to compare the clustering results to gold standard German semantic verb classes under different criteria.", "labels": [], "entities": []}, {"text": "The primary goals of the experiments are (1) to empirically utilize and investigate the well-established relationship between verb meaning and verb behavior within a cluster analysis and (2) to investigate the required technical parameters of a cluster analysis with respect to this specific linguistic task.", "labels": [], "entities": []}, {"text": "The clustering methodology is developed on a small-scale verb set and then applied to a larger-scale verb set including 883 German verbs.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Clustering is a standard procedure in multivariate data analysis.", "labels": [], "entities": [{"text": "multivariate data analysis", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6608568330605825}]}, {"text": "It is designed to allow exploration of the inherent natural structure of the data objects, where objects in the same cluster are as similar as possible and objects in different clusters are as dissimilar as possible.", "labels": [], "entities": []}, {"text": "Equivalence classes induced by the clusters provide a means for generalizing over the data objects and their features.", "labels": [], "entities": []}, {"text": "The clustering of the German verbs is performed by the k-means algorithm, a standard unsupervised clustering technique as proposed by.", "labels": [], "entities": [{"text": "clustering of the German verbs", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.7710514426231384}]}, {"text": "With k-means, initial verb clusters are iteratively reorganized by assigning each verb to its closest cluster and recalculating cluster centroids until no further changes take place.", "labels": [], "entities": []}, {"text": "Applying the k-means algorithm assumes (1) that verbs are represented by distributional vectors and (2) that verbs that are closer to each other in a mathematically defined way are also more similar to each other in a linguistic way.", "labels": [], "entities": []}, {"text": "k-Means depends on the following parameters: (1) The number of clusters is not known beforehand, so the clustering experiments investigate this parameter.", "labels": [], "entities": []}, {"text": "Related to this parameter is the level of semantic concept: The more verb clusters are found, the more specific the semantic concept, and vice versa.", "labels": [], "entities": []}, {"text": "(2) k-means is sensitive to the initial clusters, so the initialization is varied according to how much preprocessing we invest: Both random clusters and hierarchically preprocessed clusters are used as initial clusters for k-means.", "labels": [], "entities": []}, {"text": "In the case of preprocessed clusters, the hierarchical clustering is performed as bottom-up agglomerative clustering with the following criteria for merging the clusters: single linkage (minimal distance between nearest neighbor verbs), complete linkage (minimal distance between furthest neighbor verbs), average distance between verbs, distance between cluster centroids, and Ward's method (minimizing the sum of squares when merging clusters).", "labels": [], "entities": []}, {"text": "The merging method influences the shape of the clusters; for example, single linkage causes a chaining effect in the shape of the clusters, and complete linkage creates compact clusters.", "labels": [], "entities": []}, {"text": "In addition, there are several possibilities for defining the similarity between distributional vectors.", "labels": [], "entities": []}, {"text": "But which best fits the idea of verb similarity?", "labels": [], "entities": [{"text": "verb similarity", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.701139822602272}]}, {"text": "presents an overview of relevant similarity measures that are applied in the experiments.", "labels": [], "entities": []}, {"text": "x and y refer to the verb object vectors, their subscripts to the verb feature values.", "labels": [], "entities": []}, {"text": "The Minkowski metric can be applied to frequencies and probabilities.", "labels": [], "entities": []}, {"text": "It is a generalization of the two well-known instances q = 1 (Manhattan distance) and q = 2 (Euclidean distance).", "labels": [], "entities": []}, {"text": "The Kullback-Leibler divergence (KL) is a measure from information theory that determines the inefficiency of assuming a model probability distribution given the true distribution.", "labels": [], "entities": [{"text": "Kullback-Leibler divergence (KL)", "start_pos": 4, "end_pos": 36, "type": "METRIC", "confidence": 0.6833839535713195}]}, {"text": "The KL divergence is not defined in case y i = 0, so the probability distributions need to be smoothed.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.5458077043294907}]}, {"text": "Two variants of KL, information radius and skew divergence, perform a default smoothing.", "labels": [], "entities": []}, {"text": "Both variants can tolerate zero values in the distribution because they work with a weighted average of the two distributions compared. has shown that the skew divergence is an effective measure for distributional similarity in NLP.", "labels": [], "entities": []}, {"text": "Similarly to Lee's method, we set the weight w for the skew divergence to 0.9.", "labels": [], "entities": []}, {"text": "The cosine measures the similarity of the two object vectors x and y by calculating the cosine of the angle between the feature vectors.", "labels": [], "entities": []}, {"text": "The cosine measure can be applied to frequency and probability values.", "labels": [], "entities": []}, {"text": "For a detailed description of hierarchical clustering techniques and an intuitive interpretation of the similarity measures, the reader is referred to, for example,.", "labels": [], "entities": []}, {"text": "There is no agreed standard method for evaluating clustering experiments and results, but a variety of evaluation measures from diverse areas such as theoretical statistics, machine vision, and Web-page clustering are generally applicable.", "labels": [], "entities": [{"text": "Web-page clustering", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.6421549469232559}]}, {"text": "We used the following two measures for the evaluation: (1) define and evaluate a cluster analysis of adjectives, based on common cluster membership of object pairs in the clustering C and the manual classification M.", "labels": [], "entities": []}, {"text": "Recall and precision numbers are calculated in the standard way, with true positives the number of common pairs in M and C, false positives the number of pairs in C, but not M, and false negatives the number of pairs in M, but not C.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9392812252044678}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9959216117858887}]}, {"text": "We use the f -score pairF (as harmonic mean between recall and precision), which provides an easy to understand The adjusted Rand index is a measure of agreement versus disagreement between object pairs in clusterings that provides the most appropriate reference to a null model; cf. equation  The 168 German verbs are associated with distributional vectors over frame types and assigned to initial clusters.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.994067907333374}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9417096376419067}, {"text": "adjusted Rand index", "start_pos": 116, "end_pos": 135, "type": "METRIC", "confidence": 0.7412524620691935}]}, {"text": "Then k-means is allowed to run for as many iterations as it takes to reach a fixed point, and the resulting clusters are interpreted and evaluated against the manual classes.", "labels": [], "entities": []}, {"text": "The verbs are described by D1-D3, and each level refers to frequencies and probabilities, with original and smoothed values.", "labels": [], "entities": []}, {"text": "The initial clusters for k-means are generated either randomly or by a preprocessing cluster analysis, that is, hierarchical clustering as described in Section 2.3.", "labels": [], "entities": []}, {"text": "For random cluster initialization the verbs are randomly assigned to a cluster, with cluster numbers between 1 and the number of manual classes.", "labels": [], "entities": []}, {"text": "The experiments are performed with the number of k clusters being fixed to the number of gold standard classes; optimization of the number of clusters is addressed in Section 3.4.", "labels": [], "entities": []}, {"text": "The following tables present the results of the clustering experiments.", "labels": [], "entities": [{"text": "clustering", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.9668672680854797}]}, {"text": "each concentrate on one technical parameter of the clustering process; then focus on performing clustering with a fixed parameter set, in order to vary the linguistically interesting parameters concerning the feature choice for the verbs.", "labels": [], "entities": []}, {"text": "All significance tests have been performed with \u03c7 2 , df = 1, \u03b1 = 0.05.", "labels": [], "entities": [{"text": "significance", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9620561003684998}]}, {"text": "illustrates the effect of the distribution units (frequencies and probabilities) on the clustering result.", "labels": [], "entities": []}, {"text": "The experiments use distributions on D1 and D2 with random and preprocessed initialization, and the cosine as similarity measure (since it works for both distribution units).", "labels": [], "entities": []}, {"text": "To summarize the results, neither the differences between frequencies and probabilities nor between original and smoothed values are significant.", "labels": [], "entities": []}, {"text": "illustrates the usage of different similarity measures.", "labels": [], "entities": [{"text": "similarity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9424706697463989}]}, {"text": "As before, the experiments are performed for D1 and D2 with random and preprocessed initialization.", "labels": [], "entities": []}, {"text": "The similarity measures are applied to the relevant probability distributions (as the distribution unit that can be used for all measures).", "labels": [], "entities": []}, {"text": "The tables point out that there is no best-performing similarity measure in the clustering processes.", "labels": [], "entities": []}, {"text": "On the larger feature set, the Kullback-Leibler variants information radius and skew divergence tend to outperform all other similarity measures.", "labels": [], "entities": []}, {"text": "In fact, the skew divergence is the only measure that shows significant differences for some parameter settings, as compared to all other measures except information radius.", "labels": [], "entities": []}, {"text": "In further experiments, we will therefore concentrate on the two Kullback-Leibler variants.", "labels": [], "entities": []}, {"text": "compare the effects of varying the initialization of the k-means algorithm.", "labels": [], "entities": []}, {"text": "The experiments are performed for D1 and D2 with probability distributions, using the similarity measures information radius and skew divergence.", "labels": [], "entities": []}, {"text": "For random and hierarchical initialization, we cite both the evaluation scores for the k-means initial cluster analysis (i.e., the output clustering from the random assignment or the preprocessing hierarchical analysis), and for the k-means result.", "labels": [], "entities": []}, {"text": "The manual columns in the tables refer to a cluster analysis where the initial clusters provided to   k-means are the manual classification, that is, the gold standard.", "labels": [], "entities": []}, {"text": "An optimal cluster analysis should realize the \"perfect\" clustering and not perform any reorganization of the clusters.", "labels": [], "entities": []}, {"text": "In the experiments, k-means does perform iterations, so the clustering result is suboptimal.", "labels": [], "entities": []}, {"text": "This finding is caused by the syntax-semantics mismatches, which we deliberately included in the definition of the gold standard (recall, e.g., that unterst\u00fctzenunterst\u00a8unterst\u00fctzen is syntactically very different compared to the other three Support verbs).", "labels": [], "entities": [{"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9812012314796448}]}, {"text": "In addition, the results not only show that the feature sets are suboptimal, but also that the loss in quality is less for the linguistically refined feature level D2 compared to D1, as we would have hoped.", "labels": [], "entities": []}, {"text": "For random clustering initialization to k-means, the tables present both the best and the average clustering results.", "labels": [], "entities": [{"text": "clustering initialization", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7913995385169983}]}, {"text": "The best results are paired with the evaluation of their initial clusters, that is, the random clusterings.", "labels": [], "entities": []}, {"text": "As the tables show, the initial clusters receive low evaluation scores.", "labels": [], "entities": []}, {"text": "Typically, the clusterings consist of clusters with rather homogeneous numbers of verbs, but the perturbation within the clusters is high, as expected.", "labels": [], "entities": []}, {"text": "k-means is able to cope with the high degree of perturbation: The resulting clusters improve significantly and are comparable with those based on preprocessed hierarchical clustering; this competitiveness vanishes with an increasing number of features.", "labels": [], "entities": []}, {"text": "The average values of the random initialization experiments are clearly below the best ones, but not significantly different.", "labels": [], "entities": []}, {"text": "Cluster analyses as based on agglomerative hierarchical clustering with single-linkage amalgamation are evaluated as poor compared to the gold standard.", "labels": [], "entities": []}, {"text": "This result is probably due to the chaining effect in the clustering, which is characteristic for single linkage; the effect is observable in the analysis, which typically contains one very large cluster and many clusters with few verbs, mostly singletons.", "labels": [], "entities": []}, {"text": "k-means obviously cannot compensate for this strong bias in cluster sizes (and their respective centroids); the reorganization improves the clusterings, but the result is still worse than for any other initialization.", "labels": [], "entities": []}, {"text": "With average distance and centroid distance amalgamation, both the clusterings and the evaluation results are less extreme than with single linkage since the chaining effect is smoothed.", "labels": [], "entities": []}, {"text": "The overall results are better than for single linkage, but only slightly improved by k-means.", "labels": [], "entities": []}, {"text": "Hierarchical clusters as based on complete-linkage amalgamation are more compact, and result in a closer fit to the gold standard than the previous methods.", "labels": [], "entities": []}, {"text": "The hierarchical initialization is only slightly improved by k-means; in some cases the k-means output is worse than its hierarchical initialization.", "labels": [], "entities": []}, {"text": "Ward's method seems to work best on hierarchical clusters and k-means initialization.", "labels": [], "entities": []}, {"text": "The cluster sizes are more balanced and correspond to compact cluster shapes.", "labels": [], "entities": []}, {"text": "As for complete linkage, k-means improves the clusterings only slightly; in some cases the k-means output is worse than its hierarchical initialization.", "labels": [], "entities": []}, {"text": "A cluster analysis based on Ward's hierarchical clusters performs best of all the applied methods, especially with an increasing number of features.", "labels": [], "entities": []}, {"text": "The similarity of Ward's clusters (and similarly complete linkage clusters) and k-means is not by chance, since these methods aim to optimize the same criterion, the sum of distances between the verbs and their respective cluster centroids.", "labels": [], "entities": []}, {"text": "Note that for D2, Ward's method actually significantly outperforms all other initialization methods, complete linkage significantly outperforms all but Ward's.", "labels": [], "entities": []}, {"text": "Between single linkage, average and centroid distance, there are no significant differences.", "labels": [], "entities": []}, {"text": "For D1, there are no significant differences between the initializations.", "labels": [], "entities": []}, {"text": "The low scores in the tables might be surprising to the reader, but they reflect the difficulty of the task.", "labels": [], "entities": []}, {"text": "As mentioned before, we deliberately set high demands for the gold standard, especially with reference to the fine-grained, small classes.", "labels": [], "entities": []}, {"text": "Compared to related work (cf. Section 5), our results achieve lower scores because the task is more difficult; for example, Merlo and Stevenson (2001) classify 60 verbs into 3 classes, and Siegel and McKeown (2000) classify 56 verbs into 2 classes, as compared to our clustering, which assigns 168 verbs to 43 classes.", "labels": [], "entities": []}, {"text": "The following illustrations should provide an intuition about the difficulty of the task: 1.", "labels": [], "entities": []}, {"text": "Ina set of additional experiments, a random choice of a reduced number of 5/10/15/20 classes from the gold standard is performed.", "labels": [], "entities": []}, {"text": "The verbs from the respective gold standard classes are clustered with the optimal parameter set (see), which results in a pairwise f-  For illustrative purposes, we present representative parts of the cluster analysis as based on the following parameters: The clustering initialization is obtained from a hierarchical analysis of the German verbs (Ward's amalgamation method), the number of clusters being the number of manual classes (43); the similarity measure is the skew divergence.", "labels": [], "entities": []}, {"text": "The cluster analysis is based on the verb description on D3, with selectional roles for  and (e), since they use common alternations, but cluster (c) merges Existence, Position, and Aspect verbs because verb-idiosyncratic demands on selectional roles destroy the D2 class demarcation.", "labels": [], "entities": []}, {"text": "Still, the verbs in cluster (c) are close in their (more general conceptual) semantics, with a commonsense of (bringing into versus being in) existence.", "labels": [], "entities": []}, {"text": "laufen fits into the cluster with its sense of \"function.\"", "labels": [], "entities": []}, {"text": "Cluster (f) contains most verbs of Quantum Change, together with one verb of Production and Constitution each.", "labels": [], "entities": []}, {"text": "The common conceptual level of this cluster therefore refers to a quantum change including the quantum change from zero to something (as for the two verbs festlegen, 'constitute,' and bilden, 'found').", "labels": [], "entities": []}, {"text": "The verbs in this cluster typically subcategorize fora direct object, alternating with a reflexive usage, \"nr\" and \"npr\" with mostly auf Acc and um Acc . The selectional preferences help to distinguish this cluster: The verbs agree in demanding a thing or situation as subject, and various objects such as attribute, cognitive object, state, structure, or thing as object.", "labels": [], "entities": []}, {"text": "Without selectional preferences (on D1 and D2), the change of quantum verbs are not found together with the same degree of purity.", "labels": [], "entities": [{"text": "purity", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9598054885864258}]}, {"text": "There are verbs as in cluster (g) whose properties are correctly stated as similar by D1-D3, so a common cluster is justified, but the verbs only have coarse common meaning components; in this case t\u00f6ten 'kill' and unterrichten 'teach' agree in an action of one person or institution towards another.", "labels": [], "entities": []}, {"text": "geben in cluster (h) represents a singleton.", "labels": [], "entities": []}, {"text": "Syntactically, this is caused by being the only verb with a strong preference for \"xa.\"", "labels": [], "entities": []}, {"text": "From the meaning point of view, this specific frame represents an idiomatic expression, only possible with geben.", "labels": [], "entities": []}, {"text": "An overall interpretation of the clustering results gives insight into the relationship between verb properties and clustering outcome.", "labels": [], "entities": []}, {"text": "The fact that there are verbs that are clustered semantically on the basis of their corpus-based and knowledge-based empirical properties indicates (a) a relationship between the meaning components of the verbs and their behavior and (b) that the clustering algorithm is able to benefit from the linguistic descriptions and to abstract away from the noise in the distributions.", "labels": [], "entities": []}, {"text": "Low-frequency verbs were a problem in the clustering experiments.", "labels": [], "entities": []}, {"text": "Their distributions are noisier than those for more frequent verbs, so they typically constitute noisy clusters.", "labels": [], "entities": []}, {"text": "(3) As known beforehand, verb ambiguity cannot be modeled by the hard clustering algorithm k-means.", "labels": [], "entities": []}, {"text": "Ambiguous verbs were typically assigned either (a) to one of the correct clusters or (b) to a cluster whose verbs have distributions that are similar to the ambiguous distribution, or (c) to a singleton cluster.", "labels": [], "entities": []}, {"text": "The interpretation of the clusterings unexpectedly points to meaning components of verbs that have not been discovered by the manual classification.", "labels": [], "entities": []}, {"text": "An example verb is laufen, expressing not only a Manner of Motion but also a kind of existence when used in the sense of operation.", "labels": [], "entities": [{"text": "Manner", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.991823136806488}]}, {"text": "The discovery effect should be more impressive with an increasing number of verbs, since manual judgement is more difficult, and also with a soft clustering technique, where multiple cluster assignment is enabled.", "labels": [], "entities": []}, {"text": "(5) Ina similar way, the clustering interpretation exhibits semantically related verb classes, that is, verb classes that are separated in the manual classification, but semantically merged in a common cluster.", "labels": [], "entities": []}, {"text": "For example, Perception and Observation verbs are related in that all the verbs express an observation, with the Perception verbs additionally referring to a physical ability, such as hearing.", "labels": [], "entities": []}, {"text": "Related to the preceding issue, the manual verb classes as defined are demonstrated as detailed and subtle.", "labels": [], "entities": []}, {"text": "Compared to a more general classification that would appropriately merge several classes, the clustering confirms that we defined a difficult task with subtle classes.", "labels": [], "entities": []}, {"text": "We were aware of this fact but preferred a fine-grained classification, since it allows insight into verb and class properties.", "labels": [], "entities": []}, {"text": "In this way, verbs that are similar in meaning are often clustered incorrectly with respect to the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9152979254722595}]}, {"text": "To come to the main point, what exactly is the nature of the meaning-behavior relationship?", "labels": [], "entities": []}, {"text": "(1) Already a purely syntactic verb description allows a verb clustering clearly above the baseline.", "labels": [], "entities": []}, {"text": "The result is a (semantic) classification of verbs that agree in their syntactic frame definitions, for example, most of the Support verbs.", "labels": [], "entities": []}, {"text": "The clustering fails for semantically similar verbs that differ in their syntactic behavior, for example, unterst\u00fctzenunterst\u00a8unterst\u00fctzen, which belongs to the Support verbs but demands an accusative rather than a dative object.", "labels": [], "entities": []}, {"text": "In addition, it fails for syntactically similar verbs that are clustered together even though they do not exhibit semantic similarity; for example, many verbs from different semantic classes subcategorize for an accusative object, so they are falsely clustered together.", "labels": [], "entities": []}, {"text": "(2) Refining the syntactic verb information with prepositional phrases is helpful for the semantic clustering, not only in the clustering of verbs where the PPs are obligatory, but also in the clustering of verbs with optional PP arguments.", "labels": [], "entities": [{"text": "semantic clustering", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.6992235034704208}]}, {"text": "The improvement underlines the linguistic fact that verbs that are similar in their meaning agree either on a specific prepositional complement (e.g., glauben/denken an Acc ) or on a more general kind of modification, for example, directional PPs for Manner of Motion verbs.", "labels": [], "entities": []}, {"text": "(3) Defining selectional preferences for arguments improves the clustering results further, but the improvement is not as persuasive as when refining the purely syntactic verb descriptions with prepositional information.", "labels": [], "entities": []}, {"text": "For example, selectional preferences help demarcate the Quantum Change class because the respective verbs agree in their structural as well as selectional properties.", "labels": [], "entities": []}, {"text": "But in the Consumption class, essen and trinken have strong preferences fora food object, whereas konsumieren allows a wider range of object types.", "labels": [], "entities": []}, {"text": "In contrast, there are verbs that are very similar in their behavior, especially with respect to a coarse definition of selectional roles, but they do not belong to the same fine-grained semantic class, for example, t\u00f6ten and unterrichten.", "labels": [], "entities": []}, {"text": "The effect could be due to (a) noisy or (b) sparse data, but the basic verb descriptions appear reliable with respect to their desired linguistic content, and illustrates that even with little added information the effect exists (e.g., refining few arguments by 15 selectional roles results in 253 instead of 178 features, so the magnitude of feature numbers does not change).", "labels": [], "entities": []}, {"text": "Why do we encounter an indeterminism concerning the encoding and effect of verb features, especially with respect to selectional preferences?", "labels": [], "entities": []}, {"text": "The meaning of verbs comprises both properties that are general for the respective verb classes, and idiosyncratic properties that distinguish the verbs from each other.", "labels": [], "entities": []}, {"text": "As long as we define the verbs by those properties that represent the common parts of the verb classes, a clustering can succeed.", "labels": [], "entities": []}, {"text": "But by stepwise refining the verb description and including lexical idiosyncrasy, the emphasis on the common properties vanishes.", "labels": [], "entities": []}, {"text": "From a theoretical point of view, the distinction between common and idiosyncratic features is obvious, but from a practical point of view there is no perfect choice for the encoding of verb features.", "labels": [], "entities": []}, {"text": "The feature choice depends on the specific properties of the desired verb classes, and even if classes are perfectly defined on a common conceptual level, the relevant level of behavioral properties of the verb classes might differ.", "labels": [], "entities": []}, {"text": "Still, fora large-scale classification of verbs, we need to specify a combination of linguistic verb features as a basis for the clustering.", "labels": [], "entities": []}, {"text": "Which combination do we choose?", "labels": [], "entities": []}, {"text": "Both the theoretical assumption of encoding features of verb alternation as verb behavior and the practical realization by encoding syntactic frame types, prepositional phrases, and selectional preferences seem promising.", "labels": [], "entities": []}, {"text": "In addition, we aimed at a (rather linguistically than technically based) choice of selectional preferences that represents a useful compromise for the conceptual needs of the verb classes.", "labels": [], "entities": []}, {"text": "Therefore, this choice of features best utilizes the meaning-behavior relationship and will be applied in a largescale clustering experiment (cf. Section 4).", "labels": [], "entities": []}, {"text": "So far, all clustering experiments were performed on a small scale, preliminary set of 168 manually chosen German verbs.", "labels": [], "entities": []}, {"text": "One goal of this article was to develop a clustering methodology with respect to the automatic acquisition of a large-scale German verb classification.", "labels": [], "entities": [{"text": "acquisition of a large-scale German verb classification", "start_pos": 95, "end_pos": 150, "type": "TASK", "confidence": 0.5587668418884277}]}, {"text": "We therefore apply the insights on the theoretical relationship between verb meaning and verb behavior and our findings regarding the clustering parameters to a considerably larger amount of verb data.", "labels": [], "entities": []}, {"text": "We extracted all German verbs from our statistical grammar model that appeared with an empirical frequency of between 500 and 10,000 in the training corpus (cf. Section 2.2).", "labels": [], "entities": []}, {"text": "This selection resulted in a total of 809 verbs, including 94 verbs from the preliminary set of 168 verbs.", "labels": [], "entities": []}, {"text": "We added the missing verbs from the preliminary set, resulting in a total of 883 German verbs.", "labels": [], "entities": []}, {"text": "The feature description of the German verbs refers to the probability distribution over the coarse syntactic frame types, with prepositional phrase information on the 30 chosen PPs and selectional preferences for our empirically most successful combination 'n,' 'na,' 'nd,' 'nad,' and 'ns-dass.'", "labels": [], "entities": []}, {"text": "As in previous clustering experiments, the features are stepwise refined.", "labels": [], "entities": []}, {"text": "k-means is provided hierarchical clustering initialization (based on Ward's method), with the similarity measure being skew divergence.", "labels": [], "entities": [{"text": "clustering initialization", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8574851155281067}]}, {"text": "The number of clusters is set to 100, which corresponds to an average of 8.83 verbs per cluster, that is, not too fine-grained clusters but still possible to interpret.", "labels": [], "entities": []}, {"text": "The preliminary set of 168 verbs is a subset of the large-scale set in order to provide an \"auxiliary\" evaluation of the clustering results: Considering only the manually chosen verbs in the clustering result, this partial cluster analysis is evaluated against the gold standard of 43 verb classes.", "labels": [], "entities": []}, {"text": "Results were not expected to match the results of our clustering experiments using only the preliminary verb set, but to provide an indication of how different cluster analyses can be compared with each other.", "labels": [], "entities": []}, {"text": "Tables 11 to 13 present the clustering results for the large-scale verb set for D1-D3 in the rightmost columns, citing the evaluation scores of the initial (hierarchical) clusters and the resulting k-means clusters.", "labels": [], "entities": []}, {"text": "The subset of the 168 gold standard verbs is scattered over 72 of the 100 resulting clusters.", "labels": [], "entities": []}, {"text": "The results are compared to our previous results for the 168 verbs in 43 clusters, and to the case where those 168 verbs are clustered into 72 hierarchical classes.", "labels": [], "entities": []}, {"text": "The large-scale clustering results once more confirm the general insights (1) that the stepwise refinement of features improves the clustering and (2) that Ward's hierarchical clustering is seldom improved by the k-means application.", "labels": [], "entities": []}, {"text": "In addition, several of the large-scale cluster analyses were quite comparable with the clustering results using the small-scale set of verbs, especially when compared to 72 clusters.", "labels": [], "entities": []}, {"text": "In the following, we present example clusters from the optimal large-scale cluster analysis (according to the above evaluation): Ward's hierarchical cluster analysis based on subcategorization frames, PPs, and selectional preferences, without running k-means on the hierarchical clustering.", "labels": [], "entities": []}, {"text": "Some clusters are extremely good with respect to the semantic overlap of the verbs, some clusters contain a number of similar verbs mixed with semantically different verbs, and for some clusters it is difficult to recognize common elements of meaning.", "labels": [], "entities": []}, {"text": "The verbs that we think are semantically similar are marked in boldface.", "labels": [], "entities": []}, {"text": "(1) abschneiden 'cut off (2) aufhalten 'detain', aussprechen 'pronounce', auszahlen 'pay off', durchsetzen 'achieve', entwickeln 'develop', verantworten 'be responsible', verdoppeln 'double', zur\u00fcckhaltenzur\u00a8zur\u00fcckhalten 'keep away', zur\u00fcckziehenzur\u00a8zur\u00fcckziehen 'draw back', \u00a8 andern 'change'    (6) danken 'thank', entkommen 'escape', gratulieren 'congratulate' (7) beschleunigen 'speed up', bilden 'constitute', darstellen 'illustrate', decken 'cover', erf\u00fcllenerf\u00a8erf\u00fcllen 'fulfil', erh\u00f6henerh\u00a8erh\u00f6hen 'raise', erledigen 'fulfil', finanzieren 'finance', f \u00a8 ullen 'fill Clusters to are examples where the verbs do not share elements of meaning.", "labels": [], "entities": []}, {"text": "In the overall cluster analysis, such semantically incoherent clusters tend to be rather large, that is, with more than 15-20 verb members.", "labels": [], "entities": []}, {"text": "Clusters (4) to are examples of clusters where some of the verbs show overlap in meaning, but also contain considerable noise.", "labels": [], "entities": []}, {"text": "Cluster (4) mainly contains verbs of buying and selling, cluster contains verbs of wishing, cluster (6) contains verbs of expressing approval, and cluster (7) contains verbs of quantum change.", "labels": [], "entities": []}, {"text": "Clusters to are examples of clusters where most or all verbs show a strong similarity in their semantic concept.", "labels": [], "entities": []}, {"text": "Cluster (8) contains verbs expressing a propositional attitude; the underlined verbs, in addition, indicate an emotion.", "labels": [], "entities": []}, {"text": "The only unmarked verb wei\u00dfen can be explained, since some of its inflected forms are ambiguous with respect to their base verb: either wei\u00dfen or wissen, a verb that belongs to the Aspect verb class.", "labels": [], "entities": []}, {"text": "The verbs in cluster (9) describe a scene where somebody or some situation makes something possible (in the positive or negative sense); the only exception verb is veranstalten.", "labels": [], "entities": []}, {"text": "The verbs in cluster (10) are connected more loosely, all referring to a verbal discussion, with the underlined verbs denoting a negative, complaining way of utterance.", "labels": [], "entities": []}, {"text": "In cluster (11) all verbs refer to a basis, in cluster (12) the verbs describe the process from arresting to releasing a suspect, and cluster contains verbs of estimating an amount of money.", "labels": [], "entities": []}, {"text": "In cluster (14), all verbs except for entschuldigen refer to an emotional state (with some origin for the emotion).", "labels": [], "entities": []}, {"text": "The verbs in cluster (15) except for profitieren all indicate thought (with or without talking) about a certain matter.", "labels": [], "entities": []}, {"text": "Finally in cluster (16), we can recognize the same weather verb cluster as in the previously discussed small-scale cluster analyses.", "labels": [], "entities": []}, {"text": "We experimented with two variations in the clustering setup: (1) For the selection of the verb data, we considered a random choice of German verbs in approximately the same magnitude of number of verbs (900 verbs plus the preliminary verb set), but without any restriction on the verb frequency.", "labels": [], "entities": []}, {"text": "The clustering results are-both on the basis of the evaluation and on the basis of a manual inspection of the resulting clusters-much worse than in the preceding cluster analysis, since the large number of low-frequency verbs destroys the clustering.", "labels": [], "entities": []}, {"text": "The number of target clusters was set to 300 instead of 100, that is, the average number of verbs per cluster was 2.94 instead of 8.83.", "labels": [], "entities": []}, {"text": "The resulting clusters are numerically slightly worse than in the preceding cluster analysis, but easier for inspection and therefore a preferred basis fora largescale resource.", "labels": [], "entities": []}, {"text": "Several of the large, semantically incoherent clusters are split into smaller and more coherent clusters, and the formerly coherent clusters often preserved their constitution.", "labels": [], "entities": []}, {"text": "To present one example, the following cluster from the 100-cluster analysis anzeigen 'announce is split into the following four clusters from the 300-cluster analysis: where cluster (a) shows a loose semantic coherence of declaration, the verbs in cluster (b) are semantically very similar and describe an emotional impact of somebody or a situation on a person, and the verbs in cluster (c) show a protective (and the negation: nonprotective) influence of one person towards another.", "labels": [], "entities": []}, {"text": "Summarizing, the large-scale clustering experiment results in a mixture of semantically coherent and incoherent verb classes.", "labels": [], "entities": []}, {"text": "Semantically incoherent verb classes and clustering mistakes need to be split into finer and more coherent clusters, or to be filtered from the classification.", "labels": [], "entities": []}, {"text": "Semantically coherent verb classes need little manual correction as a lexical resource.", "labels": [], "entities": []}, {"text": "Interestingly, the coherence in verb classes refers to different criteria on meaning coherence, such as synonymy (e.g., reduzieren 'reduce' and verringern 'decrease'), antonymy (e.g., reduzieren 'reduce' and erh\u00f6hen 'raise'), situational overlap (e.g., emotional state containing freuen 'be glad' and\u00e4rgernand\u00a8and\u00e4rgern 'be annoyed'), and participation in a common process/script (e.g., bestellen 'order', kaufen 'buy', verkaufen 'sell', and abholen 'pick up').", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Example distributions of German verbs.", "labels": [], "entities": []}, {"text": " Table 4  Comparing distributions on D1 and D2.", "labels": [], "entities": []}, {"text": " Table 5  Comparing similarity measures on D1 and D2.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9791488647460938}]}, {"text": " Table 6  Comparing clustering initializations on D1.", "labels": [], "entities": [{"text": "clustering initializations", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.9260752201080322}]}, {"text": " Table 7  Comparing clustering initializations on D2.", "labels": [], "entities": [{"text": "clustering initializations", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.9228149652481079}]}, {"text": " Table 8  Comparing feature descriptions.", "labels": [], "entities": []}, {"text": " Table 9  Comparing selectional preference slot definitions.", "labels": [], "entities": [{"text": "selectional preference slot", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.8593630989392599}]}, {"text": " Table 10  Comparing selectional preference frame definitions.", "labels": [], "entities": []}, {"text": " Table 11  Large-scale clustering on D1.", "labels": [], "entities": [{"text": "Large-scale clustering", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.5178419947624207}]}, {"text": " Table 12  Large-scale clustering on D2.", "labels": [], "entities": [{"text": "Large-scale clustering", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.5193979442119598}]}, {"text": " Table 13  Large-scale clustering on D3 with n/na/nd/nad/ns-dass.", "labels": [], "entities": []}]}