{"title": [{"text": "Consistent Validation of Manual and Automatic Sense Annotations with the Aid of Semantic Graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "The task of annotating texts with senses from a computational lexicon is widely recognized to be complex and often subjective.", "labels": [], "entities": []}, {"text": "Although strategies like interannotator agreement and voting can be applied to deal with the divergences between sense taggers, the consistency of sense choices with respect to the reference dictionary is not always guaranteed.", "labels": [], "entities": []}, {"text": "In this article, we introduce Valido, a visual tool for the validation of manual and automatic sense annotations.", "labels": [], "entities": []}, {"text": "The tool employs semantic interconnection patterns to smooth possible divergences and support consistent decision making.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sense tagging is the task of assigning senses chosen from a computational lexicon to words in context.", "labels": [], "entities": [{"text": "Sense tagging", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7357370257377625}]}, {"text": "This is a task where both machines and humans find it difficult to reach an agreement.", "labels": [], "entities": []}, {"text": "The problem depends on a variety of factors, ranging from the inherent subjectivity of the task to the granularity of sense discretization, coverage of the reference dictionary, etc.", "labels": [], "entities": []}, {"text": "The problem of validation is even amplified when sense tags are collected through acquisition interfaces like the Open Mind Word Expert (, due to the unknown source of the contributions of possibly unskilled volunteers.", "labels": [], "entities": [{"text": "validation", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.9725832939147949}]}, {"text": "Strategies like voting for automatic sense annotations and the use of interannotator agreement with adjudication for human sense assignments only partially solve the issue of disagreement.", "labels": [], "entities": []}, {"text": "Especially when there is no clear preference towards a certain word sense, the final choice made by a judge can be subjective, if not arbitrary.", "labels": [], "entities": []}, {"text": "This is a case where analyzing the intrinsic structure of the reference lexicon is essential for producing a consistent decision.", "labels": [], "entities": []}, {"text": "A lexicographer is indeed expected to review a number of related dictionary entries in order to adjudicate a sense coherently.", "labels": [], "entities": []}, {"text": "This work can be tedious, time-consuming, and often incomplete due to the complex structure of the resource.", "labels": [], "entities": []}, {"text": "As a result, inconsistent choices can be made.", "labels": [], "entities": []}, {"text": "In this article, we present Valido, a tool for supporting the validation of both manual and automatic sense annotations through the use of semantic graphs, particularly of semantic interconnection patterns (Navigli and Velardi 2005).", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed an evaluation of the tool on SemCor (), a selection of documents from the Brown Corpus where each content word is annotated with concepts (specifically, synsets) from the WordNet inventory.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9823783338069916}, {"text": "WordNet inventory", "start_pos": 184, "end_pos": 201, "type": "DATASET", "confidence": 0.9312326014041901}]}, {"text": "The objective of our evaluation is to show that Valido constitutes good support fora validator in detecting bad or inconsistent annotations.", "labels": [], "entities": []}, {"text": "A total of 360 sentences of average length (9 or 10 content words) were uniformly selected from the set of documents in the SemCor corpus.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.7495883405208588}]}, {"text": "The average ambiguity of an arbitrary word in the data set was 5.77, while the average ambiguity of the most ambiguous word in a sentence was 8.70.", "labels": [], "entities": []}, {"text": "For each sentence \u03c3 = w 1 w 2 . .", "labels": [], "entities": []}, {"text": "w n annotated in SemCor with the senses s w 1 s w 2 . .", "labels": [], "entities": []}, {"text": "s w n (s w i \u2208 Senses(w i ), i \u2208 {1, 2, . .", "labels": [], "entities": []}, {"text": ", n}), we identified the most ambiguous word w i \u2208 \u03c3, and randomly chose a different sense s w i for that word, that is, s w i \u2208 Senses(w i ) \\ {s w i }.", "labels": [], "entities": []}, {"text": "The experiment simulates in vitro a situation in which, for each sentence, the annotators agree on which sense to assign to all the words but one, where one annotator provides an appropriate sense and the other selects a different sense.", "labels": [], "entities": []}, {"text": "The random factor guarantees an approximation to the uniform distribution in the test set of all the possible degrees of disagreement between sense annotators (ranging from regular polysemy to homonymy).", "labels": [], "entities": []}, {"text": "We applied Valido with validation policy (\u03b3) to the annotated sentences and evaluated the performance of the tool in suggesting the appropriate choice for the words with disagreement.", "labels": [], "entities": []}, {"text": "We assessed precision (the number of correct suggestions over the overall number of suggestions from the Valido tool), recall (the number of correct suggestions over the total number of words to be validated), and the F1 measure \ud97b\udf59 2pr p+r \ud97b\udf59 . The results are reported in for nouns, adjectives, and verbs (we neglected adverbs, as very few interconnections can be found for them).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993948936462402}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9995112419128418}, {"text": "F1 measure \ud97b\udf59 2pr p+r", "start_pos": 218, "end_pos": 238, "type": "METRIC", "confidence": 0.9522422552108765}]}, {"text": "The experiment shows that evidences of inconsistency are provided by the tool with good precision (and a good F1 measure, especially for nouns and verbs, beating the random baseline of 50%).", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.999030590057373}, {"text": "F1 measure", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9849279820919037}]}, {"text": "Notice that this test differs from the typical evaluation of word sense disambiguation tasks, like the Senseval exercises (http://www.senseval.org), in that we are assessing highly polysemous (possibly, very fine grained) words.", "labels": [], "entities": [{"text": "word sense disambiguation tasks", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.7930021733045578}]}, {"text": "Comparing the results with a smart baseline, like the most frequent sense heuristic, is not feasible in this experiment, as the frequency of WordNet senses was calculated on the same data set (i.e., SemCor).", "labels": [], "entities": []}, {"text": "Notice anyway that beating a baseline is not necessarily our objective if we are notable to provide justifications (like semantic graphs) of which the human validator can take advantage in order to take the final decision.", "labels": [], "entities": []}, {"text": "The low recall resulting for parts of speech other than nouns (mainly, adjectives) is due to alack of connectivity in the lexical knowledge base, especially when dealing with connections across different parts of speech.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9989789724349976}]}, {"text": "This is a problem already discussed in and partially taken into account in.", "labels": [], "entities": []}, {"text": "Valido can indeed be used as a tool to collect new, consistent collocations that could grow the LKB from which the semantic interconnection patterns are extracted, possibly in an iterative process.", "labels": [], "entities": []}, {"text": "We plan to investigate this topic in the near future.", "labels": [], "entities": []}], "tableCaptions": []}