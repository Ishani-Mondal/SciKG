{"title": [{"text": "The PARADISE Evaluation Framework: Issues and Findings", "labels": [], "entities": [{"text": "PARADISE Evaluation Framework", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.7008955975373586}]}], "abstractContent": [{"text": "There has been a great deal of interest over the past 20 years in developing metrics and frameworks for evaluating and comparing the performance of spoken-language dialogue systems.", "labels": [], "entities": []}, {"text": "One of the results of this interest is a potential general methodology, known as the PARADISE framework.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.7620185613632202}]}, {"text": "This squib highlights some important issues concerning the application of PARADISE that have, up to now, not been sufficiently emphasized or have even been neglected by the dialogue-system community.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.520819365978241}]}, {"text": "These include considerations regarding the selection of appropriate regression parameters, normalization effects on the accuracy of the prediction, the influence of speech-recognition errors on the performance function, and the selection of an appropriate user-satisfaction measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9985044002532959}]}, {"text": "In addition, it gives the results of an evaluation of data from two Wizard-of-Oz experiments.", "labels": [], "entities": []}, {"text": "These evaluations include different dependent variables and examination of individual user-satisfaction measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "A long list of objective dialogue metrics for dialogue evaluation, which can be calculated without recourse to human judgment, and subjective dialogue metrics, which are based on human judgments, have been proposed.", "labels": [], "entities": [{"text": "dialogue evaluation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8742722570896149}]}, {"text": "Their wellknown limitations led Walker, to propose their paradigm for dialogue system evaluation (PARADISE), a potentially general methodology for evaluating spoken-language dialogue systems, the goal of which was to compare and optimize different dialogue managers and task domains independently.", "labels": [], "entities": [{"text": "dialogue system evaluation (PARADISE)", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.6095057278871536}]}, {"text": "The PARADISE framework maintains that the system's primary objective is to maximize user satisfaction, and it derives a combined performance metric fora dialogue system as a weighted linear combination of tasksuccess measures and dialogue costs.", "labels": [], "entities": []}, {"text": "The dialogue costs are of two types: dialogueefficiency costs (e.g., number of utterances, dialogue time), which are measures of the system's efficiency in helping the user to complete the task, and dialogue-quality costs (e.g., system-response delay, mean recognition score), which are intended to capture other aspects that can have large effects on the user's perception of the system's performance.", "labels": [], "entities": [{"text": "mean recognition score", "start_pos": 252, "end_pos": 274, "type": "METRIC", "confidence": 0.7935364246368408}]}, {"text": "Applying PARADISE to dialogue data requires dialogue corpora to be collected via controlled experiments during which users subjectively rate their satisfaction.", "labels": [], "entities": []}, {"text": "Here, user satisfaction is calculated with a survey () that asks users to specify the degree to which they agree with several statements about the performance of the system.", "labels": [], "entities": []}, {"text": "In addition, the other parameters of the model of performance, i.e., the task-success measures and the dialogue costs, must be either automatically logged by the system or be hand-labeled.", "labels": [], "entities": []}, {"text": "The PARADISE model of performance posits that a performance function can then be derived by applying multivariate linear regression (MLR) with user satisfaction as the dependent variable and task-success measures and dialogue costs as the independent variables.", "labels": [], "entities": []}, {"text": "The squib addresses some PARADISE issues (with most of them arising from the application of MLR) that have, up to now, not been sufficiently emphasized or have even been neglected by the dialogue-system community.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.5922646522521973}]}, {"text": "Moreover, most of the considerations about these issues are supported by the results of applying PARADISE to the data from two Wizard-of-Oz (WOZ) experiments carried out during the development of a weather-information-providing, naturallanguage spoken dialogue system ( \u02c7).", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9742262363433838}]}, {"text": "In contrast to previous PARA-DISE applications, we evaluated data that were collected in the early stages of a dialogue system's design where speech understanding was simulated by a human.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 142, "end_pos": 162, "type": "TASK", "confidence": 0.7694879770278931}]}], "datasetContent": [], "tableCaptions": []}