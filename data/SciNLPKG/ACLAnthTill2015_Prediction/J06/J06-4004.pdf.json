{"title": [], "abstractContent": [{"text": "This article describes in detail an n-gram approach to statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7453148663043976}]}, {"text": "This approach consists of a log-linear combination of a translation model based on n-grams of bilingual units, which are referred to as tuples, along with four specific feature functions.", "labels": [], "entities": []}, {"text": "Translation performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9714437127113342}, {"text": "European Parliament Plenary Sessions (EPPS)", "start_pos": 153, "end_pos": 196, "type": "DATASET", "confidence": 0.7277769105775016}]}], "introductionContent": [{"text": "The beginnings of statistical machine translation (SMT) can be traced back to the early fifties, closely related to the ideas from which information theory arose and inspired by works on cryptography) during World War II.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.8394372761249542}]}, {"text": "According to this view, machine translation was conceived as the problem of finding a sentence by decoding a given \"encrypted\" version of it.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.806297242641449}]}, {"text": "Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time).", "labels": [], "entities": []}, {"text": "Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9953816533088684}]}, {"text": "The first SMT systems were developed in the early nineties ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9958149790763855}]}, {"text": "These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions.", "labels": [], "entities": []}, {"text": "For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9939127564430237}]}, {"text": "In the case of target language probabilities, these were generally trained from monolingual data by using n-grams.", "labels": [], "entities": []}, {"text": "Present SMT systems have evolved from the original ones in such away that mainly differ from them in two respects: first, word-based translation models have been replaced by phrase-based translation models which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented . As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT).", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9862197637557983}, {"text": "machine translation", "start_pos": 507, "end_pos": 526, "type": "TASK", "confidence": 0.7748563587665558}, {"text": "automatic speech recognition (ASR)", "start_pos": 576, "end_pos": 610, "type": "TASK", "confidence": 0.841779867808024}, {"text": "text to speech synthesis (TTS)", "start_pos": 615, "end_pos": 645, "type": "TASK", "confidence": 0.7761686870029995}, {"text": "spoken language translation (SLT)", "start_pos": 692, "end_pos": 725, "type": "TASK", "confidence": 0.8307773172855377}]}, {"text": "According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9943211078643799}, {"text": "ASR", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9727066159248352}, {"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9679258465766907}]}, {"text": "In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9937619566917419}]}, {"text": "As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6419212967157364}]}, {"text": "The translation system described in this article implements a translation model that has been derived from the finite-state perspective-more specifically, from the work of Casacuberta (2001) and.", "labels": [], "entities": []}, {"text": "However, whereas in this earlier work the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams.", "labels": [], "entities": []}, {"text": "In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models.", "labels": [], "entities": [{"text": "consistency", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9912630319595337}]}, {"text": "The translation model presented here actually constitutes a language model of a sort of \"bilanguage\" composed of bilingual units, which will be referred to as tuples (de).", "labels": [], "entities": []}, {"text": "An alternative approach, which relies on bilingual-unit unigram probabilities, was developed by; in contrast, the approach presented here considers bilingualunit n-gram probabilities.", "labels": [], "entities": []}, {"text": "In addition to the tuple n-gram translation model, the translation system presented here implements four specific feature functions that are log-linearly combined along with the translation model for performing the decoding ).", "labels": [], "entities": []}, {"text": "This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task.", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "First, Section 2 presents a complete description of the n-gram-based translation model.", "labels": [], "entities": []}, {"text": "Then, Section 3 describes in detail the additional feature functions that, along with the translation model, compose the n-gram-based SMT system implemented.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9751582741737366}]}, {"text": "Section 4 describes the European Parliament Plenary Session (EPPS) data, as well as the most relevant details about the translation tasks considered.", "labels": [], "entities": [{"text": "European Parliament Plenary Session (EPPS) data", "start_pos": 24, "end_pos": 71, "type": "DATASET", "confidence": 0.8318617790937424}]}, {"text": "Section 5 presents and discusses the translation experiments and their results.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9822346568107605}]}, {"text": "Finally, Section 6 presents some conclusions and intended further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents all translation experiments performed and a brief error analysis of the obtained results.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.964867353439331}]}, {"text": "In order to evaluate the relative contributions of different system elements to the overall performance of the n-gram-based translation system, three different experimental settings are considered.", "labels": [], "entities": []}, {"text": "The experiments and their results are described in Section 5.1, and a brief error analysis of results is presented in Section 5.2.", "labels": [], "entities": []}, {"text": "Finally, a comparison between n-gram-based SMT and state-of-the-art phrase-based translation systems is presented in Section 5.3.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8868129253387451}, {"text": "phrase-based translation", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6266935765743256}]}, {"text": "As already mentioned, three experimental settings are considered.", "labels": [], "entities": []}, {"text": "For each setting, the impact on translation quality of a different system parameter is evaluated, namely, feature function, n-gram size, and the source-nulled tuple strategy.", "labels": [], "entities": []}, {"text": "Evaluations in all three experimental settings are performed with respect to the same standard system configuration, which is defined in terms of the following parameters: In the three experimental settings considered, which are presented in the following subsections, a total of seven different system configurations are evaluated in both translation directions, English to Spanish and Spanish to English.", "labels": [], "entities": []}, {"text": "Thus, a total of 14 different translation experiments are performed.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.963767945766449}]}, {"text": "For each of these cases, the corresponding test set is translated by using the corresponding estimated models and set of optimal coefficients.", "labels": [], "entities": []}, {"text": "The same decoder settings (which were previously described in Section 4.2.4) that were used during the optimizations are used for all translation experiments.", "labels": [], "entities": []}, {"text": "Translation results are evaluated in terms of mWER and BLEU by using the two references available for each language test set.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9363034963607788}, {"text": "mWER", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8543881177902222}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9988075494766235}]}], "tableCaptions": [{"text": " Table 1  Basic statistics for the training, development, and test data sets (M and k stand for millions and  thousands, respectively; Lmean refers to the average sentence length in number of words, and  Ref. to the number of available translation references).", "labels": [], "entities": []}, {"text": " Table 2  Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and  translation accuracy when tuples are extracted from different alignment sets. Notice that  BLEU measurements in this table correspond to translations computed by using the tuple  n-gram model alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.977425217628479}, {"text": "BLEU", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9977872371673584}]}, {"text": " Table 3. Both translation directions, Spanish to English (ES \u2192 EN) and  English to Spanish (EN \u2192 ES), are considered in each table.  In the case of", "labels": [], "entities": []}, {"text": " Table 3  Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and  translation accuracy for different pruning values and both translation directions. Notice that  BLEU measurements in this table correspond to translations computed by using the tuple  n-gram model along with the additional four feature functions described in Section 3.2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9596474170684814}, {"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.997955322265625}]}, {"text": " Table 4  Perplexity measurements for translation and target language models of different n-gram sizes.", "labels": [], "entities": []}, {"text": " Table 5  Evaluation results for experiments on feature function contribution.", "labels": [], "entities": [{"text": "feature function contribution", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.6034827133019766}]}, {"text": " Table 6  Evaluation results for experiments on n-gram size incidence.", "labels": [], "entities": []}, {"text": " Table 7  Evaluation results for experiments on strategies for handling source-nulled tuples.", "labels": [], "entities": []}, {"text": " Table 8  Percentage of occurrence for each type of error in English-to-Spanish and Spanish-to-English  translations that were studied.", "labels": [], "entities": [{"text": "occurrence", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9603265523910522}]}, {"text": " Table 9  The four best BLEU results for the EPPS translation task in TC-STAR's first evaluation campaign.  N-gram based system results are provided in brackets. All BLEU values presented here have  been taken from TC-STAR's SLT Progress Report, available at: http://www.tc-star.org/.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9979357719421387}, {"text": "EPPS translation task", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7754697799682617}, {"text": "TC-STAR", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.787589967250824}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9973357319831848}, {"text": "TC-STAR's SLT Progress Report", "start_pos": 215, "end_pos": 244, "type": "DATASET", "confidence": 0.8021912693977356}]}]}