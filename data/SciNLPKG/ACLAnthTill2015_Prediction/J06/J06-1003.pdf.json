{"title": [{"text": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness", "labels": [], "entities": [{"text": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.6325913539954594}]}], "abstractContent": [{"text": "The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed.", "labels": [], "entities": [{"text": "quantification of lexical semantic relatedness", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.7370828628540039}]}, {"text": "We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9756487011909485}, {"text": "detecting and correcting real-word spelling errors", "start_pos": 122, "end_pos": 172, "type": "TASK", "confidence": 0.8047190507253011}]}, {"text": "An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik.", "labels": [], "entities": []}, {"text": "In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness.", "labels": [], "entities": []}], "introductionContent": [{"text": "The need to determine semantic relatedness or its inverse, semantic distance, between two lexically expressed concepts is a problem that pervades much of natural language processing.", "labels": [], "entities": []}, {"text": "Measures of relatedness or distance are used in such applications as word sense disambiguation, determining the structure of texts, text summarization and annotation, information extraction and retrieval, automatic indexing, lexical selection, and the automatic correction of word errors in text.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7396241625150045}, {"text": "text summarization", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.6854016184806824}, {"text": "information extraction and retrieval", "start_pos": 167, "end_pos": 203, "type": "TASK", "confidence": 0.8389666229486465}, {"text": "automatic correction of word errors in text", "start_pos": 252, "end_pos": 295, "type": "TASK", "confidence": 0.7967581408364433}]}, {"text": "It's important to note that semantic relatedness is a more general concept than similarity; similar entities are semantically related by virtue of their similarity (bank-trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car-wheel) and antonymy (hot-cold), or just by any kind of functional relationship or frequent association (pencil-paper, penguin-Antarctica, rain-flood).", "labels": [], "entities": []}, {"text": "Computational applications typically require relatedness rather than just similarity; for example, money and river are cues to the in-context meaning of bank that are just as good as trust company.", "labels": [], "entities": []}, {"text": "However, it is frequently unclear how to assess the relative merits of the many competing approaches that have been proposed for determining lexical semantic relatedness.", "labels": [], "entities": [{"text": "lexical semantic relatedness", "start_pos": 141, "end_pos": 169, "type": "TASK", "confidence": 0.6338752110799154}]}, {"text": "Given a measure of relatedness, how can we tell whether it is a good one or a poor one?", "labels": [], "entities": []}, {"text": "Given two measures, how can we tell whether one is better than the other, and under what conditions it is better?", "labels": [], "entities": []}, {"text": "And what is it that makes some measures better than others?", "labels": [], "entities": []}, {"text": "Our purpose in this paper is to compare the performance of a number of measures of semantic relatedness that have been proposed for use in applications in natural language processing and information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 187, "end_pos": 208, "type": "TASK", "confidence": 0.8134379386901855}]}, {"text": "turned the Longman Dictionary of Contemporary English (LDOCE)) into a network by creating anode for every headword and linking each node to the nodes for all the words used in its definition.", "labels": [], "entities": [{"text": "Longman Dictionary of Contemporary English (LDOCE))", "start_pos": 11, "end_pos": 62, "type": "DATASET", "confidence": 0.9491531029343605}]}, {"text": "The 2851-word controlled defining vocabulary of LDOCE thus becomes the densest part of the network: the remaining nodes, which represent the headwords outside of the defining vocabulary, can be pictured as being situated at the fringe of the network, as they are linked only to defining-vocabulary nodes and not to each other.", "labels": [], "entities": []}, {"text": "In this network, the similarity function sim KF between words of the defining vocabulary is computed by means of spreading activation on this network.", "labels": [], "entities": []}, {"text": "The function is extended to the rest of LDOCE by representing each word as a list W = {w 1 , . .", "labels": [], "entities": []}, {"text": ", w r } of the words in its definition; thus, for instance, sim = sim {the, study, of, style, in, written, or, spoken, language}) built on this work to derive a context-sensitive, or dynamic, measure that takes into account the \"associative direction\" of a given word pair.", "labels": [], "entities": []}, {"text": "For example, the context {car, bus} imposes the associative direction of vehicle (close words are then likely to include taxi, railway, airplane, etc.), whereas the context {car, engine} imposes the direction of components of car (tire, seat, headlight, etc.).", "labels": [], "entities": []}], "datasetContent": [{"text": "How can we reason about and evaluate computational measures of semantic relatedness?", "labels": [], "entities": []}, {"text": "Three kinds of approaches are prevalent in the literature.", "labels": [], "entities": []}, {"text": "The first kind) is a (chiefly) theoretical examination of a proposed measure for those mathematical properties thought desirable, such as whether it is a metric (or the inverse of a metric), whether it has singularities, whether its parameter-projections are smooth functions, and soon.", "labels": [], "entities": []}, {"text": "In our opinion, such analyses act at best as a coarse filter in the comparison of a set of measures and an even coarser one in the assessment of a single measure.", "labels": [], "entities": []}, {"text": "The second kind of evaluation is comparison with human judgments.", "labels": [], "entities": []}, {"text": "Insofar as human judgments of similarity and relatedness are deemed to be correct by definition, this clearly gives the best assessment of the \"goodness\" of a measure.", "labels": [], "entities": []}, {"text": "Its main drawback lies in the difficulty of obtaining a large set of reliable, subject-independent judgments for comparison -designing a psycholinguistic experiment, validating its results, and soon.", "labels": [], "entities": []}, {"text": "(In Section 4.1 below, we will employ the rather limited data that such experiments have obtained to date.)", "labels": [], "entities": []}, {"text": "The third approach is to evaluate the measures with respect to their performance in the framework of a particular application.", "labels": [], "entities": []}, {"text": "If some particular NLP system requires a measure of semantic relatedness, we can compare different measures by seeing which one the system is most effective with, while holding all other aspects of the system constant.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we will use the second and the third methods to compare several different measures (Sections 4 and 5 respectively).", "labels": [], "entities": []}, {"text": "We focus on measures that use WordNet) as their knowledge source (to keep that as a constant) and that permit straightforward implementation as functions in a programming language.", "labels": [], "entities": []}, {"text": "Therefore, we select the following five measures: Hirst and St-Onge's (Section 2.4), Jiang and Conrath's (Section 2.6.2), Leacock and Chodorow's (Section 2.5.3), Lin's (Section 2.6.3), and Resnik's (Section 2.6.1).", "labels": [], "entities": []}, {"text": "The first is claimed as a measure of semantic relatedness because it uses all noun relations in WordNet; the others are claimed only as measures of similarity because they use only the hyponymy relation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9497122764587402}]}, {"text": "We implemented each measure, and used the Brown Corpus as the basis for the frequency counts needed in the information-based approaches.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8927889168262482}]}, {"text": "We now turn to a different approach to the evaluation of similarity and relatedness measures that tries to overcome the problems of comparison to human judgments that were described in the previous section.", "labels": [], "entities": []}, {"text": "Here, we compare the measures through the performance of an application that uses them: the detection and correction of real-word spelling errors in open-class words, i.e., malapropisms.", "labels": [], "entities": [{"text": "detection and correction of real-word spelling errors in open-class words", "start_pos": 92, "end_pos": 165, "type": "TASK", "confidence": 0.6834466099739075}]}, {"text": "While malapropism correction is also a useful application in its own right, it is particularly appropriate for evaluating measures of semantic relatedness.", "labels": [], "entities": [{"text": "malapropism correction", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.6642161160707474}]}, {"text": "Naturally occurring coherent texts, by their nature, contain many instances of related pairs of words (.", "labels": [], "entities": []}, {"text": "That is, they implicitly contain human judgments of relatedness that we could use in the evaluation of our relatedness measures.", "labels": [], "entities": []}, {"text": "But, of course, we don't know in practice just which pairs of words in a text are and aren't related.", "labels": [], "entities": []}, {"text": "We can get around this problem, however, by deliberately perturbing the coherence of the text -that is, introduding semantic anomalies such as malapropisms -and looking at the ability of the different relatedness measures to detect and correct the perturbations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Human and computer ratings of the Rubenstein-Goodenough set of word pairs (part 1 of 2).", "labels": [], "entities": []}, {"text": " Table 3. (For Jiang and Conrath's measure, the  coefficients are negative because their measure returns distance rather than similarity;  so for convenience, we show absolute values in the table.) 9", "labels": [], "entities": []}, {"text": " Table 3. These discrepancies can be explained by possible  minor differences in implementation (e.g., the compound-word recognition mechanism used in collecting  the frequency data), differences between the versions of WordNet used in the experiments (Resnik), and  differences in the corpora used to obtain the frequency data (Jiang and Conrath, Lin", "labels": [], "entities": [{"text": "WordNet", "start_pos": 220, "end_pos": 227, "type": "DATASET", "confidence": 0.9551928639411926}]}, {"text": " Table 2  Human and computer ratings of the Miller-Charles set of word pairs.", "labels": [], "entities": []}, {"text": " Table 4  Precision (P S ), recall (R S ), and F-measure (F S ) for malapropism suspicion with five measures of  semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs  or the complete news article (MAX).", "labels": [], "entities": [{"text": "Precision (P S )", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9250115871429443}, {"text": "recall (R S )", "start_pos": 28, "end_pos": 41, "type": "METRIC", "confidence": 0.9468989253044129}, {"text": "F-measure (F S )", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.9697474598884582}]}, {"text": " Table 5  Precision (P D ), recall (R D ), and F-measure (F D ) for malapropism detection with five measures of  semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs  or the complete news article (MAX).", "labels": [], "entities": [{"text": "Precision (P D )", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9300784230232239}, {"text": "recall (R D )", "start_pos": 28, "end_pos": 41, "type": "METRIC", "confidence": 0.9407497406005859}, {"text": "F-measure (F D )", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.9671663761138916}, {"text": "malapropism detection", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7093925178050995}]}]}