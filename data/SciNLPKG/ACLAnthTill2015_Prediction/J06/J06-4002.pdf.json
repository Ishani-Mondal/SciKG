{"title": [{"text": "Automatic Evaluation of Information Ordering: Kendall's Tau", "labels": [], "entities": [{"text": "Automatic Evaluation of Information Ordering", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7754714369773865}]}], "abstractContent": [{"text": "This article considers the automatic evaluation of information ordering, a task underlying many text-based applications such as concept-to-text generation and multidocument summarization.", "labels": [], "entities": [{"text": "evaluation of information ordering", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.5834627151489258}, {"text": "concept-to-text generation", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.7886611819267273}, {"text": "multidocument summarization", "start_pos": 159, "end_pos": 186, "type": "TASK", "confidence": 0.7028854787349701}]}, {"text": "We propose an evaluation method based on Kendall's \u03c4, a metric of rank correlation.", "labels": [], "entities": []}, {"text": "The method is inexpensive, robust, and representation independent.", "labels": [], "entities": []}, {"text": "We show that Kendall's \u03c4 correlates reliably with human ratings and reading times.", "labels": [], "entities": []}], "introductionContent": [{"text": "The systematic evaluation of natural language processing (NLP) systems is an important prerequisite for assessing their quality and improving their performance.", "labels": [], "entities": []}, {"text": "Traditionally, human involvement is called for in evaluating systems that generate textual output.", "labels": [], "entities": []}, {"text": "Examples include text generation, summarization, and, notably, machine translation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.8307999968528748}, {"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9812228083610535}, {"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8070456385612488}]}, {"text": "Human evaluations consider many aspects of automatically generated texts ranging from grammaticality to content selection, fluency, and readability (.", "labels": [], "entities": [{"text": "content selection", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7065961062908173}]}, {"text": "The relatively high cost of producing human judgments, especially when evaluations must be performed quickly and frequently, has encouraged many researchers to seek ways of evaluating system output automatically.", "labels": [], "entities": []}, {"text": "proposed BLEU, a method for evaluating candidate translations by comparing them against reference translations (using n-gram co-occurrence overlap).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9965305924415588}]}, {"text": "Along the same lines, the content of a system summary can be assessed by measuring its similarity to one or more manual summaries (.", "labels": [], "entities": []}, {"text": "Bangalore, introduce a variety of quantitative measures for evaluating the accuracy of an automatically generated sentence against a reference corpus string.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9960792660713196}]}, {"text": "Despite differences in application and form, automatic evaluation methods usually involve the following desiderata.", "labels": [], "entities": []}, {"text": "First, they measure numeric similarity or closeness of system output to one or several gold standards.", "labels": [], "entities": []}, {"text": "Second, they are inexpensive, robust, and ideally language independent.", "labels": [], "entities": []}, {"text": "Third, correlation with human judgments is an important part of creating and testing an automated metric.", "labels": [], "entities": []}, {"text": "For instance, several studies have shown that BLEU correlates with human ratings on machine translation quality (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9984802603721619}, {"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7548303604125977}]}, {"text": "Bangalore, demonstrate that tree-based evaluation metrics for surface generation correlate significantly with human judgments on sentence quality and understandability.", "labels": [], "entities": [{"text": "surface generation", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7423354685306549}]}, {"text": "Given their simplicity, automatic evaluation methods cannot be considered as a direct replacement for human evaluations (see for discussion on some problematic aspects of BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9665705561637878}]}, {"text": "However, they can be usefully employed during system development, for example, for quickly assessing modeling ideas or for comparing across different system configurations (.", "labels": [], "entities": []}, {"text": "Automatic methods have concentrated on evaluation aspects concerning lexical choice (e.g., words or phrases shared between reference and system translations), content selection (e.g., document units shared between reference and system summaries), and grammaticality (e.g., how many insertions, substitutions, or deletions are required to transform a generated sentence to a reference string).", "labels": [], "entities": [{"text": "content selection", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.6991309225559235}]}, {"text": "Another promising, but, less studied, avenue for automatic evaluation is information ordering.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8843953907489777}]}, {"text": "The task concerns finding an acceptable ordering fora set of preselected information-bearing items.", "labels": [], "entities": []}, {"text": "It is an essential step in concept-to-text generation, multidocument summarization, and other text synthesis problems.", "labels": [], "entities": [{"text": "concept-to-text generation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.756281316280365}, {"text": "multidocument summarization", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.7715504169464111}, {"text": "text synthesis", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.718001127243042}]}, {"text": "Depending on the application and domain at hand, the items to be ordered may vary greatly from propositions to trees) or sentences (Lapata 2003;.", "labels": [], "entities": []}, {"text": "It is therefore not surprising that evaluation methods have concentrated primarily on the generated orders, thus abstracting away from the items themselves.", "labels": [], "entities": []}, {"text": "More concretely,  proposed the use of Kendall's \u03c4, a measure of rank correlation, as a means of estimating the distance between a system-generated and a human-generated gold-standard order.", "labels": [], "entities": []}, {"text": "Rank correlation is an appealing way of evaluating information ordering: It is a well-understood and widely used measure of the strength of association between two variables; it is computed straightforwardly and can operate over distinct linguistic units (e.g., sentences, trees, or propositions).", "labels": [], "entities": [{"text": "information ordering", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.6659077703952789}]}, {"text": "Indeed, several studies have adopted Kendall's \u03c4 as a performance measure for evaluating the output of information-ordering components both in the context of concept-to-text generation) and summarization.", "labels": [], "entities": [{"text": "concept-to-text generation", "start_pos": 158, "end_pos": 184, "type": "TASK", "confidence": 0.7489294111728668}, {"text": "summarization", "start_pos": 190, "end_pos": 203, "type": "TASK", "confidence": 0.9880679249763489}]}, {"text": "Despite its growing popularity, no study to date has investigated whether Kendall's \u03c4 correlates with human judgments on the information-ordering task.", "labels": [], "entities": []}, {"text": "This is in marked contrast with other automatic evaluation methods that have been shown to correlate with human assessments.", "labels": [], "entities": []}, {"text": "In this article, we aim to rectify this and undertake two studies that examine whether there is indeed a relationship between \u03c4 and behavioral data.", "labels": [], "entities": []}, {"text": "We first briefly introduce Kendall's \u03c4 and explain how it can be employed for evaluating information ordering (Section 2).", "labels": [], "entities": [{"text": "evaluating information ordering", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.6790842513243357}]}, {"text": "Next, we present a controlled experimental study that examines whether Kendall's \u03c4 is correlated with human ratings (Section 3).", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.5628578464190165}]}, {"text": "A commonly raised criticism of the judgment elicitation methodology is that it is not fine-grained enough to rule out possible confounds.", "labels": [], "entities": []}, {"text": "In the information-ordering task, for example, we cannot be certain that subjects rate a document low because it is genuinely badly organized and, therefore, difficult to comprehend or because they are unfamiliar with its content or simply disinterested or distracted.", "labels": [], "entities": []}, {"text": "Similar confounds also arise in the evaluation of the output of MT systems, where it maybe difficult to tease apart whether subjects' ratings reflect their assessment of the quality of the translated text or its subject matter and structure.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9863945245742798}]}, {"text": "To eliminate such confounds, we follow our judgment elicitation study with an on-line reading experiment and demonstrate that \u03c4 is also correlated with processing time (Section 4).", "labels": [], "entities": []}, {"text": "Our second experiment provides Lapata Automatic Evaluation of Information Ordering additional evidence for the validity of \u03c4 as a measure of text well-formedness.", "labels": [], "entities": [{"text": "validity", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9540911316871643}]}, {"text": "Discussion of our results concludes the article.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess whether Kendall's \u03c4 reliably correlates with human ratings, it is necessary to have access to several different orderings of the same input.", "labels": [], "entities": []}, {"text": "In what follows we Lapata Automatic Evaluation of Information Ordering describe our method for assembling a set of experimental materials and collecting human judgments.", "labels": [], "entities": [{"text": "Automatic Evaluation of Information Ordering", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.535692447423935}]}, {"text": "A potential criticism of our previous study is that it is based solely on ratings.", "labels": [], "entities": []}, {"text": "The problem with this off-line measure is that it indicates whether participants find a text easy or difficult to comprehend, without, however, isolating the causes for this difficulty.", "labels": [], "entities": []}, {"text": "For example, the ratings may reflect not only what subjects think about how a text is organized but also their (un)familiarity with its genre or style, their lack of attention, or disinterest in the subject matter.", "labels": [], "entities": []}, {"text": "To ascertain that this is not the case, we conducted a follow-up experiment whose aim was to explore the relationship between Kendall's \u03c4 and processing effort.", "labels": [], "entities": []}, {"text": "Much work in psychology indicates that low-coherence texts require more inferences and therefore take longer to read.", "labels": [], "entities": []}, {"text": "If Kendall's \u03c4 does indeed capture aspects of overall document organization and coherence, then documents assigned a high \u03c4 value should take less time to read than documents with low \u03c4 values.", "labels": [], "entities": []}, {"text": "Unlike ratings, reading times are an immediate measure of processing effort that participants cannot consciously control or modulate.", "labels": [], "entities": []}, {"text": "revealed that ratings for these bins were not significantly different.", "labels": [], "entities": []}, {"text": "Our set of materials consisted of 8 \u00d7 3 = 24 texts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Average subject ratings for binned \u03c4 values and descriptive statistics.", "labels": [], "entities": []}, {"text": " Table 4  Mean reading times (in milliseconds) for three experimental conditions.", "labels": [], "entities": [{"text": "Mean reading times", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9578861792882284}]}]}