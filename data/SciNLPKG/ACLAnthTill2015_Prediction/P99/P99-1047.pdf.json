{"title": [{"text": "A Decision-Based Approach to Rhetorical Parsing", "labels": [], "entities": [{"text": "Rhetorical Parsing", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8201860189437866}]}], "abstractContent": [{"text": "We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences.", "labels": [], "entities": [{"text": "shift-reduce rhetorical parsing", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6713487108548483}]}, {"text": "The algorithm exploits robust lexical, syntactic , and semantic knowledge sources.", "labels": [], "entities": []}, {"text": "I Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997).", "labels": [], "entities": []}, {"text": "In this paper , we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts.", "labels": [], "entities": [{"text": "rhetorical parser", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.730020135641098}]}, {"text": "Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning.", "labels": [], "entities": []}, {"text": "Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees.", "labels": [], "entities": []}, {"text": "2 The Corpus We used a corpus of 90 rhetorical structure trees, which were built manually using rhetorical relations that were defined informally in the style of Mann and Thompson (1988): 30 trees were built for short personal news stories from the MUC7 co-reference corpus (Hirschman and Chinchor, 1997); 30 trees for scientific texts from the Brown corpus; and 30 trees for editorials from the Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "MUC7 co-reference corpus", "start_pos": 249, "end_pos": 273, "type": "DATASET", "confidence": 0.8186859091122946}, {"text": "Brown corpus", "start_pos": 345, "end_pos": 357, "type": "DATASET", "confidence": 0.8891293704509735}, {"text": "Wall Street Journal (WSJ)", "start_pos": 396, "end_pos": 421, "type": "DATASET", "confidence": 0.9053522745768229}]}, {"text": "The average number of words for each text was 405 in the MUC corpus, 2029 in the Brown corpus, and 878 in the WSJ corpus.", "labels": [], "entities": [{"text": "MUC corpus", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9759787321090698}, {"text": "Brown corpus", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.9926645159721375}, {"text": "WSJ corpus", "start_pos": 110, "end_pos": 120, "type": "DATASET", "confidence": 0.9850149154663086}]}, {"text": "Each MUC text 365 was tagged by three annotators; each Brown and WSJ text was tagged by two annotators.", "labels": [], "entities": [{"text": "MUC text 365", "start_pos": 5, "end_pos": 17, "type": "DATASET", "confidence": 0.8569922049840292}, {"text": "Brown and WSJ text", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.7903797030448914}]}, {"text": "The rhetorical structure assigned to each text is a (possibly non-binary) tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans.", "labels": [], "entities": []}, {"text": "Each internal node is characterized by a rhetorical relation, such as ELABORATION and CONTRAST.", "labels": [], "entities": [{"text": "ELABORATION", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.9519182443618774}, {"text": "CONTRAST", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.6074017882347107}]}, {"text": "Each relation holds between two non-overlapping text spans called NUCLEUS and SATELLITE.", "labels": [], "entities": [{"text": "NUCLEUS", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.8116798400878906}, {"text": "SATELLITE", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9395052194595337}]}, {"text": "(There area few exceptions to this rule: some relations, such as SEQUENCE and CONTRAST, are multinu-clear.)", "labels": [], "entities": [{"text": "SEQUENCE", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.7241016626358032}, {"text": "CONTRAST", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.7415430545806885}]}, {"text": "The distinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer's purpose than the satellite.", "labels": [], "entities": []}, {"text": "Each node in the tree is also characterized by a promotion set that denotes the units that are important in the corresponding subtree.", "labels": [], "entities": []}, {"text": "The promotion sets of leaf nodes are the leaves themselves.", "labels": [], "entities": []}, {"text": "The promotion sets of internal nodes are given by the union of the promotion sets of the immediate nuclei nodes.", "labels": [], "entities": []}, {"text": "Edus are defined functionally as clauses or clause-like units that are unequivocally the NUCLEUS or SATELLITE of a rhetorical relation that holds between two adjacent spans of text.", "labels": [], "entities": [{"text": "NUCLEUS", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.8230115175247192}, {"text": "SATELLITE", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9253420829772949}]}, {"text": "For example , \"because of the low atmospheric pressure\" in text (1) is not a fully fleshed clause.", "labels": [], "entities": []}, {"text": "However, since it is the SATELLITE of an EXPLANATION relation , we treat it as elementary.", "labels": [], "entities": [{"text": "SATELLITE", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.995961606502533}]}, {"text": "[Only the midday sun at tropical latitudes is warm enough] [to thaw ice on occasion,] [but any liquid water formed in this way would evaporate almost instantly] [because of the low atmospheric pressure.]", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The shift-reduce action identifier uses the C4.5 program in order to learn decision trees and rules that specify how discourse segments should be assembled into trees.", "labels": [], "entities": []}, {"text": "In general, the tree-based classifiers performed slightly better than the rule-based classitiers.", "labels": [], "entities": []}, {"text": "Due to space constraints, we present here only performance results that concern the tree classifiers.", "labels": [], "entities": []}, {"text": "displays the accuracy of the shift-reduce action identifiers, determined for each of the three corpora by means of a ten-fold cross-validation procedure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9994744658470154}]}, {"text": "In table 3, the B3 column gives the accuracy of a majority-based classifier, which chooses action SHIFT in all cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9996026158332825}]}, {"text": "Since choosing only the action SHIFT never produces a discourse tree, in column B4, we present the accuracy of a baseline classifier that chooses shift-reduce operations randomly, with probabilities that reflect the probability distribution of the operations in each corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9994895458221436}]}, {"text": "shows the learning curve that corresponds to the MUC corpus.", "labels": [], "entities": [{"text": "MUC corpus", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.9175491631031036}]}, {"text": "As in the case of the discourse segmenter, this learning curve also suggests that more data can increase the accuracy of the shift-reduce action identifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9988117218017578}]}, {"text": "Obviously, by applying the two classifiers sequentiaUy, one can derive the rhetorical structure of any: Performance of the rhetorical parser: labeled (R)ecall and (P)recision.", "labels": [], "entities": []}, {"text": "The segmenter is either Decision-Tree-Based (DT) or Manual (M). text.", "labels": [], "entities": []}, {"text": "Unfortunately, the performance results presented in sections 4 and 5 only suggest how well the discourse segmenter and the shift-reduce action identifier perform with respect to individual cases.", "labels": [], "entities": []}, {"text": "They say nothing about the performance of a rhetorical parser that relies on these classifiers.", "labels": [], "entities": [{"text": "rhetorical parser", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8679099977016449}]}, {"text": "In order to evaluate the rhetorical parser as a whole, we partitioned randomly each corpus into two sets of texts: 27 texts were used for training and the last 3 texts were used for testing.", "labels": [], "entities": [{"text": "rhetorical parser", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8981567025184631}]}, {"text": "The evaluation employs labeled recall and precision measures, which are extensively used to study the performance of syntactic parsers.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9839460253715515}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9980388283729553}]}, {"text": "Labeled recall reflects the number of correctly labeled constituents identified by the rhetorical parser with respect to the number of labeled constituents in the corresponding manually built tree.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.8045153021812439}]}, {"text": "Labeled precision reflects the number of correctly labeled constituents identified by the rhetorical parser with respect to the total number of labeled constituents identified by the parser.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9148492813110352}]}, {"text": "We computed labeled recall and precision figures with respect to the ability of our discourse parser to identify elementary units, hierarchical text spans, text span nuclei and satellites, and rhetorical relations.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9745718836784363}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9888729453086853}]}, {"text": "displays results obtained using segmenters and shift-reduce action identifiers that were trained either on 27 texts from each corpus and tested on 3 unseen texts from the same corpus; or that were trained on 27\u00d73 texts from all corpora and tested on 3 unseen texts from each corpus.", "labels": [], "entities": []}, {"text": "The training and test texts were chosen randomly.", "labels": [], "entities": []}, {"text": "Table 4 also displays results obtained using a manual discourse segmenter, which identified correctly all edus.", "labels": [], "entities": [{"text": "discourse segmenter", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7605198323726654}]}, {"text": "Since all texts in our corpora were manually annotated by multiple judges, we could also 371 compute an upper-bound of the performance of the rhetorical parser by calculating for each text in the test corpus and each judge the average labeled recall and precision figures with respect to the discourse trees built by the other judges.", "labels": [], "entities": [{"text": "recall", "start_pos": 243, "end_pos": 249, "type": "METRIC", "confidence": 0.9653880596160889}, {"text": "precision", "start_pos": 254, "end_pos": 263, "type": "METRIC", "confidence": 0.9971432089805603}]}, {"text": "displays these upper-bound figures as well.", "labels": [], "entities": []}, {"text": "The results in table 4 primarily show that errors in the discourse segmentation stage affect significantly the quality of the trees our parser builds.", "labels": [], "entities": []}, {"text": "When a segmenter is trained only on 27 texts (especially for the MUC and WSJ corpora, which have shorter texts than the Brown corpus), it has very low performance.", "labels": [], "entities": [{"text": "MUC and WSJ corpora", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.7610041499137878}, {"text": "Brown corpus", "start_pos": 120, "end_pos": 132, "type": "DATASET", "confidence": 0.9288484454154968}]}, {"text": "Many of the intra-sentential edu boundaries are not identified, and as a consequence, the overall performance of the parser is low.", "labels": [], "entities": []}, {"text": "When the segmenter is trained on 27 \u00d7 3 texts, its performance increases significantly with respect to the MUC and WSJ corpora, but decreases with respect to the Brown corpus.", "labels": [], "entities": [{"text": "MUC", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9370384216308594}, {"text": "WSJ corpora", "start_pos": 115, "end_pos": 126, "type": "DATASET", "confidence": 0.7920891046524048}, {"text": "Brown corpus", "start_pos": 162, "end_pos": 174, "type": "DATASET", "confidence": 0.9743051826953888}]}, {"text": "This can be explained by the significant differences in style and discourse marker usage between the three corpora.", "labels": [], "entities": []}, {"text": "When a perfect segmenter is used, the rhetorical parser determines hierarchical constituents and assigns them a nuclearity status at levels of performance that are not far from those of humans.", "labels": [], "entities": []}, {"text": "However, the rhetorical labeling of discourse spans is even in this case about 15-20% below human performance.", "labels": [], "entities": [{"text": "rhetorical labeling of discourse spans", "start_pos": 13, "end_pos": 51, "type": "TASK", "confidence": 0.899386465549469}]}, {"text": "These results suggest that the features that we use are sufficient for determining the hierarchical structure of texts and the nuclearity statuses of discourse segments.", "labels": [], "entities": []}, {"text": "However, they are insufficient for determining correctly the elementary units of discourse and the rhetorical relations that hold between discourse segments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of a discourse segmenter that  uses a decision-tree, non-binary classifier.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the tree-based, shift-reduce  action classifiers.", "labels": [], "entities": []}, {"text": " Table 4: Performance of the rhetorical parser: labeled (R)ecall and (P)recision. The segmenter is either  Decision-Tree-Based (DT) or Manual (M).", "labels": [], "entities": [{"text": "rhetorical parser", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9005308151245117}]}]}