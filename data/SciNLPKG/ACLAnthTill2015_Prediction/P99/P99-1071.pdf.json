{"title": [{"text": "Information Fusion in the Context of Multi-Document Summarization", "labels": [], "entities": [{"text": "Information Fusion", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7838569283485413}, {"text": "Summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.7678245306015015}]}], "abstractContent": [{"text": "We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.", "labels": [], "entities": []}, {"text": "Our approach is unique in its usage of language generation to reformulate the wording of the summary.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information overload has created an acute need for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9904078841209412}]}, {"text": "Typically, the same information is described by many different online documents.", "labels": [], "entities": []}, {"text": "Hence, summaries that synthesize common information across documents and emphasize the differences would significantly help readers.", "labels": [], "entities": []}, {"text": "Such a summary would be beneficial, for example, to a user who follows a single event through several newswires.", "labels": [], "entities": []}, {"text": "In this paper, we present research on the automatic fusion of similar information across multiple documents using language generation to produce a concise summary.", "labels": [], "entities": [{"text": "automatic fusion of similar information across multiple documents", "start_pos": 42, "end_pos": 107, "type": "TASK", "confidence": 0.7847079113125801}]}, {"text": "We propose a method for summarizing a specific type of input: news articles presenting different descriptions of the same event.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9771344065666199}]}, {"text": "Hundreds of news stories on the same event are produced daily by news agencies.", "labels": [], "entities": []}, {"text": "Repeated information about the event is a good indicator of its importancy to the event, and can be used for summary generation.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.9058982133865356}]}, {"text": "Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (.", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.5705698529879252}, {"text": "sentence extraction", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7181185334920883}]}, {"text": "In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information.", "labels": [], "entities": []}, {"text": "Extracting all similar sentences would produce a verbose and repetitive summary, while extracting some similar sentences could produce a summary biased towards some sources.", "labels": [], "entities": []}, {"text": "Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7474735677242279}]}, {"text": "Our work is part of a full summarization system, which extracts sets of similax sentences, themes), in the first stage for input to the components described here.", "labels": [], "entities": []}, {"text": "Our model for multi-document summarization represents a number of departures from traditional language generation.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.5680964291095734}]}, {"text": "Typically, language generation systems have access to a full semantic representation of the domain.", "labels": [], "entities": []}, {"text": "A content planner selects and orders propositions from an underlying knowledge base to form text content.", "labels": [], "entities": []}, {"text": "A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure.", "labels": [], "entities": []}, {"text": "Our approach differs in the following ways: Content planning operates overfull sentences, producing sentence fragments.", "labels": [], "entities": [{"text": "Content planning", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.8525840640068054}]}, {"text": "Thus, content planning straddles the border between interpretation and generation.", "labels": [], "entities": [{"text": "content planning", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7773226201534271}]}, {"text": "We preprocess the similar sentences using an existing shallow parser and a mapping to predicateargument structure.", "labels": [], "entities": []}, {"text": "The content planner finds an intersection of phrases by comparing the predicate-argument structures; through this process it selects the phrases that can adequately convey the common information of the theme.", "labels": [], "entities": []}, {"text": "It also orders selected phrases and augments them with On 3th of September 1995, 120 hostages were released by Bosnian Serbs.", "labels": [], "entities": [{"text": "3th of September 1995", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.8632745146751404}]}, {"text": "Serbs were holding over 250 U.N. personnel.", "labels": [], "entities": []}, {"text": "Bosnian serb leader Radovan Karadjic said he expected \"a sign of goodwill\" from the international community.", "labels": [], "entities": []}, {"text": "U.S. F-16 fighter jet was shot down by Bosnian ! Serbs.", "labels": [], "entities": [{"text": "Bosnian ! Serbs", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9401672085126241}]}, {"text": "Electronic beacon signals, which might have been i transmitted by a downed U.S. fighter pilot in Bosnia, were no longer being received.", "labels": [], "entities": []}, {"text": "After six days, O'Grady, downed pilot, was rescued by Marine force.", "labels": [], "entities": [{"text": "O'Grady", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7566201090812683}]}, {"text": "The mission was carried out by CH-53 helicopters with an escort of missile-and rocket-armed Cobra helicopters.", "labels": [], "entities": []}, {"text": "Figure 1: Summary produced by our system using 12 news articles as input.", "labels": [], "entities": []}, {"text": "information needed for clarification (entity descriptions, temporal references, and newswire source references).", "labels": [], "entities": []}, {"text": "Sentence generation begins with phrases.", "labels": [], "entities": [{"text": "Sentence generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9679497480392456}]}, {"text": "Our task is to produce fluent sentences that combine these phrases, arranging them in novel contexts.", "labels": [], "entities": []}, {"text": "In this process, new grammatical constraints maybe imposed and paraphrasing maybe required.", "labels": [], "entities": []}, {"text": "We developed techniques to map predicateargument structure produced by the content-planner to the functional representation expected by FUF/SURGE and to integrate new constraints on realization choice, using surface features in place of semantic or pragmatic ones typically used in sentence generation.", "labels": [], "entities": [{"text": "FUF/SURGE", "start_pos": 136, "end_pos": 145, "type": "DATASET", "confidence": 0.7728427251180013}, {"text": "sentence generation", "start_pos": 282, "end_pos": 301, "type": "TASK", "confidence": 0.739328920841217}]}, {"text": "An example summary automatically generated by the system from our corpus of themes is shown in.", "labels": [], "entities": []}, {"text": "We collected a corpus of themes, that was divided into a training portion and a testing portion.", "labels": [], "entities": []}, {"text": "We used the training data for identification of paraphrasing rules on which our comparison algorithm is built.", "labels": [], "entities": []}, {"text": "The system we describe has been fully implemented and tested on a variety of input articles; there are, of course, many open research issues that we are continuing to explore.", "labels": [], "entities": []}, {"text": "In the following sections, we provide an overview of existing multi-document summarization systems, then we will detail our sentence comparison technique, and describe the sentence generation component.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.5931059271097183}, {"text": "sentence comparison", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.729424238204956}, {"text": "sentence generation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.717582568526268}]}, {"text": "We provide examples of generated summaries and conclude with a discussion of evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of multi-document summarization is difficult.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.5406952053308487}]}, {"text": "First, we have not yet found an existing collection of human written summaries of multiple documents which could serve as a gold standard.", "labels": [], "entities": []}, {"text": "We have begun a joint project with the Columbia Journalism School which will provide such data in the future.", "labels": [], "entities": [{"text": "Columbia Journalism School", "start_pos": 39, "end_pos": 65, "type": "DATASET", "confidence": 0.8893179694811503}]}, {"text": "Second, methods used for evaluation of extraction-based systems are not applicable fora system which involves text regeneration.", "labels": [], "entities": [{"text": "text regeneration", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7893382608890533}]}, {"text": "Finally, the manual effort needed to develop test beds and to judge sys-tem output is far more extensive than for single document summarization; consider that a human judge would have to read many input articles (our largest test set contained 27 input articles) to rate the validity of a summary.", "labels": [], "entities": []}, {"text": "Consequently, the evaluation that we performed to date is limited.", "labels": [], "entities": []}, {"text": "We performed a quantitative evaluation of our content-selection component.", "labels": [], "entities": []}, {"text": "In order to prevent noisy input from the theme construction component from skewing the evaluation, we manually constructed 26 themes, each containing 4 sentences on average.", "labels": [], "entities": []}, {"text": "Far more training data is needed to tune the generation portion.", "labels": [], "entities": []}, {"text": "While we have tuned the system to perform with minor errors on the manual set of themes we have created (the missing article in the fourth sentence of the summary in is an example), we need more robust input data from the theme construction component, which is still underdevelopment, to train the generator before beginning large scale testing.", "labels": [], "entities": []}, {"text": "One problem in improving output is determining how to recover from errors in tools used in early stages of the process, such as the tagger and the parser.", "labels": [], "entities": []}], "tableCaptions": []}