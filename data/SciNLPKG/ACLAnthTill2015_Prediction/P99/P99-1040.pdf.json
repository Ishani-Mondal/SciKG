{"title": [{"text": "Automatic Detection of Poor Speech Recognition at the Dialogue Level", "labels": [], "entities": [{"text": "Automatic Detection of Poor Speech Recognition", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7718659291664759}, {"text": "Dialogue Level", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.8724460005760193}]}], "abstractContent": [{"text": "The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction.", "labels": [], "entities": []}, {"text": "An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand.", "labels": [], "entities": []}, {"text": "To do so, a system must be able to identify dialogue properties that suggest adaptation.", "labels": [], "entities": []}, {"text": "This paper focuses on identifying situations where the speech recognizer is performing poorly.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.6892207562923431}]}, {"text": "We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations.", "labels": [], "entities": []}, {"text": "Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Builders of spoken dialogue systems face a number of fundamental design choices that strongly influence both performance and user satisfaction.", "labels": [], "entities": []}, {"text": "Examples include choices between user, system, or mixed initiative, and between explicit and implicit confirmation of user commands.", "labels": [], "entities": []}, {"text": "An ideal system wouldn't make such choices a priori, but rather would adapt to the circumstances at hand.", "labels": [], "entities": []}, {"text": "For instance, a system detecting that a user is repeatedly uncertain about what to say might move from user to system initiative, and a system detecting that speech recognition performance is poor might switch to a dialogUe strategy with more explicit prompting, an explicit confirmation mode, or keyboard input mode.", "labels": [], "entities": []}, {"text": "Any of these adaptations might have been appropriate in dialogue D1 from the Annie system), shown in.", "labels": [], "entities": [{"text": "Annie system", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.9397430121898651}]}, {"text": "In order to improve performance through such adaptation, a system must first be able to identify, in real time, salient properties of an ongoing dialogue that call for some useful change in system strategy.", "labels": [], "entities": []}, {"text": "In other words, adaptive systems should try to automatically identify actionable properties of ongoing dialogues.", "labels": [], "entities": []}, {"text": "Previous work has shown that speech recognition performance is an important predictor of user satisfaction, and that changes in dialogue behavior impact speech recognition performance ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7643774151802063}, {"text": "speech recognition", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.7377660274505615}]}, {"text": "Therefore, in this work, we focus on the task of automatically detecting poor speech recognition performance in several spoken dialogue systems developed at AT&T Labs.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.6681921631097794}]}, {"text": "Rather than hand-crafting rules that classify speech recognition performance in an ongoing dialogue, we take a machine learning approach.", "labels": [], "entities": [{"text": "classify speech recognition performance", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.7025110051035881}]}, {"text": "We begin with a collection of system logs from actual dialogues that were labeled by humans as having had \"good\" or \"bad\" speech recognition (the training set).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7877615392208099}]}, {"text": "We then apply standard machine learning algorithms to this training set in the hope of discovering, in a principled manner, classifiers that can automatically detect poor speech recognition during novel dialogues.", "labels": [], "entities": [{"text": "speech recognition during novel dialogues", "start_pos": 171, "end_pos": 212, "type": "TASK", "confidence": 0.856698501110077}]}, {"text": "In order to train such classifiers, we must provide them with a number of \"features\" of dialogues derived from the system logs that might allow the system to automatically identify poor recognition performance.", "labels": [], "entities": []}, {"text": "In addition to identifying features that provide the best quantitative solutions, we are also interested in comparing the performance of classifiers derived solely from acoustic features or from \"high-level\" dialogue features, and from combinations of these and other feature types.", "labels": [], "entities": []}, {"text": "Note that we are free to invent as many features as we like, as long as they can be computed in real time from the raw system logs.", "labels": [], "entities": []}, {"text": "Since the dialogue systems we examine use automatic speech recognition (ASR), one obvious feature available in the system log is a per-utterance score from the speech recognizer representing its \"confidence\" in its interpretation of the user's utterance.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.801261305809021}]}, {"text": "For dialogue D1, the recognizer's output and the associated confidence scores", "labels": [], "entities": [{"text": "dialogue D1", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7336459457874298}]}], "datasetContent": [{"text": "The final input for learning is training data, i.e., a representation of a set of dialogues in terms of feature and class values.", "labels": [], "entities": []}, {"text": "In order to induce classification rules from a variety of feature representations our training data is represented differently in different experiments.", "labels": [], "entities": []}, {"text": "Our learning experiments can be roughly categorized as follows.", "labels": [], "entities": []}, {"text": "First, examples are represented using all of the features in (to evaluate the optimal level of performance).", "labels": [], "entities": []}, {"text": "shows how Dialogue D1 from is represented using all 23 features.", "labels": [], "entities": []}, {"text": "Next, examples are represented using only the features in a single knowledge source (to comparatively evaluate the utility of each knowledge source for classification), as well as using features from two or more knowledge sources (to gain insight into the interactions between knowledge sources).", "labels": [], "entities": []}, {"text": "Finally, examples are represented using feature sets corresponding to hypotheses in the literature (to empirically test theoretically motivated proposals).", "labels": [], "entities": []}, {"text": "The output of each machine learning experiment is a classification model learned from the training data.", "labels": [], "entities": []}, {"text": "To evaluate these results, the error rates of the learned classification models are estimated using the resampling method of cross-validation.", "labels": [], "entities": []}, {"text": "In 25-fold cross-validation, the total set of examples is randomly divided into 25 disjoint test sets, and 25 runs of the learning program are performed.", "labels": [], "entities": []}, {"text": "Thus, each run uses the exampies not in the test set for training and the remaining examples for testing.", "labels": [], "entities": []}, {"text": "An estimated error rate is obtained by averaging the error rate on the testing portion of the data from each of the 25 runs..", "labels": [], "entities": [{"text": "error rate", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9676339328289032}, {"text": "error rate", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9715718626976013}]}], "tableCaptions": []}