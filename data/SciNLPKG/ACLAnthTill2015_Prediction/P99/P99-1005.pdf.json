{"title": [{"text": "Distributional Similarity Models: Clustering vs. Nearest Neighbors", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional similarity is a useful notion in estimating the probabilities of rare joint events.", "labels": [], "entities": []}, {"text": "It has been employed both to cluster events according to their distributions, and to directly compute averages of estimates for distributional neighbors of a target event.", "labels": [], "entities": []}, {"text": "Here, we examine the tradeoffs between model size and prediction accuracy for cluster-based and nearest neighbors distributional models of unseen events.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9663782715797424}]}], "introductionContent": [{"text": "In many statistical language-processing problems, it is necessary to estimate the joint probability or cooeeurrence probability of events drawn from two prescribed sets.", "labels": [], "entities": []}, {"text": "Data sparseness can make such estimates difficult when the events under consideration are sufficiently fine-grained, for instance, when they correspond to occurrences of specific words in given configurations.", "labels": [], "entities": []}, {"text": "In particular, in many practical modeling tasks, a substantial fraction of the cooccurrences of interest have never been seen in training data.", "labels": [], "entities": []}, {"text": "In most previous work, this lack of information is addressed by reserving some mass in the probability model for unseen joint events, and then assigning that mass to those events as a function of their marginal frequencies.", "labels": [], "entities": []}, {"text": "An intuitively appealing alternative to relying on marginal frequencies alone is to combine estimates of the probabilities of \"similar\" events.", "labels": [], "entities": []}, {"text": "More specifically, a joint event (x, y) would be considered similar to another (x t, y) if the distributions of Y given x and Y given x' (the cooccurrence distributions of x and x ') meet an appropriate definition of distributional similarity.", "labels": [], "entities": []}, {"text": "For example, one can infer that the bigram \"after ACL-99\" is plausible --even if it has never 33 occurred before --from the fact that the bigram \"after ACL-95\" has occurred, if \"ACL-99\" and \"ACL-95\" have similar cooccurrence distributions.", "labels": [], "entities": []}, {"text": "For concreteness and experimental evaluation, we focus in this paper on a particular type of cooccurrence, that of a main verb and the head noun of its direct object in English text.", "labels": [], "entities": []}, {"text": "Our main goal is to obtain estimates ~(vln ) of the conditional probability of a main verb v given a direct object head noun n, which can then be used in particular prediction tasks.", "labels": [], "entities": []}, {"text": "In previous work, we and our co-authors have proposed two different probability estimation methods that incorporate word similarity information: distributional clustering and nearestneighbors averaging.", "labels": [], "entities": []}, {"text": "Distributional clustering) assigns to each word a probability distribution over clusters to which it may belong, and characterizes each cluster by a centroid, which is an average of cooccurrence distributions of words weighted according to cluster membership probabilities.", "labels": [], "entities": [{"text": "Distributional clustering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7352561950683594}]}, {"text": "Cooccurrence probabilities can then be derived from either a membership-weighted average of the clusters to which the words in the cooccurrence belong, or just from the highest-probability cluster.", "labels": [], "entities": []}, {"text": "In contrast, nearest-neighbors averaging 1) does not explicitly cluster words.", "labels": [], "entities": []}, {"text": "Rather, a given cooccurrence probability is estimated by averaging probabilities for the set of cooccurrences most similar to the target cooccurrence.", "labels": [], "entities": []}, {"text": "That is, while both methods involve appealing to similar \"witnesses\" (in the clustering case, these witnesses are the centroids; for nearest-neighbors averaging, they are the most similar words), in nearest-neighbors averaging the witnesses vary for different cooccurrences, whereas in distributional clustering the same set of witnesses is used for every cooccurrence (see).", "labels": [], "entities": []}, {"text": "We thus see that distributional clustering and nearest-neighbors averaging are complementary approaches.", "labels": [], "entities": []}, {"text": "Distributional clustering generally creates a compact representation of the data, namely, the cluster membership probability tables and the cluster centroids.", "labels": [], "entities": [{"text": "Distributional clustering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.804579883813858}]}, {"text": "Nearestneighbors averaging, on the other hand, associates a specific set of similar words to each word and thus typically increases the amount of storage required.", "labels": [], "entities": []}, {"text": "Ina way, it is clustering taken to the limit -each word forms its own cluster.", "labels": [], "entities": []}, {"text": "In previous work, we have shown that both distributional clustering and nearest-neighbors averaging can yield improvements of up to 40% with respect to state-of-the-art backoffmethod in the prediction of unseen cooccurrences.", "labels": [], "entities": []}, {"text": "In the case of nearest-neighbors averaging, we have also demonstrated perplexity reductions of 20% and statistically significant improvement in speech recognition error rate.", "labels": [], "entities": [{"text": "speech recognition error rate", "start_pos": 144, "end_pos": 173, "type": "TASK", "confidence": 0.7029566317796707}]}, {"text": "Furthermore, each method has generated some discussion in the literature ().", "labels": [], "entities": []}, {"text": "Given the relative success of these methods and their complementarity, it is natural to wonder how they compare in practice.", "labels": [], "entities": []}, {"text": "Several authors) have suggested that clustering methods, by reducing data to a small set of representatives, might perform less well than nearest-neighbors averaging-type methods.", "labels": [], "entities": []}, {"text": "For instance, argue: This [class-based] approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture \"typical\" properties of classes of words.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.7438844740390778}]}, {"text": "it is not clear that word co-occurrence patterns can be generalized to class cooccurrence parameters without losing too much information.", "labels": [], "entities": []}, {"text": "Furthermore, early work on class-based language models was inconclusive (.", "labels": [], "entities": []}, {"text": "In this paper, we present a detailed comparison of distributional clustering and nearestneighbors averaging on several large datasets, exploring the tradeoff in similarity-based modeling between memory usage on the one hand and estimation accuracy on the other.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.5977335572242737}]}, {"text": "We find that the performances of the two methods are in general very similar: with respect to Katz's back-off, they both provide average error reductions of up to 40% on one task and up to 7% on a related, but somewhat more difficult, task.", "labels": [], "entities": [{"text": "error reductions", "start_pos": 137, "end_pos": 153, "type": "METRIC", "confidence": 0.9700931012630463}]}, {"text": "Only in a fairly unrealistic setting did nearestneighbors averaging clearly beat distributional clustering, but even in this case, both methods were able to achieve average error reductions of at least 18% in comparison to backoff.", "labels": [], "entities": []}, {"text": "Therefore, previous claims that clustering methods are necessarily inferior are not strongly supported by the evidence of these experiments, although it is of course possible that the situation maybe different for other tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data for the three types of experiments. All numbers are averages over the ten splits.", "labels": [], "entities": []}]}