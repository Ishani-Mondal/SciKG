{"title": [{"text": "A Second-Order Hidden Markov Model for Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.8085136711597443}]}], "abstractContent": [{"text": "This paper describes an extension to the hidden Markov model for part-of-speech tagging using second-order approximations for both contex-tual and lexical probabilities.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.6761317402124405}]}, {"text": "This model increases the accuracy of the tagger to state of the art levels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995427131652832}]}, {"text": "These approximations make use of more contextual information than standard statistical systems.", "labels": [], "entities": []}, {"text": "New methods of smoothing the estimated probabilities are also introduced to address the sparse data problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech tagging is the act of assigning each word in a sentence a tag that describes how that word is used in the sentence.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7612307369709015}]}, {"text": "Typically, these tags indicate syntactic categories, such as noun or verb, and occasionally include additional feature information, such as number (singular or plural) and verb tense.", "labels": [], "entities": []}, {"text": "The Penn Treebank documentation) defines a commonly used set of tags.", "labels": [], "entities": [{"text": "Penn Treebank documentation", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.9942139983177185}]}, {"text": "Part-of-speech tagging is an important research topic in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7998230457305908}, {"text": "Natural Language Processing (NLP)", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.6980698506037394}]}, {"text": "Taggers are often preprocessors in NLP systems, making accurate performance especially important.", "labels": [], "entities": [{"text": "accurate", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9530492424964905}]}, {"text": "Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs),; rule-based systems,; memory-based systems (; maximum-entropy systems; path voting constraint systems; linear separator systems; and majority voting systems.", "labels": [], "entities": [{"text": "tagging", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9670112729072571}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.8352090120315552}]}, {"text": "This paper describes various modifications to an HMM tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers.", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.8371718227863312}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9988896250724792}]}, {"text": "This improvement comes from using secondorder approximations of the Markov assumptions.", "labels": [], "entities": []}, {"text": "Section 2 discusses a basic first-order hidden Markov model for part-of-speech tagging and extensions to that model to handle out-oflexicon words.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6873680949211121}]}, {"text": "The new second-order HMM is described in Section 3, and Section 4 presents experimental results and conclusions.", "labels": [], "entities": [{"text": "HMM", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.868996262550354}]}], "datasetContent": [{"text": "The new tagging model is tested in several different ways.", "labels": [], "entities": []}, {"text": "The basic experimental technique is a 10-fold cross validation.", "labels": [], "entities": []}, {"text": "The corpus in question-is randomly split into ten sections with nine of the sections combined to train the tagger and the tenth for testing.", "labels": [], "entities": []}, {"text": "The results of the ten possible training/testing combinations are merged to give an overall accuracy measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9993762373924255}]}, {"text": "The tagger was tested on two corpora--the Brown corpus (from the Treebank II CD-ROM () and the Wall Street Journal corpus (from the same source).", "labels": [], "entities": [{"text": "Brown corpus (from the Treebank II CD-ROM", "start_pos": 42, "end_pos": 83, "type": "DATASET", "confidence": 0.832006610929966}, {"text": "Wall Street Journal corpus", "start_pos": 95, "end_pos": 121, "type": "DATASET", "confidence": 0.9659729301929474}]}, {"text": "Comparing results for taggers can be difficult, especially across different researchers.", "labels": [], "entities": []}, {"text": "Care has been taken in this paper that, when comparing two systems, the comparisons are from experiments that were as similar as possible and that differences are highlighted in the comparison.", "labels": [], "entities": []}, {"text": "First, we compare the results on each corpus of four different versions of our HMM tagger: a standard (bigram) HMM tagger, an HMM using second-order lexical probabilities, an HMM using second-order contextual probabilities (a standard trigram tagger), and a full secondorder HMM tagger.", "labels": [], "entities": []}, {"text": "The results from both corpora for each tagger are given in.", "labels": [], "entities": []}, {"text": "As might be expected, the full second-order HMM had the highest accuracy levels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9995132684707642}]}, {"text": "The model using only second-order contextual information (a standard trigram model) was second best, the model using only second-order lexical information was third, and the standard bigram HMM had the lowest accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 209, "end_pos": 219, "type": "METRIC", "confidence": 0.9870708584785461}]}, {"text": "The full secondorder HMM reduced the number of errors on known words by around 16% over bigram taggers (raising the accuracy about 0.6-0.7%), and by around 6% over conventional trigram taggets (accuracy increase of about 0.2%).", "labels": [], "entities": [{"text": "number of errors", "start_pos": 37, "end_pos": 53, "type": "METRIC", "confidence": 0.8776270548502604}, {"text": "bigram taggers", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.6509298533201218}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9984825253486633}, {"text": "accuracy increase", "start_pos": 194, "end_pos": 211, "type": "METRIC", "confidence": 0.985503762960434}]}, {"text": "Similar results were seen in the overall accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9873177409172058}]}, {"text": "Unknown word accuracy rates were increased by around 2-3% over bigrams.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.9536638259887695}]}, {"text": "The full second-order HMM tagger is also compared to other researcher's taggers in Table 2.", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.7908848226070404}]}, {"text": "It is important to note that both SNOW, a linear separator model (Roth and Zelenko, THE SECOND-ORDER VITERBI ALGORITHM The variables: \u2022 gp(i,j)= max P(rl,...,rp-2, rp-1 =ti, rp=tj,vl,...vp),2<p<P Tl ~...rTp--2 \u2022 Cp(i,j) = arg max P(rl,...,rp-2, rp-1 = ti,rp = tj,vl,...vp),2 < p < P Tl~...iTp--2 The procedure: 4.", "labels": [], "entities": [{"text": "THE SECOND-ORDER VITERBI ALGORITHM", "start_pos": 84, "end_pos": 118, "type": "METRIC", "confidence": 0.7354864105582237}]}, {"text": "r; = Cp+l (r~+l, r;+2),p = P-2, P-3,...,2,1: Comparison between Full Second-Order HMM and Other Taggers did not include numbers in the lexicon, which accounts for the inflated accuracy on unknown words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9891389012336731}]}, {"text": "compares the accuracies of the taggers on known words, unknown words, and overall accuracy.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9558519124984741}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9977560639381409}]}, {"text": "The table also contains two additional pieces of information.", "labels": [], "entities": []}, {"text": "The first indicates if the corresponding tagger was tested using a closed lexicon (one in which all words appearing in the testing data are known to the tagger) or an open lexicon (not all words are known to.the system).", "labels": [], "entities": []}, {"text": "The second indicates whether a hold-out method (such as cross-validation) was used, and whether the tagger was tested on the entire WSJ corpus or a reduced corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.9624448716640472}]}, {"text": "Two cross-validation tests with the full second-order HMM were run: the first with an open lexicon (created from the training data), and the second where the entire WSJ lexicon was used for each test set.", "labels": [], "entities": [{"text": "WSJ lexicon", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.9170859456062317}]}, {"text": "These two tests allow more direct comparisons between our system and the others.", "labels": [], "entities": []}, {"text": "As shown in the table, the full second-order HMM has improved overall accuracies on the WSJ corpus to state-of-the-art 1The full WSJ is used, but the paper does not indicate whether a cross-vaiidation was performed.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9857496023178101}, {"text": "WSJ corpus", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.9177226424217224}, {"text": "WSJ", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.866441547870636}]}, {"text": "2MBT did not place numbers in the lexicon, so all numbers were treated as unknown words.", "labels": [], "entities": [{"text": "2MBT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.960588276386261}]}, {"text": "aBoth the rule-based and maximum-entropy models use the full WSJ for training/testing with only a single test set.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8811212182044983}]}, {"text": "4SNOW used a fixed subset of WSJ for training and testing with no cross-validation.", "labels": [], "entities": [{"text": "4SNOW", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9301337003707886}, {"text": "WSJ", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9204972982406616}]}, {"text": "5The voting constraints tagger used a subset of WSJ for training and testing with cross-validation.", "labels": [], "entities": [{"text": "voting constraints tagger", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.6315268774827322}, {"text": "WSJ", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.929654598236084}]}, {"text": "levels--96.9% is the greatest accuracy reported on the full WSJ for an experiment using an open lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992930889129639}, {"text": "WSJ", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.890656590461731}]}, {"text": "Finally, using a closed lexicon, the full second-order HMM achieved an accuracy of 98.05%, the highest reported for the WSJ corpus for this type of experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9993940591812134}, {"text": "WSJ corpus", "start_pos": 120, "end_pos": 130, "type": "DATASET", "confidence": 0.975092202425003}]}, {"text": "The accuracy of our system on unknown words is 84.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997450709342957}]}, {"text": "This accuracy was achieved by creating separate classifiers for capitalized, hyphenated, and numeric digit words: tests on the Wall Street Journal corpus with the full secondorder HMM show that the accuracy rate on unknown words without separating these types of words is only 80.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9987334609031677}, {"text": "Wall Street Journal corpus", "start_pos": 127, "end_pos": 153, "type": "DATASET", "confidence": 0.9758236557245255}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9985440969467163}]}, {"text": "6 This is below the performance of our bigram tagger that separates the classifiers.", "labels": [], "entities": []}, {"text": "Unfortunately, unknown word accuracy is still below some of the other systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9875575304031372}]}, {"text": "This maybe due in part to experimental differences.", "labels": [], "entities": []}, {"text": "It should also be noted that some of these other systems use hand-crafted rules for unknown word rules, whereas our system uses only statistical data.", "labels": [], "entities": []}, {"text": "Adding additional rules to our system could result in comparable performance.", "labels": [], "entities": []}, {"text": "Improving our model on unknown words is a major focus of future research.", "labels": [], "entities": []}, {"text": "In conclusion, anew statistical model, the full second-order HMM, has been shown to improve part-of-speech tagging accuracies over current models.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7343945801258087}]}, {"text": "This model makes use of second-order approximations fora hidden Markov model and 8Mikheev (1997) also separates suffix probabilities into different estimates, but fails to provide any data illustrating the implied accuracy increase.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9406748414039612}]}, {"text": "improves the state of the art for taggers with no increase in asymptotic running time over traditional trigram taggers based on the hidden Markov model.", "labels": [], "entities": [{"text": "taggers", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9646366834640503}]}, {"text": "A new smoothing method is also explained, which allows the use of second-order statistics while avoiding sparse data problems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison between Taggers on the Brown and WSJ Corpora", "labels": [], "entities": [{"text": "Brown and WSJ Corpora", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.855721116065979}]}, {"text": " Table 2: Comparison between Full Second-Order HMM and Other Taggers", "labels": [], "entities": []}]}