{"title": [], "abstractContent": [{"text": "This paper describes a program which revises a draft text by aggregating together descriptions of discourse entities, in addition to deleting extraneous information.", "labels": [], "entities": []}, {"text": "In contrast to knowledge-rich sentence aggregation approaches explored in the past, this approach exploits statistical parsing and robust coreference detection.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7108200192451477}, {"text": "coreference detection", "start_pos": 138, "end_pos": 159, "type": "TASK", "confidence": 0.8552050292491913}]}, {"text": "In an evaluation involving revision of topic-related summaries using informativeness measures from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability.", "labels": [], "entities": [{"text": "TIPSTER SUMMAC evaluation", "start_pos": 103, "end_pos": 128, "type": "DATASET", "confidence": 0.8031486868858337}]}], "introductionContent": [{"text": "Authors are familiar with the process of condensing along paper into a shorter one: this is an iterative process, with the results improved over successive drafts.", "labels": [], "entities": []}, {"text": "Professional abstractors carryout substantial revision and editing of abstracts.", "labels": [], "entities": []}, {"text": "We therefore expect revision to be useful in automatic text summarization.", "labels": [], "entities": [{"text": "revision", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9470489025115967}, {"text": "text summarization", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.619928166270256}]}, {"text": "Prior research exploring the use of revision in summarization, e.g.,,,) has focused mainly on structured data as the input.", "labels": [], "entities": [{"text": "summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9337818622589111}]}, {"text": "Here, we examine the use of revision in summarization of text input.", "labels": [], "entities": [{"text": "summarization of text input", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.8615996688604355}]}, {"text": "First, we review some summarization terminology.", "labels": [], "entities": []}, {"text": "In revising draft summaries, these condensation operations, as well as stylistic rewording of sentences, play an important role.", "labels": [], "entities": []}, {"text": "Summaries can be used to indicate what topics are addressed in the source text, and thus can be used to alert the user as to the source content (the indicative function).", "labels": [], "entities": [{"text": "Summaries", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8851072192192078}]}, {"text": "Summaries can also be used to cover the concepts in the source text to the extent possible given the compression requirements for the summary (the informative function).", "labels": [], "entities": []}, {"text": "Summaries can be tailored to a reader's interests and expertise, yielding topicrelated summaries, or they can be aimed at a particular-usually broad -readership community, as in the cash of (so-called) generic summaries.", "labels": [], "entities": []}, {"text": "Revision here applies to generic and topic-related informative summaries, intended for publishing and dissemination.", "labels": [], "entities": []}, {"text": "Summarization can be viewed as a text-totext reduction operation involving three main condensation operations: selection of salient portions of the text, aggregation of information from different portions of the text, and abstraction of specific information with more general information (.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9833263754844666}, {"text": "text-totext reduction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7260247468948364}]}, {"text": "Our approach to revision is to construct an initial draft summary of a source text and then to add to the draft additional background information.", "labels": [], "entities": [{"text": "revision", "start_pos": 16, "end_pos": 24, "type": "TASK", "confidence": 0.9770064949989319}]}, {"text": "Rather than concatenate material in the draft (as surface-oriented, sentence extraction summarizers do), information in the draft is combined and excised based on revision rules involving aggregation and elimination operations.", "labels": [], "entities": [{"text": "sentence extraction summarizers", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.8139439423878988}]}, {"text": "Elimination can increase the amount of compression (summary length/source length) available, while aggregation can potentially gather and draw in relevant background information, in the form of descriptions of discourse entities from different parts of the source.", "labels": [], "entities": []}, {"text": "We therefore hypothesize that these operations can result in packing in more information per unit compression than possible by concatenation.", "labels": [], "entities": []}, {"text": "Rather than opportunistically adding as much background information that can fit in the available compression, as in, our approach adds background information from the source text to the draft based on an information weighting function.", "labels": [], "entities": []}, {"text": "Our revision approach assumes input sentences are represented as syntactic trees whose nodes are annotated with coreference information.", "labels": [], "entities": []}, {"text": "In order to provide open-domain coverage the approach does not assume a meaninglevel representation of each sentence, and so, unlike many generation systems, the system does not represent and reason about what is being said 1.", "labels": [], "entities": []}, {"text": "Meaning-dependent revision operations are restricted to situations where it is clear from coreference that the same entity is being talked about.", "labels": [], "entities": [{"text": "Meaning-dependent revision", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7486888766288757}]}, {"text": "There are several criteria our revision model needs to satisfy.", "labels": [], "entities": [{"text": "revision", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.9598114490509033}]}, {"text": "The final draft needs to be informative, coherent, and grammatically wellformed.", "labels": [], "entities": []}, {"text": "Informativeness is explored in Section 4.2.", "labels": [], "entities": []}, {"text": "We can also strive to guarantee, based on our revision rule set, that each revision will be syntactically well-formed.", "labels": [], "entities": []}, {"text": "Regarding coherence, revision alters rhetorical structure in away which can produce disfiuencies.", "labels": [], "entities": []}, {"text": "As rhetorical structure is hard to extract from the source 2, our program instead uses coreference to guide the revision, and attempts to patch the coherence by adjusting references in revised drafts.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this Q&A evaluation, the summarization system, given a document and a topic, needed to produce an informative, topic-related summary that contained the correct answers found in that document to a set of topic-related questions.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8439342975616455}]}, {"text": "These questions covered \"obligatory\" information that has to be provided in any document judged relevant to the topic.", "labels": [], "entities": []}, {"text": "The topics chosen (3 in all) were drawn from the TREC (Harman and Voorhees 1996) data sets.", "labels": [], "entities": [{"text": "TREC (Harman and Voorhees 1996) data sets", "start_pos": 49, "end_pos": 90, "type": "DATASET", "confidence": 0.8884842130872939}]}, {"text": "For each topic, 30 relevant TREC documents were chosen as the source texts for topic-related summarization.", "labels": [], "entities": []}, {"text": "The principal tasks of each Q&A evaluator were to prepare the questions and answer keys and to score the system summaries.", "labels": [], "entities": []}, {"text": "To construct the answer key, each evaluator marked off any passages in the text that provided an answer to a question (example shown in).", "labels": [], "entities": []}, {"text": "Two kinds of scoring were carried out.", "labels": [], "entities": [{"text": "scoring", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.8769823312759399}]}, {"text": "In the first, a manual method, the answer to each question was judged Correct, Partially Correct, or Missing based on guidelines involving a human comparison of the summary of a document against the set of tagged passages for that question in the answer key for that document.", "labels": [], "entities": []}, {"text": "The second method of scoring was an automatic method.", "labels": [], "entities": []}, {"text": "This program 7 took as input a key file and a summary to be scored, and returns an informativeness score on four different metrics.", "labels": [], "entities": []}, {"text": "The key file includes tags identifying passages in the file which answer certain questions.", "labels": [], "entities": []}, {"text": "The scoring uses the overlap measures shown in Table 2 s.", "labels": [], "entities": [{"text": "overlap", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.988018274307251}]}, {"text": "The automatically computed V4 thru V7 informativeness scores were strongly correlated with the human-evaluated scores (Pearson r > .97, ~ < 0.0001).", "labels": [], "entities": [{"text": "Pearson r > .97", "start_pos": 119, "end_pos": 134, "type": "METRIC", "confidence": 0.9494730949401855}]}, {"text": "Given this correlation, we decided to use these informativeness measures.", "labels": [], "entities": []}, {"text": "To evaluate the revised summaries, we first converted each summary into a weighting function which scored each full-text sentence in the summary's source in terms of its similarity to the most similar summary sentence.", "labels": [], "entities": []}, {"text": "The weight of a source document sentence s given a sum7The program was reimplemented by us for use in the revision evaluation.", "labels": [], "entities": []}, {"text": "S Passage matching here involves a sequential match with stop words and punctuation removed.", "labels": [], "entities": [{"text": "S Passage matching", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8351388772328695}]}, {"text": "Inspection of the results of revision indicates that the syntactic well-formedness revision criterion is satisfied to a very great extent.", "labels": [], "entities": [{"text": "syntactic well-formedness revision", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.6310538748900095}]}, {"text": "Improper extraction from coordinated NPs is an issue (see), but we expect additional revision rules to handle such cases.", "labels": [], "entities": [{"text": "Improper extraction from coordinated NPs", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7679854750633239}]}, {"text": "Coherence disfiuencies do occur; for example, since we don't resolve possessive pronouns or plural definites, we can get infelicitous revisions like \"A computer virus, which entered ,their computers through ARPANET, infected systems from MIT.\"", "labels": [], "entities": []}, {"text": "Other limitations in definite NP coreference can and do result in infelicitous reference adjustments.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8100393712520599}]}, {"text": "For one thing, we don't link definites to proper name antecedents, resulting in inappropriate indefinitization (e.g., \"Bill Gates ... *A computer tycoon\").", "labels": [], "entities": []}, {"text": "In addition, the \"same head word\" test doesn't of course address inferential relationships between the definite NP and its antecedent (even when the antecedent is explicitly mentioned), again resulting in inappropriate indefinitization (e.g., \"The program ....a developer ~', and \"The developer 11 Similar results hold while using a variety of other compression normalization metrics.", "labels": [], "entities": []}, {"text": "An anonymous caller said .a very high order hacker was a graduate student\").", "labels": [], "entities": []}, {"text": "To measure fluency without conducting an elaborate experiment involving human judgmentsl we fell back on some extremely coarse measurea based on word and sentence length computed by the (gnu) unix program style.", "labels": [], "entities": []}, {"text": "The FOG index sums the average sentence length with the percentage of words over 3 syllables, with a \"grade\" level over 12 indicating difficulty for the average reader.", "labels": [], "entities": [{"text": "FOG index", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9413363039493561}, {"text": "difficulty", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.963420569896698}]}, {"text": "The Kincaid index, intended for technical text, computes a weighted sum of sentence length and word length.", "labels": [], "entities": []}, {"text": "As can be seen from, there is a slight but significant lowering of scores on both metrics, revealing that according to these metrics revision is not resulting in more complex text.", "labels": [], "entities": []}, {"text": "This suggests that elimination rather than aggregation is mainly responsible for this.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Informativeness measures for Automatic Scoring of each question that has an answer  according to the key.", "labels": [], "entities": [{"text": "Scoring", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.7006952166557312}]}, {"text": " Table 3: Readability of Summaries Before (Original Summary) and After Revision (A+E). Overall,  both FOG and Kincaid scores show a slight but statistically significant drop on revision (~ <: 0.05).", "labels": [], "entities": [{"text": "After Revision (A+E)", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.9138811997004918}, {"text": "FOG", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.7779582142829895}, {"text": "revision", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9903974533081055}]}]}