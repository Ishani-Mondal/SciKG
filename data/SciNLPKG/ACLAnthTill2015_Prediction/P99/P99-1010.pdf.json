{"title": [{"text": "Supervised Grammar Induction using Training Data with Limited Constituent Information *", "labels": [], "entities": [{"text": "Grammar Induction", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7766177952289581}]}], "abstractContent": [{"text": "Corpus-based grammar induction generally relies on hand-parsed training data to learn the structure of the language.", "labels": [], "entities": [{"text": "Corpus-based grammar induction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.764771302541097}]}, {"text": "Unfortunately, the cost of building large annotated corpora is prohibitively expensive.", "labels": [], "entities": []}, {"text": "This work aims to improve the induction strategy when there are few labels in the training data.", "labels": [], "entities": [{"text": "induction", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.9456508755683899}]}, {"text": "We show that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses.", "labels": [], "entities": []}, {"text": "They account for only 20% of all constituents.", "labels": [], "entities": []}, {"text": "For inducing grammars from sparsely labeled training data (e.g., only higher-level constituent labels), we propose an adaptation strategy, which produces grammars that parse almost as well as grammars induced from fully labeled corpora.", "labels": [], "entities": []}, {"text": "Our results suggest that fora partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "The availability of large hand-parsed corpora such as the Penn Treebank Project has made high-quality statistical parsers possible.", "labels": [], "entities": [{"text": "Penn Treebank Project", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.9931488235791525}]}, {"text": "However, the parsers risk becoming too tailored to these labeled training data that they cannot reliably process sentences from an arbitrary domain.", "labels": [], "entities": []}, {"text": "Thus, while a parser trained on the from that domain, in which the training process would require hand-parsed sentences from the new domain.", "labels": [], "entities": []}, {"text": "Because parsing a large corpus by hand is a labor-intensive task, it would be beneficial to minimize the number of labels needed to induce the new grammar.", "labels": [], "entities": []}, {"text": "We propose to adapt a grammar already trained on an old domain to the new domain.", "labels": [], "entities": []}, {"text": "Adaptation can exploit the structural similarity between the two domains so that fewer labeled data might be needed to update the grammar to reflect the structure of the new domain.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9598957896232605}]}, {"text": "This paper presents a quantitative study comparing direct induction and adaptation under different training conditions.", "labels": [], "entities": [{"text": "direct induction", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7325806617736816}]}, {"text": "Our goal is to understand the effect of the amounts and types of labeled data on the training process for both induction strategies.", "labels": [], "entities": []}, {"text": "For example, how much training data need to be hand-labeled?", "labels": [], "entities": []}, {"text": "Must the parse trees for each sentence be fully specified?", "labels": [], "entities": []}, {"text": "Are some linguistic constituents in the parse more informative than others?", "labels": [], "entities": []}, {"text": "To answer these questions, we have performed experiments that compare the parsing qualities of grammars induced under different training conditions using both adaptation and direct induction.", "labels": [], "entities": []}, {"text": "We vary the number of labeled brackets and the linguistic classes of the labeled brackets.", "labels": [], "entities": []}, {"text": "The study is conducted on both a simple Air Travel Information System (ATIS) corpus () and the more complex Wall Street Journal (WSJ) corpus).", "labels": [], "entities": [{"text": "Air Travel Information System (ATIS) corpus", "start_pos": 40, "end_pos": 83, "type": "DATASET", "confidence": 0.7410396113991737}, {"text": "Wall Street Journal (WSJ) corpus", "start_pos": 108, "end_pos": 140, "type": "DATASET", "confidence": 0.9519300035067967}]}, {"text": "Our results show that the training examples do not need to be fully parsed for either strategy, but adaptation produces better grammars than direct induction under the conditions of minimally labeled training data.", "labels": [], "entities": []}, {"text": "For instance, the most informative brackets, which label constituents higher up in the parse trees, typically identifying complex noun phrases and sentential clauses, account for only 17% of all constituents in ATIS and 21% in WSJ.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 211, "end_pos": 215, "type": "DATASET", "confidence": 0.825663149356842}, {"text": "WSJ", "start_pos": 227, "end_pos": 230, "type": "DATASET", "confidence": 0.9375846982002258}]}, {"text": "Trained on this type of label, the adapted grammars parse better than the directly induced grammars and almost as well as those trained on fully labeled data.", "labels": [], "entities": []}, {"text": "Training on ATIS sentences labeled with higher-level constituent brackets, a directly induced grammar parses test sentences with 66% accuracy, whereas an adapted grammar parses with 91% accuracy, which is only 2% lower than the score of a grammar induced from fully labeled training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9974697828292847}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9973887801170349}]}, {"text": "Training on WSJ sentences labeled with higher-level constituent brackets, a directly induced grammar parses with 70% accuracy, whereas an adapted grammar parses with 72% accuracy, which is 6% lower than the score of a grammar induced from fully labeled training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9985289573669434}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9977043271064758}]}, {"text": "That the most informative brackets are higher-level constituents and makeup only onefifth of all the labels in the corpus has two implications.", "labels": [], "entities": []}, {"text": "First, it shows that there is potential reduction of labor for the human annotators.", "labels": [], "entities": []}, {"text": "Although the annotator still must process an entire sentence mentally, the task of identifying higher-level structures such as sentential clauses and complex nouns should be less tedious than to fully specify the complete parse tree for each sentence.", "labels": [], "entities": []}, {"text": "Second, one might speculate the possibilities of replacing human supervision altogether with a partial parser that locates constituent chunks within a sentence.", "labels": [], "entities": []}, {"text": "However, as our results indicate that the most informative constituents are higher-level phrases, the parser would have to identify sentential clauses and complex noun phrases rather than low-level base noun phrases.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first uses ATIS as the corpus from which the different types of partially labeled training sets are generated.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.5533384084701538}]}, {"text": "Both induction strategies train from these data, but the adaptive strategy pretrains its grammars with fully labeled data drawn from the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 137, "end_pos": 147, "type": "DATASET", "confidence": 0.9762995541095734}]}, {"text": "The trained grammars are scored on their parsing abilities on unseen ATIS test sets.", "labels": [], "entities": [{"text": "ATIS test sets", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.924674928188324}]}, {"text": "We use the non-crossing bracket measurement as the parsing metric.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9726403951644897}]}, {"text": "This experiment will show whether annotations of a particular linguistic category maybe more useful for training grammars than others.", "labels": [], "entities": []}, {"text": "It will also indicate the comparative merits of the two induction strategies trained on data annotated with these linguistic categories.", "labels": [], "entities": []}, {"text": "However, pretraining on the much more complex WSJ corpus maybe too much of an advantage for the adaptive strategy.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.8772035241127014}]}, {"text": "Therefore, we reverse the roles of the corpus in the second experiment.", "labels": [], "entities": []}, {"text": "The partially labeled data are from the WSJ corpus, and the adaptive strategy is pretrained on fully labeled ATIS data.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9649733304977417}, {"text": "ATIS data", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.833138644695282}]}, {"text": "In both cases, part-of-speech(POS) tags are used as the lexical items of the sentences.", "labels": [], "entities": []}, {"text": "Backing off to POS tags is necessary because the tags provide a considerable intersection in the vocabulary sets of the two corpora.", "labels": [], "entities": []}, {"text": "The easier learning task is to induce grammars to parse ATIS sentences.", "labels": [], "entities": [{"text": "parse ATIS sentences", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6593939264615377}]}, {"text": "The ATIS corpus consists of 577 short sentences with simple structures, and the vocabulary set is made up of 32 \u2022 POS tags, a subset of the 47 tags used for the WSJ.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9539038240909576}, {"text": "WSJ", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.8509931564331055}]}, {"text": "Due to the limited size of this corpus, ten sets of randomly partitioned train-test-held-out triples are generated to ensure the statistical significance of our results.", "labels": [], "entities": []}, {"text": "We use 80 sentences for testing, 90 sentences for held-out data, and the rest for training.", "labels": [], "entities": []}, {"text": "Before proceeding with the main discussion on training from the ATIS, we briefly describe the pretraining stage of the adaptive strategy.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7782928943634033}]}, {"text": "In the previous section, we have seen that annotations of complex clauses are the most helpful for inducing ATIS-style grammars.", "labels": [], "entities": [{"text": "ATIS-style grammars", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7620171308517456}]}, {"text": "One of the goals of this experiment is to verify whether the result also holds for the WSJ corpus, which is structurally very different from ATIS.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.9580590724945068}, {"text": "ATIS", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.9347623586654663}]}, {"text": "The WSJ corpus uses 47 POS tags, and its sentences are longer and have more embedded clauses.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9571181833744049}]}, {"text": "As in the previous experiment, we construct training sets with annotations of different constituent types and of different numbers of randomly chosen labels.", "labels": [], "entities": []}, {"text": "Each training set consists of 3600 sentences, and 1780 sentences are used as held-out data.", "labels": [], "entities": []}, {"text": "The trained grammars are tested on a set of 2245 sentences.", "labels": [], "entities": []}, {"text": "Different from the previous experiment, however, the AI1NP training sets do not seem to provide as much information for this learning task.", "labels": [], "entities": []}, {"text": "This maybe due to the increase in the sentence complexity of the WSJ, which further de-emphasized the role of the simple phrases.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8329530954360962}]}, {"text": "Thus, grammars trained on AllNP labels have comparable parsing scores to those trained on HighP labels.", "labels": [], "entities": []}, {"text": "Also, we do not see as big a gap between the scores of the two induction strategies in the HighP case because the adapted grammar's advantage of having seen annotated ATIS base nouns is reduced.", "labels": [], "entities": []}, {"text": "Nonetheless, the adapted grammars still perform 2% better than the directly induced grammars, and this improvement is statistically significant.", "labels": [], "entities": []}, {"text": "2 Furthermore, grammars trained on NotBaseP do not fall as far below the baseline and have higher parsing scores than those trained on HighP and AllNP.", "labels": [], "entities": [{"text": "AllNP", "start_pos": 145, "end_pos": 150, "type": "DATASET", "confidence": 0.8481862545013428}]}, {"text": "This suggests that for more complex domains, other linguistic constituents 2A pair-wise t-test comparing the parsing scores of the ten test sets for the two strategies shows 99% confidence in the difference.", "labels": [], "entities": []}, {"text": "such as verb phrases 3 become more informative.", "labels": [], "entities": []}, {"text": "A second goal of this experiment is to test the adaptive strategy under more stringent conditions.", "labels": [], "entities": []}, {"text": "In the previous experiment, a WSJ-style grammar was retrained for the simpler ATIS corpus.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.9449902176856995}]}, {"text": "Now, we reverse the roles of the corpora to see whether the adaptive strategy still offers any advantage over direct induction.", "labels": [], "entities": []}, {"text": "In the adaptive method's pretraining stage, a grammar is induced from 400 fully labeled ATIS sentences.", "labels": [], "entities": []}, {"text": "Testing this ATIS-style grammar on the WSJ test set without further training renders a parsing accuracy of 40%.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.980900247891744}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9314529895782471}]}, {"text": "The low score suggests that fully labeled ATIS data does not teach the grammar as much about the structure of WSJ.", "labels": [], "entities": [{"text": "ATIS data", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.7391350716352463}, {"text": "WSJ", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7475956082344055}]}, {"text": "Nonetheless, the adaptive strategy proves to be beneficial for learning WSJ from sparsely labeled training sets.", "labels": [], "entities": []}, {"text": "The adapted grammars out-perform the directly induced grammars when more than 50% of the brackets are missing from the training data.", "labels": [], "entities": []}, {"text": "The most significant difference is when the training data contains no label information at all.", "labels": [], "entities": []}, {"text": "The adapted grammar parses with 60.1% accuracy whereas the directly induced grammar parses with 49.8% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9988254904747009}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9967023730278015}]}], "tableCaptions": []}