{"title": [{"text": "Modeling Filled Pauses in Medical Dictations", "labels": [], "entities": [{"text": "Modeling Filled Pauses in Medical Dictations", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7196308970451355}]}], "abstractContent": [{"text": "Filled pauses are characteristic of spontaneous speech and can present considerable problems for speech recognition by being often recognized as short words.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7275399267673492}]}, {"text": "An um can be recognized as thumb or arm if the recognizer's language model does not adequately represent FP's.", "labels": [], "entities": []}, {"text": "Recognition of quasi-spontaneous speech (medical dictation) is subject to this problem as well.", "labels": [], "entities": [{"text": "Recognition of quasi-spontaneous speech (medical dictation)", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7186777032911777}]}, {"text": "Results from medical dictations by 21 family practice physicians show that using an FP model trained on the corpus populated with FP's produces overall better results than a model trained on a corpus that excluded FP's or a corpus that had random FP's.", "labels": [], "entities": []}], "introductionContent": [{"text": "Filled pauses (FP's), false starts, repetitions, fragments, etc. are characteristic of spontaneous speech and can present considerable problems for speech recognition.", "labels": [], "entities": [{"text": "Filled pauses (FP's", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8244025945663452}, {"text": "speech recognition", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7331427037715912}]}, {"text": "FP's are often recognized as short words of similar phonetic quality.", "labels": [], "entities": []}, {"text": "For example, an um can be recognized as thumb or arm if the recognizer's language model does not adequately represent FP's.", "labels": [], "entities": []}, {"text": "Recognition of quasi-spontaneous speech (medical dictation) is subject to this problem as well.", "labels": [], "entities": [{"text": "Recognition of quasi-spontaneous speech (medical dictation)", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7186777032911777}]}, {"text": "The FP problem becomes especially pertinent where the corpora used to build language models are compiled from text with no FP's. has shown that representing FP's in a language model helps decrease the model' s perplexity.", "labels": [], "entities": [{"text": "FP", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9614525437355042}]}, {"text": "She finds that when a FP occurs at a major phrase or discourse boundary, the FP itself is the best predictor of the following lexical material; conversely, in a non-boundary context, FP's are predictable from the preceding words.", "labels": [], "entities": [{"text": "FP", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.8913483619689941}]}, {"text": "shows that the rate of disfluencies grows exponentially with the length of the sentence, and that FP's occur more often in the initial position (see also).", "labels": [], "entities": [{"text": "FP", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9929378628730774}]}, {"text": "This paper presents a method of using bigram probabilities for extracting FP distribution from a corpus of handtranscribed dam.", "labels": [], "entities": []}, {"text": "The resulting bigram model is used to populate another Iraining corpus that originally had no FP's.", "labels": [], "entities": [{"text": "Iraining corpus", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.728689044713974}]}, {"text": "Results from medical dictations by 21 family practice physicians show that using an FP model trained on the corpus populated with FP's produces overall better results than a model trained on a corpus that excluded FP's or a corpus that had random FP's.", "labels": [], "entities": []}, {"text": "Recognition accuracy improves proportionately to the frequency of FP's in the speech.", "labels": [], "entities": [{"text": "Recognition", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8366546034812927}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9084128737449646}, {"text": "FP", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9356749057769775}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1.  ADAPTFP-LM based on CONTROLLED-FP- CORPUS has lower perplexity in general.  When tested on conditions B and C, ADAPTFP- LM does better on frequent FP users, whereas", "labels": [], "entities": []}, {"text": " Table 2. Recognition accuracy tests for LM's.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8093824982643127}]}]}