{"title": [], "abstractContent": [{"text": "We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.", "labels": [], "entities": []}, {"text": "Our contributions are threefold: an empirical comparison of abroad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions.", "labels": [], "entities": []}], "introductionContent": [{"text": "An inherent problem for statistical methods in natural language processing is that of sparse data --the inaccurate representation in any training corpus of the probability of low frequency events.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6489286025365194}]}, {"text": "In particular, reasonable events that happen to not occur in the training set may mistakenly be assigned a probability of zero.", "labels": [], "entities": []}, {"text": "These unseen events generally makeup a substantial portion of novel data; for example, report that 12% of the test-set bigrams in a 75%-25% split of one million words did not occur in the training partition.", "labels": [], "entities": []}, {"text": "We consider here the question of how to estimate the conditional cooccurrence probability P(v[n) of an unseen word pair (n, v) drawn from some finite set N x V.", "labels": [], "entities": []}, {"text": "Two state-of-the-art technologies are backoff method and interpolation method.", "labels": [], "entities": []}, {"text": "Both use P(v) to estimate P(v when (n, v) is unseen, essentially ignoring the identity of n.", "labels": [], "entities": []}, {"text": "An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for 25 cooccurrences involving similar words: 1 /P(v[n) ----~-~mES(n) sim(n, m)P(v[m) ~-]mES(n) sim(n, m) , where S(n) is a set of candidate similar words and sim(n, m) is a function of the similarity between n and m.", "labels": [], "entities": [{"text": "distance-weighted averaging", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.7080172896385193}]}, {"text": "We focus on distributional rather than semantic similarity (e.g.,) because the goal of distance-weighted averaging is to smooth probability distributions --although the words \"chance\" and \"probability\" are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in.", "labels": [], "entities": []}, {"text": "There are many plausible measures of distributional similarity.", "labels": [], "entities": []}, {"text": "In previous work, we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability.", "labels": [], "entities": []}, {"text": "Our experiments on a frequency-controlled pseudoword disambiguation task showed that using any of the three in a distance-weighted averaging scheme yielded large improvements over Katz's backoff smoothing method in predicting unseen coocurrences.", "labels": [], "entities": [{"text": "pseudoword disambiguation task", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7942371567090353}]}, {"text": "Furthermore, by using a restricted version of model (1) that stripped incomparable parameters, we were able to empirically demonstrate that the confusion probability is fundamentally worse at selecting useful similar words.", "labels": [], "entities": []}, {"text": "D. Lin also found that the choice of similarity function can affect the quality of automatically-constructed thesauri to a statistically significant degree (1998a) and the ability to determine common morphological roots by as much as 49% in precision (1998b).", "labels": [], "entities": [{"text": "precision", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.9987210631370544}]}, {"text": "These empirical results indicate that investigating different similarity measures can lead to improved natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6614933808644613}]}, {"text": "On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature, there has been some doubt expressed in that community that the choice of similarity metric has any practical impact: Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised.", "labels": [], "entities": []}, {"text": "But no contradiction arises because, as van Rijsbergen continues, \"one would expect this since most measures incorporate the same information\".", "labels": [], "entities": []}, {"text": "In the language-modeling domain, there is currently no agreed-upon best similarity metric because there is no agreement on what the \"same information\"-the key data that a similarity function should incorporate --is.", "labels": [], "entities": []}, {"text": "The overall goal of the work described here was to discover these key characteristics.", "labels": [], "entities": []}, {"text": "To this end, we first compared a number of common similarity measures, evaluating them in a parameter-free way on a decision task.", "labels": [], "entities": []}, {"text": "When grouped by average performance, they fell into several coherent classes, which corresponded to the extent to which the functions focused on the intersection of the supports (regions of positive probability) of the distributions.", "labels": [], "entities": []}, {"text": "Using this insight, we developed an information-theoretic metric, the skew divergence, which incorporates the support-intersection data in an asymmetric fashion.", "labels": [], "entities": []}, {"text": "This function yielded the best performance overall: an average error rate reduction of 4% (significant at the .01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments ().", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 63, "end_pos": 83, "type": "METRIC", "confidence": 0.9731545448303223}]}, {"text": "Our contributions are thus three-fold: an empirical comparison of abroad range of similarity metrics using an evaluation methodology that factors out inessential degrees of freedom; a proposal, building on this comparison, of a characteristic for classifying similarity functions; and the introduction of anew similarity metric incorporating this characteristic that is superior at evaluating potential proxy distributions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Similarity functions, written in terms of sums over supports and grouped by average  performance. \\ denotes set difference; A denotes symmetric set difference.", "labels": [], "entities": []}]}