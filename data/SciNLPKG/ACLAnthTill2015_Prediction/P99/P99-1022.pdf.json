{"title": [{"text": "Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based Adaptation", "labels": [], "entities": [{"text": "Dynamic Nonlocal Language Modeling", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6880877166986465}]}], "abstractContent": [{"text": "This paper presents a novel method of generating and applying hierarchical, dynamic topic-based language models.", "labels": [], "entities": []}, {"text": "It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques.", "labels": [], "entities": [{"text": "cluster generation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7919588983058929}, {"text": "topic-probability estimation", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6626038402318954}]}, {"text": "These combined models help capture long-distance lexical dependencies.", "labels": [], "entities": []}, {"text": "\u00b0Experiments on the Broadcast News corpus show significant improvement in perplexity (10.5% overall and 33.5% on target vocabulary).", "labels": [], "entities": [{"text": "Broadcast News corpus", "start_pos": 20, "end_pos": 41, "type": "DATASET", "confidence": 0.9484596649805704}, {"text": "perplexity", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9607241153717041}]}], "introductionContent": [{"text": "Statistical language models are core components of speech recognizers, optical character recognizers and even some machine translation systems.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6990978419780731}, {"text": "optical character recognizers", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.6444049080212911}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7217559814453125}]}, {"text": "The most common language modeling paradigm used today is based on n-grams, local word sequences.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7148898243904114}]}, {"text": "These models make a Markovian assumption on word dependencies; usually that word predictions depend on at most m previous words.", "labels": [], "entities": []}, {"text": "Therefore they offer the following approximation for the computation of a word sequence probability: P(wU) = -') = 1-I =lP(w, where w{ denotes the sequence wi...", "labels": [], "entities": []}, {"text": "wj ; a common size form is 3 (trigram language models).", "labels": [], "entities": []}, {"text": "Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context.", "labels": [], "entities": []}, {"text": "Cache language models (,) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (), even more general, try to capture the interrelationships between words.", "labels": [], "entities": []}, {"text": "Models based on syntactic structure (,) effectively estimate intra-sentence syntactic word dependencies.", "labels": [], "entities": []}, {"text": "The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics.", "labels": [], "entities": []}, {"text": "We propose to compute the conditional language model probability as a dynamic mixture model of K topicspecific language models:  The motivation for developing topic-sensitive language models is twofold.", "labels": [], "entities": []}, {"text": "First, empirically speaking, many n-gram probabilities vary substantially when conditioned on topic (such as in the case of content words following several function words).", "labels": [], "entities": []}, {"text": "A more important benefit, however, is that even when a given bigram or trigram probability is not topic sensitive, as in the case of sparse n-gram statistics, the topicsensitive unigram or bigram probabilities may constitute a more informative backoff estimate than the single global unigram or bigram estimates.", "labels": [], "entities": []}, {"text": "Discussion of these important smoothing issues is given in Section 4.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.9736905694007874}]}, {"text": "Finally, we observe that lexical probability distributions vary not only with topic but with subtopic too, in a hierarchical manner.", "labels": [], "entities": []}, {"text": "For example, consider the variation of the probability of the word peace given major news topic distinctions (e.g. BUSI-NESS and INTERNATIONAL news) as illustrated in.", "labels": [], "entities": [{"text": "BUSI-NESS", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.6819660663604736}]}, {"text": "There is substantial subtopic probability variation for peace within INTERNATIONAL news (the word usage is 50-times more likely in INTERNATIONAL:MIDDLE-EAST than INTERNA-TIONAL:JAPAN).", "labels": [], "entities": [{"text": "INTERNATIONAL news", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.8208188414573669}, {"text": "INTERNATIONAL:MIDDLE-EAST", "start_pos": 131, "end_pos": 156, "type": "DATASET", "confidence": 0.6564901868502299}, {"text": "JAPAN", "start_pos": 177, "end_pos": 182, "type": "DATASET", "confidence": 0.44126081466674805}]}, {"text": "We propose methods of hierarchical smoothing of P(w~ Itopict) in a topic-tree to capture this subtopic variation robustly.", "labels": [], "entities": []}], "datasetContent": [{"text": "Estimating the language model probabilities is a two-phase process.", "labels": [], "entities": []}, {"text": "First, the topic-sensitive lani--1 gnage model probabilities P (wilt, wi_,~+~ ) are computed during the training phase.", "labels": [], "entities": [{"text": "topic-sensitive lani--1 gnage model probabilities P", "start_pos": 11, "end_pos": 62, "type": "METRIC", "confidence": 0.6715498343110085}]}, {"text": "Then, at run-time, or in the testing phase, topic is dynamically identified by computing the probabilities P (tlw~ -1) as in section 4.2 and the final language model probabilities are computed using Equation (1).", "labels": [], "entities": [{"text": "Equation", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.888966977596283}]}, {"text": "The tree used in the following experiments was generated using average-linkage agglomerative clustering, using parameters that optimize the objective function in Section 3.", "labels": [], "entities": []}, {"text": "It is important to note that for 66% of the vocabulary the topic-based LM is identical to the core bigram model.", "labels": [], "entities": []}, {"text": "On the 34% of the data that falls in the model's target vocabulary, however, perplexity reduction is a much more substantial 33.5% improvement.", "labels": [], "entities": []}, {"text": "The ability to isolate a well-defined target subtask and perform very well on it makes this work especially promising for use in model combination.", "labels": [], "entities": []}], "tableCaptions": []}