{"title": [], "abstractContent": [{"text": "Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic push-down automata (PPDAs) have been used for language modeling and maximum likelihood parsing.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7538344264030457}, {"text": "maximum likelihood parsing", "start_pos": 148, "end_pos": 174, "type": "TASK", "confidence": 0.7306067148844401}]}, {"text": "We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilis-tic languages, they appear to impose different inductive biases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current work in stochastic language models and maximum likelihood parsers falls into two main approaches.", "labels": [], "entities": [{"text": "maximum likelihood parsers", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.6009774307409922}]}, {"text": "The first approach) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree.", "labels": [], "entities": []}, {"text": "The second approach) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree.", "labels": [], "entities": []}, {"text": "These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively.", "labels": [], "entities": []}, {"text": "It is well known that these two classical formalisms define the same language class.", "labels": [], "entities": []}, {"text": "In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending the classical result to the stochastic case.", "labels": [], "entities": []}, {"text": "We also touch on the perhaps more interesting question of whether PCFGs and shift-reduce parsing models have the same inductive bias with respect to the automatic learning of model parameters from data.", "labels": [], "entities": []}, {"text": "Though we cannot provide a definitive answer, the constructions we use to answer the equivalence question involve blowups in the number of parameters in both directions, suggesting that the two models impose different inductive biases.", "labels": [], "entities": []}, {"text": "We are concerned herewith probabilistic shift-reduce parsing models that define probability distributions over word sequences, and in particular the model of.", "labels": [], "entities": []}, {"text": "Most other probabilistic shiftreduce parsing models give only the conditional probability of a parse tree given a word sequence. has argued that those models fail to capture the appropriate dependency relations of natural language.", "labels": [], "entities": []}, {"text": "Furthermore, they are not directly comparable to PCFGs, which define probability distributions over word sequences.", "labels": [], "entities": []}, {"text": "To make the discussion somewhat more concrete, we now present a simplified version of the Chelba-Jelinek model.", "labels": [], "entities": []}, {"text": "Consider the following sentence: The small woman gave the fat man her sandwich.", "labels": [], "entities": []}, {"text": "The model under discussion is based on shiftreduce PPDAs.", "labels": [], "entities": []}, {"text": "In such a model, shift transitions generate the next word wand its associated syntactic category X and push the pair (X, w) on the stack.", "labels": [], "entities": []}, {"text": "Each shift transition is followed by zero or more reduce transitions that combine topmost stack entries.", "labels": [], "entities": []}, {"text": "For example the stack elements (Det, the), (hdj, small), (N, woman) can be combined to form the single entry (NP, woman) representing the phrase \"the small woman\".", "labels": [], "entities": []}, {"text": "In general each stack entry consists of a syntactic category and ahead word.", "labels": [], "entities": []}, {"text": "After generating the prefix \"The small woman gave the fat man\" the stack might contain the sequence (NP, woman)<Y, gave)(NP, man).", "labels": [], "entities": []}, {"text": "The Chelba-Jelinek model then executes a shift tran-S --+ (S, admired) (S, admired) --+ (NP, Mary)(VP, admired) (VP, admired) -+ (V, admired) (NP, oak) -+ (Det, the)(N, oak) (N, oak) -+ (Adj, towering> (N, oak> (N, oak> -~ (Adj, strong>(N, oak> (N, oak) -+ (hdj, old>(N, oak) (NP, Mary) -+ Mary (N, oak) -+ oak: Lexicalized context-free grammar sition by generating the next word.", "labels": [], "entities": [{"text": "Lexicalized context-free grammar sition", "start_pos": 312, "end_pos": 351, "type": "TASK", "confidence": 0.6902800053358078}]}, {"text": "This is done in a manner similar to that of a trigram model except that, rather than generate the next word based on the two preceding words, it generates the next word based on the two topmost stack entries.", "labels": [], "entities": []}, {"text": "In this example the ChelbaJelinek model generates the word \"her\" from (V, gave)(NP, man) while a classical trigram model would generate \"her\" from \"fat man\".", "labels": [], "entities": []}, {"text": "We now contrast Chelba-Jelinek style models with lexicalized PCFG models.", "labels": [], "entities": []}, {"text": "A PCFG is a context-free grammar in which each production is associated with a weight in the interval and such that the weights of the productions from any given nonterminal sum to 1.", "labels": [], "entities": []}, {"text": "For instance, the sentence Mary admired the towering strong old oak can be derived using a lexicalized PCFG based on the productions in.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.8424139618873596}]}, {"text": "Production probabilities in the PCFG would reflect the likelihood that a phrase headed by a certain word can be expanded in a certain way.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9222813248634338}]}, {"text": "Since it can be difficult to estimate fully these likelihoods, we might restrict ourselves to models based on bilexical relationships (Eisner, 1997), those between pairs of words.", "labels": [], "entities": []}, {"text": "The simplest bilexical relationship is a bigram statistic, the fraction of times that \"oak\" follows \"old\".", "labels": [], "entities": []}, {"text": "Bilexical relationships fora PCFG include that between the head-word of a phrase and the head-word of a non-head immediate constituent, for instance.", "labels": [], "entities": []}, {"text": "In particular, the generation of the above sentence using a PCFG based on would exploit a bilexical statistic between \"towering\" and \"oak\" contained in the weight of the fifth production.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9061284065246582}]}, {"text": "This bilexical relationship between \"towering\" and \"oak\" would not be exploited in either a trigram model or in a Chelba-Jelinek style model.", "labels": [], "entities": []}, {"text": "Ina Chelba-Jelinek style model one must generate \"towering\" before generating \"oak\" and then \"oak\" must be generated from (Adj, strong), (Adj, old).", "labels": [], "entities": []}, {"text": "In this example the Chelba-Jelinek model behaves more like a classical trigram model than like a PCFG model.", "labels": [], "entities": []}, {"text": "This contrast between PPDAs and PCFGs is formalized in theorem 1, which exhibits a PCFG for which no stochastic parameterization of the corresponding shift-reduce parser yields the same probability distribution over strings.", "labels": [], "entities": []}, {"text": "That is, the standard shift-reduce translation from CFGs to PDAs cannot be generalized to the stochastic case.", "labels": [], "entities": []}, {"text": "We give two ways of getting around the above difficulty.", "labels": [], "entities": []}, {"text": "The first is to construct a top-down PPDA that mimics directly the process of generating a PCFG derivation from the start symbol by repeatedly replacing the leftmost nonterminal in a sentential form by the right-hand side of one of its rules.", "labels": [], "entities": []}, {"text": "Theorem 2 states that any PCFG can be translated into a topdown PPDA.", "labels": [], "entities": []}, {"text": "Conversely, theorem 3 states that any PPDA can be translated to a PCFG, not just those that are top-down PPDAs for some PCFG.", "labels": [], "entities": []}, {"text": "Hence PCFGs and general PPDAs define the same class of stochastic languages.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.917441189289093}]}, {"text": "Unfortunately, top-down PPDAs do not allow the simple left-to-right processing that motivates shift-reduce PPDAs.", "labels": [], "entities": []}, {"text": "A second way around the difficulty formalized in theorem 1 is to encode additional information about the derivation context with richer stack and state alphabets.", "labels": [], "entities": []}, {"text": "Theorem 7 shows that it is thus possible to translate an arbitrary PCFG to a shift-reduce PPDA.", "labels": [], "entities": []}, {"text": "The construction requires a fair amount of machinery including proofs that any PCFG can be put in Chomsky normal form, that weights can be renormalized to ensure that the result of grammar transformations can be made into PCFGs, that any PCFG can be put in Greibach normal form, and, finally, that a Greibach normal form PCFG can be converted to a shift-reduce PPDA.", "labels": [], "entities": []}, {"text": "The construction also involves a blow-up in the size of the shift-reduce parsing automaton.", "labels": [], "entities": []}, {"text": "This suggests that some languages that are concisely describable by a PCFG are not concisely describable by a shift-reduce PPDA, hence that the class of PCFGs and the class of shift-reduce PPDAs impose different inductive biases on the CF languages.", "labels": [], "entities": []}, {"text": "In the conversion from shiftreduce PPDAs to PCFGs, there is also a blowup, if a less dramatic one, leaving open the possibility that the biases are incomparable, and that neither formalism is inherently more concise.", "labels": [], "entities": []}, {"text": "Our main conclusion is then that, while the generative and shift-reduce parsing approaches are weakly equivalent, they impose different inductive biases.", "labels": [], "entities": [{"text": "generative and shift-reduce parsing", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.6582217663526535}]}], "datasetContent": [], "tableCaptions": []}