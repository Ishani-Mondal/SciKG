{"title": [{"text": "A Pylonic Decision-Tree Language Model with Optimal Question Selection", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper discusses a decision-tree approach to the problem of assigning probabilities to words following a given text.", "labels": [], "entities": [{"text": "assigning probabilities to words following a given text", "start_pos": 64, "end_pos": 119, "type": "TASK", "confidence": 0.7431793138384819}]}, {"text": "In contrast with previous decision-tree language model attempts, an algorithm for selecting nearly optimal questions is considered.", "labels": [], "entities": []}, {"text": "The model is to be tested on a standard task, The Wall Street Journal, allowing a fair comparison with the well-known tri-gram model.", "labels": [], "entities": [{"text": "The Wall Street Journal", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.8627241551876068}]}], "introductionContent": [{"text": "In many applications such as automatic speech recognition, machine translation, spelling correction, etc., a statistical language model (LM) is needed to assign ~probabilities to sentences.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.6697144707043966}, {"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7906236052513123}, {"text": "spelling correction", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.924132376909256}]}, {"text": "This probability assignment maybe used, e.g., to choose one of many transcriptions hypothesized by the recognizer or to make decisions about capitalization.", "labels": [], "entities": []}, {"text": "Without any loss of generality, we consider models that operate left-to-right on the sentences, assigning a probability to the next word given its word history.", "labels": [], "entities": []}, {"text": "Specifically, we consider statistical LM's which compute probabilities of the type P{wn ]Wl, W2,..-, Wn--1}, where wi denotes the i-th word in the text.", "labels": [], "entities": []}, {"text": "Even fora small vocabulary, the space of word histories is so large that any attempt to estimate the conditional probabilities for each distinct history from raw frequencies is infeasible.", "labels": [], "entities": []}, {"text": "To make the problem manageable, one partitions the word histories into some classes C(wl,w2,...,Wn-1), and identifies the word probabilities with P{wn [ C(wl, w2,.", "labels": [], "entities": []}, {"text": "Such probabilities are easier to estimate as each class gets significantly more counts from a training corpus.", "labels": [], "entities": []}, {"text": "With this setup, building a language model becomes a classification problem: group the word histories into a small number of classes 606 while preserving their predictive power.", "labels": [], "entities": []}, {"text": "Currently, popular N-gram models classify the word histories by their last N -1 words.", "labels": [], "entities": []}, {"text": "N varies from 2 to 4 and the trigram model P{wn [Wn-2, wn-1} is commonly used.", "labels": [], "entities": []}, {"text": "Although these simple models perform surprisingly well, there is much room for improvement.", "labels": [], "entities": []}, {"text": "The approach used in this paper is to classify the histories by means of a decision tree: to cluster word histories Wl,W2,...", "labels": [], "entities": []}, {"text": ",wn-1 for which the distributions of the following word Wn in a training corpus are similar.", "labels": [], "entities": []}, {"text": "The decision tree is pylonic in the sense that histories at different nodes in the tree maybe recombined in anew node to increase the complexity of questions and avoid data fragmentation.", "labels": [], "entities": []}, {"text": "The method has been tried before ( and had promising results.", "labels": [], "entities": []}, {"text": "In the work presented here we made two major changes to the previous attempts: we have used an optimal tree growing algorithm () not known at the time of publication of (, and we have replaced the ad-hoc clustering of vocabulary items used by Bahl with a data-driven clustering scheme proposed in.", "labels": [], "entities": []}], "datasetContent": [{"text": "The decision tree is being trained and tested on the Wall Street Journal corpus from 1987 to 1989 containing 45 million words.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.980668306350708}]}, {"text": "The data is divided into 15 million words for growing the nodes, 15 million for cross-validation, 10 million for estimating probabilities, and 5 million for testing.", "labels": [], "entities": []}, {"text": "To compare the results with other similar attempts (, the vocabulary consists of only the 5000 most frequent words and a special \"unknown\" word that replaces all the others.", "labels": [], "entities": []}, {"text": "The model tries to predict the word following a 20-word history.", "labels": [], "entities": []}, {"text": "At the time this paper was written, the implementation of the presented algorithms was nearly complete and preliminary results on the performance of the decision tree were expected soon.", "labels": [], "entities": []}, {"text": "The evaluation criterion to be used is the perplexity of the test data with respect to the tree.", "labels": [], "entities": []}, {"text": "A comparison with the perplexity of a standard back-off trigram model will indicate which model performs better.", "labels": [], "entities": []}, {"text": "Although decision-tree letter language models are inferior to their N-gram counterparts, the situation should be reversed for word language models.", "labels": [], "entities": []}], "tableCaptions": []}