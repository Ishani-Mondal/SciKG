{"title": [{"text": "Inside-Outside Estimation of a Lexicalized PCFG for German", "labels": [], "entities": []}], "abstractContent": [{"text": "The paper describes an extensive experiment in inside-outside estimation of a lexicalized proba-bilistic context free grammar for German verb-final clauses.", "labels": [], "entities": []}, {"text": "Grammar and formalism features which make the experiment feasible are described.", "labels": [], "entities": []}, {"text": "Successive models are evaluated on precision and recall of phrase markup.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9995063543319702}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.997822642326355}]}], "introductionContent": [{"text": "Charniak and present head-lexicalized probabilistic context free grammar formalisms, and show that they can effectively be applied in inside-outside estimation of syntactic language models for English, the parameterization of which encodes lexicalized rule probabilities and syntactically conditioned word-word bigram collocates.", "labels": [], "entities": []}, {"text": "The present paper describes an experiment where a slightly modified version of Carroll and Rooth's model was applied in a systematic experiment on German, which is a language with rich inflectional morphology and free word order (or rather, compared to English, free-er phrase order).", "labels": [], "entities": []}, {"text": "We emphasize techniques which made it practical to apply inside-outside estimation of a lexicalized context free grammar to such a language.", "labels": [], "entities": []}, {"text": "These techniques relate to the treatment of argument cancellation and scrambled phrase order; to the treatment of case features in category labels; to the category vocabulary for nouns, articles, adjectives and their projections; to lexicalization based on uninflected lemmata rather than word forms; and to exploitation of a parameter-tying feature.", "labels": [], "entities": [{"text": "argument cancellation", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7470372915267944}]}], "datasetContent": [{"text": "For the evaluation, a total of 600 randomly selected clauses were manually annotated by two labelers.", "labels": [], "entities": []}, {"text": "Using a chart browser, the labellers filled the appropriate cells with category names of NCs and those of maximal VP projections (cf. for an example of NC-labelling).", "labels": [], "entities": []}, {"text": "Subsequent alignment of the labelers decisions resulted in a total of 1353 labelled NC categories (with four different cases).", "labels": [], "entities": []}, {"text": "The total of 584 labelled VP categories subdivides into 21 different verb frames with 340 different lemma heads.", "labels": [], "entities": []}, {"text": "The dominant frames are active transitive (164 occurrences) and active intransitive (117 occurrences).", "labels": [], "entities": []}, {"text": "They represent almost half of the annotated frames.", "labels": [], "entities": []}, {"text": "Thirteen frames occur less than ten times, five of which just once.", "labels": [], "entities": []}, {"text": "Figure 8 plots precision/recall for the training runs described in section 5.1, with lexicalized parsing starting after 0, 2, or 60 unlexicalized iterations.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9995278120040894}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9972586631774902}]}, {"text": "The best results are achieved by starting with lexicalized training after two iterations of unlexicalized training.", "labels": [], "entities": []}, {"text": "Of a total of 1353 annotated NCs with case, 1103 are correctly recognized in the best unlexicalized model and 1112 in the last lexicalized model.", "labels": [], "entities": []}, {"text": "With a number of 1295 guesses in the unlexicalized and 1288 guesses in the final lexicalized model, we gain 1.2% in precision (85.1% vs. 86.3%) and 0.6% in recall (81.5% vs. 82.1%) through lexicalized training.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9994482398033142}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9993942975997925}]}, {"text": "Adjustment to parsed clauses yields 88% vs. 89.2% in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9982200264930725}]}, {"text": "As shown in, the gain is achieved already within the first iteration; it is equally distributed between corrections of category boundaries and labels.", "labels": [], "entities": []}, {"text": "The comparatively small gain with lexicalized training could be viewed as evidence that the chunking task is too simple for lexical information to make a difference.", "labels": [], "entities": []}, {"text": "However, we find about 7% revised guesses from the unlexicalized to the first lexicalized model.", "labels": [], "entities": []}, {"text": "Currently, we do not have a clear picture of the newly introduced errors.", "labels": [], "entities": []}, {"text": "The plots labeled \"00\" are results for lexicalized training starting from a random initial grammar.", "labels": [], "entities": []}, {"text": "The precision measure of the first lexicalized model falls below that of the unlexicalized random model (74%), only recovering through lexicalized training to equalize the precision measure of the random model (75.6%).", "labels": [], "entities": [{"text": "precision measure", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9881008267402649}, {"text": "precision measure", "start_pos": 172, "end_pos": 189, "type": "METRIC", "confidence": 0.9841316938400269}]}, {"text": "This indicates that some degree of unlexicalized initialization is necessary, if a good lexica]ized model is to be obtained.", "labels": [], "entities": []}, {"text": "() report 84.4% recall and 84.2% for NP and PP chunking without case labels.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9989007711410522}, {"text": "NP and PP chunking", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.6626973748207092}]}, {"text": "While these are numbers fora simpler problem and are slightly below ours, they are figures for an experiment on unrestricted sentences.", "labels": [], "entities": []}, {"text": "A genuine comparison has to await extension of our model to free text.", "labels": [], "entities": []}, {"text": "gives results for verb frame recognition under the same training conditions.", "labels": [], "entities": [{"text": "verb frame recognition", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7493172685305277}]}, {"text": "Again, we achieve best results by lexicalising the second unlexicalized model.", "labels": [], "entities": []}, {"text": "Of a total of 584 annotated verb frames, 384 are correctly recognized in the best unlexicalized model and 397 through subsequent lexicalized training.", "labels": [], "entities": []}, {"text": "Precision for the best unlexicalized model is 68.4%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9960786700248718}]}, {"text": "This is raised by 2% to 70.4% through lexicalized training; recall is 65.7%/68%; adjustment by 41 unparsed misses makes for 70.4%/72.8% in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.99968421459198}, {"text": "adjustment", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9764165878295898}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9982662796974182}]}, {"text": "The rather small improvements are in contrast to 88 differences in parser markup, i.e. 15.7%, between the unlexicalized and second lexicalized model.", "labels": [], "entities": []}, {"text": "The main gain is observed within the first two iterations (cf.; for readability, we dropped the recall curves when more or less parallel to the precision curves).", "labels": [], "entities": [{"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.997619092464447}, {"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9880549311637878}]}, {"text": "Results for lexicalized training without prior unlexicalized training are better than in the NC evaluation, but fall short of our best results by more than 2%.", "labels": [], "entities": [{"text": "NC evaluation", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.847603976726532}]}, {"text": "The most notable observation in verb frame evaluation is the decrease of precision of frame recognition in unlexicalized training from the second iteration onward.", "labels": [], "entities": [{"text": "verb frame evaluation", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7849352955818176}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9994254112243652}, {"text": "frame recognition", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.658839076757431}]}, {"text": "After several dozen it-  The plot labeled \"lex 60\" gives precision fora lexicalized training starting from the unlexicalized model obtained with 60 iterations, which measured by linguistic criteria is a very poor state.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9918776750564575}]}, {"text": "As far as we know, lexicalized EM estimation never recovers from this bad state.", "labels": [], "entities": [{"text": "EM estimation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9248242974281311}]}, {"text": "Because examination of individual cases showed that PP attachments are responsible for many errors, we did a separate evaluation of non-PP frames.", "labels": [], "entities": []}, {"text": "We filtered out all frames labelled with a PP argument from both the maximal probability parses and the manually annotated frames (91 filtered frames), measuring precision and recall against the remaining 493 labeller annotated non-PP frames.", "labels": [], "entities": [{"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.999697208404541}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9994016885757446}]}, {"text": "For the best lexicalized model, we find somewhat but not excessively better results than those of the evaluation of the entire set of frames.", "labels": [], "entities": []}, {"text": "Of 527 guessed frames in parser markup, 382 are correct, i.e. a precision of 72.5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9994403719902039}]}, {"text": "The recall figure of 77.5~0 is considerably better since overgeneration of 34 guesses is neglected.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9996246099472046}]}, {"text": "The differences with respect to different starting points for lexicalization emulate those in the evaluation of all frames.", "labels": [], "entities": []}, {"text": "The rather spectacular looking precision and recall differences in unlexicalized training confirm what was observed for the full frame set.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9983672499656677}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9990276098251343}]}, {"text": "From the first trained unlexicalized model throughout unlexicalized training, we find a steady increase in precision (70% first trained model to 78% final model) against a sharp drop in recall (78% peek in the second model vs. 50% in the final).", "labels": [], "entities": [{"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9995912909507751}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9994710087776184}]}, {"text": "Considering our above remarks on the difficulties of frame recognition in unlexicalized training, the sharp drop in recall is to be expected: Since recall measures the correct parser guesses against the annotator's baseline, the tendency to favor PP arguments over PP-adjuncts leads to a loss in guesses when PP-frames are abandoned.", "labels": [], "entities": [{"text": "frame recognition", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7424910664558411}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9993407130241394}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9892740845680237}]}, {"text": "Similarly, the rise in precision is mainly explained by the decreasing number of guesses when cutting out non-PP frames.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9994438290596008}]}, {"text": "For further discussion of what happens with individual frames, we refer the reader to.", "labels": [], "entities": []}, {"text": "One systematic result in these plots is that performance of lexicalized training stabilizes after a few iterations.", "labels": [], "entities": []}, {"text": "This is consistent with what happens with rule parameters for individual verbs, which are close to their final values within five iterations.", "labels": [], "entities": []}], "tableCaptions": []}