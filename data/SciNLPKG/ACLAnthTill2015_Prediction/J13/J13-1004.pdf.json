{"title": [{"text": "Morphological and Syntactic Case in Statistical Dependency Parsing", "labels": [], "entities": [{"text": "Statistical Dependency Parsing", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.71261199315389}]}], "abstractContent": [{"text": "Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb.", "labels": [], "entities": []}, {"text": "The standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation of morphological (case) information prior to automatic syntactic analysis.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.8077477812767029}]}, {"text": "Parsing experiments on Czech, German, and Hungarian show that this approach is susceptible to propagating morphological annotation errors when parsing languages displaying syncretism in their morphological case paradigms.", "labels": [], "entities": []}, {"text": "We develop a different architecture where we use case as a possibly underspecified filtering device restricting the options for syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7273821085691452}]}, {"text": "Carefully designed morpho-syntactic constraints can delimit the search space of a statistical dependency parser and exclude solutions that would violate the restrictions overtly marked in the morphology of the words in a given sentence.", "labels": [], "entities": []}, {"text": "The constrained system outperforms a state-of-the-art data-driven pipeline architecture, as we show experimentally, and, in addition, the parser output comes with guarantees about local and global morpho-syntactic wellformedness, which can be useful for downstream applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "In statistical parsing, many of the first models were developed and optimized for English.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8719562888145447}]}, {"text": "This is not surprising, given that English is the predominant language for research in both computational linguistics and linguistics proper.", "labels": [], "entities": []}, {"text": "By design, the statistical parsing approach avoids language-specific decisions built into the model architecture; models should in principle be trainable on any data following the general treebank representation scheme.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7877472043037415}]}, {"text": "At the same time, it is well known from theoretical and typological work in linguistics that there is abroad multi-dimensional spectrum of language types, and that English is in a rather \"extreme\" area in that it marks grammatical relations (subject, object, etc.) strictly with phrase-structural configurations.", "labels": [], "entities": []}, {"text": "There are only residues of an inflectional morphology left.", "labels": [], "entities": []}, {"text": "In other words, one cannot exclude that architectural or representational modeling decisions established as empirically useful on English data maybe favoring the specific language type of English.", "labels": [], "entities": []}, {"text": "Indeed, carrying over successful model architectures from English to typologically different languages mostly leads to a substantial drop in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 141, "end_pos": 148, "type": "TASK", "confidence": 0.97408527135849}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9376885890960693}]}, {"text": "Linguistically aware representational adjustments can help reduce the problem significantly, as showed in their pivotal study adjusting a statistical (constituent) parsing model to a highly inflectional language with free word order, Czech in that case, pushing the results more than seven percentage points up to a final 80% dependency accuracy (as compared with 91% accuracy for the English \"source\" parser on the Wall Street Journal).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 337, "end_pos": 345, "type": "METRIC", "confidence": 0.8409916162490845}, {"text": "accuracy", "start_pos": 368, "end_pos": 376, "type": "METRIC", "confidence": 0.9924020767211914}, {"text": "Wall Street Journal", "start_pos": 416, "end_pos": 435, "type": "DATASET", "confidence": 0.908658484617869}]}, {"text": "Even in recent years, however, a clear gap has remained between the top parsing architecture for English and morphologically rich(er) languages.", "labels": [], "entities": []}, {"text": "The relative hardness of the parsing task, compared with English, cuts across statistical parsing approaches (constituent or dependency parsing) and across morphological subtypes, such as languages with a moderately sized remaining inflectional system (like German), highly inflected languages (like Czech), and languages in which interactions with derivational morphology make the segmentation question non-trivial (such as Turkish or Arabic, compare, for example, Eryi\u02c7gitEryi\u02c7 Eryi\u02c7git, Nivre, and Oflazer).", "labels": [], "entities": [{"text": "parsing task", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9134626686573029}, {"text": "constituent or dependency parsing", "start_pos": 110, "end_pos": 143, "type": "TASK", "confidence": 0.7479522675275803}]}, {"text": "Still, it remains hard to pinpoint systematic architectural or representational factors that explain the empirical picture, although there is a collection of \"recipes\" one can try to tune an approach to a \"hard language.\"", "labels": [], "entities": []}, {"text": "Of course, there are good reasons for adjusting a well-proven system rather than developing a more general one from scratch-given that part of the success of statistical parsing in general lies in subtle ways of exploiting statistical patterns that reflect inaccessible levels of information in an indirect way.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.7879146635532379}]}, {"text": "This article attempts to do justice to the special status of mature data-driven systems and still contribute to a systematic clarification, by (1) focusing on a clear-cut aspect of morphological marking relevant to syntactic parsing (namely, case marking of core arguments); (2) comparing a selection of languages covering part of the typological spectrum (Czech, German, and Hungarian); (3) using a state-of-the-art data-driven parser to establish how far the technique of representational adjustments may take us; and (4) performing a problem-oriented comparison with an alternative architecture, which allows us to add constraints motivated from linguistic considerations.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 215, "end_pos": 232, "type": "TASK", "confidence": 0.716633141040802}, {"text": "case marking of core arguments)", "start_pos": 242, "end_pos": 273, "type": "TASK", "confidence": 0.8283847173055013}]}, {"text": "Ina first experiment, we vary the morphological information available to the parser and examine the errors of the parser with respect to the case-related functions.", "labels": [], "entities": []}, {"text": "It turns out that although the parser is indeed able to learn the case-function mapping for all three languages, it is susceptible to errors that are propagated through the pipeline model when parsing languages that show syncretism 2 in their morphological paradigms, in our case Czech and German (e. g., for neuter nouns, nominative and accusative case have the same surface form).", "labels": [], "entities": []}, {"text": "In contrast, due to its mostly unambiguous case system, we find a much smaller effect for Hungarian.", "labels": [], "entities": []}, {"text": "Although the parser itself profits much from morphological information as our experiments with gold standard morphology show, errors in automatically predicted morphological information frequently cause errors in the syntactic analysis.", "labels": [], "entities": []}, {"text": "In order to better handle syncretism in the morphological description, we then propose a different way of integrating morphology into the parsing process.", "labels": [], "entities": []}, {"text": "We develop an alternative architecture that circumvents the strict separation of morphological and syntactic analysis in the pipeline model.", "labels": [], "entities": []}, {"text": "We adopt the integer linear programming (henceforth ILP) approach by, which we augment with a set of linguistically motivated constraints modeling the morpho-syntactic dependencies in the languages.", "labels": [], "entities": []}, {"text": "Case is herein interpreted as an underspecified filtering device that guides a statistical model by restricting the search space of the parser.", "labels": [], "entities": []}, {"text": "Due to the constraints, the output of the ILP parser is guaranteed to obey all syntactic restrictions that are marked overtly in the morphological form of the words.", "labels": [], "entities": []}, {"text": "Although the restrictions are implemented as symbolic constraints, they are applied to the parser during the search for the best tree, which is driven by a statistical model.", "labels": [], "entities": []}, {"text": "We show in a second experiment that restricting the search space in this way improves the performance on argument functions (indicated by case morphology) considerably on all three languages while the performance on all other functions stays stable.", "labels": [], "entities": []}, {"text": "We proceed by first discussing the role of case morphology in syntax (Section 2), followed by a presentation of the parsing architecture of the Bohnet parser with a discussion of the relevant aspects for our first experiment (Section 3).", "labels": [], "entities": []}, {"text": "Next, we compare the morphological annotation quality of automatic tools with the gold standard across languages (Section 4).", "labels": [], "entities": []}, {"text": "We then turn to the first experiment in this article where we examine the performance of the parser with respect to core argument functions on the three languages (Section 5).", "labels": [], "entities": []}, {"text": "In the second experiment (Section 6), we apply an ILP parser to the data sets augmented with a set of linguistic constraints that integrate morphological information in an underspecified way into the parsing architecture.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Having examined the quality of the predicted morphological information in the data sets, we can now investigate how the parser deals with this information.", "labels": [], "entities": []}, {"text": "We proceed as follows: We train three different models for each language, one using gold standard morphology, one using predicted morphology, and one using no morphological information (henceforth GOLD-M, PRED-M, and NO-M).", "labels": [], "entities": []}, {"text": "Comparing the performance of these three models allows us to seethe effect that the morphological information has on the parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 121, "end_pos": 128, "type": "TASK", "confidence": 0.9705360531806946}]}, {"text": "The model using gold morphology serves as an upper bound where we can observe the behavior of the parser when it is not disturbed by errors coming from the automatic morphological analyzers.", "labels": [], "entities": []}, {"text": "Note that this model is very unrealistic in the sense that syncretisms are fully resolved in the morphological information.", "labels": [], "entities": []}, {"text": "The model using predicted morphology serves as a realistic scenario where we can observe the problems introduced by imperfect preprocessing and propagated errors in the pipeline (e.g., due to syncretism).", "labels": [], "entities": []}, {"text": "And finally, the model using no morphology shows us how much non-morphological information contributes to the parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9773549437522888}]}, {"text": "In comparison with the other two models, we can then seethe contribution of morphological information 13 to the parsing process.", "labels": [], "entities": [{"text": "parsing", "start_pos": 112, "end_pos": 119, "type": "TASK", "confidence": 0.9847837090492249}]}, {"text": "All models use the same predicted lemma and POS information as discussed in the previous section.", "labels": [], "entities": [{"text": "POS", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9552741646766663}]}, {"text": "We performed a five-fold cross annotation 14 on the training portions of the data sets of Czech and German, and on the whole subcorpus of Hungarian, varying the morphological annotation as described.", "labels": [], "entities": []}, {"text": "The overall parsing performance is shown in, where the German and the Hungarian scores exclude punctuation and the Czech scores include them.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9649701714515686}]}, {"text": "15 gives us the usual picture that has been noticed in several shared tasks on dependency parsing for multiple languages (e.g.,).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8036633431911469}]}, {"text": "The performance on German is pretty high, although not as high as it would be for English, and the performance on Czech is rather low.", "labels": [], "entities": []}, {"text": "Note the extreme divergence between labeled (LAS) and unlabeled attachment score (UAS) for Czech.", "labels": [], "entities": [{"text": "labeled (LAS) and unlabeled attachment score (UAS)", "start_pos": 36, "end_pos": 86, "type": "METRIC", "confidence": 0.7868442995981737}]}, {"text": "For Hungarian, the performance is comparable to Czech in terms of UAS but the LAS for Hungarian is better.", "labels": [], "entities": [{"text": "UAS", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9793456792831421}, {"text": "LAS", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9980384707450867}]}, {"text": "We also seethe expected ordering in performance for the models using different kinds of morphological information.", "labels": [], "entities": []}, {"text": "The gold models always outperform the models using predicted morphology, which in turn outperform the models using no morphological information.", "labels": [], "entities": []}, {"text": "Note, however, that whereas the performance on German does not degrade very much when using no morphological information, it is very 13 It should be noted that by morphological information we always mean the complete annotation available in the treebanks.", "labels": [], "entities": [{"text": "German", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.9313374757766724}]}, {"text": "Although we concentrate in the analysis on the phi-features (gender, number, case), the models using morphological information always use the whole set, including also, for example, verbal morphology.", "labels": [], "entities": []}, {"text": "14 The number of iterations during training was set to 10. 15 Punctuation in the Czech data set is sometimes used as the head in coordination.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.975155234336853}, {"text": "Czech data set", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.9450424512227377}]}, {"text": "16 This is due to the way the Czech data label certain phenomena, which makes it difficult for the parser to decide on the correct label.", "labels": [], "entities": [{"text": "Czech data label", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8850964307785034}]}, {"text": "See Boyd, Dickinson, and Meurers (2008, pages 8-9) for examples.", "labels": [], "entities": [{"text": "Boyd, Dickinson, and Meurers (2008, pages 8-9)", "start_pos": 4, "end_pos": 50, "type": "DATASET", "confidence": 0.796723964313666}]}, {"text": "harmful for Hungarian to do so (78.04% LAS for NO-M in comparison with 84.33% LAS for PRED-M).", "labels": [], "entities": [{"text": "LAS", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9880234003067017}]}, {"text": "The Czech results lie in between.", "labels": [], "entities": []}, {"text": "To give a general impression of the performance of the parser, the last row shows parsing results for the three languages reported in the literature.", "labels": [], "entities": []}, {"text": "The results have been obtained on different data sets, however, so a direct comparison would be invalid.", "labels": [], "entities": []}, {"text": "In the second experiment, we now apply the ILP parser to the same data sets that we used in the first experiment, again with a five-fold cross-annotation.", "labels": [], "entities": []}, {"text": "We trained two  models for each language, one using the constraints (c) and one without the constraints (no-c).", "labels": [], "entities": []}, {"text": "In both cases, we used the predicted morphology in the feature set.", "labels": [], "entities": []}, {"text": "shows the parsing results for the ILP parsing models in terms of LAS and UAS in comparison to the results of the Bohnet parser (repeated from).", "labels": [], "entities": [{"text": "parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9617824554443359}, {"text": "ILP parsing", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.6567225158214569}, {"text": "LAS", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9941787719726562}, {"text": "UAS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9895848631858826}]}, {"text": "Both ILP models should be compared to the PRED-M model because they have the most similar feature sets.", "labels": [], "entities": []}, {"text": "As can be seen from the results, the ILP parser without constraints performs overall slightly worse than the Bohnet parser and the ILP parser using constraints performs overall slightly better or equal.", "labels": [], "entities": []}, {"text": "This shows that both parsers perform on a similar level.", "labels": [], "entities": []}, {"text": "The differences between the Czech and the German models (ILP C vs. PRED-M) are statistically significant.", "labels": [], "entities": []}, {"text": "The interesting results, however, occur for the argument functions.", "labels": [], "entities": []}, {"text": "shows the performance of the unconstrained (no-c) and constrained (c) ILP models and the PRED-M models of the Bohnet parser on the argument functions.", "labels": [], "entities": []}, {"text": "Again, Parsing results for the unconstrained (NO-C) and the constrained (C) ILP models, and the Bohnet parser in terms of F-score (LAS) for core grammatical functions marked by case.", "labels": [], "entities": [{"text": "F-score (LAS)", "start_pos": 122, "end_pos": 135, "type": "METRIC", "confidence": 0.97598996758461}]}, {"text": "We omit locative objects in Czech, and second accusative objects in German because of their extremely low frequency.", "labels": [], "entities": []}, {"text": "* Statistically significant when comparing the performance on a grammatical function for the C model to the PRED-M model (\u03b1 = 0.05, two-tailed t-test for related samples).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Annotation quality of the phi-features (case, gender, and number) for all words and for those  words with a correctly predicted POS tag.", "labels": [], "entities": [{"text": "Annotation quality", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9315202534198761}]}, {"text": " Table 3  Overall performance of the Bohnet parser on the five-fold cross annotation for every language  and different kind of morphological annotation. All results in percent. LAS = labeled attachment  score; UAS = unlabeled attachment score. Results for German and Hungarian are without  punctuation. Best score for Czech on the CoNLL 2009 Shared Task was by Gesmundo et al.  (2009), best score for German was by Bohnet (2009), best score for Hungarian on the CoNLL  2007 Shared Task was by Nivre et al. (2007a). Best CoNLL 09/07 results were obtained on  different data sets.", "labels": [], "entities": [{"text": "LAS", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.9854881167411804}, {"text": "UAS", "start_pos": 210, "end_pos": 213, "type": "METRIC", "confidence": 0.977141261100769}, {"text": "CoNLL 2009 Shared Task", "start_pos": 331, "end_pos": 353, "type": "DATASET", "confidence": 0.8541035056114197}, {"text": "CoNLL  2007 Shared Task", "start_pos": 462, "end_pos": 485, "type": "DATASET", "confidence": 0.9439643621444702}, {"text": "CoNLL 09/07", "start_pos": 520, "end_pos": 531, "type": "DATASET", "confidence": 0.9001743048429489}]}, {"text": " Table 5  Precision, recall, and F-score (LAS) for core grammatical functions marked by case. We omit  locative objects in Czech, and second accusative objects in German, due to their low frequency.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.996976375579834}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9964867830276489}, {"text": "F-score (LAS)", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.969058483839035}]}, {"text": " Table 7  Top five functions with which subjects were confused when parsing with the GOLD-M models.  M marks a coordinated function in Czech.", "labels": [], "entities": [{"text": "GOLD-M", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.763961136341095}]}, {"text": " Table 8  Top five functions with which accusative objects were confused when parsing with the gold  (left) and predicted (right) morphology models. M marks a coordinated function in Czech.", "labels": [], "entities": []}, {"text": " Table 9  Number of times a core grammatical function was annotated more than once in the treebank  (TRBK) by the model using gold morphology (GOLD-M), and by the model using predicted  morphology (PRED-M).", "labels": [], "entities": [{"text": "TRBK", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9481292963027954}]}, {"text": " Table 10  Overall performance of the Bohnet parser and the ILP parser on the five-fold cross annotation  for every language. All results in percent. LAS = labeled attachment score, UAS = unlabeled  attachment score. Results for German and Hungarian are without punctuation.", "labels": [], "entities": [{"text": "LAS = labeled attachment score", "start_pos": 150, "end_pos": 180, "type": "METRIC", "confidence": 0.7258923768997192}, {"text": "UAS", "start_pos": 182, "end_pos": 185, "type": "METRIC", "confidence": 0.9905264377593994}]}, {"text": " Table 11  Parsing results for the unconstrained (NO-C) and the constrained (C) ILP models, and the  Bohnet parser in terms of F-score (LAS) for core grammatical functions marked by case.  We omit locative objects in Czech, and second accusative objects in German because of their  extremely low frequency.  *  Statistically significant when comparing the performance on a  grammatical function for the C model to the PRED-M model (\u03b1 = 0.05, two-tailed t-test for  related samples).", "labels": [], "entities": [{"text": "F-score (LAS)", "start_pos": 127, "end_pos": 140, "type": "METRIC", "confidence": 0.9788110703229904}]}]}