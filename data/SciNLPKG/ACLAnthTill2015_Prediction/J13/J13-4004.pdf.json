{"title": [{"text": "Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules United States Naval Academy", "labels": [], "entities": [{"text": "Deterministic Coreference Resolution", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7265047430992126}]}], "abstractContent": [{"text": "We propose anew deterministic approach to coreference resolution that combines the global information and precise features of modern machine-learning models with the transparency and modularity of deterministic, rule-based systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9759383201599121}]}, {"text": "Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model's cluster output.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9784922003746033}]}, {"text": "The two stages of our sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision-oriented, offer a powerful way to achieve both high precision and high recall.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.698078840970993}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9942511320114136}, {"text": "precision-oriented", "start_pos": 142, "end_pos": 160, "type": "METRIC", "confidence": 0.9871298670768738}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9942480325698853}, {"text": "recall", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9916089177131653}]}, {"text": "Further, our approach makes use of global information through an entity-centric model that encourages the sharing of features across all mentions that point to the same real-world entity.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and Arabic.", "labels": [], "entities": []}, {"text": "Our system thus offers anew paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9143110513687134}, {"text": "natural language understanding", "start_pos": 123, "end_pos": 153, "type": "TASK", "confidence": 0.634931355714798}, {"text": "summarization", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.985310435295105}, {"text": "question answering", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.873301774263382}, {"text": "information extraction", "start_pos": 204, "end_pos": 226, "type": "TASK", "confidence": 0.8578419983386993}]}, {"text": "The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution;.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9670215249061584}]}, {"text": "Recent work has also shown the importance of global inference-performing coreference resolution jointly for several or all mentions in a document-rather than greedily disambiguating individual pairs of mentions (Morton 2000;.", "labels": [], "entities": [{"text": "global inference-performing coreference resolution", "start_pos": 45, "end_pos": 95, "type": "TASK", "confidence": 0.7857091426849365}]}, {"text": "Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.9094994068145752}]}, {"text": "But machine learning, although powerful, has limitations.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7149643748998642}]}, {"text": "Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains.", "labels": [], "entities": []}, {"text": "Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well.", "labels": [], "entities": []}, {"text": "Rule-based models like were a popular early solution to the subtask of pronominal anaphora resolution.", "labels": [], "entities": [{"text": "pronominal anaphora resolution", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.7406505147616068}]}, {"text": "Rules are easy to create and maintain and error analysis is more transparent.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.8426597416400909}]}, {"text": "But early rule-based systems relied on hand-tuned weights and were not capable of global inference, two factors that led to poor performance and replacement by machine learning.", "labels": [], "entities": []}, {"text": "We propose anew approach that brings together the insights of these modern supervised and unsupervised models with the advantages of deterministic, rule-based systems.", "labels": [], "entities": []}, {"text": "We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules.", "labels": [], "entities": []}, {"text": "Our work is inspired both by the seminal early work of, who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference ( and for named entity recognition (.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.757736474275589}, {"text": "named entity recognition", "start_pos": 330, "end_pos": 354, "type": "TASK", "confidence": 0.6374318500359853}]}, {"text": "illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8206794857978821}, {"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.9669952690601349}]}, {"text": "In the mention detection stage, nominal and pronominal mentions are identified using a high-recall algorithm that selects all noun phrases (NPs), pronouns, and named entity mentions, and then filters out non-mentions (pleonastic it, i-within-i, numeric entities, partitives, etc.).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7236322909593582}]}, {"text": "The coreference resolution stage is based on a succession often independent coreference models (or \"sieves\"), applied from highest to lowest precision.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9454918801784515}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9811660647392273}]}, {"text": "Precision can be informed by linguistic intuition, or empirically determined on a coreference corpus (see Section 4.4.3).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9033103585243225}]}, {"text": "For example, the first (highest precision) sieve links first-person pronouns inside a quotation with the speaker of a quotation, and the tenth sieve (i.e., low precision but high recall) implements generic pronominal coreference resolution.", "labels": [], "entities": [{"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9969608187675476}, {"text": "generic pronominal coreference resolution", "start_pos": 198, "end_pos": 239, "type": "TASK", "confidence": 0.7734636962413788}]}, {"text": "Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8709925711154938}]}], "datasetContent": [{"text": "Tables 4 and 5 compare the performance of our system with other state-of-the-art systems in the CoNLL-2011 shared task and the ACE and MUC corpora, respectively.", "labels": [], "entities": [{"text": "CoNLL-2011 shared task", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.7829728921254476}, {"text": "ACE", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.9269235134124756}, {"text": "MUC corpora", "start_pos": 135, "end_pos": 146, "type": "DATASET", "confidence": 0.774820476770401}]}, {"text": "For the CoNLL-2011 shared task we report results in the closed track, which did not allow the use of external resources, and the open track, which allowed any other is that in the former (other than its last block) we used predicted mentions (detected with the algorithm described in Section 3.1), whereas in the latter we used gold mentions.", "labels": [], "entities": []}, {"text": "The only reason for this distinction is to facilitate comparison with previous work (all systems listed in used gold mention boundaries).", "labels": [], "entities": []}, {"text": "The two tables show that, regardless of evaluation corpus and methodology, our system generally outperforms the previous state of the art.", "labels": [], "entities": []}, {"text": "In the CoNLL shared task, our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.7183865308761597}, {"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9151606559753418}]}, {"text": "(2011) system has marginally higher B 3 and BLANC F1 scores, but does not outperform our model on the other two metrics and the average F1 score.", "labels": [], "entities": [{"text": "B 3", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9903381764888763}, {"text": "BLANC F1 scores", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.836479922135671}, {"text": "F1 score", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9763784408569336}]}, {"text": "shows that our model has higher B 3 F1 scores than all the other models in the two ACE corpora.", "labels": [], "entities": [{"text": "B 3 F1 scores", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9031083136796951}, {"text": "ACE corpora", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.8388893306255341}]}, {"text": "The model of minimally outperforms ours by 0.6 B 3 F1 points in the MUC corpus.", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9696836471557617}, {"text": "MUC corpus", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9827093482017517}]}, {"text": "All in all, these results prove that our approach compares favorably with a wide range of models, which include most aspects deemed important for coreference resolution, among other things, supervised learning using rich feature sets), joint inference using spectral clustering, and deterministic rule-based models.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.9517664015293121}]}, {"text": "We discuss in more detail the similarities and differences between our approach and previous work in Section 6.", "labels": [], "entities": []}, {"text": "shows that using additional resources yields minimal improvement: There is a difference of only 0.5 CoNLL F1 points between our open-track and closed-track systems.", "labels": [], "entities": [{"text": "CoNLL F1", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8586617708206177}]}, {"text": "We show in Section 5 that the explanation of this modest improvement is that most of the remaining errors require complex, context-sensitive semantics to be solved.", "labels": [], "entities": []}, {"text": "Such semantic models cannot be built with our shallow feature set that relies on simple semantic dictionaries (e.g., animacy or even hyponymy).", "labels": [], "entities": []}, {"text": "It is not trivial to compare the mention detection system alone because its score is affected by the performance of the coreference resolution model.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.731877326965332}, {"text": "coreference resolution", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.8946762382984161}]}, {"text": "For example, even if we start with a perfect set of gold mentions, if we miss all coreference relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post processing, resulting in zero mentions in the final output.", "labels": [], "entities": []}, {"text": "Therefore, we included the score using gold mention boundaries in the last part of (\"Closed Track -gold boundaries\") to isolate the performance of the coreference resolution component.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.9156363010406494}]}, {"text": "This experiment shows that our system outperforms the others with a considerable margin, demonstrating that our coreference resolution model, rather than the mention detection component, is the one responsible for the overall performance.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.8441410660743713}, {"text": "mention detection", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.6585667133331299}]}, {"text": "We use five evaluation metrics widely used in the literature.", "labels": [], "entities": []}, {"text": "B 3 and CEAF have implementation variations in how to take system mentions into account.", "labels": [], "entities": [{"text": "B 3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7837733924388885}, {"text": "CEAF", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9127674102783203}]}, {"text": "We followed the same implementation as used in CoNLL-2011 shared task.", "labels": [], "entities": []}, {"text": "r MUC () -link-based metric which measures how many predicted and gold mention clusters need to be merged to cover the gold and predicted clusters, respectively.", "labels": [], "entities": [{"text": "MUC", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.7106289267539978}]}, {"text": "(S i : a system mention cluster, p(S i ): partitions of Si ) F1 = 2PR P+R r B 3 (Bagga and Baldwin 1998) -mention-based metric which measures the proportion of overlap between predicted and gold mention clusters fora given mention.", "labels": [], "entities": [{"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9974443912506104}, {"text": "2PR P+R r B 3", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9529315999576023}]}, {"text": "When G mi is the gold cluster of mention mi and Sm i is the system cluster of mention mi , on entity alignment.", "labels": [], "entities": []}, {"text": "For best alignment g * = argmax g\u2208G m \u03a6(g) (\u03a6(g): total similarity of g, a one-to-one mapping from G: gold mention clusters to S: system mention clusters), |R|+|S| , it is called entity-based CEAF (CEAF-\u03c6 4 ).", "labels": [], "entities": []}, {"text": "r BLANC (BiLateral Assessment of NounPhrase Coreference) (Recasens and Hovy 2011) -metric applying the Rand index) to coreference to deal with imbalance between singletons and coreferent mentions by considering coreference and non-coreference links.", "labels": [], "entities": [{"text": "BLANC", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9808534383773804}]}], "tableCaptions": [{"text": " Table 1  A sample run-through of our approach, applied to a made-up sentence. In each step we mark in  bold the affected mentions; superscript and subscript indicate entity id and mention id.", "labels": [], "entities": []}, {"text": " Table 4  Performance of the top systems in the CoNLL-2011 shared task. All these systems use  automatically detected mentions. We report results for both the closed and the open tracks,  which allowed the use of resources not provided by the task organizers. MD indicates mention  detection, and gold boundaries indicate that mention boundary information is given.", "labels": [], "entities": [{"text": "CoNLL-2011 shared task", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.7707570592562357}, {"text": "mention  detection", "start_pos": 273, "end_pos": 291, "type": "TASK", "confidence": 0.6631901860237122}]}, {"text": " Table 5  Comparison of our system with the other reported results on the ACE and MUC corpora. All  these systems use gold mention boundaries.", "labels": [], "entities": [{"text": "ACE", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9093157649040222}, {"text": "MUC corpora", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.7793335318565369}]}, {"text": " Table 6  Comparison of our entity-centric model against a baseline that handles mention pairs  independently. The former model shares mention features across entities as they are constructed.  The latter model does not.", "labels": [], "entities": []}, {"text": " Table 7  Impact of the multi-pass model. The single-pass baseline uses the same sequence of sieves as the  multi-pass model (i.e., all the sieves introduced in Section 3 with the exception of the optional  ones) but it applies all of them at the same time.", "labels": [], "entities": []}, {"text": " Table 8  Cumulative performance as sieves are added to the system.", "labels": [], "entities": []}, {"text": " Table 11  Distribution of errors.", "labels": [], "entities": [{"text": "errors", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.506446123123169}]}]}