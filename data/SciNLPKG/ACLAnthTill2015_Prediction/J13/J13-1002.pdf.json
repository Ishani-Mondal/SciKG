{"title": [{"text": "Going to the Roots of Dependency Parsing", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6111600697040558}]}], "abstractContent": [{"text": "Dependency trees used in syntactic parsing often include a root node representing a dummy word prefixed or suffixed to the sentence, a device that is generally considered a mere technical convenience and is tacitly assumed to have no impact on empirical results.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7772097289562225}]}, {"text": "We demonstrate that this assumption is false and that the accuracy of data-driven dependency parsers can in fact be sensitive to the existence and placement of the dummy root node.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9987055063247681}]}, {"text": "In particular, we show that a greedy, left-to-right, arc-eager transition-based parser consistently performs worse when the dummy root node is placed at the beginning of the sentence (following the current convention in data-driven dependency parsing) than when it is placed at the end or omitted completely.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 232, "end_pos": 250, "type": "TASK", "confidence": 0.7430641651153564}]}, {"text": "Control experiments with an arc-standard transition-based parser and an arc-factored graph-based parser reveal no consistent preferences but nevertheless exhibit considerable variation in results depending on root placement.", "labels": [], "entities": []}, {"text": "We conclude that the treatment of dummy root nodes in data-driven dependency parsing is an underestimated source of variation in experiments and may also be a parameter worth tuning for some parsers.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6949829012155533}]}], "introductionContent": [{"text": "It is a lesson learned in many studies on natural language processing that choosing the right linguistic representation can be crucial for obtaining high accuracy on a given task.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6954415043195089}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9931579232215881}]}, {"text": "In constituency-based parsing, for example, adding or deleting nodes in syntactic trees can have a substantial impact on the performance of a statistical parser.", "labels": [], "entities": [{"text": "constituency-based parsing", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6386602818965912}]}, {"text": "In dependency parsing, the syntactic representations used offer less opportunity for transformation, given that the nodes of a dependency tree are basically determined by the tokens of the input sentence, except for the possible addition of a dummy word acting as the root of the tree.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8738210499286652}]}, {"text": "In this article, we show that even this seemingly trivial modification can make a difference, and that the exact placement of the dummy root node can have a significant impact on the accuracy of a given parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9987379908561707}]}, {"text": "This suggests that the placement of the dummy root is a parameter worth tuning for certain parsing systems as well as a source of variation to betaken into account when interpreting experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to test the hypothesis that the existence and placement of the dummy root node can have an impact on parsing accuracy, we performed an experiment using two widely used data-driven dependency parsers, MaltParser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.980517566204071}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9349586367607117}]}, {"text": "When creating the data sets, we took the original version from the CoNLL-X shared task as None, because it does not include the dummy root node as an explicit input token.", "labels": [], "entities": []}, {"text": "In this representation, the tokens of a sentence are indexed from 1 ton and the dependency graph is specified by giving each word ahead index ranging from 0 ton, where 0 signifies that the token is not a dependent on any other token in the sentence.", "labels": [], "entities": []}, {"text": "The First version was created by adding an extra token at the beginning of the sentence with index 1 and head index 0, increasing all other token and head indices by 1, meaning that all tokens that previously had ahead index of 0 would now be attached to the new Experimental results for arc-eager (AE), arc-standard (AS), and maximum spanning tree parsing (MST) on all the CoNLL-X data sets plus the English Penn Treebank converted to Stanford dependencies with three different dependency graph types (None, First, Last).", "labels": [], "entities": [{"text": "maximum spanning tree parsing (MST)", "start_pos": 327, "end_pos": 362, "type": "TASK", "confidence": 0.7978397863251823}, {"text": "CoNLL-X data sets", "start_pos": 374, "end_pos": 391, "type": "DATASET", "confidence": 0.938614547252655}, {"text": "English Penn Treebank", "start_pos": 401, "end_pos": 422, "type": "DATASET", "confidence": 0.8325080672899882}]}, {"text": "Evaluation metrics are labeled attachment score (LAS), unlabeled attachment score (UAS), root attachment (or no attachment in the case of None) measured as recall (RR) and precision (RP).", "labels": [], "entities": [{"text": "attachment score (LAS)", "start_pos": 31, "end_pos": 53, "type": "METRIC", "confidence": 0.9055166840553284}, {"text": "unlabeled attachment score (UAS)", "start_pos": 55, "end_pos": 87, "type": "METRIC", "confidence": 0.8348194261391958}, {"text": "recall (RR)", "start_pos": 156, "end_pos": 167, "type": "METRIC", "confidence": 0.9573810696601868}, {"text": "precision (RP)", "start_pos": 172, "end_pos": 186, "type": "METRIC", "confidence": 0.9759461581707001}]}, {"text": "Scores in bold are best in their column (per language); scores in italic are not comparable to the rest because of informative arc labels that cannot be predicted with the None representation.", "labels": [], "entities": []}, {"text": "The Last version was created by adding an extra token at the end of the sentence with index n+1, and changing every head index that previously was 0 to n+1.", "labels": [], "entities": []}, {"text": "In both First and Last, we made sure that the new dummy token had a unique word form and unique values for all other features, so that it could not be mistaken for any real word.", "labels": [], "entities": [{"text": "Last", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9442495107650757}]}, {"text": "For First and Last, we applied an inverse transformation to the parser output before evaluation.", "labels": [], "entities": []}, {"text": "Both MaltParser and MSTParser by default add a dummy root node at the beginning of the sentence internally before parsing, so we had to modify the parsers so that they only considered arcs involving nodes corresponding to input tokens.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.9452130794525146}]}, {"text": "For MaltParser this only required setting a flag that makes the parser start with an empty stack instead of a stack containing an extra dummy root node.", "labels": [], "entities": []}, {"text": "For MSTParser, we modified the parser implementation so that it extracts a maximum spanning tree that is still rooted in an extra dummy root node but where the score of a tree is based only on the scores of arcs connecting real token nodes.", "labels": [], "entities": []}, {"text": "Finally, because MaltParser with the arc-eager and arc-standard transition systems can only construct projective dependency graphs, we projectivized all training sets before training the MaltParser models using the baseline pseudo-projective transformation of Nivre and.", "labels": [], "entities": []}, {"text": "Except for these modifications, all parsers were run with out-of-the-box settings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Experimental results for arc-eager (AE), arc-standard (AS), and maximum spanning tree  parsing (MST) on all the CoNLL-X data sets plus the English Penn Treebank converted to  Stanford dependencies with three different dependency graph types (None, First, Last).  Evaluation metrics are labeled attachment score (LAS), unlabeled attachment score (UAS), root  attachment (or no attachment in the case of None) measured as recall (RR) and precision (RP).  Scores in bold are best in their column (per language); scores in italic are not comparable to the  rest because of informative arc labels that cannot be predicted with the None representation.", "labels": [], "entities": [{"text": "CoNLL-X data sets", "start_pos": 122, "end_pos": 139, "type": "DATASET", "confidence": 0.952849268913269}, {"text": "English Penn Treebank", "start_pos": 149, "end_pos": 170, "type": "DATASET", "confidence": 0.7747592528661092}, {"text": "attachment score (LAS)", "start_pos": 304, "end_pos": 326, "type": "METRIC", "confidence": 0.8027859330177307}, {"text": "unlabeled attachment score (UAS)", "start_pos": 328, "end_pos": 360, "type": "METRIC", "confidence": 0.731975202759107}, {"text": "recall (RR)", "start_pos": 430, "end_pos": 441, "type": "METRIC", "confidence": 0.9491751044988632}, {"text": "precision (RP)", "start_pos": 446, "end_pos": 460, "type": "METRIC", "confidence": 0.9545835852622986}]}]}