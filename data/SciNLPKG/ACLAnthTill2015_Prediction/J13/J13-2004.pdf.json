{"title": [], "abstractContent": [{"text": "Syntactic representations based on word-to-word dependencies have a long-standing tradition in descriptive linguistics, and receive considerable interest in many applications.", "labels": [], "entities": [{"text": "descriptive linguistics", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.8873794376850128}]}, {"text": "Nevertheless, dependency syntax has remained something of an island from a formal point of view.", "labels": [], "entities": [{"text": "dependency syntax", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.8634263575077057}]}, {"text": "Moreover, most formalisms available for dependency grammar are restricted to projective analyses, and thus notable to support natural accounts of phenomena such as wh-movement and cross-serial dependencies.", "labels": [], "entities": [{"text": "dependency grammar", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.8109270334243774}]}, {"text": "In this article we present a formalism for non-projective dependency grammar in the framework of linear context-free rewriting systems.", "labels": [], "entities": []}, {"text": "A characteristic property of our formalism is a close correspondence between the non-projectivity of the dependency trees admitted by a grammar on the one hand, and the parsing complexity of the grammar on the other.", "labels": [], "entities": []}, {"text": "We show that parsing with unrestricted grammars is intractable.", "labels": [], "entities": []}, {"text": "We therefore study two constraints on non-projectivity, block-degree and well-nestedness.", "labels": [], "entities": []}, {"text": "Jointly, these two constraints define a class of \"mildly\" non-projective dependency grammars that can be parsed in polynomial time.", "labels": [], "entities": []}, {"text": "An evaluation on five dependency treebanks shows that these grammars have a good coverage of empirical data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic representations based on word-to-word dependencies have a long-standing tradition in descriptive linguistics.", "labels": [], "entities": [{"text": "Syntactic representations", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8282849788665771}, {"text": "descriptive linguistics", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.8926073908805847}]}, {"text": "Since the seminal work ofTesn\u00ec ere (1959), they have become the basis for several linguistic theories, such as Functional Generative Description, Meaning-Text Theory, and Word Grammar.", "labels": [], "entities": [{"text": "Functional Generative Description", "start_pos": 111, "end_pos": 144, "type": "TASK", "confidence": 0.7454536557197571}, {"text": "Meaning-Text Theory", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7151385992765427}, {"text": "Word Grammar", "start_pos": 171, "end_pos": 183, "type": "TASK", "confidence": 0.7880334854125977}]}, {"text": "In recent years they have also been used fora wide range of practical applications, such as information extraction, machine translation, and question answering.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.880084365606308}, {"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8356724381446838}, {"text": "question answering", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.9566506445407867}]}, {"text": "We ascribe the widespread interest in dependency structures to their intuitive appeal, their conceptual simplicity, and in particular to the availability of accurate and efficient dependency parsers fora wide range of languages.", "labels": [], "entities": []}, {"text": "Although there exist both a considerable practical interest and an extensive linguistic literature, dependency syntax has remained something of an island from a formal point of view.", "labels": [], "entities": [{"text": "dependency syntax", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.8394544720649719}]}, {"text": "In particular, there are relatively few results that bridge between dependency syntax and other traditions, such as phrase structure or categorial syntax.", "labels": [], "entities": []}, {"text": "This makes it hard to gauge the similarities and differences between the paradigms, and hampers the exchange of linguistic resources and computational methods.", "labels": [], "entities": []}, {"text": "An overarching goal of this article is to bring dependency grammar closer to the mainland of formal study.", "labels": [], "entities": [{"text": "dependency grammar", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8229632377624512}]}, {"text": "One of the few bridging results for dependency grammar is thanks to, who studied a formalism that we will refer to as Hays-Gaifman grammar, and proved it to be weakly equivalent to context-free phrase structure grammar.", "labels": [], "entities": [{"text": "dependency grammar", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8574407398700714}, {"text": "phrase structure grammar", "start_pos": 194, "end_pos": 218, "type": "TASK", "confidence": 0.725905030965805}]}, {"text": "Although this result is of fundamental importance from a theoretical point of view, its practical usefulness is limited.", "labels": [], "entities": []}, {"text": "In particular, Hays-Gaifman grammar is restricted to projective dependency structures, which is similar to the familiar restriction to contiguous constituents.", "labels": [], "entities": []}, {"text": "Yet, non-projective dependencies naturally arise in the analysis of natural language.", "labels": [], "entities": []}, {"text": "One classic example of this is the phenomenon of cross-serial dependencies in Dutch.", "labels": [], "entities": []}, {"text": "In this language, the nominal arguments of verbs that also select an infinitival complement occur in the same order as the verbs themselves: (i) dat Jan 1 Piet 2 Marie 3 zag 1 helpen 2 lezen (Dutch) that Jan Piet Marie saw help read 'that Jan saw Piet help Marie read' In German, the order of the nominal arguments instead inverts the verb order: (ii) dass Jan 1 Piet 2 Marie 3 lesen 3 helfen 2 sah 1 (German) that Jan Piet Marie read help saw shows dependency trees for the two examples.", "labels": [], "entities": []}, {"text": "The German linearization gives rise to a projective structure, where the verb-argument dependencies are nested within each other, whereas the Dutch linearization induces a non-projective structure with crossing edges.", "labels": [], "entities": []}, {"text": "To account for such structures we need to turn to formalisms more expressive than Hays-Gaifman grammars.", "labels": [], "entities": []}, {"text": "In this article we present a formalism for non-projective dependency grammar based on linear context-free rewriting systems (LCFRSs).", "labels": [], "entities": []}, {"text": "This framework was introduced to facilitate the comparison of various grammar formalisms, including standard context-free grammar, tree-adjoining grammar, and combinatory categorial grammar.", "labels": [], "entities": []}, {"text": "It also comprises, among others, multiple context-free grammars (, minimalist grammars, and simple range concatenation grammars).", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we provide the technical background to our work; in particular, we introduce our terminology and notation for linear context-free rewriting systems.", "labels": [], "entities": []}, {"text": "An LCFRS generates a set of terms (formal expressions) which are interpreted as derivation trees of objects from some domain.", "labels": [], "entities": []}, {"text": "Each term also has a secondary interpretation under which it denotes a tuple of strings, representing the string yield of the derived object.", "labels": [], "entities": []}, {"text": "In Section 3 we introduce the central notion of a lexicalized linear context-free rewriting system, which is an LCFRS in which each rule of the grammar is associated with an overt lexical item, representing a syntactic head (cf..", "labels": [], "entities": []}, {"text": "We show that this property gives rise to an additional interpretation under which each term denotes a dependency tree on its yield.", "labels": [], "entities": []}, {"text": "With this interpretation, lexicalized LCFRSs can be used as dependency grammars.", "labels": [], "entities": []}, {"text": "In Section 4 we show how to acquire lexicalized LCFRSs from dependency treebanks.", "labels": [], "entities": []}, {"text": "This works in much the same way as the extraction of context-free grammars from phrase structure treebanks (cf., except that the derivation trees of dependency trees are not immediately accessible in the treebank.", "labels": [], "entities": []}, {"text": "We therefore present an efficient algorithm for computing a canonical derivation tree for an input dependency tree; from this derivation tree, the rules of the grammar can be extracted in a straightforward way.", "labels": [], "entities": []}, {"text": "The algorithm was originally published by.", "labels": [], "entities": []}, {"text": "It produces a restricted type of lexicalized LCFRS that we call \"canonical.\"", "labels": [], "entities": []}, {"text": "In Section 5 we provide a declarative characterization of this class of grammars, and show that every lexicalized LCFRS is (strongly) equivalent to a canonical one, in the sense that it induces the same set of dependency trees.", "labels": [], "entities": []}, {"text": "In Section 6 we present a simple parsing algorithm for LCFRSs.", "labels": [], "entities": []}, {"text": "Although the runtime of this algorithm is polynomial in the length of the sentence, the degree of the polynomial depends on two grammar-specific measures called fan-out and rank.", "labels": [], "entities": []}, {"text": "We show that even in the restricted case of canonical grammars, parsing is an NPhard problem.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9637279510498047}]}, {"text": "It is important therefore to keep the fan-out and the rank of a grammar as low as possible, and much of the recent work on LCFRSs has been devoted to the development of techniques that optimize parsing complexity in various scenarios G \u00b4 omez-Rodr\u00edguez and Satta 2009; G \u00b4 omez-).", "labels": [], "entities": [{"text": "G", "start_pos": 269, "end_pos": 270, "type": "METRIC", "confidence": 0.9216393232345581}]}, {"text": "In this article we explore the impact of non-projectivity on parsing complexity.", "labels": [], "entities": [{"text": "parsing complexity", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9119765162467957}]}, {"text": "In Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) ().", "labels": [], "entities": []}, {"text": "Although there is no theoretical upper bound on the block-degree of the dependency trees needed for linguistic analysis, we provide evidence from several dependency treebanks showing that, from a practical point of view, this upper bound can be put at a value of as low as 2.", "labels": [], "entities": []}, {"text": "In Section 8 we study a second constraint on non-projectivity called well-nestedness, and show that its presence facilitates tractable parsing.", "labels": [], "entities": []}, {"text": "This comes at the cost of a small loss in coverage on treebank data.", "labels": [], "entities": [{"text": "coverage", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9685254096984863}]}, {"text": "Bounded block-degree and well-nestedness jointly define a class of \"mildly\" non-projective dependency grammars that can be parsed in polynomial time.", "labels": [], "entities": []}, {"text": "Section 9 summarizes our main contributions and concludes the article.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3  Loss in coverage under the restriction to yield functions with fan-out = 1 and fan-out \u2264 2.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9948946237564087}]}, {"text": " Table 4  Loss in coverage under the restriction to yield functions with fan-out = 1, fan-out \u2264 2,  and to well-nested yield functions with fan-out \u2264 2 (last column).", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9936098456382751}]}]}