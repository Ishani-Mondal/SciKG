{"title": [{"text": "Word Segmentation, Unknown-word Resolution, and Morphological Agreement in a Hebrew Parsing System", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6719921976327896}, {"text": "Unknown-word Resolution", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7531834244728088}]}], "abstractContent": [{"text": "We present a constituency parsing system for Modern Hebrew.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.6944196671247482}]}, {"text": "The system is based on the PCFG-LA parsing method of Petrov et al.", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.730218768119812}]}, {"text": "(2006), which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank.", "labels": [], "entities": []}, {"text": "We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, specifically, a lexicon-based morphological analyzer.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9673429131507874}]}, {"text": "We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes.", "labels": [], "entities": []}, {"text": "We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing.", "labels": [], "entities": [{"text": "CKY lattice parsing", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7394772370656332}]}, {"text": "Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model.", "labels": [], "entities": []}, {"text": "We suggest modeling grammatical agreement in a constituency-based parser as a filter mechanism that is orthogonal to the grammar, and present a concrete implementation of the method.", "labels": [], "entities": []}, {"text": "Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make.", "labels": [], "entities": []}, {"text": "These contributions extend outside of the scope of Hebrew processing, and are of general applicability to the NLP community.", "labels": [], "entities": [{"text": "Hebrew processing", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7649824321269989}]}, {"text": "Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English.", "labels": [], "entities": []}, {"text": "The lattice-based parsing methodology is useful in any case where the input is uncertain.", "labels": [], "entities": []}, {"text": "Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank.", "labels": [], "entities": []}], "introductionContent": [{"text": "Different languages have different syntactic properties.", "labels": [], "entities": []}, {"text": "In English, word order is relatively fixed, whereas in other languages word order is much more flexible (in Hebrew, the subject may appear either before or after a verb).", "labels": [], "entities": []}, {"text": "In languages with a flexible word order, the meaning of the sentence is realized using other structural elements, like word inflections or markers, which are referred to as morphology (in Hebrew, the marker \u202b\u05d0\u05ea\u202c is used to mark definite objects, distinguishing them from subjects in the same position.", "labels": [], "entities": []}, {"text": "In addition, verbs and nouns are marked for gender and number, and subject and verb must share the same gender and number).", "labels": [], "entities": []}, {"text": "A limited form of morphology also exists in English: the -s and -ed suffixes are examples of English morphological markings.", "labels": [], "entities": []}, {"text": "In other languages, morphological processes maybe much more involved.", "labels": [], "entities": []}, {"text": "The lexical units (words) in English are always separated by white space.", "labels": [], "entities": []}, {"text": "In Chinese, such separation is not available.", "labels": [], "entities": []}, {"text": "In Hebrew (and Arabic), most words are separated by white space, but many of the function words (determiners like the, conjunctions such as and, and prepositions like in or of ) do not stand on their own but are instead attached to the following words.", "labels": [], "entities": []}, {"text": "A large part of the parsing literature is devoted to automatic parsing of English, a language with a relatively simple morphology, relatively fixed word order, and a large treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9664061069488525}]}, {"text": "Data-driven English parsing is now at the state where naturally occurring text in the news domain can be automatically parsed with accuracies of around 90% (according to standard parsing evaluation measures).", "labels": [], "entities": [{"text": "Data-driven English parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5254020790259043}, {"text": "accuracies", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9870554208755493}]}, {"text": "When moving from English to languages with richer morphologies and less-rigid word orders, however, the parsing algorithms developed for English exhibit a large drop inaccuracy.", "labels": [], "entities": []}, {"text": "In addition, whereas English has a large treebank, containing over one million annotated words, many other languages have much smaller treebanks, which also contribute to the drop in the accuracies of the data-driven parsers.", "labels": [], "entities": []}, {"text": "A similar drop in parsing accuracy is also exhibited in English when moving from the news domain, on which parsers have traditionally been trained, to other genres such as prose, blogs, poetry, product reviews, or biomedical texts, which use different vocabularies and, to some extent, different syntactic rules.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.97067791223526}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9318070411682129}]}, {"text": "This work focuses on constituency parsing of Modern Hebrew, a Semitic language with a rich and productive morphology, relatively free word order, and a small treebank.", "labels": [], "entities": [{"text": "constituency parsing of Modern Hebrew", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.8235816955566406}]}, {"text": "Several natural questions arise: Can the small size of the treebank be compensated for using other available resources or sources of information?", "labels": [], "entities": []}, {"text": "How should the word segmentation issue (that function words do not appear in isolation but attach to the next word, forming ambiguous letter patterns) be handled?", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7299939543008804}]}, {"text": "Can morphological information be used effectively in order to improve parsing accuracy?", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9729869365692139}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8834137320518494}]}, {"text": "We present a system which is based on a state-of-the-art model for constituency parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations (PCFG-LA) model of, as implemented in the BerkeleyParser.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.8276523053646088}, {"text": "BerkeleyParser", "start_pos": 209, "end_pos": 223, "type": "DATASET", "confidence": 0.9551738500595093}]}, {"text": "After evaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew treebank, we discuss some of its limitations and then goon to extend the PCFG-LA parsing model in several directions, making it more suitable for parsing Hebrew and related languages.", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 157, "end_pos": 172, "type": "TASK", "confidence": 0.6879701614379883}, {"text": "parsing Hebrew and related languages", "start_pos": 230, "end_pos": 266, "type": "TASK", "confidence": 0.8764427304267883}]}, {"text": "Our extensions are based on the following themes.", "labels": [], "entities": []}, {"text": "Separation of lexical and syntactic knowledge.", "labels": [], "entities": [{"text": "Separation of lexical and syntactic knowledge", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8092428545157114}]}, {"text": "There are two kinds of knowledge inherent in a parsing system.", "labels": [], "entities": []}, {"text": "One of them is syntactic knowledge governing the way in which words can be combined to form structures, which, in turn, can be combined to form ever larger structures.", "labels": [], "entities": []}, {"text": "The other is lexical knowledge about the identities of individual words, the word classes they belong to, and the kinds of syntactic structures they can participate in.", "labels": [], "entities": []}, {"text": "We argue that the amount of syntactic knowledge needed fora parsing system is relatively limited, and that sufficiently large parts of it can be captured also based on a relatively small treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9664486646652222}]}, {"text": "Lexical knowledge, on the other hand, is much more vast, and we should not rely on a treebank (small or large) to provide adequate lexical coverage.", "labels": [], "entities": []}, {"text": "Instead, we should aim to find ways of integrating lexical knowledge, which is external to the treebank, into the parsing process.", "labels": [], "entities": []}, {"text": "We extend the lexical coverage of a treebank-based parser using a dictionary-based morphological analyzer.", "labels": [], "entities": []}, {"text": "We present away of integrating the two resources also for the common case where their annotations schemes diverge.", "labels": [], "entities": []}, {"text": "This method is very effective in improving parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9865690469741821}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9075602889060974}]}, {"text": "Encoding input uncertainty using a lattice-based representation.", "labels": [], "entities": []}, {"text": "Sometimes, the language signal (the input to the parser) maybe uncertain.", "labels": [], "entities": []}, {"text": "This happens in Hebrew when a space-delimited token such as \u202b\u200c\u05dc\u202c \u202b\u05d1\u05e6\u202c can represent either a single word (' onion') or a sequence of two words or three words ('in shadow' and 'in the shadow,' respectively).", "labels": [], "entities": []}, {"text": "When computationally feasible, it is best to let the uncertainty be resolved by the parser rather than in a separate preprocessing step.", "labels": [], "entities": []}, {"text": "We propose encoding the input-uncertainty in a word lattice, and use lattice parsing () to perform joint word segmentation and syntactic disambiguation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7453140020370483}, {"text": "syntactic disambiguation", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.7219033539295197}]}, {"text": "Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model.", "labels": [], "entities": []}, {"text": "Using morphological information to improve parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9847803115844727}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9482402205467224}]}, {"text": "Morphology provides useful hints for resolving syntactic ambiguity, and the parsing model should have away of utilizing these hints.", "labels": [], "entities": []}, {"text": "There is a range of morphological hints than can be utilized: from functional marking elements (such as the \u202b\u05d0\u05ea\u202c marker indicating a definite direct object); to elements marking syntactic properties such as definiteness (such as the Hebrew \u202b\u05d4\u202c marker); to agreement patterns requiring a compatibility in properties such as gender, number, and person between syntactic constituents (such as a verb and its subject or an adjective and the noun it modifies).", "labels": [], "entities": []}, {"text": "We suggest modeling agreement as a filtering process that is orthogonal to the grammar.", "labels": [], "entities": []}, {"text": "Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make, without introducing new mistakes.", "labels": [], "entities": []}, {"text": "Aspects of the work presented in this article are discussed in earlier publications.", "labels": [], "entities": []}, {"text": "suggest the lattice-parsing mechanism, discuss ways of interfacing a treebank-derived PCFG-parser with an external lexicon, and present experiments using the PCFG-LA BerkeleyParser.", "labels": [], "entities": [{"text": "PCFG-LA BerkeleyParser", "start_pos": 158, "end_pos": 180, "type": "DATASET", "confidence": 0.900462806224823}]}, {"text": "Here we provide a cohesive presentation of the entire system, as well as a more detailed description and an expanded evaluation.", "labels": [], "entities": []}, {"text": "We also extend the previous work in several dimensions: We introduce anew method of interfacing the parser and the external lexicon, which contributes to an improved parsing accuracy, and suggest incorporating agreement information as a filter.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9502689242362976}]}, {"text": "The methodologies we suggest extend outside the scope of Hebrew processing, and are of general applicability to the NLP community.", "labels": [], "entities": [{"text": "Hebrew processing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7010374367237091}]}, {"text": "Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English.", "labels": [], "entities": []}, {"text": "The lattice-based parsing methodology is useful in any case where the input is uncertain.", "labels": [], "entities": []}, {"text": "Indeed, we have used it to solve the problem of parsing while recovering null elements in both, and others have used it for the joint segmentation and parsing of Arabic (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9725273847579956}, {"text": "joint segmentation", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.6870647966861725}]}, {"text": "Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank, and also for domain adaptation scenarios for English.", "labels": [], "entities": []}, {"text": "Finally, the agreement-as-filter methodology is applicable to any morphologically rich language, and although its contribution to the parsing task maybe limited, it is of wide applicability to syntactic generation tasks, such as target-side-syntax machine translation in a morphologically rich language.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 134, "end_pos": 146, "type": "TASK", "confidence": 0.9106015563011169}, {"text": "target-side-syntax machine translation", "start_pos": 229, "end_pos": 267, "type": "TASK", "confidence": 0.7212906082471212}]}], "datasetContent": [{"text": "The baseline system is an \"out-of-the-box\" PCFG-LA parser, as described in and and implemented in the BerkeleyParser.", "labels": [], "entities": [{"text": "BerkeleyParser", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.944884717464447}]}, {"text": "The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact experimental settings) after stripping all the functional and morphological information from the non-terminals.", "labels": [], "entities": [{"text": "Modern Hebrew Treebank", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.8258103132247925}]}, {"text": "We evaluate the resulting models on the development set, and consider three settings: Seg+POS Oracle: The parser has access to the gold segmentation and POS tags.", "labels": [], "entities": []}, {"text": "Seg Oracle: The parser has access to the gold segmentation, but not the POS tags.", "labels": [], "entities": [{"text": "Seg Oracle", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.5391983687877655}]}, {"text": "Pipeline: A POS-tagger is used to perform word segmentation, which is then used as parser input.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7472987771034241}]}, {"text": "Glossing over the parses revealed that the parser failed to learn the distinction between finite and non-finite verbs.", "labels": [], "entities": []}, {"text": "The importance of this linguistic distinction for parsing is obvious, and was also noted in for English and in our previous work on parsing Hebrew (Goldberg and Tsarfaty 2008).", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9800772070884705}]}, {"text": "Finite and non-finite verbs are easily distinguishable from each other based on surface form alone.", "labels": [], "entities": []}, {"text": "Although finiteness is clearly annotated in the treebank, it is not on the \"core\" part of the POS tags and was removed prior to training the parser.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.8253318965435028}]}, {"text": "Ina second set of experiments the core tag set of the parser was modified to distinguish finite verbs, infinitives, and modals.", "labels": [], "entities": []}, {"text": "The original core-tag set already includes some important distinctions, such as construct from non-construct nouns.", "labels": [], "entities": []}, {"text": "presents the parsing results on the development set.", "labels": [], "entities": []}, {"text": "With gold POS tags and segmentation, the results are very high.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9676304459571838}]}, {"text": "Accuracy drops considerably when the parser is not given access to the gold tags (from about 90 to less than 84 F 1 ), indicating that the POS tags are both informative and ambiguous.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9963192939758301}, {"text": "F 1", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9563294649124146}]}, {"text": "Results drop even further (from 84 to 77) in the pipeline case where the gold segmentation is not available, indicating that correct segmentation also provides valuable information to the parser and that segmentation mistakes are costly.", "labels": [], "entities": []}, {"text": "Enriching the tag set to distinguish modals and finite and infinite verbs proved useful, with an increase of about 1 F 1 points (absolute) after four split-merge-smooth cycles, and a smaller increase after five cycles.", "labels": [], "entities": []}, {"text": "This stresses the importance of the core representation: The automatic learning procedure goes along way, but it can be aided by linguistically motivated manual interventions in some cases.", "labels": [], "entities": []}, {"text": "For all the experiments we use Version 2 of the Hebrew Treebank (, with the established test-train-dev splits: Sentences 484-5,740 are used for training, sentences 1-483 are the development set, and sentences 5,741-6,220 are used for the final test set.", "labels": [], "entities": [{"text": "Hebrew Treebank", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9296982884407043}]}, {"text": "In the cases where the gold segmentation is given, we use the wellknown evalb F 1 score.", "labels": [], "entities": [{"text": "gold segmentation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.5776829272508621}, {"text": "wellknown evalb F 1 score", "start_pos": 62, "end_pos": 87, "type": "METRIC", "confidence": 0.6312722265720367}]}, {"text": "Namely, each tree is treated as a set of labeled constituents.", "labels": [], "entities": []}, {"text": "Each constituent is represented as a 3-tuple i, j, L, in which i and j are the indices of the first and the last words in the constituent, respectively, and L is the constituency label.", "labels": [], "entities": []}, {"text": "For example, (2, 4, NP) indicates an NP spanning from word 2 to word 4.", "labels": [], "entities": []}, {"text": "The performance of a parser is evaluated based on the amount of constituents it recovered correctly.", "labels": [], "entities": []}, {"text": "Let G denote the set of constituents in a gold-standard constituency tree, and P denote the set of constituents in a predicted tree.", "labels": [], "entities": []}, {"text": "Precision (P), recall (R), and F 1 are defined as: F 1 ranges from 0 to 1, and it is 1 iff both precision and recall are 1, indicating the trees are identical.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9269955307245255}, {"text": "recall (R)", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9385975301265717}, {"text": "F 1", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9940882325172424}, {"text": "F 1", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9656517207622528}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9982051849365234}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9883948564529419}]}, {"text": "We report numbers in precentages rather than fractions.", "labels": [], "entities": []}, {"text": "When measuring the performance of models in which the token-segmentation is predicted and can contradict the gold-standard, a generalization of these measures is used.", "labels": [], "entities": []}, {"text": "Instead of representing a constituent by a triplet i, j, L, each constituent is represented by a pair containing the concatenation of the words at its yield, and its label L.", "labels": [], "entities": []}, {"text": "This measure was suggested by and used in subsequent work.", "labels": [], "entities": []}, {"text": "This is equivalent to reassigning the i and j indices to represent character positions instead of word numbers.", "labels": [], "entities": []}, {"text": "When the yields of the gold standard and the predicted trees are the same, this is equivalent to the standard evaluation measure using the i, j, L triplets of word indices and a label, and it will produce the same precision, recall, and F 1 as above.", "labels": [], "entities": [{"text": "precision", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9994332194328308}, {"text": "recall", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.9996132254600525}, {"text": "F 1", "start_pos": 237, "end_pos": 240, "type": "METRIC", "confidence": 0.990601509809494}]}, {"text": "We start by evaluating the effect of extending the parser's lexical model with an external lexicon, as described in Section 6.1.", "labels": [], "entities": []}, {"text": "The rare-word threshold is set to 100.", "labels": [], "entities": []}, {"text": "We use the morphological analyzer described in Section 2.3.3.", "labels": [], "entities": [{"text": "morphological analyzer", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7629501521587372}]}, {"text": "We test two conditions: UNIFORM, in which the P(T ext |w) distribution is uniform overall the analyses suggested by the morphological analyzer for the word, and HMM-BASED in which the P(T ext |w) distribution is based on pseudo-counts from the final round of EM-HMM training of the semi-supervised POS tagger described in Section 2.3.4.", "labels": [], "entities": [{"text": "UNIFORM", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.7601699233055115}, {"text": "POS tagger", "start_pos": 298, "end_pos": 308, "type": "TASK", "confidence": 0.6610781699419022}]}, {"text": "Incorporating the external lexicon helps both in the case where the correct segmentation is assumed to be known, as well as in the pipeline case where the segmentation is automatically induced by a sequential tagger.", "labels": [], "entities": []}, {"text": "Incorporating the semi-supervised lexical probabilities learned overlarge unannotated corpora (HMM-BASED) further improves the results, up to 86.1 F 1 for the gold-segmentation case and 78.7 F 1 for the pipeline case.", "labels": [], "entities": [{"text": "F", "start_pos": 147, "end_pos": 148, "type": "METRIC", "confidence": 0.9773005247116089}, {"text": "F 1", "start_pos": 191, "end_pos": 194, "type": "METRIC", "confidence": 0.9820274114608765}]}, {"text": "The pipeline model still lags behind the gold-segmentation case, indicating that the correct segmentation is very informative for the parser.", "labels": [], "entities": []}, {"text": "Having established that the external lexicon can be effectively incorporated into the parser, we turn to evaluate the method for joint segmentation and parsing.", "labels": [], "entities": [{"text": "joint segmentation", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7042936235666275}]}, {"text": "We follow the same conditions as before (UNIFORM and HMM-BASED lexical probabilities), but in this set of experiments the parser is allowed to choose its preferred segmentation using the lattice-parsing methodology presented in Section 7.2.", "labels": [], "entities": []}, {"text": "The lattice is constructed according to the analyses licensed by the morphological analyzer.", "labels": [], "entities": []}, {"text": "Lattice parsing is effective, leading to an improvement of about 2-3 F 1 points over the pipeline model.", "labels": [], "entities": [{"text": "Lattice parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5765093863010406}]}, {"text": "We now turn to add the agreement filtering on top of the lexiconenhanced models.", "labels": [], "entities": []}, {"text": "In this setting, the model outputs its 100-best trees for each sentence, agreement features are propagated, and agreement violations are checked as described  in Section 12, and the first tree that does not contain any agreement violation is returned as the final parse for the sentence (or the first-best tree in case that all of the output trees contain an agreement violation).", "labels": [], "entities": []}, {"text": "lists the results when agreement filtering is performed on top of parses based on gold segmentation, and lists the results when agreement filtering is performed on top of a lattice-based parsing model that does not assume gold segmentation is available.", "labels": [], "entities": [{"text": "agreement filtering", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7465834021568298}, {"text": "agreement filtering", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7524906694889069}]}, {"text": "Discussion of agreement filter results.", "labels": [], "entities": []}, {"text": "Although the agreement filter does not hurt the parser performance, the benefits from it are very small.", "labels": [], "entities": []}, {"text": "To understand why that is the case, we analyzed the 1-best parses produced by the 5-cycles-trained grammar on the gold-segmented development set (these conditions corresponds to the last column of the third row in).", "labels": [], "entities": []}, {"text": "The analysis revealed the following reasons for the low impact of the agreement filter: (1) The grammar is strong enough to produce fairly accurate structures, which have very few agreement mistakes to begin with, and fixing an agreement mistake does not necessarily mean fixing the entire parse-in some cases it is very easy for the parser to fix the agreement mistake and still produce an incorrect parse for other parts of the structure.", "labels": [], "entities": []}, {"text": "The 1-best trees of the 480 sentences of the development set contain 22,500 parsetree nodes.", "labels": [], "entities": []}, {"text": "Of these 22,500 nodes, 2,368 nodes triggered a gender-agreement check: about 10% of the parsing decisions could benefit from gender agreement.", "labels": [], "entities": []}, {"text": "Of the 2,368 relevant nodes, however, 130 nodes involved conjunctions or possessives, and were outside of the scope of our agreement verification rules.", "labels": [], "entities": []}, {"text": "Of the remaining 2,238 parsetree nodes, 2,204 passed the agreement check, and only 34 nodes (1.5% of the relevant nodes, and 0.15% of the total number of nodes) were flagged as gender-agreement violations.", "labels": [], "entities": []}, {"text": "Similarly for number agreement, 2,244 nodes triggered an agreement check, of which 2,131 nodes could be handled by our system.", "labels": [], "entities": []}, {"text": "Of these relevant nodes, 2,109 nodes passed the gender-agreement check, and only 23 nodes (1.07% of relevant nodes, Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parser does both segmentation and parsing).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Baseline: Out-of-the-box BerkeleyParser performance on the dev-set.", "labels": [], "entities": []}, {"text": " Table 2  Number of learned splits per POS category after five split-merge cycles.", "labels": [], "entities": []}, {"text": " Table 3  Number of learned splits per NT-category after five split-merge cycles.", "labels": [], "entities": []}, {"text": " Table 4  Dev-set results when incorporating an external lexicon.", "labels": [], "entities": []}, {"text": " Table 5  Dev-set results when using lattice parsing on top of an external lexicon/analyzer.", "labels": [], "entities": []}, {"text": " Table 6  Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser (starting from  gold segmentation).", "labels": [], "entities": []}, {"text": " Table 7  Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parser  does both segmentation and parsing).", "labels": [], "entities": []}, {"text": " Table 8  Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or  number agreement checks, and the results of these checks.", "labels": [], "entities": []}, {"text": " Table 8. It is clear that the vast majority of the parser decisions are  compatible with the agreement constraints.  Turning to inspect the cases in which the agreement filter caught an agreement  violation, we note that the agreement filter marked 51 of the 480 development sentences  as having an agreement violation in the 1-best parse-about 10% of the sentences could  potentially benefit from the agreement filter. For 38 of the 51 agreement violations, the  agreement violation was fixed in the tree suggested in the 100-best list. We manually  inspected these 51 parse trees, and highlight some the trends we observed. In the  13 cases in which the 100-best list did not contain a fix to the agreement violation,  the cause was usually that the 1-best parse had many mistakes that were not related  to the agreement violation, and diversity in the 100-best list reflected fixes to these  mistakes without affecting the agreement violation. Another cause of error was an erro- neous agreement mistake due to an omission in the lexicon. Of the 38 fixable agreement  violations, 25 were local to a noun-phrase, 10 were cases of subject-verb agreement,  and the remaining three were either corner-cases or harder to categorize. The subject- verb agreement violations were handled almost exclusively by keeping the structure  mostly intact and changing the NPSUBJ label to some other closely related label that does  not require verb agreement, usually NP. This is a good strategy for fixing subject-less  sentences (about half of the cases), but it is only a partial fix in case the subject should  be assigned to a different NP (which does not happen in practice) or in case a more  drastic structural change to the parse-structure is needed. In one of the 10 cases, the  subject-verb agreement mistake indeed resulted in a structural change that improved  the overall parse quality. The NP internal agreement violations include many cases of  noun-compound attachments, and some cases involving coordination. The corrections  to the agreement violation were mostly local, and usually resulted in correct structure,  but sometimes introduced new errors.", "labels": [], "entities": [{"text": "NPSUBJ", "start_pos": 1360, "end_pos": 1366, "type": "DATASET", "confidence": 0.9468989372253418}]}, {"text": " Table 9  Test-set results of the best-performing models.", "labels": [], "entities": []}]}