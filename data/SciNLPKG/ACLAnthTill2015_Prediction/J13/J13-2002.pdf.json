{"title": [{"text": "Automatically Assessing Machine Summary Content Without a Gold Standard", "labels": [], "entities": [{"text": "Automatically Assessing Machine Summary Content", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8141549170017243}]}], "abstractContent": [{"text": "The most widely adopted approaches for evaluation of summary content follow some protocol for comparing a summary with gold-standard human summaries, which are traditionally called model summaries.", "labels": [], "entities": [{"text": "evaluation of summary content", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7791493386030197}]}, {"text": "This evaluation paradigm falls short when human summaries are not available and becomes less accurate when only a single model is available.", "labels": [], "entities": []}, {"text": "We propose three novel evaluation techniques.", "labels": [], "entities": []}, {"text": "Two of them are model-free and do not rely on a gold standard for the assessment.", "labels": [], "entities": []}, {"text": "The third technique improves standard automatic evaluations by expanding the set of available model summaries with chosen system summaries.", "labels": [], "entities": []}, {"text": "We show that quantifying the similarity between the source text and its summary with appropriately chosen measures produces summary scores which replicate human assessments accurately.", "labels": [], "entities": []}, {"text": "We also explore ways of increasing evaluation quality when only one human model summary is available as a gold standard.", "labels": [], "entities": []}, {"text": "We introduce pseudomodels, which are system summaries deemed to contain good content according to automatic evaluation.", "labels": [], "entities": []}, {"text": "Combining the pseudomodels with the single human model to form the gold-standard leads to higher correlations with human judgments compared to using only the one available model.", "labels": [], "entities": []}, {"text": "Finally, we explore the feasibility of another measure-similarity between a system summary and the pool of all other system summaries for the same input.", "labels": [], "entities": []}, {"text": "This method of comparison with the consensus of systems produces impressively accurate rankings of system summaries, achieving correlation with human rankings above 0.9.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this work, we present evaluation metrics for summary content which make use of little or no human involvement.", "labels": [], "entities": []}, {"text": "Evaluation methods such as manual pyramid scores and automatic ROUGE scores (Lin and Hovy 2003) rely on multiple human summaries as a gold standard (model) against which they compare a summary to assess how informative the candidate summary is.", "labels": [], "entities": [{"text": "automatic ROUGE scores", "start_pos": 53, "end_pos": 75, "type": "METRIC", "confidence": 0.8173083066940308}]}, {"text": "It is desirable that evaluation of similar quality be done quickly and cheaply on non-standard test sets that have few or no human summaries, or on large test sets for which creating human model summaries is infeasible.", "labels": [], "entities": []}, {"text": "In our work, we aim to identify indicators of summary content quality that do not make use of human summaries but can replicate scores based on comparison with a gold standard very accurately.", "labels": [], "entities": []}, {"text": "Such indicators would need to be easily computable from existing resources and to provide rankings of systems that agree with rankings obtained through human judgments.", "labels": [], "entities": []}, {"text": "There have been some early proposals for alternative methods.", "labels": [], "entities": []}, {"text": "Donaway, propose that a comparison of the source text with a summary can tell us how good the summary is.", "labels": [], "entities": []}, {"text": "A summary that has higher similarity with the source text can be considered better than one with lower similarity.", "labels": [], "entities": []}, {"text": "perform a large scale evaluation with thousands of test documents.", "labels": [], "entities": []}, {"text": "Their work is setup in a search engine scenario.", "labels": [], "entities": []}, {"text": "They first rank the test documents using the search engine.", "labels": [], "entities": []}, {"text": "Then they perform the same experiment now substituting the summaries from one system in place of the original documents.", "labels": [], "entities": []}, {"text": "The system whose summaries have the most similar ranking as that generated for the full documents is considered the best system because not much information loss is introduced by the summarization process.", "labels": [], "entities": []}, {"text": "But these methods did not gain much popularity and their performance was never compared to human evaluations.", "labels": [], "entities": []}, {"text": "Part of the reason is that only in the last decade have several large data sets with system summaries and their ratings from human judges become available for performing such studies.", "labels": [], "entities": []}, {"text": "Our work is the first to provide a comprehensive report of the strengths of such approaches and we show that human ratings can be reproduced by these fully automatic metrics with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9943357110023499}]}, {"text": "Our results are based on data for multi-document news summarization.", "labels": [], "entities": [{"text": "multi-document news summarization", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.672623078028361}]}, {"text": "The key insights of our approach can be summarized as follows: Input-summary similarity: Good summaries are representative of the input and so one would expect that the more similar a summary is to the input, the better its content.", "labels": [], "entities": [{"text": "Input-summary similarity", "start_pos": 63, "end_pos": 87, "type": "METRIC", "confidence": 0.9351480007171631}]}, {"text": "Identifying a suitable input-summary similarity metric will provide a means for fully automatic evaluation of summaries.", "labels": [], "entities": []}, {"text": "We present a quantitative analysis of this hypothesis and show that input-summary similarity is highly predictive of scores assigned by humans for the summaries.", "labels": [], "entities": []}, {"text": "The choice of an appropriate metric to measure similarity is critical, however, and we show that information-theoretic measures turnout to be the most powerful for this task (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "Summary quality is defined by two key aspects-content and linguistic quality.", "labels": [], "entities": [{"text": "Summary quality", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7878068387508392}]}, {"text": "A good summary should contain the most important content in the input and also structure the content and present it as well-written text.", "labels": [], "entities": []}, {"text": "Several methods have been proposed for evaluating system-produced summaries; some only assess content, others only linguistic quality, and some combine assessment of both.", "labels": [], "entities": []}, {"text": "Some of these approaches are manual and others can be performed automatically.", "labels": [], "entities": []}, {"text": "In our work, we consider the problem of automatic evaluation of content quality.", "labels": [], "entities": []}, {"text": "To establish the context for our work, we provide an overview of current content evaluation methods used at the annual evaluations run by NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.9440853595733643}]}, {"text": "The Text Analysis Conference (TAC, previously called the Document Understanding Conference 3 ) conducts large scale evaluation of automatic systems on different summarization tasks.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8260860800743103}, {"text": "summarization tasks", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.8888536691665649}]}, {"text": "These conferences have been held every year since 2001 and the test sets and evaluation methods adopted by TAC/DUC have become the standard for reporting results in publications.", "labels": [], "entities": [{"text": "TAC/DUC", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.8036511739095052}]}, {"text": "TAC has employed a range of manual and automatic metrics over the years.", "labels": [], "entities": [{"text": "TAC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7376783490180969}]}, {"text": "Manual evaluations of the systems are performed at NIST by trained assessors.", "labels": [], "entities": [{"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9509354829788208}]}, {"text": "The assessors score the summaries either a) by comparing with a gold-standard summary written by humans, or b) by providing a direct rating on a scale (1 to 5 or 1 to 10).", "labels": [], "entities": []}, {"text": "The human summaries against which other summaries are compared are interchangeably called models, gold standards, and references.", "labels": [], "entities": []}, {"text": "Within TAC, they are typically called models.", "labels": [], "entities": []}, {"text": "The pyramid evaluation method) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.9695107936859131}]}, {"text": "It uses multiple human models from which annotators identify semantically defined Summary Content Units (SCUs).", "labels": [], "entities": []}, {"text": "Each SCU is assigned a weight equal to the number of human model summaries that express that SCU.", "labels": [], "entities": []}, {"text": "An ideal maximally informative summary would express a subset of the most highly weighted SCUs, with multiple maximally informative summaries being possible.", "labels": [], "entities": []}, {"text": "The pyramid score fora system summary S is equal to the following ratio: py(S) = sum of weights of SCUs expressed in S sum of weights of an ideal summary with the same number of In this way, a more reliable score fora summary is obtained using multiple reference summaries.", "labels": [], "entities": []}, {"text": "Four human summaries are normally used for pyramid evaluation at TAC.", "labels": [], "entities": [{"text": "pyramid evaluation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9744671583175659}, {"text": "TAC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.7354064583778381}]}, {"text": "Responsiveness of a summary is a measure of overall quality combining both content selection and linguistic quality.", "labels": [], "entities": []}, {"text": "It measures to what extent summaries convey appropriate content in a structured fashion.", "labels": [], "entities": [{"text": "summaries convey appropriate content", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.8464157581329346}]}, {"text": "Responsiveness is assessed by direct ratings given by the judges.", "labels": [], "entities": [{"text": "Responsiveness", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.6365102529525757}]}, {"text": "For example, a scale of 1 (poor summary) to 5 (very good summary) is used and these assessments are done without reference to any model summaries.", "labels": [], "entities": []}, {"text": "Pyramid and responsiveness are the standardly used manual approaches for content evaluation.", "labels": [], "entities": [{"text": "content evaluation", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7639453113079071}]}, {"text": "They produce rather similar rankings of systems at TAC.", "labels": [], "entities": [{"text": "TAC", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9019758105278015}]}, {"text": "The (Spearman) correlation between the two for ranking systems that participated in the TAC 2009 conference is 0.85 (p-value 6.8e-16, 53 systems).", "labels": [], "entities": [{"text": "TAC 2009 conference", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.8491250475247701}]}, {"text": "The responsiveness measure involves some aspects of linguistic quality whereas the pyramid metric was designed for content only.", "labels": [], "entities": []}, {"text": "Such high correlation indicates that the content factor has substantial influence on the responsiveness judgments, however.", "labels": [], "entities": []}, {"text": "The high correlation also indicates that two types of human judgments made on very different basisgold-standard summaries and direct judgments-can agree and provide fairly similar rankings of summaries.", "labels": [], "entities": []}, {"text": "All of these methods require significant human involvement.", "labels": [], "entities": []}, {"text": "In evaluations where goldstandard summaries are needed, assessors first read the input documents (10 or more per input) and write a summary.", "labels": [], "entities": []}, {"text": "Then manual comparison of system and gold standard is done, which takes additional time.", "labels": [], "entities": []}, {"text": "hypothesize that at least 17.5 hours are needed to evaluate two systems under this setup on a standard test set.", "labels": [], "entities": []}, {"text": "Moreover, multiple gold-standard summaries are needed for the same input, so different assessors have to read and create summaries.", "labels": [], "entities": []}, {"text": "The more reliable evaluation methods such as pyramid involve even more annotations at the clause level.", "labels": [], "entities": []}, {"text": "Although responsiveness does not require gold-standard summaries, in a system development setting, responsiveness judgments are resource-intensive.", "labels": [], "entities": []}, {"text": "It requires judges to directly assign scores to summaries, so humans are in the loop each time the evaluation needs to be done, making it rather costly.", "labels": [], "entities": []}, {"text": "For ROUGE, however, once the human summaries are created, the scores can be computed automatically for repeated system development runs.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.47864845395088196}]}, {"text": "This benefit has made ROUGE immensely popular.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7463181614875793}]}, {"text": "But the initial investment of time for gold-standard creation is still necessary.", "labels": [], "entities": [{"text": "gold-standard creation", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.700965940952301}]}, {"text": "Another important point is that for TAC, the gold standards are created by trained assessors at NIST.", "labels": [], "entities": [{"text": "TAC", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8473501205444336}, {"text": "NIST", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.9732243418693542}]}, {"text": "Non-expert evaluation options such as Mechanical Turk have recently been explored by.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.6176212131977081}]}, {"text": "They provided annotators with gold-standard references and system summaries and asked them to score the system summaries on a scale from 1 to 10 with respect to how well they convey the same information as the models.", "labels": [], "entities": []}, {"text": "They analyzed how these scores are related to responsiveness judgments given by the expert TAC assessors.", "labels": [], "entities": [{"text": "TAC assessors", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.7295486330986023}]}, {"text": "The study assessed only eight automatic systems from TAC 2009 and the correlation between the ratings from experts and Mechanical Turk annotations was 0.62 (Spearman).", "labels": [], "entities": [{"text": "TAC 2009", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9239227473735809}]}, {"text": "The analysis concludes that evaluations produced in this way tend to be noisy.", "labels": [], "entities": []}, {"text": "One reason was that non-expert annotators were quite influenced by the readability of the summaries.", "labels": [], "entities": []}, {"text": "For example, they tended to assign high scores to the baseline summary that picks the lead paragraph.", "labels": [], "entities": []}, {"text": "The baseline summary, however, is ranked by expert annotators as low in responsiveness compared to other systems' summaries.", "labels": [], "entities": []}, {"text": "Further, the non-expert evaluation led to few significant differences in the system rankings (score of system A is significantly greater/lesser than that of B) compared with the TAC evaluations of the same systems.", "labels": [], "entities": []}, {"text": "Another problem with non-expert evaluation is the quality of the model summaries.", "labels": [], "entities": []}, {"text": "Evaluations based on model summaries assume that the gold standards are of high quality.", "labels": [], "entities": []}, {"text": "Through the years at TAC, considerable effort has been invested to ensure that the evaluation scores do not vary depending on the particular gold standard.", "labels": [], "entities": [{"text": "TAC", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.7474600672721863}]}, {"text": "In the early years of TAC only one gold-standard summary was used.", "labels": [], "entities": [{"text": "TAC", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.839536726474762}]}, {"text": "During this time, papers reported ANOVA tests examining the factors that most influenced summary scores from the evaluations and found that the identity of the judge turned out to be the most significant factor (.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.8227593898773193}]}, {"text": "But it is desirable that a model summary or a human judgment be representative of important content in general and does not depict the individual biases of the person who created the summary or made the judgment.", "labels": [], "entities": []}, {"text": "So the evaluation methodology was refined to remove the influence of the assessor identity on the evaluation.", "labels": [], "entities": []}, {"text": "The pyramid evaluation was also developed with this goal of smoothing out the variation between judges.", "labels": [], "entities": []}, {"text": "point out that Mechanical Turk evaluations have this undesirable outcome: The identity of the judges turns out to be the most significant factor influencing summary scores.", "labels": [], "entities": [{"text": "Mechanical Turk evaluations", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.7256108522415161}]}, {"text": "Gillick and Liu do not elicit model summaries, only direct judgments on quality.", "labels": [], "entities": []}, {"text": "We suspect that the task would only be harder if model summaries were to be created by non-experts.", "labels": [], "entities": []}, {"text": "The problem that has been little addressed by any of these discussed metrics is evaluation when there are no gold-standard summaries available.", "labels": [], "entities": []}, {"text": "Systems are developed by fine-tuning on the TAC data sets, but in non-TAC data sets in novel or very large domains model summaries may not be available.", "labels": [], "entities": [{"text": "TAC data sets", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8966106176376343}]}, {"text": "Even though ROUGE provides good performance in automatic evaluation, it is not usable under these conditions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9530346393585205}]}, {"text": "Further, pyramid and ROUGE use multiple gold-standard summaries for evaluation (ROUGE correlates with human judgments better when computed using multiple models; we discuss this aspect further in Section 5) so even a single gold-standard summary may not be sufficient for reliable evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9495772123336792}]}, {"text": "In our work, we propose fully automatic methods for content evaluation which can be used in the absence of human summaries.", "labels": [], "entities": [{"text": "content evaluation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7694563865661621}]}, {"text": "We also explore methods to further improve the evaluation performance when only one model summary is available.", "labels": [], "entities": []}, {"text": "In this section, we describe the data we use throughout our article.", "labels": [], "entities": []}, {"text": "We carryout our analysis on the test sets and system scores from TAC 2009.", "labels": [], "entities": [{"text": "TAC 2009", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9349178671836853}]}, {"text": "TAC 2009 is also the year when NIST introduced a special track called AESOP (Automatically Evaluating Summaries of Peers).", "labels": [], "entities": [{"text": "TAC 2009", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8696157038211823}, {"text": "NIST", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8637629747390747}, {"text": "AESOP", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9905865788459778}, {"text": "Automatically Evaluating Summaries of Peers)", "start_pos": 77, "end_pos": 121, "type": "TASK", "confidence": 0.7086736758550009}]}, {"text": "The goal of AESOP is to identify automatic metrics that correlate well with human judgments of summary quality.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.71913743019104}]}, {"text": "We use the data from the TAC 2009 query focused-summarization task.", "labels": [], "entities": [{"text": "TAC 2009 query focused-summarization task", "start_pos": 25, "end_pos": 66, "type": "DATASET", "confidence": 0.7755561709403992}]}, {"text": "Each input consists often news documents.", "labels": [], "entities": []}, {"text": "In addition, the user's information needs associated with each input is given by a query statement consisting of a title and narrative.", "labels": [], "entities": []}, {"text": "An example query statement is shown here: Title: Airbus A380 Narrative: Describe developments in the production and launch of the Airbus A380.", "labels": [], "entities": [{"text": "Title", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9334572553634644}]}, {"text": "A system must produce a summary that addresses the information required by the query.", "labels": [], "entities": []}, {"text": "The maximum length for summaries is 100 words.", "labels": [], "entities": [{"text": "summaries", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.9741925001144409}]}, {"text": "The test set contains 44 inputs, and 53 automatic systems (including baselines) participated that year.", "labels": [], "entities": []}, {"text": "These systems were manually evaluated for content using both pyramid and responsiveness methods.", "labels": [], "entities": []}, {"text": "In TAC 2009, two oracle systems were introduced during evaluation whose outputs are in fact summaries created by people.", "labels": [], "entities": [{"text": "TAC 2009", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.7748766839504242}]}, {"text": "We ignore these two systems and use only the automatic participant submissions and the automatic baseline systems.", "labels": [], "entities": []}, {"text": "As a development set, we use the inputs, summaries, and evaluations from the previous year, TAC 2008.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.8828949928283691}]}, {"text": "There were 48 inputs in the query-focused task in 2008 and 58 automatic systems participated.", "labels": [], "entities": []}, {"text": "TAC 2009 also involved an update summarization task and we obtained similar results on the summaries from this task.", "labels": [], "entities": [{"text": "TAC 2009", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9370419383049011}, {"text": "summarization task", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7595153450965881}]}, {"text": "In this article, for clarity we only present results on evaluating the query-focused summaries, but the update task results are described in detail in).", "labels": [], "entities": []}, {"text": "Here we present and evaluate a suite of metrics which do not require gold-standard human summaries for evaluation.", "labels": [], "entities": []}, {"text": "The underlying intuition is that good summaries will tend to be similar to the input in terms of content.", "labels": [], "entities": []}, {"text": "Accordingly, we use the similarity of the distribution of terms in the input and summaries as a measure of summary content.", "labels": [], "entities": [{"text": "similarity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9815409183502197}]}, {"text": "Although the motivation for this metric is highly intuitive, it is not clear how similarity should be defined for this particular problem.", "labels": [], "entities": []}, {"text": "Here we provide a comprehensive study of input-summary similarity metrics and show that some of these measures can indeed be very accurate predictors of summary quality even while using no goldstandard human summaries at all.", "labels": [], "entities": []}, {"text": "Prior to our work, the proposal for using the input for evaluation has been brought up in a few studies.", "labels": [], "entities": []}, {"text": "These studies did not involve a direct evaluation of the capacity of input-summary similarity to replicate human ratings, however, and they did not compare similarity metrics for the task.", "labels": [], "entities": []}, {"text": "Because large scale manual evaluation results are available now, our work is the first to evaluate this possibility in a direct manner and involving study of correlations with different types of human evaluations.", "labels": [], "entities": []}, {"text": "In the following section we detail some of the prior studies on input-summary similarity for summary evaluation.", "labels": [], "entities": []}, {"text": "We now detail our experiments on the TAC 2009 data.", "labels": [], "entities": [{"text": "TAC 2009 data", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9733651280403137}]}, {"text": "TAC provides four model summaries for each input.", "labels": [], "entities": [{"text": "TAC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.744537889957428}]}, {"text": "We assume that only one is available and choose a model for each input: the first in alphabetical order of identifier names.", "labels": [], "entities": []}, {"text": "Based on this model, we compute the RSU4 scores for all systems.", "labels": [], "entities": []}, {"text": "We use two methods to choose the pseudomodel systems.", "labels": [], "entities": []}, {"text": "In the first approach, we rank all the systems based on their average scores over the entire test set.", "labels": [], "entities": []}, {"text": "The summaries of the top three overall best systems (global selection) are added to the set of models for all inputs.", "labels": [], "entities": []}, {"text": "Alternatively, we also investigate a different selection method.", "labels": [], "entities": []}, {"text": "For each input, the top scoring three summaries are added as models for that input (local selection).", "labels": [], "entities": []}, {"text": "In both cases RSU4 was used to identify the best systems according to the single available gold standard.", "labels": [], "entities": []}, {"text": "The final rankings for all systems are produced using the RSU4 comparison based on the expanded set of models (1 human model + 3 pseudomodel summaries).", "labels": [], "entities": []}, {"text": "We implemented a jackknifing procedure so that the systems selected to be pseudomodels (and therefore reference systems) could also be compared to other systems.", "labels": [], "entities": []}, {"text": "For each input, one of the reference systems (pseudomodels or human model) was removed at a time from the set of models and added to the set of systems.", "labels": [], "entities": []}, {"text": "The scores for the systems were then computed by comparison with the three remaining models.", "labels": [], "entities": []}, {"text": "The final score fora system summary (not a pseudomodel) is the mean value of the scores with the four different sets of reference summaries created by the jackknifing procedure.", "labels": [], "entities": []}, {"text": "For pseudomodel systems, a single score value will be obtained per input resulting from the comparison with the other three models.", "labels": [], "entities": []}, {"text": "From our experiments with pseudomodels, we see that the addition of system summaries to available models proved beneficial and improved the micro-level performance of ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 167, "end_pos": 172, "type": "METRIC", "confidence": 0.4813927114009857}]}, {"text": "One question that arises is whether the collection of system summaries together will be useful for evaluation without any human models at all.", "labels": [], "entities": []}, {"text": "Again, this idea is related to model-free evaluation.", "labels": [], "entities": []}, {"text": "When several systems are available, we investigate if their collective knowledge can help assess summary quality.", "labels": [], "entities": []}, {"text": "Systems use varied methods to select content, and agreement among systems could be indicative of important information.", "labels": [], "entities": []}, {"text": "This intuition is similar to that behind the manual pyramid method: Facts mentioned only in one human summary are less important compared to content that is mentioned in multiple human models.", "labels": [], "entities": []}, {"text": "For the experiments reported in this section, we rely entirely on the combined knowledge from system summaries as a gold standard.", "labels": [], "entities": []}, {"text": "For each input, we collect all the summaries produced by automatic systems and calculate the probabilities of words in the combined set.", "labels": [], "entities": []}, {"text": "In this way, we obtain a global probability distribution of words selected in system summaries.", "labels": [], "entities": []}, {"text": "In this distribution, the content selected by multiple systems will be more prominent, representing the more important information.", "labels": [], "entities": []}, {"text": "The word probabilities from each individual summary are then calculated and compared to the overall distribution using JS divergence.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.4479494094848633}]}, {"text": "If we assume that system summaries are collectively indicative of important content, then good summaries will tend to have properties that are similar to this global distribution, resulting in low divergence values.", "labels": [], "entities": []}, {"text": "We compute the correlations of these divergence values with human-assigned summary scores and shows the results from this evaluation.", "labels": [], "entities": []}, {"text": "Ina similar vein, one would like to understand the performance guarantees from the consensus-based evaluation method.", "labels": [], "entities": []}, {"text": "Here, the metric depends on the availability of a number of diverse system summaries.", "labels": [], "entities": []}, {"text": "In the TAC workshops, over 50 systems compete and thus we have a large pool of system summaries with which to compute consensus.", "labels": [], "entities": []}, {"text": "For other data sets, when we have to evaluate a few different systems, it is unclear if the same performance can be obtained.", "labels": [], "entities": []}, {"text": "To understand the dependence on the number of systems, we study how well the consensus evaluation method works when a small set of standard summarization methods is taken as the available system pool.", "labels": [], "entities": []}, {"text": "We expected that when the standard algorithms are chosen to be diverse, their strengths can be combined usefully in a similar manner as the TAC systems.", "labels": [], "entities": []}, {"text": "We choose a set of nine different summarization approaches.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9721601009368896}]}, {"text": "They are briefly described here.", "labels": [], "entities": []}, {"text": "Baseline: One of the commonly used baseline approaches for multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6487910747528076}]}, {"text": "The first sentence from each document in the input is first included in the summary.", "labels": [], "entities": []}, {"text": "After including the first sentence from each document, the second sentence is included and soon up to the length limit.", "labels": [], "entities": [{"text": "length", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9823437929153442}]}, {"text": "Mead:) rank sentences using a combination of three aspects (sentence length, position in the article, and a centroid score which indicates how central the content of the sentence is) computed by comparison with all other sentences.", "labels": [], "entities": []}, {"text": "CLASSY 11: This is a query-focused summarization system used by in TAC 2011.", "labels": [], "entities": [{"text": "CLASSY 11", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8363034129142761}, {"text": "in TAC 2011", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.8273806571960449}]}, {"text": "It uses features related to topic words and other important keywords identified using a graph-based approach.", "labels": [], "entities": []}, {"text": "Rather than greedy selection of top sentences, CLASSY 11 solves an approximate knapsack problem to obtain a more globally optimal summary.", "labels": [], "entities": []}, {"text": "Further, the scoring in this method uses bigrams as the basic unit/keyword in contrast to the other methods we have described previously that assume that a sentence is composed of a bag of unigrams.", "labels": [], "entities": []}, {"text": "We generated 100 word summaries from each described system.", "labels": [], "entities": []}, {"text": "Except for the CLASSY system, which performs more sophisticated redundancy removal, for the other methods we used the greedy Maximum Marginal Relevance technique (Carbonell and Goldstein 1998) for reducing redundancy.", "labels": [], "entities": [{"text": "redundancy removal", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7420991063117981}, {"text": "Maximum Marginal Relevance", "start_pos": 125, "end_pos": 151, "type": "METRIC", "confidence": 0.6596025625864664}]}, {"text": "After the sentence rankings were obtained, we added each sentence in order if it was not highly similar (a threshold value on cosine overlap is specified to indicate high similarity) to any of the already added sentences.", "labels": [], "entities": []}, {"text": "Each of the original TAC systems summaries were evaluated as follows.", "labels": [], "entities": [{"text": "TAC systems summaries", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.8003592093785604}]}, {"text": "We added the candidate summary to the pool of summaries from these other standard methods.", "labels": [], "entities": []}, {"text": "Then we computed the JS divergence between the candidate summary and the combined pool to obtain the score for the candidate.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7397962510585785}]}, {"text": "The procedure is the same as the one we followed in Section 6 except that here we assumed that for each TAC system, we only had these standard systems as peers rather than the full set of all TAC systems.", "labels": [], "entities": []}, {"text": "The results from this evaluation are shown in as SysSumm-std 9 . The previous evaluation results, using all TAC systems as consensus, is reproduced in the table as SysSumm-full.", "labels": [], "entities": [{"text": "SysSumm-std 9", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8543730676174164}, {"text": "SysSumm-full", "start_pos": 164, "end_pos": 176, "type": "DATASET", "confidence": 0.8628271222114563}]}, {"text": "We found that even with these few systems, the consensus evaluation is rather strong and produces correlations of 0.91 with pyramid and 0.77 with responsiveness scores.", "labels": [], "entities": []}, {"text": "These results provide additional support for the argument that high quality evaluation is feasible even with standard systems as peers and that a small set of such systems appears to be sufficient for forming the consensus.", "labels": [], "entities": []}, {"text": "Because the CLASSY systems are currently some of the top performing systems at TAC, we also evaluated how useful the consensus is if the CLASSY summaries are left out.", "labels": [], "entities": [{"text": "TAC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8494269847869873}]}, {"text": "So we evaluated the TAC systems using only the seven other standard systems (i.e., all except CLASSY04 and CLASSY11) as the peers and the results from this evaluation are reported in as SysSumm-std . We find that the correlations Performance of consensus evaluation approach on TAC'09 data (53 systems).", "labels": [], "entities": [{"text": "SysSumm-std", "start_pos": 186, "end_pos": 197, "type": "DATASET", "confidence": 0.9005727171897888}, {"text": "TAC'09 data", "start_pos": 278, "end_pos": 289, "type": "DATASET", "confidence": 0.9141684770584106}]}, {"text": "For input level (micro), the percentage of inputs with significant correlations is reported.", "labels": [], "entities": []}, {"text": "The results using TAC'09 systems as pseudomodels are indicated as SysSumm-full and those with off-the-shelf systems as SysSumm-std.", "labels": [], "entities": [{"text": "TAC'09", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.861388623714447}, {"text": "SysSumm-full", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.8254949450492859}]}, {"text": "remain the same even when the strongest systems are removed.", "labels": [], "entities": []}, {"text": "The usefulness of the consensus therefore is not heavily dependent on the presence of best-quality systems in the pool.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Spearman correlation between manual scores and ROUGE metrics on TAC 2009 data  (53 systems). All correlations are highly significant with p-value < 10 \u221210 .", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9720313549041748}, {"text": "TAC 2009 data", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9731074770291647}]}, {"text": " Table 2  Spearman correlation on the macro level for TAC'08 data (58 systems). All results are highly  significant with p-values < 0.000001 except unigram and multinomial summary probability,  which are not significant even at the 0.05 level.", "labels": [], "entities": [{"text": "correlation", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9744039177894592}, {"text": "TAC'08 data", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9334602355957031}]}, {"text": " Table 3  Spearman correlations at micro level for TAC'08 data (58 systems). Only the minimum and  maximum values of the significant correlations are reported, together with the number and  percentage of inputs that obtained significant correlation.", "labels": [], "entities": [{"text": "TAC'08 data", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9269226789474487}]}, {"text": " Table 5  Input-summary similarity evaluation: Results on TAC'09 (53 systems).", "labels": [], "entities": [{"text": "TAC'09", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.7757441401481628}]}, {"text": " Table 6  ROUGE evaluation with different number of models: macro level (Spearman correlations), micro  level (percentage of inputs with significant correlations on TAC'09 data). No. of systems = 53.", "labels": [], "entities": [{"text": "TAC'09 data", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.9111843705177307}, {"text": "No.", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.9627582430839539}]}, {"text": " Table 7  Spearman correlations between pseudoreference-based regression scores and manual content  scores. The first column lists the type of pseudoreference chosen.", "labels": [], "entities": []}, {"text": " Table 8  Performance before and after the addition of pseudomodel summaries: TAC'09 data  (53 systems).", "labels": [], "entities": [{"text": "TAC'09 data", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8317502737045288}]}, {"text": " Table 9  Performance of consensus evaluation approach on TAC'09 data (53 systems). For input level  (micro), the percentage of inputs with significant correlations is reported.", "labels": [], "entities": [{"text": "TAC'09 data", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9539583921432495}]}, {"text": " Table 12  Performance of consensus evaluation approach on TAC'09 data (53 systems). For input level  (micro), the percentage of inputs with significant correlations is reported. The results using  TAC'09 systems as pseudomodels are indicated as SysSumm-full and those with off-the-shelf  systems as SysSumm-std.", "labels": [], "entities": [{"text": "TAC'09 data", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.94195556640625}]}, {"text": " Table 13  Cross-year system level correlations for different metrics. The ROUGE-SU4 scores use all four  human summaries for reference. Improved correlations in 2009 are bolded and decreases in  correlations are italicized.", "labels": [], "entities": [{"text": "ROUGE-SU4", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9589617252349854}, {"text": "Improved correlations", "start_pos": 137, "end_pos": 158, "type": "METRIC", "confidence": 0.9569631218910217}]}]}