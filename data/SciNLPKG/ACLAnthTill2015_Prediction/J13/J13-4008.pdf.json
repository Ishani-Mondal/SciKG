{"title": [{"text": "Incremental, Predictive Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar", "labels": [], "entities": []}], "abstractContent": [{"text": "Psycholinguistic research shows that key properties of the human sentence processor are incre-mentality, connectedness (partial structures contain no unattached nodes), and prediction (up-coming syntactic structure is anticipated).", "labels": [], "entities": []}, {"text": "There is currently no broad-coverage parsing model with these properties, however.", "labels": [], "entities": [{"text": "broad-coverage parsing", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6633182168006897}]}, {"text": "In this article, we present the first broad-coverage probabilistic parser for PLTAG, a variant of TAG that supports all three requirements.", "labels": [], "entities": []}, {"text": "We train our parser on a TAG-transformed version of the Penn Treebank and show that it achieves performance comparable to existing TAG parsers that are incremental but not predictive.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9926729798316956}]}, {"text": "We also use our PLTAG model to predict human reading times, demonstrating a better fit on the Dundee eye-tracking corpus than a standard surprisal model.", "labels": [], "entities": [{"text": "PLTAG", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.7811219096183777}, {"text": "Dundee eye-tracking corpus", "start_pos": 94, "end_pos": 120, "type": "DATASET", "confidence": 0.9121529261271158}]}], "introductionContent": [{"text": "Evidence from psycholinguistic research suggests that human language comprehension is incremental.", "labels": [], "entities": []}, {"text": "Comprehenders do not wait until the end of the sentence before they build a syntactic representation for the sentence; rather, they construct a sequence of partial representations for sentence prefixes.", "labels": [], "entities": []}, {"text": "Experimental results indicate that each new word that is read or heard triggers an update of the representation constructed so far (.", "labels": [], "entities": []}, {"text": "There is also evidence for connectedness inhuman language processing.", "labels": [], "entities": []}, {"text": "Connectedness means that all input words are attached to the same syntactic structure (though connected structures can be constructed in parallel); comprehenders build no unconnected tree fragments, even for the sentence prefixes that arise during incremental processing.", "labels": [], "entities": []}, {"text": "Furthermore, a range of studies show that comprehenders make predictions about upcoming material on the basis of sentence prefixes.", "labels": [], "entities": []}, {"text": "There is experimental evidence that listeners predict complements of verbs based on their selectional restrictions (; readers predict a phrase introduced by or on encountering the word either; also the subcategorization frame of a verb can be used for prediction.", "labels": [], "entities": []}, {"text": "These studies find processing facilitation if predictions can be verified successfully, compared with sentences where predictions cannot be made or turnout to be incorrect.", "labels": [], "entities": []}, {"text": "Presumably, the human sentence processor uses prediction mechanisms to enable efficient comprehension in real time.", "labels": [], "entities": []}, {"text": "The three concepts of incrementality, connectedness, and prediction are fundamentally interrelated: Maintaining connected partial analyses is only nontrivial if the parsing process is incremental, and prediction means that a connected analysis is required also for words the parser has not yet seen.", "labels": [], "entities": [{"text": "Maintaining connected partial analyses", "start_pos": 100, "end_pos": 138, "type": "TASK", "confidence": 0.8805647790431976}]}, {"text": "In this article, we exploit the interrelatedness of incrementality, connectedness, and prediction to develop a parsing model for psycholinguistically motivated TAG (PLTAG;.", "labels": [], "entities": []}, {"text": "This formalism augments standard tree-adjoining grammar (TAG; Joshi,) with a predictive lexicon and a verification operation for validating predicted structures.", "labels": [], "entities": []}, {"text": "As we show in Section 2, these operations are motivated by psycholinguistic findings.", "labels": [], "entities": []}, {"text": "We argue that our PLTAG parser can form the basis fora new model of human sentence processing.", "labels": [], "entities": [{"text": "PLTAG parser", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.6431799530982971}, {"text": "human sentence processing", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.637130618095398}]}, {"text": "We successfully evaluate the predictions of this model against reading time data from an eye-tracking corpus, showing that it provides a better fit with the psycholinguistic data than the standard surprisal model of human sentence processing.", "labels": [], "entities": []}, {"text": "Crucially, this evaluation relies on the broad-coverage nature of our PLTAG parser, that is, the fact that it achieves high coverage and good parsing accuracy on corpus data.", "labels": [], "entities": [{"text": "PLTAG parser", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.6642151325941086}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9037219882011414}]}, {"text": "Only a broad-coverage parser can be used to model naturalistic data such as reading times from an eye-tracking corpus; this sets our approach apart from most other psycholinguistic models, for which only small-scale implementations for restricted data sets are available.", "labels": [], "entities": []}, {"text": "On the technical side, our key contribution is a novel parsing algorithm for probabilistic PLTAG.", "labels": [], "entities": []}, {"text": "Incremental fully connected parsing is fundamentally more difficult than non-incremental parsing or parsing without connectedness: Explicit hypotheses about how the words in a sentence are connected have to be made before all of the relevant evidence has been encountered in the input.", "labels": [], "entities": [{"text": "Incremental fully connected parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7166789621114731}]}, {"text": "The number of connected analyses grows quickly with the length of the sentence, and this problem gets worse in the presence of predicted structure.", "labels": [], "entities": []}, {"text": "Our parsing algorithm addresses this by grouping equivalent analyses together by only considering the fringes of trees, and by controlling the prediction process via supertagging.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9665125608444214}]}, {"text": "We evaluate our parser on a TAG-converted version of the Penn Treebank, achieving a coverage of 98.09% and an F-score of 79.41.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9764072597026825}, {"text": "coverage", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9987114667892456}, {"text": "F-score", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9996434450149536}]}, {"text": "These results approach the performance of previous (non-predictive) incremental TAG parsers.", "labels": [], "entities": [{"text": "TAG parsers", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.8289898931980133}]}, {"text": "We present a formalization of PLTAG in Section 3, introduce the PLTAG parsing algorithm and probability model in Section 4, show how a PLTAG lexicon can be induced from an augmented version of the Penn Treebank in Section 5, test parsing performance in Section 6, and finally provide a psycholinguistic evaluation on an eyetracking corpus in Section 7.", "labels": [], "entities": [{"text": "PLTAG parsing", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.6254749447107315}, {"text": "Penn Treebank", "start_pos": 197, "end_pos": 210, "type": "DATASET", "confidence": 0.9926171004772186}]}], "datasetContent": [{"text": "In order to compare the PLTAG parser to other probabilistic parsers, we evaluated parsing accuracy on the Penn Treebank (PTB).", "labels": [], "entities": [{"text": "PLTAG parser", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.5465094596147537}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9675639271736145}, {"text": "Penn Treebank (PTB)", "start_pos": 106, "end_pos": 125, "type": "DATASET", "confidence": 0.9772491097450257}]}, {"text": "We first converted the PTB into a PLTAG treebank as described in Section 5.", "labels": [], "entities": [{"text": "PTB", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.9723517894744873}, {"text": "PLTAG treebank", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.872423380613327}]}, {"text": "We then trained the parser on Sections 2-21 of the Penn Treebank and evaluated it on Section 23; only sentences of length 40 or less were used for evaluation.", "labels": [], "entities": [{"text": "Sections 2-21 of the Penn Treebank", "start_pos": 30, "end_pos": 64, "type": "DATASET", "confidence": 0.7437511384487152}]}, {"text": "It is important to note that because the parser is trained and evaluated on a converted treebank, its accuracy is not directly comparable to parsers that run directly on the PTB.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9992497563362122}, {"text": "PTB", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.9596304893493652}]}, {"text": "We will discuss this point in more detail in Section 6.1.", "labels": [], "entities": []}, {"text": "For the results reported here, the beam width of the parser was set such that all analyses whose log probability is less than the log probability of the best analysis minus 8 are removed (our implementation uses the natural logarithm).", "labels": [], "entities": []}, {"text": "The beam width was set using the development set (Section 0 of the PTB).", "labels": [], "entities": [{"text": "Section 0 of the PTB)", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.6887746453285217}]}, {"text": "Furthermore, only the 250 best analyses in a chart entry are maintained.", "labels": [], "entities": []}, {"text": "The supertagger was set to select the best 20 prediction trees at each step.", "labels": [], "entities": []}, {"text": "We found that parsing accuracy is higher when using a probability model that does not factor in the NONE-adjunction events (parsing accuracy decreases by about 1.5 percentage points in a model that takes these events into account).", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9729655385017395}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9671081304550171}, {"text": "NONE-adjunction", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.5481911897659302}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.8983842134475708}]}, {"text": "We believe that this decrease in parsing performance is due to the low probability of adjunction operations relative to substitution operations, which is a result of the normalization with NONE events.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9791818857192993}]}, {"text": "The high occurrence of NONE adjunctions in our parser is a consequence of the PLTAG conversion procedure, which leads to trees with more nodes overall, and hence many nodes at which no adjunction happens.", "labels": [], "entities": [{"text": "PLTAG conversion", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7176210880279541}]}, {"text": "In what follows we will present results for parsing with the probability model that does not include NONE-adjunction.", "labels": [], "entities": [{"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9807181358337402}, {"text": "NONE-adjunction", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.7051308155059814}]}, {"text": "We first evaluated the coverage of our PLTAG parser on the test set.", "labels": [], "entities": [{"text": "coverage", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9669234156608582}, {"text": "PLTAG parser", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.5369253158569336}]}, {"text": "The parser found valid parses for 98.09% of the sentences in Section 23 within reasonable memory usage (2 GB RAM).", "labels": [], "entities": [{"text": "Section 23", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.9347386062145233}]}, {"text": "The reason for failing to parse a sentence can be that all valid parses have fallen out of the beam, that an essential prediction tree was not selected by the supertagger, or that no parse can be derived given the PLTAG lexicon acquired during training.", "labels": [], "entities": []}, {"text": "A way of dealing with out-of-coverage sentences is to return a flat structure in which all words are attached directly to the root node.", "labels": [], "entities": []}, {"text": "This way a coverage of 100% is obtained, which facilitates the comparison between parsers.", "labels": [], "entities": [{"text": "coverage", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9844653010368347}]}, {"text": "We also give results for such a \"full coverage\" version of our parser in the following.", "labels": [], "entities": []}, {"text": "gives the parsing results for the variants of the PLTAG model that we evaluated.", "labels": [], "entities": []}, {"text": "The full PLTAG probability model as described in Section 3.6 achieved an F-score of 79.41 with Witten-Bell smoothing, given the gold-standard POS tags.", "labels": [], "entities": [{"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.999672532081604}, {"text": "POS tags", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.7474184334278107}]}, {"text": "When gold-standard POS tags are given, the algorithm only retrieves elementary trees fora word which includes the correct POS tag, whereas it retrieves all elementary trees fora word, and hence has a larger search space, when no gold-standard POS tags are given.", "labels": [], "entities": []}, {"text": "Without gold-standard POS tags, parsing performance drops to an F-score of Parsing results for the PLTAG parser with gold standard POS tags.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9725727438926697}, {"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9984825253486633}, {"text": "Parsing", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9801150560379028}, {"text": "PLTAG parser", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.6201727390289307}]}, {"text": "Prediction tree oracle = correct prediction tree provided; No gold POS = PLTAG parser with no gold-standard POS tags provided; full cov = flat structure returned for out-of-coverage sentences.", "labels": [], "entities": []}, {"text": "Although we have focused on the computational issues of PLTAG parsing so far, a key motivation behind our incremental parser is to develop a more realistic model of human language processing.", "labels": [], "entities": [{"text": "PLTAG parsing", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.8790255486965179}]}, {"text": "A treebank-based evaluation as in the previous section does not directly provide evidence of psycholinguistic validity; however, a parser with good coverage and high parsing accuracy is a prerequisite for an evaluation on eye-tracking corpora, which argues are the benchmark for models of human sentence processing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9348357915878296}]}, {"text": "In what follows, we report an evaluation study that uses our PLTAG parser to predict human reading times, and compares its performance on this task to a standard model based on surprisal (Hale 2001).", "labels": [], "entities": []}, {"text": "Surprisal assumes that processing difficulty is associated with expectations built up by the sentence processor: A word that is unexpected given its preceding context is harder to process.", "labels": [], "entities": []}, {"text": "Mathematically, the amount of surprisal at word w i can be formalized as the negative logarithm of the conditional probability of w i given the preceding words in the sentence w 1 . .", "labels": [], "entities": []}, {"text": "w i\u22121 : = \u2212 log P(w 1 . .", "labels": [], "entities": []}, {"text": "Here, P(\u03c4 p w 1 ...w i ) is the probability of the prefix tree \u03c4 p w 1 ...w i that spans the words w 1 . .", "labels": [], "entities": []}, {"text": "w i . If the surprisal at word w i is high, then w i should be difficult to process.", "labels": [], "entities": []}, {"text": "This manifests itself in elevated reading times, for example, in eye-tracking data ().", "labels": [], "entities": []}, {"text": "Surprisal can be estimated in two different ways: as lexical surprisal and structural surprisal, following.", "labels": [], "entities": []}, {"text": "We calculated lexical surprisal using the prefix probabilities returned by the incremental probabilistic parser of.", "labels": [], "entities": []}, {"text": "Lexical surprisal takes into account the lexical items that makeup a sentence prefix, and is thus influenced byword frequency and by the probability of a word being assigned a specific part of speech.", "labels": [], "entities": [{"text": "Lexical surprisal", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8615382313728333}]}, {"text": "This quantity will be referred to as the factor LEXICALSURPRISAL in the following.", "labels": [], "entities": [{"text": "LEXICALSURPRISAL", "start_pos": 48, "end_pos": 64, "type": "METRIC", "confidence": 0.9810581207275391}]}, {"text": "This can be contrasted with structural surprisal (factor STRUCTURAL-SURPRISAL), which uses unlexicalized parses, and is based only on the probability of the syntactic structures assigned to a sentence prefix.", "labels": [], "entities": []}, {"text": "Following, we replaced each word in the training corpus with its part-of-speech tag and then trained the Roark parser on this version of the corpus.", "labels": [], "entities": []}, {"text": "The unlexicalized parser obtained this way was run on the Dundee corpus and prefix probabilities were obtained in the usual way to compute unlexicalized surprisal scores.", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.9956285059452057}]}, {"text": "In previous work, Prediction Theory difficulty scores estimated using PLTAG have been shown to successfully account for individual experimental results in psycholinguistics.", "labels": [], "entities": [{"text": "Prediction Theory difficulty scores", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.7800595462322235}]}, {"text": "showed that Prediction Theory can account for the relative clause asymmetry, that is, the fact that subject relative clauses are easier to process than object relative clauses.", "labels": [], "entities": [{"text": "Prediction Theory", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.9441669285297394}]}, {"text": "Prediction Theory provides an explanation for this fact in terms of higher verification cost for object relative clauses.", "labels": [], "entities": [{"text": "Prediction Theory", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9109972715377808}]}, {"text": "also show that Prediction Theory successfully models the either . .", "labels": [], "entities": [{"text": "Prediction Theory", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8375093042850494}]}, {"text": ". or effect (Staub and Clifton 2006): Coordinate structures involving either are easier to process than ones involving just or.", "labels": [], "entities": []}, {"text": "Prediction Theory explains this in terms of the lexicon entry for either, which introduces a prediction tree for the whole coordinate structure.", "labels": [], "entities": [{"text": "Prediction Theory", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8786654472351074}]}], "tableCaptions": [{"text": " Table 2  Parsing results for the PLTAG parser with gold standard POS tags. Prediction tree oracle =  correct prediction tree provided; No gold POS = PLTAG parser with no gold-standard  POS tags provided; full cov = flat structure returned for out-of-coverage sentences.", "labels": [], "entities": []}]}