{"title": [{"text": "Clustering and Diversifying Web Search Results with Graph-Based Word Sense Induction", "labels": [], "entities": []}], "abstractContent": [{"text": "Web search result clustering aims to facilitate information search on the Web.", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6712942868471146}]}, {"text": "Rather than the results of a query being presented as a flat list, they are grouped on the basis of their similarity and subsequently shown to the user as a list of clusters.", "labels": [], "entities": []}, {"text": "Each cluster is intended to represent a different meaning of the input query, thus taking into account the lexical ambiguity (i.e., polysemy) issue.", "labels": [], "entities": []}, {"text": "Existing Web clustering methods typically rely on some shallow notion of textual similarity between search result snippets, however.", "labels": [], "entities": []}, {"text": "As a result, text snippets with no word in common tend to be clustered separately even if they share the same meaning, whereas snippets with words in common maybe grouped together even if they refer to different meanings of the input query.", "labels": [], "entities": []}, {"text": "In this article we present a novel approach to Web search result clustering based on the automatic discovery of word senses from raw text, a task referred to as Word Sense Induction.", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6638643145561218}, {"text": "Word Sense Induction", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.6075812578201294}]}, {"text": "Key to our approach is to first acquire the various senses (i.e., meanings) of an ambiguous query and then cluster the search results based on their semantic similarity to the word senses induced.", "labels": [], "entities": []}, {"text": "Our experiments, conducted on data sets of ambiguous queries, show that our approach outperforms both Web clustering and search engines.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Web is by far the largest information archive available worldwide.", "labels": [], "entities": []}, {"text": "This vast pool of text contains information of the most wildly disparate kinds, and is potentially capable of satisfying virtually any conceivable user need.", "labels": [], "entities": []}, {"text": "Unfortunately, however, in this setting retrieving the precise item of information that is relevant to a given user search can be like looking fora needle in a haystack.", "labels": [], "entities": []}, {"text": "State-of-the-art search engines such as Google and Yahoo!", "labels": [], "entities": []}, {"text": "generally do a good job at retrieving a small number of relevant results from such an enormous collection of data (i.e., retrieving with high precision, low recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9958261847496033}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9991198182106018}]}, {"text": "Such systems today, however, still find themselves up against the lexical ambiguity issue In this article, we present a novel approach to Web search result clustering that explicitly addresses the language ambiguity issue.", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 138, "end_pos": 166, "type": "TASK", "confidence": 0.6288023665547371}]}, {"text": "Key to our approach is the use of Word Sense Induction (WSI), that is, techniques aimed at automatically discovering the different meanings of a given term (i.e., query).", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7416714082161585}]}, {"text": "Each sense of the query is represented as a cluster of words co-occurring in raw text with the query.", "labels": [], "entities": []}, {"text": "Each search result snippet returned by a Web search engine is then mapped to the most appropriate meaning (i.e., cluster) and the resulting clustering of snippets is returned.", "labels": [], "entities": []}, {"text": "This article provides four main contributions: r We present a general evaluation framework for Web search result clustering, which we also exploit to perform a large-scale end-to-end experimental comparison of several graph-based WSI algorithms.", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6884332895278931}]}, {"text": "In fact, the output of WSI (i.e., the automatically discovered senses) is evaluated in terms of both the quality of the corresponding search result clusters and the resulting ability to diversify search results.", "labels": [], "entities": []}, {"text": "This is in contrast with most literature in the field of Word Sense Induction, where experiments are mainly performed in vitro (i.e., not in the context of an everyday application;).", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7541227142016093}]}, {"text": "r In order to test whether our results were strongly dependent on the evaluation measures we implemented in the framework, we complemented our extrinsic experimental evaluation with a qualitative analysis of the automatically induced senses.", "labels": [], "entities": []}, {"text": "This study was performed via a manual evaluation carried out by several human annotators.", "labels": [], "entities": []}, {"text": "r We present novel versions of previously proposed WSI graph-based algorithms, namely, SquaT++ and Balanced Maximum Spanning Tree (B-MST) (the former is an enhancement of the original SquaT algorithm, and the latter is a variant of MST that produces more balanced clusters).", "labels": [], "entities": [{"text": "WSI graph-based", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7842063307762146}]}, {"text": "r We show how, thanks to our framework, WSI can be successfully integrated into real-world applications, such as Web search result clustering, so as to outperform non-semantic state-of-the-art Web clustering systems.", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.6659283190965652}]}, {"text": "To the best of our knowledge, with the exception of some very preliminary results, this is the first time that unsupervised text understanding techniques have been shown to considerably boost an Information Retrieval task in a solid evaluation framework.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7441232204437256}, {"text": "Information Retrieval task", "start_pos": 195, "end_pos": 221, "type": "TASK", "confidence": 0.8803576827049255}]}, {"text": "This article extends previous conference work ( by performing a novel, in-depth study of the interactions between different corpora and several different WSI algorithms, including novel ones, within the same framework, and, additionally, by providing a comparison with a stateof-the-art search result clustering engine.", "labels": [], "entities": []}, {"text": "The article is structured as follows: in Section 2 we present related work, in Section 3 we illustrate our approach, end-to-end experiments are reported in Section 4, and in vitro experiments are discussed in Section 5.", "labels": [], "entities": []}, {"text": "We present a time performance analysis in Section 6, and conclude the paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present two extrinsic experiments aimed at determining the impact of WSI when integrated into Web search result clustering.", "labels": [], "entities": [{"text": "WSI", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.7597118020057678}, {"text": "Web search result clustering", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.6070527881383896}]}, {"text": "We first describe our experimental set-up (Section 4.1).", "labels": [], "entities": []}, {"text": "Next, we present a first experiment focused on the quality of the output search result clusters (Section 4.2) and a second experiment on the degree of diversification of semantically enhanced versus non-semantic search result clustering algorithms (Section 4.3).", "labels": [], "entities": []}, {"text": "In all our experiments our lexicon was given by the entire WordNet vocabulary () augmented with the set of queries in our test data sets.", "labels": [], "entities": [{"text": "WordNet vocabulary", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.95365771651268}]}, {"text": "r AMBIENT (AMBIguous ENTries), a data set that contains 44 ambiguous queries.", "labels": [], "entities": [{"text": "AMBIENT", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.5664199590682983}]}, {"text": "The sense inventory for the meanings (i.e., subtopics) 13 of queries is given by Wikipedia disambiguation pages.", "labels": [], "entities": []}, {"text": "For instance, given the beagle query, its disambiguation page in Wikipedia provides the meanings of dog, Mars lander, computer search service, beer brand, and so forth.", "labels": [], "entities": []}, {"text": "The top 100 Web results of each query returned by the Yahoo!", "labels": [], "entities": []}, {"text": "search engine were tagged with the most appropriate query senses according to Wikipedia (amounting to 4,400 sense-annotated search results).", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.9496975541114807}]}, {"text": "To our knowledge, this is currently the largest data set of ambiguous queries available on-line.", "labels": [], "entities": []}, {"text": "In fact, other existing data sets, such as those from the TREC Interactive Tracks, are not focused on distinguishing the subtopics of a query.", "labels": [], "entities": [{"text": "TREC Interactive Tracks", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.8479253252347311}]}, {"text": "r MORESQUE (MORE Sense-tagged QUEry results), a data set that we developed as an integration of AMBIENT following guidelines provided by its authors.", "labels": [], "entities": [{"text": "MORESQUE", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9153245687484741}, {"text": "MORE Sense-tagged QUEry results)", "start_pos": 12, "end_pos": 44, "type": "METRIC", "confidence": 0.7902793645858764}, {"text": "AMBIENT", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9038230776786804}]}, {"text": "In fact, our aim was to study the behavior of Web search algorithms on queries of different lengths, ranging from one to four words.", "labels": [], "entities": []}, {"text": "The AMBIENT data set, however, is composed in the main of one-word queries.", "labels": [], "entities": [{"text": "AMBIENT data set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8993259270985922}]}, {"text": "MORESQUE provides dozens of queries of length 2, 3, and 4, together with the top 100 results from Yahoo!", "labels": [], "entities": [{"text": "MORESQUE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.651792049407959}]}, {"text": "for each query annotated precisely as was done in the AMBIENT data set.", "labels": [], "entities": [{"text": "AMBIENT data set", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9702027837435404}]}, {"text": "We decided not to discontinue the use of Yahoo!", "labels": [], "entities": []}, {"text": "Wikipedia has already been used as a sense inventory by, among others,,, and.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.904731273651123}]}, {"text": "have investigated in depth the benefit of using Wikipedia as the sense inventory for diversifying search results, showing that Wikipedia offers much more sense coverage for search results than other resources such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 217, "end_pos": 224, "type": "DATASET", "confidence": 0.9443445801734924}]}, {"text": "We report the statistics on the composition of the two data sets in.", "labels": [], "entities": []}, {"text": "Given that the snippets could possibly be annotated with more than one Wikipedia subtopic, we also determined the average number of subtopics per snippet.", "labels": [], "entities": []}, {"text": "This amounted to 1.01 for AMBIENT and 1.04 for MORESQUE for snippets with at least one subtopic annotation.", "labels": [], "entities": [{"text": "AMBIENT", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.4448007643222809}, {"text": "MORESQUE", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.910674512386322}]}, {"text": "We can thus conclude that multiple subtopic annotations are infrequent.", "labels": [], "entities": []}, {"text": "Finally, we analyzed how the different subtopics are distributed over the snippet results for each query.", "labels": [], "entities": []}, {"text": "To do this we calculated the standard deviation of the subtopic population for each individual query, which we show in.", "labels": [], "entities": []}, {"text": "We observed a considerable difference in the standard deviations of shorter and longer queries (e.g., between those from the AMBIENT data set [from 1 to 44 in the and the MORESQUE data set [from 45 to 158]).", "labels": [], "entities": [{"text": "AMBIENT data set", "start_pos": 125, "end_pos": 141, "type": "DATASET", "confidence": 0.9619971911112467}, {"text": "MORESQUE data set", "start_pos": 171, "end_pos": 188, "type": "DATASET", "confidence": 0.8848335146903992}]}, {"text": "We further calculated the average standard deviation over the two data sets' queries, obtaining 6.5 for AMBIENT and 13.1 for MORESQUE.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 34, "end_pos": 52, "type": "METRIC", "confidence": 0.9340420365333557}, {"text": "AMBIENT", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.7711215019226074}, {"text": "MORESQUE", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.6662929058074951}]}, {"text": "Therefore we anticipate that the longer the query length, the more unbalanced will be the distribution of its subtopics over the top-ranking results.", "labels": [], "entities": []}, {"text": "In line with previous experiments on search result clustering, our data set does not contain monosemous queries for two reasons: (i) we are interested in queries with multiple meanings, and (ii) monosemous queries would increase the performance of our experiments because no diversification would be needed for them.", "labels": [], "entities": [{"text": "search result clustering", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6809883117675781}]}, {"text": "In this first experiment our goal is to evaluate the quality of the output produced by our search result clustering systems.", "labels": [], "entities": []}, {"text": "Unfortunately, the clustering evaluation problem is a notably hard issue, and one for which there exists no unequivocal solution.", "labels": [], "entities": [{"text": "clustering evaluation", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.948251873254776}]}, {"text": "Many evaluation measures have been proposed in the literature, inter alia) so, in order to get exhaustive results, we tested three different clustering quality measures, namely, Adjusted Rand Index, Jaccard Index, and F1-measure, which we introduce hereafter.", "labels": [], "entities": [{"text": "Adjusted Rand Index", "start_pos": 178, "end_pos": 197, "type": "METRIC", "confidence": 0.842080275217692}, {"text": "Jaccard Index", "start_pos": 199, "end_pos": 212, "type": "METRIC", "confidence": 0.7361310124397278}, {"text": "F1-measure", "start_pos": 218, "end_pos": 228, "type": "METRIC", "confidence": 0.9976828098297119}]}, {"text": "Each of these measures M(C, G) calculates the quality of a clustering C, output fora given query q, against the gold standard clustering G for that query.", "labels": [], "entities": []}, {"text": "We then determine the overall results on the entire set of queries Q in the test set according to the measure M by averaging the values of M(C, G) obtained for each single test query q \u2208 Q.", "labels": [], "entities": []}, {"text": "Given a gold standard clustering G, the Rand Index (RI; of a clustering C is a measure of clustering agreement commonly used in the literature, calculated as follows: where TP is the number of true positives (i.e., snippet pairs) that are in the same cluster both in C and G, TN is the number of true negatives (i.e., pairs which are in different clusters in both clusterings), and FP and FN are, respectively, the number of false positives and false negatives.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.8812706768512726}, {"text": "FP", "start_pos": 382, "end_pos": 384, "type": "METRIC", "confidence": 0.9984526634216309}, {"text": "FN", "start_pos": 389, "end_pos": 391, "type": "METRIC", "confidence": 0.9870872497558594}]}, {"text": "For the gold standard G we use the clustering induced by the sense annotations provided in our data sets for each snippet (i.e., each cluster contains the snippets manually associated with a particular Wikipedia page, that is, subtopic, of the query).", "labels": [], "entities": []}, {"text": "Rand Index determines the percentage of snippet pairs that are in the same configuration in both C and G, but its main weakness is that it does not take chance into account.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.611405074596405}]}, {"text": "In fact, the expected value of the RI of two random clusterings is not a constant value (e.g., 0).", "labels": [], "entities": [{"text": "RI", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.8036409020423889}]}, {"text": "This issue is addressed by the Adjusted Rand Index (ARI;, which corrects the RI for chance agreement and makes it vary according to expectation: where E(RI(C, G)) is the expected value of the RI.", "labels": [], "entities": [{"text": "Adjusted Rand Index (ARI;", "start_pos": 31, "end_pos": 56, "type": "METRIC", "confidence": 0.899304191271464}]}, {"text": "Given two clusterings C = (C 1 , . .", "labels": [], "entities": []}, {"text": ", Cm ) and G = . .", "labels": [], "entities": [{"text": "G", "start_pos": 11, "end_pos": 12, "type": "METRIC", "confidence": 0.9879446029663086}]}, {"text": ", G g ), we first quantify the degree of overlap between C and G using the contingency table reported in, where n ij denotes the number of objects in common between G i and C j (i.e., n ij = |G i \u2229 C j |) and a i and b j represent, respectively, the number of objects in G i and C j . Now, Equation can be reformulated as follows: Differently from the original RI (which ranges between 0 and 1), the ARI ranges between \u22121 and +1 and is 0 when the index equals its expected value.", "labels": [], "entities": [{"text": "Equation", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.7357656955718994}, {"text": "ARI", "start_pos": 400, "end_pos": 403, "type": "METRIC", "confidence": 0.9698173999786377}]}, {"text": "Given the issues with RI, in our experiments we focused on ARI.", "labels": [], "entities": [{"text": "RI", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.822605550289154}]}, {"text": "The ARI compares a clustering C with a gold standard G both in terms of the snippets occurring in the same cluster (TP) and those which are assigned to different clusters (TN).", "labels": [], "entities": []}, {"text": "There are typically many TN in a clustering, however; therefore this measure tends to overweight the usefulness of snippets placed in different clusters.", "labels": [], "entities": []}, {"text": "The Jaccard Index (JI) is a measure that addresses this issue.", "labels": [], "entities": [{"text": "Jaccard Index (JI)", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.8849992871284484}]}, {"text": "JI is calculated as follows: In fact, in contrast to RI (cf. Equation), neither the numerator nor the denominator of JI include the TN term.", "labels": [], "entities": [{"text": "RI", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9692747592926025}]}, {"text": "Most of today's search engines return a flat list of search results.", "labels": [], "entities": []}, {"text": "We thus performed a second experiment aimed at quantifying the impact of our Web search result clustering systems on flat-list search engines.", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.5387415960431099}]}, {"text": "In other words, our goal was to determine how many different meanings of a query are covered in the top-ranking results shown to the user.", "labels": [], "entities": []}, {"text": "One natural way of measuring such performance is given by S-recall@K (Subtopic recall at rank K) and S-precision@r (Subtopic precision at recall r).", "labels": [], "entities": []}, {"text": "S-recall@K counts the number of different subtopics retrieved for q in the top K results returned: where subtopics(r i ) is the set of subtopics manually assigned to the search result r i and m is the number of subtopics for query q in the gold standard.", "labels": [], "entities": []}, {"text": "In order to cutout some noise, we calculated the S-recall@K considering only the subtopics assigned to at least two snippets.", "labels": [], "entities": [{"text": "S-recall", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9378921985626221}]}, {"text": "S-precision@r instead determines the ratio of different subtopics retrieved for q in the first Kr documents, where Kr is the minimum number of top results for which the system achieves recall r.", "labels": [], "entities": [{"text": "recall r", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9704833030700684}]}, {"text": "Formally: So whereas S-recall@K aims at determining the performance of a system at retrieving the largest number of topics for the query q in the K top-ranking results, S-precision@r quantifies the ratio of distinct subtopics covered by the minimal set of results returned for which the system obtains a specific recall r.", "labels": [], "entities": []}, {"text": "Note that unambiguous queries would perform with S-precision@r = S-recall@K = 1 for all values of rand K.", "labels": [], "entities": []}, {"text": "18 These two measures are only suitable, however, for systems returning ranked lists (such as Yahoo! and Essential Pages).", "labels": [], "entities": []}, {"text": "In order to apply them to search result clustering systems, we flatten each clustering to a list of search results.", "labels": [], "entities": []}, {"text": "To do so, given a clustering C = (C 1 , C 2 , . .", "labels": [], "entities": []}, {"text": ", Cm ), we add to the initially empty list the first element of each cluster C j (j = 1, . .", "labels": [], "entities": []}, {"text": ", m); then we iterate the process by selecting the second element of each cluster C j such that |C j | \u2265 2, and soon.", "labels": [], "entities": []}, {"text": "The remaining elements returned by the search engine, but not included in any cluster of C, are appended to the bottom of the list in their original order.", "labels": [], "entities": []}, {"text": "Although the primary aim of this work was to demonstrate a relevant, end-to-end application of sense discovery techniques, we performed an additional in vitro experiment aimed at verifying the quality of the discovered senses independently of the task in which they are used.", "labels": [], "entities": [{"text": "sense discovery", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7327893972396851}]}, {"text": "When performing in vitro evaluations, no single intrinsic measure provides a clear hint as to which algorithm performs best ().", "labels": [], "entities": []}, {"text": "In fact, some measures favor large clusters, whereas others are based on the expectaction that the WSI algorithm will discover more fine-grained sense distinctions.", "labels": [], "entities": []}, {"text": "To provide further insights into the clusters produced by our graph-based WSI algorithms, we performed  a qualitative evaluation of the output clusters.", "labels": [], "entities": []}, {"text": "To this end we randomly selected 17 queries from our query data set.", "labels": [], "entities": []}, {"text": "For each query, we submitted in random order the output of three representative WSI algorithms on the ukWaC corpus, namely, Curvature, HyperLex, and B-MST, to five annotators.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.9922995567321777}, {"text": "B-MST", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.9075883626937866}]}, {"text": "We show an excerpt of the evaluation procedure for the query excalibur in.", "labels": [], "entities": []}, {"text": "On the left side of the table we propose an example of an anonymized set of three clusterings (i.e., one for each algorithm, shown in columns 2-4) presented to our annotators.", "labels": [], "entities": []}, {"text": "Each algorithm produced a group of clusters, each of which consisted of a set of words strictly related to the meaning conveyed by the cluster itself, as discussed in An example of the manual evaluation procedure for the query excalibur: We show a clustering triple proposed to the evaluator (left side) and an example of produced ranking (right side).", "labels": [], "entities": []}, {"text": "we show an example of ranking for the three clusterings.", "labels": [], "entities": []}, {"text": "In the example, clustering B was deemed to be more representative, because it better models three meanings of excalibur, namely: the filmnovel meaning, the sword meaning, and the hotel casino meaning, whereas clustering A mixes the movie and the casino meaning within cluster 1, and, even worse, clustering C just provides a singleton cluster.", "labels": [], "entities": []}, {"text": "Finally, for each query, and for the entire set of 17 queries, we calculated the average ranking obtained by each WSI algorithm.", "labels": [], "entities": []}, {"text": "The overall results are shown in (last row): 1.7 for HyperLex, 1.8 for B-MST, and 2.4 for Curvature.", "labels": [], "entities": [{"text": "HyperLex", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8177286982536316}, {"text": "B-MST", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.8389293551445007}, {"text": "Curvature", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.9022416472434998}]}, {"text": "This experiment corroborates the findings obtained from our extrinsic experiments: Curvature is the worst-ranking system (probably because of the low number of induced senses), whereas HyperLex and B-MST are more apt to discriminate between the meanings of an input query.", "labels": [], "entities": []}, {"text": "It is worth noting that the annotators often assigned the same rank to the clusters produced by B-MST and HyperLex, confirming our extrinsic finding that the two algorithms tend to have a similar behavior, compared with local graph pattern WSI.", "labels": [], "entities": []}], "tableCaptions": []}