{"title": [], "abstractContent": [{"text": "Word sense disambiguation (WSD) is an old and important task in computational linguistics that still remains challenging, to machines as well as to human annotators.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8436005065838496}]}, {"text": "Recently there have been several proposals for representing word meaning in context that diverge from the traditional use of a single best sense for each occurrence.", "labels": [], "entities": []}, {"text": "They represent word meaning in context through multiple paraphrases, as points in vector space, or as distributions over latent senses.", "labels": [], "entities": []}, {"text": "New methods of evaluating and comparing these different representations are needed.", "labels": [], "entities": []}, {"text": "In this paper we propose two novel annotation schemes that characterize word meaning in context in a graded fashion.", "labels": [], "entities": [{"text": "characterize word meaning in context", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.7727093696594238}]}, {"text": "In WSsim annotation, the applicability of each dictionary sense is rated on an ordinal scale.", "labels": [], "entities": []}, {"text": "Usim annotation directly rates the similarity of pairs of usages of the same lemma, again on a scale.", "labels": [], "entities": [{"text": "similarity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.958123505115509}]}, {"text": "We find that the novel annotation schemes show good inter-annotator agreement, as well as a strong correlation with traditional single-sense annotation and with annotation of multiple lexical paraphrases.", "labels": [], "entities": []}, {"text": "Annotators make use of the whole ordinal scale, and give very fine-grained judgments that \"mix and match\" senses for each individual usage.", "labels": [], "entities": []}, {"text": "We also find that the Usim ratings obey the triangle inequality, justifying models that treat usage similarity as metric.", "labels": [], "entities": []}, {"text": "There has recently been much work on grouping senses into coarse-grained groups.", "labels": [], "entities": [{"text": "grouping senses", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.9264993667602539}]}, {"text": "We demonstrate that graded WSsim and Usim ratings can be used to analyze existing coarse-grained sense groupings to identify sense groups that may not match intuitions of untrained native speakers.", "labels": [], "entities": []}, {"text": "In the course of the comparison, we also show that the WSsim ratings are not subsumed by any static sense grouping.", "labels": [], "entities": [{"text": "WSsim", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.5885800123214722}]}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is a task that has attracted much work in computational linguistics (see Agirre and Edmonds and Navigli for an overview), including a series of workshops, and, which were originally organized expressly as a forum for shared tasks in WSD.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8354009489218394}]}, {"text": "In WSD, polysemy is typically modeled through a dictionary, where the senses of a word are understood to be mutually disjoint.", "labels": [], "entities": [{"text": "WSD", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9703004956245422}]}, {"text": "The meaning of an occurrence of a word is then characterized through the best-fitting among its dictionary senses.", "labels": [], "entities": []}, {"text": "The assumption of senses that are mutually disjoint and that have clear boundaries has been drawn into doubt by lexicographers, linguists, and psychologists.", "labels": [], "entities": []}, {"text": "argues that word senses have uses where they clearly fit, and borderline uses where only a few of a sense's identifying features apply.", "labels": [], "entities": []}, {"text": "This notion matches results in psychology on human concept representation: Mental categories show \"fuzzy boundaries,\" and category members differ in typicality and degree of membership.", "labels": [], "entities": [{"text": "human concept representation", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.8194433053334554}]}, {"text": "This raises the question of annotation: Is it possible to collect word meaning annotation that captures degrees to which a sense applies?", "labels": [], "entities": []}, {"text": "Recently, there have been several proposals for modeling word meaning in context that can represent different degrees of similarity to a word sense, as well as different degrees of similarity between occurrences of a word.", "labels": [], "entities": []}, {"text": "The SemEval Lexical Substitution task represents each occurrence through multiple weighted paraphrases.", "labels": [], "entities": [{"text": "SemEval Lexical Substitution task", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.9099017083644867}]}, {"text": "Other approaches represent meaning in context through a vector space model or through a distribution over latent senses (.", "labels": [], "entities": []}, {"text": "Again, this raises the question of annotation: Can human annotators give fine-grained judgments about degrees of similarity between word occurrences, like these computational models predict?", "labels": [], "entities": []}, {"text": "The question that we explore in this paper is: Can word meaning be described through annotation in the form of graded judgments?", "labels": [], "entities": []}, {"text": "We want to know whether annotators can provide graded meaning annotation in a consistent fashion.", "labels": [], "entities": []}, {"text": "Also, we want to know whether annotators will use the whole graded scale, or whether they will fallback on binary ratings of either \"identical\" or \"different.\"", "labels": [], "entities": []}, {"text": "Our question, however, is not whether annotators can be trained to do this.", "labels": [], "entities": []}, {"text": "Rather, our aim is to describe word meaning as language users perceive it.", "labels": [], "entities": []}, {"text": "We want to tap into the annotators' intuitive notions of word meaning.", "labels": [], "entities": []}, {"text": "As a consequence, we use untrained annotators.", "labels": [], "entities": []}, {"text": "We view it as an important aim on its own to capture language users' intuitions on word meaning, but it is also instrumental in answering our first question, of whether word meaning can be described through graded annotator judgments: Training annotators in depth on how to distinguish predefined hand-crafted senses could influence them to assign those senses in a binary fashion.", "labels": [], "entities": []}, {"text": "We introduce two novel annotation tasks in which human annotators characterize word meaning in context.", "labels": [], "entities": [{"text": "characterize word meaning in context", "start_pos": 66, "end_pos": 102, "type": "TASK", "confidence": 0.7892890214920044}]}, {"text": "In the first task, they rate the applicability of dictionary senses on a graded scale.", "labels": [], "entities": []}, {"text": "In the second task, they rate the similarity between pairs of usages of the same word, also on a graded scale.", "labels": [], "entities": []}, {"text": "In designing the annotation tasks, we utilize techniques from psycholinguistic experimentation: Annotators give ratings on a scale, rather than selecting a single label; we also use multiple annotators for each item, retaining all annotator judgments.", "labels": [], "entities": []}, {"text": "The result of this graded annotation can then be used to evaluate computational models of word meaning: either to evaluate graded models of word meaning, or to evaluate traditional WSD systems in a graded fashion.", "labels": [], "entities": []}, {"text": "They can also be used to analyze existing word sense inventories, in particular to identify sense distinctions worth revisiting-we say more on this latter use subsequently.", "labels": [], "entities": []}, {"text": "Our aim is not to improve inter-annotator agreement over traditional sense annotation.", "labels": [], "entities": []}, {"text": "It is highly unlikely that ratings on a scale would ever achieve higher exact agreement than binary annotation.", "labels": [], "entities": [{"text": "exact agreement", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.8077202439308167}]}, {"text": "Our aim is also not to maximize exact agreement, as we expect to see individual differences in perceived meaning, and want to capture those differences.", "labels": [], "entities": []}, {"text": "Still it is desirable to have an end product of the annotation that is robust against such individual differences.", "labels": [], "entities": []}, {"text": "In order to achieve this, we average judgments over multiple annotators after first inspecting pairwise correlations between annotators to ensure that they are all doing their work diligently and with similar outcomes.", "labels": [], "entities": []}, {"text": "Analyzing the annotation results, we find that the annotators make use of intermediate points on the graded scale and do not treat the task as inherently binary.", "labels": [], "entities": []}, {"text": "We find that there is good inter-annotator agreement, measured as correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9681118726730347}]}, {"text": "There is also a highly significant correlation across tasks and with traditional WSD and lexical substitution tasks.", "labels": [], "entities": [{"text": "WSD", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.8787338733673096}]}, {"text": "This indicates that the annotators performed these tasks in a consistent fashion.", "labels": [], "entities": []}, {"text": "It also indicates that diverse ways of representing word meaning in context-single best sense, weighted senses, multiple paraphrases, usage similarityyield similar characterizations.", "labels": [], "entities": []}, {"text": "We find that annotators frequently give high scores to more than one sense, in away that is not remedied by a more coarse-grained sense inventory.", "labels": [], "entities": []}, {"text": "In fact, the annotations are often inconsistent with disjoint sense partitions.", "labels": [], "entities": []}, {"text": "The work reported here is based on our earlier work reported in Erk, McCarthy, and.", "labels": [], "entities": []}, {"text": "The current paper extends the previous work in three ways.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because both graded annotation tasks, WSsim and Usim, use ratings on five-point scales rather than binary ratings, we measure agreement in terms of correlation.", "labels": [], "entities": [{"text": "WSsim", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.8225646615028381}]}, {"text": "Because ratings were not normally distributed, we choose a non-parametric test which uses ranks rather than absolute values: We use Spearmans rank correlation coefficient (rho), following.", "labels": [], "entities": [{"text": "Spearmans rank correlation coefficient (rho)", "start_pos": 132, "end_pos": 176, "type": "METRIC", "confidence": 0.7317359106881278}]}, {"text": "For assessing inter-tagger agreement on the R2 WSbest task we adopt the standard WSD measure of average pairwise agreement, and for R2 SYNbest, we use the same pairwise agreement calculation used in LEXSUB.", "labels": [], "entities": [{"text": "R2 WSbest task", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.4564625918865204}]}, {"text": "When comparing graded ratings with single-sense or lexical substitution annotation, we use the mean of all annotator ratings in the WSsim or Usim annotation.", "labels": [], "entities": [{"text": "WSsim", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.916575014591217}]}, {"text": "This is justified because the inter-annotator agreement is highly significant, with respectable rho compared with previous work.", "labels": [], "entities": [{"text": "rho", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9768324494361877}]}, {"text": "As the annotation schemes differ between R1 and R2 (as mentioned previously, the number of annotators and the amount of visible context are different, and R2 annotators did traditional word sense annotation in the WSbest task in addition to the graded tasks) we report the results of R1 and R2 separately.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Best word sense disambiguation performance in SensEval/SemEval English lexical sample  tasks.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6680935323238373}, {"text": "SensEval/SemEval English lexical sample  tasks", "start_pos": 56, "end_pos": 102, "type": "TASK", "confidence": 0.6074508811746325}]}, {"text": " Table 7  WSsim example, R1: Annotator judgments for the different senses of paper.", "labels": [], "entities": []}, {"text": " Table 8  WSsim example, R2: Annotator judgments for the different senses of neat.", "labels": [], "entities": []}, {"text": " Table 9  Correlation matrix for pairwise correlation agreement for WSsim-1. The last row provides the  agreement of the annotator in that column against the average from the other annotators.", "labels": [], "entities": [{"text": "WSsim-1", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.7994413375854492}]}, {"text": " Table 10  Correlation matrix for pairwise correlation agreement for WSsim-2. The last row provides the  agreement of the annotator in that column against the average from the other annotators.", "labels": [], "entities": [{"text": "WSsim-2", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.8223862648010254}]}, {"text": " Table 11  WSsim annotation: Proportion of sentences where multiple senses received a rating of 5 (highest  judgment) from the same annotator.", "labels": [], "entities": []}, {"text": " Table 12  The relative frequency of the annotations at each judgment from all annotators.", "labels": [], "entities": []}, {"text": " Table 16  Correlation matrix for pairwise correlation agreement for Usim-1. The last row provides the  agreement of the annotator in that column against the average from the other annotators.", "labels": [], "entities": []}, {"text": " Table 18  Average range and average variance of judgments for each of the graded experiments.", "labels": [], "entities": [{"text": "average variance of", "start_pos": 29, "end_pos": 48, "type": "METRIC", "confidence": 0.7795530557632446}]}, {"text": " Table 19  Triangle inequality analysis by annotator, Usim-1.", "labels": [], "entities": [{"text": "Triangle inequality analysis", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.7262915372848511}]}, {"text": " Table 20  Triangle inequality analysis by annotator, Usim-2.", "labels": [], "entities": [{"text": "Triangle inequality analysis", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.711580773194631}]}, {"text": " Table 22  Inter-annotator agreement without one individual for WSbest and SYNbest R2.", "labels": [], "entities": [{"text": "WSbest", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.8430781364440918}]}, {"text": " Table 23  Spearman's correlation between lexical paraphrase overlap on the one hand, and Usim  similarity or WSsim dissimilarity on the other hand.", "labels": [], "entities": []}, {"text": " Table 24  WordNet 2.1 senses of the noun account, and their groups in OntoNotes (ON) and EAW.", "labels": [], "entities": [{"text": "EAW", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.5209996700286865}]}, {"text": " Table 26  Sentences that have positive judgments for senses in different coarse groupings: percentage, and  absolute number in parentheses. J. = WSsim judgment, averaged over annotators.", "labels": [], "entities": [{"text": "absolute number", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.9339272081851959}, {"text": "J.", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9522254467010498}, {"text": "WSsim judgment", "start_pos": 146, "end_pos": 160, "type": "METRIC", "confidence": 0.8975023627281189}]}, {"text": " Table 28  WSsim ratings for two senses of the noun account for 10 annotated sentences (averaged over  annotators).", "labels": [], "entities": []}, {"text": " Table 29  Average Usim rating for R2 where WSbest annotations suggested the same or different coarse  grouping.", "labels": [], "entities": [{"text": "Average Usim rating", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.8978398442268372}]}]}