{"title": [{"text": "Parsing Models for Identifying Multiword Expressions", "labels": [], "entities": [{"text": "Identifying Multiword Expressions", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.8911941448847452}]}], "abstractContent": [{"text": "Multiword expressions lie at the syntax/semantics interface and have motivated alternative theories of syntax like Construction Grammar.", "labels": [], "entities": [{"text": "Construction Grammar", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.8403991758823395}]}, {"text": "Until now, however, syntactic analysis and multiword expression identification have been modeled separately in natural language processing.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7668084800243378}, {"text": "multiword expression identification", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.7969438433647156}]}, {"text": "We develop two structured prediction models for joint parsing and multiword expression identification.", "labels": [], "entities": [{"text": "joint parsing", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.6352870017290115}, {"text": "multiword expression identification", "start_pos": 66, "end_pos": 101, "type": "TASK", "confidence": 0.72878364721934}]}, {"text": "The first is based on context-free grammars and the second uses tree substitution grammars, a formalism that can store larger syntactic fragments.", "labels": [], "entities": []}, {"text": "Our experiments show that both models can identify multiword expressions with much higher accuracy than a state-of-the-art system based on word co-occurrence statistics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9963891506195068}]}, {"text": "We experiment with Arabic and French, which both have pervasive multiword expressions.", "labels": [], "entities": []}, {"text": "Relative to English, they also have richer morphology, which induces lexical sparsity infinite corpora.", "labels": [], "entities": []}, {"text": "To combat this sparsity, we develop a simple factored lexical representation for the context-free parsing model.", "labels": [], "entities": []}, {"text": "Morphological analyses are automatically transformed into rich feature tags that are scored jointly with lexical items.", "labels": [], "entities": []}, {"text": "This technique, which we calla factored lexicon, improves both standard parsing and multiword expression identification accuracy.", "labels": [], "entities": [{"text": "multiword expression identification", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.7222563425699869}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.864091157913208}]}], "introductionContent": [{"text": "Multiword expressions are groups of words which, taken together, can have unpredictable semantics.", "labels": [], "entities": []}, {"text": "For example, the expression part of speech refers not to some aspect of speaking, but to the syntactic category of a word.", "labels": [], "entities": []}, {"text": "If the expression is altered in some ways-part of speeches, part of speaking, type of speech-then the idiomatic meaning is lost.", "labels": [], "entities": []}, {"text": "Other modifications, however, are permitted, as in the plural parts of speech.", "labels": [], "entities": []}, {"text": "These characteristics make multiword expressions (MWEs) difficult to identify and classify.", "labels": [], "entities": [{"text": "multiword expressions (MWEs)", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6721229612827301}]}, {"text": "But if they can be identified, then the incorporation of MWE knowledge has been shown to improve task accuracy fora range of NLP applications including dependency parsing, supertagging), sentence generation (), machine translation, and shallow parsing (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9813025593757629}, {"text": "dependency parsing", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.7731643617153168}, {"text": "sentence generation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.8100806176662445}, {"text": "machine translation", "start_pos": 211, "end_pos": 230, "type": "TASK", "confidence": 0.8509239554405212}, {"text": "shallow parsing", "start_pos": 236, "end_pos": 251, "type": "TASK", "confidence": 0.7016460597515106}]}, {"text": "The standard approach to MWE identification is n-gram classification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9876112341880798}, {"text": "n-gram classification", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7290667593479156}]}, {"text": "Given a corpus, all n-grams are extracted, filtered using heuristics, and assigned feature vectors.", "labels": [], "entities": []}, {"text": "Each coordinate in the feature vector is a real-valued quantity such as log likelihood or pointwise mutual information.", "labels": [], "entities": []}, {"text": "A binary classifier is then trained to render a MWE/non-MWE decision.", "labels": [], "entities": [{"text": "MWE", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8794144988059998}]}, {"text": "All entries into the 2008 MWE Shared Task) utilized variants of this technique.", "labels": [], "entities": [{"text": "2008 MWE Shared Task)", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.545495456457138}]}, {"text": "Broadly speaking, n-gram classification methods measure word co-occurrence.", "labels": [], "entities": [{"text": "n-gram classification", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7230583131313324}]}, {"text": "Suppose that a corpus contains more occurrences of part of speech than parts of speech.", "labels": [], "entities": []}, {"text": "Surface statistics may erroneously predict that only the former is an MWE and the latter is not.", "labels": [], "entities": []}, {"text": "More worrisome is that the statistics for the two n-grams are separate, thus missing an obvious generalization.", "labels": [], "entities": []}, {"text": "In this article, we show that statistical parsing models generalize more effectively over arbitrary-length multiword expressions.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7191681563854218}]}, {"text": "This approach has not been previously demonstrated.", "labels": [], "entities": []}, {"text": "To show its effectiveness, we build two parsing models for MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.974921852350235}]}, {"text": "The first model is based on a context-free grammar (CFG) with manual rule refinements (.", "labels": [], "entities": []}, {"text": "This parser also includes a novel lexical model-the factored lexicon-that incorporates morphological features.", "labels": [], "entities": []}, {"text": "The second model is based on tree substitution grammar (TSG), a formalism with greater strong generative capacity that can store larger structural tree fragments, some of which are lexicalized.", "labels": [], "entities": [{"text": "tree substitution grammar (TSG)", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.8516357243061066}]}, {"text": "We apply the models to Modern Standard Arabic (henceforth MSA, or simply \"Arabic\") and French, two morphologically rich languages (MRLs).", "labels": [], "entities": []}, {"text": "The lexical sparsity (in finite corpora) induced by rich morphology poses a particular challenge for n-gram classification.", "labels": [], "entities": [{"text": "n-gram classification", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.693093553185463}]}, {"text": "Relative to English, French has a richer array of morphological featuressuch as grammatical gender and verbal conjugation for aspect and voice.", "labels": [], "entities": []}, {"text": "Arabic also has richer morphology including gender and dual number.", "labels": [], "entities": []}, {"text": "It has pervasive verbinitial matrix clauses, although preposed subjects are also possible.", "labels": [], "entities": []}, {"text": "For languages like these it is well known that constituency parsing models designed for English often do not generalize well.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8620320558547974}]}, {"text": "Therefore, we focus on the interplay among language, annotation choices, and parsing model design for each language (, inter alia), although our methods are ultimately very general.", "labels": [], "entities": [{"text": "parsing model design", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.8205633362134298}]}, {"text": "Our modeling strategy for MWEs is simple: We mark them with flat bracketings in phrase structure trees.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9553443193435669}]}, {"text": "This representation implicitly assumes a locality constraint on idioms, an assumption with a precedent in linguistics.", "labels": [], "entities": []}, {"text": "Of course, it is easy to find non-local idioms that do not correspond to surface constituents or even contiguous strings.", "labels": [], "entities": []}, {"text": "Utterances such as All hell seemed to break loose and The cat got Mary's tongue are clearly idiomatic, yet the idiomatic elements are discontiguous.", "labels": [], "entities": []}, {"text": "Our models cannot identify these MWEs, but then again, neither can n-gram classification.", "labels": [], "entities": [{"text": "n-gram classification", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.7347528040409088}]}, {"text": "Nonetheless, many common MWE types like nominal compounds are contiguous and often correspond to constituent boundaries.", "labels": [], "entities": []}, {"text": "Consider again the phrasal compound part of speech, 1 which is non-compositional: The idiomatic meaning \"syntactic category\" does not derive from any of the component words.", "labels": [], "entities": []}, {"text": "This non-compositionality affects the syntactic environment of the compound as shown by the addition of an attributive adjective: (1) a.", "labels": [], "entities": []}, {"text": "Noun is apart of speech. b. *Noun is a big part of speech. c. *Noun is a big part.", "labels": [], "entities": []}, {"text": "Liquidity is apart of growth. b. Liquidity is a big part of growth. c. Liquidity is a big part.", "labels": [], "entities": []}, {"text": "In Example (1a) the copula predicate part of speech as a whole describes Noun.", "labels": [], "entities": []}, {"text": "In Examples (1b) and (1c) big clearly modifies only part and the idiomatic meaning is lost.", "labels": [], "entities": []}, {"text": "The attributive adjective cannot probe arbitrarily into the non-compositional compound.", "labels": [], "entities": []}, {"text": "In contrast, Example (2) contains parallel data without idiomatic semantics.", "labels": [], "entities": []}, {"text": "The conventional syntactic analysis of Example (2a) is identical to that of Example (1a) except for the lexical items, yet part of growth is not idiomatic.", "labels": [], "entities": []}, {"text": "Consequently, many premodifiers are appropriate for part, which is semantically vacuous.", "labels": [], "entities": []}, {"text": "In Example (2b), big clearly modifies part, and of growth is just an optional PP complement, as shown by Example (2c), which is still grammatical.", "labels": [], "entities": []}, {"text": "This article proposes different phrase structures for examples such as (1a) and (2a).", "labels": [], "entities": []}, {"text": "shows a Penn Treebank (PTB) parse of Example (1a), and shows the parse of a paraphrase.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.9638828039169312}, {"text": "Example", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.8555739521980286}]}, {"text": "The phrasal compound part of speech functions syntactically like a single-word nominal like category, and indeed Noun is a big category is grammatical.", "labels": [], "entities": []}, {"text": "Single-word paraphrasability is a common, though not mandatory, characteristic of MWEs (.", "labels": [], "entities": []}, {"text": "Starting from the paraphrase parse, we create a representation like.", "labels": [], "entities": [{"text": "paraphrase parse", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.8013424873352051}]}, {"text": "The MWE is indicated by a label in the predicted structure, which is flat.", "labels": [], "entities": []}, {"text": "This representation explicitly models the idiomatic semantics of the compound and is context-free, so we can build efficient parsers for it.", "labels": [], "entities": []}, {"text": "Crucially, MWE identification becomes a by-product of parsing as we can trivially extract MWE spans from full parses.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9857662916183472}]}, {"text": "We convert existing Arabic and French syntactic treebanks to the new MWE representation.", "labels": [], "entities": []}, {"text": "With this representation, the TSG model yields the best MWE identification results for Arabic (81.9% F1) and competitive results for French (71.3%), even though its parsing results lag state-of-the-art probabilistic CFG (PCFG)-based parsers.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9586721956729889}, {"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9875533580780029}]}, {"text": "The TSG model also learns human-interpretable MWE rules.", "labels": [], "entities": [{"text": "MWE rules", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.8331098258495331}]}, {"text": "The factored lexicon model with gold morphological annotations achieves the best MWE results for French (87.3% F1) and competitive results for Arabic (78.2% F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9822524785995483}, {"text": "F1", "start_pos": 157, "end_pos": 159, "type": "METRIC", "confidence": 0.9864842295646667}]}, {"text": "For both languages the factored lexicon model also approaches state-of-the-art basic parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.829768717288971}]}, {"text": "The remainder of this article begins with linguistic background on common MWE types in Arabic and French (Section 2).", "labels": [], "entities": []}, {"text": "We then describe two constituency parsing models that are tuned for MWE identification (Sections 3 and 4).", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8477400541305542}, {"text": "MWE identification", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9795658588409424}]}, {"text": "These models are supervised and can be trained on existing linguistic resources (Section 5).", "labels": [], "entities": []}, {"text": "We evaluate the models for both basic parsing and MWE identification (Section 6).", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.7383859753608704}, {"text": "MWE identification", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9766462743282318}]}, {"text": "Finally, we compare our results with a state-of-the-art n-gram classification system (Section 7) and to prior work (Section 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "For each language, we ran two experiments: standard parsing and MWE identification.", "labels": [], "entities": [{"text": "standard parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.5169357657432556}, {"text": "MWE identification", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9481735527515411}]}, {"text": "The evaluation included the Stanford, Stanford+factored lexicon, and DP-TSG models.", "labels": [], "entities": [{"text": "Stanford", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9250847101211548}]}, {"text": "All experiments used gold tokenization/segmentation.", "labels": [], "entities": []}, {"text": "Unlike the ATB, the FTB does not contain the raw source documents, so we could not start from raw text for both languages.", "labels": [], "entities": [{"text": "ATB", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.928167998790741}, {"text": "FTB", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.9483392834663391}]}, {"text": "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.9694594740867615}, {"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.6534886956214905}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9684939980506897}, {"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9995949864387512}]}, {"text": "Morphological analysis accuracy was another experimental resource asymmetry between the two languages.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.817523181438446}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9480432271957397}]}, {"text": "The morphological analyses were obtained with significantly different tools: in Arabic, we had a morphological generator/ranker (MADA), whereas for French we had only a discriminative classifier (Morfette).", "labels": [], "entities": []}, {"text": "Consequently, French analysis quality was lower (Section 5.3).", "labels": [], "entities": [{"text": "French analysis quality", "start_pos": 14, "end_pos": 37, "type": "METRIC", "confidence": 0.6366141438484192}]}, {"text": "We included two parsing baselines: a parent-annotated PCFG (PAPCFG) and a PCFG with the grammar features in the Stanford parser (SplitPCFG).", "labels": [], "entities": []}, {"text": "The PAPCFG is the standard baseline for TSG models.", "labels": [], "entities": [{"text": "PAPCFG", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.5649631023406982}]}, {"text": "We previously showed optimal Berkeley parser () parameterizations for both the Arabic (Green and Manning 2010) and French () data sets.", "labels": [], "entities": [{"text": "Arabic (Green and Manning 2010) and French () data sets", "start_pos": 79, "end_pos": 134, "type": "DATASET", "confidence": 0.6652845963835716}]}, {"text": "For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline.", "labels": [], "entities": [{"text": "Berkeley ATB baseline", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.8536204894383749}]}, {"text": "Others had used the Berkeley parser for French, but on an older revision of the FTB.", "labels": [], "entities": [{"text": "FTB", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.990026593208313}]}, {"text": "To our knowledge, we are the first to use the Berkeley parser for MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9279148280620575}]}, {"text": "We selected features for the factored lexicon on the development sets.", "labels": [], "entities": []}, {"text": "For Arabic, we used gender, number, tense, mood, and definiteness.", "labels": [], "entities": []}, {"text": "For French, we used the grammatical and syntactic features in the CC tag set in addition to grammatical number.", "labels": [], "entities": []}, {"text": "For the experiments in which we evaluated with predicted morphological analyses, we also trained the parser on predicted analyses.", "labels": [], "entities": []}, {"text": "We report three evaluation metrics.", "labels": [], "entities": []}, {"text": "Evalb is the standard labeled precision/recall metric.", "labels": [], "entities": [{"text": "Evalb", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5342181921005249}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9925439357757568}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9289810657501221}]}, {"text": "20 Leaf Ancestor measures the cost of transforming guess trees to the reference (, and is less biased against flat treebanks like the FTB (.", "labels": [], "entities": [{"text": "FTB", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.9265488982200623}]}, {"text": "The Leaf Ancestor score ranges from 0 to 1 (higher is better).", "labels": [], "entities": [{"text": "Ancestor score", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.8809404373168945}]}, {"text": "We report micro-averaged (Corpus) and macro-averaged (Sent.) scores.", "labels": [], "entities": []}, {"text": "Finally, EX% is the percentage of perfectly parsed sentences according to Evalb.", "labels": [], "entities": [{"text": "EX", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9984803795814514}, {"text": "Evalb", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9480896592140198}]}, {"text": "We report results for sentences of lengths \u2264 40 words.", "labels": [], "entities": []}, {"text": "This cutoff accounts for similar proportions of the ATB and FTB.", "labels": [], "entities": [{"text": "ATB", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8371139168739319}, {"text": "FTB", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9106490612030029}]}, {"text": "The DP-TSG grammar extractor produces very large grammars for Arabic, and we found that the grammar constant was too large for parsing all sentences.", "labels": [], "entities": [{"text": "DP-TSG grammar extractor", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6578895648320516}]}, {"text": "For example, the ATB development set contains a sentence that is 268 tokens long.", "labels": [], "entities": [{"text": "ATB development set", "start_pos": 17, "end_pos": 36, "type": "DATASET", "confidence": 0.9562208453814188}]}, {"text": "show Arabic and French parsing results, respectively.", "labels": [], "entities": [{"text": "French parsing", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.4530983716249466}]}, {"text": "For both languages, the Berkeley parser produces the best results in terms of Evalb F1.", "labels": [], "entities": [{"text": "Evalb F1", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.6605612635612488}]}, {"text": "The gold factored lexicon setting compares favorably in terms of exact match.", "labels": [], "entities": [{"text": "exact match", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9215627610683441}]}, {"text": "The predominant approach to MWE identification is the combination of lexical association measures (surface statistics) with a binary classifier.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9895715713500977}]}, {"text": "A state-of-the-art, language-independent package that implements this approach for higher order n-grams is mwetoolkit (Ramisch, Villavicencio, and Boitet 2010).", "labels": [], "entities": []}, {"text": "We configured mwetoolkit with the four standard lexical features: the maximum likelihood estimator, Dice's coefficient, pointwise mutual information, and Student's t-score.", "labels": [], "entities": [{"text": "maximum likelihood estimator", "start_pos": 70, "end_pos": 98, "type": "METRIC", "confidence": 0.6974539756774902}, {"text": "Dice's coefficient", "start_pos": 100, "end_pos": 118, "type": "METRIC", "confidence": 0.6765766243139902}]}, {"text": "We also included POS tags predicted by the Stanford tagger ().", "labels": [], "entities": []}, {"text": "We filtered the training instances by removing unigrams and  non-MWE n-grams that occurred only once.", "labels": [], "entities": []}, {"text": "For each resulting n-gram, we created realvalued feature vectors and trained a binary SVM classifier with Weka () with an RBF kernel.", "labels": [], "entities": []}, {"text": "See Appendix D for further configuration details.", "labels": [], "entities": []}, {"text": "Because our parsers mark MWEs as labeled spans, MWE identification is a byproduct of parsing.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9061056971549988}]}, {"text": "Our evaluation metric is category-level Evalb for the MWE nonterminal categories.", "labels": [], "entities": [{"text": "Evalb", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.771844744682312}, {"text": "MWE nonterminal categories", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.5419596433639526}]}, {"text": "We report both the per-category scores, and a weighted average for all categories.", "labels": [], "entities": []}, {"text": "shows aggregate MWE identification results.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9492236077785492}]}, {"text": "All parsing models-even the baselines-exceed mwetoolkit by a wide margin.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9698842167854309}]}, {"text": "Arabic Statistical Constituency Parsing. were the first to parse the sections of the ATB used in this article.", "labels": [], "entities": [{"text": "Arabic Statistical Constituency Parsing.", "start_pos": 0, "end_pos": 40, "type": "DATASET", "confidence": 0.7875246852636337}, {"text": "ATB", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.5721706748008728}]}, {"text": "They adapted the Bikel parser) and improved accuracy primarily through punctuation equivalence classing and the Kulick tag set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9987446069717407}, {"text": "Kulick tag set", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.823668897151947}]}, {"text": "The ATB was subsequently revised produced the first results on the revision for our split of the revised corpus.", "labels": [], "entities": [{"text": "The ATB", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6859617829322815}]}, {"text": "They only reported development set results with gold POS tags, however.", "labels": [], "entities": []}, {"text": "Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline.", "labels": [], "entities": [{"text": "ATB", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9507231712341309}]}, {"text": "We also adapted the Stanford parser to the ATB, and provided the first results for non-gold tokenization.", "labels": [], "entities": [{"text": "ATB", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9508095979690552}]}, {"text": "developed an Arabic unknown word model for the Berkeley parser based on signatures, much like those in French Statistical Constituency Parsing. and identified the linguistic and computational attractiveness of lexicalized grammars for modeling non-compositional constructions in French well before DOP.", "labels": [], "entities": []}, {"text": "They developed a small tree adjoining grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs.", "labels": [], "entities": []}, {"text": "Recent statistical parsing work on French has included stochastic tree insertion grammars (STIG), which are related to TAGs, but with a restricted adjunction operation. and showed that STIGs underperform CFG-based parsers on the FTB.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.6077029407024384}, {"text": "stochastic tree insertion grammars", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.7437776923179626}, {"text": "FTB", "start_pos": 229, "end_pos": 232, "type": "DATASET", "confidence": 0.9833236336708069}]}, {"text": "In their experiments, MWEs were grouped.", "labels": [], "entities": []}, {"text": "Appendix B describes additional prior work on CFG-based FTB parsing.", "labels": [], "entities": [{"text": "CFG-based FTB parsing", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.6485383709271749}]}, {"text": "Statistical French MWE identification has only been investigated recently.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7078693211078644}]}, {"text": "We previously reported the first results on the FTB using a parser for MWE identification ().", "labels": [], "entities": [{"text": "FTB", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.8802616000175476}, {"text": "MWE identification", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8319209814071655}]}, {"text": "Contemporaneously, applied n-gram methods to a French corpus of multiword adverbs.", "labels": [], "entities": [{"text": "French corpus of multiword adverbs", "start_pos": 47, "end_pos": 81, "type": "DATASET", "confidence": 0.9034046649932861}]}, {"text": "used a linear chain conditional random fields model (CRF) for joint POS tagging and MWE identification.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.814330130815506}, {"text": "MWE identification", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9648453891277313}]}, {"text": "They incorporated external linguistic resources as features, but reported results fora much older version of the FTB.", "labels": [], "entities": [{"text": "FTB", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9794368147850037}]}, {"text": "Subsequently, Constant, Sigogne, and Watrin (2012) integrated the CRF model into the Berkeley parser and evaluated on the pre-processed FTB used in this article.", "labels": [], "entities": [{"text": "FTB", "start_pos": 136, "end_pos": 139, "type": "DATASET", "confidence": 0.7555703520774841}]}, {"text": "Their best model (with external lexicon features) achieved 77.8% F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9999114274978638}]}], "tableCaptions": [{"text": " Table 2  French grammar development. Incremental effects on grammar size and labeled F1 for each of  the manual grammar features (development set, sentences \u2264 40 words). The baseline is a  parent-annotated grammar. The features tradeoff between maximizing two objectives: overall  parsing F1 and MWE F1.", "labels": [], "entities": [{"text": "French grammar development", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.713448961575826}, {"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9772403836250305}, {"text": "F1", "start_pos": 290, "end_pos": 292, "type": "METRIC", "confidence": 0.8057087659835815}, {"text": "MWE F1", "start_pos": 297, "end_pos": 303, "type": "METRIC", "confidence": 0.7950182259082794}]}, {"text": " Table 5  Gross corpus statistics for the pre-processed corpora used to train and evaluate our models. We  compare to the WSJ section of the PTB: train (Sections 02-21); dev. (Section 22); test (Section 23).  Due to its flat annotation style, the FTB sentences have fewer constituents per sentence. In the  ATB, morphological variation accounts for the high proportion of word types to sentences.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.7080250382423401}, {"text": "PTB", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.6598151326179504}, {"text": "FTB", "start_pos": 247, "end_pos": 250, "type": "DATASET", "confidence": 0.8233231902122498}, {"text": "ATB", "start_pos": 307, "end_pos": 310, "type": "DATASET", "confidence": 0.8698906898498535}]}, {"text": " Table 6  Frequency distribution of the MWE types in the ATB and FTB training sets.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9636180400848389}, {"text": "ATB", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.9004273414611816}, {"text": "FTB training sets", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.8753183285395304}]}, {"text": " Table 8  Arabic standard parsing experiments (test set, sentences \u2264 40 words). SplitPCFG is the same  grammar used in the Stanford parser, but without the dependency model. FactLex uses basic  POS tags predicted by the parser and morphological analyses from MADA. FactLex* uses gold  morphological analyses. Berkeley and DP-TSG results are the average of three independent runs.", "labels": [], "entities": []}, {"text": " Table 9  French standard parsing experiments (test set, sentences \u2264 40 words). FactLex uses basic POS  tags predicted by the parser and morphological analyses from Morfette. FactLex* uses gold  morphological analyses.", "labels": [], "entities": [{"text": "FactLex", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.8653637766838074}]}, {"text": " Table 10  Arabic MWE identification per category and overall results (test set, sentences \u2264 40 words).", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7503933310508728}]}, {"text": " Table 11  French MWE identification per category and overall results (test set, sentences \u2264 40 words).  MWI and MWCL do not occur in the test set.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6825084686279297}]}, {"text": " Table 12  MWE identification F1 of the parsing models vs. the mwetoolkit baseline (test set, sentences  \u2264 40 words). FactLex  *  uses gold morphological analyses at test time.", "labels": [], "entities": []}]}