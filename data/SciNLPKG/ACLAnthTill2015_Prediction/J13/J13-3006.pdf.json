{"title": [{"text": "Selectional Preferences for Semantic Role Classification", "labels": [], "entities": [{"text": "Selectional Preferences", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9133884608745575}, {"text": "Semantic Role Classification", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7925580342610677}]}], "abstractContent": [{"text": "This paper focuses on a well-known open issue in Semantic Role Classification (SRC) research: the limited influence and sparseness of lexical features.", "labels": [], "entities": [{"text": "Semantic Role Classification (SRC) research", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.856038783277784}]}, {"text": "We mitigate this problem using models that integrate automatically learned selectional preferences (SP).", "labels": [], "entities": []}, {"text": "We explore a range of models based on WordNet and distributional-similarity SPs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9572569131851196}]}, {"text": "Furthermore, we demonstrate that the SRC task is better modeled by SP models centered on both verbs and prepositions, rather than verbs alone.", "labels": [], "entities": [{"text": "SRC task", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.9283953607082367}]}, {"text": "Our experiments with SP-based models in isolation indicate that they outperform a lexical baseline with 20 F 1 points in domain and almost 40 F 1 points out of domain.", "labels": [], "entities": []}, {"text": "Furthermore, we show that a state-of-the-art SRC system extended with features based on selectional preferences performs significantly better, both in domain (17% error reduction) and out of domain (13% error reduction).", "labels": [], "entities": [{"text": "SRC", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.984010636806488}]}, {"text": "Finally, we show that in an end-to-end semantic role labeling system we obtain small but statistically significant improvements, even though our modified SRC model affects only approximately 4% of the argument candidates.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6773073871930441}]}, {"text": "Our post hoc error analysis indicates that the SP-based features help mostly in situations where syntactic information is either incorrect or insufficient to disambiguate the correct role.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Role Labeling (SRL) is the problem of analyzing clause predicates in text by identifying arguments and tagging them with semantic labels indicating the role they play with respect to the predicate.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8327052195866903}]}, {"text": "Such sentence-level semantic analysis allows the determination of who did what to whom, when and where, and thus characterizes the participants and properties of the events established by the predicates.", "labels": [], "entities": []}, {"text": "For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.", "labels": [], "entities": []}, {"text": "(1) Recognizing these event structures has been shown to be important fora broad spectrum of NLP applications.", "labels": [], "entities": []}, {"text": "Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; the reader can find abroad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8176627159118652}, {"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9775441288948059}, {"text": "question answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.837665855884552}, {"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7565426826477051}, {"text": "SRL", "start_pos": 291, "end_pos": 294, "type": "TASK", "confidence": 0.9356624484062195}]}, {"text": "State-of-the-art systems leverage existing hand-tagged corpora to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification.", "labels": [], "entities": [{"text": "SRL", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.9627482295036316}, {"text": "argument identification", "start_pos": 160, "end_pos": 183, "type": "TASK", "confidence": 0.7159420996904373}, {"text": "argument classification", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.7114448994398117}]}, {"text": "Whereas the former is mostly a syntactic recognition task, the latter usually requires semantic knowledge to betaken into account.", "labels": [], "entities": [{"text": "syntactic recognition task", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.8216149608294169}]}, {"text": "The semantic knowledge that most current systems capture from text is basically limited to the predicates and the lexical units contained in their arguments, including the argument head.", "labels": [], "entities": []}, {"text": "These \"lexical features\" tend to be sparse, especially when the training corpus is small, and thus SRL systems are prone to overfit the training data and generalize poorly to new corpora.", "labels": [], "entities": [{"text": "SRL", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9585286974906921}]}, {"text": "As a simplified example of the effect of sparsity, consider the following sentences occurring in an imaginary training data set for SRL: All four sentences share the same syntactic structure, so the lexical features (i.e., the words Dallas, New York, November, and winter) represent the most relevant knowledge for discriminating between the Location and Temporal adjunct labels in learning.", "labels": [], "entities": [{"text": "SRL", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9493013620376587}]}, {"text": "The problem is that, as in the following sentences, for the same predicate, one may encounter similar expressions with new words like Texas or December, which the classifiers cannot match with the lexical features seen during training, and thus become useless for classification: (6) was assassinated [in Texas] (7) was assassinated This problem is exacerbated when SRL systems are applied to texts coming from new domains where the number of new predicates and argument heads increases considerably.", "labels": [], "entities": [{"text": "Texas", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.928075909614563}, {"text": "SRL", "start_pos": 366, "end_pos": 369, "type": "TASK", "confidence": 0.9710143208503723}]}, {"text": "The CoNLL-2004 and 2005 evaluation exercises on semantic role labeling) reported a significant performance degradation of around 10 F 1 points when applied to out-of-domain texts from the Brown corpus.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6982500553131104}, {"text": "F 1", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9706195592880249}, {"text": "Brown corpus", "start_pos": 188, "end_pos": 200, "type": "DATASET", "confidence": 0.9168556928634644}]}, {"text": "Pradhan, showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons.", "labels": [], "entities": []}, {"text": "In this work, we will focus on Semantic Role Classification (SRC), and we will show that selectional preferences (SP) are useful for generalizing lexical features, helping fight sparseness and domain shifts, and improving SRC results.", "labels": [], "entities": [{"text": "Semantic Role Classification (SRC)", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.8116768002510071}, {"text": "SRC", "start_pos": 222, "end_pos": 225, "type": "TASK", "confidence": 0.9901184439659119}]}, {"text": "Selectional preferences try to model the kind of words that can fill a specific argument of a predicate, and have been widely used in computational linguistics since the early days.", "labels": [], "entities": []}, {"text": "Both semantic classes from existing lexical resources like WordNet) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.946121335029602}]}, {"text": "The contributions of this work to the field of SRL are the following: 1.", "labels": [], "entities": [{"text": "SRL", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9423367381095886}]}, {"text": "We formalize and implement a method that applies several selectional preference models to Semantic Role Classification, introducing for the first time the use of selectional preferences for prepositions, in addition to selectional preferences for verbs.", "labels": [], "entities": [{"text": "Semantic Role Classification", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.8722705046335856}]}], "datasetContent": [{"text": "In this section we evaluate the ability of selectional preference models to discriminate among different roles.", "labels": [], "entities": []}, {"text": "For that, SP models will be used in isolation, according to the classification rule in Equation (11), to predict role labels fora set of (predicate, argument-head) pairs.", "labels": [], "entities": []}, {"text": "That is, we are interested in the discriminative power of the semantic information carried by the SPs, factoring out any other feature commonly used by the state-of-theart SRL systems.", "labels": [], "entities": []}, {"text": "The data sets used and the experimental results are presented in the following.", "labels": [], "entities": []}, {"text": "In this section we advance the use of SP in SRL one step further and show that selectional preferences are able to effectively improve performance of a state-of-the-art SRL system.", "labels": [], "entities": [{"text": "SRL", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9326584339141846}, {"text": "SRL", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.9761022329330444}]}, {"text": "More concretely, we integrate the information of selectional preference models in a SRL system and show significant improvements in role classification, especially when applied to out-of-domain corpora.", "labels": [], "entities": [{"text": "SRL", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9287775158882141}, {"text": "role classification", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.8453418910503387}]}, {"text": "We will use some of the selectional preference models presented in the previous section.", "labels": [], "entities": []}, {"text": "We will focus on the combination of verb-role and prep-role models.", "labels": [], "entities": []}, {"text": "Regarding the similarity models, we will choose the best two performing models from each of the three families that we tried, namely, the two WordNet models, the two best models based on the BNC corpus (sim Jac ,sim cos ), and the two best models based on Lin's precomputed similarity metrics (sim 2 Jac ,sim 2 cos ).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 142, "end_pos": 149, "type": "DATASET", "confidence": 0.9394025206565857}, {"text": "BNC corpus", "start_pos": 191, "end_pos": 201, "type": "DATASET", "confidence": 0.9592158198356628}]}, {"text": "We left the exploration of other combinations for future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Example of verb-role lexical SP models for write, listed in alphabetical order. Number of heads  indicates the number of head words attested, Unique heads indicates the number of distinct  head words attested, and Examples lists some of the heads in alphabetical order.", "labels": [], "entities": []}, {"text": " Table 2  Excerpt from the selectional preferences for write-Arg0 according to SP Res , showing the synsets  that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets  by Equation (1). Description includes the words and glosses in the synset.", "labels": [], "entities": []}, {"text": " Table 3  Excerpt from the selectional preferences for write-Arg1 according to SP Res , showing the synsets  that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets  by Equation (1). Description includes the words and glosses in the synset.", "labels": [], "entities": []}, {"text": " Table 4  Excerpt from the selectional preferences for write-Arg0 according to SP wn , showing from deeper  to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists  the depth of synsets in WordNet. Description includes the words and glosses in the synset.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9670892953872681}, {"text": "WordNet", "start_pos": 227, "end_pos": 234, "type": "DATASET", "confidence": 0.9496222734451294}]}, {"text": " Table 5  Excerpt from the selectional preferences for write-Arg1 according to SP wn , showing from deeper  to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists  the depth of synsets in WordNet. Description includes the words and glosses in the synset.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9680795669555664}, {"text": "WordNet", "start_pos": 227, "end_pos": 234, "type": "DATASET", "confidence": 0.950417160987854}]}, {"text": " Table 8  Example of prep-role lexical models for the preposition from, listed in alphabetical order.", "labels": [], "entities": []}, {"text": " Table 9  Statistics of the three most and least frequent verbs in the training set. Role frame lists the types  of arguments seen in training for each verb; Heads indicates the total number of arguments for  the verb; Heads per role shows the average number of head words for each role; and Unique  heads per role lists the average number of unique head words for each verb's role.", "labels": [], "entities": []}, {"text": " Table 10  Statistics of the three most and least frequent prepositions in the training set. Role frame lists  the types of arguments seen in training for each preposition; Heads indicates the total number  of arguments for the preposition; Heads per role shows the average number of head words for  each role; and Unique heads per role lists the average number of unique head words for each  preposition's role.", "labels": [], "entities": []}, {"text": " Table 11  Results for verb-role SPs in the development partition of WSJ, the test partition of WSJ, and the  Brown corpus. For each experiment, we show precision (P), recall (R), and F 1 . Values in boldface  font are the highest in the corresponding column. F 1 values marked with  \u2020 are significantly  lower than the highest F 1 score in the same column.", "labels": [], "entities": [{"text": "verb-role SPs", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.6799057424068451}, {"text": "WSJ", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9645668268203735}, {"text": "WSJ", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9659206867218018}, {"text": "Brown corpus", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.9014417231082916}, {"text": "precision (P)", "start_pos": 153, "end_pos": 166, "type": "METRIC", "confidence": 0.9437759965658188}, {"text": "recall (R)", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9511425793170929}, {"text": "F 1", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9949022531509399}, {"text": "F 1", "start_pos": 260, "end_pos": 263, "type": "METRIC", "confidence": 0.945225864648819}, {"text": "F 1 score", "start_pos": 328, "end_pos": 337, "type": "METRIC", "confidence": 0.9333227475484213}]}, {"text": " Table 12  Results for combined verb-role and prep-role SPs in the development partition of WSJ, the test  partition of WSJ, and the Brown corpus. For each experiment, we show precision (P), recall (R),  and F 1 . Values in boldface font are the highest in the corresponding column. F 1 values marked  with  \u2020 are significantly lower from the highest F 1 score in the same column.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9631828665733337}, {"text": "WSJ", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.9777936339378357}, {"text": "Brown corpus", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.8937950134277344}, {"text": "precision (P)", "start_pos": 176, "end_pos": 189, "type": "METRIC", "confidence": 0.9390845596790314}, {"text": "recall (R)", "start_pos": 191, "end_pos": 201, "type": "METRIC", "confidence": 0.951371505856514}, {"text": "F 1", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.9948296844959259}, {"text": "F 1", "start_pos": 283, "end_pos": 286, "type": "METRIC", "confidence": 0.9546014368534088}, {"text": "F 1 score", "start_pos": 351, "end_pos": 360, "type": "METRIC", "confidence": 0.9079559842745463}]}, {"text": " Table 13  Results for the combination approaches. Accuracy shows the overall results. Core and Adj  contain F 1 results restricted to the core numbered roles and adjuncts, respectively. SRC is  SwiRL's standalone SRC model; +SP x stands for the SRC model extended with a feature given by  the corresponding SP model. Values in boldface font are the highest in the corresponding  column. Accuracy values marked with  \u2020 are significantly lower than the highest accuracy score  in the same column.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.998221218585968}, {"text": "Accuracy", "start_pos": 388, "end_pos": 396, "type": "METRIC", "confidence": 0.998256504535675}, {"text": "accuracy score", "start_pos": 460, "end_pos": 474, "type": "METRIC", "confidence": 0.9748040735721588}]}, {"text": " Table 15  Precision (P), recall (R), and F 1 results per argument for the end-to-end semantic role labeling  task. We compared two models: the original SwiRL model and the one where the classification  component was replaced with the meta-classifier introduced at the beginning of the section. We  used the official CoNLL-2005 shared-task scorer to produce these results. We checked for  statistical significance for the overall F 1 scores (All row", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9297029227018356}, {"text": "recall (R)", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.937068447470665}, {"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9938980937004089}, {"text": "semantic role labeling  task", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.7229268997907639}]}]}