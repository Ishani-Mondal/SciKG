{"title": [], "abstractContent": [{"text": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts.", "labels": [], "entities": []}, {"text": "The core part of such a system is the semantic parser that maps questions to logical forms.", "labels": [], "entities": []}, {"text": "Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.", "labels": [], "entities": []}, {"text": "Our goal is to instead learn a semantic parser from question-answer pairs, where the logical form is modeled as a latent variable.", "labels": [], "entities": []}, {"text": "We develop anew semantic formalism, dependency-based compositional semantics (DCS) and define a log-linear distribution over DCS logical forms.", "labels": [], "entities": [{"text": "dependency-based compositional semantics (DCS)", "start_pos": 36, "end_pos": 82, "type": "TASK", "confidence": 0.7283643285433451}]}, {"text": "The model parameters are estimated using a simple procedure that alternates between beam search and numerical optimization.", "labels": [], "entities": [{"text": "beam search", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.8723165988922119}]}, {"text": "On two standard semantic parsing benchmarks, we show that our system obtains comparable accuracies to even state-of-the-art systems that do require annotated logical forms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7207449525594711}]}], "introductionContent": [{"text": "One of the major challenges in natural language processing (NLP) is building systems that both handle complex linguistic phenomena and require minimal human effort.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.8312018613020579}]}, {"text": "The difficulty of achieving both criteria is particularly evident in training semantic parsers, where annotating linguistic expressions with their associated logical forms is expensive but until recently, seemingly unavoidable.", "labels": [], "entities": []}, {"text": "Advances in learning latent-variable models, however, have made it possible to progressively reduce the amount of supervision required for various semantics-related tasks).", "labels": [], "entities": []}, {"text": "In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision.", "labels": [], "entities": []}, {"text": "We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see for an example in the domain of U.S. geography.", "labels": [], "entities": []}, {"text": "This problem of building natural language interfaces to databases (NLIDBs) has along history in NLP, starting from the early days of artificial intelligence with systems such as LUNAR, CHAT-80 (, and many others (see Androutsopoulos, Ritchie, and Thanisch for an overview).", "labels": [], "entities": []}, {"text": "We believe NLIDBs provide an appropriate starting point for semantic parsing because they lead directly to practical systems, and they allow us to temporarily sidestep intractable philosophical questions on how to represent meaning in general.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.811053454875946}]}, {"text": "Early NLIDBs were quite successful in their respective limited domains, but because these systems were constructed from manually built rules, they became difficult to scale up, both to other domains and to more complex utterances.", "labels": [], "entities": []}, {"text": "In response, against the backdrop of a statistical revolution in NLP during the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods.", "labels": [], "entities": []}, {"text": "One of the earliest statistical efforts was the CHILL system (, which learned a shift-reduce semantic parser.", "labels": [], "entities": []}, {"text": "Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (.", "labels": [], "entities": [{"text": "semantic parsers", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7195562422275543}]}, {"text": "Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.8270848393440247}]}, {"text": "One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain.", "labels": [], "entities": []}, {"text": "Furthermore, the annotators must be proficient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope of acquiring enough data to fulfill the vision of learning highly accurate systems.", "labels": [], "entities": []}, {"text": "In response to these concerns, researchers have recently begun to explore the possibility of learning a semantic parser without any annotated logical forms (Clarke et al.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have now completed the conceptual part of this article-using DCS trees to represent logical forms (Section 2), and learning a probabilistic model over these trees (Section 3).", "labels": [], "entities": []}, {"text": "In this section, we evaluate and study our approach empirically.", "labels": [], "entities": []}, {"text": "Our main result is that our system can obtain comparable accuracies to state-of-the-art systems that require annotated logical forms.", "labels": [], "entities": []}, {"text": "All the code and data are available at cs.stanford.edu/ ~ pliang/software/.", "labels": [], "entities": []}, {"text": "We first describe the data sets (Section 4.1.1) that we use to train and evaluate our system.", "labels": [], "entities": []}, {"text": "We then mention various choices in the model and learning algorithm (Section 4.1.2).", "labels": [], "entities": []}, {"text": "One of these choices is the lexical triggers, which are further discussed in Section 4.1.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Results on GEO with 250 training and 250 test examples. Our system (LJK11 with base triggers  and no logical forms) obtains higher test accuracy than CGCR10, even when CGCR10 is trained  using logical forms.", "labels": [], "entities": [{"text": "GEO", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8708451986312866}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9643000364303589}]}, {"text": " Table 3  Results on GEO: Logical form accuracy (LF) and answer accuracy (Answer) of the various  systems. The first group of systems are evaluated using 10-fold cross-validation on all 880  examples; the second are evaluated on the 680 + 200 split of", "labels": [], "entities": [{"text": "GEO", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8236290812492371}, {"text": "Logical form accuracy (LF)", "start_pos": 26, "end_pos": 52, "type": "METRIC", "confidence": 0.728691299756368}, {"text": "answer accuracy (Answer)", "start_pos": 57, "end_pos": 81, "type": "METRIC", "confidence": 0.7621657967567443}, {"text": "680 + 200 split", "start_pos": 233, "end_pos": 248, "type": "DATASET", "confidence": 0.8418589532375336}]}, {"text": " Table 4  Results on JOBS: Both PRECISE and our system use database type constraints, which results in a  decisive advantage over the other systems. In addition, LJK11 incorporates learning and  therefore obtains the highest accuracies.", "labels": [], "entities": [{"text": "JOBS", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.839221179485321}]}]}