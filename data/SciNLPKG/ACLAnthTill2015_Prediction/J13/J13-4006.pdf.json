{"title": [{"text": "Multilingual Joint Parsing of Syntactic and Semantic Dependencies with a Latent Variable Model", "labels": [], "entities": []}], "abstractContent": [{"text": "dMetrics Current investigations in data-driven models of parsing have shifted from purely syntactic analysis to richer semantic representations, showing that the successful recovery of the meaning of text requires structured analyses of both its grammar and its semantics.", "labels": [], "entities": [{"text": "dMetrics", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9083107709884644}]}, {"text": "In this article, we report on a joint generative history-based model to predict the most likely derivation of a dependency parser for both syntactic and semantic dependencies, in multiple languages.", "labels": [], "entities": []}, {"text": "Because these two dependency structures are not isomorphic, we propose a weak synchronization at the level of meaningful subsequences of the two derivations.", "labels": [], "entities": []}, {"text": "These synchronized subsequences encompass decisions about the left side of each individual word.", "labels": [], "entities": []}, {"text": "We also propose novel derivations for semantic dependency structures, which are appropriate for the relatively unconstrained nature of these graphs.", "labels": [], "entities": []}, {"text": "To train a joint model of these synchronized derivations, we make use of a latent variable model of parsing, the Incremental Sigmoid Belief Network (ISBN) architecture.", "labels": [], "entities": []}, {"text": "This architecture induces latent feature representations of the derivations, which are used to discover correlations both within and between the two derivations, providing the first application of ISBNs to a multi-task learning problem.", "labels": [], "entities": []}, {"text": "This joint model achieves competitive performance on both syntactic and semantic dependency parsing for several languages.", "labels": [], "entities": [{"text": "syntactic and semantic dependency parsing", "start_pos": 58, "end_pos": 99, "type": "TASK", "confidence": 0.6549458026885986}]}, {"text": "Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntactic-semantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned.", "labels": [], "entities": [{"text": "Computational Linguistics Volume 39", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.7562019973993301}]}], "introductionContent": [{"text": "Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees-both constituency-based) and dependency-based)-has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence) or learning the propositional argument-structure of its main predicates.", "labels": [], "entities": [{"text": "statistical syntactic parsing", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.6397789120674133}]}, {"text": "Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (, and has recently been argued to be useful in machine translation and its evaluation (), dialogue systems (; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others.", "labels": [], "entities": [{"text": "question answering", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8724740147590637}, {"text": "information extraction", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.7470759451389313}, {"text": "machine translation and its evaluation", "start_pos": 200, "end_pos": 238, "type": "TASK", "confidence": 0.7098278999328613}, {"text": "automatic data generation", "start_pos": 305, "end_pos": 330, "type": "TASK", "confidence": 0.6459853152434031}, {"text": "authorship attribution", "start_pos": 397, "end_pos": 419, "type": "TASK", "confidence": 0.6739898473024368}]}, {"text": "The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics.", "labels": [], "entities": [{"text": "recovery of the full meaning of text", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7954686284065247}]}, {"text": "These two forms of linguistic knowledge are usually thought to beat least partly independent, as demonstrated by speakers' ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and structures to unknown words and nonsense sentences.", "labels": [], "entities": [{"text": "assign grammatical categories and structures to unknown words and nonsense sentences", "start_pos": 196, "end_pos": 280, "type": "TASK", "confidence": 0.6119063686240803}]}, {"text": "These two levels of representation of language, however, are closely correlated.", "labels": [], "entities": []}, {"text": "From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory).", "labels": [], "entities": []}, {"text": "Linking theory assumes the existence of a ranking of semantic roles that are mapped by default on a ranking of grammatical functions and syntactic positions, and it attempts to predict the mapping of the underlying semantic component of a predicate's meaning onto the syntactic structure.", "labels": [], "entities": [{"text": "Linking theory", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9637892842292786}]}, {"text": "For example, Agents are always mapped in syntactically higher positions than Themes.", "labels": [], "entities": []}, {"text": "Linking theory has been confirmed statistically.", "labels": [], "entities": [{"text": "Linking", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9784886837005615}]}, {"text": "It is currently common to represent the syntactic and semantic role structures of a sentence in terms of dependencies, as illustrated in.", "labels": [], "entities": []}, {"text": "The complete graph of both the syntax and the semantics of the sentences is composed of two half graphs, which share all their vertices-namely, the words.", "labels": [], "entities": []}, {"text": "Internally, these two half graphs exhibit different properties.", "labels": [], "entities": []}, {"text": "The syntactic graph is a single connected tree.", "labels": [], "entities": []}, {"text": "The semantic graph is just a set of one-level treelets, one for each proposition, which maybe disconnected and may share children.", "labels": [], "entities": []}, {"text": "In both graphs, it is not generally appropriate to assume independence across the different treelets in the structure.", "labels": [], "entities": []}, {"text": "In the semantic graph, linguistic evidence that propositions are not independent of each other comes from constructions such as coordinations where some of the arguments are shared and semantically parallel.", "labels": [], "entities": []}, {"text": "The semantic graph is also generally assumed not to be independent of the syntactic graph, as discussed earlier.", "labels": [], "entities": []}, {"text": "As can be observed in, however, arcs in the semantic graph do not correspond one-to-one to arcs in the syntactic graph, indicating that a rather flexible framework is needed to capture the correlations between graphs.", "labels": [], "entities": []}, {"text": "Developing models to learn these structured analyses of syntactic and shallow semantic representations raises, then, several interesting questions.", "labels": [], "entities": []}, {"text": "We concentrate on the following two central questions.", "labels": [], "entities": []}, {"text": "r How do we design the interface between the syntactic and the semantic parsing representations?", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.758529931306839}]}, {"text": "r Are there any benefits to joint learning of syntax and semantics?", "labels": [], "entities": [{"text": "joint learning of syntax and semantics", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.7384586532910665}]}, {"text": "The answer to the second issue depends in part on the solution to the first issue, as indicated by the difficulty of achieving any benefit of joint learning with more traditional approaches (.", "labels": [], "entities": []}, {"text": "We begin by explaining how we address the first issue, using a semi-synchronized latent-variable approach.", "labels": [], "entities": []}, {"text": "We then discuss how this approach benefits from the joint learning of syntax and semantics.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the design of the syntax semantic interface and the use of a latent variable model, we train and evaluate our models on data provided for the CoNLL-2008 shared task on joint learning of syntactic and semantic dependencies for English.", "labels": [], "entities": []}, {"text": "Furthermore, we test the cross-linguistic generality of these models on data from the CoNLL-2009 shared task for seven languages.", "labels": [], "entities": [{"text": "CoNLL-2009 shared task", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8161462942759196}]}, {"text": "In our experiments, we use the measures of performance used in the shared tasks, typical of dependency parsing and semantic role labeling.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7239466607570648}, {"text": "semantic role labeling", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.638448566198349}]}, {"text": "Syntactic performance is measured by the percentage of correct labeled attachments (LAS in the tables).", "labels": [], "entities": [{"text": "LAS", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.7581234574317932}]}, {"text": "Semantic performance is indicated by the F-measure on precision and recall on semantic arcs plus predicate sense labels (indicated as Semantic measures in the table).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9960065484046936}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9981411695480347}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9966199398040771}]}, {"text": "For the CoNLL-2008 scores the predicate sense labeling includes predicate identification, but for the CoNLL-2009 scores predicate identification was given in the task input.", "labels": [], "entities": [{"text": "CoNLL-2008 scores", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.928148627281189}, {"text": "predicate sense labeling", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6780407627423605}, {"text": "predicate identification", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.6862363219261169}, {"text": "CoNLL-2009 scores", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.8906902074813843}, {"text": "predicate identification", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.763926237821579}]}, {"text": "The syntactic LAS and the semantic F 1 are then averaged with equal weight to produce an overall score called Macro F 1 . When we evaluate the impact of the Swap action on crossing arcs, we also calculate precision, recall, and F-measure on pairs of crossing arcs.", "labels": [], "entities": [{"text": "LAS", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.839642345905304}, {"text": "Macro F 1", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.7666287620862325}, {"text": "precision", "start_pos": 205, "end_pos": 214, "type": "METRIC", "confidence": 0.9996960163116455}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9995805621147156}, {"text": "F-measure", "start_pos": 228, "end_pos": 237, "type": "METRIC", "confidence": 0.9988346695899963}]}, {"text": "In our experiments, the statistical significance levels we report are all computed using a stratified shuffling test) with 10,000 randomized trials.", "labels": [], "entities": []}, {"text": "We start by describing the monolingual English experiments.", "labels": [], "entities": []}, {"text": "We train and evaluate our English models on data provided for the CoNLL-2008 shared task on joint learning of syntactic and semantic dependencies.", "labels": [], "entities": []}, {"text": "The data is derived by merging a dependency transformation of the Penn Treebank with PropBank and NomBank ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9959264695644379}, {"text": "PropBank", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9270593523979187}]}, {"text": "An illustrative example of the kind of labeled structures that we need to parse is given in.", "labels": [], "entities": []}, {"text": "Training, development, and test data follow the usual partition as sections 02-21, 24, and 23 of the Penn Treebank, respectively.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.9935210347175598}]}, {"text": "More details and references on the data, on the conversion of the Penn Treebank format to dependencies, and on the experimental set-up are given in.", "labels": [], "entities": [{"text": "Penn Treebank format", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.990517278512319}]}, {"text": "We set the size of the latent variable vector to 80 units, and the word frequency cut-off to 20, resulting in a vocabulary of only 4,000 words.", "labels": [], "entities": []}, {"text": "These two parameters were chosen initially based on previous experience with syntactic dependency parsing).", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.6307790875434875}]}, {"text": "Additionally, preliminary experiments on the development set indicated that larger cut-offs and smaller dimensionality of the latent variable vector results in a sizable decrease in performance.", "labels": [], "entities": []}, {"text": "We did not experiment with decreasing cut-off parameters or increasing the latent space dimensionality beyond these values as it would adversely affect the efficiency of the model.", "labels": [], "entities": []}, {"text": "The efficiency of the model is discussed in more detail in Section 6.5.", "labels": [], "entities": []}, {"text": "We use abeam size of 50 to prune derivations after each Shift operation, and a branching factor of 3.", "labels": [], "entities": []}, {"text": "Larger beam sizes, within a tractable range, did not seem to result in any noticeable improvement in performance on the held-out development set.", "labels": [], "entities": []}, {"text": "We compare several experiments in which we manipulate the connectivity of the model and the allowed operations.", "labels": [], "entities": []}, {"text": "The availability of syntactically annotated corpora for multiple languages () has provided anew opportunity for evaluating the cross-linguistic validity of statistical models of syntactic structure.", "labels": [], "entities": []}, {"text": "This opportunity has been significantly expanded with the creation and annotation of syntactic and semantic resources in seven languages) belonging to several different language families.", "labels": [], "entities": []}, {"text": "This data set was released for the CoNLL-2009 shared task.", "labels": [], "entities": [{"text": "CoNLL-2009 shared task", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.7170372804005941}]}, {"text": "To evaluate the ability of our model to generalize across languages, we take the model as it was developed for English and apply it directly to all of the six other languages.", "labels": [], "entities": []}, {"text": "The only adaptation of the code was done to handle differences in the data format.", "labels": [], "entities": []}, {"text": "Although this consistency across languages was not a requirement of the shared task-individual-language optimization was allowed, and indeed was performed by many teams-the use of latent variables to induce features automatically from the data gives our method the adaptability necessary to perform well across all seven languages, and demonstrates the lack of language specificity in the models.", "labels": [], "entities": []}, {"text": "The data and set-up correspond to the joint task of the closed challenge of the CoNLL-2009 shared task, as described in.", "labels": [], "entities": []}, {"text": "The scoring measures are the same as those for the previous experiments.", "labels": [], "entities": []}, {"text": "We made two modifications to reflect differences in the annotation of these data from the experiments reported in the previous section (based on CoNLL-2008 shared task data).", "labels": [], "entities": [{"text": "CoNLL-2008 shared task data", "start_pos": 145, "end_pos": 172, "type": "DATASET", "confidence": 0.8724157363176346}]}, {"text": "The system was adapted to use two features not provided in the previous shared task: automatically predicted morphological features and features specifying which words were annotated as predicates.", "labels": [], "entities": []}, {"text": "Both these features resulted in improved accuracy for all the languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9992726445198059}]}, {"text": "We also made use of one type of feature that had previously been found not to result in any improvement for English, but resulted in some overall improvement across the languages.", "labels": [], "entities": []}, {"text": "Also, in comparison with previous experiments, the search beam used in the parsing phase was increased from 50 to up to 80, producing a small improvement in the overall development score.", "labels": [], "entities": [{"text": "parsing phase", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9287073016166687}]}, {"text": "The vocabulary frequency cut-off was also changed to 5, from 20.", "labels": [], "entities": []}, {"text": "All the development effort to change from the English-only 2008 task to the multilingual 2009 task took about two person-months, mostly by someone who had no previous experience with the system.", "labels": [], "entities": []}, {"text": "Most of this time was spent on the differences in the task definition between the 2008 and 2009 shared tasks.", "labels": [], "entities": []}, {"text": "The official results on the testing set and out of domain data are shown in, and 11.", "labels": [], "entities": []}, {"text": "The best results across systems participating in the CoNLL-2009 shared task are shown in bold.", "labels": [], "entities": [{"text": "CoNLL-2009 shared task", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.701483945051829}]}, {"text": "There was only a 0.5% difference between our average macro F 1 score and that of the best system, and there was a 1.29% difference between our score and the fourth ranked system.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.8961535890897115}]}, {"text": "The differences between our average scores reported in, and 11 and the average scores achieved by the other systems participating in the shared task are all statistically significant at p = 0.05. 15 Morphological features of a word are not conditionally independent.", "labels": [], "entities": []}, {"text": "To integrate them into a generative model, one needs to either make some independence assumptions or model sets of features as atomic feature bundles.", "labels": [], "entities": []}, {"text": "In our model, morphological features are treated as anatomic bundle, when computing the probability of the word before shifting the previous word to the stack.", "labels": [], "entities": []}, {"text": "When estimating probabilities of future actions, however, we condition latent variables on elementary morphological features of the words.", "labels": [], "entities": []}, {"text": "16 Because the testing data included a specification of which words were annotated as predicates, we constrained the parser's output so as to be consistent with this specification.", "labels": [], "entities": []}, {"text": "For rare predicates, if the predicate was not in the parser's lexicon (extracted from the training set), then a frameset was taken from the list of framesets reported in the resources available for the closed challenge.", "labels": [], "entities": []}, {"text": "If this information was not available, then a default frameset name was constructed based on the automatically predicted lemma of the predicate.", "labels": [], "entities": []}, {"text": "17 When predicting a semantic arc between the word on the front of the queue and the word on the top of the stack, these features explicitly specify any syntactic dependency already predicted between the same two words.", "labels": [], "entities": []}, {"text": "Despite the good results, a more detailed analysis of the source of errors seems to indicate that our system is still having trouble with crossing dependencies, even after the introduction of the Swap operation.", "labels": [], "entities": []}, {"text": "In, our recall on English crossing semantic dependencies is relatively low.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9994901418685913}, {"text": "English crossing semantic dependencies", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.6470242515206337}]}, {"text": "Some statistics that illustrate the nature of the input and could explain some of the errors are shown in.", "labels": [], "entities": []}, {"text": "As can be observed, semantic representations often have many more crossing arcs than syntactic ones, and they often do not form a fully connected tree, as each proposition is represented by an independent treelet.", "labels": [], "entities": []}, {"text": "We observe that, with the exception of German, we do relatively well on those languages that do not have crossing arcs, such as Catalan and Spanish, or have even large amounts of crossing arcs that can be parsed with the Swap operation, such as Czech.", "labels": [], "entities": []}, {"text": "As indicated in, only 2% of Czech sentences are unparsable, despite 16% requiring the Swap action.", "labels": [], "entities": []}, {"text": "The training and parsing times for our models are reported in, using the same meta-parameters (discussed subsequently) as for the accuracies reported in the previous section, which optimize accuracy at the expense of speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9504141211509705}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9985073208808899}]}, {"text": "Training times are mostly affected by data-set size, which increases the time taken for each iteration.", "labels": [], "entities": []}, {"text": "This is not only because the full training set must be processed, but also because a larger data set For each language, percentage of training sentences with crossing arcs in syntax and semantics, with semantic arcs forming a tree, and which were not parsable using the Swap action, as well as the performance of our system in the CoNLL-2009 shared task by syntactic accuracy and semantic F 1 .  tends to result in more parameters to train, including larger vocabulary sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 367, "end_pos": 375, "type": "METRIC", "confidence": 0.9804993271827698}, {"text": "semantic F 1", "start_pos": 380, "end_pos": 392, "type": "METRIC", "confidence": 0.7281926572322845}]}, {"text": "Also, larger data sets tend to result in more iterations of training, which further increases training times.", "labels": [], "entities": []}, {"text": "Normalizing for data-set size and number of iterations (second row of, we get fairly consistent speeds across languages.", "labels": [], "entities": []}, {"text": "The remaining differences are correlated with the number of parameters in the model, and with the proportion of words which are predicates in the SRL annotation, shown in the bottom panel of.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.6747528612613678}]}, {"text": "Parsing times are more variable, even when normalizing for the number of sentences in the data set, as shown in the third row of.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9417975544929504}]}, {"text": "As discussed earlier, this is in part an effect of the different beam widths used for different languages, and the different distributions of sentence lengths.", "labels": [], "entities": []}, {"text": "If we divide times by beam width and by average sentence length (fourth row of, we get more consistent numbers, but still with a lot of variation.", "labels": [], "entities": []}, {"text": "18 These differences are in part explained by the relative complexity of the SRL annotation in the different languages.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.8179254531860352}]}, {"text": "They are correlated with both the percentage of words that are predicates and the percentage of sentences that", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  For each language, percentages of training sentences with crossing arcs in syntax and semantics,  and percentages of training sentences with semantic arcs forming a tree whose root immediately  dominates the predicates.", "labels": [], "entities": []}, {"text": " Table 4  Scores on the development set of the CoNLL-2008 shared task (percentages).", "labels": [], "entities": [{"text": "CoNLL-2008 shared task", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.7095795075098673}]}, {"text": " Table 5  Proposition scores on the development set of the CoNLL-2008 shared task (percentages).", "labels": [], "entities": [{"text": "CoNLL-2008 shared task", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.701821525891622}]}, {"text": " Table 5. The differences in precision, recall, and F 1 are  all statistically significant at p = 0.05. These results clearly indicate that the connectivity  of latent vectors both within representational layers and across them influences the  accuracy of recovering the whole propositional content associated with predicates. In  particular, our model connecting the latent vectors within the semantic layer signifi- cantly improves both the precision and the recall of the predicted propositions over  the model where these connections are removed (second vs. third line). Furthermore,  the model integrating both the connections from syntax to semantics and the connec- tions within semantics significantly outperforms the model with no connections from  syntax to semantics (first vs. second line). Overall, these results suggest that whole  propositions are best learned jointly by connecting latent vectors, even when these  latent vectors are conditioned on a rich set of predefined features, including semantic  siblings.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9993927478790283}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9993160963058472}, {"text": "F 1", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.994430422782898}, {"text": "accuracy", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.9981549382209778}, {"text": "precision", "start_pos": 443, "end_pos": 452, "type": "METRIC", "confidence": 0.99843829870224}, {"text": "recall", "start_pos": 461, "end_pos": 467, "type": "METRIC", "confidence": 0.9947333335876465}]}, {"text": " Table 6  Scores on the development set (percentages).", "labels": [], "entities": []}, {"text": " Table 7  Scores of the fully connected model on the final testing sets of the CoNLL-2008 shared task  (percentages).", "labels": [], "entities": []}, {"text": " Table 8  Comparison with other models on the CoNLL-2008 test set (percentages).", "labels": [], "entities": [{"text": "CoNLL-2008 test set", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.9817512432734171}]}, {"text": " Table 9  The three main scores for our system. Rank indicates ranking in the CoNLL 2009 shared task.  Best results across systems are marked in bold.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.8487230390310287}]}, {"text": " Table 10  Semantic precision and recall and macro precision and recall for our system. Rank indicates  ranking in the CoNLL-2009 shared task. Best results across systems are marked in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9940138459205627}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9949617385864258}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.942794144153595}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9983363747596741}, {"text": "CoNLL-2009", "start_pos": 119, "end_pos": 129, "type": "DATASET", "confidence": 0.7969011664390564}]}, {"text": " Table 11  Results on out-of-domain for our system. Rank indicates ranking in the CoNLL-2009 shared  task. Best results across systems are marked in bold.", "labels": [], "entities": [{"text": "CoNLL-2009 shared  task", "start_pos": 82, "end_pos": 105, "type": "DATASET", "confidence": 0.6951862176259359}]}, {"text": " Table 12  For each language, percentage of training sentences with crossing arcs in syntax and semantics,  with semantic arcs forming a tree, and which were not parsable using the Swap action, as well as  the performance of our system in the CoNLL-2009 shared task by syntactic accuracy and  semantic F 1 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 279, "end_pos": 287, "type": "METRIC", "confidence": 0.9786332249641418}, {"text": "semantic F 1", "start_pos": 293, "end_pos": 305, "type": "METRIC", "confidence": 0.7738177378972372}]}, {"text": " Table 13  Parsing and training times for different languages, run on a 3.4 GHz machine with 16 GB of  memory. Parsing times computed on the test set. Indicators of SRL complexity provided for  comparison.", "labels": [], "entities": [{"text": "SRL", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.8557782769203186}]}]}