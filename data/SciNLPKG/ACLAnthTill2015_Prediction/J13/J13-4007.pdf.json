{"title": [{"text": "Improving Statistical Machine Translation by Adapting Translation Models to Translationese", "labels": [], "entities": [{"text": "Improving Statistical Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8720582574605942}, {"text": "Adapting Translation Models to Translationese", "start_pos": 45, "end_pos": 90, "type": "TASK", "confidence": 0.8157681465148926}]}], "abstractContent": [{"text": "Translation models used for statistical machine translation are compiled from parallel corpora that are manually translated.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.6943064530690511}]}, {"text": "The common assumption is that parallel texts are symmetrical: The direction of translation is deemed irrelevant and is consequently ignored.", "labels": [], "entities": []}, {"text": "Much research in Translation Studies indicates that the direction of translation matters, however, as translated language (translationese) has many unique properties.", "labels": [], "entities": [{"text": "Translation Studies", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.9712492823600769}]}, {"text": "It has already been shown that phrase tables constructed from parallel corpora translated in the same direction as the translation task outperform those constructed from corpora translated in the opposite direction.", "labels": [], "entities": []}, {"text": "We reconfirm that this is indeed the case, but emphasize the importance of also using texts translated in the \"wrong\" direction.", "labels": [], "entities": []}, {"text": "We take advantage of information pertaining to the direction of translation in constructing phrase tables by adapting the translation model to the special properties of translationese.", "labels": [], "entities": []}, {"text": "We explore two adaptation techniques: First, we create a mixture model by interpolating phrase tables trained on texts translated in the \"right\" and the \"wrong\" directions.", "labels": [], "entities": []}, {"text": "The weights for the interpolation are determined by minimizing perplexity.", "labels": [], "entities": []}, {"text": "Second, we define entropy-based measures that estimate the correspondence of target-language phrases to translationese, thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation.", "labels": [], "entities": []}, {"text": "We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent, statistically significant improvement in the quality of the translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.6118102272351583}]}], "introductionContent": [{"text": "Much research in translation studies indicates that translated texts have unique characteristics that set them apart from original texts.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9698349833488464}]}, {"text": "Known as translationese, translated texts (in any language) constitute a genre, or a dialect, of the target language, which reflects both artifacts of the translation process and traces of the original language from which the texts were translated.", "labels": [], "entities": []}, {"text": "Among the better-known properties of translationese are simplification and explicitation: Translated texts tend to be shorter, to have lower type/token ratio, and to use certain discourse markers more frequently than original texts.", "labels": [], "entities": []}, {"text": "Interestingly, translated texts are so markedly different from original ones that automatic classification can identify them with very high accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.994818389415741}]}, {"text": "Contemporary statistical machine translation (SMT) systems use parallel corpora to train translation models that reflect source-and target-language phrase correspondences.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.7803085496028265}]}, {"text": "Typically, SMT systems ignore the direction of translation of the parallel corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9941434264183044}]}, {"text": "Given the unique properties of translationese, which operate asymmetrically from source to target language, it is reasonable to assume that this direction may affect the quality of the translation.", "labels": [], "entities": []}, {"text": "Recently, showed that this is indeed the case.", "labels": [], "entities": []}, {"text": "They trained a system to translate between French and English (and vice versa) using a French-translated-to-English parallel corpus, and then an Englishtranslated-to-French one.", "labels": [], "entities": [{"text": "translate between French and English", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.8434535980224609}]}, {"text": "They find that in translating into French the latter parallel corpus yields better results (in terms of higher BLEU scores), whereas for translating into English it is better to use the former.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9994716048240662}]}, {"text": "Typically, however, parallel corpora are not marked for direction.", "labels": [], "entities": []}, {"text": "Therefore, trained an SVM-based classifier to predict which side of a bi-text is the origin and which one is the translation, and trained a translation model by utilizing only the subset of the corpus that corresponds to the direction of the task.", "labels": [], "entities": []}, {"text": "We use these results as our departure point, but improve them in two major ways.", "labels": [], "entities": []}, {"text": "First, we demonstrate that the other subset of the corpus, reflecting translation in the \"wrong\" direction, is also important for the translation task, and must not be ignored; second, we show that explicit information on the direction of translation of the parallel corpus, whether manually annotated or machine-learned, is not mandatory.", "labels": [], "entities": [{"text": "translation task", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.9239082932472229}]}, {"text": "This is achieved by casting the problem in the framework of domain adaptation: We use domain-adaptation techniques to direct the SMT system toward producing output that better reflects the properties of translationese.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9857272505760193}]}, {"text": "We show that SMT systems adapted to translationese produce better translations than vanilla systems trained on exactly the same resources.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9928120970726013}, {"text": "translationese", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.9755471348762512}]}, {"text": "We confirm these findings using automatic evaluation metrics, as well as through a qualitative analysis of the results.", "labels": [], "entities": []}, {"text": "After reviewing related work in Section 2, we begin by replicating the results of Kurokawa, Goutte, and Isabelle (2009) in Section 3.", "labels": [], "entities": []}, {"text": "We then (Section 4) explain why translation quality improves when the parallel corpus is translated in the \"right\" direction.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.958510160446167}]}, {"text": "We do so by showing that the subset of the corpus that was translated in the direction of the translation task (the \"right\" direction, henceforth source-to-target, or S \u2192 T) yields phrase tables that are better suited for translation of the original language than the subset translated in the reverse direction (the \"wrong\" direction, henceforth target-to-source, or T \u2192 S).", "labels": [], "entities": []}, {"text": "We use several statistical measures that indicate the better quality of the phrase tables in the former case.", "labels": [], "entities": []}, {"text": "We then show (Section 5) that using the entire parallel corpus, including texts that are translated both in the \"right\" and in the \"wrong\" direction, improves the quality of the results.", "labels": [], "entities": []}, {"text": "Next, we investigate several ways to improve the translation quality by adapting a translation model to the nature of translationese, thereby making the output of machine translation more similar to actual, human translation.", "labels": [], "entities": []}, {"text": "Specifically, we create two phrase tables, one for the S \u2192 T portion of the corpus, and one for the T \u2192 S portion, and combine them into a mixture model using perplexity minimization) to set the model weights.", "labels": [], "entities": []}, {"text": "We show that this combination significantly outperforms a simple union of the two portions of the parallel corpus.", "labels": [], "entities": []}, {"text": "Furthermore, we show that the direction of translation used for producing the parallel corpus can be approximated by defining several entropy-based measures that correlate well with translationese, and, consequently, with translation quality.", "labels": [], "entities": []}, {"text": "We use the entire corpus, create a single, unified phrase table, and then use these measures, and in particular cross-entropy, as a clue for selecting phrase pairs from this table.", "labels": [], "entities": []}, {"text": "The benefit of this method is that not only does it improve the translation quality, but it also eliminates the need to directly predict the direction of translation of the parallel corpus.", "labels": [], "entities": []}, {"text": "The main contribution of this work, therefore, is a methodology that improves the quality of SMT by building translation models that are adapted to the nature of translationese.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9945189356803894}]}, {"text": "To demonstrate the contribution of our methodology, we conduct in Section 6 a thorough analysis of our results, both quantitatively and qualitatively.", "labels": [], "entities": []}, {"text": "We show that translations produced by our best-performing system indeed reflect some well-known properties of translationese better than the output of baseline systems.", "labels": [], "entities": []}, {"text": "Furthermore, we provide several examples of SMT outputs that demonstrate in what ways our adapted system generates better results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9941496849060059}]}], "datasetContent": [{"text": "The task we focus on in our experiments is translation from French to English (FR-EN) and from English to French (EN-FR).", "labels": [], "entities": [{"text": "translation from French to English", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.8571539640426635}, {"text": "FR-EN", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.7011746168136597}]}, {"text": "To establish the robustness of our approach, we also conduct experiments with other translation tasks, including German-English (DE-EN), English-German (EN-DE), Italian-English (IT-EN), and English-Italian (EN-IT).", "labels": [], "entities": []}, {"text": "Our corpus is Europarl), specifically, portions collected over the years.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9388051629066467}]}, {"text": "This is a large multilingual corpus, containing sentences translated from several European languages.", "labels": [], "entities": []}, {"text": "In most cases the corpus is annotated with the original language and the name of the speaker.", "labels": [], "entities": []}, {"text": "For each language pair we extract from the multilingual corpus two subsets, corresponding to the original languages in which the sentences were produced.", "labels": [], "entities": []}, {"text": "For example, in the case of FR-EN we extract from our corpus all sentences produced in French and translated into English, and all sentences produced in English and translated into French.", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.7826353311538696}]}, {"text": "All sentences are lowercased and tokenized using Moses ( ).", "labels": [], "entities": []}, {"text": "Sentences longer than 80 words are discarded.", "labels": [], "entities": []}, {"text": "depicts the size of the subsets whose target language is English.", "labels": [], "entities": []}, {"text": "We use each subset to train two phrase-based statistical machine translation (PB-SMT) systems ( ), translating in both directions between the languages in each language pair.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PB-SMT)", "start_pos": 32, "end_pos": 85, "type": "TASK", "confidence": 0.6754626546587262}]}, {"text": "In other words, we train two PB-SMTs for each translation task, each based on a parallel corpus produced and translated in a different direction.", "labels": [], "entities": []}, {"text": "We use GIZA++ (Och and Ney 2000) with grow-diag-final alignment, and extract phrases of length up to 10 words.", "labels": [], "entities": []}, {"text": "We prune the resulting phrase tables as in, using at most 30 translations per source phrase and discarding singleton phrase pairs.", "labels": [], "entities": []}, {"text": "We use all Europarl corpora between the years 1996-1999 and 2001-2009 to construct English, German, French, and Italian 5-gram language models, using interpolated modified Kneser-Ney discounting and no cut-off on all n-grams.", "labels": [], "entities": [{"text": "Europarl corpora", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.9536548554897308}]}, {"text": "We use a specific symbol to mark out-of-vocabulary words (OOVs).", "labels": [], "entities": []}, {"text": "The OOV rate is low, less than 0.5%, and very similar in all our experiments.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9815622568130493}]}, {"text": "We use the portion of Europarl collected over the year 2000 for tuning and evaluation.", "labels": [], "entities": [{"text": "Europarl collected over the year 2000", "start_pos": 22, "end_pos": 59, "type": "DATASET", "confidence": 0.9378170470396677}]}, {"text": "For each translation task we randomly extract 1,000 parallel sentences for the tuning set and another set of 5,000 parallel sentences for evaluation.", "labels": [], "entities": []}, {"text": "These sentences are originally written in the translation task's source language and are translated into the translation task's target language (in realworld scenarios, the directionality of the test set is typically known).", "labels": [], "entities": []}, {"text": "We use the MERT algorithm (Och 2003) for tuning and BLEU ( as our evaluation metric.", "labels": [], "entities": [{"text": "MERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9710227251052856}, {"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9996060729026794}]}, {"text": "We test the statistical significance of the differences between the results using the bootstrap resampling method.", "labels": [], "entities": []}, {"text": "A word on notation: We use S \u2192 T when the translation direction of the parallel corpus corresponds to the translation task and T \u2192 S when a corpus is translated in the opposite direction to the translation task.", "labels": [], "entities": []}, {"text": "For example, suppose the translation tasks are English-to-French (E2F) and French-to-English (F2E).", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9603598713874817}, {"text": "French-to-English (F2E)", "start_pos": 75, "end_pos": 98, "type": "METRIC", "confidence": 0.5930244773626328}]}, {"text": "We use S \u2192 T when the Frenchoriginal corpus is used for the F2E task or when the English-original corpus is used for the E2F task; and T \u2192 S when the French-original corpus is used for the E2F task or when the English-original corpus is used for the F2E task.", "labels": [], "entities": [{"text": "F2E", "start_pos": 250, "end_pos": 253, "type": "DATASET", "confidence": 0.8345223665237427}]}, {"text": "texts always outperform systems trained on T \u2192 S texts.", "labels": [], "entities": []}, {"text": "The difference in BLEU score can be as high as 3 points.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.950131893157959}]}, {"text": "The corpora used in the Europarl experiments are small (up to 200,000 sentences).", "labels": [], "entities": [{"text": "Europarl experiments", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.9533704519271851}]}, {"text": "Also, the ratio between S \u2192 T and T \u2192 S materials varies greatly for different language pairs.", "labels": [], "entities": []}, {"text": "To mitigate these issues we use the Hansard corpus, containing transcripts of the Canadian parliament from 1996-2007, as another source of parallel data.", "labels": [], "entities": [{"text": "Hansard corpus", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9871318340301514}]}, {"text": "The Hansard is a bilingual French-English corpus comprising approximately 80% English-original texts and 20% French-original texts.", "labels": [], "entities": [{"text": "The Hansard", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7096521854400635}]}, {"text": "Crucially, each sentence pair in the corpus is annotated with the direction of translation.", "labels": [], "entities": []}, {"text": "To address the effect of corpus size, we compile six subsets of different sizes (250K, 500K, 750K, 1M, 1.25M, and 1.5M parallel sentences) from each portion (English-original and French-original) of the corpus.", "labels": [], "entities": []}, {"text": "Additionally, we use the devtest section of the Hansard corpus to randomly select French-original and English-original sentences that are used for tuning (1,000 sentences each) and evaluation (5,000 sentences each).", "labels": [], "entities": [{"text": "Hansard corpus", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9851876199245453}]}, {"text": "On these corpora we train twelve French-to-English and twelve English-to-French PB-SMT systems using the Moses toolkit ( ).", "labels": [], "entities": []}, {"text": "We use the same GIZA++ configuration and phrase table pruning as in the Europarl experiments.", "labels": [], "entities": [{"text": "Europarl experiments", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.9706893861293793}]}, {"text": "We also reuse the English and French language models.", "labels": [], "entities": []}, {"text": "French-to-English MT systems are tuned and tested on French-original sentences and English-to-French systems on English-original ones.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.8137246966362}]}, {"text": "depicts the BLEU scores of the Hansard systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9978212118148804}, {"text": "Hansard", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9261642098426819}]}, {"text": "The data are consistent with our previous findings: Systems trained on S \u2192 T parallel texts always outperform  systems trained on T \u2192 S texts, even when the latter are much larger.", "labels": [], "entities": []}, {"text": "For example, a French-to-English SMT system trained on 250,000 S \u2192 T sentences outperforms a system trained on 1,500,000 T \u2192 S sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.914495050907135}]}], "tableCaptions": [{"text": " Table 2 depicts the BLEU scores of the SMT systems. The data are consistent with  the findings of Kurokawa, Goutte, and Isabelle (2009): Systems trained on S \u2192 T parallel", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9992565512657166}, {"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.993710994720459}]}, {"text": " Table 2  BLEU scores of the Europarl baseline systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991492033004761}, {"text": "Europarl baseline", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9882210791110992}]}, {"text": " Table 3  BLEU scores of the Hansard baseline systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991191029548645}, {"text": "Hansard baseline", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9607429504394531}]}, {"text": " Table 4  Statistic measures computed on the phrase tables: total size, in tokens (Total), the number of  unique source phrases (Source), and the average number of translations per source phrase  (AvgTran).", "labels": [], "entities": [{"text": "AvgTran)", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.868223249912262}]}, {"text": " Table 5  Entropy-based measures computed on the phrase tables: covering set entropy (CovEnt),  covering set cross-entropy (CovCrEnt), and covering set average length (CovLen).", "labels": [], "entities": [{"text": "set average length (CovLen)", "start_pos": 148, "end_pos": 175, "type": "METRIC", "confidence": 0.7370095402002335}]}, {"text": " Table 6  Correlation of BLEU scores with phrase table statistical measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9971876740455627}]}, {"text": " Table 7  Evaluation results of various ways for combining phrase tables.", "labels": [], "entities": []}, {"text": " Table 8  MultEval scores for UNION and PPLMIN-2 systems.", "labels": [], "entities": [{"text": "MultEval", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9610775709152222}]}, {"text": " Table 9  Adaption without classification results.", "labels": [], "entities": [{"text": "Adaption", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.7558249235153198}]}, {"text": " Table 10  Entropy-based measures, computed on phrase tables of baseline and adapted SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9728302359580994}]}, {"text": " Table 11  BLEU scores computed on portions of UNION and PPLMIN-2 systems outputs below and above  the MOR median.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9991723299026489}, {"text": "MOR median", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9604111611843109}]}, {"text": " Table 12  Combining TMs and LMs: SMT system evaluation results.", "labels": [], "entities": [{"text": "SMT system evaluation", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8644330302874247}]}, {"text": " Table 13  Adapting TMs and LMs: SMT system evaluation results.", "labels": [], "entities": [{"text": "Adapting TMs", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.9225353896617889}, {"text": "SMT system evaluation", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.868600070476532}]}]}