{"title": [{"text": "Relational Features in Fine-Grained Opinion Analysis", "labels": [], "entities": [{"text": "Fine-Grained Opinion Analysis", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6616732875506083}]}], "abstractContent": [{"text": "Fine-grained opinion analysis methods often make use of linguistic features but typically do not take the interaction between opinions into account.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7410077750682831}]}, {"text": "This article describes a set of experiments that demonstrate that relational features, mainly derived from dependency-syntactic and semantic role structures, can significantly improve the performance of automatic systems fora number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion holders, and determining the polarities of opinion expressions.", "labels": [], "entities": []}, {"text": "These features make it possible to model the way opinions expressed in natural-language discourse interact in a sentence over arbitrary distances.", "labels": [], "entities": []}, {"text": "The use of relations requires us to consider multiple opinions simultaneously, which makes the search for the optimal analysis intractable.", "labels": [], "entities": []}, {"text": "However, a reranker can be used as a sufficiently accurate and efficient approximation.", "labels": [], "entities": []}, {"text": "A number of feature sets and machine learning approaches for the rerankers are evaluated.", "labels": [], "entities": []}, {"text": "For the task of opinion expression extraction, the best model shows a 10-point absolute improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local contextual features, while precision decreases only slightly.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.7205091118812561}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9102608561515808}, {"text": "MPQA corpus", "start_pos": 122, "end_pos": 133, "type": "DATASET", "confidence": 0.9473528563976288}, {"text": "precision", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.9993334412574768}]}, {"text": "Significant improvements are also seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall, respectively.", "labels": [], "entities": [{"text": "holders", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9823142290115356}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.999314546585083}]}, {"text": "In addition, the systems outperform previously published results for unlabeled (6 F-measure points) and polarity-labeled (10-15 points) opinion expression extraction.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 136, "end_pos": 165, "type": "TASK", "confidence": 0.615834653377533}]}, {"text": "Finally, as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical opinion mining tasks.", "labels": [], "entities": [{"text": "opinion mining tasks", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.8090013265609741}]}, {"text": "In all scenarios considered, the machine learning features derived from the opinion expressions lead to statistically significant improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic methods for the analysis of opinions (textual expressions of emotions, beliefs, and evaluations) have attracted considerable attention in the natural language processing community in recent years (.", "labels": [], "entities": [{"text": "analysis of opinions (textual expressions of emotions, beliefs, and evaluations)", "start_pos": 26, "end_pos": 106, "type": "TASK", "confidence": 0.7896488777228764}]}, {"text": "Apart from their interest from a linguistic and psychological point of view, the technologies emerging from this research have obvious practical uses, either as stand-alone applications or supporting other tools such as information retrieval or question answering systems.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 220, "end_pos": 241, "type": "TASK", "confidence": 0.7640102207660675}, {"text": "question answering", "start_pos": 245, "end_pos": 263, "type": "TASK", "confidence": 0.7507176995277405}]}, {"text": "The research community initially focused on high-level tasks such as retrieving documents or passages expressing opinion, or classifying the polarity of a given text, and these coarse-grained problem formulations naturally led to the application of methods derived from standard retrieval or text categorization techniques.", "labels": [], "entities": [{"text": "classifying the polarity of a given text", "start_pos": 125, "end_pos": 165, "type": "TASK", "confidence": 0.8162688612937927}]}, {"text": "The models underlying these approaches have used very simple feature representations such as purely lexical or low-level grammatical features such as part-of-speech tags and functional words).", "labels": [], "entities": []}, {"text": "This is inline with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7295104563236237}]}, {"text": "There area few exceptions, such as, who showed that construction features added to a bag-ofwords representation resulted in improved performance on a number of coarse-grained opinion analysis tasks.", "labels": [], "entities": [{"text": "opinion analysis tasks", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.7494153380393982}]}, {"text": "Similarly, argued that a speaker's attitude can be predicted from syntactic features such as the selection of a transitive or intransitive verb frame.", "labels": [], "entities": []}, {"text": "In contrast to the early work, recent years have seen a shift towards more detailed problem formulations where the task is not only to find apiece of opinionated text, but also to extract a structured representation of the opinion.", "labels": [], "entities": []}, {"text": "For instance, we may determine the person holding the opinion (the holder) and towards which entity or fact it is directed (the topic), whether it is positive or negative (the polarity), and the strength of the opinion (the intensity).", "labels": [], "entities": []}, {"text": "The increasing complexity of representation leads us from retrieval and categorization deep into natural language processing territory; the methods used here have been inspired by information extraction and semantic role labeling, combinatorial optimization, and structured machine learning.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.7837220728397369}, {"text": "semantic role labeling", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.641942967971166}]}, {"text": "For such tasks, deeper representations of linguistic structure have seen more use than in the coarsegrained case.", "labels": [], "entities": []}, {"text": "Syntactic and shallow-semantic relations have repeatedly proven useful for subtasks of opinion analysis that are relational in nature, above all for determining the holder or topic of a given opinion, in which case there is considerable similarity to tasks such as semantic role labeling.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7469558715820312}, {"text": "semantic role labeling", "start_pos": 265, "end_pos": 287, "type": "TASK", "confidence": 0.6588559746742249}]}, {"text": "There has been no systematic research, however, on the role played by linguistic structure in the relations between opinions expressed in text, despite the fact that the opinion expressions in a sentence are not independent but organized rhetorically to achieve a communicative effect intended by the speaker.", "labels": [], "entities": []}, {"text": "We therefore expect that the interplay between opinion expressions can be exploited to derive information useful for the analysis of opinions expressed in text.", "labels": [], "entities": [{"text": "analysis of opinions expressed in text", "start_pos": 121, "end_pos": 159, "type": "TASK", "confidence": 0.83297265569369}]}, {"text": "In this article, we start from this intuition and propose several novel features derived from the interdependencies between opinion expressions on the syntactic and shallow-semantic levels.", "labels": [], "entities": []}, {"text": "Based on these features, we devised structured prediction models for (1) extraction of opinion expressions, (2) joint expression extraction and holder extraction, and (3) joint expression extraction and polarity labeling.", "labels": [], "entities": [{"text": "joint expression extraction", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.6432339350382487}, {"text": "holder extraction", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.7159276455640793}, {"text": "joint expression extraction", "start_pos": 171, "end_pos": 198, "type": "TASK", "confidence": 0.6534263590971628}, {"text": "polarity labeling", "start_pos": 203, "end_pos": 220, "type": "TASK", "confidence": 0.6568029820919037}]}, {"text": "The models were trained using a number of discriminative machine learning methods.", "labels": [], "entities": []}, {"text": "Because the interdependency features required us to consider more than one opinion expression at a time, the inference steps carried out at training and prediction time could not rely on commonly used opinion expression mark-up methods based on Viterbi search, but we show that an approximate search method using reranking suffices for this purpose: Ina first step abase system using local features and efficient search generates a small set of hypotheses, and in a second step a classifier using the complex features selects the final output from the hypothesis set.", "labels": [], "entities": []}, {"text": "This approach allows us to make use of arbitrary features extracted from the complete set of opinion expressions in a sentence, without having to impose any restriction on the expressivity of the features.", "labels": [], "entities": []}, {"text": "An additional advantage is that it is fairly easy to implement as long as the underlying system is able to generate k-best output.", "labels": [], "entities": []}, {"text": "The interaction-based reranking systems were evaluated on a test set extracted from the MPQA corpus, and compared to strong baselines consisting of stand-alone systems for opinion expression mark-up, opinion holder extraction, and polarity classification.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9839881658554077}, {"text": "opinion holder extraction", "start_pos": 200, "end_pos": 225, "type": "TASK", "confidence": 0.6625106533368429}, {"text": "polarity classification", "start_pos": 231, "end_pos": 254, "type": "TASK", "confidence": 0.7116067707538605}]}, {"text": "Our evaluations showed that (1) the best opinion expression mark-up system we evaluated achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler.", "labels": [], "entities": [{"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9559484124183655}, {"text": "F-measure", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9981404542922974}]}, {"text": "Our system outperformed previously described opinion expression mark-up tools by six points in overlap F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9838362336158752}]}, {"text": "(2) The recall was boosted by almost 10 points for the holder extraction task (over three points in F-measure) by modeling the interaction of opinion expressions with respect to holders.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9991074204444885}, {"text": "holder extraction task", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8733139236768087}, {"text": "F-measure", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.975642204284668}]}, {"text": "(3) We saw an improvement for the extraction of polaritylabeled expression of four F-measure points.", "labels": [], "entities": []}, {"text": "Our result for opinion extraction and polarity labeling is especially striking when compared with the best previously published end-to-end system for this task: 10-15 points in F-measure improvement.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7449040114879608}, {"text": "polarity labeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7302687764167786}, {"text": "F-measure", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9218090772628784}]}, {"text": "In addition to the performance evaluations, we studied the impact of features on the subtasks, and the effect of the choice of the machine learning method for training the reranker.", "labels": [], "entities": []}, {"text": "As a final extrinsic evaluation of the system, we evaluated the usefulness of its output in a number of applications.", "labels": [], "entities": []}, {"text": "Although there have been several publications detailing the extraction of MPQA-style opinion expressions, as far as we are aware there has been no attempt to use them in an application.", "labels": [], "entities": [{"text": "extraction of MPQA-style opinion expressions", "start_pos": 60, "end_pos": 104, "type": "TASK", "confidence": 0.7118471205234528}]}, {"text": "In contrast, we show that the opinion expressions as defined by the MPQA corpus maybe used to derive machine learning features that are useful in two practical opinion mining tasks; the addition of these features leads to statistically significant improvements in all scenarios we evaluated.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.9722755253314972}, {"text": "opinion mining", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.7446516156196594}]}, {"text": "First, we develop a system for the extraction of evaluations of product attributes from product reviews (;, and we show that the features derived from opinion expressions lead to significant improvement.", "labels": [], "entities": [{"text": "extraction of evaluations of product attributes from product reviews", "start_pos": 35, "end_pos": 103, "type": "TASK", "confidence": 0.7823633948961893}]}, {"text": "Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews.", "labels": [], "entities": [{"text": "document polarity classification of reviews", "start_pos": 148, "end_pos": 191, "type": "TASK", "confidence": 0.7962531566619873}]}, {"text": "After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline systems: a sequence labeler for the extraction of opinion expressions and classifiers for opinion holder extraction and polarity labeling; Section 5 reports on the main contribution: the description of the interaction models and their features; finally, Section 7 presents the experimental results and Section 8 derives the conclusions.", "labels": [], "entities": [{"text": "MPQA opinion corpus", "start_pos": 133, "end_pos": 152, "type": "DATASET", "confidence": 0.8880890011787415}, {"text": "opinion holder extraction", "start_pos": 313, "end_pos": 338, "type": "TASK", "confidence": 0.7258013288180033}, {"text": "polarity labeling", "start_pos": 343, "end_pos": 360, "type": "TASK", "confidence": 0.7444311082363129}]}], "datasetContent": [{"text": "We trained and evaluated the rerankers on version 2.0 of the MPQA corpus, 2 which contains 692 documents.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9554881155490875}]}, {"text": "We discarded one document whose annotation was garbled and we split the remaining 691 into a training set (541 documents) and a test set (150 documents).", "labels": [], "entities": []}, {"text": "We also set aside a development set of 90 documents from the training set that we used when developing features and tuning learning algorithm parameters; all experiments described in this article, however, used models that were trained on the full training set.", "labels": [], "entities": []}, {"text": "shows some statistics about the training and test sets: the number of documents and sentences; the number of DSEs, ESEs, and OSEs; and the number of expressions marked with the various polarity labels.", "labels": [], "entities": []}, {"text": "We considered three experimental settings: (1) opinion expression extraction; (2) joint opinion expression and holder extraction; and (3) joint opinion expression and polarity classification.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.668326755364736}, {"text": "holder extraction", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.6459876745939255}, {"text": "joint opinion expression and polarity classification", "start_pos": 138, "end_pos": 190, "type": "TASK", "confidence": 0.5958361625671387}]}, {"text": "Finally, the polarity-based opinion extraction system was used in an extrinsic evaluation: document polarity classification of movie reviews.", "labels": [], "entities": [{"text": "polarity-based opinion extraction", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.6671046117941538}, {"text": "extrinsic evaluation: document polarity classification of movie reviews", "start_pos": 69, "end_pos": 140, "type": "TASK", "confidence": 0.707391361395518}]}, {"text": "To generate the training data for the rerankers, we carried out a 5-fold hold-out procedure: We split the training set into five pieces, trained a sequence labeler and secondary classifiers on pieces 1-4, applied them to piece 5, and soon.", "labels": [], "entities": []}, {"text": "Because expression boundaries are hard to define rigorously (Wiebe, Wilson, and Cardie 2005), our evaluations mainly used intersection-based precision and recall measures to score the quality of the system output.", "labels": [], "entities": [{"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9700517654418945}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9856895208358765}]}, {"text": "The idea is to assign values between 0 and 1, as opposed to traditional precision and recall where a span is counted as either correctly or incorrectly detected.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9989258646965027}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9873195886611938}]}, {"text": "We thus define the span coverage c of a span s (a set of token indices) with respect to another span s \u2032 , which measures how well s \u2032 is covered by s: In this formula, |s| means the length of the span s, and the intersection \u2229 gives the set of token indices that two spans have in common.", "labels": [], "entities": []}, {"text": "Because our evaluation takes span labels (DSE, ESE, OSE) into account, we set c(s, s \u2032 ) to zero if the labels associated with sand s \u2032 are different.", "labels": [], "entities": [{"text": "ESE", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.6930927038192749}, {"text": "OSE", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.7212203741073608}]}, {"text": "Using the span coverage, we define the span set coverage C of a set of spans SS S with respect to a set S \u2032 S \u2032 S \u2032 : We now define the intersection-based precision P and recall R of a proposed set of spans\u02c6S\u02c6S\u02c6S spans\u02c6 spans\u02c6Sspans\u02c6S\u02c6 spans\u02c6S\u02c6Sspans\u02c6S\u02c6S\u02c6 spans\u02c6S\u02c6S\u02c6S with respect to a gold standard set SS S as follows: Note that in this formula, |S| means the number of spans in a set S.", "labels": [], "entities": [{"text": "intersection-based precision P", "start_pos": 136, "end_pos": 166, "type": "METRIC", "confidence": 0.6511330207188925}]}, {"text": "Conventionally, when measuring the quality of a system for an information extraction task, a predicted entity is counted as correct if it exactly matches the boundaries of a corresponding entity in the gold standard; there is thus no reward for close matches.", "labels": [], "entities": [{"text": "information extraction task", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.8142324686050415}]}, {"text": "Because the boundaries of the spans annotated in the MPQA corpus are not strictly defined in the annotation guidelines, however, measuring precision and recall using exact boundary scoring will result in figures that are too low to be indicative of the usefulness of the system.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9621191024780273}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9992824196815491}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9985899329185486}]}, {"text": "Therefore, most work using this corpus instead use overlap-based precision and recall measures, where a span is counted as correctly detected if it overlaps with a span in the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9656862020492554}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9957051873207092}]}, {"text": "As pointed out by, this is problematic because it will tend to reward long spans-for instance, a span covering the whole sentence will always be counted as correct if the gold standard contains any span for that sentence.", "labels": [], "entities": []}, {"text": "Conversely, the overlap metric does not give higher credit to a span that is perfectly detected than to one that has a very low overlap with the gold standard.", "labels": [], "entities": [{"text": "overlap", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9692521691322327}]}, {"text": "The precision and recall measures proposed here correct the problem with overlapbased measures: If the system proposes a span covering the whole sentence, the span coverage will below and result in a low soft precision, and a low soft recall will be assigned if only a small part of a gold standard span is covered.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9989933371543884}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9948733448982239}, {"text": "recall", "start_pos": 235, "end_pos": 241, "type": "METRIC", "confidence": 0.7962491512298584}]}, {"text": "Note that our measures are bounded below by the exact measures and above by the overlap-based measures.", "labels": [], "entities": []}, {"text": "The first task we considered was the extraction of opinion expression (labeled with expression types).", "labels": [], "entities": [{"text": "extraction of opinion expression", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.8102715462446213}]}, {"text": "We first studied the impact of the machine learning method and hypothesis set size on the reranker performance.", "labels": [], "entities": []}, {"text": "Then, we carried out an analysis of the effectiveness of the features used by the reranker.", "labels": [], "entities": []}, {"text": "We finally compared the performance of the expression extraction system with previous work.", "labels": [], "entities": [{"text": "expression extraction", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7856924831867218}]}, {"text": "61.8 \u00b1 1.5 52.5 \u00b1 1.3 56.8 \u00b1 1.1 Perceptron 62.8 \u00b1 1.5 48.1 \u00b1 1.3 54.5 \u00b1 1.2 Passive-Aggressive 63.5 \u00b1 1.5 51.8 \u00b1 1.3 57.0 \u00b1 1.1 7.2.1 Evaluation of Machine Learning Methods.", "labels": [], "entities": []}, {"text": "We compared the machine learning methods described in Section 5.", "labels": [], "entities": []}, {"text": "In these experiments, we used a hypothesis set size k of 8.", "labels": [], "entities": []}, {"text": "All features from Section 6.2 were used.", "labels": [], "entities": [{"text": "Section 6.2", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.8403722941875458}]}, {"text": "shows the results of the evaluations using the precision and recall measures described earlier.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.999388575553894}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.993320882320404}]}, {"text": "The baseline is the result of taking the top-scoring labeling from the base sequence labeler.", "labels": [], "entities": [{"text": "baseline", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9644591212272644}]}, {"text": "We note that the margin-based methods-structured SVM and the on-line PA algorithm-outperform the perceptron soundly, which shows the benefit of learning methods that make use of the cost function \u2206.", "labels": [], "entities": []}, {"text": "Comparing the two best-performing learning methods, we note that the reranker using the structured SVM is more recalloriented whereas the PA-based reranker more precision-oriented; the difference in F-measure is not statistically significant.", "labels": [], "entities": [{"text": "recalloriented", "start_pos": 111, "end_pos": 125, "type": "METRIC", "confidence": 0.9686821103096008}, {"text": "precision-oriented", "start_pos": 161, "end_pos": 179, "type": "METRIC", "confidence": 0.9755502343177795}, {"text": "F-measure", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9879675507545471}]}, {"text": "In the remainder of this article, all rerankers are trained using the PA learning algorithm (with the same parameters) because its training process is much faster than that of the structured SVM.", "labels": [], "entities": []}, {"text": "As an extrinsic evaluation of the opinion expression extraction system, we evaluated the impact of the expressions on a practical application: extraction of evaluations of attributes from product reviews.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.7417082985242208}, {"text": "extraction of evaluations of attributes from product reviews", "start_pos": 143, "end_pos": 203, "type": "TASK", "confidence": 0.8375715836882591}]}, {"text": "We first describe the collection we used and then the implementation of the extractor.", "labels": [], "entities": []}, {"text": "We used the annotated data set by for the experiments in extraction of attribute evaluations from product reviews.", "labels": [], "entities": [{"text": "extraction of attribute evaluations from product reviews", "start_pos": 57, "end_pos": 113, "type": "TASK", "confidence": 0.8533453345298767}]}, {"text": "The collection contains reviews of five products: one DVD player, two cameras, one MP3 player, and one cellular phone.", "labels": [], "entities": []}, {"text": "In this data set, every sentence is associated with a set of attribute evaluations.", "labels": [], "entities": []}, {"text": "An evaluation consists of an attribute name and an evaluation value between \u22123 and +3, where \u22123 means a strongly negative evaluation and +3 strongly positive.", "labels": [], "entities": []}, {"text": "For instance, the sentence this player boasts a decent size and weight, a relatively-intuitive navigational system that categorizes based on id3 tags, and excellent sound is tagged with the attribute evaluations size +2, weight +2, navigational system +2, sound +2.", "labels": [], "entities": []}, {"text": "In this work, we do not make use of the exact value of the evaluation but only its sign.", "labels": [], "entities": []}, {"text": "We removed the product attribute mentions in the form of anaphoric pronouns referring to entities mentioned in previous sentences; these cases are directly marked in the data set.", "labels": [], "entities": []}, {"text": "We considered two problems: (1) extraction of attribute evaluations without taking the polarity into account, and (2) extraction with polarity (positive or negative).", "labels": [], "entities": [{"text": "extraction of attribute evaluations", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8370358496904373}]}, {"text": "The former is modeled as a binary classifier that tags each word in the review (except the punctuation) as an evaluation or not, and the latter requires the definition of a three-class polarity classifier.", "labels": [], "entities": []}, {"text": "For both tasks, we compared three feature sets: a baseline using simple features, a stronger baseline using a lexicon, and finally a system using features derived from opinion expressions.", "labels": [], "entities": []}, {"text": "Similarly to the opinion expression polarity classifier, we implemented the classifiers as SVMs that we trained using LIBLINEAR.", "labels": [], "entities": [{"text": "opinion expression polarity classifier", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.7185646444559097}, {"text": "LIBLINEAR", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.7590014934539795}]}, {"text": "For the extraction task without polarities, the best results were obtained using an L2-regularized L2-loss SVM and a C value of 0.1.", "labels": [], "entities": []}, {"text": "For the polarity task, we used a multiclass SVM (Crammer and Singer 2001) with the same parameters.", "labels": [], "entities": []}, {"text": "To handle the precision/recall tradeoff, we varied the class weighting for the null class.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9987409710884094}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9702116250991821}]}, {"text": "The baseline classifier used features based on lexical information (word, POS tag, and lemma) in a window of size 3 around the word under consideration (the focus word).", "labels": [], "entities": []}, {"text": "In addition, it had two features representing the overall sentence polarities.", "labels": [], "entities": []}, {"text": "To compute the polarities, we trained bag-of-words classifiers following the implementation by.", "labels": [], "entities": []}, {"text": "Two separate classifiers were used: one for positive and one for negative polarity.", "labels": [], "entities": []}, {"text": "Note that these classifiers detect the presence of positive or negative polarity, which may thus occur in the same sentence.", "labels": [], "entities": []}, {"text": "The classifiers were trained on the MPQA corpus, where we counted a sentence as positive if it contained a positive opinion expression with an intensity of at least MEDIUM, and conversely for the negative polarity.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9696353077888489}, {"text": "MEDIUM", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9986905455589294}]}, {"text": "Ina second extrinsic evaluation of the opinion expression extractor, we investigated how expression-based features affect the performance of a document-level polarity classifier of reviews as positive or negative.", "labels": [], "entities": [{"text": "opinion expression extractor", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7498927513758341}]}, {"text": "We followed the same evaluation protocol as in the first extrinsic evaluation, where we compare three classifiers of increasing complexity: (1) a baseline using a pure word-based representation, (2) a stronger baseline adding features derived from a sentiment lexicon, and (3) a classifier with features extracted from opinion expressions.", "labels": [], "entities": []}, {"text": "The task of categorizing a full document as positive or negative can be viewed as a document categorization task, and this has led to the application of standard text categorization techniques (Pang, Lee, and Vaithyanathan 2002).", "labels": [], "entities": []}, {"text": "We followed this approach and implemented the document polarity classifier as a binary linear SVM; this learning method has along tradition of successful application in text categorization.", "labels": [], "entities": [{"text": "document polarity classifier", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.725288858016332}, {"text": "text categorization", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7726013660430908}]}, {"text": "For these experiments, we used six collections.", "labels": [], "entities": []}, {"text": "The first one consisted of movie reviews written in English extracted from the Web by.", "labels": [], "entities": []}, {"text": "This data set is an extension of a smaller set (Pang, Lee, and Vaithyanathan 2002) that has been used in a large number of experiments.", "labels": [], "entities": []}, {"text": "The remaining five sets consisted of product reviews gathered by.", "labels": [], "entities": []}, {"text": "We used five of the largest subsets: reviews of DVDs, software, books, music, and cameras.", "labels": [], "entities": []}, {"text": "In all six collections, 1,000 documents were labeled by humans as positive and 1,000 as negative.", "labels": [], "entities": []}, {"text": "Following, the documents were represented as bag-of-word feature vectors based on presence features for individual words.", "labels": [], "entities": []}, {"text": "No weighting such as IDF was used.", "labels": [], "entities": [{"text": "IDF", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9848604798316956}]}, {"text": "The vectors were normalized to unit length.", "labels": [], "entities": []}, {"text": "Again, we trained the SVMs using LIBLINEAR, and the best results were obtained using an L2-regularized L2-loss version of the SVM with a C value of 1. 7.6.1 Features Based on the Subjectivity Lexicon.", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9330231547355652}]}, {"text": "We used features based on the subjectivity lexicon by that we used for opinion expression segmentation in Section 4.1 and for polarity classification in Section 4.3.", "labels": [], "entities": [{"text": "opinion expression segmentation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6205147604147593}, {"text": "polarity classification", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.7929552495479584}]}, {"text": "For every word whose lemma is listed in the lexicon, we added a feature consisting of the word and its prior polarity and intensity to the bag-of-words feature vector.", "labels": [], "entities": []}, {"text": "The feature examples are taken from the sentence HRW has denounced the defenseless situation of these prisoners, where denounce is listed in the lexicon as strong/negative and prisoner as weak/negative.", "labels": [], "entities": [{"text": "HRW", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8697088360786438}]}, {"text": "To evaluate the performance of the document polarity classifiers, we carried out a 10-fold cross-validation procedure for every review collection.", "labels": [], "entities": [{"text": "document polarity classifiers", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.7308837374051412}]}, {"text": "We evaluated three classifiers: one using only bag-of-words features (\"Baseline\"); one using features extracted from the subjectivity lexicon (\"Lexicon\"); and finally one also using the expression-based features (\"Expressions\").", "labels": [], "entities": []}, {"text": "In order to abstract away from the tuning threshold, the performances were measured using AUC, the area under ROC curve.", "labels": [], "entities": [{"text": "AUC", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9937264919281006}, {"text": "ROC", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.8688738942146301}]}, {"text": "The AUC values are given in.", "labels": [], "entities": [{"text": "AUC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9806264042854309}]}, {"text": "These evaluations show that the classifier adding features extracted from the opinion expressions significantly outperforms the classifier using only a bag-of-words feature representation and also that using the lexicon-based features.", "labels": [], "entities": []}, {"text": "This demonstrates that the extraction and disambiguation of opinion expressions in their context is useful fora coarse-grained task such as document polarity classification.", "labels": [], "entities": [{"text": "document polarity classification", "start_pos": 140, "end_pos": 172, "type": "TASK", "confidence": 0.8070004185040792}]}, {"text": "The differences in AUC values between the two best configurations are statistically significant (p < 0.005 for all six collections).", "labels": [], "entities": [{"text": "AUC", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9772870540618896}]}, {"text": "In addition, we show the precision/recall plots in; we see that for all six collections, the expression-based set-up outperforms the other two near the precision/recall breakeven point.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9969754219055176}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.5550761222839355}, {"text": "precision/recall breakeven point", "start_pos": 152, "end_pos": 184, "type": "METRIC", "confidence": 0.8030177474021911}]}, {"text": "The collection where we can seethe most significant difference is the movie review set.", "labels": [], "entities": [{"text": "movie review set", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.8426257371902466}]}, {"text": "The main difference of this collection compared with the other collections is that its documents are larger: The average size of a document here is about four times larger than in the other collections.", "labels": [], "entities": []}, {"text": "In addition, its reviews often contain large sections that are purely factual in nature, mainly plot descriptions.", "labels": [], "entities": []}, {"text": "The opinion expression identification maybe seen as away to process the document to highlight the interesting parts on which the classifier should focus.", "labels": [], "entities": [{"text": "opinion expression identification", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.6829036672910055}]}], "tableCaptions": [{"text": " Table 1  Statistics for the training and test splits of the MPQA collection.", "labels": [], "entities": [{"text": "MPQA collection", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.97701296210289}]}, {"text": " Table 2  Evaluation of reranking learning methods.", "labels": [], "entities": []}, {"text": " Table 3  Oracle and reranker performance as a function of candidate set size.", "labels": [], "entities": []}, {"text": " Table 4  Investigation of the contribution of syntactic features.", "labels": [], "entities": []}, {"text": " Table 5  Investigation of the contribution of semantic features.", "labels": [], "entities": []}, {"text": " Table 6  Structural features compared to label pairs.", "labels": [], "entities": []}, {"text": " Table 7  Performance depending on the type of expression.", "labels": [], "entities": []}, {"text": " Table 8  Results using the evaluation setting from Breck, Choi, and Cardie (2007).", "labels": [], "entities": []}, {"text": " Table 11  Detailed opinion holder extraction results.", "labels": [], "entities": [{"text": "Detailed opinion holder extraction", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.8038501739501953}]}, {"text": " Table 12  Opinion holder extraction results for external holders.", "labels": [], "entities": [{"text": "Opinion holder extraction", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.6106732686360677}]}, {"text": " Table 13  Overall evaluation of polarity-labeled opinion expression extraction.", "labels": [], "entities": [{"text": "polarity-labeled opinion expression extraction", "start_pos": 33, "end_pos": 79, "type": "TASK", "confidence": 0.6508706212043762}]}, {"text": " Table 14  Intersection-based evaluation for individual polarity values.", "labels": [], "entities": []}, {"text": " Table 15  Overlap-based evaluation for individual polarity values, and comparison with the results  reported by Choi and Cardie (2010).", "labels": [], "entities": []}, {"text": " Table 15. The table shows our baseline and integrated systems  along with the figures 5 from Choi and Cardie. Instead of a single value for all polarities,  we show the performance for every individual polarity value (POSITIVE, NEUTRAL,  NEGATIVE). This evaluation uses the overlap metric instead of the intersection-based  one. As we have pointed out, we use the overlap metric for compatibility although it is  problematic.", "labels": [], "entities": [{"text": "POSITIVE", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9779494404792786}, {"text": "NEUTRAL", "start_pos": 229, "end_pos": 236, "type": "METRIC", "confidence": 0.8941755890846252}, {"text": "NEGATIVE", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.843171238899231}]}, {"text": " Table 16  Product attribute evaluation extraction performance.", "labels": [], "entities": [{"text": "Product attribute evaluation extraction", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.6474523171782494}]}]}