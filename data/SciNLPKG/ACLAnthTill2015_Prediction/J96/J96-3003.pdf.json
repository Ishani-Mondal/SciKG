{"title": [{"text": "Efficient Multilingual Phoneme-to-Grapheme Conversion Based on HMM", "labels": [], "entities": [{"text": "Multilingual Phoneme-to-Grapheme Conversion", "start_pos": 10, "end_pos": 53, "type": "TASK", "confidence": 0.5827917953332266}, {"text": "HMM", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.4106438457965851}]}], "abstractContent": [{"text": "Grapheme-to-phoneme conversion (GTPC) has been achieved inmost European languagesby dictionary look-up or using rules.", "labels": [], "entities": [{"text": "Grapheme-to-phoneme conversion (GTPC)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7319164037704468}]}, {"text": "The application of these methods, however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems, especially in inflectionally rich languages.", "labels": [], "entities": [{"text": "phoneme-to-grapheme conversion [PTGC", "start_pos": 78, "end_pos": 114, "type": "TASK", "confidence": 0.7900093644857407}]}, {"text": "In this paper the PTGC problem is approached from a completely different point of view.", "labels": [], "entities": [{"text": "PTGC", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.6962715983390808}]}, {"text": "Instead of rules or a dictionary, the statistics of language connecting pronunciation to spelling are exploited.", "labels": [], "entities": []}, {"text": "The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "The PTGC system has been established and tested on various multilingual corpora.", "labels": [], "entities": [{"text": "PTGC system", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8614261746406555}]}, {"text": "Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word.", "labels": [], "entities": []}, {"text": "Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones).", "labels": [], "entities": [{"text": "PTGC", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8928572535514832}]}, {"text": "This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates)for most of the seven languages it was tested on (Dutch, English, French, German, Greek, Italian, and Spanish).", "labels": [], "entities": []}, {"text": "The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7042511999607086}]}], "introductionContent": [{"text": "Phoneme-based speech recognition systems incorporate a phoneme-to-grapheme conversion (PTGC) module to produce orthographically correct output.", "labels": [], "entities": [{"text": "Phoneme-based speech recognition", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7185637752215067}, {"text": "phoneme-to-grapheme conversion (PTGC)", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.7064953207969665}]}, {"text": "Many approaches have been used, most of which compare the phonemic strings to a (usually applicationspecific) dictionary containing both the phonemic and the graphemic form of every word the system can handle).", "labels": [], "entities": []}, {"text": "Considering the effort and cost required to create such a dictionary, this is a serious limitation, especially for inflectionally rich languages such as Greek and German.", "labels": [], "entities": []}, {"text": "Another very important issue when searching for words in a dictionary is the number of candidates resulting from each phonemic input.", "labels": [], "entities": []}, {"text": "Depending on the language and the errors of the recognizer, this number maybe very large, rendering the disambiguation of the words by a subsequent language model a time-consuming and unreliable task.", "labels": [], "entities": []}, {"text": "The domain of application is another factor that strongly influences conversion performance; a general dictionary can omit the specialized words of specific domains (e.g., legal, engineering, or medical terminology) and vice versa.", "labels": [], "entities": [{"text": "conversion", "start_pos": 69, "end_pos": 79, "type": "TASK", "confidence": 0.9756487011909485}]}, {"text": "Finally, applications that must handle a large number of proper names (e.g., directory service applications) generally cannot include all the possible names.", "labels": [], "entities": []}, {"text": "The only remedy in such situations would be to increase the size of the reference dictionary, so that every possible input word is included.", "labels": [], "entities": []}, {"text": "A final consideration is the type of errors a dictionary-based PTGC system introduces when it encounters a word that is not contained in the dictionary: the system will produce the closest existing word (in its dictionary) as the best candidate, which may give a completely incomprehensible (if not wrong) meaning to the input phrase.", "labels": [], "entities": []}, {"text": "Another approach to phoneme-to-grapheme conversion is the use of linguistic and/or heuristic rules (.", "labels": [], "entities": [{"text": "phoneme-to-grapheme conversion", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7060176581144333}]}, {"text": "This method works on a phoneme or syllable basis and can give adequate results in languages where the spelling is very similar to the pronunciation (such as Italian).", "labels": [], "entities": []}, {"text": "Nevertheless, languages with diphthongs or double letters cannot benefit from this method, since it creates long lists of homophonic candidates that are all correct (in the sense that they are pronounced as the input word) but that do not exist in the language.", "labels": [], "entities": []}, {"text": "In Greek, for example, where the phoneme/i/is the sound of five different graphemes (~, z, v, ~, o~) and the phoneme /1/can come from ~ and ;~;~, the phonemic form/m'ila/would produce a list containing the following 10 transcriptions: #i)~c~, #~&c~, #(;Ae~, l~i&o~, #oi&~, #i)~&c~, #~&&c~, #~)~&a, #ci&o~, and #oi&&e~ all having the same pronunciation.", "labels": [], "entities": []}, {"text": "From this list, only two represent existing orthographically correct words; \"#i),c~\" 'speak!' and \"#~)~c~\" 'apples.'", "labels": [], "entities": []}, {"text": "Previous work has shown that an average of 30 graphernic candidates is produced by this transcription for every input phonemic word.", "labels": [], "entities": []}, {"text": "To overcome the disadvantages of the above mentioned methods, a novel statistical approach to the problem of PTGC, which is based on hidden Markov models (HMM), has been investigated and is presented in this paper.", "labels": [], "entities": [{"text": "PTGC", "start_pos": 109, "end_pos": 113, "type": "TASK", "confidence": 0.58475261926651}]}, {"text": "Although statistical approaches have already been widely applied in several fields of natural language processing, they have not been considered for PTGC.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6642371813456217}, {"text": "PTGC", "start_pos": 149, "end_pos": 153, "type": "TASK", "confidence": 0.5925796627998352}]}, {"text": "The proposed method is language independent, does not use a dictionary, and can be applied with only minimal linguistic knowledge, thus reducing the cost of system development.", "labels": [], "entities": []}, {"text": "Initially, the first-order HMM and the common Viterbi algorithm were used to provide a simple transcription for each input word.", "labels": [], "entities": []}, {"text": "In its current version, the method is based on second-order HMM and on a modified Viterbi algorithm, which can provide more than one graphemic output for each phonemic input, in descending order of probability.", "labels": [], "entities": []}, {"text": "The multiple outputs make it possible to apply a language model in sentence level for disambiguation at a subsequent stage.", "labels": [], "entities": []}, {"text": "This version of the algorithm raised the number of correctly transcribed phonemes to 97%-100% for most of the languages the system was tested on.", "labels": [], "entities": []}, {"text": "The proposed system assumes that the word boundaries are known; that is, it is a subsequent stage in an isolated-word speech recognition system.", "labels": [], "entities": [{"text": "isolated-word speech recognition", "start_pos": 104, "end_pos": 136, "type": "TASK", "confidence": 0.7385736107826233}]}, {"text": "The PTGC method can work as a stand-alone module or in co-operation with a look-up module with a small to moderate size dictionary containing the most common words of the language.", "labels": [], "entities": []}, {"text": "In the latter case, the look-up module employs a distance threshold: when the difference between the input and the words in the dictionary is greater than this threshold, control is passed to the HMM system, which converts the input phoneme string to graphemes.", "labels": [], "entities": []}, {"text": "The basic theory, the pilot implementation, and the proposed final system are presented in Section 2.", "labels": [], "entities": []}, {"text": "The evaluation procedure and the error-measure methodology are described in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, the experimental results of the system are presented and the nature of the errors is discussed.", "labels": [], "entities": []}, {"text": "The multilingual aspects of the algorithm and experimental results for seven languages are also given in this section.", "labels": [], "entities": []}, {"text": "Finally, some conclusions are drawn about the system and topics for further research and hardware implementation are discussed in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all languages tested, the models were created using full-form dictionaries setup during the EEC ESPRIT project 291/860 \"Linguistic Analysis of the European Languages\") from corpora of about 300,000 words.", "labels": [], "entities": [{"text": "EEC ESPRIT project 291/860 \"Linguistic Analysis of the European Languages\")", "start_pos": 96, "end_pos": 171, "type": "DATASET", "confidence": 0.7108321466616222}]}, {"text": "These dictionaries cover three domains: office environment, newspapers, and law.", "labels": [], "entities": []}, {"text": "The input to the conversion process was separate 10,000 word texts not included in the training dictionaries.", "labels": [], "entities": [{"text": "conversion", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.9672659039497375}]}, {"text": "The testing material was taken from the above domains.", "labels": [], "entities": []}, {"text": "Furthermore, for Greek, two additional dictionaries of proper names provided by the ONOMASTICA (LRE 61004) project were used for training and testing the algorithm in a name directory environment.", "labels": [], "entities": [{"text": "ONOMASTICA (LRE 61004) project", "start_pos": 84, "end_pos": 114, "type": "DATASET", "confidence": 0.7759591142336527}]}, {"text": "This was done to get a more accurate indication of the system's performance in applications where a complete dictionary can never be available.", "labels": [], "entities": []}, {"text": "A second set of training and testing material was created from the above using a phoneme confusion matrix that simulated the output of a speech recognizer.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7108529508113861}]}, {"text": "The confusion matrix relates the input (correct) with the output (corrupted) phonemes employing probabilities of the form mq = P(Oout = Pj I Oin ~-Pi).", "labels": [], "entities": []}, {"text": "The texts used for the training and testing phase were corrupted according to these probabilities.", "labels": [], "entities": []}, {"text": "Different confusion matrices were applied to show the degradation of the performance as a function of the input phonemic corruption.", "labels": [], "entities": []}, {"text": "In Section 4.2, the two sets of results are presented and compared.", "labels": [], "entities": []}, {"text": "Finally, one more experiment per language was performed using a first-order HMM, so that the ambiguity in each of the languages tested could be revealed and a comparison of the performance of the two HMM models could be made.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 9  Conversion results for Dutch.", "labels": [], "entities": []}, {"text": " Table 10  Conversion results for English.", "labels": [], "entities": []}, {"text": " Table 11  Conversion results for French.", "labels": [], "entities": [{"text": "French", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.957662045955658}]}, {"text": " Table 12  Conversion results for German.", "labels": [], "entities": []}, {"text": " Table 13  Conversion results for German with no capital letters.", "labels": [], "entities": []}, {"text": " Table 14  Conversion results for Greek.", "labels": [], "entities": []}, {"text": " Table 15  Conversion results for Italian.", "labels": [], "entities": []}, {"text": " Table 16  Conversion results for Spanish.", "labels": [], "entities": []}, {"text": " Table 17  Conversion results for names.", "labels": [], "entities": []}]}