{"title": [{"text": "A Stochastic Finite-State Word-Segmentation Algorithm for Chinese Chilin Shih~ Bell Laboratories", "labels": [], "entities": [{"text": "Bell Laboratories", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9092044234275818}]}], "abstractContent": [{"text": "The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7435452044010162}, {"text": "tokenization of the input into words", "start_pos": 73, "end_pos": 109, "type": "TASK", "confidence": 0.7890953024228414}]}, {"text": "For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.", "labels": [], "entities": []}, {"text": "In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to \"reconstruct\" the word-boundary information.", "labels": [], "entities": []}, {"text": "In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.", "labels": [], "entities": []}, {"text": "The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and-since the primary intended application of this model is to text-to-speech synthesis-provides pronunciations for these words.", "labels": [], "entities": [{"text": "text-to-speech synthesis-provides pronunciations", "start_pos": 178, "end_pos": 226, "type": "TASK", "confidence": 0.825834055741628}]}, {"text": "We evaluate the system's performance by comparing its segmentation 'Tudgments\" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.", "labels": [], "entities": []}, {"text": "1. The Problem Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TTS).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7599289417266846}, {"text": "information retrieval", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.7801131010055542}, {"text": "text-to-speech synthesis (TTS)", "start_pos": 217, "end_pos": 247, "type": "TASK", "confidence": 0.8028624176979064}]}, {"text": "An initial step of any text-analysis task is the tokenization of the input into words.", "labels": [], "entities": [{"text": "tokenization of the input into words", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.8172514239947001}]}, {"text": "For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.", "labels": [], "entities": []}, {"text": "Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces.", "labels": [], "entities": []}, {"text": "A moment's reflection will reveal that things are not quite that simple.", "labels": [], "entities": []}, {"text": "There are clearly eight orthographic words in the example given, but if one were doing syntactic analysis one would probably want to consider I'm to consist of two syntactic words, namely I and am.", "labels": [], "entities": []}, {"text": "If one is interested in translation, one would probably want to consider show up as a single dictionary word since its semantic interpretation is not trivially derivable from the meanings of show and up.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9810912609100342}]}, {"text": "And if one is interested in TTS, one would probably consider the single orthographic word ACL to consist of three phonological words-/eJ s'i ~l/-corresponding to the pronunciation of each of the letters in the acronym.", "labels": [], "entities": [{"text": "TTS", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9612036943435669}]}], "introductionContent": [], "datasetContent": [{"text": "In this section we present a partial evaluation of the current system, in three parts.", "labels": [], "entities": []}, {"text": "The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis.", "labels": [], "entities": [{"text": "proper-name identification", "start_pos": 146, "end_pos": 172, "type": "TASK", "confidence": 0.7634040713310242}]}, {"text": "To date we have not done a separate evaluation of foreign-name recognition.", "labels": [], "entities": [{"text": "foreign-name recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7951893508434296}]}, {"text": "Evaluation of the Segmentation as a Whole.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9355126619338989}]}, {"text": "Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.5844795107841492}, {"text": "precision-recall", "start_pos": 143, "end_pos": 159, "type": "METRIC", "confidence": 0.9767208695411682}]}, {"text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.", "labels": [], "entities": []}, {"text": "Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.", "labels": [], "entities": []}, {"text": "To this end, we picked 100 sentences at random containing 4,372 total hanzi from a test corpus.", "labels": [], "entities": []}, {"text": "14 (There were 487 marks of punctuation in the test sentences, including the sentence-final periods, meaning that the average inter-punctuation distance was about 9 hanzi.)", "labels": [], "entities": []}, {"text": "We asked six native speakers--three from Taiwan (T1-T3), and three from the Mainland (M1-M3)--to segment the corpus.", "labels": [], "entities": []}, {"text": "Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud.", "labels": [], "entities": []}, {"text": "An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units.", "labels": [], "entities": []}, {"text": "(See also Wu and Fung.)", "labels": [], "entities": []}, {"text": "Various segmentation approaches were then compared with human performance: . .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  The cost as a novel given name (second position) for hanzi  from various radical classes.", "labels": [], "entities": []}]}