{"title": [{"text": "The Effects of Lexical Specialization on the Growth Curve of the Vocabulary", "labels": [], "entities": [{"text": "Lexical Specialization", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.8814254701137543}]}], "abstractContent": [{"text": "The number of different words expected on the basis of the urn model to appear in, for example, the first half of a text, is known to overestimate the observed number of different words.", "labels": [], "entities": []}, {"text": "This paper examines the source of this overestimation bias.", "labels": [], "entities": []}, {"text": "It is shown that this bias does not arise due to sentence-bound syntactic constraints, but that it is a direct consequence of topic cohesion in discourse.", "labels": [], "entities": []}, {"text": "The nonrandom, clustered appearance of lexically specialized words, often the key words of the text, explains the main trends in the overestimation bias both quantitatively and qualitatively.", "labels": [], "entities": []}, {"text": "The effects of nonrandomness are so strong that they introduce an overestimation bias in distributions of units derived from words, such as syllables and digrams.", "labels": [], "entities": []}, {"text": "Nonrandom word usage also affects the accuracy of the Good-Turing frequency estimates which,for the lowest frequencies, reveal a strong underestimation bias.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9994915723800659}]}, {"text": "A heuristic adjusted frequency estimate is proposed that, at least for novel-sized texts, is considerably more accurate.", "labels": [], "entities": []}], "introductionContent": [{"text": "When reading through a text, word token byword token, the number of different word types encountered increases, quickly at first, and evermore slowly as one progresses through the text.", "labels": [], "entities": []}, {"text": "The number of different word types encountered after reading N tokens, the vocabulary size V(N), is a function of N.", "labels": [], "entities": []}, {"text": "Analytical expressions for V(N) based on the urn model are available.", "labels": [], "entities": []}, {"text": "A classic problem in word frequency studies is, however, that these analytical expressions tend to overestimate the observed vocabulary size, irrespective of whether these expressions are nonparametric or parametric in nature.", "labels": [], "entities": []}, {"text": "Although the theoretical or expected vocabulary size E[V(N)] generally is of the same order of magnitude as the observed vocabulary size, the lack of precision one observes time and again casts serious doubt on the reliability of a number of measures in word frequency statistics.", "labels": [], "entities": [{"text": "vocabulary size E[V(N)]", "start_pos": 37, "end_pos": 60, "type": "METRIC", "confidence": 0.8023459501564503}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9949151277542114}]}, {"text": "For instance, and exploit the Good-Turing estimate for the probability of sampling unseen types) to develop measures for the degree of productivity of affixes, Baayen and Sproat (to appear) apply this Good-Turing estimate to obtain enhanced estimates of lexical priors for unseen words, and the Good-Turing estimates also play an important role for estimating population probabilities.", "labels": [], "entities": []}, {"text": "If a simple random variable such as the vocabulary size reveals consistent and significant deviation from its expectation, the accuracy of the Good-Turing estimates is also called into question.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9995044469833374}]}, {"text": "The aim of this paper is to understand why this deviation between the-ory and observation arises in word frequency distributions, and in this light evaluate applications of the Good-Turing results.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, I introduce some basic notation and the expressions for the growth curve of the vocabulary with which we will be concerned throughout, including a model proposed by, which, by introducing a smoothing parameter, leads to much-improved fits.", "labels": [], "entities": []}, {"text": "Unfortunately, this model is based on a series of unrealistic simplifications, and cannot serve as an explanation for the divergence between the observed and expected vocabulary size.", "labels": [], "entities": []}, {"text": "In Section 3, therefore, I consider a number of possible sources for the misfit in greater detail: nonrandomness at the sentence level due to syntactic structure, nonrandomness due to the discourse structure of the text as a whole, and nonrandomness due to thematic cohesion in restricted sequences of sentences (paragraphs).", "labels": [], "entities": []}, {"text": "Section 4 traces the implications of the results obtained for distributions of units derived from words, such as syllables and digrams, and examines the accuracy of the Good-Turing frequency estimates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9991601705551147}]}, {"text": "A list of symbols is provided at the end of the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}