{"title": [], "abstractContent": [{"text": "A fundamental debate in the machine learning of language has been the role of prior knowledge in the learning process.", "labels": [], "entities": [{"text": "machine learning of language", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.794157437980175}]}, {"text": "Purely nativist approaches, such as the Principles and Parameters model, build parameterized linguistic generalizations directly into the learning system.", "labels": [], "entities": []}, {"text": "Purely empirical approaches use a general, domain-independent learning rule (Error Back-Propagation, Instance-based Generalization, Minimum Description Length) to learn linguistic generalizations directly from the data.", "labels": [], "entities": [{"text": "Minimum Description Length)", "start_pos": 132, "end_pos": 159, "type": "METRIC", "confidence": 0.7107027098536491}]}, {"text": "In this paper we suggest that an alternative to the purely nativist or purely empiricist learning paradigms is to represent the prior knowledge of language as a set of abstract learning biases, which guide an empirical inductive learning algorithm.", "labels": [], "entities": []}, {"text": "We test our idea by examining the machine learning of simple Sound Pattern of English (S P E)-style phonological rules.", "labels": [], "entities": []}, {"text": "We represent phonological rules as finite-state transducers that accept underlying forms as input and generate surface forms as output.", "labels": [], "entities": []}, {"text": "We show that OSTIA, a general-purpose transducer induction algorithm, was incapable of learning simple phonological rules like flapping.", "labels": [], "entities": [{"text": "transducer induction algorithm", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.7248888115088145}]}, {"text": "We then augmented OSTIA with three kinds of learning biases that are specific to natural language phonology, and that are assumed explicitly or implicitly by every theory of phonology: faithfulness (underlying segments tend to be realized similarly on the surface), community (similar segments behave similarly), and context (phonological rules need access to variables in their context).", "labels": [], "entities": []}, {"text": "These biases are so fundamental to generative phonology that they are left implicit in many theories.", "labels": [], "entities": [{"text": "generative phonology", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.9343648254871368}]}, {"text": "But explicitly modifying the OSTIA algorithm with these biases allowed it to learn more compact, accurate, and general transducers, and our implementation successfully learns a number of rules from English and German.", "labels": [], "entities": []}, {"text": "Furthermore, we show that some of the remaining errors in our augmented model are due to implicit biases in the traditional SPE-style rewrite system that are not similarly represented in the transducer formalism, suggesting that while transducers maybe formally equivalent to SPE-style rules, they may not have identical evaluation procedures.", "labels": [], "entities": []}, {"text": "Because our biases were applied to the learning of very simple SPE-style rules, and to a non-psychologically-motivated and nonprobabilistic theory of purely deterministic transducers, we do not expect that our model as implemented has any practical use as a phonological learning device, nor is it intended as a cognitive model of human learning.", "labels": [], "entities": []}, {"text": "Indeed, because of the noise and nondeterminism inherent to linguistic data, we feel strongly that stochastic algorithms for language induction are much more likely to be a fruitful research direction.", "labels": [], "entities": [{"text": "language induction", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7182942181825638}]}, {"text": "Our model is rather intended to suggest the kind of biases that maybe added to other empiricist induction models, and the way in which they maybe added, in order to build a cognitively and computationally plausible learning model for phonological rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "A fundamental debate in the machine learning of language has been the role of prior knowledge in the learning process.", "labels": [], "entities": [{"text": "machine learning of language", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.794157437980175}]}, {"text": "Nativist models suggest that learning in a complex domain like natural language requires that the learning mechanism either have some previous knowledge about language, or some learning bias that helps direct the formation of correct generalizations.", "labels": [], "entities": []}, {"text": "In linguistics, theories of such prior knowledge are referred to as Universal Grammar (UG); nativist linguistic models of learning assume, implicitly or explicitly, that some kind of prior knowledge that contributes to language learning is innate, a product of evolution.", "labels": [], "entities": [{"text": "Universal Grammar (UG)", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.686341542005539}]}, {"text": "Despite sharing this assumption, nativist researchers disagree strongly about the exact constitution of this Universal Grammar.", "labels": [], "entities": []}, {"text": "Many models, for example, assume that much of the prior knowledge that children bring to bear in learning language is not linguistic at all, but derives from constraints imposed by our general cognitive architecture.", "labels": [], "entities": []}, {"text": "Others, such the influential Principles and Parameters model, assert that what is innate is linguistic knowledge itself, and that the learning process consists mainly of searching for the values of a relatively small number of parameters.", "labels": [], "entities": []}, {"text": "Such nativist models of phonological learning include, for example, model of the acquisition of stress-assignment rules, and model of learning in Optimality Theory.", "labels": [], "entities": []}, {"text": "Other scholars have argued that a purely nativist, parameterized learning algorithm is incapable of dealing with the noise, irregularity, and great variation of human language data, and that a more empiricist learning paradigm is possible.", "labels": [], "entities": []}, {"text": "Such datadriven models include the stress acquisition models of (an application of Instance-based Learning) and (an application of Error Back-Propagation), as well as Ellison's (1992) Minimum-Description-Length-based model of the acquisition of the basic concepts of syllabicity and the sonority hierarchy.", "labels": [], "entities": [{"text": "stress acquisition", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7167229652404785}]}, {"text": "In each of these cases a general, domain-independent learning rule (BP, IBL, MDL) is used to learn directly from the data.", "labels": [], "entities": [{"text": "BP", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9068698883056641}, {"text": "IBL", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.5623531341552734}]}, {"text": "In this paper we suggest that an alternative to the purely nativist or purely empiricist learning paradigms is to represent the prior knowledge of language as a set of abstract learning biases, which guide an empirical inductive learning algorithm.", "labels": [], "entities": []}, {"text": "Such biases are implicit, for example, in the work of and, who induced decision trees to predict the realization of a phone in its context.", "labels": [], "entities": []}, {"text": "By initializing the decision-tree inducer with a set of phonological features, they essentially gave it a priori knowledge about the kind of phonological generalizations that the system might be expected to learn.", "labels": [], "entities": []}, {"text": "Our idea is that abstract biases from the domain of phonology, whether innate (i.e., part of UG) or merely learned prior to the learning of rules, can be used to guide a domain-independent empirical induction algorithm.", "labels": [], "entities": []}, {"text": "We test this idea by examining the machine learning of simple Sound Pattern of English (SPE)-style phonological rules, beginning by representing phonological rules as finitestate transducers that accept underlying forms as input and generate surface forms as output.", "labels": [], "entities": [{"text": "Sound Pattern of English (SPE)-style phonological rules", "start_pos": 62, "end_pos": 117, "type": "TASK", "confidence": 0.7106797456741333}]}, {"text": "first observed that traditional phonological rewrite rules can be expressed as regular (finite-state) relations if one accepts the constraint that no rule may reapply directly to its own output.", "labels": [], "entities": []}, {"text": "This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms.", "labels": [], "entities": []}, {"text": "The fact that the weaker generative capacity of FSTs makes them easier to learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata (FSAs) (), deterministic transducers, as well as nondeterministic (stochastic) FSAs.", "labels": [], "entities": []}, {"text": "Like the empiricist models discussed above, these algorithms are all general-purpose; none include any domain knowledge about phonology, or indeed natural language; at most they include a bias toward simpler models (like the MDL-inspired algorithms of Ellison).", "labels": [], "entities": []}, {"text": "Our experiments were based on the OSTIA (Oncina, Garcia, and Vidal 1993) algorithm, which learns general subsequential finite-state transducers (SFSTs; formally defined in Section 2).", "labels": [], "entities": [{"text": "OSTIA (Oncina, Garcia, and Vidal 1993) algorithm", "start_pos": 34, "end_pos": 82, "type": "DATASET", "confidence": 0.7285820516673002}]}, {"text": "We presented pairs of underlying and surface forms to OSTIA, and examined the resulting transducers.", "labels": [], "entities": [{"text": "OSTIA", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.5739814639091492}]}, {"text": "Although OSTIA is capable of learning arbitrary SFSTs in the limit, large dictionaries of actual English pronunciations did not give enough samples to correctly induce phonological rules.", "labels": [], "entities": [{"text": "SFSTs", "start_pos": 48, "end_pos": 53, "type": "TASK", "confidence": 0.8613519668579102}]}, {"text": "We then augmented OSTIA with three kinds of learning biases, which are specific to natural language phonology, and are assumed explicitly or implicitly by every theory of phonology: faithfulness (underlying segments tend to be realized similarly on the surface), community (similar segments behave similarly), and context (phonological rules need access to variables in their context).", "labels": [], "entities": []}, {"text": "These biases are so fundamental to generative phonology that they are left implicit in many theories.", "labels": [], "entities": [{"text": "generative phonology", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.9343648254871368}]}, {"text": "But explicitly modifying the OSTIA algorithm with these biases allowed it to learn more compact, accurate, and general transducers, and our implementation successfully learns a number of rules from English and German.", "labels": [], "entities": []}, {"text": "The algorithm is also successful in learning the composition of multiple rules applied in series.", "labels": [], "entities": []}, {"text": "The more difficult problem of decomposing the learned underlying/surface correspondences into simple, individual rules remains unsolved.", "labels": [], "entities": []}, {"text": "Our transducer induction algorithm is not intended as a cognitive model of human phonological learning.", "labels": [], "entities": [{"text": "transducer induction algorithm", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.7907799084981283}]}, {"text": "First, for reasons of simplicity, we base our model on simple segmental SPE-style rules; it is not clear what the formal correspondence is of these rules to the more recent theoretical machinery of phonology (e.g., optimality constraints).", "labels": [], "entities": []}, {"text": "Second, we assume that a cognitive model of automaton induction would be more stochastic and hence more robust than the OSTIA algorithm underlying our work.", "labels": [], "entities": [{"text": "automaton induction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7328396737575531}]}, {"text": "1 Rather, our model is intended to suggest the kind of biases that maybe added to empiricist induction models to build a learning model for phonological rules that is cognitively and computationally plausible.", "labels": [], "entities": []}, {"text": "Furthermore, our model is not necessarily nativist; these biases maybe innate, but they may also be the product of some other earlier learning algorithm, as the results of and suggest (see Section 5.2).", "labels": [], "entities": []}, {"text": "So our results suggest that assuming in the system some very general and fundamental properties of phonological knowledge (whether innate or previously learned) and learning others empirically may provide a basis for future learning models.", "labels": [], "entities": []}, {"text": ", for example, has shown how to map the optimality constraints of to finite-state automata; given this result, models of 1 Although our assumption of the simultaneous presentation of surface and underlying forms to the learner may seem at first glance to be unnatural as well, it is quite compatible with certain theories of word-based morphology.", "labels": [], "entities": []}, {"text": "For example, in the word-based morphology of, word-formation rules apply only to already existing words.", "labels": [], "entities": []}, {"text": "Thus the underlying form for any morphological rule must be a word of the language.", "labels": [], "entities": []}, {"text": "Even if this word-based morphology assumption holds only fora subset of the language (see e.g., Orgun) it is not unreasonable to assume that apart of the learning process will involve previously-identified underlying/surface pairs.", "labels": [], "entities": []}, {"text": "automaton induction enriched in the way we suggest may contribute to the current debate on optimality learning.", "labels": [], "entities": []}, {"text": "This may obviate the need to build in every phonological constraint, as for example nativist models of OT learning suggest.", "labels": [], "entities": [{"text": "OT learning", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.9230164587497711}]}, {"text": "We hope in this way to begin to help assess the role of computational phonology in answering the general question of the necessity and nature of linguistic innateness in learning.", "labels": [], "entities": []}, {"text": "The next sections (2 and 3) introduce the idea of representing phonological rules with transducers, and describe the OSTIA algorithm for inducing such transducers.", "labels": [], "entities": []}, {"text": "Section 4 shows that the unaugmented OSTIA algorithm is unable to induce the correct transducer for the simple flapping rule of American English.", "labels": [], "entities": []}, {"text": "Section 5 then describes each of the augmentations to OSTIA, based on the faithfulness, community, and context principles.", "labels": [], "entities": [{"text": "OSTIA", "start_pos": 54, "end_pos": 59, "type": "TASK", "confidence": 0.7925134301185608}]}, {"text": "We conclude with some observations about computational complexity and the inherent bias of the context-sensitive rewrite-rule formalism.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  A slightly expanded ARPAbet phoneset  (including alveolar flap, syllabic nasals and  liquids, and reduced vowels), and the  corresponding IPA symbols. Vowels may be  annotated with the numbers 1 and 2 to  indicate primary and secondary stress,  respectively.", "labels": [], "entities": [{"text": "ARPAbet phoneset", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.7095018625259399}]}, {"text": " Table 2  Unmodified OSTIA learning  flapping on 49,280-word test  set. Error rates are the  percentage of incorrect  transductions.", "labels": [], "entities": [{"text": "OSTIA learning  flapping", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8583986361821493}, {"text": "Error", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9943206310272217}]}, {"text": " Table 6  Results on three rules composed.", "labels": [], "entities": []}, {"text": " Table 7  Results on three rules composed;  12,500 training size, 49,280 test size.", "labels": [], "entities": []}, {"text": " Table 8  Results on German word-final stop devoicing;  50,000-word test set.", "labels": [], "entities": [{"text": "German word-final stop devoicing", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.54204261302948}]}]}