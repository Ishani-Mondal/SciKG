{"title": [{"text": "Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies", "labels": [], "entities": [{"text": "Improving Statistical Language Model Performance", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8841557979583741}]}], "abstractContent": [{"text": "An automatic word-classification system has been designed that uses word unigram and bigram frequency statistics to implement a binary top-down form of word clustering and employs an average class mutual information metric.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 152, "end_pos": 167, "type": "TASK", "confidence": 0.7183151543140411}]}, {"text": "Words are represented as structural tags-n-bit numbers the most significant bit-patterns of which incorporate class information.", "labels": [], "entities": []}, {"text": "The classification system has revealed some of the lexical structure of English, as well as some phonemic and semantic structure.", "labels": [], "entities": []}, {"text": "The system has been compared-directly and indirectly-with other recent word-classification systems.", "labels": [], "entities": []}, {"text": "We see our classification as a means towards the end of constructing multilevel class-based interpolated language models.", "labels": [], "entities": []}, {"text": "We have built some of these models and carried out experiments that show a 7% drop in test set perplexity compared to a standard interpolated trigram language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings; that is, by using statistical language model information (.", "labels": [], "entities": []}, {"text": "For example, speech recognition systems often require some model of the prior likelihood of a given utterance).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7974415421485901}]}, {"text": "For convenience, the quality of these components can be measured by test set perplexity, PP, in spite of some limitations: PP = P(wlN) -~, where there are N words in the word stream (w~/and ibis some estimate of the probability of that word stream.", "labels": [], "entities": []}, {"text": "Perplexity is related to entropy, so our goal is to find models that estimate a low perplexity for some unseen representative sample of the language being modeled.", "labels": [], "entities": []}, {"text": "Also, since entropy provides a lower bound on the average code length, the project of statistical language modeling makes some connections with text compression--good compression algorithms correspond to good models of the source that generated the text in the first place.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.8050160606702169}, {"text": "text compression", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.6965031623840332}]}, {"text": "With an arbitrarily chosen standard test set, statistical language models can be compared . This allows researchers to make incremental improvements to the models (.", "labels": [], "entities": []}, {"text": "It is in this context that we investigate automatic word classification; also, some cognitive scientists are interested in those features of automatic word classification that have implications for language acquisition.", "labels": [], "entities": [{"text": "automatic word classification", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6195116440455118}, {"text": "automatic word classification", "start_pos": 141, "end_pos": 170, "type": "TASK", "confidence": 0.7280708352724711}, {"text": "language acquisition", "start_pos": 198, "end_pos": 218, "type": "TASK", "confidence": 0.7015529423952103}]}, {"text": "One common model of language calculates the probability of the ith word wi in a test set by considering then -1 most recent words i-1 (wi_,+l> in a more compact notation.", "labels": [], "entities": []}, {"text": "The model is finitary (according to the Chomsky hierarchy) and linguistically naive, but it has the advantage of being easy to construct and its structure allows the application of Markov model theory.", "labels": [], "entities": []}, {"text": "Much work has been carried out on word-based n-gram models, although there are recognized weaknesses in the paradigm.", "labels": [], "entities": []}, {"text": "One such problem concerns the way that n-grams partition the space of possible word contexts.", "labels": [], "entities": []}, {"text": "In estimating the probability of the ith word in a word stream, the model considers all previous word contexts to be identical if and only if they share the same final n -1 words.", "labels": [], "entities": []}, {"text": "This simultaneously fails to differentiate some linguistically important contexts and unnecessarily fractures others.", "labels": [], "entities": []}, {"text": "For example, if we restrict our consideration to the two previous words in a stream--that is, to the trigram conditional probability estimate P(wilwi-~)--then the sentences: (1) a.", "labels": [], "entities": []}, {"text": "The boys eat the sandwiches quickly.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}