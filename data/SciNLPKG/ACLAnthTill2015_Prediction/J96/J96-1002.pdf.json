{"title": [{"text": "A Maximum Entropy Approach to Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times.", "labels": [], "entities": []}, {"text": "Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition.", "labels": [], "entities": [{"text": "statistical estimation", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.8492187857627869}, {"text": "pattern recognition", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.8800751566886902}]}, {"text": "In this paper, we describe a method for statistical modeling based on maximum entropy.", "labels": [], "entities": [{"text": "statistical modeling", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8450600504875183}]}, {"text": "We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical modeling addresses the problem of constructing a stochastic model to predict the behavior of a random process.", "labels": [], "entities": [{"text": "Statistical modeling", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.819232314825058}]}, {"text": "In constructing this model, we typically have at our disposal a sample of output from the process.", "labels": [], "entities": []}, {"text": "Given this sample, which represents an incomplete state of knowledge about the process, the modeling problem is to parlay this knowledge into a representation of the process.", "labels": [], "entities": []}, {"text": "We can then use this representation to make predictions about the future behavior about the process.", "labels": [], "entities": []}, {"text": "Baseball managers (who rank among the better paid statistical modelers) employ batting averages, compiled from a history of at-bats, to gauge the likelihood that a player will succeed in his next appearance at the plate.", "labels": [], "entities": []}, {"text": "Thus informed, they manipulate their lineups accordingly.", "labels": [], "entities": []}, {"text": "Wall Street speculators (who rank among the best paid statistical modelers) build models based on past stock price movements to predict tomorrow's fluctuations and alter their portfolios to capitalize on the predicted future.", "labels": [], "entities": [{"text": "Wall", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9338001608848572}]}, {"text": "At the other end of the pay scale reside natural language researchers, who design language and acoustic models for use in speech recognition systems and related applications.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7142443507909775}]}, {"text": "The past few decades have witnessed significant progress toward increasing the predictive capacity of statistical models of natural language.", "labels": [], "entities": []}, {"text": "In language modeling, for instance, have used decision tree models and Della  have used automatically inferred link grammars to model long range correlations in language.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7257959842681885}]}, {"text": "In parsing, have described how to extract grammatical rules from annotated text automatically and incorporate these rules into statistical models of grammar.", "labels": [], "entities": []}, {"text": "In speech recognition, have introduced a technique for automatically discovering relevant features for the translation of word spelling to word pronunciation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.794474720954895}, {"text": "translation of word spelling to word pronunciation", "start_pos": 107, "end_pos": 157, "type": "TASK", "confidence": 0.7551649979182652}]}, {"text": "These efforts, while varied in specifics, all confront two essential tasks of statistical modeling.", "labels": [], "entities": [{"text": "statistical modeling", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8747561275959015}]}, {"text": "The first task is to determine a set of statistics that captures the behavior of a random proceSs.", "labels": [], "entities": []}, {"text": "Given a set of statistics, the second task is to corral these facts into an accurate model of the process--a model capable of predicting the future output of the process.", "labels": [], "entities": []}, {"text": "The first task is one of feature selection; the second is one of model selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.692555695772171}, {"text": "model selection", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7262066304683685}]}, {"text": "In the following pages we present a unified approach to these two tasks based on the maximum entropy philosophy.", "labels": [], "entities": []}, {"text": "In Section 2 we give an overview of the maximum entropy philosophy and work through a motivating example.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the mathematical structure of maximum entropy models and give an efficient algorithm for estimating the parameters of such models.", "labels": [], "entities": []}, {"text": "In Section 4 we discuss feature selection, and present an automatic method for discovering facts about a process from a sample of output from the process.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6998087167739868}]}, {"text": "We then present a series of refinements to the method to make it practical to implement.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we describe the application of maximum entropy ideas to several tasks in stochastic language processing: bilingual sense disambiguation, word reordering, and sentence segmentation.", "labels": [], "entities": [{"text": "bilingual sense disambiguation", "start_pos": 127, "end_pos": 157, "type": "TASK", "confidence": 0.6395694216092428}, {"text": "word reordering", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.7681853175163269}, {"text": "sentence segmentation", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.725703164935112}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2  Most frequent French translations of in as  estimated using EM-training. (OTHER)  represents a catch-all classifier for any  French phrase not listed, none of which  had a probability exceeding 0.0043.", "labels": [], "entities": [{"text": "OTHER)", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9769372642040253}]}, {"text": " Table 6  Maximum entropy model to predict French translation of to run:  top-ranked features not from template 1.", "labels": [], "entities": [{"text": "French translation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.5099502950906754}]}, {"text": " Table 9  NOUN de NOUN model performance: simple approach vs. maximum entropy.", "labels": [], "entities": []}]}